torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_deepspeech --submission_path=reference_algorithms/target_setting_algorithms/pytorch_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_deepspeech/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_deepspeech_pytorch/nadamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=36000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --torch_compile=true 2>&1 | tee -a /logs/librispeech_deepspeech_pytorch_10-04-2023-19-53-17.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-10-04 19:53:27.283946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:53:27.283946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:53:27.283947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:53:27.283946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:53:27.283949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:53:27.283946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:53:27.283954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:53:27.283957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1004 19:53:41.733643 140275149285184 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I1004 19:53:41.733679 139833187854144 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I1004 19:53:41.733703 140374149367616 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I1004 19:53:41.733721 140001428408128 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I1004 19:53:41.734658 139742330496832 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I1004 19:53:41.734687 139904569501504 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I1004 19:53:41.734916 139975777949504 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I1004 19:53:41.735216 139975777949504 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1004 19:53:41.735136 139809401644864 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I1004 19:53:41.735507 139809401644864 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1004 19:53:41.744403 140275149285184 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1004 19:53:41.744434 139833187854144 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1004 19:53:41.744464 140374149367616 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1004 19:53:41.744510 140001428408128 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1004 19:53:41.745187 139742330496832 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1004 19:53:41.745217 139904569501504 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1004 19:53:42.214320 139975777949504 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_deepspeech_pytorch/nadamw_run_0/librispeech_deepspeech_pytorch.
W1004 19:53:42.973935 140275149285184 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1004 19:53:42.973936 139904569501504 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1004 19:53:42.973934 140374149367616 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1004 19:53:42.973942 139975777949504 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1004 19:53:42.973978 139809401644864 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1004 19:53:42.974206 140001428408128 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1004 19:53:42.974860 139833187854144 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1004 19:53:42.975050 139742330496832 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I1004 19:53:42.979728 139975777949504 submission_runner.py:507] Using RNG seed 3473552374
I1004 19:53:42.981729 139975777949504 submission_runner.py:516] --- Tuning run 1/1 ---
I1004 19:53:42.981868 139975777949504 submission_runner.py:521] Creating tuning directory at /experiment_runs/targets_check_deepspeech_pytorch/nadamw_run_0/librispeech_deepspeech_pytorch/trial_1.
I1004 19:53:42.982088 139975777949504 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_deepspeech_pytorch/nadamw_run_0/librispeech_deepspeech_pytorch/trial_1/hparams.json.
I1004 19:53:42.982941 139975777949504 submission_runner.py:191] Initializing dataset.
I1004 19:53:42.983077 139975777949504 input_pipeline.py:20] Loading split = train-clean-100
I1004 19:53:43.020395 139975777949504 input_pipeline.py:20] Loading split = train-clean-360
I1004 19:53:43.414268 139975777949504 input_pipeline.py:20] Loading split = train-other-500
I1004 19:53:43.901311 139975777949504 submission_runner.py:198] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
W1004 19:53:52.081689 140001428408128 submission_runner.py:219] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1004 19:53:52.082639 139742330496832 submission_runner.py:219] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1004 19:53:52.082988 139975777949504 submission_runner.py:219] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1004 19:53:52.084471 140374149367616 submission_runner.py:219] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1004 19:53:52.085989 140275149285184 submission_runner.py:219] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1004 19:53:52.087912 139904569501504 submission_runner.py:219] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1004 19:53:52.098708 139833187854144 submission_runner.py:219] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1004 19:53:52.101286 139809401644864 submission_runner.py:219] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
I1004 19:53:52.393388 139975777949504 submission_runner.py:232] Initializing optimizer.
I1004 19:53:52.394320 139975777949504 submission_runner.py:239] Initializing metrics bundle.
I1004 19:53:52.394456 139975777949504 submission_runner.py:257] Initializing checkpoint and logger.
I1004 19:53:52.395037 139975777949504 submission_runner.py:277] Saving meta data to /experiment_runs/targets_check_deepspeech_pytorch/nadamw_run_0/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I1004 19:53:52.395291 139975777949504 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1004 19:53:52.395366 139975777949504 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1004 19:53:52.643467 139975777949504 logger_utils.py:220] Unable to record git information. Continuing without it.
I1004 19:53:52.872894 139975777949504 submission_runner.py:280] Saving flags to /experiment_runs/targets_check_deepspeech_pytorch/nadamw_run_0/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I1004 19:53:52.885890 139975777949504 submission_runner.py:290] Starting training loop.
[2023-10-04 19:53:55,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:55,232] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:55,321] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:55,335] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:55,515] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:55,555] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:55,556] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:55,592] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:56,267] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:56,283] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:56,287] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:56,288] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:56,393] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:56,393] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:56,393] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:56,394] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:56,404] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:56,404] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:56,405] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:56,405] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:56,414] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:56,415] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:56,415] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:56,415] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:56,418] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:56,419] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:56,419] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:56,419] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:56,462] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:56,478] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:56,511] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:56,511] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:56,576] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:56,576] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:56,577] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:56,577] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:56,594] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:56,594] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:56,594] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:56,594] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:56,642] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:56,642] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:56,643] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:56,643] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:56,691] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:56,691] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:56,691] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:56,692] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-04 19:53:57,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-04 19:53:57,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-04 19:53:57,100] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-04 19:53:57,110] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,110] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,110] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,110] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,110] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,110] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,110] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,110] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,111] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,111] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,111] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,112] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,114] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,114] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,114] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,114] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,120] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-04 19:53:57,122] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-04 19:53:57,125] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-04 19:53:57,128] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-04 19:53:57,128] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-04 19:53:57,129] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-04 19:53:57,132] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,132] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,132] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,132] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,134] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,134] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-04 19:53:57,134] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,134] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,134] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,136] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-04 19:53:57,136] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-04 19:53:57,137] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,137] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,137] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,137] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,137] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-04 19:53:57,138] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,138] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,138] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,138] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,138] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,138] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,138] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,138] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,139] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,139] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,140] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,140] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-04 19:53:57,144] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-04 19:53:57,145] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,145] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,146] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,146] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-04 19:53:57,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-04 19:53:57,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-04 19:53:57,150] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-04 19:53:57,153] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-04 19:53:57,153] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,154] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,154] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,154] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,157] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-04 19:53:57,158] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,159] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,159] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,159] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,165] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-04 19:53:57,170] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-04 19:53:57,171] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-04 19:53:57,174] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-04 19:53:57,179] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-04 19:53:57,181] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,181] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,181] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,181] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-04 19:53:57,183] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-04 19:53:57,184] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,184] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,184] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,184] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,185] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,185] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,185] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,185] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,188] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-04 19:53:57,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-04 19:53:57,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-04 19:53:57,192] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-04 19:53:57,194] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,194] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-04 19:53:57,194] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,194] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,194] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-04 19:53:57,194] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,194] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,194] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,195] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,195] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,195] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,195] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,196] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,196] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,208] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-04 19:53:57,209] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,209] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,210] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,211] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,211] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,212] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,212] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,216] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-04 19:53:57,219] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,219] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,220] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,220] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,221] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-04 19:53:57,232] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:53:57,232] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:53:57,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:53:57,235] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,235] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,235] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,235] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,235] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,235] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,235] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,235] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,235] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,235] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,235] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,235] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,236] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,236] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,237] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-04 19:53:57,237] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,237] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,240] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,240] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,240] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,240] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,242] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-04 19:53:57,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:53:57,246] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,246] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,246] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,246] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,247] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,247] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,248] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,248] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,267] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,268] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:53:57,271] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,271] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,271] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,271] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:53:57,276] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,276] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,276] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,276] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,280] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:53:57,281] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,282] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,283] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,283] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,283] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,287] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,292] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:53:57,299] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:57,299] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:57,300] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:57,300] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:57,325] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,627] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,649] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,665] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:53:57,969] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:57,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:57,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:57,972] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:57,982] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:57,982] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:58,011] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:58,021] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:58,113] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,114] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,115] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,122] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,123] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,123] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,123] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,124] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,124] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,124] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,124] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,124] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,124] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,124] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,124] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,127] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,127] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,128] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,128] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,128] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,133] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,138] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,138] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,138] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,138] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,142] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:58,143] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:58,143] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:58,143] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:58,143] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,143] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,143] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,144] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,151] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:58,155] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:58,162] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,171] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,172] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,172] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,172] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,179] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,179] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,180] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,180] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,183] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:58,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:53:58,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,264] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,264] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,264] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,264] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,265] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,265] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,266] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,267] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,267] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,267] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,268] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,268] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,268] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,271] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,274] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,274] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,274] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,277] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,278] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,278] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,299] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,305] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,305] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,306] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,308] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:53:58,313] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:53:58,314] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:53:58,314] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:53:58,330] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,332] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,332] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,333] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,339] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,343] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,370] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:53:58,377] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:02,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-04 19:54:02,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-04 19:54:02,348] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-04 19:54:02,348] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-04 19:54:02,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-04 19:54:02,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-04 19:54:02,356] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-04 19:54:02,357] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-04 19:54:02,360] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:02,361] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-04 19:54:02,364] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:02,369] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:02,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-04 19:54:02,429] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-04 19:54:02,433] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-04 19:54:02,436] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:02,436] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:02,437] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:02,437] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:02,441] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:02,443] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:02,443] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:02,443] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:02,443] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:02,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:02,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:02,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:02,450] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:02,455] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-04 19:54:02,463] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-04 19:54:02,467] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-04 19:54:02,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-04 19:54:02,474] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:02,476] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-04 19:54:02,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-04 19:54:02,488] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:02,520] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:02,520] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:02,520] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:02,520] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:02,551] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:02,551] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:02,552] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:02,552] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:02,570] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:02,570] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:02,571] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:02,571] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:02,572] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-04 19:54:02,580] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-04 19:54:02,584] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-04 19:54:02,591] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-04 19:54:02,591] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:02,599] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-04 19:54:02,603] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-04 19:54:02,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:02,670] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:02,670] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:02,670] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:02,670] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:02,690] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:02,690] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:02,691] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:02,691] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:02,704] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,704] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,705] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,705] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,705] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,705] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,705] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,706] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,751] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:02,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:03,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,119] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,121] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,125] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,132] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,135] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,142] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,195] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,195] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,195] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,195] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,197] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,197] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,197] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,197] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,197] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,197] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,197] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,197] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,202] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,202] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,202] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,202] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,203] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,203] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,203] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,203] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,210] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,210] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,211] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,211] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,212] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,212] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,212] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,212] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,219] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,219] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,219] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,219] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,249] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,249] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,249] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,249] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,323] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,323] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,324] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,324] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,324] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,324] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,325] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,325] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,325] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,325] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,325] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,325] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,325] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,325] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,325] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,325] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,325] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,325] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,326] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,326] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,326] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,326] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,326] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,326] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,326] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,326] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,326] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,326] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,327] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,327] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,327] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,328] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,357] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,357] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,357] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,357] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,357] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,358] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,358] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,358] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,433] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,433] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,433] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,433] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,433] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,433] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,433] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,433] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,433] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,433] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,433] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,434] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,434] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,434] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,434] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,435] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,435] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,435] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,439] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,439] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,440] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,440] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,471] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,545] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,545] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,546] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,546] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,546] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,546] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,546] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,546] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,546] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,546] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,546] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,546] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,547] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,547] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,547] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,547] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,547] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,547] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,547] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,547] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,547] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,547] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,547] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,547] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,548] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,548] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,579] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,579] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,652] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,652] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,652] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,652] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,653] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,653] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,653] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,653] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,653] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,653] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,653] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,653] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,654] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,654] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,654] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,654] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,654] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,654] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,654] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,654] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,654] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,654] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,654] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,655] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,655] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,655] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,655] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,655] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,655] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,656] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,656] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,656] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,684] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,770] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,770] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,772] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,772] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,772] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,772] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,776] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,776] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,776] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,776] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,776] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,777] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,777] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,777] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,777] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,778] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,778] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,778] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,778] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,778] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,778] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,778] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,779] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,779] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,779] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,779] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,779] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,779] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,779] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,779] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,780] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,780] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,781] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,781] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,781] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,781] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,781] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,782] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,787] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,787] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,787] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,788] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,789] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,789] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,790] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,870] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,872] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,876] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,876] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,876] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,876] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,877] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,877] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,878] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,878] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,878] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,879] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,880] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,881] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,881] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,881] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,881] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,882] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,882] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,882] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,882] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,882] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,882] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,883] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,883] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,883] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,883] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,884] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,887] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,887] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,888] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,888] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,888] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,890] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,890] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,890] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,891] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,891] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,893] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,894] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,894] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,894] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,899] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,901] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,976] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,978] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,978] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,979] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,982] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,982] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,983] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,983] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,983] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,983] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,984] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,984] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,984] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,984] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,984] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,984] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,984] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,985] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,985] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,985] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,985] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,986] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,986] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,986] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,986] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,989] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,989] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:03,989] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,989] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,989] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,989] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,989] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,990] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,990] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,992] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,994] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:03,994] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:03,995] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:03,995] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:03,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,996] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:03,999] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:04,000] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:04,003] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:04,013] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:04,014] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:04,014] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:04,015] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:04,015] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:04,016] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:04,016] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:04,016] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:04,016] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:04,016] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:04,016] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:04,017] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:04,017] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:04,017] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:04,017] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:04,017] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:04,017] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:04,018] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:04,018] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:04,018] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:04,018] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:04,018] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:04,018] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:04,019] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:04,019] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:04,020] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:04,020] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:04,022] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:04,022] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:04,022] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:04,022] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:04,022] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:04,022] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:04,023] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:04,023] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:04,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:04,027] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:04,027] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:04,028] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:04,028] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I1004 19:54:29.538554 139949764507392 logging_writer.py:48] [0] global_step=0, grad_norm=18.525835, loss=33.732407
I1004 19:54:29.566605 139975777949504 pytorch_submission_base.py:86] 0) loss = 33.732, grad_norm = 18.526
I1004 19:54:29.946650 139975777949504 spec.py:321] Evaluating on the training split.
I1004 19:54:29.947743 139975777949504 input_pipeline.py:20] Loading split = train-clean-100
I1004 19:54:29.983325 139975777949504 input_pipeline.py:20] Loading split = train-clean-360
I1004 19:54:30.113161 139975777949504 input_pipeline.py:20] Loading split = train-other-500
[2023-10-04 19:54:32,680] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:32,799] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:32,827] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:32,855] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:32,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:32,927] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:32,944] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:32,945] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:32,945] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:32,946] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:32,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:32,999] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,012] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,020] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:33,031] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,031] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,032] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,032] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,046] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,048] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,068] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,074] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,091] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,091] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,092] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,092] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,131] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:33,134] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,134] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,135] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,135] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,135] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,135] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,136] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,136] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,144] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,170] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:33,180] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,180] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,181] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,181] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,182] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,182] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,183] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,183] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,193] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,194] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,197] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:33,202] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,203] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,203] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,203] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,204] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,204] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,204] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,205] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,213] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,220] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,222] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:33,224] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,224] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,225] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,225] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,247] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:33,250] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,250] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,251] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,251] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,260] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,276] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,280] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:33,286] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,286] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,286] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,286] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,288] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,288] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,288] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,301] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,307] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,320] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,320] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,320] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,321] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,321] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,321] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,321] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,321] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,329] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:33,332] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,332] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,333] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,333] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,334] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,336] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:33,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,348] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,349] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,349] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,358] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,359] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,359] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,360] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,360] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:33,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:33,365] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,365] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,365] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,365] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,365] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,366] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,366] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,366] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,374] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:33,375] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,375] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,381] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,381] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,382] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,390] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,397] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:33,404] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,404] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,404] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,424] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,436] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,437] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,437] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,437] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,444] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,453] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,455] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,457] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,477] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,484] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:33,496] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,496] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,496] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,505] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,512] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,512] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,512] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:33,513] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,513] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,519] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,520] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,520] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,524] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:33,526] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,527] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,527] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,528] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,528] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,531] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,531] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,531] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,531] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,531] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,531] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,532] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,539] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,557] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,558] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,558] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,559] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,559] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,565] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,569] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,590] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,592] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,599] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,605] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,605] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,606] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,606] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,607] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,610] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,610] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,611] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,611] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,624] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,628] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,632] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,633] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,635] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,636] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,638] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,649] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,650] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,651] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,651] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,651] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,662] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,662] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,679] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,679] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,680] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,680] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,680] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,680] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,680] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,681] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,684] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,685] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,685] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,685] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,688] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,688] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,688] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,689] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,689] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,689] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,689] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,689] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,707] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,707] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,707] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,708] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,708] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,712] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,717] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,734] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,736] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,737] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,737] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,737] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,752] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,753] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,753] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,754] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,754] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,758] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,758] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,758] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,765] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,767] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,767] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,768] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,768] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,771] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,781] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,781] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,782] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,782] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,787] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,793] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,797] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,807] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:33,809] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,812] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,812] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,812] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,812] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,830] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,830] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,830] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,831] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,834] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,834] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,835] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,835] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,835] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,839] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,847] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,847] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,847] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,848] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,849] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,850] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,850] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,850] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,850] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,850] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,851] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,851] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,862] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,864] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,864] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,865] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,865] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,876] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,880] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,882] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,883] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,883] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,884] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,884] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,886] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,886] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,887] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,887] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,895] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,917] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,917] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,918] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,918] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,924] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,924] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,925] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,925] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,929] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,929] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,929] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,929] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,936] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,937] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,938] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,938] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,953] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,956] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,959] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,959] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,960] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,960] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,961] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,961] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:33,962] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,962] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,969] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:33,969] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:33,969] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:33,969] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:33,972] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,979] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,986] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,989] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:33,996] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,003] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,003] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,003] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,004] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,006] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,007] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,007] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,007] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,032] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,032] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,033] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,033] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,034] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,035] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,036] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,036] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,036] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,036] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,036] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,036] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,036] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,044] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,044] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,045] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,045] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,058] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,062] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,067] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,067] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,085] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,086] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,086] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,086] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,089] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,089] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,089] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,090] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,100] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,112] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,112] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,112] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,112] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,115] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,122] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,132] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,132] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,133] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,133] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,134] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,134] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,134] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,135] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,138] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,164] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,164] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,166] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,168] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,168] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,169] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,169] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,169] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,169] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,169] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,169] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,175] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,175] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,175] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,175] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,179] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,180] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,180] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,181] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,181] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,193] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,193] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,194] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,194] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,198] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,202] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,202] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,202] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,203] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,215] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,223] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,224] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,225] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,225] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,243] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,254] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,254] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,255] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,255] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,263] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,287] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,288] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,290] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,290] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,291] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,291] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,311] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,312] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,315] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,315] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,316] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,316] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,331] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,332] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,333] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,333] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,334] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,334] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,343] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,344] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,344] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,344] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,344] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,344] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,344] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,344] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,345] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,347] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,353] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,354] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,383] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,383] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,384] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,384] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,396] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,405] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,410] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,410] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,411] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,411] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,411] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,412] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,412] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,412] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,419] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,422] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,424] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,424] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,425] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,425] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,425] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,433] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,435] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,436] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,436] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,436] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,436] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,444] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,459] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,463] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,464] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,464] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,464] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,471] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,471] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,471] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,472] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,484] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,485] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,488] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,489] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,489] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,489] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,490] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,490] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,491] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,491] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,496] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,498] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,500] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,500] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,500] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,501] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,501] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,502] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,509] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,517] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,518] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,518] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,518] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,520] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,523] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,524] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,524] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,524] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,528] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,532] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,535] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,535] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,536] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,536] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,546] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,550] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,550] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,551] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,551] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,568] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,568] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,569] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,569] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,575] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,584] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,589] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,589] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,589] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,590] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,593] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,598] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,598] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,600] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,600] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,601] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,601] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,616] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,619] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,619] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,619] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,619] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,658] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,662] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,663] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,663] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,663] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:34,673] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:34,694] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:34,696] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:34,696] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:34,697] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:34,697] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I1004 19:54:48.640525 139975777949504 spec.py:333] Evaluating on the validation split.
I1004 19:54:48.641848 139975777949504 input_pipeline.py:20] Loading split = dev-clean
I1004 19:54:48.646340 139975777949504 input_pipeline.py:20] Loading split = dev-other
[2023-10-04 19:54:59,528] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,532] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,614] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,628] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,634] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,671] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,710] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,714] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,714] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,715] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,715] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,715] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,722] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,722] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,723] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,723] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,727] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:59,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:59,748] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:59,750] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,751] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,751] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,751] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,755] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:59,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,758] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,758] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,758] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,758] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,765] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,775] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,804] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,805] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,805] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,805] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,813] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,817] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:59,826] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,831] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,839] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:59,841] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,841] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,841] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,841] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,848] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,863] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,863] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,864] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,864] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,876] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,877] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:59,878] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:59,883] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,884] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,884] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:59,891] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,892] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,892] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,901] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:59,901] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,902] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,902] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,902] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,903] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,904] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,904] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,904] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,911] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,914] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,914] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,914] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,914] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,917] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:59,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:54:59,938] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,939] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:59,941] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,941] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,941] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,942] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,944] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,947] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:54:59,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,949] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,949] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,950] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,950] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,956] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,957] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:54:59,966] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:54:59,967] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,967] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,967] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,967] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:54:59,972] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:54:59,972] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:54:59,973] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:54:59,979] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:00,000] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:55:00,003] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:00,003] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:00,003] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:00,003] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:00,010] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:00,030] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:00,031] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:00,031] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:00,031] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:00,033] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:00,035] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:00,042] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:00,042] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:00,042] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:00,046] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:00,068] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:55:00,071] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:00,071] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:00,071] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:00,071] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:00,071] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:00,077] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:00,078] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:00,078] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:00,078] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:00,079] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:00,085] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:00,085] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:00,085] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:00,106] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:00,133] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:00,139] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:00,139] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:00,139] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:00,141] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:00,146] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:00,197] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:00,201] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:00,203] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:00,203] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:00,204] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:00,264] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,374] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,399] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,417] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,417] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,418] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,418] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,425] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,439] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,443] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,444] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,444] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,444] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,451] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,457] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,467] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,468] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,468] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,468] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,498] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,500] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,500] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,500] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,501] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,510] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,522] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,530] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,543] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,543] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,544] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,544] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,550] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,561] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,561] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,561] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,561] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,565] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,565] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,565] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,565] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,570] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,571] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,571] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,571] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,580] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,584] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,584] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,590] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,596] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,598] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,610] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,620] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,620] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,621] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,621] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,626] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,626] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,626] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,626] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,628] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,628] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,629] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,629] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,629] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,629] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,629] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,629] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,636] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,640] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,640] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,640] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,641] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,641] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,645] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,648] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,648] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,650] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,654] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,654] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,655] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,655] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,662] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,662] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,663] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,663] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,663] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,665] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,666] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,670] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,678] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,680] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,680] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,681] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,681] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,683] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,689] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,689] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,689] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,689] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,689] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,689] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,690] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,690] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,700] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,708] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,708] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,709] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,709] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,710] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,710] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,715] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,715] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,716] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,716] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,717] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,726] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,726] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,727] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,727] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,728] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,728] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,729] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,729] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,736] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,736] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,736] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,736] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,736] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,737] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:02,740] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,740] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,741] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,741] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,746] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,748] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,748] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,748] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,748] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,748] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,751] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,751] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,751] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,751] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,754] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,768] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,770] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,778] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,779] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,779] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,779] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,788] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,788] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,789] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,789] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,790] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,790] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,791] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,791] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,795] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,795] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,795] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,795] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,799] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,801] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,801] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,801] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,801] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,807] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,807] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,808] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,808] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,808] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,808] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:02,811] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,811] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,812] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,812] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,812] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,813] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,813] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,814] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,814] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,814] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,820] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,823] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,827] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,833] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,841] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,841] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,842] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,842] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,847] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,847] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,848] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,848] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,852] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,853] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,853] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,853] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,854] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,854] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,855] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,855] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,862] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,863] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,864] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,864] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,864] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,866] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,870] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:02,874] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:02,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,875] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,875] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,875] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,878] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,878] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,878] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,878] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,883] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,883] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,884] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:02,886] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,887] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,888] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,888] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,888] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,896] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,903] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,904] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,904] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,904] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,906] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,906] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,906] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,906] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,911] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,911] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,911] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,911] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,919] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,919] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,919] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,919] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,924] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,924] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,924] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,924] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,928] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,930] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:02,934] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,934] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,934] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,934] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,935] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:02,938] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,938] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,939] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,939] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,940] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,941] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,944] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,944] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:02,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,948] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,948] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,948] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,948] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,956] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,960] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:02,962] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,963] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,963] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,963] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,965] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,965] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,965] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,966] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,967] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,967] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,967] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,967] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,969] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,970] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,970] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,970] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,984] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,984] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,985] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,985] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,985] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:02,990] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:02,993] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:02,993] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:02,994] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:02,994] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:02,998] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,001] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,001] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,002] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,002] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,002] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,004] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,004] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,008] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,008] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,008] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,008] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,008] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,014] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,027] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,027] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,028] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,028] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,032] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,033] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,035] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,035] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,035] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,035] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,035] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,035] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,036] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,036] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,036] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,037] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,039] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,039] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,039] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,039] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,041] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,041] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,041] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,041] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,043] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,044] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,045] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,045] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,045] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,047] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,050] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,051] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,055] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,055] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,055] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,055] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,062] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,064] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,091] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,095] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,095] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,095] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,095] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,095] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,099] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,099] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,099] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,099] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,102] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,102] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,103] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,103] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,103] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,107] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,110] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,111] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,112] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,114] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,115] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,115] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,115] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,116] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,116] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,117] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,117] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,122] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,123] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,143] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,143] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,144] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,144] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,153] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,156] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,156] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,156] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,157] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,157] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,159] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,160] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,160] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,160] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,160] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,163] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,163] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,163] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,164] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,164] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,166] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,170] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,173] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,177] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,177] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,177] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,177] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,183] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,183] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,183] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,183] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,185] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,186] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,186] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,187] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,187] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,189] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,192] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,192] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,192] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,192] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,231] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,234] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,235] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,235] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,235] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:03,240] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:03,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:03,260] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:03,261] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:03,261] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:03,261] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I1004 19:55:03.454387 139975777949504 spec.py:349] Evaluating on the test split.
I1004 19:55:03.455749 139975777949504 input_pipeline.py:20] Loading split = test-clean
[2023-10-04 19:55:09,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,095] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,119] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,141] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,145] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,145] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,172] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,187] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,200] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,206] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,206] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,206] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,206] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,220] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:09,241] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:55:09,244] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,244] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,244] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,244] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,246] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,249] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,251] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,274] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,275] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,275] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,275] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,276] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,286] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,286] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:09,289] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,290] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,290] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,290] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,301] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:09,308] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:55:09,310] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,310] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,311] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,311] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,318] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,323] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:55:09,326] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,326] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,326] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,326] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,333] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,335] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,335] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,336] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,336] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,337] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,337] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,337] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,338] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,341] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,342] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,342] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,342] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,346] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:09,348] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:09,353] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:09,365] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,365] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,366] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,366] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:09,370] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:55:09,371] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:55:09,372] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,372] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,372] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,373] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,373] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,373] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,373] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,373] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,373] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,374] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,374] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,376] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:55:09,378] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:09,378] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,378] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,379] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,379] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,379] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,379] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,379] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,379] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,381] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,382] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,386] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,392] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:09,401] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:55:09,403] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,403] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,403] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,404] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,411] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-04 19:55:09,418] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,418] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,419] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,419] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,426] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:09,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:09,448] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,449] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,449] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,464] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:09,470] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,470] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,471] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,508] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:09,509] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:09,513] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:09,515] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,515] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,516] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,516] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,516] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,516] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,517] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,520] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,520] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,520] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,536] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:09,543] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,544] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,544] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,556] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:09,563] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:09,563] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:09,564] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:09,583] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,820] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,916] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,952] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,962] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,977] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:09,998] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,304] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,345] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,345] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,345] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,346] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:11,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:11,380] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,422] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,422] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,423] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,423] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,441] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,481] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,481] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,482] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,482] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,500] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,540] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,540] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,540] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,540] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,560] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,598] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,599] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,599] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,599] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,602] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,603] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,603] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,603] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,610] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:11,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,622] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:11,640] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,679] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,679] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,679] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,679] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,738] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,738] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,738] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,738] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,796] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,796] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,797] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,797] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,803] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,803] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,803] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,804] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,811] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:11,816] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,822] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:11,839] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,856] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,856] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,856] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,856] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,879] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,879] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,879] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,880] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,936] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,937] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,937] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,937] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:11,955] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:11,995] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:11,995] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:11,995] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:11,995] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,001] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,013] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,021] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,027] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,040] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,040] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,041] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,041] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,044] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,044] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,045] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,045] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,050] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,052] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:12,054] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,055] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,055] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,055] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,064] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,064] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,065] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,065] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,067] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:12,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:12,072] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,072] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,072] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,072] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,073] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,080] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:12,086] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,086] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:12,094] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:12,096] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,097] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,097] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,097] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,100] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,101] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,101] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,101] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,104] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:12,109] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:12,110] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,113] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,114] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,114] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,114] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,114] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,114] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,115] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,115] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,115] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,122] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,129] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,129] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:12,130] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,130] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-04 19:55:12,130] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,130] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,133] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,145] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,146] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,146] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,146] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,151] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,155] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,155] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,155] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,155] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,165] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,174] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,175] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,178] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,178] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,179] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,179] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,184] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,184] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,185] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,185] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,187] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,192] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,194] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,194] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,194] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,194] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,194] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,194] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,194] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,194] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,196] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,196] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,196] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,196] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,206] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,206] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,207] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,207] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,215] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,215] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,215] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,215] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,215] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,218] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,220] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,225] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,236] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,240] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,240] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,240] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,240] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,247] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,247] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,248] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,248] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,248] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,256] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,258] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,258] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,258] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,259] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,260] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,260] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,260] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,260] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,263] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,263] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,264] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,264] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,266] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,266] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,266] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,266] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,267] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,269] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,269] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,270] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,270] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,274] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,275] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,275] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,275] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,278] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,282] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,282] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,282] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,282] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,282] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,285] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,286] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,294] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,301] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,304] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,307] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,307] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,308] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,308] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,313] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,321] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,321] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,321] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,321] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,324] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,324] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,324] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,324] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,327] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,327] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,327] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,328] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,329] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,330] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,330] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,330] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,332] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,335] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,335] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,335] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,335] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,335] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,335] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,336] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,336] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,341] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,345] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,347] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,349] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,352] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,352] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,353] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,353] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,354] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,360] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,381] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,381] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,382] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,382] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,387] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,387] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,387] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,387] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,387] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,387] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,388] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,388] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,393] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,393] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,393] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,393] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,393] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,393] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,393] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,394] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,401] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,406] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,409] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,409] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,413] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,413] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,413] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,413] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,415] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,450] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,451] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,451] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,452] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,452] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,454] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,455] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,455] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,455] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,455] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,458] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,458] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,458] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,458] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,459] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,460] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,462] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,463] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,463] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,463] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,471] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,472] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,472] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,472] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,473] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,473] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,477] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,479] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,497] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,500] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,500] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,500] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,500] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,509] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,512] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,512] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,513] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,513] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,516] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,519] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,520] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,520] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,520] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,520] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,523] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,523] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,524] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,524] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,525] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,525] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,525] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,526] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,526] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,528] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,529] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,529] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,529] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,530] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,569] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,572] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,572] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,572] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,572] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,576] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,580] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,580] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,580] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,580] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,580] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,584] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,584] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,585] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,585] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,585] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,585] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,586] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,586] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,588] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,591] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,592] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,592] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,592] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,592] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,594] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,598] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,600] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,600] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,600] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,601] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,601] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,608] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,608] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,610] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,611] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,611] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,611] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,611] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,611] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,611] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,611] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,643] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,646] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,646] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,647] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,647] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,648] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,651] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,651] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,651] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,652] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,653] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,658] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-04 19:55:12,672] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,674] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,675] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,675] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,675] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-04 19:55:12,677] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-04 19:55:12,679] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-04 19:55:12,679] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-04 19:55:12,680] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-04 19:55:12,680] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I1004 19:55:12.840740 139975777949504 submission_runner.py:381] Time since start: 79.96s, 	Step: 1, 	{'train/ctc_loss': 32.15358443060801, 'train/wer': 2.27367888995777, 'validation/ctc_loss': 31.020205445047218, 'validation/wer': 2.17933664848163, 'validation/num_examples': 5348, 'test/ctc_loss': 31.02377895348087, 'test/wer': 2.0662563727581094, 'test/num_examples': 2472, 'score': 36.68256998062134, 'total_duration': 79.95515942573547, 'accumulated_submission_time': 36.68256998062134, 'accumulated_eval_time': 42.89408588409424, 'accumulated_logging_time': 0}
I1004 19:55:12.861417 139935797470976 logging_writer.py:48] [1] accumulated_eval_time=42.894086, accumulated_logging_time=0, accumulated_submission_time=36.682570, global_step=1, preemption_count=0, score=36.682570, test/ctc_loss=31.023779, test/num_examples=2472, test/wer=2.066256, total_duration=79.955159, train/ctc_loss=32.153584, train/wer=2.273679, validation/ctc_loss=31.020205, validation/num_examples=5348, validation/wer=2.179337
I1004 19:55:13.209901 139975777949504 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1004 19:55:13.217604 139904569501504 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1004 19:55:13.217767 139742330496832 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1004 19:55:13.217631 140374149367616 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1004 19:55:13.217786 139833187854144 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1004 19:55:13.218395 140001428408128 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1004 19:55:13.218608 140275149285184 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1004 19:55:13.219125 139809401644864 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1004 19:55:14.187153 139935789078272 logging_writer.py:48] [1] global_step=1, grad_norm=18.537827, loss=33.106682
I1004 19:55:14.190743 139975777949504 pytorch_submission_base.py:86] 1) loss = 33.107, grad_norm = 18.538
I1004 19:55:15.274352 139935797470976 logging_writer.py:48] [2] global_step=2, grad_norm=27.806656, loss=33.395535
I1004 19:55:15.277838 139975777949504 pytorch_submission_base.py:86] 2) loss = 33.396, grad_norm = 27.807
I1004 19:55:16.191028 139935789078272 logging_writer.py:48] [3] global_step=3, grad_norm=61.550526, loss=32.285118
I1004 19:55:16.195641 139975777949504 pytorch_submission_base.py:86] 3) loss = 32.285, grad_norm = 61.551
I1004 19:55:17.104151 139935797470976 logging_writer.py:48] [4] global_step=4, grad_norm=46.717171, loss=30.271334
I1004 19:55:17.108371 139975777949504 pytorch_submission_base.py:86] 4) loss = 30.271, grad_norm = 46.717
I1004 19:55:18.018094 139935789078272 logging_writer.py:48] [5] global_step=5, grad_norm=38.303585, loss=28.966541
I1004 19:55:18.022233 139975777949504 pytorch_submission_base.py:86] 5) loss = 28.967, grad_norm = 38.304
I1004 19:55:18.939833 139935797470976 logging_writer.py:48] [6] global_step=6, grad_norm=30.858763, loss=27.761030
I1004 19:55:18.943974 139975777949504 pytorch_submission_base.py:86] 6) loss = 27.761, grad_norm = 30.859
I1004 19:55:19.851045 139935789078272 logging_writer.py:48] [7] global_step=7, grad_norm=28.623699, loss=25.523783
I1004 19:55:19.854912 139975777949504 pytorch_submission_base.py:86] 7) loss = 25.524, grad_norm = 28.624
I1004 19:55:20.765680 139935797470976 logging_writer.py:48] [8] global_step=8, grad_norm=29.388229, loss=24.408699
I1004 19:55:20.770019 139975777949504 pytorch_submission_base.py:86] 8) loss = 24.409, grad_norm = 29.388
I1004 19:55:21.676691 139935789078272 logging_writer.py:48] [9] global_step=9, grad_norm=28.298487, loss=22.754599
I1004 19:55:21.680916 139975777949504 pytorch_submission_base.py:86] 9) loss = 22.755, grad_norm = 28.298
I1004 19:55:22.594584 139935797470976 logging_writer.py:48] [10] global_step=10, grad_norm=28.426291, loss=20.948675
I1004 19:55:22.598797 139975777949504 pytorch_submission_base.py:86] 10) loss = 20.949, grad_norm = 28.426
I1004 19:55:23.504655 139935789078272 logging_writer.py:48] [11] global_step=11, grad_norm=28.205509, loss=19.324560
I1004 19:55:23.508697 139975777949504 pytorch_submission_base.py:86] 11) loss = 19.325, grad_norm = 28.206
I1004 19:55:24.419992 139935797470976 logging_writer.py:48] [12] global_step=12, grad_norm=25.259607, loss=17.430634
I1004 19:55:24.424010 139975777949504 pytorch_submission_base.py:86] 12) loss = 17.431, grad_norm = 25.260
I1004 19:55:25.334751 139935789078272 logging_writer.py:48] [13] global_step=13, grad_norm=21.417103, loss=15.600842
I1004 19:55:25.338480 139975777949504 pytorch_submission_base.py:86] 13) loss = 15.601, grad_norm = 21.417
I1004 19:55:26.247698 139935797470976 logging_writer.py:48] [14] global_step=14, grad_norm=17.116180, loss=14.177853
I1004 19:55:26.251628 139975777949504 pytorch_submission_base.py:86] 14) loss = 14.178, grad_norm = 17.116
I1004 19:55:27.156838 139935789078272 logging_writer.py:48] [15] global_step=15, grad_norm=13.588686, loss=12.430949
I1004 19:55:27.160492 139975777949504 pytorch_submission_base.py:86] 15) loss = 12.431, grad_norm = 13.589
I1004 19:55:28.066754 139935797470976 logging_writer.py:48] [16] global_step=16, grad_norm=11.287498, loss=11.905341
I1004 19:55:28.070469 139975777949504 pytorch_submission_base.py:86] 16) loss = 11.905, grad_norm = 11.287
I1004 19:55:28.986336 139935789078272 logging_writer.py:48] [17] global_step=17, grad_norm=9.855675, loss=10.862887
I1004 19:55:28.990383 139975777949504 pytorch_submission_base.py:86] 17) loss = 10.863, grad_norm = 9.856
I1004 19:55:29.906946 139935797470976 logging_writer.py:48] [18] global_step=18, grad_norm=7.551450, loss=9.822771
I1004 19:55:29.910946 139975777949504 pytorch_submission_base.py:86] 18) loss = 9.823, grad_norm = 7.551
I1004 19:55:30.819096 139935789078272 logging_writer.py:48] [19] global_step=19, grad_norm=5.849036, loss=9.394331
I1004 19:55:30.822936 139975777949504 pytorch_submission_base.py:86] 19) loss = 9.394, grad_norm = 5.849
I1004 19:55:31.737953 139935797470976 logging_writer.py:48] [20] global_step=20, grad_norm=4.672389, loss=8.718972
I1004 19:55:31.741819 139975777949504 pytorch_submission_base.py:86] 20) loss = 8.719, grad_norm = 4.672
I1004 19:55:32.656782 139935789078272 logging_writer.py:48] [21] global_step=21, grad_norm=3.692800, loss=8.289556
I1004 19:55:32.660553 139975777949504 pytorch_submission_base.py:86] 21) loss = 8.290, grad_norm = 3.693
I1004 19:55:33.577229 139935797470976 logging_writer.py:48] [22] global_step=22, grad_norm=3.530665, loss=8.038226
I1004 19:55:33.581445 139975777949504 pytorch_submission_base.py:86] 22) loss = 8.038, grad_norm = 3.531
I1004 19:55:34.493554 139935789078272 logging_writer.py:48] [23] global_step=23, grad_norm=3.373009, loss=7.819862
I1004 19:55:34.497477 139975777949504 pytorch_submission_base.py:86] 23) loss = 7.820, grad_norm = 3.373
I1004 19:55:35.413620 139935797470976 logging_writer.py:48] [24] global_step=24, grad_norm=2.876032, loss=7.668022
I1004 19:55:35.417572 139975777949504 pytorch_submission_base.py:86] 24) loss = 7.668, grad_norm = 2.876
I1004 19:55:36.328727 139935789078272 logging_writer.py:48] [25] global_step=25, grad_norm=2.878833, loss=7.433573
I1004 19:55:36.332719 139975777949504 pytorch_submission_base.py:86] 25) loss = 7.434, grad_norm = 2.879
I1004 19:55:37.245347 139935797470976 logging_writer.py:48] [26] global_step=26, grad_norm=2.405892, loss=7.263704
I1004 19:55:37.249125 139975777949504 pytorch_submission_base.py:86] 26) loss = 7.264, grad_norm = 2.406
I1004 19:55:38.165459 139935789078272 logging_writer.py:48] [27] global_step=27, grad_norm=10.593143, loss=7.361273
I1004 19:55:38.168984 139975777949504 pytorch_submission_base.py:86] 27) loss = 7.361, grad_norm = 10.593
I1004 19:55:39.081878 139935797470976 logging_writer.py:48] [28] global_step=28, grad_norm=5.221884, loss=7.287868
I1004 19:55:39.085597 139975777949504 pytorch_submission_base.py:86] 28) loss = 7.288, grad_norm = 5.222
I1004 19:55:39.993575 139935789078272 logging_writer.py:48] [29] global_step=29, grad_norm=4.032753, loss=7.142803
I1004 19:55:39.997283 139975777949504 pytorch_submission_base.py:86] 29) loss = 7.143, grad_norm = 4.033
I1004 19:55:40.911416 139935797470976 logging_writer.py:48] [30] global_step=30, grad_norm=5.801985, loss=7.063930
I1004 19:55:40.915039 139975777949504 pytorch_submission_base.py:86] 30) loss = 7.064, grad_norm = 5.802
I1004 19:55:41.822756 139935789078272 logging_writer.py:48] [31] global_step=31, grad_norm=4.698246, loss=6.990511
I1004 19:55:41.826484 139975777949504 pytorch_submission_base.py:86] 31) loss = 6.991, grad_norm = 4.698
I1004 19:55:42.740318 139935797470976 logging_writer.py:48] [32] global_step=32, grad_norm=2.798924, loss=6.812626
I1004 19:55:42.744202 139975777949504 pytorch_submission_base.py:86] 32) loss = 6.813, grad_norm = 2.799
I1004 19:55:43.656783 139935789078272 logging_writer.py:48] [33] global_step=33, grad_norm=2.954769, loss=6.710656
I1004 19:55:43.660621 139975777949504 pytorch_submission_base.py:86] 33) loss = 6.711, grad_norm = 2.955
I1004 19:55:44.570786 139935797470976 logging_writer.py:48] [34] global_step=34, grad_norm=2.729533, loss=6.604418
I1004 19:55:44.575076 139975777949504 pytorch_submission_base.py:86] 34) loss = 6.604, grad_norm = 2.730
I1004 19:55:45.487995 139935789078272 logging_writer.py:48] [35] global_step=35, grad_norm=1.649875, loss=6.469289
I1004 19:55:45.491839 139975777949504 pytorch_submission_base.py:86] 35) loss = 6.469, grad_norm = 1.650
I1004 19:55:46.401208 139935797470976 logging_writer.py:48] [36] global_step=36, grad_norm=1.220422, loss=6.394082
I1004 19:55:46.405618 139975777949504 pytorch_submission_base.py:86] 36) loss = 6.394, grad_norm = 1.220
I1004 19:55:47.310476 139935789078272 logging_writer.py:48] [37] global_step=37, grad_norm=0.951608, loss=6.296043
I1004 19:55:47.314447 139975777949504 pytorch_submission_base.py:86] 37) loss = 6.296, grad_norm = 0.952
I1004 19:55:48.227235 139935797470976 logging_writer.py:48] [38] global_step=38, grad_norm=1.374567, loss=6.268231
I1004 19:55:48.231150 139975777949504 pytorch_submission_base.py:86] 38) loss = 6.268, grad_norm = 1.375
I1004 19:55:49.139221 139935789078272 logging_writer.py:48] [39] global_step=39, grad_norm=1.384713, loss=6.200799
I1004 19:55:49.143680 139975777949504 pytorch_submission_base.py:86] 39) loss = 6.201, grad_norm = 1.385
I1004 19:55:50.055360 139935797470976 logging_writer.py:48] [40] global_step=40, grad_norm=2.561245, loss=6.150761
I1004 19:55:50.059826 139975777949504 pytorch_submission_base.py:86] 40) loss = 6.151, grad_norm = 2.561
I1004 19:55:50.969782 139935789078272 logging_writer.py:48] [41] global_step=41, grad_norm=3.289411, loss=6.109522
I1004 19:55:50.973607 139975777949504 pytorch_submission_base.py:86] 41) loss = 6.110, grad_norm = 3.289
I1004 19:55:51.892004 139935797470976 logging_writer.py:48] [42] global_step=42, grad_norm=2.628790, loss=6.064991
I1004 19:55:51.896110 139975777949504 pytorch_submission_base.py:86] 42) loss = 6.065, grad_norm = 2.629
I1004 19:55:52.803228 139935789078272 logging_writer.py:48] [43] global_step=43, grad_norm=1.627983, loss=6.034758
I1004 19:55:52.807492 139975777949504 pytorch_submission_base.py:86] 43) loss = 6.035, grad_norm = 1.628
I1004 19:55:53.717826 139935797470976 logging_writer.py:48] [44] global_step=44, grad_norm=1.605730, loss=6.000773
I1004 19:55:53.721659 139975777949504 pytorch_submission_base.py:86] 44) loss = 6.001, grad_norm = 1.606
I1004 19:55:54.631377 139935789078272 logging_writer.py:48] [45] global_step=45, grad_norm=1.801248, loss=5.979846
I1004 19:55:54.635696 139975777949504 pytorch_submission_base.py:86] 45) loss = 5.980, grad_norm = 1.801
I1004 19:55:55.548953 139935797470976 logging_writer.py:48] [46] global_step=46, grad_norm=3.327576, loss=5.976011
I1004 19:55:55.552905 139975777949504 pytorch_submission_base.py:86] 46) loss = 5.976, grad_norm = 3.328
I1004 19:55:56.463617 139935789078272 logging_writer.py:48] [47] global_step=47, grad_norm=4.408567, loss=5.987448
I1004 19:55:56.467666 139975777949504 pytorch_submission_base.py:86] 47) loss = 5.987, grad_norm = 4.409
I1004 19:55:57.375277 139935797470976 logging_writer.py:48] [48] global_step=48, grad_norm=4.551896, loss=5.994850
I1004 19:55:57.379436 139975777949504 pytorch_submission_base.py:86] 48) loss = 5.995, grad_norm = 4.552
I1004 19:55:58.292572 139935789078272 logging_writer.py:48] [49] global_step=49, grad_norm=2.979288, loss=5.959215
I1004 19:55:58.296587 139975777949504 pytorch_submission_base.py:86] 49) loss = 5.959, grad_norm = 2.979
I1004 19:55:59.200793 139935797470976 logging_writer.py:48] [50] global_step=50, grad_norm=1.831992, loss=5.939183
I1004 19:55:59.204715 139975777949504 pytorch_submission_base.py:86] 50) loss = 5.939, grad_norm = 1.832
I1004 19:56:00.120074 139935789078272 logging_writer.py:48] [51] global_step=51, grad_norm=2.545951, loss=5.913795
I1004 19:56:00.123944 139975777949504 pytorch_submission_base.py:86] 51) loss = 5.914, grad_norm = 2.546
I1004 19:56:01.030318 139935797470976 logging_writer.py:48] [52] global_step=52, grad_norm=3.003923, loss=5.926944
I1004 19:56:01.034094 139975777949504 pytorch_submission_base.py:86] 52) loss = 5.927, grad_norm = 3.004
I1004 19:56:01.947158 139935789078272 logging_writer.py:48] [53] global_step=53, grad_norm=3.383439, loss=5.926680
I1004 19:56:01.951047 139975777949504 pytorch_submission_base.py:86] 53) loss = 5.927, grad_norm = 3.383
I1004 19:56:02.859134 139935797470976 logging_writer.py:48] [54] global_step=54, grad_norm=4.761444, loss=5.948936
I1004 19:56:02.863198 139975777949504 pytorch_submission_base.py:86] 54) loss = 5.949, grad_norm = 4.761
I1004 19:56:03.769659 139935789078272 logging_writer.py:48] [55] global_step=55, grad_norm=4.314184, loss=5.942559
I1004 19:56:03.773661 139975777949504 pytorch_submission_base.py:86] 55) loss = 5.943, grad_norm = 4.314
I1004 19:56:04.684727 139935797470976 logging_writer.py:48] [56] global_step=56, grad_norm=1.594348, loss=5.872338
I1004 19:56:04.688627 139975777949504 pytorch_submission_base.py:86] 56) loss = 5.872, grad_norm = 1.594
I1004 19:56:05.596203 139935789078272 logging_writer.py:48] [57] global_step=57, grad_norm=1.018320, loss=5.876419
I1004 19:56:05.600211 139975777949504 pytorch_submission_base.py:86] 57) loss = 5.876, grad_norm = 1.018
I1004 19:56:06.509017 139935797470976 logging_writer.py:48] [58] global_step=58, grad_norm=1.693041, loss=5.878273
I1004 19:56:06.512837 139975777949504 pytorch_submission_base.py:86] 58) loss = 5.878, grad_norm = 1.693
I1004 19:56:07.422093 139935789078272 logging_writer.py:48] [59] global_step=59, grad_norm=3.169496, loss=5.890750
I1004 19:56:07.425893 139975777949504 pytorch_submission_base.py:86] 59) loss = 5.891, grad_norm = 3.169
I1004 19:56:08.334519 139935797470976 logging_writer.py:48] [60] global_step=60, grad_norm=6.832601, loss=5.956600
I1004 19:56:08.338567 139975777949504 pytorch_submission_base.py:86] 60) loss = 5.957, grad_norm = 6.833
I1004 19:56:09.251045 139935789078272 logging_writer.py:48] [61] global_step=61, grad_norm=5.481821, loss=5.924898
I1004 19:56:09.255118 139975777949504 pytorch_submission_base.py:86] 61) loss = 5.925, grad_norm = 5.482
I1004 19:56:10.165499 139935797470976 logging_writer.py:48] [62] global_step=62, grad_norm=1.607924, loss=5.883834
I1004 19:56:10.169421 139975777949504 pytorch_submission_base.py:86] 62) loss = 5.884, grad_norm = 1.608
I1004 19:56:11.078277 139935789078272 logging_writer.py:48] [63] global_step=63, grad_norm=1.334937, loss=5.868839
I1004 19:56:11.082769 139975777949504 pytorch_submission_base.py:86] 63) loss = 5.869, grad_norm = 1.335
I1004 19:56:11.989320 139935797470976 logging_writer.py:48] [64] global_step=64, grad_norm=1.276290, loss=5.854186
I1004 19:56:11.993035 139975777949504 pytorch_submission_base.py:86] 64) loss = 5.854, grad_norm = 1.276
I1004 19:56:12.905044 139935789078272 logging_writer.py:48] [65] global_step=65, grad_norm=2.106806, loss=5.886629
I1004 19:56:12.908916 139975777949504 pytorch_submission_base.py:86] 65) loss = 5.887, grad_norm = 2.107
I1004 19:56:13.816789 139935797470976 logging_writer.py:48] [66] global_step=66, grad_norm=4.567565, loss=5.876684
I1004 19:56:13.820891 139975777949504 pytorch_submission_base.py:86] 66) loss = 5.877, grad_norm = 4.568
I1004 19:56:14.737386 139935789078272 logging_writer.py:48] [67] global_step=67, grad_norm=6.100200, loss=5.910988
I1004 19:56:14.741317 139975777949504 pytorch_submission_base.py:86] 67) loss = 5.911, grad_norm = 6.100
I1004 19:56:15.649849 139935797470976 logging_writer.py:48] [68] global_step=68, grad_norm=2.859626, loss=5.860209
I1004 19:56:15.653941 139975777949504 pytorch_submission_base.py:86] 68) loss = 5.860, grad_norm = 2.860
I1004 19:56:16.567677 139935789078272 logging_writer.py:48] [69] global_step=69, grad_norm=0.283685, loss=5.856047
I1004 19:56:16.571799 139975777949504 pytorch_submission_base.py:86] 69) loss = 5.856, grad_norm = 0.284
I1004 19:56:17.479400 139935797470976 logging_writer.py:48] [70] global_step=70, grad_norm=0.468886, loss=5.823744
I1004 19:56:17.483239 139975777949504 pytorch_submission_base.py:86] 70) loss = 5.824, grad_norm = 0.469
I1004 19:56:18.389454 139935789078272 logging_writer.py:48] [71] global_step=71, grad_norm=1.030608, loss=5.825840
I1004 19:56:18.393294 139975777949504 pytorch_submission_base.py:86] 71) loss = 5.826, grad_norm = 1.031
I1004 19:56:19.302161 139935797470976 logging_writer.py:48] [72] global_step=72, grad_norm=2.668388, loss=5.817848
I1004 19:56:19.306164 139975777949504 pytorch_submission_base.py:86] 72) loss = 5.818, grad_norm = 2.668
I1004 19:56:20.212811 139935789078272 logging_writer.py:48] [73] global_step=73, grad_norm=9.120790, loss=5.952464
I1004 19:56:20.216747 139975777949504 pytorch_submission_base.py:86] 73) loss = 5.952, grad_norm = 9.121
I1004 19:56:21.127212 139935797470976 logging_writer.py:48] [74] global_step=74, grad_norm=5.286695, loss=5.893440
I1004 19:56:21.131641 139975777949504 pytorch_submission_base.py:86] 74) loss = 5.893, grad_norm = 5.287
I1004 19:56:22.046630 139935789078272 logging_writer.py:48] [75] global_step=75, grad_norm=0.998530, loss=5.840544
I1004 19:56:22.050998 139975777949504 pytorch_submission_base.py:86] 75) loss = 5.841, grad_norm = 0.999
I1004 19:56:22.967210 139935797470976 logging_writer.py:48] [76] global_step=76, grad_norm=0.608679, loss=5.833122
I1004 19:56:22.971140 139975777949504 pytorch_submission_base.py:86] 76) loss = 5.833, grad_norm = 0.609
I1004 19:56:23.881630 139935789078272 logging_writer.py:48] [77] global_step=77, grad_norm=0.373949, loss=5.828432
I1004 19:56:23.885477 139975777949504 pytorch_submission_base.py:86] 77) loss = 5.828, grad_norm = 0.374
I1004 19:56:24.800318 139935797470976 logging_writer.py:48] [78] global_step=78, grad_norm=0.510854, loss=5.848670
I1004 19:56:24.804500 139975777949504 pytorch_submission_base.py:86] 78) loss = 5.849, grad_norm = 0.511
I1004 19:56:25.714920 139935789078272 logging_writer.py:48] [79] global_step=79, grad_norm=1.533693, loss=5.830442
I1004 19:56:25.719148 139975777949504 pytorch_submission_base.py:86] 79) loss = 5.830, grad_norm = 1.534
I1004 19:56:26.625665 139935797470976 logging_writer.py:48] [80] global_step=80, grad_norm=4.346534, loss=5.867448
I1004 19:56:26.629385 139975777949504 pytorch_submission_base.py:86] 80) loss = 5.867, grad_norm = 4.347
I1004 19:56:27.537910 139935789078272 logging_writer.py:48] [81] global_step=81, grad_norm=7.302960, loss=5.911564
I1004 19:56:27.541898 139975777949504 pytorch_submission_base.py:86] 81) loss = 5.912, grad_norm = 7.303
I1004 19:56:28.451481 139935797470976 logging_writer.py:48] [82] global_step=82, grad_norm=4.446044, loss=5.852344
I1004 19:56:28.455308 139975777949504 pytorch_submission_base.py:86] 82) loss = 5.852, grad_norm = 4.446
I1004 19:56:29.360131 139935789078272 logging_writer.py:48] [83] global_step=83, grad_norm=0.726053, loss=5.825962
I1004 19:56:29.363940 139975777949504 pytorch_submission_base.py:86] 83) loss = 5.826, grad_norm = 0.726
I1004 19:56:30.274525 139935797470976 logging_writer.py:48] [84] global_step=84, grad_norm=1.059317, loss=5.821980
I1004 19:56:30.278090 139975777949504 pytorch_submission_base.py:86] 84) loss = 5.822, grad_norm = 1.059
I1004 19:56:31.195899 139935789078272 logging_writer.py:48] [85] global_step=85, grad_norm=1.904338, loss=5.801922
I1004 19:56:31.199917 139975777949504 pytorch_submission_base.py:86] 85) loss = 5.802, grad_norm = 1.904
I1004 19:56:32.109451 139935797470976 logging_writer.py:48] [86] global_step=86, grad_norm=3.557897, loss=5.819109
I1004 19:56:32.113446 139975777949504 pytorch_submission_base.py:86] 86) loss = 5.819, grad_norm = 3.558
I1004 19:56:33.033448 139935789078272 logging_writer.py:48] [87] global_step=87, grad_norm=5.605757, loss=5.838161
I1004 19:56:33.037224 139975777949504 pytorch_submission_base.py:86] 87) loss = 5.838, grad_norm = 5.606
I1004 19:56:33.945199 139935797470976 logging_writer.py:48] [88] global_step=88, grad_norm=4.441212, loss=5.850399
I1004 19:56:33.948987 139975777949504 pytorch_submission_base.py:86] 88) loss = 5.850, grad_norm = 4.441
I1004 19:56:34.861119 139935789078272 logging_writer.py:48] [89] global_step=89, grad_norm=2.132777, loss=5.835644
I1004 19:56:34.864952 139975777949504 pytorch_submission_base.py:86] 89) loss = 5.836, grad_norm = 2.133
I1004 19:56:35.777832 139935797470976 logging_writer.py:48] [90] global_step=90, grad_norm=2.143992, loss=5.792960
I1004 19:56:35.781676 139975777949504 pytorch_submission_base.py:86] 90) loss = 5.793, grad_norm = 2.144
I1004 19:56:36.687757 139935789078272 logging_writer.py:48] [91] global_step=91, grad_norm=3.624000, loss=5.812552
I1004 19:56:36.691918 139975777949504 pytorch_submission_base.py:86] 91) loss = 5.813, grad_norm = 3.624
I1004 19:56:37.609359 139935797470976 logging_writer.py:48] [92] global_step=92, grad_norm=4.962332, loss=5.841099
I1004 19:56:37.613200 139975777949504 pytorch_submission_base.py:86] 92) loss = 5.841, grad_norm = 4.962
I1004 19:56:38.520030 139935789078272 logging_writer.py:48] [93] global_step=93, grad_norm=3.983548, loss=5.824206
I1004 19:56:38.524238 139975777949504 pytorch_submission_base.py:86] 93) loss = 5.824, grad_norm = 3.984
I1004 19:56:39.434327 139935797470976 logging_writer.py:48] [94] global_step=94, grad_norm=2.975624, loss=5.819521
I1004 19:56:39.438396 139975777949504 pytorch_submission_base.py:86] 94) loss = 5.820, grad_norm = 2.976
I1004 19:56:40.350790 139935789078272 logging_writer.py:48] [95] global_step=95, grad_norm=3.410448, loss=5.806093
I1004 19:56:40.355072 139975777949504 pytorch_submission_base.py:86] 95) loss = 5.806, grad_norm = 3.410
I1004 19:56:41.271700 139935797470976 logging_writer.py:48] [96] global_step=96, grad_norm=3.407870, loss=5.824311
I1004 19:56:41.276407 139975777949504 pytorch_submission_base.py:86] 96) loss = 5.824, grad_norm = 3.408
I1004 19:56:42.181843 139935789078272 logging_writer.py:48] [97] global_step=97, grad_norm=2.834815, loss=5.791327
I1004 19:56:42.186059 139975777949504 pytorch_submission_base.py:86] 97) loss = 5.791, grad_norm = 2.835
I1004 19:56:43.096335 139935797470976 logging_writer.py:48] [98] global_step=98, grad_norm=2.634204, loss=5.795981
I1004 19:56:43.100672 139975777949504 pytorch_submission_base.py:86] 98) loss = 5.796, grad_norm = 2.634
I1004 19:56:44.011892 139935789078272 logging_writer.py:48] [99] global_step=99, grad_norm=3.916760, loss=5.810982
I1004 19:56:44.015773 139975777949504 pytorch_submission_base.py:86] 99) loss = 5.811, grad_norm = 3.917
I1004 19:56:44.924561 139935797470976 logging_writer.py:48] [100] global_step=100, grad_norm=5.097204, loss=5.837872
I1004 19:56:44.928577 139975777949504 pytorch_submission_base.py:86] 100) loss = 5.838, grad_norm = 5.097
I1004 20:02:44.755986 139935789078272 logging_writer.py:48] [500] global_step=500, grad_norm=3.378070, loss=2.942890
I1004 20:02:44.760195 139975777949504 pytorch_submission_base.py:86] 500) loss = 2.943, grad_norm = 3.378
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I1004 20:10:12.684546 139935797470976 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.855735, loss=2.429855
I1004 20:10:12.689454 139975777949504 pytorch_submission_base.py:86] 1000) loss = 2.430, grad_norm = 2.856
I1004 20:17:43.381323 139947113707264 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.843024, loss=2.217918
I1004 20:17:43.390506 139975777949504 pytorch_submission_base.py:86] 1500) loss = 2.218, grad_norm = 2.843
I1004 20:19:13.748198 139975777949504 spec.py:321] Evaluating on the training split.
I1004 20:19:27.250196 139975777949504 spec.py:333] Evaluating on the validation split.
I1004 20:19:38.142111 139975777949504 spec.py:349] Evaluating on the test split.
I1004 20:19:44.153652 139975777949504 submission_runner.py:381] Time since start: 1551.27s, 	Step: 1601, 	{'train/ctc_loss': 1.0921613971964206, 'train/wer': 0.32182746082731406, 'validation/ctc_loss': 1.3457177430241611, 'validation/wer': 0.35752426012649063, 'validation/num_examples': 5348, 'test/ctc_loss': 0.9301977143686689, 'test/wer': 0.2827371884711474, 'test/num_examples': 2472, 'score': 1475.9301266670227, 'total_duration': 1551.2678656578064, 'accumulated_submission_time': 1475.9301266670227, 'accumulated_eval_time': 73.29950070381165, 'accumulated_logging_time': 0.03090071678161621}
I1004 20:19:44.190058 139947105314560 logging_writer.py:48] [1601] accumulated_eval_time=73.299501, accumulated_logging_time=0.030901, accumulated_submission_time=1475.930127, global_step=1601, preemption_count=0, score=1475.930127, test/ctc_loss=0.930198, test/num_examples=2472, test/wer=0.282737, total_duration=1551.267866, train/ctc_loss=1.092161, train/wer=0.321827, validation/ctc_loss=1.345718, validation/num_examples=5348, validation/wer=0.357524
I1004 20:25:44.153355 139947096921856 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.501546, loss=2.143257
I1004 20:25:44.158377 139975777949504 pytorch_submission_base.py:86] 2000) loss = 2.143, grad_norm = 3.502
I1004 20:33:15.486257 139947105314560 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.536706, loss=2.143644
I1004 20:33:15.495179 139975777949504 pytorch_submission_base.py:86] 2500) loss = 2.144, grad_norm = 3.537
I1004 20:40:44.134706 139947096921856 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.784633, loss=2.084068
I1004 20:40:44.142163 139975777949504 pytorch_submission_base.py:86] 3000) loss = 2.084, grad_norm = 2.785
I1004 20:43:45.023773 139975777949504 spec.py:321] Evaluating on the training split.
I1004 20:43:58.195131 139975777949504 spec.py:333] Evaluating on the validation split.
I1004 20:44:09.469339 139975777949504 spec.py:349] Evaluating on the test split.
I1004 20:44:15.429707 139975777949504 submission_runner.py:381] Time since start: 3022.54s, 	Step: 3200, 	{'train/ctc_loss': 0.7693514277329795, 'train/wer': 0.23767752034044773, 'validation/ctc_loss': 1.0031631834563994, 'validation/wer': 0.2792835417370733, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6636896936899155, 'test/wer': 0.20729998171957834, 'test/num_examples': 2472, 'score': 2914.8871920108795, 'total_duration': 3022.5438780784607, 'accumulated_submission_time': 2914.8871920108795, 'accumulated_eval_time': 103.70521712303162, 'accumulated_logging_time': 0.07859945297241211}
I1004 20:44:15.462154 139947088529152 logging_writer.py:48] [3200] accumulated_eval_time=103.705217, accumulated_logging_time=0.078599, accumulated_submission_time=2914.887192, global_step=3200, preemption_count=0, score=2914.887192, test/ctc_loss=0.663690, test/num_examples=2472, test/wer=0.207300, total_duration=3022.543878, train/ctc_loss=0.769351, train/wer=0.237678, validation/ctc_loss=1.003163, validation/num_examples=5348, validation/wer=0.279284
I1004 20:48:47.006375 139947080136448 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.505296, loss=2.028809
I1004 20:48:47.010696 139975777949504 pytorch_submission_base.py:86] 3500) loss = 2.029, grad_norm = 2.505
I1004 20:56:16.533631 139947088529152 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.571457, loss=1.976871
I1004 20:56:16.542565 139975777949504 pytorch_submission_base.py:86] 4000) loss = 1.977, grad_norm = 2.571
I1004 21:03:46.541463 139947088529152 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.012712, loss=1.968385
I1004 21:03:46.550354 139975777949504 pytorch_submission_base.py:86] 4500) loss = 1.968, grad_norm = 3.013
I1004 21:08:16.109569 139975777949504 spec.py:321] Evaluating on the training split.
I1004 21:08:29.457366 139975777949504 spec.py:333] Evaluating on the validation split.
I1004 21:08:40.552147 139975777949504 spec.py:349] Evaluating on the test split.
I1004 21:08:46.339679 139975777949504 submission_runner.py:381] Time since start: 4493.45s, 	Step: 4801, 	{'train/ctc_loss': 0.7131809057194379, 'train/wer': 0.2193398661905616, 'validation/ctc_loss': 0.9386733662346796, 'validation/wer': 0.2621252353594361, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6216358212626621, 'test/wer': 0.18995389271423638, 'test/num_examples': 2472, 'score': 4353.6596512794495, 'total_duration': 4493.453898191452, 'accumulated_submission_time': 4353.6596512794495, 'accumulated_eval_time': 133.93509721755981, 'accumulated_logging_time': 0.12140130996704102}
I1004 21:08:46.379571 139947080136448 logging_writer.py:48] [4801] accumulated_eval_time=133.935097, accumulated_logging_time=0.121401, accumulated_submission_time=4353.659651, global_step=4801, preemption_count=0, score=4353.659651, test/ctc_loss=0.621636, test/num_examples=2472, test/wer=0.189954, total_duration=4493.453898, train/ctc_loss=0.713181, train/wer=0.219340, validation/ctc_loss=0.938673, validation/num_examples=5348, validation/wer=0.262125
I1004 21:11:46.927430 139947071743744 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.822979, loss=1.973808
I1004 21:11:46.934828 139975777949504 pytorch_submission_base.py:86] 5000) loss = 1.974, grad_norm = 2.823
I1004 21:19:18.059781 139947080136448 logging_writer.py:48] [5500] global_step=5500, grad_norm=4.266629, loss=2.004595
I1004 21:19:18.067023 139975777949504 pytorch_submission_base.py:86] 5500) loss = 2.005, grad_norm = 4.267
I1004 21:26:48.273455 139947071743744 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.380287, loss=1.893790
I1004 21:26:48.278717 139975777949504 pytorch_submission_base.py:86] 6000) loss = 1.894, grad_norm = 4.380
I1004 21:32:47.113158 139975777949504 spec.py:321] Evaluating on the training split.
I1004 21:33:00.396533 139975777949504 spec.py:333] Evaluating on the validation split.
I1004 21:33:11.412398 139975777949504 spec.py:349] Evaluating on the test split.
I1004 21:33:17.234712 139975777949504 submission_runner.py:381] Time since start: 5964.35s, 	Step: 6398, 	{'train/ctc_loss': 0.6628117982545677, 'train/wer': 0.2072252748745876, 'validation/ctc_loss': 0.8951692433632208, 'validation/wer': 0.2520446096654275, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5748758488450868, 'test/wer': 0.18018402291146182, 'test/num_examples': 2472, 'score': 5792.530745983124, 'total_duration': 5964.348899841309, 'accumulated_submission_time': 5792.530745983124, 'accumulated_eval_time': 164.05646991729736, 'accumulated_logging_time': 0.17159247398376465}
I1004 21:33:17.269770 139947063351040 logging_writer.py:48] [6398] accumulated_eval_time=164.056470, accumulated_logging_time=0.171592, accumulated_submission_time=5792.530746, global_step=6398, preemption_count=0, score=5792.530746, test/ctc_loss=0.574876, test/num_examples=2472, test/wer=0.180184, total_duration=5964.348900, train/ctc_loss=0.662812, train/wer=0.207225, validation/ctc_loss=0.895169, validation/num_examples=5348, validation/wer=0.252045
I1004 21:34:50.486464 139947054958336 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.702962, loss=1.860057
I1004 21:34:50.491090 139975777949504 pytorch_submission_base.py:86] 6500) loss = 1.860, grad_norm = 1.703
I1004 21:42:19.416327 139947063351040 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.599981, loss=1.877457
I1004 21:42:19.424951 139975777949504 pytorch_submission_base.py:86] 7000) loss = 1.877, grad_norm = 3.600
I1004 21:49:49.648939 139947063351040 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.655310, loss=1.899001
I1004 21:49:49.656120 139975777949504 pytorch_submission_base.py:86] 7500) loss = 1.899, grad_norm = 2.655
I1004 21:57:17.885734 139975777949504 spec.py:321] Evaluating on the training split.
I1004 21:57:31.087921 139975777949504 spec.py:333] Evaluating on the validation split.
I1004 21:57:42.278264 139975777949504 spec.py:349] Evaluating on the test split.
I1004 21:57:48.316463 139975777949504 submission_runner.py:381] Time since start: 7435.43s, 	Step: 8000, 	{'train/ctc_loss': 0.6248732073593958, 'train/wer': 0.1954367829210894, 'validation/ctc_loss': 0.8528151371308017, 'validation/wer': 0.2415294742432289, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5524409840957482, 'test/wer': 0.1720797026384742, 'test/num_examples': 2472, 'score': 7231.2451684474945, 'total_duration': 7435.430392503738, 'accumulated_submission_time': 7231.2451684474945, 'accumulated_eval_time': 194.48667073249817, 'accumulated_logging_time': 0.21728301048278809}
I1004 21:57:48.361044 139947063351040 logging_writer.py:48] [8000] accumulated_eval_time=194.486671, accumulated_logging_time=0.217283, accumulated_submission_time=7231.245168, global_step=8000, preemption_count=0, score=7231.245168, test/ctc_loss=0.552441, test/num_examples=2472, test/wer=0.172080, total_duration=7435.430393, train/ctc_loss=0.624873, train/wer=0.195437, validation/ctc_loss=0.852815, validation/num_examples=5348, validation/wer=0.241529
I1004 21:57:49.947012 139947054958336 logging_writer.py:48] [8000] global_step=8000, grad_norm=5.579677, loss=1.822483
I1004 21:57:49.951250 139975777949504 pytorch_submission_base.py:86] 8000) loss = 1.822, grad_norm = 5.580
I1004 22:05:20.714897 139947063351040 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.415323, loss=2.183520
I1004 22:05:20.722913 139975777949504 pytorch_submission_base.py:86] 8500) loss = 2.184, grad_norm = 4.415
I1004 22:12:52.424996 139947054958336 logging_writer.py:48] [9000] global_step=9000, grad_norm=7.559509, loss=1.850253
I1004 22:12:52.433104 139975777949504 pytorch_submission_base.py:86] 9000) loss = 1.850, grad_norm = 7.560
I1004 22:20:23.143643 139947063351040 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.094437, loss=1.845175
I1004 22:20:23.150562 139975777949504 pytorch_submission_base.py:86] 9500) loss = 1.845, grad_norm = 2.094
I1004 22:21:49.153315 139975777949504 spec.py:321] Evaluating on the training split.
I1004 22:22:02.364793 139975777949504 spec.py:333] Evaluating on the validation split.
I1004 22:22:14.111477 139975777949504 spec.py:349] Evaluating on the test split.
I1004 22:22:19.934299 139975777949504 submission_runner.py:381] Time since start: 8907.05s, 	Step: 9596, 	{'train/ctc_loss': 0.5936486190500493, 'train/wer': 0.1869364595392216, 'validation/ctc_loss': 0.8201114776408981, 'validation/wer': 0.23116883116883116, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5192390436479772, 'test/wer': 0.1615379928097008, 'test/num_examples': 2472, 'score': 8670.195932865143, 'total_duration': 8907.048548698425, 'accumulated_submission_time': 8670.195932865143, 'accumulated_eval_time': 225.2674674987793, 'accumulated_logging_time': 0.27629590034484863}
I1004 22:22:19.961941 139947063351040 logging_writer.py:48] [9596] accumulated_eval_time=225.267467, accumulated_logging_time=0.276296, accumulated_submission_time=8670.195933, global_step=9596, preemption_count=0, score=8670.195933, test/ctc_loss=0.519239, test/num_examples=2472, test/wer=0.161538, total_duration=8907.048549, train/ctc_loss=0.593649, train/wer=0.186936, validation/ctc_loss=0.820111, validation/num_examples=5348, validation/wer=0.231169
I1004 22:28:24.901133 139947054958336 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.704880, loss=1.818040
I1004 22:28:24.906005 139975777949504 pytorch_submission_base.py:86] 10000) loss = 1.818, grad_norm = 2.705
I1004 22:35:56.002367 139947063351040 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.321004, loss=1.871298
I1004 22:35:56.013812 139975777949504 pytorch_submission_base.py:86] 10500) loss = 1.871, grad_norm = 2.321
I1004 22:43:25.940266 139947054958336 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.914177, loss=1.794748
I1004 22:43:25.945630 139975777949504 pytorch_submission_base.py:86] 11000) loss = 1.795, grad_norm = 2.914
I1004 22:46:20.709151 139975777949504 spec.py:321] Evaluating on the training split.
I1004 22:46:33.948676 139975777949504 spec.py:333] Evaluating on the validation split.
I1004 22:46:45.043515 139975777949504 spec.py:349] Evaluating on the test split.
I1004 22:46:50.922006 139975777949504 submission_runner.py:381] Time since start: 10378.04s, 	Step: 11195, 	{'train/ctc_loss': 0.5737404732741316, 'train/wer': 0.18285478251890017, 'validation/ctc_loss': 0.7909071180555556, 'validation/wer': 0.22535605658282237, 'validation/num_examples': 5348, 'test/ctc_loss': 0.49879563802503385, 'test/wer': 0.15633822842402453, 'test/num_examples': 2472, 'score': 10109.002165794373, 'total_duration': 10378.035343170166, 'accumulated_submission_time': 10109.002165794373, 'accumulated_eval_time': 255.4791874885559, 'accumulated_logging_time': 0.31482791900634766}
I1004 22:46:50.953681 139947063351040 logging_writer.py:48] [11195] accumulated_eval_time=255.479187, accumulated_logging_time=0.314828, accumulated_submission_time=10109.002166, global_step=11195, preemption_count=0, score=10109.002166, test/ctc_loss=0.498796, test/num_examples=2472, test/wer=0.156338, total_duration=10378.035343, train/ctc_loss=0.573740, train/wer=0.182855, validation/ctc_loss=0.790907, validation/num_examples=5348, validation/wer=0.225356
I1004 22:51:28.448707 139947063351040 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.535505, loss=1.744440
I1004 22:51:28.456897 139975777949504 pytorch_submission_base.py:86] 11500) loss = 1.744, grad_norm = 2.536
I1004 22:58:57.319046 139947054958336 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.170283, loss=1.825095
I1004 22:58:57.324323 139975777949504 pytorch_submission_base.py:86] 12000) loss = 1.825, grad_norm = 3.170
I1004 23:06:28.141199 139947063351040 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.233597, loss=1.718438
I1004 23:06:28.148070 139975777949504 pytorch_submission_base.py:86] 12500) loss = 1.718, grad_norm = 3.234
I1004 23:10:52.328651 139975777949504 spec.py:321] Evaluating on the training split.
I1004 23:11:05.445179 139975777949504 spec.py:333] Evaluating on the validation split.
I1004 23:11:17.015215 139975777949504 spec.py:349] Evaluating on the test split.
I1004 23:11:22.836648 139975777949504 submission_runner.py:381] Time since start: 11849.95s, 	Step: 12794, 	{'train/ctc_loss': 0.5504678834507162, 'train/wer': 0.1744740289032735, 'validation/ctc_loss': 0.7800603969666717, 'validation/wer': 0.22056679380099453, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4834641045390025, 'test/wer': 0.15211341986066257, 'test/num_examples': 2472, 'score': 11548.54773068428, 'total_duration': 11849.95093035698, 'accumulated_submission_time': 11548.54773068428, 'accumulated_eval_time': 285.9870653152466, 'accumulated_logging_time': 0.35784387588500977}
I1004 23:11:22.864065 139947054958336 logging_writer.py:48] [12794] accumulated_eval_time=285.987065, accumulated_logging_time=0.357844, accumulated_submission_time=11548.547731, global_step=12794, preemption_count=0, score=11548.547731, test/ctc_loss=0.483464, test/num_examples=2472, test/wer=0.152113, total_duration=11849.950930, train/ctc_loss=0.550468, train/wer=0.174474, validation/ctc_loss=0.780060, validation/num_examples=5348, validation/wer=0.220567
I1004 23:14:30.135142 139947046565632 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.853559, loss=1.766948
I1004 23:14:30.139990 139975777949504 pytorch_submission_base.py:86] 13000) loss = 1.767, grad_norm = 1.854
I1004 23:22:00.370992 139947054958336 logging_writer.py:48] [13500] global_step=13500, grad_norm=6.601371, loss=1.691381
I1004 23:22:00.378939 139975777949504 pytorch_submission_base.py:86] 13500) loss = 1.691, grad_norm = 6.601
I1004 23:29:30.147628 139947046565632 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.059518, loss=1.657205
I1004 23:29:30.153864 139975777949504 pytorch_submission_base.py:86] 14000) loss = 1.657, grad_norm = 3.060
I1004 23:35:24.332753 139975777949504 spec.py:321] Evaluating on the training split.
I1004 23:35:37.677687 139975777949504 spec.py:333] Evaluating on the validation split.
I1004 23:35:48.896919 139975777949504 spec.py:349] Evaluating on the test split.
I1004 23:35:54.735447 139975777949504 submission_runner.py:381] Time since start: 13321.85s, 	Step: 14396, 	{'train/ctc_loss': 0.5290127079718281, 'train/wer': 0.1685661954530879, 'validation/ctc_loss': 0.7479713321905767, 'validation/wer': 0.21184763192198136, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4635446238466998, 'test/wer': 0.14709645969167023, 'test/num_examples': 2472, 'score': 12988.076228141785, 'total_duration': 13321.848577022552, 'accumulated_submission_time': 12988.076228141785, 'accumulated_eval_time': 316.3883891105652, 'accumulated_logging_time': 0.39840173721313477}
I1004 23:35:54.770764 139947054958336 logging_writer.py:48] [14396] accumulated_eval_time=316.388389, accumulated_logging_time=0.398402, accumulated_submission_time=12988.076228, global_step=14396, preemption_count=0, score=12988.076228, test/ctc_loss=0.463545, test/num_examples=2472, test/wer=0.147096, total_duration=13321.848577, train/ctc_loss=0.529013, train/wer=0.168566, validation/ctc_loss=0.747971, validation/num_examples=5348, validation/wer=0.211848
I1004 23:37:31.412212 139947054958336 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.740907, loss=1.756038
I1004 23:37:31.419538 139975777949504 pytorch_submission_base.py:86] 14500) loss = 1.756, grad_norm = 3.741
I1004 23:44:59.810250 139947046565632 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.189880, loss=1.713857
I1004 23:44:59.815490 139975777949504 pytorch_submission_base.py:86] 15000) loss = 1.714, grad_norm = 2.190
I1004 23:52:31.735451 139947054958336 logging_writer.py:48] [15500] global_step=15500, grad_norm=4.022820, loss=1.710285
I1004 23:52:31.743830 139975777949504 pytorch_submission_base.py:86] 15500) loss = 1.710, grad_norm = 4.023
I1004 23:59:56.084207 139975777949504 spec.py:321] Evaluating on the training split.
I1005 00:00:09.450428 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 00:00:20.527804 139975777949504 spec.py:349] Evaluating on the test split.
I1005 00:00:26.440046 139975777949504 submission_runner.py:381] Time since start: 14793.55s, 	Step: 15995, 	{'train/ctc_loss': 0.49769854079480114, 'train/wer': 0.15823971564135592, 'validation/ctc_loss': 0.7151972805373468, 'validation/wer': 0.2040264568145609, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4378461412833086, 'test/wer': 0.13828123413157842, 'test/num_examples': 2472, 'score': 14427.456665754318, 'total_duration': 14793.554208755493, 'accumulated_submission_time': 14427.456665754318, 'accumulated_eval_time': 346.7441167831421, 'accumulated_logging_time': 0.44506216049194336}
I1005 00:00:26.481813 139947046565632 logging_writer.py:48] [15995] accumulated_eval_time=346.744117, accumulated_logging_time=0.445062, accumulated_submission_time=14427.456666, global_step=15995, preemption_count=0, score=14427.456666, test/ctc_loss=0.437846, test/num_examples=2472, test/wer=0.138281, total_duration=14793.554209, train/ctc_loss=0.497699, train/wer=0.158240, validation/ctc_loss=0.715197, validation/num_examples=5348, validation/wer=0.204026
I1005 00:00:32.650858 139947038172928 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.504283, loss=1.683155
I1005 00:00:32.654611 139975777949504 pytorch_submission_base.py:86] 16000) loss = 1.683, grad_norm = 2.504
I1005 00:08:03.793843 139947046565632 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.233827, loss=1.616446
I1005 00:08:03.804776 139975777949504 pytorch_submission_base.py:86] 16500) loss = 1.616, grad_norm = 3.234
I1005 00:15:32.299142 139947038172928 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.239186, loss=1.634155
I1005 00:15:32.304339 139975777949504 pytorch_submission_base.py:86] 17000) loss = 1.634, grad_norm = 2.239
I1005 00:23:00.253202 139947046565632 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.141433, loss=1.679249
I1005 00:23:00.259068 139975777949504 pytorch_submission_base.py:86] 17500) loss = 1.679, grad_norm = 2.141
I1005 00:24:27.875994 139975777949504 spec.py:321] Evaluating on the training split.
I1005 00:24:41.138252 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 00:24:52.949514 139975777949504 spec.py:349] Evaluating on the test split.
I1005 00:24:58.830071 139975777949504 submission_runner.py:381] Time since start: 16265.94s, 	Step: 17596, 	{'train/ctc_loss': 0.4638430450648158, 'train/wer': 0.14843499480958514, 'validation/ctc_loss': 0.6864642295967701, 'validation/wer': 0.1959735431854391, 'validation/num_examples': 5348, 'test/ctc_loss': 0.41357645553745404, 'test/wer': 0.1333658318607438, 'test/num_examples': 2472, 'score': 15866.92526960373, 'total_duration': 16265.944315433502, 'accumulated_submission_time': 15866.92526960373, 'accumulated_eval_time': 377.6980426311493, 'accumulated_logging_time': 0.5017662048339844}
I1005 00:24:58.862020 139947046565632 logging_writer.py:48] [17596] accumulated_eval_time=377.698043, accumulated_logging_time=0.501766, accumulated_submission_time=15866.925270, global_step=17596, preemption_count=0, score=15866.925270, test/ctc_loss=0.413576, test/num_examples=2472, test/wer=0.133366, total_duration=16265.944315, train/ctc_loss=0.463843, train/wer=0.148435, validation/ctc_loss=0.686464, validation/num_examples=5348, validation/wer=0.195974
I1005 00:31:03.741454 139947038172928 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.085731, loss=1.688080
I1005 00:31:03.747117 139975777949504 pytorch_submission_base.py:86] 18000) loss = 1.688, grad_norm = 2.086
I1005 00:38:31.853647 139947046565632 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.332535, loss=1.613231
I1005 00:38:31.859050 139975777949504 pytorch_submission_base.py:86] 18500) loss = 1.613, grad_norm = 2.333
I1005 00:46:03.220753 139947046565632 logging_writer.py:48] [19000] global_step=19000, grad_norm=5.341642, loss=1.584875
I1005 00:46:03.231458 139975777949504 pytorch_submission_base.py:86] 19000) loss = 1.585, grad_norm = 5.342
I1005 00:49:00.111230 139975777949504 spec.py:321] Evaluating on the training split.
I1005 00:49:13.416657 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 00:49:24.547111 139975777949504 spec.py:349] Evaluating on the test split.
I1005 00:49:30.348319 139975777949504 submission_runner.py:381] Time since start: 17737.46s, 	Step: 19197, 	{'train/ctc_loss': 0.4328374066465925, 'train/wer': 0.13951617724587348, 'validation/ctc_loss': 0.6467028114482871, 'validation/wer': 0.18370105730700526, 'validation/num_examples': 5348, 'test/ctc_loss': 0.39494697742596296, 'test/wer': 0.12493652631365142, 'test/num_examples': 2472, 'score': 17306.245272397995, 'total_duration': 17737.462498664856, 'accumulated_submission_time': 17306.245272397995, 'accumulated_eval_time': 407.93481492996216, 'accumulated_logging_time': 0.5452718734741211}
I1005 00:49:30.383554 139947046565632 logging_writer.py:48] [19197] accumulated_eval_time=407.934815, accumulated_logging_time=0.545272, accumulated_submission_time=17306.245272, global_step=19197, preemption_count=0, score=17306.245272, test/ctc_loss=0.394947, test/num_examples=2472, test/wer=0.124937, total_duration=17737.462499, train/ctc_loss=0.432837, train/wer=0.139516, validation/ctc_loss=0.646703, validation/num_examples=5348, validation/wer=0.183701
I1005 00:54:04.092651 139947038172928 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.979148, loss=1.569235
I1005 00:54:04.099966 139975777949504 pytorch_submission_base.py:86] 19500) loss = 1.569, grad_norm = 1.979
I1005 01:01:35.040160 139947046565632 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.739568, loss=1.567671
I1005 01:01:35.051607 139975777949504 pytorch_submission_base.py:86] 20000) loss = 1.568, grad_norm = 1.740
I1005 01:09:04.538074 139947038172928 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.744602, loss=1.497503
I1005 01:09:04.543388 139975777949504 pytorch_submission_base.py:86] 20500) loss = 1.498, grad_norm = 2.745
I1005 01:13:31.777203 139975777949504 spec.py:321] Evaluating on the training split.
I1005 01:13:45.141898 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 01:13:56.239830 139975777949504 spec.py:349] Evaluating on the test split.
I1005 01:14:02.097921 139975777949504 submission_runner.py:381] Time since start: 19209.21s, 	Step: 20795, 	{'train/ctc_loss': 0.4026566115052227, 'train/wer': 0.13121694847086574, 'validation/ctc_loss': 0.6187789024826703, 'validation/wer': 0.17838072707961183, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3699857148041809, 'test/wer': 0.1176446692259257, 'test/num_examples': 2472, 'score': 18745.71274995804, 'total_duration': 19209.212159872055, 'accumulated_submission_time': 18745.71274995804, 'accumulated_eval_time': 438.25547099113464, 'accumulated_logging_time': 0.5907702445983887}
I1005 01:14:02.136518 139947046565632 logging_writer.py:48] [20795] accumulated_eval_time=438.255471, accumulated_logging_time=0.590770, accumulated_submission_time=18745.712750, global_step=20795, preemption_count=0, score=18745.712750, test/ctc_loss=0.369986, test/num_examples=2472, test/wer=0.117645, total_duration=19209.212160, train/ctc_loss=0.402657, train/wer=0.131217, validation/ctc_loss=0.618779, validation/num_examples=5348, validation/wer=0.178381
I1005 01:17:08.755451 139947038172928 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.465667, loss=1.509838
I1005 01:17:08.765583 139975777949504 pytorch_submission_base.py:86] 21000) loss = 1.510, grad_norm = 1.466
I1005 01:24:38.079458 139947046565632 logging_writer.py:48] [21500] global_step=21500, grad_norm=4.132095, loss=1.551637
I1005 01:24:38.086601 139975777949504 pytorch_submission_base.py:86] 21500) loss = 1.552, grad_norm = 4.132
I1005 01:32:09.054091 139947046565632 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.258405, loss=1.484382
I1005 01:32:09.061920 139975777949504 pytorch_submission_base.py:86] 22000) loss = 1.484, grad_norm = 2.258
I1005 01:38:03.224457 139975777949504 spec.py:321] Evaluating on the training split.
I1005 01:38:16.580760 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 01:38:27.882951 139975777949504 spec.py:349] Evaluating on the test split.
I1005 01:38:33.875369 139975777949504 submission_runner.py:381] Time since start: 20680.99s, 	Step: 22395, 	{'train/ctc_loss': 0.3832056635109387, 'train/wer': 0.12441234177387184, 'validation/ctc_loss': 0.6021300521461221, 'validation/wer': 0.17340800463477044, 'validation/num_examples': 5348, 'test/ctc_loss': 0.34795402626782374, 'test/wer': 0.11163244165498751, 'test/num_examples': 2472, 'score': 20184.850664377213, 'total_duration': 20680.989580631256, 'accumulated_submission_time': 20184.850664377213, 'accumulated_eval_time': 468.90610575675964, 'accumulated_logging_time': 0.640247106552124}
I1005 01:38:33.914685 139947046565632 logging_writer.py:48] [22395] accumulated_eval_time=468.906106, accumulated_logging_time=0.640247, accumulated_submission_time=20184.850664, global_step=22395, preemption_count=0, score=20184.850664, test/ctc_loss=0.347954, test/num_examples=2472, test/wer=0.111632, total_duration=20680.989581, train/ctc_loss=0.383206, train/wer=0.124412, validation/ctc_loss=0.602130, validation/num_examples=5348, validation/wer=0.173408
I1005 01:40:09.661882 139947038172928 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.204962, loss=1.522334
I1005 01:40:09.667228 139975777949504 pytorch_submission_base.py:86] 22500) loss = 1.522, grad_norm = 2.205
I1005 01:47:39.540934 139947046565632 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.007915, loss=1.488278
I1005 01:47:39.553729 139975777949504 pytorch_submission_base.py:86] 23000) loss = 1.488, grad_norm = 2.008
I1005 01:55:08.861057 139947038172928 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.395058, loss=1.450799
I1005 01:55:08.866195 139975777949504 pytorch_submission_base.py:86] 23500) loss = 1.451, grad_norm = 2.395
I1005 02:02:34.794328 139975777949504 spec.py:321] Evaluating on the training split.
I1005 02:02:48.080912 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 02:02:59.481499 139975777949504 spec.py:349] Evaluating on the test split.
I1005 02:03:05.496345 139975777949504 submission_runner.py:381] Time since start: 22152.61s, 	Step: 23995, 	{'train/ctc_loss': 0.3503936153924671, 'train/wer': 0.11550439418890936, 'validation/ctc_loss': 0.5685644197684347, 'validation/wer': 0.16321150967991116, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3235348349087038, 'test/wer': 0.10474681615989276, 'test/num_examples': 2472, 'score': 21623.8597779274, 'total_duration': 22152.610466957092, 'accumulated_submission_time': 21623.8597779274, 'accumulated_eval_time': 499.6078772544861, 'accumulated_logging_time': 0.6946549415588379}
I1005 02:03:05.524605 139947046565632 logging_writer.py:48] [23995] accumulated_eval_time=499.607877, accumulated_logging_time=0.694655, accumulated_submission_time=21623.859778, global_step=23995, preemption_count=0, score=21623.859778, test/ctc_loss=0.323535, test/num_examples=2472, test/wer=0.104747, total_duration=22152.610467, train/ctc_loss=0.350394, train/wer=0.115504, validation/ctc_loss=0.568564, validation/num_examples=5348, validation/wer=0.163212
I1005 02:03:11.773833 139947038172928 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.254980, loss=1.457218
I1005 02:03:11.778115 139975777949504 pytorch_submission_base.py:86] 24000) loss = 1.457, grad_norm = 3.255
I1005 02:10:41.133953 139947046565632 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.954624, loss=1.448996
I1005 02:10:41.141958 139975777949504 pytorch_submission_base.py:86] 24500) loss = 1.449, grad_norm = 1.955
I1005 02:18:11.189448 139947046565632 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.095007, loss=1.489586
I1005 02:18:11.197863 139975777949504 pytorch_submission_base.py:86] 25000) loss = 1.490, grad_norm = 2.095
I1005 02:25:39.974061 139947038172928 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.623423, loss=1.378033
I1005 02:25:39.982936 139975777949504 pytorch_submission_base.py:86] 25500) loss = 1.378, grad_norm = 2.623
I1005 02:27:06.591367 139975777949504 spec.py:321] Evaluating on the training split.
I1005 02:27:20.039488 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 02:27:31.048866 139975777949504 spec.py:349] Evaluating on the test split.
I1005 02:27:36.924623 139975777949504 submission_runner.py:381] Time since start: 23624.04s, 	Step: 25597, 	{'train/ctc_loss': 0.3249483791807016, 'train/wer': 0.1072377753501492, 'validation/ctc_loss': 0.5447510207108951, 'validation/wer': 0.15571863081156762, 'validation/num_examples': 5348, 'test/ctc_loss': 0.30927149029776113, 'test/wer': 0.09922206650011171, 'test/num_examples': 2472, 'score': 23062.970512390137, 'total_duration': 23624.03781104088, 'accumulated_submission_time': 23062.970512390137, 'accumulated_eval_time': 529.9398543834686, 'accumulated_logging_time': 0.7340688705444336}
I1005 02:27:36.957949 139947046565632 logging_writer.py:48] [25597] accumulated_eval_time=529.939854, accumulated_logging_time=0.734069, accumulated_submission_time=23062.970512, global_step=25597, preemption_count=0, score=23062.970512, test/ctc_loss=0.309271, test/num_examples=2472, test/wer=0.099222, total_duration=23624.037811, train/ctc_loss=0.324948, train/wer=0.107238, validation/ctc_loss=0.544751, validation/num_examples=5348, validation/wer=0.155719
I1005 02:33:42.079123 139947046565632 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.053029, loss=1.355950
I1005 02:33:42.087643 139975777949504 pytorch_submission_base.py:86] 26000) loss = 1.356, grad_norm = 2.053
I1005 02:41:11.164450 139947038172928 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.696131, loss=1.356638
I1005 02:41:11.179416 139975777949504 pytorch_submission_base.py:86] 26500) loss = 1.357, grad_norm = 1.696
I1005 02:48:41.498727 139947046565632 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.797863, loss=1.363849
I1005 02:48:41.506835 139975777949504 pytorch_submission_base.py:86] 27000) loss = 1.364, grad_norm = 3.798
I1005 02:51:37.908546 139975777949504 spec.py:321] Evaluating on the training split.
I1005 02:51:51.180080 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 02:52:02.510110 139975777949504 spec.py:349] Evaluating on the test split.
I1005 02:52:08.330543 139975777949504 submission_runner.py:381] Time since start: 25095.44s, 	Step: 27197, 	{'train/ctc_loss': 0.29260963709128646, 'train/wer': 0.09654171626094471, 'validation/ctc_loss': 0.5106116554117691, 'validation/wer': 0.14739535557379424, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2873681618572166, 'test/wer': 0.09079276095301932, 'test/num_examples': 2472, 'score': 24502.093911409378, 'total_duration': 25095.44479584694, 'accumulated_submission_time': 24502.093911409378, 'accumulated_eval_time': 560.361724615097, 'accumulated_logging_time': 0.7785224914550781}
I1005 02:52:08.356801 139947046565632 logging_writer.py:48] [27197] accumulated_eval_time=560.361725, accumulated_logging_time=0.778522, accumulated_submission_time=24502.093911, global_step=27197, preemption_count=0, score=24502.093911, test/ctc_loss=0.287368, test/num_examples=2472, test/wer=0.090793, total_duration=25095.444796, train/ctc_loss=0.292610, train/wer=0.096542, validation/ctc_loss=0.510612, validation/num_examples=5348, validation/wer=0.147395
I1005 02:56:41.322216 139947038172928 logging_writer.py:48] [27500] global_step=27500, grad_norm=6.824221, loss=1.352397
I1005 02:56:41.326842 139975777949504 pytorch_submission_base.py:86] 27500) loss = 1.352, grad_norm = 6.824
I1005 03:04:11.363914 139947046565632 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.300215, loss=1.369418
I1005 03:04:11.373372 139975777949504 pytorch_submission_base.py:86] 28000) loss = 1.369, grad_norm = 2.300
I1005 03:11:40.567395 139947038172928 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.520052, loss=1.268265
I1005 03:11:40.575734 139975777949504 pytorch_submission_base.py:86] 28500) loss = 1.268, grad_norm = 2.520
I1005 03:16:09.476855 139975777949504 spec.py:321] Evaluating on the training split.
I1005 03:16:22.823037 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 03:16:33.872178 139975777949504 spec.py:349] Evaluating on the test split.
I1005 03:16:39.839721 139975777949504 submission_runner.py:381] Time since start: 26566.95s, 	Step: 28800, 	{'train/ctc_loss': 0.26803140204484366, 'train/wer': 0.08871533156152679, 'validation/ctc_loss': 0.47866906519991964, 'validation/wer': 0.13769130497755033, 'validation/num_examples': 5348, 'test/ctc_loss': 0.26637547684528035, 'test/wer': 0.08581642394329007, 'test/num_examples': 2472, 'score': 25941.26508665085, 'total_duration': 26566.953872442245, 'accumulated_submission_time': 25941.26508665085, 'accumulated_eval_time': 590.7242813110352, 'accumulated_logging_time': 0.8186581134796143}
I1005 03:16:39.873501 139947046565632 logging_writer.py:48] [28800] accumulated_eval_time=590.724281, accumulated_logging_time=0.818658, accumulated_submission_time=25941.265087, global_step=28800, preemption_count=0, score=25941.265087, test/ctc_loss=0.266375, test/num_examples=2472, test/wer=0.085816, total_duration=26566.953872, train/ctc_loss=0.268031, train/wer=0.088715, validation/ctc_loss=0.478669, validation/num_examples=5348, validation/wer=0.137691
I1005 03:19:42.873985 139947046565632 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.371932, loss=1.288829
I1005 03:19:42.885679 139975777949504 pytorch_submission_base.py:86] 29000) loss = 1.289, grad_norm = 2.372
I1005 03:27:11.288372 139947038172928 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.311398, loss=1.307928
I1005 03:27:11.293643 139975777949504 pytorch_submission_base.py:86] 29500) loss = 1.308, grad_norm = 3.311
I1005 03:34:41.639513 139947046565632 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.903561, loss=1.257142
I1005 03:34:41.647880 139975777949504 pytorch_submission_base.py:86] 30000) loss = 1.257, grad_norm = 2.904
I1005 03:40:41.313684 139975777949504 spec.py:321] Evaluating on the training split.
I1005 03:40:54.602694 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 03:41:05.719291 139975777949504 spec.py:349] Evaluating on the test split.
I1005 03:41:11.612428 139975777949504 submission_runner.py:381] Time since start: 28038.73s, 	Step: 30401, 	{'train/ctc_loss': 0.24179364537613554, 'train/wer': 0.0806063274146299, 'validation/ctc_loss': 0.4575536726755576, 'validation/wer': 0.132129580456718, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2511621717530163, 'test/wer': 0.08043385534092987, 'test/num_examples': 2472, 'score': 27380.76740050316, 'total_duration': 28038.726625919342, 'accumulated_submission_time': 27380.76740050316, 'accumulated_eval_time': 621.0227508544922, 'accumulated_logging_time': 0.8638122081756592}
I1005 03:41:11.650324 139947046565632 logging_writer.py:48] [30401] accumulated_eval_time=621.022751, accumulated_logging_time=0.863812, accumulated_submission_time=27380.767401, global_step=30401, preemption_count=0, score=27380.767401, test/ctc_loss=0.251162, test/num_examples=2472, test/wer=0.080434, total_duration=28038.726626, train/ctc_loss=0.241794, train/wer=0.080606, validation/ctc_loss=0.457554, validation/num_examples=5348, validation/wer=0.132130
I1005 03:42:42.629118 139947038172928 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.188904, loss=1.203380
I1005 03:42:42.633393 139975777949504 pytorch_submission_base.py:86] 30500) loss = 1.203, grad_norm = 2.189
I1005 03:50:14.521078 139947046565632 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.571617, loss=1.277454
I1005 03:50:14.528936 139975777949504 pytorch_submission_base.py:86] 31000) loss = 1.277, grad_norm = 2.572
I1005 03:57:43.684857 139947038172928 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.633316, loss=1.239629
I1005 03:57:43.691185 139975777949504 pytorch_submission_base.py:86] 31500) loss = 1.240, grad_norm = 1.633
I1005 04:05:13.129198 139975777949504 spec.py:321] Evaluating on the training split.
I1005 04:05:26.524222 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 04:05:38.071261 139975777949504 spec.py:349] Evaluating on the test split.
I1005 04:05:43.955621 139975777949504 submission_runner.py:381] Time since start: 29511.07s, 	Step: 32000, 	{'train/ctc_loss': 0.22502126501309924, 'train/wer': 0.07506807324191682, 'validation/ctc_loss': 0.4394842250916717, 'validation/wer': 0.12580504996861874, 'validation/num_examples': 5348, 'test/ctc_loss': 0.239368870572295, 'test/wer': 0.07752929945361851, 'test/num_examples': 2472, 'score': 28820.322923898697, 'total_duration': 29511.069817066193, 'accumulated_submission_time': 28820.322923898697, 'accumulated_eval_time': 651.848982334137, 'accumulated_logging_time': 0.9210188388824463}
I1005 04:05:43.992149 139947046565632 logging_writer.py:48] [32000] accumulated_eval_time=651.848982, accumulated_logging_time=0.921019, accumulated_submission_time=28820.322924, global_step=32000, preemption_count=0, score=28820.322924, test/ctc_loss=0.239369, test/num_examples=2472, test/wer=0.077529, total_duration=29511.069817, train/ctc_loss=0.225021, train/wer=0.075068, validation/ctc_loss=0.439484, validation/num_examples=5348, validation/wer=0.125805
I1005 04:05:45.602312 139947038172928 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.335825, loss=1.243164
I1005 04:05:45.605995 139975777949504 pytorch_submission_base.py:86] 32000) loss = 1.243, grad_norm = 2.336
I1005 04:13:14.819725 139947046565632 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.310772, loss=1.194679
I1005 04:13:14.828496 139975777949504 pytorch_submission_base.py:86] 32500) loss = 1.195, grad_norm = 3.311
I1005 04:20:45.744593 139947046565632 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.753337, loss=1.257238
I1005 04:20:45.757629 139975777949504 pytorch_submission_base.py:86] 33000) loss = 1.257, grad_norm = 2.753
I1005 04:28:15.170833 139947038172928 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.452084, loss=1.167156
I1005 04:28:15.181503 139975777949504 pytorch_submission_base.py:86] 33500) loss = 1.167, grad_norm = 2.452
I1005 04:29:44.905642 139975777949504 spec.py:321] Evaluating on the training split.
I1005 04:29:58.482508 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 04:30:09.625712 139975777949504 spec.py:349] Evaluating on the test split.
I1005 04:30:15.596652 139975777949504 submission_runner.py:381] Time since start: 30982.71s, 	Step: 33600, 	{'train/ctc_loss': 0.21147590355040657, 'train/wer': 0.07052985711412935, 'validation/ctc_loss': 0.4256438994374121, 'validation/wer': 0.12226138174093565, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2318315647380476, 'test/wer': 0.07326386773099344, 'test/num_examples': 2472, 'score': 30259.286304712296, 'total_duration': 30982.709978342056, 'accumulated_submission_time': 30259.286304712296, 'accumulated_eval_time': 682.538982629776, 'accumulated_logging_time': 0.9696075916290283}
I1005 04:30:15.630529 139947046565632 logging_writer.py:48] [33600] accumulated_eval_time=682.538983, accumulated_logging_time=0.969608, accumulated_submission_time=30259.286305, global_step=33600, preemption_count=0, score=30259.286305, test/ctc_loss=0.231832, test/num_examples=2472, test/wer=0.073264, total_duration=30982.709978, train/ctc_loss=0.211476, train/wer=0.070530, validation/ctc_loss=0.425644, validation/num_examples=5348, validation/wer=0.122261
I1005 04:36:18.181237 139947046565632 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.706275, loss=1.181836
I1005 04:36:18.189007 139975777949504 pytorch_submission_base.py:86] 34000) loss = 1.182, grad_norm = 2.706
I1005 04:43:46.882889 139947038172928 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.218127, loss=1.142847
I1005 04:43:46.925892 139975777949504 pytorch_submission_base.py:86] 34500) loss = 1.143, grad_norm = 3.218
I1005 04:51:14.753800 139947046565632 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.076115, loss=1.178934
I1005 04:51:14.759382 139975777949504 pytorch_submission_base.py:86] 35000) loss = 1.179, grad_norm = 2.076
I1005 04:54:16.310224 139975777949504 spec.py:321] Evaluating on the training split.
I1005 04:54:29.612802 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 04:54:41.004627 139975777949504 spec.py:349] Evaluating on the test split.
I1005 04:54:46.909614 139975777949504 submission_runner.py:381] Time since start: 32454.02s, 	Step: 35201, 	{'train/ctc_loss': 0.20667522051818585, 'train/wer': 0.06911132488735984, 'validation/ctc_loss': 0.42249808101642555, 'validation/wer': 0.12082267175203978, 'validation/num_examples': 5348, 'test/ctc_loss': 0.22909764339634814, 'test/wer': 0.07287794771799402, 'test/num_examples': 2472, 'score': 31698.03949546814, 'total_duration': 32454.023847818375, 'accumulated_submission_time': 31698.03949546814, 'accumulated_eval_time': 713.1381816864014, 'accumulated_logging_time': 1.0139715671539307}
I1005 04:54:46.941575 139947046565632 logging_writer.py:48] [35201] accumulated_eval_time=713.138182, accumulated_logging_time=1.013972, accumulated_submission_time=31698.039495, global_step=35201, preemption_count=0, score=31698.039495, test/ctc_loss=0.229098, test/num_examples=2472, test/wer=0.072878, total_duration=32454.023848, train/ctc_loss=0.206675, train/wer=0.069111, validation/ctc_loss=0.422498, validation/num_examples=5348, validation/wer=0.120823
I1005 04:59:16.983631 139947038172928 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.225163, loss=1.182426
I1005 04:59:16.989096 139975777949504 pytorch_submission_base.py:86] 35500) loss = 1.182, grad_norm = 2.225
I1005 05:06:44.651133 139975777949504 spec.py:321] Evaluating on the training split.
I1005 05:06:57.396871 139975777949504 spec.py:333] Evaluating on the validation split.
I1005 05:07:08.597854 139975777949504 spec.py:349] Evaluating on the test split.
I1005 05:07:14.462027 139975777949504 submission_runner.py:381] Time since start: 33201.58s, 	Step: 36000, 	{'train/ctc_loss': 0.20638512586761254, 'train/wer': 0.06900262509986793, 'validation/ctc_loss': 0.42228636493495075, 'validation/wer': 0.12074542557813933, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2290650052019485, 'test/wer': 0.0731623098328357, 'test/num_examples': 2472, 'score': 32414.40204501152, 'total_duration': 33201.576246738434, 'accumulated_submission_time': 32414.40204501152, 'accumulated_eval_time': 742.9489779472351, 'accumulated_logging_time': 1.0564579963684082}
I1005 05:07:14.492928 139947046565632 logging_writer.py:48] [36000] accumulated_eval_time=742.948978, accumulated_logging_time=1.056458, accumulated_submission_time=32414.402045, global_step=36000, preemption_count=0, score=32414.402045, test/ctc_loss=0.229065, test/num_examples=2472, test/wer=0.073162, total_duration=33201.576247, train/ctc_loss=0.206385, train/wer=0.069003, validation/ctc_loss=0.422286, validation/num_examples=5348, validation/wer=0.120745
I1005 05:07:15.048062 139947038172928 logging_writer.py:48] [36000] global_step=36000, preemption_count=0, score=32414.402045
I1005 05:07:15.458289 139975777949504 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_deepspeech_pytorch/nadamw_run_0/librispeech_deepspeech_pytorch/trial_1/checkpoint_36000.
I1005 05:07:15.591888 139975777949504 submission_runner.py:549] Tuning trial 1/1
I1005 05:07:15.592171 139975777949504 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.004958460849689891, beta1=0.863744242567442, beta2=0.6291854735396584, warmup_steps=720, weight_decay=0.1147386261512052)
I1005 05:07:15.593497 139975777949504 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/ctc_loss': 32.15358443060801, 'train/wer': 2.27367888995777, 'validation/ctc_loss': 31.020205445047218, 'validation/wer': 2.17933664848163, 'validation/num_examples': 5348, 'test/ctc_loss': 31.02377895348087, 'test/wer': 2.0662563727581094, 'test/num_examples': 2472, 'score': 36.68256998062134, 'total_duration': 79.95515942573547, 'accumulated_submission_time': 36.68256998062134, 'accumulated_eval_time': 42.89408588409424, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1601, {'train/ctc_loss': 1.0921613971964206, 'train/wer': 0.32182746082731406, 'validation/ctc_loss': 1.3457177430241611, 'validation/wer': 0.35752426012649063, 'validation/num_examples': 5348, 'test/ctc_loss': 0.9301977143686689, 'test/wer': 0.2827371884711474, 'test/num_examples': 2472, 'score': 1475.9301266670227, 'total_duration': 1551.2678656578064, 'accumulated_submission_time': 1475.9301266670227, 'accumulated_eval_time': 73.29950070381165, 'accumulated_logging_time': 0.03090071678161621, 'global_step': 1601, 'preemption_count': 0}), (3200, {'train/ctc_loss': 0.7693514277329795, 'train/wer': 0.23767752034044773, 'validation/ctc_loss': 1.0031631834563994, 'validation/wer': 0.2792835417370733, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6636896936899155, 'test/wer': 0.20729998171957834, 'test/num_examples': 2472, 'score': 2914.8871920108795, 'total_duration': 3022.5438780784607, 'accumulated_submission_time': 2914.8871920108795, 'accumulated_eval_time': 103.70521712303162, 'accumulated_logging_time': 0.07859945297241211, 'global_step': 3200, 'preemption_count': 0}), (4801, {'train/ctc_loss': 0.7131809057194379, 'train/wer': 0.2193398661905616, 'validation/ctc_loss': 0.9386733662346796, 'validation/wer': 0.2621252353594361, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6216358212626621, 'test/wer': 0.18995389271423638, 'test/num_examples': 2472, 'score': 4353.6596512794495, 'total_duration': 4493.453898191452, 'accumulated_submission_time': 4353.6596512794495, 'accumulated_eval_time': 133.93509721755981, 'accumulated_logging_time': 0.12140130996704102, 'global_step': 4801, 'preemption_count': 0}), (6398, {'train/ctc_loss': 0.6628117982545677, 'train/wer': 0.2072252748745876, 'validation/ctc_loss': 0.8951692433632208, 'validation/wer': 0.2520446096654275, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5748758488450868, 'test/wer': 0.18018402291146182, 'test/num_examples': 2472, 'score': 5792.530745983124, 'total_duration': 5964.348899841309, 'accumulated_submission_time': 5792.530745983124, 'accumulated_eval_time': 164.05646991729736, 'accumulated_logging_time': 0.17159247398376465, 'global_step': 6398, 'preemption_count': 0}), (8000, {'train/ctc_loss': 0.6248732073593958, 'train/wer': 0.1954367829210894, 'validation/ctc_loss': 0.8528151371308017, 'validation/wer': 0.2415294742432289, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5524409840957482, 'test/wer': 0.1720797026384742, 'test/num_examples': 2472, 'score': 7231.2451684474945, 'total_duration': 7435.430392503738, 'accumulated_submission_time': 7231.2451684474945, 'accumulated_eval_time': 194.48667073249817, 'accumulated_logging_time': 0.21728301048278809, 'global_step': 8000, 'preemption_count': 0}), (9596, {'train/ctc_loss': 0.5936486190500493, 'train/wer': 0.1869364595392216, 'validation/ctc_loss': 0.8201114776408981, 'validation/wer': 0.23116883116883116, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5192390436479772, 'test/wer': 0.1615379928097008, 'test/num_examples': 2472, 'score': 8670.195932865143, 'total_duration': 8907.048548698425, 'accumulated_submission_time': 8670.195932865143, 'accumulated_eval_time': 225.2674674987793, 'accumulated_logging_time': 0.27629590034484863, 'global_step': 9596, 'preemption_count': 0}), (11195, {'train/ctc_loss': 0.5737404732741316, 'train/wer': 0.18285478251890017, 'validation/ctc_loss': 0.7909071180555556, 'validation/wer': 0.22535605658282237, 'validation/num_examples': 5348, 'test/ctc_loss': 0.49879563802503385, 'test/wer': 0.15633822842402453, 'test/num_examples': 2472, 'score': 10109.002165794373, 'total_duration': 10378.035343170166, 'accumulated_submission_time': 10109.002165794373, 'accumulated_eval_time': 255.4791874885559, 'accumulated_logging_time': 0.31482791900634766, 'global_step': 11195, 'preemption_count': 0}), (12794, {'train/ctc_loss': 0.5504678834507162, 'train/wer': 0.1744740289032735, 'validation/ctc_loss': 0.7800603969666717, 'validation/wer': 0.22056679380099453, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4834641045390025, 'test/wer': 0.15211341986066257, 'test/num_examples': 2472, 'score': 11548.54773068428, 'total_duration': 11849.95093035698, 'accumulated_submission_time': 11548.54773068428, 'accumulated_eval_time': 285.9870653152466, 'accumulated_logging_time': 0.35784387588500977, 'global_step': 12794, 'preemption_count': 0}), (14396, {'train/ctc_loss': 0.5290127079718281, 'train/wer': 0.1685661954530879, 'validation/ctc_loss': 0.7479713321905767, 'validation/wer': 0.21184763192198136, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4635446238466998, 'test/wer': 0.14709645969167023, 'test/num_examples': 2472, 'score': 12988.076228141785, 'total_duration': 13321.848577022552, 'accumulated_submission_time': 12988.076228141785, 'accumulated_eval_time': 316.3883891105652, 'accumulated_logging_time': 0.39840173721313477, 'global_step': 14396, 'preemption_count': 0}), (15995, {'train/ctc_loss': 0.49769854079480114, 'train/wer': 0.15823971564135592, 'validation/ctc_loss': 0.7151972805373468, 'validation/wer': 0.2040264568145609, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4378461412833086, 'test/wer': 0.13828123413157842, 'test/num_examples': 2472, 'score': 14427.456665754318, 'total_duration': 14793.554208755493, 'accumulated_submission_time': 14427.456665754318, 'accumulated_eval_time': 346.7441167831421, 'accumulated_logging_time': 0.44506216049194336, 'global_step': 15995, 'preemption_count': 0}), (17596, {'train/ctc_loss': 0.4638430450648158, 'train/wer': 0.14843499480958514, 'validation/ctc_loss': 0.6864642295967701, 'validation/wer': 0.1959735431854391, 'validation/num_examples': 5348, 'test/ctc_loss': 0.41357645553745404, 'test/wer': 0.1333658318607438, 'test/num_examples': 2472, 'score': 15866.92526960373, 'total_duration': 16265.944315433502, 'accumulated_submission_time': 15866.92526960373, 'accumulated_eval_time': 377.6980426311493, 'accumulated_logging_time': 0.5017662048339844, 'global_step': 17596, 'preemption_count': 0}), (19197, {'train/ctc_loss': 0.4328374066465925, 'train/wer': 0.13951617724587348, 'validation/ctc_loss': 0.6467028114482871, 'validation/wer': 0.18370105730700526, 'validation/num_examples': 5348, 'test/ctc_loss': 0.39494697742596296, 'test/wer': 0.12493652631365142, 'test/num_examples': 2472, 'score': 17306.245272397995, 'total_duration': 17737.462498664856, 'accumulated_submission_time': 17306.245272397995, 'accumulated_eval_time': 407.93481492996216, 'accumulated_logging_time': 0.5452718734741211, 'global_step': 19197, 'preemption_count': 0}), (20795, {'train/ctc_loss': 0.4026566115052227, 'train/wer': 0.13121694847086574, 'validation/ctc_loss': 0.6187789024826703, 'validation/wer': 0.17838072707961183, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3699857148041809, 'test/wer': 0.1176446692259257, 'test/num_examples': 2472, 'score': 18745.71274995804, 'total_duration': 19209.212159872055, 'accumulated_submission_time': 18745.71274995804, 'accumulated_eval_time': 438.25547099113464, 'accumulated_logging_time': 0.5907702445983887, 'global_step': 20795, 'preemption_count': 0}), (22395, {'train/ctc_loss': 0.3832056635109387, 'train/wer': 0.12441234177387184, 'validation/ctc_loss': 0.6021300521461221, 'validation/wer': 0.17340800463477044, 'validation/num_examples': 5348, 'test/ctc_loss': 0.34795402626782374, 'test/wer': 0.11163244165498751, 'test/num_examples': 2472, 'score': 20184.850664377213, 'total_duration': 20680.989580631256, 'accumulated_submission_time': 20184.850664377213, 'accumulated_eval_time': 468.90610575675964, 'accumulated_logging_time': 0.640247106552124, 'global_step': 22395, 'preemption_count': 0}), (23995, {'train/ctc_loss': 0.3503936153924671, 'train/wer': 0.11550439418890936, 'validation/ctc_loss': 0.5685644197684347, 'validation/wer': 0.16321150967991116, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3235348349087038, 'test/wer': 0.10474681615989276, 'test/num_examples': 2472, 'score': 21623.8597779274, 'total_duration': 22152.610466957092, 'accumulated_submission_time': 21623.8597779274, 'accumulated_eval_time': 499.6078772544861, 'accumulated_logging_time': 0.6946549415588379, 'global_step': 23995, 'preemption_count': 0}), (25597, {'train/ctc_loss': 0.3249483791807016, 'train/wer': 0.1072377753501492, 'validation/ctc_loss': 0.5447510207108951, 'validation/wer': 0.15571863081156762, 'validation/num_examples': 5348, 'test/ctc_loss': 0.30927149029776113, 'test/wer': 0.09922206650011171, 'test/num_examples': 2472, 'score': 23062.970512390137, 'total_duration': 23624.03781104088, 'accumulated_submission_time': 23062.970512390137, 'accumulated_eval_time': 529.9398543834686, 'accumulated_logging_time': 0.7340688705444336, 'global_step': 25597, 'preemption_count': 0}), (27197, {'train/ctc_loss': 0.29260963709128646, 'train/wer': 0.09654171626094471, 'validation/ctc_loss': 0.5106116554117691, 'validation/wer': 0.14739535557379424, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2873681618572166, 'test/wer': 0.09079276095301932, 'test/num_examples': 2472, 'score': 24502.093911409378, 'total_duration': 25095.44479584694, 'accumulated_submission_time': 24502.093911409378, 'accumulated_eval_time': 560.361724615097, 'accumulated_logging_time': 0.7785224914550781, 'global_step': 27197, 'preemption_count': 0}), (28800, {'train/ctc_loss': 0.26803140204484366, 'train/wer': 0.08871533156152679, 'validation/ctc_loss': 0.47866906519991964, 'validation/wer': 0.13769130497755033, 'validation/num_examples': 5348, 'test/ctc_loss': 0.26637547684528035, 'test/wer': 0.08581642394329007, 'test/num_examples': 2472, 'score': 25941.26508665085, 'total_duration': 26566.953872442245, 'accumulated_submission_time': 25941.26508665085, 'accumulated_eval_time': 590.7242813110352, 'accumulated_logging_time': 0.8186581134796143, 'global_step': 28800, 'preemption_count': 0}), (30401, {'train/ctc_loss': 0.24179364537613554, 'train/wer': 0.0806063274146299, 'validation/ctc_loss': 0.4575536726755576, 'validation/wer': 0.132129580456718, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2511621717530163, 'test/wer': 0.08043385534092987, 'test/num_examples': 2472, 'score': 27380.76740050316, 'total_duration': 28038.726625919342, 'accumulated_submission_time': 27380.76740050316, 'accumulated_eval_time': 621.0227508544922, 'accumulated_logging_time': 0.8638122081756592, 'global_step': 30401, 'preemption_count': 0}), (32000, {'train/ctc_loss': 0.22502126501309924, 'train/wer': 0.07506807324191682, 'validation/ctc_loss': 0.4394842250916717, 'validation/wer': 0.12580504996861874, 'validation/num_examples': 5348, 'test/ctc_loss': 0.239368870572295, 'test/wer': 0.07752929945361851, 'test/num_examples': 2472, 'score': 28820.322923898697, 'total_duration': 29511.069817066193, 'accumulated_submission_time': 28820.322923898697, 'accumulated_eval_time': 651.848982334137, 'accumulated_logging_time': 0.9210188388824463, 'global_step': 32000, 'preemption_count': 0}), (33600, {'train/ctc_loss': 0.21147590355040657, 'train/wer': 0.07052985711412935, 'validation/ctc_loss': 0.4256438994374121, 'validation/wer': 0.12226138174093565, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2318315647380476, 'test/wer': 0.07326386773099344, 'test/num_examples': 2472, 'score': 30259.286304712296, 'total_duration': 30982.709978342056, 'accumulated_submission_time': 30259.286304712296, 'accumulated_eval_time': 682.538982629776, 'accumulated_logging_time': 0.9696075916290283, 'global_step': 33600, 'preemption_count': 0}), (35201, {'train/ctc_loss': 0.20667522051818585, 'train/wer': 0.06911132488735984, 'validation/ctc_loss': 0.42249808101642555, 'validation/wer': 0.12082267175203978, 'validation/num_examples': 5348, 'test/ctc_loss': 0.22909764339634814, 'test/wer': 0.07287794771799402, 'test/num_examples': 2472, 'score': 31698.03949546814, 'total_duration': 32454.023847818375, 'accumulated_submission_time': 31698.03949546814, 'accumulated_eval_time': 713.1381816864014, 'accumulated_logging_time': 1.0139715671539307, 'global_step': 35201, 'preemption_count': 0}), (36000, {'train/ctc_loss': 0.20638512586761254, 'train/wer': 0.06900262509986793, 'validation/ctc_loss': 0.42228636493495075, 'validation/wer': 0.12074542557813933, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2290650052019485, 'test/wer': 0.0731623098328357, 'test/num_examples': 2472, 'score': 32414.40204501152, 'total_duration': 33201.576246738434, 'accumulated_submission_time': 32414.40204501152, 'accumulated_eval_time': 742.9489779472351, 'accumulated_logging_time': 1.0564579963684082, 'global_step': 36000, 'preemption_count': 0})], 'global_step': 36000}
I1005 05:07:15.593675 139975777949504 submission_runner.py:552] Timing: 32414.40204501152
I1005 05:07:15.593737 139975777949504 submission_runner.py:554] Total number of evals: 24
I1005 05:07:15.593788 139975777949504 submission_runner.py:555] ====================
I1005 05:07:15.593980 139975777949504 submission_runner.py:625] Final librispeech_deepspeech score: 32414.40204501152
