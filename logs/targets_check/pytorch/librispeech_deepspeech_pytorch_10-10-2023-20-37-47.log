torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_deepspeech --submission_path=reference_algorithms/target_setting_algorithms/pytorch_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_deepspeech/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_deepspeech/nadamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=36000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --torch_compile=true 2>&1 | tee -a /logs/librispeech_deepspeech_pytorch_10-10-2023-20-37-47.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-10-10 20:37:57.559383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 20:37:57.559379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 20:37:57.559384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 20:37:57.559378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 20:37:57.559386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 20:37:57.559391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 20:37:57.559383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-10 20:37:57.559386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1010 20:38:12.066029 140495175153472 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I1010 20:38:12.066087 140682782320448 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I1010 20:38:12.066086 140299758511936 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I1010 20:38:12.067134 140710599534400 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I1010 20:38:12.067964 139627942434624 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I1010 20:38:13.043879 140654889465664 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I1010 20:38:13.044248 139672192104256 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I1010 20:38:13.045342 140143741998912 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I1010 20:38:13.045891 140143741998912 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 20:38:13.053959 140495175153472 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 20:38:13.053988 140299758511936 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 20:38:13.054016 140682782320448 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 20:38:13.054049 140710599534400 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 20:38:13.054098 139627942434624 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 20:38:13.054650 140654889465664 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 20:38:13.054812 139672192104256 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I1010 20:38:13.554740 140143741998912 logger_utils.py:61] Removing existing experiment directory /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_pytorch because --overwrite was set.
I1010 20:38:13.556594 140143741998912 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_pytorch.
W1010 20:38:14.315901 140654889465664 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 20:38:14.315919 140710599534400 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 20:38:14.316004 140143741998912 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 20:38:14.316108 140299758511936 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 20:38:14.316236 140682782320448 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 20:38:14.316445 140495175153472 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 20:38:14.318668 139672192104256 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W1010 20:38:14.319819 139627942434624 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I1010 20:38:14.326505 140143741998912 submission_runner.py:523] Using RNG seed 2178457709
I1010 20:38:14.328254 140143741998912 submission_runner.py:532] --- Tuning run 1/1 ---
I1010 20:38:14.328409 140143741998912 submission_runner.py:537] Creating tuning directory at /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_pytorch/trial_1.
I1010 20:38:14.328598 140143741998912 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_pytorch/trial_1/hparams.json.
I1010 20:38:14.329408 140143741998912 submission_runner.py:200] Initializing dataset.
I1010 20:38:14.329565 140143741998912 input_pipeline.py:20] Loading split = train-clean-100
I1010 20:38:14.367235 140143741998912 input_pipeline.py:20] Loading split = train-clean-360
I1010 20:38:14.778645 140143741998912 input_pipeline.py:20] Loading split = train-other-500
I1010 20:38:15.257639 140143741998912 submission_runner.py:207] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
W1010 20:38:23.410622 140143741998912 submission_runner.py:228] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1010 20:38:23.411810 140654889465664 submission_runner.py:228] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1010 20:38:23.412104 139672192104256 submission_runner.py:228] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1010 20:38:23.412773 140495175153472 submission_runner.py:228] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1010 20:38:23.413365 140682782320448 submission_runner.py:228] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1010 20:38:23.414454 140710599534400 submission_runner.py:228] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1010 20:38:23.424653 139627942434624 submission_runner.py:228] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
W1010 20:38:23.433338 140299758511936 submission_runner.py:228] These workloads cannot be fully compiled under current PyTorch version. Proceeding with `backend=eager`.
I1010 20:38:23.740237 140143741998912 submission_runner.py:241] Initializing optimizer.
I1010 20:38:23.740978 140143741998912 submission_runner.py:248] Initializing metrics bundle.
I1010 20:38:23.741080 140143741998912 submission_runner.py:266] Initializing checkpoint and logger.
I1010 20:38:23.741568 140143741998912 submission_runner.py:286] Saving meta data to /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I1010 20:38:23.741830 140143741998912 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1010 20:38:23.741901 140143741998912 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1010 20:38:24.012529 140143741998912 logger_utils.py:220] Unable to record git information. Continuing without it.
I1010 20:38:24.247539 140143741998912 submission_runner.py:289] Saving flags to /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I1010 20:38:24.260269 140143741998912 submission_runner.py:299] Starting training loop.
[2023-10-10 20:38:26,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:26,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:26,651] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:26,770] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:26,802] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:26,810] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:26,839] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:26,886] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:27,588] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:27,596] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:27,602] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:27,674] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:27,691] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:27,692] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:27,692] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:27,692] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:27,706] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:27,706] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:27,706] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:27,707] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:27,707] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:27,707] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:27,707] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:27,707] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:27,709] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:27,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:27,728] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:27,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:27,780] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:27,780] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:27,781] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:27,781] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:27,828] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:27,829] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:27,829] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:27,829] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:27,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:27,829] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:27,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:27,830] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:27,843] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:27,844] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:27,844] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:27,844] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:27,915] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:27,916] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:27,917] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:27,917] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
<eval_with_key>.1:27: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  abs_1 = torch.abs(fft_rfft);  fft_rfft = None
[2023-10-10 20:38:28,391] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-10 20:38:28,391] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-10 20:38:28,397] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-10 20:38:28,403] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,403] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,404] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,404] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,407] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
<eval_with_key>.1:27: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  abs_1 = torch.abs(fft_rfft);  fft_rfft = None
[2023-10-10 20:38:28,407] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,408] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,408] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,409] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,409] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,410] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,410] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-10 20:38:28,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-10 20:38:28,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-10 20:38:28,424] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,425] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,425] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,425] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,425] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,425] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-10 20:38:28,425] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,425] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,425] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,425] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-10 20:38:28,427] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-10 20:38:28,428] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing linear_to_mel_weight_matrix
[2023-10-10 20:38:28,431] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-10 20:38:28,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,433] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,434] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,437] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,437] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,438] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,438] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,439] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-10 20:38:28,440] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,440] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,440] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,441] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,441] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,441] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,441] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-10 20:38:28,441] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,441] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-10 20:38:28,449] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-10 20:38:28,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,450] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-10 20:38:28,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,451] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,453] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,453] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,453] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,453] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,458] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-10 20:38:28,460] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-10 20:38:28,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-10 20:38:28,467] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-10 20:38:28,468] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-10 20:38:28,468] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,468] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,469] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,471] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-10 20:38:28,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-10 20:38:28,476] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-10 20:38:28,478] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-10 20:38:28,479] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,479] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,479] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,479] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-10 20:38:28,482] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,482] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,482] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,482] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,483] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,483] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,484] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,484] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,486] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-10 20:38:28,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _hertz_to_mel
[2023-10-10 20:38:28,490] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-10 20:38:28,490] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-10 20:38:28,493] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-10 20:38:28,494] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,494] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,494] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,494] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,499] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-10 20:38:28,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _hertz_to_mel (RETURN_VALUE)
[2023-10-10 20:38:28,503] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,503] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,503] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,503] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-10 20:38:28,504] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,504] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,505] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,505] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,507] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,507] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,507] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,507] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,510] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,511] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in linear_to_mel_weight_matrix>
[2023-10-10 20:38:28,517] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,518] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-10 20:38:28,521] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,521] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,521] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,521] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,528] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-10 20:38:28,529] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,532] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,532] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,532] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,532] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,534] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:38:28,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-10 20:38:28,537] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,537] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,537] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,537] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,539] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,539] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,539] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,539] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:38:28,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,543] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,543] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,543] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,543] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,544] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-10 20:38:28,547] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,547] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,547] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,547] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,548] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:38:28,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in linear_to_mel_weight_matrix> (RETURN_VALUE)
[2023-10-10 20:38:28,555] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,555] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,556] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,556] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,556] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,557] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,557] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,557] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,563] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:38:28,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:38:28,565] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,566] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,566] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,566] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,566] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,566] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,567] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,567] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,567] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,568] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,569] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,569] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,571] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:38:28,572] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,573] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,573] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,574] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,574] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,583] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,588] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:38:28,591] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,591] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,591] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,591] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,596] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:38:28,599] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:28,599] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:28,599] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:28,599] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:28,601] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,624] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,923] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,923] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,923] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,923] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,923] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:28,951] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:29,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,268] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,268] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,271] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,272] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,281] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,293] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,408] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,409] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,411] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,414] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,417] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,417] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,417] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,418] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,418] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,419] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,419] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,419] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,419] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,420] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,420] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,421] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,421] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,423] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,424] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,424] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,424] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,424] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,426] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,426] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,426] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,427] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,427] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,427] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,427] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,427] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,434] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,434] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,438] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,438] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,438] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,438] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,439] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,440] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,446] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,446] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,447] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,447] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,457] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:29,551] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,551] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,551] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,552] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,552] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,555] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,557] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,557] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,557] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,557] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,557] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,557] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,557] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,558] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,558] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,558] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,558] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,558] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,558] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,559] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,559] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,560] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,561] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,561] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,561] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,566] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,566] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,566] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,571] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:29,577] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:29,578] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:29,578] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:29,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,623] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,625] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,629] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:29,641] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:33,631] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-10 20:38:33,639] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-10 20:38:33,643] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-10 20:38:33,650] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:33,725] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:33,725] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:33,726] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:33,726] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:33,817] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-10 20:38:33,825] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-10 20:38:33,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-10 20:38:33,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-10 20:38:33,836] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-10 20:38:33,836] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:33,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-10 20:38:33,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-10 20:38:33,847] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:33,848] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-10 20:38:33,851] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-10 20:38:33,853] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-10 20:38:33,859] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:33,861] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-10 20:38:33,865] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-10 20:38:33,867] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-10 20:38:33,873] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:33,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-10 20:38:33,878] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-10 20:38:33,878] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-10 20:38:33,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__
[2023-10-10 20:38:33,886] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-10 20:38:33,886] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:33,889] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-10 20:38:33,893] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index
[2023-10-10 20:38:33,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:33,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__
[2023-10-10 20:38:33,904] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:33,918] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:33,918] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:33,918] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:33,918] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:33,926] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:33,927] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:33,927] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:33,927] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:33,934] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:33,934] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:33,934] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:33,934] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:33,948] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:33,948] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:33,949] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:33,949] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:33,963] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:33,964] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:33,964] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:33,964] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:33,973] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:33,973] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:33,973] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:33,973] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:33,981] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:33,981] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:33,981] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:33,981] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:33,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:33,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:33,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:33,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:33,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:33,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:33,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:33,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:34,031] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:34,031] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:34,031] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:34,031] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:34,031] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:34,031] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:34,031] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:34,032] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:38:34,379] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,381] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,385] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,394] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,418] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,455] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,455] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,455] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,455] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,460] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,461] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,461] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,461] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,461] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,461] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,461] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,461] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,470] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,471] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,471] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,471] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,489] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,489] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,489] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,489] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,491] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,492] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,492] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,492] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,493] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,493] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,494] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,494] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,506] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,507] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,507] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,507] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,536] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,538] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,538] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,538] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,611] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,611] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,611] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,611] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,611] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,611] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,611] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,611] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,612] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,612] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,612] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,612] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,612] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,612] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,612] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,612] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,612] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,612] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,612] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,612] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,613] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,613] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,613] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,614] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,614] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,614] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,614] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,614] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,617] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,617] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,617] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,617] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,646] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,648] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,718] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,718] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,718] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,719] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,720] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,720] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,720] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,720] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,721] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,721] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,721] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,721] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,721] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,721] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,722] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,722] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,722] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,722] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,722] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,722] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,723] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,723] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,723] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,724] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,724] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,724] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,725] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,725] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,725] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,725] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,754] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,754] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,755] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,828] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,828] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,828] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,828] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,829] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,829] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,829] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,830] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,830] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,830] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,830] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,830] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,830] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,830] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,830] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,830] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,830] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,831] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,831] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,831] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,832] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,832] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,833] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,833] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,833] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,833] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,834] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,834] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,835] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,835] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,864] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,864] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,864] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,865] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,865] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,865] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,866] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,866] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,938] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,938] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,939] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,939] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,939] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,939] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,939] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,939] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,939] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,939] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,940] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,940] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,940] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,940] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,940] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,940] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,940] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,940] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,940] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,940] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,940] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,941] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,941] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,941] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:34,941] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,941] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:34,941] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,941] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,941] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,941] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,941] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:34,941] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:34,969] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,972] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,972] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:34,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,052] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,053] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,055] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,055] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,055] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,057] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,058] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,058] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,058] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,059] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,059] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,059] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,059] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,059] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,059] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,059] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,060] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,060] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,060] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,061] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,061] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,061] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,061] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,061] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,061] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,061] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,061] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,061] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,062] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,062] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,062] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,062] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,063] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,063] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,063] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,063] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,063] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,064] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,064] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,071] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,071] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,071] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,073] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,153] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,154] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,155] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,155] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,156] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,156] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,159] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,159] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,160] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,160] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,160] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,160] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,160] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,160] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,160] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,160] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,161] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,161] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,161] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,162] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,162] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,162] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,162] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,162] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,162] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,162] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,162] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,162] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,162] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,166] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,166] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,166] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,166] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,166] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,167] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,167] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,167] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,171] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,172] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,173] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,173] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,174] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,174] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,254] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,255] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,256] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,256] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,257] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,260] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,260] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,260] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,260] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,260] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,261] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,261] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,261] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,261] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,261] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,262] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,262] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,262] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,262] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,262] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,262] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,263] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,263] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,263] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,263] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,263] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,264] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,264] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,264] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,266] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,266] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,267] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,267] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,267] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,268] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,268] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,268] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,271] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,271] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,273] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,274] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,277] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:38:35,292] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,293] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,293] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,293] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,293] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,294] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,295] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,295] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,295] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,295] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,295] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,296] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,296] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,296] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,296] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,296] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,296] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,296] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,296] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,296] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,296] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,296] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,296] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,296] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,296] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,296] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,296] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,296] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,297] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,297] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,297] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,297] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,297] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,297] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,298] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:38:35,298] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:38:35,301] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:38:35,301] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:38:35,301] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:38:35,301] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I1010 20:39:00.798785 140117565941504 logging_writer.py:48] [0] global_step=0, grad_norm=76.322311, loss=33.007797
I1010 20:39:00.826965 140143741998912 pytorch_submission_base.py:86] 0) loss = 33.008, grad_norm = 76.322
I1010 20:39:01.227331 140143741998912 spec.py:321] Evaluating on the training split.
I1010 20:39:01.228866 140143741998912 input_pipeline.py:20] Loading split = train-clean-100
I1010 20:39:01.298714 140143741998912 input_pipeline.py:20] Loading split = train-clean-360
I1010 20:39:01.547614 140143741998912 input_pipeline.py:20] Loading split = train-other-500
[2023-10-10 20:39:04,047] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,136] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,165] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,201] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,292] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,311] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,316] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,319] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,319] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,320] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,320] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,324] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,375] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,404] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:04,406] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,407] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,407] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,407] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,410] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,412] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,412] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,412] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,412] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,426] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,433] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,433] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,434] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,435] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,448] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,449] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,449] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,449] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,452] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:04,454] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,455] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,455] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,455] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,464] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,465] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,491] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,493] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:04,497] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,497] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,497] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,497] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,506] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,518] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:04,521] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,521] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,522] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,522] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,543] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,551] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:04,584] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,584] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,585] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,592] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,592] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,593] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,596] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,599] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:04,606] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,606] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,607] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,652] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:04,656] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,656] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,657] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,657] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,657] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,659] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,659] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,659] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,662] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,671] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,679] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,681] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:04,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,688] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,689] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,689] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,693] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:04,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:04,697] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,697] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,697] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,697] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,698] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,698] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,699] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,699] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,708] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,716] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,721] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,733] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,737] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,737] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,737] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,737] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,759] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,761] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,771] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,772] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,772] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,772] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,782] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,782] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,783] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,783] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,790] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,796] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,812] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,812] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,813] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,813] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,824] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:04,826] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,826] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,827] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,827] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,837] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,840] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,840] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,841] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,841] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,843] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:04,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,850] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,850] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,851] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,855] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,855] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,867] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,869] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,880] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:04,888] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,888] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,888] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,895] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,895] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,899] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,911] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,923] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,926] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,938] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,944] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,944] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,945] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,945] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,946] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,946] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,946] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,947] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,950] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,950] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,951] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,951] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,951] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,963] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:04,964] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,966] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:04,983] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,983] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,984] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,984] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:04,989] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:04,991] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,991] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,992] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,992] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:04,996] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:04,996] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:04,996] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:04,998] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,000] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,003] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,003] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,004] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,004] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,009] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,009] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,010] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,010] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,010] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,022] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,022] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,022] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,023] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,025] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,025] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,025] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,025] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,035] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,051] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,053] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,055] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,055] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,055] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,056] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,056] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,056] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,056] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,056] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,073] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,079] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,080] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,080] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,080] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,082] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,089] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:05,098] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,098] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,099] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,099] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,100] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:05,103] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,103] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,103] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,103] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,103] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,127] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,128] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,128] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,128] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,131] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,134] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,136] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:05,141] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,141] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,142] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,150] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:05,151] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,151] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,151] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,151] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,154] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,159] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,159] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,159] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,160] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,173] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,173] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,174] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,174] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,176] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,176] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,176] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,176] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,179] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,181] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,182] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,182] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,182] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,201] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,201] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,201] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,201] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,201] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,207] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,210] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,214] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,224] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,225] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,225] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,225] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,239] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:05,241] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,241] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,242] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,242] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,243] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,248] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,248] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,249] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,249] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,251] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,253] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:05,257] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,257] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,258] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,258] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,266] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,266] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,267] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,267] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,276] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,278] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,278] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,279] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,279] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,282] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,283] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,293] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,297] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,297] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,297] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,298] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,298] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,298] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,298] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,298] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,323] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,323] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,324] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,324] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,324] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,334] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,335] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,335] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,335] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,345] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,347] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,347] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,348] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,348] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,348] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,351] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,363] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,363] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,364] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,364] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,374] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,380] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,384] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,384] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,384] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,384] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,394] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,399] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,399] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,399] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,399] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:05,407] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:05,413] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,413] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,414] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,414] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,425] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,426] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,426] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,426] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,426] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,426] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,427] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,427] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,438] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,440] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,452] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,453] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,455] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,456] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,456] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,457] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,457] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,459] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,460] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,460] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,460] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,488] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,492] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,492] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,492] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,492] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,494] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,494] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,495] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,495] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,495] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,495] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,495] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,495] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,500] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,500] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,501] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,501] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,505] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,523] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,526] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,527] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,527] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,527] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,529] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,529] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,529] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,529] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,530] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,538] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,538] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,542] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,542] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,542] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,543] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,543] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,543] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,543] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,543] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,562] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,575] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,576] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,576] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,577] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,577] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,577] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,577] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,578] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,578] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,578] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,579] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,579] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,583] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,586] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,586] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,587] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,587] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,598] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,598] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,599] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,599] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,601] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,605] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,618] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,619] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,619] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,619] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,627] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,633] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,635] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,638] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,638] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,639] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,639] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,651] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,654] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,654] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,655] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,655] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,665] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,666] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,667] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,667] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,667] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,676] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,678] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,680] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,681] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,681] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,681] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,710] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,723] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,727] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,727] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,727] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,728] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,728] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,755] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,755] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,756] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,756] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,762] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,762] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,763] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,763] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,777] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,782] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,782] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,783] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,783] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,784] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,784] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,784] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,784] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,796] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,803] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,807] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,807] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,808] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,808] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,831] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,862] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,863] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,866] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,889] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,893] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,893] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,893] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,893] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,901] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,901] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,901] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,902] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,902] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:05,927] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,929] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,929] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,930] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,930] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,935] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,943] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,943] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,944] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,944] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:05,950] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,950] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,951] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,951] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,951] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:05,951] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:05,952] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:05,952] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:05,961] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:06,008] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:06,016] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:06,020] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:06,020] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:06,021] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:06,021] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:06,035] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:06,061] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:06,062] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:06,062] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:06,062] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:06,090] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:06,093] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:06,094] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:06,094] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:06,094] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:06,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:06,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:06,129] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:06,129] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:06,129] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:06,129] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:06,143] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:06,206] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:06,218] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:06,219] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:06,219] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:06,219] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:06,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:06,287] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:06,292] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:06,293] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:06,293] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:06,293] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:06,304] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:06,355] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:06,359] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:06,359] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:06,359] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:06,359] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:06,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:06,386] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:06,389] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:06,389] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:06,390] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:06,390] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I1010 20:39:16.844753 140143741998912 spec.py:333] Evaluating on the validation split.
I1010 20:39:16.846050 140143741998912 input_pipeline.py:20] Loading split = dev-clean
I1010 20:39:16.851061 140143741998912 input_pipeline.py:20] Loading split = dev-other
[2023-10-10 20:39:26,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,376] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,463] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,463] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,463] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,463] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,474] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:26,495] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:26,497] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,497] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,498] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,498] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,499] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,504] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,511] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,563] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,586] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,599] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,610] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,623] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:26,629] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,630] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,630] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,632] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,637] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,686] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,687] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,687] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,687] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,690] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,691] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,698] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,698] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,699] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,699] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,700] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:26,712] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:26,720] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,720] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,721] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,721] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,721] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:26,723] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,724] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,724] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,724] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,731] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,734] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:26,735] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:26,736] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,736] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,737] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,737] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,744] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,754] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,754] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,754] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,754] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:26,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,759] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,759] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,759] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,766] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,769] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:26,781] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,782] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,782] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,782] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:26,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,793] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,794] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,794] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,794] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:26,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,817] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:26,819] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,820] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,820] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,820] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,827] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,829] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,829] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,830] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:26,854] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:26,860] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,861] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,861] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,866] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:26,867] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:26,870] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,870] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,870] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,870] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,872] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,872] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,873] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,877] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,882] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:26,885] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,886] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,886] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,886] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,888] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,888] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,888] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,923] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:26,930] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,931] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,931] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,933] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,942] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:26,947] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,947] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,947] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,948] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,955] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:26,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:26,979] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:26,980] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:26,980] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:26,980] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,986] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:26,993] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:26,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:27,001] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:27,002] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:27,002] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:27,007] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:27,064] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:27,108] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:27,114] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:27,114] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:27,114] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:27,175] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,094] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,137] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,137] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,138] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,138] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,144] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,158] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,220] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,221] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,221] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,221] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,241] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,277] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,283] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,283] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,283] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,284] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,302] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,319] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,320] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,320] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,320] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,326] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,343] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,344] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,344] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,344] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,363] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,401] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,401] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,401] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,401] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,404] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,404] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,404] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,404] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,406] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,406] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,406] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,406] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,459] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,459] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,460] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,460] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,463] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,464] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,465] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,465] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,465] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,478] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,484] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,502] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,503] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,503] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,503] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,507] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,514] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,517] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,517] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,518] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,518] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,520] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,536] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,537] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,537] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,538] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,538] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,543] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,545] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,549] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,550] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,551] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,551] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,551] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,557] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,557] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,558] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,558] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,559] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,559] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,560] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,560] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,571] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,572] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,576] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,576] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,577] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,577] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,580] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,587] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,587] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,588] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,588] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,591] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,591] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,592] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,592] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,592] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,593] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,595] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,596] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,596] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,597] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,597] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,598] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,599] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,601] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,604] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,612] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,616] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,616] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,617] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,617] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,617] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,617] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,618] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,618] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,624] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,629] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,632] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,635] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,635] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,636] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,636] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,636] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,639] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:29,639] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,639] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,640] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,640] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,642] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,642] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,642] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,642] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,655] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,656] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,657] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,658] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,659] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,659] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,659] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,660] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,663] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,665] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,670] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,670] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,671] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,671] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,673] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,673] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,673] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,673] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,674] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,675] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,675] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,675] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,682] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,684] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,685] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,685] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,685] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,693] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,693] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,696] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,696] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,696] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,696] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,696] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,696] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,697] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,697] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,703] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,703] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,703] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,703] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,708] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,711] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,711] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,711] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,712] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,715] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,722] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,728] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,728] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,729] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,729] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,732] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,732] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,733] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,733] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,733] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,733] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,733] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,733] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,747] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,750] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,753] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,753] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,753] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,753] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,754] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,754] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,754] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,755] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,755] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,762] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,762] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,762] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,762] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,768] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,770] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,772] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,772] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,772] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,773] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,773] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,780] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,788] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,789] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,789] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,789] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,794] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,794] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,794] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,798] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,801] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,801] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,802] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,802] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,809] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,809] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,811] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,811] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,811] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,811] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,814] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,814] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,814] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,814] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,815] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,823] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,823] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,823] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,823] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,829] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,832] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,832] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,832] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,833] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,833] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,838] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,842] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,849] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,849] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,849] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,849] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,854] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,854] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,854] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,854] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,856] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,858] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,858] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,859] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,859] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,859] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,862] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,863] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,863] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,863] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,866] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,866] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,866] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,866] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,867] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,872] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,872] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,872] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,872] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,882] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,882] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,883] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,883] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,890] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,901] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,907] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,907] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,907] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,907] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,913] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,913] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,914] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,914] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,917] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,920] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,921] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,921] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,921] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,921] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,922] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,922] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,922] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,926] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,938] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,940] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,940] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,940] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,940] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,945] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,947] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,947] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,947] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,947] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,948] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,948] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,951] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,951] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,952] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,952] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,959] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,972] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,975] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,976] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,976] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,976] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,981] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:29,984] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,984] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,984] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,984] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,986] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:29,987] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:29,987] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:29,987] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:29,992] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:29,996] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,000] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,000] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,001] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,001] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,006] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,007] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:30,009] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,009] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,010] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,010] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:30,034] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,037] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,037] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,038] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,038] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,041] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,043] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,043] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,043] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,043] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,045] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,045] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,045] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,045] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,045] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:30,050] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:30,052] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:30,053] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,056] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,056] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,056] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,056] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,062] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:30,066] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,069] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,070] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,070] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,070] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,075] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:30,080] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,082] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,082] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,083] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,083] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,092] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,093] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,095] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,095] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,095] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,095] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,095] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,096] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,096] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,096] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,102] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:30,102] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,102] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,102] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,102] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,103] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,103] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,103] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,103] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,108] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:30,108] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:30,120] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,123] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,123] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,123] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,123] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,125] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:30,127] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,127] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,127] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,127] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:30,129] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:30,129] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:30,129] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:30,129] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I1010 20:39:30.272921 140143741998912 spec.py:349] Evaluating on the test split.
I1010 20:39:30.274195 140143741998912 input_pipeline.py:20] Loading split = test-clean
[2023-10-10 20:39:35,313] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,320] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,323] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,332] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,334] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,361] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,386] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,424] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,433] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,462] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,468] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,491] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,500] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,501] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,501] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,501] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,510] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,510] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,510] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,510] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,510] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,511] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,511] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,511] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,513] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:35,519] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,520] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,520] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,520] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:35,522] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:35,526] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,526] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,526] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,526] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:35,535] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:35,537] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:35,538] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,538] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,538] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,538] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,544] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:35,544] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:35,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,546] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,546] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,546] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,546] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,547] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,547] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,547] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,550] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,550] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,551] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,551] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,554] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,554] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,556] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:35,558] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,558] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,558] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,558] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,558] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,559] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,559] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,559] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,559] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:35,562] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,562] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,562] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,562] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:35,566] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,570] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,572] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:35,578] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,579] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,579] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,579] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,585] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:35,588] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,588] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,588] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,588] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,589] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:35,594] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:35,595] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,596] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,596] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,597] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,597] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,604] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,610] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)
[2023-10-10 20:39:35,612] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,612] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,612] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,613] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,619] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:35,668] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:35,668] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:35,670] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:35,674] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,674] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,674] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,675] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,675] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,676] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,676] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,676] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,676] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:35,691] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:35,691] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,692] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,692] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,697] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,697] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,698] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:35,720] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,721] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:35,730] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,731] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,731] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,735] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,739] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,745] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:35,751] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:35,751] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:35,752] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:35,755] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,763] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,785] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:35,815] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:36,097] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:36,165] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,582] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,589] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,624] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,624] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,624] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,625] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,631] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,632] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,632] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,632] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,632] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,638] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,640] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,645] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,648] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,650] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,651] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,668] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,670] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,670] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,670] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,671] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,678] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,683] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,683] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,683] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,683] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,689] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,689] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,689] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,689] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,690] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,692] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,692] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,692] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,692] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,696] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,697] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,700] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,701] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,708] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,709] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,709] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,710] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,710] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,711] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,712] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,712] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,712] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,716] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,717] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,720] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,725] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,729] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,732] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,736] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,759] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,759] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,759] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,759] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,760] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,760] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,760] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,765] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,765] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,766] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,766] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,768] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,768] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,768] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,768] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,773] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,773] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,773] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,773] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,780] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,780] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,780] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,780] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,780] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,785] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,787] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,793] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,800] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,820] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,821] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,821] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,821] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,825] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,825] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,825] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,825] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,826] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,826] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,827] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,827] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,827] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,827] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,827] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,827] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,832] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,832] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,833] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,833] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,842] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,842] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,843] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,843] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,845] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,847] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,848] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,851] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,862] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,880] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,880] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,880] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,881] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,885] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,885] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,885] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,885] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,888] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,888] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,889] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,889] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,890] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,890] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,891] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,891] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,891] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,892] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,892] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,892] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,899] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,904] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,904] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,904] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,905] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,905] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,908] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,911] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,912] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,915] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,923] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,940] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,940] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,940] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,941] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,948] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,949] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,949] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,949] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,955] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,955] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,956] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,956] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,957] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,958] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,958] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,958] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,960] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,965] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,966] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:37,967] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:37,967] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:37,967] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:37,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:37,986] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:37,996] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,038] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,038] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,039] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,039] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,059] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,100] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,100] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,101] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,101] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,107] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,120] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,149] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,149] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,150] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,150] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,157] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:38,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,161] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,162] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,162] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,170] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>
[2023-10-10 20:39:38,181] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,189] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,223] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,223] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,223] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,223] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,229] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,230] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,230] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,230] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,243] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,251] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,282] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,282] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,283] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,283] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,290] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,291] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,291] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,291] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,301] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,311] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,321] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,321] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,321] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,321] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,331] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,331] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,331] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,331] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,339] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,349] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,351] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,354] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,354] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,354] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,354] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,355] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,355] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,355] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,355] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,363] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,375] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,376] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,376] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,376] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,376] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,381] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,381] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,382] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,382] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,387] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,391] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,391] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,392] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,392] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,396] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,399] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,401] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,402] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,402] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,402] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,409] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,409] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,410] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,410] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,410] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,410] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,413] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,413] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,413] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,413] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,418] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,418] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,419] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,419] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,430] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,439] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,442] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,442] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,442] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,442] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,445] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,447] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,448] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,449] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,449] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,449] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,451] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,451] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,451] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,451] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,455] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,455] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,456] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,456] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,457] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,459] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,461] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,461] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,464] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,465] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,465] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,465] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,465] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,471] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,473] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,474] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,475] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,475] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,475] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,480] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,480] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,480] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,480] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,481] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,483] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,484] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,484] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,484] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,492] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,498] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,500] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,500] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,500] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,500] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,500] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,507] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,509] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,510] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,510] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,510] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,511] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,511] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,512] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,513] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,513] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,513] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,514] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,514] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,514] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,514] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,514] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,515] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,515] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,516] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,516] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,518] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,522] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,522] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,522] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,525] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,525] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,525] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,525] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,543] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,543] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,543] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,543] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,544] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,547] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,547] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,547] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,548] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,548] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,549] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,551] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,551] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,552] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,552] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,552] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,552] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,553] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,553] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,555] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,560] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,567] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,569] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,570] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,571] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,571] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,571] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,571] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,572] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,572] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,573] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,573] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,574] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,574] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,574] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,574] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,582] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,596] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,598] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,599] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,599] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,599] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,599] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,599] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,599] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,599] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,603] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,607] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,607] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,608] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,608] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,610] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,613] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,613] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,614] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,614] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,614] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,621] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,632] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,634] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,634] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,635] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,635] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,635] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,637] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,637] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,638] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,638] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,644] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,661] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,663] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,664] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,664] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,664] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,673] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,673] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,673] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,673] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-10-10 20:39:38,679] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-10-10 20:39:38,696] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-10-10 20:39:38,698] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-10-10 20:39:38,698] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-10-10 20:39:38,698] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-10-10 20:39:38,698] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I1010 20:39:38.808282 140143741998912 submission_runner.py:393] Time since start: 74.55s, 	Step: 1, 	{'train/ctc_loss': 31.82299629733199, 'train/wer': 0.9550567868682394, 'validation/ctc_loss': 30.675350361663654, 'validation/wer': 0.9286921257181481, 'validation/num_examples': 5348, 'test/ctc_loss': 30.83303761533002, 'test/wer': 0.909552535900717, 'test/num_examples': 2472, 'score': 36.568456411361694, 'total_duration': 74.54756426811218, 'accumulated_submission_time': 36.568456411361694, 'accumulated_eval_time': 37.580496311187744, 'accumulated_logging_time': 0}
I1010 20:39:38.830493 140103668745984 logging_writer.py:48] [1] accumulated_eval_time=37.580496, accumulated_logging_time=0, accumulated_submission_time=36.568456, global_step=1, preemption_count=0, score=36.568456, test/ctc_loss=30.833038, test/num_examples=2472, test/wer=0.909553, total_duration=74.547564, train/ctc_loss=31.822996, train/wer=0.955057, validation/ctc_loss=30.675350, validation/num_examples=5348, validation/wer=0.928692
I1010 20:39:39.181092 140682782320448 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 20:39:39.181116 140710599534400 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 20:39:39.181097 140495175153472 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 20:39:39.182145 139672192104256 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 20:39:39.182160 140299758511936 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 20:39:39.181533 140143741998912 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 20:39:39.184690 140654889465664 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 20:39:39.185001 139627942434624 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I1010 20:39:40.137064 140103660353280 logging_writer.py:48] [1] global_step=1, grad_norm=75.940094, loss=32.442104
I1010 20:39:40.140869 140143741998912 pytorch_submission_base.py:86] 1) loss = 32.442, grad_norm = 75.940
I1010 20:39:41.219512 140103668745984 logging_writer.py:48] [2] global_step=2, grad_norm=81.290359, loss=31.509306
I1010 20:39:41.223534 140143741998912 pytorch_submission_base.py:86] 2) loss = 31.509, grad_norm = 81.290
I1010 20:39:42.139029 140103660353280 logging_writer.py:48] [3] global_step=3, grad_norm=54.232536, loss=29.932358
I1010 20:39:42.142863 140143741998912 pytorch_submission_base.py:86] 3) loss = 29.932, grad_norm = 54.233
I1010 20:39:43.063654 140103668745984 logging_writer.py:48] [4] global_step=4, grad_norm=36.464138, loss=28.269976
I1010 20:39:43.067399 140143741998912 pytorch_submission_base.py:86] 4) loss = 28.270, grad_norm = 36.464
I1010 20:39:43.977121 140103660353280 logging_writer.py:48] [5] global_step=5, grad_norm=27.758987, loss=27.347815
I1010 20:39:43.980537 140143741998912 pytorch_submission_base.py:86] 5) loss = 27.348, grad_norm = 27.759
I1010 20:39:44.897344 140103668745984 logging_writer.py:48] [6] global_step=6, grad_norm=24.630583, loss=26.501711
I1010 20:39:44.900945 140143741998912 pytorch_submission_base.py:86] 6) loss = 26.502, grad_norm = 24.631
I1010 20:39:45.813190 140103660353280 logging_writer.py:48] [7] global_step=7, grad_norm=24.116238, loss=24.898090
I1010 20:39:45.816803 140143741998912 pytorch_submission_base.py:86] 7) loss = 24.898, grad_norm = 24.116
I1010 20:39:46.741944 140103668745984 logging_writer.py:48] [8] global_step=8, grad_norm=26.059034, loss=23.915400
I1010 20:39:46.745696 140143741998912 pytorch_submission_base.py:86] 8) loss = 23.915, grad_norm = 26.059
I1010 20:39:47.652184 140103660353280 logging_writer.py:48] [9] global_step=9, grad_norm=25.207275, loss=22.554674
I1010 20:39:47.655702 140143741998912 pytorch_submission_base.py:86] 9) loss = 22.555, grad_norm = 25.207
I1010 20:39:48.577211 140103668745984 logging_writer.py:48] [10] global_step=10, grad_norm=25.649555, loss=20.815098
I1010 20:39:48.581138 140143741998912 pytorch_submission_base.py:86] 10) loss = 20.815, grad_norm = 25.650
I1010 20:39:49.491615 140103660353280 logging_writer.py:48] [11] global_step=11, grad_norm=24.556652, loss=19.314571
I1010 20:39:49.495427 140143741998912 pytorch_submission_base.py:86] 11) loss = 19.315, grad_norm = 24.557
I1010 20:39:50.405817 140103668745984 logging_writer.py:48] [12] global_step=12, grad_norm=22.341305, loss=17.761387
I1010 20:39:50.409266 140143741998912 pytorch_submission_base.py:86] 12) loss = 17.761, grad_norm = 22.341
I1010 20:39:51.322678 140103660353280 logging_writer.py:48] [13] global_step=13, grad_norm=20.129171, loss=16.108202
I1010 20:39:51.326430 140143741998912 pytorch_submission_base.py:86] 13) loss = 16.108, grad_norm = 20.129
I1010 20:39:52.235567 140103668745984 logging_writer.py:48] [14] global_step=14, grad_norm=15.656874, loss=14.580241
I1010 20:39:52.239428 140143741998912 pytorch_submission_base.py:86] 14) loss = 14.580, grad_norm = 15.657
I1010 20:39:53.141026 140103660353280 logging_writer.py:48] [15] global_step=15, grad_norm=12.507428, loss=12.984200
I1010 20:39:53.144640 140143741998912 pytorch_submission_base.py:86] 15) loss = 12.984, grad_norm = 12.507
I1010 20:39:54.054115 140103668745984 logging_writer.py:48] [16] global_step=16, grad_norm=11.952477, loss=12.315213
I1010 20:39:54.058109 140143741998912 pytorch_submission_base.py:86] 16) loss = 12.315, grad_norm = 11.952
I1010 20:39:54.972532 140103660353280 logging_writer.py:48] [17] global_step=17, grad_norm=10.423328, loss=11.273933
I1010 20:39:54.976220 140143741998912 pytorch_submission_base.py:86] 17) loss = 11.274, grad_norm = 10.423
I1010 20:39:55.883023 140103668745984 logging_writer.py:48] [18] global_step=18, grad_norm=9.120771, loss=10.312985
I1010 20:39:55.886580 140143741998912 pytorch_submission_base.py:86] 18) loss = 10.313, grad_norm = 9.121
I1010 20:39:56.794946 140103660353280 logging_writer.py:48] [19] global_step=19, grad_norm=6.568607, loss=9.923511
I1010 20:39:56.798762 140143741998912 pytorch_submission_base.py:86] 19) loss = 9.924, grad_norm = 6.569
I1010 20:39:57.713236 140103668745984 logging_writer.py:48] [20] global_step=20, grad_norm=4.797252, loss=9.084986
I1010 20:39:57.717227 140143741998912 pytorch_submission_base.py:86] 20) loss = 9.085, grad_norm = 4.797
I1010 20:39:58.629673 140103660353280 logging_writer.py:48] [21] global_step=21, grad_norm=4.713730, loss=8.926285
I1010 20:39:58.633377 140143741998912 pytorch_submission_base.py:86] 21) loss = 8.926, grad_norm = 4.714
I1010 20:39:59.542242 140103668745984 logging_writer.py:48] [22] global_step=22, grad_norm=5.580416, loss=8.599913
I1010 20:39:59.546243 140143741998912 pytorch_submission_base.py:86] 22) loss = 8.600, grad_norm = 5.580
I1010 20:40:00.462823 140103660353280 logging_writer.py:48] [23] global_step=23, grad_norm=4.497580, loss=8.052983
I1010 20:40:00.466451 140143741998912 pytorch_submission_base.py:86] 23) loss = 8.053, grad_norm = 4.498
I1010 20:40:01.380426 140103668745984 logging_writer.py:48] [24] global_step=24, grad_norm=4.273974, loss=7.929205
I1010 20:40:01.384252 140143741998912 pytorch_submission_base.py:86] 24) loss = 7.929, grad_norm = 4.274
I1010 20:40:02.298986 140103660353280 logging_writer.py:48] [25] global_step=25, grad_norm=9.202699, loss=7.732639
I1010 20:40:02.303268 140143741998912 pytorch_submission_base.py:86] 25) loss = 7.733, grad_norm = 9.203
I1010 20:40:03.221274 140103668745984 logging_writer.py:48] [26] global_step=26, grad_norm=3.956681, loss=7.597346
I1010 20:40:03.225035 140143741998912 pytorch_submission_base.py:86] 26) loss = 7.597, grad_norm = 3.957
I1010 20:40:04.142002 140103660353280 logging_writer.py:48] [27] global_step=27, grad_norm=4.575879, loss=7.716543
I1010 20:40:04.145765 140143741998912 pytorch_submission_base.py:86] 27) loss = 7.717, grad_norm = 4.576
I1010 20:40:05.063943 140103668745984 logging_writer.py:48] [28] global_step=28, grad_norm=4.992661, loss=7.410083
I1010 20:40:05.068076 140143741998912 pytorch_submission_base.py:86] 28) loss = 7.410, grad_norm = 4.993
I1010 20:40:05.974435 140103660353280 logging_writer.py:48] [29] global_step=29, grad_norm=5.801074, loss=7.385213
I1010 20:40:05.978215 140143741998912 pytorch_submission_base.py:86] 29) loss = 7.385, grad_norm = 5.801
I1010 20:40:06.890617 140103668745984 logging_writer.py:48] [30] global_step=30, grad_norm=13.536386, loss=7.234618
I1010 20:40:06.894475 140143741998912 pytorch_submission_base.py:86] 30) loss = 7.235, grad_norm = 13.536
I1010 20:40:07.801578 140103660353280 logging_writer.py:48] [31] global_step=31, grad_norm=4.794730, loss=7.249687
I1010 20:40:07.805379 140143741998912 pytorch_submission_base.py:86] 31) loss = 7.250, grad_norm = 4.795
I1010 20:40:08.715052 140103668745984 logging_writer.py:48] [32] global_step=32, grad_norm=4.480678, loss=7.141013
I1010 20:40:08.718890 140143741998912 pytorch_submission_base.py:86] 32) loss = 7.141, grad_norm = 4.481
I1010 20:40:09.625807 140103660353280 logging_writer.py:48] [33] global_step=33, grad_norm=2.742528, loss=7.007691
I1010 20:40:09.629468 140143741998912 pytorch_submission_base.py:86] 33) loss = 7.008, grad_norm = 2.743
I1010 20:40:10.542474 140103668745984 logging_writer.py:48] [34] global_step=34, grad_norm=3.045506, loss=6.948304
I1010 20:40:10.546591 140143741998912 pytorch_submission_base.py:86] 34) loss = 6.948, grad_norm = 3.046
I1010 20:40:11.456595 140103660353280 logging_writer.py:48] [35] global_step=35, grad_norm=1.822889, loss=6.872723
I1010 20:40:11.460541 140143741998912 pytorch_submission_base.py:86] 35) loss = 6.873, grad_norm = 1.823
I1010 20:40:12.370632 140103668745984 logging_writer.py:48] [36] global_step=36, grad_norm=2.282864, loss=6.767828
I1010 20:40:12.374568 140143741998912 pytorch_submission_base.py:86] 36) loss = 6.768, grad_norm = 2.283
I1010 20:40:13.275974 140103660353280 logging_writer.py:48] [37] global_step=37, grad_norm=1.572518, loss=6.728231
I1010 20:40:13.280421 140143741998912 pytorch_submission_base.py:86] 37) loss = 6.728, grad_norm = 1.573
I1010 20:40:14.189944 140103668745984 logging_writer.py:48] [38] global_step=38, grad_norm=4.393801, loss=6.767761
I1010 20:40:14.193991 140143741998912 pytorch_submission_base.py:86] 38) loss = 6.768, grad_norm = 4.394
I1010 20:40:15.098189 140103660353280 logging_writer.py:48] [39] global_step=39, grad_norm=2.943198, loss=6.678090
I1010 20:40:15.101871 140143741998912 pytorch_submission_base.py:86] 39) loss = 6.678, grad_norm = 2.943
I1010 20:40:16.023395 140103668745984 logging_writer.py:48] [40] global_step=40, grad_norm=1.308660, loss=6.602580
I1010 20:40:16.027159 140143741998912 pytorch_submission_base.py:86] 40) loss = 6.603, grad_norm = 1.309
I1010 20:40:16.933970 140103660353280 logging_writer.py:48] [41] global_step=41, grad_norm=2.295591, loss=6.441684
I1010 20:40:16.937870 140143741998912 pytorch_submission_base.py:86] 41) loss = 6.442, grad_norm = 2.296
I1010 20:40:17.850588 140103668745984 logging_writer.py:48] [42] global_step=42, grad_norm=1.314369, loss=6.343846
I1010 20:40:17.854554 140143741998912 pytorch_submission_base.py:86] 42) loss = 6.344, grad_norm = 1.314
I1010 20:40:18.763293 140103660353280 logging_writer.py:48] [43] global_step=43, grad_norm=6.972770, loss=6.325690
I1010 20:40:18.767396 140143741998912 pytorch_submission_base.py:86] 43) loss = 6.326, grad_norm = 6.973
I1010 20:40:19.678258 140103668745984 logging_writer.py:48] [44] global_step=44, grad_norm=4.803134, loss=6.333244
I1010 20:40:19.682058 140143741998912 pytorch_submission_base.py:86] 44) loss = 6.333, grad_norm = 4.803
I1010 20:40:20.588355 140103660353280 logging_writer.py:48] [45] global_step=45, grad_norm=2.150599, loss=6.174570
I1010 20:40:20.592175 140143741998912 pytorch_submission_base.py:86] 45) loss = 6.175, grad_norm = 2.151
I1010 20:40:21.513968 140103668745984 logging_writer.py:48] [46] global_step=46, grad_norm=2.918732, loss=6.148436
I1010 20:40:21.517826 140143741998912 pytorch_submission_base.py:86] 46) loss = 6.148, grad_norm = 2.919
I1010 20:40:22.424826 140103660353280 logging_writer.py:48] [47] global_step=47, grad_norm=2.078843, loss=6.051333
I1010 20:40:22.428779 140143741998912 pytorch_submission_base.py:86] 47) loss = 6.051, grad_norm = 2.079
I1010 20:40:23.334016 140103668745984 logging_writer.py:48] [48] global_step=48, grad_norm=1.807786, loss=6.023999
I1010 20:40:23.338109 140143741998912 pytorch_submission_base.py:86] 48) loss = 6.024, grad_norm = 1.808
I1010 20:40:24.244629 140103660353280 logging_writer.py:48] [49] global_step=49, grad_norm=1.368636, loss=5.978128
I1010 20:40:24.248425 140143741998912 pytorch_submission_base.py:86] 49) loss = 5.978, grad_norm = 1.369
I1010 20:40:25.153338 140103668745984 logging_writer.py:48] [50] global_step=50, grad_norm=3.001165, loss=5.969663
I1010 20:40:25.157392 140143741998912 pytorch_submission_base.py:86] 50) loss = 5.970, grad_norm = 3.001
I1010 20:40:26.067938 140103660353280 logging_writer.py:48] [51] global_step=51, grad_norm=4.873762, loss=5.997850
I1010 20:40:26.071892 140143741998912 pytorch_submission_base.py:86] 51) loss = 5.998, grad_norm = 4.874
I1010 20:40:27.013154 140103668745984 logging_writer.py:48] [52] global_step=52, grad_norm=4.762984, loss=5.974552
I1010 20:40:27.017230 140143741998912 pytorch_submission_base.py:86] 52) loss = 5.975, grad_norm = 4.763
I1010 20:40:27.920905 140103660353280 logging_writer.py:48] [53] global_step=53, grad_norm=3.495339, loss=5.958862
I1010 20:40:27.924947 140143741998912 pytorch_submission_base.py:86] 53) loss = 5.959, grad_norm = 3.495
I1010 20:40:28.831928 140103668745984 logging_writer.py:48] [54] global_step=54, grad_norm=1.637477, loss=5.930998
I1010 20:40:28.836395 140143741998912 pytorch_submission_base.py:86] 54) loss = 5.931, grad_norm = 1.637
I1010 20:40:29.742087 140103660353280 logging_writer.py:48] [55] global_step=55, grad_norm=0.901727, loss=5.915890
I1010 20:40:29.745793 140143741998912 pytorch_submission_base.py:86] 55) loss = 5.916, grad_norm = 0.902
I1010 20:40:30.659387 140103668745984 logging_writer.py:48] [56] global_step=56, grad_norm=0.379073, loss=5.869501
I1010 20:40:30.663309 140143741998912 pytorch_submission_base.py:86] 56) loss = 5.870, grad_norm = 0.379
I1010 20:40:31.575845 140103660353280 logging_writer.py:48] [57] global_step=57, grad_norm=0.298477, loss=5.888805
I1010 20:40:31.580072 140143741998912 pytorch_submission_base.py:86] 57) loss = 5.889, grad_norm = 0.298
I1010 20:40:32.499693 140103668745984 logging_writer.py:48] [58] global_step=58, grad_norm=0.839650, loss=5.881364
I1010 20:40:32.503557 140143741998912 pytorch_submission_base.py:86] 58) loss = 5.881, grad_norm = 0.840
I1010 20:40:33.408799 140103660353280 logging_writer.py:48] [59] global_step=59, grad_norm=4.133385, loss=5.913445
I1010 20:40:33.412833 140143741998912 pytorch_submission_base.py:86] 59) loss = 5.913, grad_norm = 4.133
I1010 20:40:34.316380 140103668745984 logging_writer.py:48] [60] global_step=60, grad_norm=7.136105, loss=6.013340
I1010 20:40:34.320225 140143741998912 pytorch_submission_base.py:86] 60) loss = 6.013, grad_norm = 7.136
I1010 20:40:35.230747 140103660353280 logging_writer.py:48] [61] global_step=61, grad_norm=2.140578, loss=5.887222
I1010 20:40:35.234709 140143741998912 pytorch_submission_base.py:86] 61) loss = 5.887, grad_norm = 2.141
I1010 20:40:36.144535 140103668745984 logging_writer.py:48] [62] global_step=62, grad_norm=1.093541, loss=5.869381
I1010 20:40:36.148461 140143741998912 pytorch_submission_base.py:86] 62) loss = 5.869, grad_norm = 1.094
I1010 20:40:37.055564 140103660353280 logging_writer.py:48] [63] global_step=63, grad_norm=0.716862, loss=5.850725
I1010 20:40:37.059784 140143741998912 pytorch_submission_base.py:86] 63) loss = 5.851, grad_norm = 0.717
I1010 20:40:37.979322 140103668745984 logging_writer.py:48] [64] global_step=64, grad_norm=1.638189, loss=5.850024
I1010 20:40:37.983291 140143741998912 pytorch_submission_base.py:86] 64) loss = 5.850, grad_norm = 1.638
I1010 20:40:38.899677 140103660353280 logging_writer.py:48] [65] global_step=65, grad_norm=3.171368, loss=5.901682
I1010 20:40:38.903709 140143741998912 pytorch_submission_base.py:86] 65) loss = 5.902, grad_norm = 3.171
I1010 20:40:39.808647 140103668745984 logging_writer.py:48] [66] global_step=66, grad_norm=5.878136, loss=5.902630
I1010 20:40:39.812306 140143741998912 pytorch_submission_base.py:86] 66) loss = 5.903, grad_norm = 5.878
I1010 20:40:40.718721 140103660353280 logging_writer.py:48] [67] global_step=67, grad_norm=5.238908, loss=5.900654
I1010 20:40:40.722585 140143741998912 pytorch_submission_base.py:86] 67) loss = 5.901, grad_norm = 5.239
I1010 20:40:41.630686 140103668745984 logging_writer.py:48] [68] global_step=68, grad_norm=1.307782, loss=5.846791
I1010 20:40:41.634769 140143741998912 pytorch_submission_base.py:86] 68) loss = 5.847, grad_norm = 1.308
I1010 20:40:42.551507 140103660353280 logging_writer.py:48] [69] global_step=69, grad_norm=0.465973, loss=5.853894
I1010 20:40:42.555227 140143741998912 pytorch_submission_base.py:86] 69) loss = 5.854, grad_norm = 0.466
I1010 20:40:43.475065 140103668745984 logging_writer.py:48] [70] global_step=70, grad_norm=0.769048, loss=5.822262
I1010 20:40:43.478895 140143741998912 pytorch_submission_base.py:86] 70) loss = 5.822, grad_norm = 0.769
I1010 20:40:44.383262 140103660353280 logging_writer.py:48] [71] global_step=71, grad_norm=2.365644, loss=5.827641
I1010 20:40:44.386967 140143741998912 pytorch_submission_base.py:86] 71) loss = 5.828, grad_norm = 2.366
I1010 20:40:45.294950 140103668745984 logging_writer.py:48] [72] global_step=72, grad_norm=5.151810, loss=5.874950
I1010 20:40:45.299045 140143741998912 pytorch_submission_base.py:86] 72) loss = 5.875, grad_norm = 5.152
I1010 20:40:46.210777 140103660353280 logging_writer.py:48] [73] global_step=73, grad_norm=6.116843, loss=5.889058
I1010 20:40:46.214604 140143741998912 pytorch_submission_base.py:86] 73) loss = 5.889, grad_norm = 6.117
I1010 20:40:47.122011 140103668745984 logging_writer.py:48] [74] global_step=74, grad_norm=3.409681, loss=5.849051
I1010 20:40:47.125975 140143741998912 pytorch_submission_base.py:86] 74) loss = 5.849, grad_norm = 3.410
I1010 20:40:48.034750 140103660353280 logging_writer.py:48] [75] global_step=75, grad_norm=2.561369, loss=5.844296
I1010 20:40:48.038688 140143741998912 pytorch_submission_base.py:86] 75) loss = 5.844, grad_norm = 2.561
I1010 20:40:48.964495 140103668745984 logging_writer.py:48] [76] global_step=76, grad_norm=3.761335, loss=5.844747
I1010 20:40:48.968567 140143741998912 pytorch_submission_base.py:86] 76) loss = 5.845, grad_norm = 3.761
I1010 20:40:49.876339 140103660353280 logging_writer.py:48] [77] global_step=77, grad_norm=2.996383, loss=5.836581
I1010 20:40:49.880056 140143741998912 pytorch_submission_base.py:86] 77) loss = 5.837, grad_norm = 2.996
I1010 20:40:50.792943 140103668745984 logging_writer.py:48] [78] global_step=78, grad_norm=2.281579, loss=5.847720
I1010 20:40:50.796912 140143741998912 pytorch_submission_base.py:86] 78) loss = 5.848, grad_norm = 2.282
I1010 20:40:51.702775 140103660353280 logging_writer.py:48] [79] global_step=79, grad_norm=2.378816, loss=5.830383
I1010 20:40:51.706814 140143741998912 pytorch_submission_base.py:86] 79) loss = 5.830, grad_norm = 2.379
I1010 20:40:52.610375 140103668745984 logging_writer.py:48] [80] global_step=80, grad_norm=2.919550, loss=5.828769
I1010 20:40:52.614015 140143741998912 pytorch_submission_base.py:86] 80) loss = 5.829, grad_norm = 2.920
I1010 20:40:53.518156 140103660353280 logging_writer.py:48] [81] global_step=81, grad_norm=4.451727, loss=5.843149
I1010 20:40:53.521763 140143741998912 pytorch_submission_base.py:86] 81) loss = 5.843, grad_norm = 4.452
I1010 20:40:54.446299 140103668745984 logging_writer.py:48] [82] global_step=82, grad_norm=4.668613, loss=5.852254
I1010 20:40:54.450327 140143741998912 pytorch_submission_base.py:86] 82) loss = 5.852, grad_norm = 4.669
I1010 20:40:55.357872 140103660353280 logging_writer.py:48] [83] global_step=83, grad_norm=3.078177, loss=5.838163
I1010 20:40:55.361740 140143741998912 pytorch_submission_base.py:86] 83) loss = 5.838, grad_norm = 3.078
I1010 20:40:56.268995 140103668745984 logging_writer.py:48] [84] global_step=84, grad_norm=2.390045, loss=5.824738
I1010 20:40:56.273113 140143741998912 pytorch_submission_base.py:86] 84) loss = 5.825, grad_norm = 2.390
I1010 20:40:57.187078 140103660353280 logging_writer.py:48] [85] global_step=85, grad_norm=3.214585, loss=5.805465
I1010 20:40:57.190725 140143741998912 pytorch_submission_base.py:86] 85) loss = 5.805, grad_norm = 3.215
I1010 20:40:58.101398 140103668745984 logging_writer.py:48] [86] global_step=86, grad_norm=4.068985, loss=5.826951
I1010 20:40:58.105011 140143741998912 pytorch_submission_base.py:86] 86) loss = 5.827, grad_norm = 4.069
I1010 20:40:59.014343 140103660353280 logging_writer.py:48] [87] global_step=87, grad_norm=3.078228, loss=5.804062
I1010 20:40:59.018350 140143741998912 pytorch_submission_base.py:86] 87) loss = 5.804, grad_norm = 3.078
I1010 20:40:59.932726 140103668745984 logging_writer.py:48] [88] global_step=88, grad_norm=1.842900, loss=5.807948
I1010 20:40:59.937105 140143741998912 pytorch_submission_base.py:86] 88) loss = 5.808, grad_norm = 1.843
I1010 20:41:00.842349 140103660353280 logging_writer.py:48] [89] global_step=89, grad_norm=2.271585, loss=5.823135
I1010 20:41:00.846229 140143741998912 pytorch_submission_base.py:86] 89) loss = 5.823, grad_norm = 2.272
I1010 20:41:01.760631 140103668745984 logging_writer.py:48] [90] global_step=90, grad_norm=3.485673, loss=5.809422
I1010 20:41:01.765609 140143741998912 pytorch_submission_base.py:86] 90) loss = 5.809, grad_norm = 3.486
I1010 20:41:02.690750 140103660353280 logging_writer.py:48] [91] global_step=91, grad_norm=4.854313, loss=5.833488
I1010 20:41:02.694087 140143741998912 pytorch_submission_base.py:86] 91) loss = 5.833, grad_norm = 4.854
I1010 20:41:03.601470 140103668745984 logging_writer.py:48] [92] global_step=92, grad_norm=4.343073, loss=5.819748
I1010 20:41:03.604606 140143741998912 pytorch_submission_base.py:86] 92) loss = 5.820, grad_norm = 4.343
I1010 20:41:04.508772 140103660353280 logging_writer.py:48] [93] global_step=93, grad_norm=2.253126, loss=5.792234
I1010 20:41:04.512285 140143741998912 pytorch_submission_base.py:86] 93) loss = 5.792, grad_norm = 2.253
I1010 20:41:05.427583 140103668745984 logging_writer.py:48] [94] global_step=94, grad_norm=1.838226, loss=5.788882
I1010 20:41:05.431134 140143741998912 pytorch_submission_base.py:86] 94) loss = 5.789, grad_norm = 1.838
I1010 20:41:06.339377 140103660353280 logging_writer.py:48] [95] global_step=95, grad_norm=2.722923, loss=5.790884
I1010 20:41:06.342841 140143741998912 pytorch_submission_base.py:86] 95) loss = 5.791, grad_norm = 2.723
I1010 20:41:07.252670 140103668745984 logging_writer.py:48] [96] global_step=96, grad_norm=3.244997, loss=5.802752
I1010 20:41:07.255905 140143741998912 pytorch_submission_base.py:86] 96) loss = 5.803, grad_norm = 3.245
I1010 20:41:08.158397 140103660353280 logging_writer.py:48] [97] global_step=97, grad_norm=3.537505, loss=5.788981
I1010 20:41:08.161652 140143741998912 pytorch_submission_base.py:86] 97) loss = 5.789, grad_norm = 3.538
I1010 20:41:09.067613 140103668745984 logging_writer.py:48] [98] global_step=98, grad_norm=3.270662, loss=5.793993
I1010 20:41:09.070906 140143741998912 pytorch_submission_base.py:86] 98) loss = 5.794, grad_norm = 3.271
I1010 20:41:09.968427 140103660353280 logging_writer.py:48] [99] global_step=99, grad_norm=4.629505, loss=5.807605
I1010 20:41:09.971961 140143741998912 pytorch_submission_base.py:86] 99) loss = 5.808, grad_norm = 4.630
I1010 20:41:10.886678 140103668745984 logging_writer.py:48] [100] global_step=100, grad_norm=3.946048, loss=5.808663
I1010 20:41:10.890163 140143741998912 pytorch_submission_base.py:86] 100) loss = 5.809, grad_norm = 3.946
I1010 20:47:09.902317 140103660353280 logging_writer.py:48] [500] global_step=500, grad_norm=2.353987, loss=2.918670
I1010 20:47:09.906669 140143741998912 pytorch_submission_base.py:86] 500) loss = 2.919, grad_norm = 2.354
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I1010 20:54:38.429335 140103668745984 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.445954, loss=2.478714
I1010 20:54:38.435802 140143741998912 pytorch_submission_base.py:86] 1000) loss = 2.479, grad_norm = 2.446
I1010 21:02:09.839566 140103668745984 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.944243, loss=2.255959
I1010 21:02:09.848100 140143741998912 pytorch_submission_base.py:86] 1500) loss = 2.256, grad_norm = 2.944
I1010 21:03:39.445644 140143741998912 spec.py:321] Evaluating on the training split.
I1010 21:03:52.535008 140143741998912 spec.py:333] Evaluating on the validation split.
I1010 21:04:03.342800 140143741998912 spec.py:349] Evaluating on the test split.
I1010 21:04:09.130702 140143741998912 submission_runner.py:393] Time since start: 1544.87s, 	Step: 1600, 	{'train/ctc_loss': 1.111247864817242, 'train/wer': 0.32832103221344516, 'validation/ctc_loss': 1.3705229603049025, 'validation/wer': 0.36369429826678895, 'validation/num_examples': 5348, 'test/ctc_loss': 0.9449809261887864, 'test/wer': 0.28830256129019155, 'test/num_examples': 2472, 'score': 1475.5464942455292, 'total_duration': 1544.8705775737762, 'accumulated_submission_time': 1475.5464942455292, 'accumulated_eval_time': 67.26539826393127, 'accumulated_logging_time': 0.03159737586975098}
I1010 21:04:09.162214 140103668745984 logging_writer.py:48] [1600] accumulated_eval_time=67.265398, accumulated_logging_time=0.031597, accumulated_submission_time=1475.546494, global_step=1600, preemption_count=0, score=1475.546494, test/ctc_loss=0.944981, test/num_examples=2472, test/wer=0.288303, total_duration=1544.870578, train/ctc_loss=1.111248, train/wer=0.328321, validation/ctc_loss=1.370523, validation/num_examples=5348, validation/wer=0.363694
I1010 21:10:09.817102 140103660353280 logging_writer.py:48] [2000] global_step=2000, grad_norm=6.141758, loss=2.148835
I1010 21:10:09.822567 140143741998912 pytorch_submission_base.py:86] 2000) loss = 2.149, grad_norm = 6.142
I1010 21:17:40.514033 140103668745984 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.777232, loss=2.053704
I1010 21:17:40.526338 140143741998912 pytorch_submission_base.py:86] 2500) loss = 2.054, grad_norm = 4.777
I1010 21:25:10.218502 140103660353280 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.751025, loss=2.052542
I1010 21:25:10.224973 140143741998912 pytorch_submission_base.py:86] 3000) loss = 2.053, grad_norm = 2.751
I1010 21:28:10.296146 140143741998912 spec.py:321] Evaluating on the training split.
I1010 21:28:23.500493 140143741998912 spec.py:333] Evaluating on the validation split.
I1010 21:28:34.458738 140143741998912 spec.py:349] Evaluating on the test split.
I1010 21:28:40.440320 140143741998912 submission_runner.py:393] Time since start: 3016.18s, 	Step: 3199, 	{'train/ctc_loss': 0.7799915792940336, 'train/wer': 0.24180075427657527, 'validation/ctc_loss': 1.0105672617477899, 'validation/wer': 0.280094626563028, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6630146501387186, 'test/wer': 0.20876241545304977, 'test/num_examples': 2472, 'score': 2914.8431997299194, 'total_duration': 3016.180022716522, 'accumulated_submission_time': 2914.8431997299194, 'accumulated_eval_time': 97.40930008888245, 'accumulated_logging_time': 0.07384705543518066}
I1010 21:28:40.472185 140115070408448 logging_writer.py:48] [3199] accumulated_eval_time=97.409300, accumulated_logging_time=0.073847, accumulated_submission_time=2914.843200, global_step=3199, preemption_count=0, score=2914.843200, test/ctc_loss=0.663015, test/num_examples=2472, test/wer=0.208762, total_duration=3016.180023, train/ctc_loss=0.779992, train/wer=0.241801, validation/ctc_loss=1.010567, validation/num_examples=5348, validation/wer=0.280095
I1010 21:33:12.169781 140115062015744 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.977333, loss=1.961209
I1010 21:33:12.173827 140143741998912 pytorch_submission_base.py:86] 3500) loss = 1.961, grad_norm = 1.977
I1010 21:40:40.523636 140115070408448 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.263451, loss=1.978127
I1010 21:40:40.530412 140143741998912 pytorch_submission_base.py:86] 4000) loss = 1.978, grad_norm = 3.263
I1010 21:48:10.785745 140115070408448 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.769499, loss=1.930385
I1010 21:48:10.797723 140143741998912 pytorch_submission_base.py:86] 4500) loss = 1.930, grad_norm = 2.769
I1010 21:52:41.639842 140143741998912 spec.py:321] Evaluating on the training split.
I1010 21:52:54.858046 140143741998912 spec.py:333] Evaluating on the validation split.
I1010 21:53:05.805029 140143741998912 spec.py:349] Evaluating on the test split.
I1010 21:53:11.639423 140143741998912 submission_runner.py:393] Time since start: 4487.38s, 	Step: 4802, 	{'train/ctc_loss': 0.712790123747949, 'train/wer': 0.22448373118360906, 'validation/ctc_loss': 0.9383341103450874, 'validation/wer': 0.2645391782938251, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6078709211078134, 'test/wer': 0.19190380435886498, 'test/num_examples': 2472, 'score': 4354.127371549606, 'total_duration': 4487.379282951355, 'accumulated_submission_time': 4354.127371549606, 'accumulated_eval_time': 127.4086799621582, 'accumulated_logging_time': 0.11646175384521484}
I1010 21:53:11.674206 140115053623040 logging_writer.py:48] [4802] accumulated_eval_time=127.408680, accumulated_logging_time=0.116462, accumulated_submission_time=4354.127372, global_step=4802, preemption_count=0, score=4354.127372, test/ctc_loss=0.607871, test/num_examples=2472, test/wer=0.191904, total_duration=4487.379283, train/ctc_loss=0.712790, train/wer=0.224484, validation/ctc_loss=0.938334, validation/num_examples=5348, validation/wer=0.264539
I1010 21:56:11.255505 140115045230336 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.559115, loss=1.962892
I1010 21:56:11.262636 140143741998912 pytorch_submission_base.py:86] 5000) loss = 1.963, grad_norm = 1.559
I1010 22:03:41.598713 140115053623040 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.517941, loss=1.960210
I1010 22:03:41.606034 140143741998912 pytorch_submission_base.py:86] 5500) loss = 1.960, grad_norm = 2.518
I1010 22:11:10.722493 140115045230336 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.410824, loss=2.106170
I1010 22:11:10.730945 140143741998912 pytorch_submission_base.py:86] 6000) loss = 2.106, grad_norm = 2.411
I1010 22:17:12.627290 140143741998912 spec.py:321] Evaluating on the training split.
I1010 22:17:25.978373 140143741998912 spec.py:333] Evaluating on the validation split.
I1010 22:17:37.225716 140143741998912 spec.py:349] Evaluating on the test split.
I1010 22:17:43.026946 140143741998912 submission_runner.py:393] Time since start: 5958.77s, 	Step: 6401, 	{'train/ctc_loss': 0.6474165236192485, 'train/wer': 0.20469747890078777, 'validation/ctc_loss': 0.8625625141274864, 'validation/wer': 0.24497658475353642, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5506018533453771, 'test/wer': 0.17451709219425995, 'test/num_examples': 2472, 'score': 5793.230083703995, 'total_duration': 5958.766754388809, 'accumulated_submission_time': 5793.230083703995, 'accumulated_eval_time': 157.8081395626068, 'accumulated_logging_time': 0.16199016571044922}
I1010 22:17:43.059081 140115028444928 logging_writer.py:48] [6401] accumulated_eval_time=157.808140, accumulated_logging_time=0.161990, accumulated_submission_time=5793.230084, global_step=6401, preemption_count=0, score=5793.230084, test/ctc_loss=0.550602, test/num_examples=2472, test/wer=0.174517, total_duration=5958.766754, train/ctc_loss=0.647417, train/wer=0.204697, validation/ctc_loss=0.862563, validation/num_examples=5348, validation/wer=0.244977
I1010 22:19:13.485213 140115020052224 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.579019, loss=1.833950
I1010 22:19:13.489864 140143741998912 pytorch_submission_base.py:86] 6500) loss = 1.834, grad_norm = 3.579
I1010 22:26:41.773927 140115028444928 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.189996, loss=1.882448
I1010 22:26:41.778976 140143741998912 pytorch_submission_base.py:86] 7000) loss = 1.882, grad_norm = 3.190
I1010 22:34:12.012483 140115028444928 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.204120, loss=1.867124
I1010 22:34:12.020180 140143741998912 pytorch_submission_base.py:86] 7500) loss = 1.867, grad_norm = 3.204
I1010 22:41:41.679296 140115020052224 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.064468, loss=1.823425
I1010 22:41:41.684342 140143741998912 pytorch_submission_base.py:86] 8000) loss = 1.823, grad_norm = 3.064
I1010 22:41:44.100335 140143741998912 spec.py:321] Evaluating on the training split.
I1010 22:41:57.576551 140143741998912 spec.py:333] Evaluating on the validation split.
I1010 22:42:08.743336 140143741998912 spec.py:349] Evaluating on the test split.
I1010 22:42:14.766555 140143741998912 submission_runner.py:393] Time since start: 7430.51s, 	Step: 8003, 	{'train/ctc_loss': 0.685541433074023, 'train/wer': 0.21437447994899447, 'validation/ctc_loss': 0.9164249296765119, 'validation/wer': 0.2558103606430744, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5668348562810503, 'test/wer': 0.17762476387788678, 'test/num_examples': 2472, 'score': 7232.417771577835, 'total_duration': 7430.5063552856445, 'accumulated_submission_time': 7232.417771577835, 'accumulated_eval_time': 188.47413563728333, 'accumulated_logging_time': 0.20534777641296387}
I1010 22:42:14.796490 140115020052224 logging_writer.py:48] [8003] accumulated_eval_time=188.474136, accumulated_logging_time=0.205348, accumulated_submission_time=7232.417772, global_step=8003, preemption_count=0, score=7232.417772, test/ctc_loss=0.566835, test/num_examples=2472, test/wer=0.177625, total_duration=7430.506355, train/ctc_loss=0.685541, train/wer=0.214374, validation/ctc_loss=0.916425, validation/num_examples=5348, validation/wer=0.255810
I1010 22:49:44.417193 140115020052224 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.430128, loss=1.785794
I1010 22:49:44.424621 140143741998912 pytorch_submission_base.py:86] 8500) loss = 1.786, grad_norm = 2.430
I1010 22:57:13.738990 140115011659520 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.616808, loss=1.877599
I1010 22:57:13.744461 140143741998912 pytorch_submission_base.py:86] 9000) loss = 1.878, grad_norm = 3.617
I1010 23:04:45.244338 140115020052224 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.331411, loss=1.838112
I1010 23:04:45.252139 140143741998912 pytorch_submission_base.py:86] 9500) loss = 1.838, grad_norm = 3.331
I1010 23:06:15.411797 140143741998912 spec.py:321] Evaluating on the training split.
I1010 23:06:28.408431 140143741998912 spec.py:333] Evaluating on the validation split.
I1010 23:06:40.071255 140143741998912 spec.py:349] Evaluating on the test split.
I1010 23:06:45.870050 140143741998912 submission_runner.py:393] Time since start: 8901.61s, 	Step: 9601, 	{'train/ctc_loss': 0.5908270211378882, 'train/wer': 0.18743448708112256, 'validation/ctc_loss': 0.8053476166145519, 'validation/wer': 0.22934389031043306, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5099174745144848, 'test/wer': 0.16302073812280382, 'test/num_examples': 2472, 'score': 8671.24452161789, 'total_duration': 8901.609928369522, 'accumulated_submission_time': 8671.24452161789, 'accumulated_eval_time': 218.9322865009308, 'accumulated_logging_time': 0.24665021896362305}
I1010 23:06:45.893227 140114994874112 logging_writer.py:48] [9601] accumulated_eval_time=218.932287, accumulated_logging_time=0.246650, accumulated_submission_time=8671.244522, global_step=9601, preemption_count=0, score=8671.244522, test/ctc_loss=0.509917, test/num_examples=2472, test/wer=0.163021, total_duration=8901.609928, train/ctc_loss=0.590827, train/wer=0.187434, validation/ctc_loss=0.805348, validation/num_examples=5348, validation/wer=0.229344
I1010 23:12:47.435986 140114986481408 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.197378, loss=1.838775
I1010 23:12:47.440076 140143741998912 pytorch_submission_base.py:86] 10000) loss = 1.839, grad_norm = 2.197
I1010 23:20:18.195001 140114994874112 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.433774, loss=1.835915
I1010 23:20:18.207068 140143741998912 pytorch_submission_base.py:86] 10500) loss = 1.836, grad_norm = 2.434
I1010 23:27:47.737976 140114986481408 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.712847, loss=1.743349
I1010 23:27:47.746162 140143741998912 pytorch_submission_base.py:86] 11000) loss = 1.743, grad_norm = 2.713
I1010 23:30:47.143872 140143741998912 spec.py:321] Evaluating on the training split.
I1010 23:31:00.431639 140143741998912 spec.py:333] Evaluating on the validation split.
I1010 23:31:11.417375 140143741998912 spec.py:349] Evaluating on the test split.
I1010 23:31:17.364859 140143741998912 submission_runner.py:393] Time since start: 10373.10s, 	Step: 11200, 	{'train/ctc_loss': 0.5743054654782641, 'train/wer': 0.18089670301170319, 'validation/ctc_loss': 0.8010115084074242, 'validation/wer': 0.22565538550668662, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4959669273017614, 'test/wer': 0.1547539252127638, 'test/num_examples': 2472, 'score': 10110.585166454315, 'total_duration': 10373.104720592499, 'accumulated_submission_time': 10110.585166454315, 'accumulated_eval_time': 249.1530773639679, 'accumulated_logging_time': 0.28049254417419434}
I1010 23:31:17.399047 140114986481408 logging_writer.py:48] [11200] accumulated_eval_time=249.153077, accumulated_logging_time=0.280493, accumulated_submission_time=10110.585166, global_step=11200, preemption_count=0, score=10110.585166, test/ctc_loss=0.495967, test/num_examples=2472, test/wer=0.154754, total_duration=10373.104721, train/ctc_loss=0.574305, train/wer=0.180897, validation/ctc_loss=0.801012, validation/num_examples=5348, validation/wer=0.225655
I1010 23:35:50.099015 140114986481408 logging_writer.py:48] [11500] global_step=11500, grad_norm=4.871621, loss=1.841334
I1010 23:35:50.105504 140143741998912 pytorch_submission_base.py:86] 11500) loss = 1.841, grad_norm = 4.872
I1010 23:43:19.427307 140114978088704 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.537863, loss=1.778953
I1010 23:43:19.434429 140143741998912 pytorch_submission_base.py:86] 12000) loss = 1.779, grad_norm = 2.538
I1010 23:50:50.018226 140114986481408 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.660954, loss=1.746418
I1010 23:50:50.025030 140143741998912 pytorch_submission_base.py:86] 12500) loss = 1.746, grad_norm = 2.661
I1010 23:55:18.117122 140143741998912 spec.py:321] Evaluating on the training split.
I1010 23:55:31.478317 140143741998912 spec.py:333] Evaluating on the validation split.
I1010 23:55:42.255037 140143741998912 spec.py:349] Evaluating on the test split.
I1010 23:55:48.129505 140143741998912 submission_runner.py:393] Time since start: 11843.87s, 	Step: 12799, 	{'train/ctc_loss': 0.5457353409385708, 'train/wer': 0.17277580263456488, 'validation/ctc_loss': 0.7661650820336046, 'validation/wer': 0.21578718679090425, 'validation/num_examples': 5348, 'test/ctc_loss': 0.46752154372862764, 'test/wer': 0.14717770601019642, 'test/num_examples': 2472, 'score': 11549.5086414814, 'total_duration': 11843.869337320328, 'accumulated_submission_time': 11549.5086414814, 'accumulated_eval_time': 279.1652705669403, 'accumulated_logging_time': 0.32468652725219727}
I1010 23:55:48.152720 140114986481408 logging_writer.py:48] [12799] accumulated_eval_time=279.165271, accumulated_logging_time=0.324687, accumulated_submission_time=11549.508641, global_step=12799, preemption_count=0, score=11549.508641, test/ctc_loss=0.467522, test/num_examples=2472, test/wer=0.147178, total_duration=11843.869337, train/ctc_loss=0.545735, train/wer=0.172776, validation/ctc_loss=0.766165, validation/num_examples=5348, validation/wer=0.215787
I1010 23:58:50.192849 140114978088704 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.008822, loss=1.792728
I1010 23:58:50.197427 140143741998912 pytorch_submission_base.py:86] 13000) loss = 1.793, grad_norm = 4.009
I1011 00:06:20.269037 140114986481408 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.886781, loss=1.730180
I1011 00:06:20.276515 140143741998912 pytorch_submission_base.py:86] 13500) loss = 1.730, grad_norm = 2.887
I1011 00:13:48.855484 140114978088704 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.496849, loss=1.667717
I1011 00:13:48.864335 140143741998912 pytorch_submission_base.py:86] 14000) loss = 1.668, grad_norm = 2.497
I1011 00:19:49.367088 140143741998912 spec.py:321] Evaluating on the training split.
I1011 00:20:02.671880 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 00:20:13.584795 140143741998912 spec.py:349] Evaluating on the test split.
I1011 00:20:20.052428 140143741998912 submission_runner.py:393] Time since start: 13315.79s, 	Step: 14401, 	{'train/ctc_loss': 0.5180211319921971, 'train/wer': 0.16689179697208745, 'validation/ctc_loss': 0.7325320988264767, 'validation/wer': 0.20996475643315793, 'validation/num_examples': 5348, 'test/ctc_loss': 0.45102763686366865, 'test/wer': 0.14575589543598805, 'test/num_examples': 2472, 'score': 12988.80303144455, 'total_duration': 13315.792248010635, 'accumulated_submission_time': 12988.80303144455, 'accumulated_eval_time': 309.85040044784546, 'accumulated_logging_time': 0.3591785430908203}
I1011 00:20:20.085305 140114986481408 logging_writer.py:48] [14401] accumulated_eval_time=309.850400, accumulated_logging_time=0.359179, accumulated_submission_time=12988.803031, global_step=14401, preemption_count=0, score=12988.803031, test/ctc_loss=0.451028, test/num_examples=2472, test/wer=0.145756, total_duration=13315.792248, train/ctc_loss=0.518021, train/wer=0.166892, validation/ctc_loss=0.732532, validation/num_examples=5348, validation/wer=0.209965
I1011 00:21:52.382042 140114986481408 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.545496, loss=1.675717
I1011 00:21:52.389763 140143741998912 pytorch_submission_base.py:86] 14500) loss = 1.676, grad_norm = 2.545
I1011 00:29:21.441048 140114978088704 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.986594, loss=1.720357
I1011 00:29:21.449252 140143741998912 pytorch_submission_base.py:86] 15000) loss = 1.720, grad_norm = 2.987
I1011 00:36:51.748912 140114986481408 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.060359, loss=1.597398
I1011 00:36:51.759508 140143741998912 pytorch_submission_base.py:86] 15500) loss = 1.597, grad_norm = 3.060
I1011 00:44:20.872095 140143741998912 spec.py:321] Evaluating on the training split.
I1011 00:44:34.116286 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 00:44:45.045668 140143741998912 spec.py:349] Evaluating on the test split.
I1011 00:44:51.053240 140143741998912 submission_runner.py:393] Time since start: 14786.79s, 	Step: 16000, 	{'train/ctc_loss': 0.4884862796609313, 'train/wer': 0.15645295497033684, 'validation/ctc_loss': 0.7110507651603627, 'validation/wer': 0.20221117172790035, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4264211098296664, 'test/wer': 0.13456421505900515, 'test/num_examples': 2472, 'score': 14427.66909623146, 'total_duration': 14786.793115615845, 'accumulated_submission_time': 14427.66909623146, 'accumulated_eval_time': 340.0313973426819, 'accumulated_logging_time': 0.40206003189086914}
I1011 00:44:51.084966 140114986481408 logging_writer.py:48] [16000] accumulated_eval_time=340.031397, accumulated_logging_time=0.402060, accumulated_submission_time=14427.669096, global_step=16000, preemption_count=0, score=14427.669096, test/ctc_loss=0.426421, test/num_examples=2472, test/wer=0.134564, total_duration=14786.793116, train/ctc_loss=0.488486, train/wer=0.156453, validation/ctc_loss=0.711051, validation/num_examples=5348, validation/wer=0.202211
I1011 00:44:52.696563 140114978088704 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.176144, loss=1.661757
I1011 00:44:52.700403 140143741998912 pytorch_submission_base.py:86] 16000) loss = 1.662, grad_norm = 3.176
I1011 00:52:25.174403 140114986481408 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.539199, loss=1.591793
I1011 00:52:25.183418 140143741998912 pytorch_submission_base.py:86] 16500) loss = 1.592, grad_norm = 2.539
I1011 00:59:53.690997 140114978088704 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.238747, loss=1.607219
I1011 00:59:53.724377 140143741998912 pytorch_submission_base.py:86] 17000) loss = 1.607, grad_norm = 3.239
I1011 01:07:23.242251 140114986481408 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.084293, loss=1.625877
I1011 01:07:23.247608 140143741998912 pytorch_submission_base.py:86] 17500) loss = 1.626, grad_norm = 2.084
I1011 01:08:51.793855 140143741998912 spec.py:321] Evaluating on the training split.
I1011 01:09:05.304417 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 01:09:16.878206 140143741998912 spec.py:349] Evaluating on the test split.
I1011 01:09:22.709891 140143741998912 submission_runner.py:393] Time since start: 16258.45s, 	Step: 17597, 	{'train/ctc_loss': 0.46204370470633027, 'train/wer': 0.14951533947849016, 'validation/ctc_loss': 0.6852441151170384, 'validation/wer': 0.19551972191377395, 'validation/num_examples': 5348, 'test/ctc_loss': 0.40804847792599525, 'test/wer': 0.12765827798427884, 'test/num_examples': 2472, 'score': 15866.438121795654, 'total_duration': 16258.44973039627, 'accumulated_submission_time': 15866.438121795654, 'accumulated_eval_time': 370.9473388195038, 'accumulated_logging_time': 0.4441380500793457}
I1011 01:09:22.744360 140114986481408 logging_writer.py:48] [17597] accumulated_eval_time=370.947339, accumulated_logging_time=0.444138, accumulated_submission_time=15866.438122, global_step=17597, preemption_count=0, score=15866.438122, test/ctc_loss=0.408048, test/num_examples=2472, test/wer=0.127658, total_duration=16258.449730, train/ctc_loss=0.462044, train/wer=0.149515, validation/ctc_loss=0.685244, validation/num_examples=5348, validation/wer=0.195520
I1011 01:15:27.055852 140114978088704 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.961353, loss=1.657720
I1011 01:15:27.063363 140143741998912 pytorch_submission_base.py:86] 18000) loss = 1.658, grad_norm = 2.961
I1011 01:22:56.323308 140114986481408 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.926568, loss=1.603374
I1011 01:22:56.331271 140143741998912 pytorch_submission_base.py:86] 18500) loss = 1.603, grad_norm = 1.927
I1011 01:30:26.654891 140114986481408 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.484118, loss=1.605631
I1011 01:30:26.662117 140143741998912 pytorch_submission_base.py:86] 19000) loss = 1.606, grad_norm = 3.484
I1011 01:33:23.392624 140143741998912 spec.py:321] Evaluating on the training split.
I1011 01:33:36.605900 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 01:33:47.500608 140143741998912 spec.py:349] Evaluating on the test split.
I1011 01:33:53.588821 140143741998912 submission_runner.py:393] Time since start: 17729.33s, 	Step: 19197, 	{'train/ctc_loss': 0.4293711444579584, 'train/wer': 0.1395789883184387, 'validation/ctc_loss': 0.6379733002279234, 'validation/wer': 0.18361415536136727, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3833823956384283, 'test/wer': 0.12318973046533829, 'test/num_examples': 2472, 'score': 17305.15603661537, 'total_duration': 17729.328581094742, 'accumulated_submission_time': 17305.15603661537, 'accumulated_eval_time': 401.14332270622253, 'accumulated_logging_time': 0.4889554977416992}
I1011 01:33:53.621951 140114986481408 logging_writer.py:48] [19197] accumulated_eval_time=401.143323, accumulated_logging_time=0.488955, accumulated_submission_time=17305.156037, global_step=19197, preemption_count=0, score=17305.156037, test/ctc_loss=0.383382, test/num_examples=2472, test/wer=0.123190, total_duration=17729.328581, train/ctc_loss=0.429371, train/wer=0.139579, validation/ctc_loss=0.637973, validation/num_examples=5348, validation/wer=0.183614
I1011 01:38:26.910138 140114978088704 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.175343, loss=1.556971
I1011 01:38:26.914622 140143741998912 pytorch_submission_base.py:86] 19500) loss = 1.557, grad_norm = 2.175
I1011 01:45:57.690124 140114986481408 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.481741, loss=1.566051
I1011 01:45:57.697458 140143741998912 pytorch_submission_base.py:86] 20000) loss = 1.566, grad_norm = 2.482
I1011 01:53:27.154831 140114978088704 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.764402, loss=1.607430
I1011 01:53:27.161734 140143741998912 pytorch_submission_base.py:86] 20500) loss = 1.607, grad_norm = 2.764
I1011 01:57:54.212873 140143741998912 spec.py:321] Evaluating on the training split.
I1011 01:58:08.276305 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 01:58:19.406735 140143741998912 spec.py:349] Evaluating on the test split.
I1011 01:58:25.195749 140143741998912 submission_runner.py:393] Time since start: 19200.94s, 	Step: 20796, 	{'train/ctc_loss': 0.40767045454545453, 'train/wer': 0.1323279914414463, 'validation/ctc_loss': 0.6278902973208007, 'validation/wer': 0.17901800801429055, 'validation/num_examples': 5348, 'test/ctc_loss': 0.367233974772566, 'test/wer': 0.11502447545345601, 'test/num_examples': 2472, 'score': 18743.885443925858, 'total_duration': 19200.93560576439, 'accumulated_submission_time': 18743.885443925858, 'accumulated_eval_time': 432.1260290145874, 'accumulated_logging_time': 0.5318717956542969}
I1011 01:58:25.231689 140114986481408 logging_writer.py:48] [20796] accumulated_eval_time=432.126029, accumulated_logging_time=0.531872, accumulated_submission_time=18743.885444, global_step=20796, preemption_count=0, score=18743.885444, test/ctc_loss=0.367234, test/num_examples=2472, test/wer=0.115024, total_duration=19200.935606, train/ctc_loss=0.407670, train/wer=0.132328, validation/ctc_loss=0.627890, validation/num_examples=5348, validation/wer=0.179018
I1011 02:01:30.177118 140114978088704 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.477573, loss=1.511131
I1011 02:01:30.185777 140143741998912 pytorch_submission_base.py:86] 21000) loss = 1.511, grad_norm = 2.478
I1011 02:09:00.133781 140114986481408 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.107451, loss=1.527797
I1011 02:09:00.139050 140143741998912 pytorch_submission_base.py:86] 21500) loss = 1.528, grad_norm = 2.107
I1011 02:16:30.405443 140114986481408 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.040116, loss=1.486759
I1011 02:16:30.412248 140143741998912 pytorch_submission_base.py:86] 22000) loss = 1.487, grad_norm = 3.040
I1011 02:22:26.590831 140143741998912 spec.py:321] Evaluating on the training split.
I1011 02:22:39.789845 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 02:22:50.870282 140143741998912 spec.py:349] Evaluating on the test split.
I1011 02:22:56.717985 140143741998912 submission_runner.py:393] Time since start: 20672.46s, 	Step: 22396, 	{'train/ctc_loss': 0.3767205698537463, 'train/wer': 0.12326694690887086, 'validation/ctc_loss': 0.5863044712709715, 'validation/wer': 0.1694298266788973, 'validation/num_examples': 5348, 'test/ctc_loss': 0.34915906873024066, 'test/wer': 0.10923567525846485, 'test/num_examples': 2472, 'score': 20183.34495472908, 'total_duration': 20672.457748889923, 'accumulated_submission_time': 20183.34495472908, 'accumulated_eval_time': 462.25299310684204, 'accumulated_logging_time': 0.5779612064361572}
I1011 02:22:56.752849 140114986481408 logging_writer.py:48] [22396] accumulated_eval_time=462.252993, accumulated_logging_time=0.577961, accumulated_submission_time=20183.344955, global_step=22396, preemption_count=0, score=20183.344955, test/ctc_loss=0.349159, test/num_examples=2472, test/wer=0.109236, total_duration=20672.457749, train/ctc_loss=0.376721, train/wer=0.123267, validation/ctc_loss=0.586304, validation/num_examples=5348, validation/wer=0.169430
I1011 02:24:31.757377 140114978088704 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.746741, loss=1.561200
I1011 02:24:31.761190 140143741998912 pytorch_submission_base.py:86] 22500) loss = 1.561, grad_norm = 1.747
I1011 02:32:02.618199 140114986481408 logging_writer.py:48] [23000] global_step=23000, grad_norm=4.261585, loss=1.458650
I1011 02:32:02.629412 140143741998912 pytorch_submission_base.py:86] 23000) loss = 1.459, grad_norm = 4.262
I1011 02:39:32.602738 140114978088704 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.081901, loss=1.469276
I1011 02:39:32.607781 140143741998912 pytorch_submission_base.py:86] 23500) loss = 1.469, grad_norm = 2.082
I1011 02:46:57.586511 140143741998912 spec.py:321] Evaluating on the training split.
I1011 02:47:10.973426 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 02:47:21.826060 140143741998912 spec.py:349] Evaluating on the test split.
I1011 02:47:27.736732 140143741998912 submission_runner.py:393] Time since start: 22143.48s, 	Step: 23994, 	{'train/ctc_loss': 0.34825333522285074, 'train/wer': 0.11348188331406218, 'validation/ctc_loss': 0.5658504707592426, 'validation/wer': 0.16233283445179356, 'validation/num_examples': 5348, 'test/ctc_loss': 0.326212452214659, 'test/wer': 0.10334531716531595, 'test/num_examples': 2472, 'score': 21622.357850074768, 'total_duration': 22143.47648215294, 'accumulated_submission_time': 21622.357850074768, 'accumulated_eval_time': 492.402957201004, 'accumulated_logging_time': 0.6239519119262695}
I1011 02:47:27.768599 140114986481408 logging_writer.py:48] [23994] accumulated_eval_time=492.402957, accumulated_logging_time=0.623952, accumulated_submission_time=21622.357850, global_step=23994, preemption_count=0, score=21622.357850, test/ctc_loss=0.326212, test/num_examples=2472, test/wer=0.103345, total_duration=22143.476482, train/ctc_loss=0.348253, train/wer=0.113482, validation/ctc_loss=0.565850, validation/num_examples=5348, validation/wer=0.162333
I1011 02:47:34.807147 140114978088704 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.770519, loss=1.492234
I1011 02:47:34.810850 140143741998912 pytorch_submission_base.py:86] 24000) loss = 1.492, grad_norm = 1.771
I1011 02:55:04.027951 140114986481408 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.723790, loss=1.427125
I1011 02:55:04.033939 140143741998912 pytorch_submission_base.py:86] 24500) loss = 1.427, grad_norm = 2.724
I1011 03:02:34.348778 140114986481408 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.181720, loss=1.424309
I1011 03:02:34.355922 140143741998912 pytorch_submission_base.py:86] 25000) loss = 1.424, grad_norm = 2.182
I1011 03:10:03.099573 140114978088704 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.652583, loss=1.349881
I1011 03:10:03.104755 140143741998912 pytorch_submission_base.py:86] 25500) loss = 1.350, grad_norm = 1.653
I1011 03:11:29.021519 140143741998912 spec.py:321] Evaluating on the training split.
I1011 03:11:42.364603 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 03:11:53.517157 140143741998912 spec.py:349] Evaluating on the test split.
I1011 03:11:59.396683 140143741998912 submission_runner.py:393] Time since start: 23615.14s, 	Step: 25596, 	{'train/ctc_loss': 0.314643552927264, 'train/wer': 0.10422632619760318, 'validation/ctc_loss': 0.53053092075108, 'validation/wer': 0.15236807801863564, 'validation/num_examples': 5348, 'test/ctc_loss': 0.30384548499096714, 'test/wer': 0.09625657587390571, 'test/num_examples': 2472, 'score': 23061.674258232117, 'total_duration': 23615.13649058342, 'accumulated_submission_time': 23061.674258232117, 'accumulated_eval_time': 522.7779037952423, 'accumulated_logging_time': 0.6701712608337402}
I1011 03:11:59.432199 140114986481408 logging_writer.py:48] [25596] accumulated_eval_time=522.777904, accumulated_logging_time=0.670171, accumulated_submission_time=23061.674258, global_step=25596, preemption_count=0, score=23061.674258, test/ctc_loss=0.303845, test/num_examples=2472, test/wer=0.096257, total_duration=23615.136491, train/ctc_loss=0.314644, train/wer=0.104226, validation/ctc_loss=0.530531, validation/num_examples=5348, validation/wer=0.152368
I1011 03:18:05.984888 140114986481408 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.063226, loss=1.420587
I1011 03:18:05.992027 140143741998912 pytorch_submission_base.py:86] 26000) loss = 1.421, grad_norm = 3.063
I1011 03:25:35.495413 140114978088704 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.138832, loss=1.344016
I1011 03:25:35.503514 140143741998912 pytorch_submission_base.py:86] 26500) loss = 1.344, grad_norm = 2.139
I1011 03:33:06.644738 140114986481408 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.643645, loss=1.340332
I1011 03:33:06.651497 140143741998912 pytorch_submission_base.py:86] 27000) loss = 1.340, grad_norm = 2.644
I1011 03:36:00.718956 140143741998912 spec.py:321] Evaluating on the training split.
I1011 03:36:13.795450 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 03:36:24.946573 140143741998912 spec.py:349] Evaluating on the test split.
I1011 03:36:30.916985 140143741998912 submission_runner.py:393] Time since start: 25086.66s, 	Step: 27194, 	{'train/ctc_loss': 0.2886271722431295, 'train/wer': 0.09586228509061044, 'validation/ctc_loss': 0.5040586110169278, 'validation/wer': 0.14507797035678077, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2861319681914962, 'test/wer': 0.08961468933438954, 'test/num_examples': 2472, 'score': 24501.153069972992, 'total_duration': 25086.65680551529, 'accumulated_submission_time': 24501.153069972992, 'accumulated_eval_time': 552.9757218360901, 'accumulated_logging_time': 0.7154347896575928}
I1011 03:36:30.941301 140114986481408 logging_writer.py:48] [27194] accumulated_eval_time=552.975722, accumulated_logging_time=0.715435, accumulated_submission_time=24501.153070, global_step=27194, preemption_count=0, score=24501.153070, test/ctc_loss=0.286132, test/num_examples=2472, test/wer=0.089615, total_duration=25086.656806, train/ctc_loss=0.288627, train/wer=0.095862, validation/ctc_loss=0.504059, validation/num_examples=5348, validation/wer=0.145078
I1011 03:41:07.442979 140114978088704 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.317506, loss=1.274967
I1011 03:41:07.447118 140143741998912 pytorch_submission_base.py:86] 27500) loss = 1.275, grad_norm = 2.318
I1011 03:48:38.726591 140114986481408 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.266800, loss=1.330610
I1011 03:48:38.738848 140143741998912 pytorch_submission_base.py:86] 28000) loss = 1.331, grad_norm = 3.267
I1011 03:56:08.042059 140114978088704 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.219559, loss=1.267195
I1011 03:56:08.050357 140143741998912 pytorch_submission_base.py:86] 28500) loss = 1.267, grad_norm = 2.220
I1011 04:00:32.296427 140143741998912 spec.py:321] Evaluating on the training split.
I1011 04:00:45.658986 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 04:00:56.684772 140143741998912 spec.py:349] Evaluating on the test split.
I1011 04:01:02.829878 140143741998912 submission_runner.py:393] Time since start: 26558.57s, 	Step: 28794, 	{'train/ctc_loss': 0.2608003871545384, 'train/wer': 0.08682285306735539, 'validation/ctc_loss': 0.47436405708446355, 'validation/wer': 0.13540288707574952, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2637504082924705, 'test/wer': 0.08301342595413645, 'test/num_examples': 2472, 'score': 25940.585669517517, 'total_duration': 26558.569600105286, 'accumulated_submission_time': 25940.585669517517, 'accumulated_eval_time': 583.5088813304901, 'accumulated_logging_time': 0.7505090236663818}
I1011 04:01:02.867117 140114986481408 logging_writer.py:48] [28794] accumulated_eval_time=583.508881, accumulated_logging_time=0.750509, accumulated_submission_time=25940.585670, global_step=28794, preemption_count=0, score=25940.585670, test/ctc_loss=0.263750, test/num_examples=2472, test/wer=0.083013, total_duration=26558.569600, train/ctc_loss=0.260800, train/wer=0.086823, validation/ctc_loss=0.474364, validation/num_examples=5348, validation/wer=0.135403
I1011 04:04:11.219729 140114986481408 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.901710, loss=1.255009
I1011 04:04:11.228276 140143741998912 pytorch_submission_base.py:86] 29000) loss = 1.255, grad_norm = 2.902
I1011 04:11:41.143007 140114978088704 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.907937, loss=1.323927
I1011 04:11:41.147954 140143741998912 pytorch_submission_base.py:86] 29500) loss = 1.324, grad_norm = 1.908
I1011 04:19:12.832269 140114986481408 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.987659, loss=1.301772
I1011 04:19:12.839326 140143741998912 pytorch_submission_base.py:86] 30000) loss = 1.302, grad_norm = 1.988
I1011 04:25:04.040377 140143741998912 spec.py:321] Evaluating on the training split.
I1011 04:25:17.258662 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 04:25:28.190282 140143741998912 spec.py:349] Evaluating on the test split.
I1011 04:25:34.126049 140143741998912 submission_runner.py:393] Time since start: 28029.87s, 	Step: 30392, 	{'train/ctc_loss': 0.23927310187727627, 'train/wer': 0.08034990652589719, 'validation/ctc_loss': 0.4507412809862869, 'validation/wer': 0.12947424322889006, 'validation/num_examples': 5348, 'test/ctc_loss': 0.25163006988354086, 'test/wer': 0.07750898787398695, 'test/num_examples': 2472, 'score': 27379.83411717415, 'total_duration': 28029.865797281265, 'accumulated_submission_time': 27379.83411717415, 'accumulated_eval_time': 613.5942578315735, 'accumulated_logging_time': 0.8003129959106445}
I1011 04:25:34.160363 140114986481408 logging_writer.py:48] [30392] accumulated_eval_time=613.594258, accumulated_logging_time=0.800313, accumulated_submission_time=27379.834117, global_step=30392, preemption_count=0, score=27379.834117, test/ctc_loss=0.251630, test/num_examples=2472, test/wer=0.077509, total_duration=28029.865797, train/ctc_loss=0.239273, train/wer=0.080350, validation/ctc_loss=0.450741, validation/num_examples=5348, validation/wer=0.129474
I1011 04:27:13.016368 140114978088704 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.615820, loss=1.244750
I1011 04:27:13.020277 140143741998912 pytorch_submission_base.py:86] 30500) loss = 1.245, grad_norm = 3.616
I1011 04:34:44.214874 140114986481408 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.961434, loss=1.243134
I1011 04:34:44.224968 140143741998912 pytorch_submission_base.py:86] 31000) loss = 1.243, grad_norm = 1.961
I1011 04:42:13.709142 140114978088704 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.357745, loss=1.244992
I1011 04:42:13.718192 140143741998912 pytorch_submission_base.py:86] 31500) loss = 1.245, grad_norm = 2.358
I1011 04:49:35.290925 140143741998912 spec.py:321] Evaluating on the training split.
I1011 04:49:48.549008 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 04:49:59.475762 140143741998912 spec.py:349] Evaluating on the test split.
I1011 04:50:05.947781 140143741998912 submission_runner.py:393] Time since start: 29501.69s, 	Step: 31990, 	{'train/ctc_loss': 0.21894690954405635, 'train/wer': 0.0738499443477885, 'validation/ctc_loss': 0.43360670019590114, 'validation/wer': 0.12411528991454641, 'validation/num_examples': 5348, 'test/ctc_loss': 0.23886440254209948, 'test/wer': 0.07474661304409644, 'test/num_examples': 2472, 'score': 28819.062165498734, 'total_duration': 29501.687607765198, 'accumulated_submission_time': 28819.062165498734, 'accumulated_eval_time': 644.2509214878082, 'accumulated_logging_time': 0.8451745510101318}
I1011 04:50:05.988384 140114986481408 logging_writer.py:48] [31990] accumulated_eval_time=644.250921, accumulated_logging_time=0.845175, accumulated_submission_time=28819.062165, global_step=31990, preemption_count=0, score=28819.062165, test/ctc_loss=0.238864, test/num_examples=2472, test/wer=0.074747, total_duration=29501.687608, train/ctc_loss=0.218947, train/wer=0.073850, validation/ctc_loss=0.433607, validation/num_examples=5348, validation/wer=0.124115
I1011 04:50:16.525245 140114978088704 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.842671, loss=1.220213
I1011 04:50:16.529541 140143741998912 pytorch_submission_base.py:86] 32000) loss = 1.220, grad_norm = 1.843
I1011 04:57:45.777800 140114986481408 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.649882, loss=1.255343
I1011 04:57:45.782989 140143741998912 pytorch_submission_base.py:86] 32500) loss = 1.255, grad_norm = 2.650
I1011 05:05:16.669693 140114986481408 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.970028, loss=1.195063
I1011 05:05:16.678117 140143741998912 pytorch_submission_base.py:86] 33000) loss = 1.195, grad_norm = 1.970
I1011 05:12:45.853331 140114978088704 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.554891, loss=1.187443
I1011 05:12:45.864635 140143741998912 pytorch_submission_base.py:86] 33500) loss = 1.187, grad_norm = 1.555
I1011 05:14:07.369606 140143741998912 spec.py:321] Evaluating on the training split.
I1011 05:14:20.662640 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 05:14:31.746388 140143741998912 spec.py:349] Evaluating on the test split.
I1011 05:14:37.550925 140143741998912 submission_runner.py:393] Time since start: 30973.29s, 	Step: 33591, 	{'train/ctc_loss': 0.2068397363196693, 'train/wer': 0.07034871783788457, 'validation/ctc_loss': 0.4194643504652652, 'validation/wer': 0.11952879833920727, 'validation/num_examples': 5348, 'test/ctc_loss': 0.23000017138202464, 'test/wer': 0.07163894136046961, 'test/num_examples': 2472, 'score': 30258.5070476532, 'total_duration': 30973.290828466415, 'accumulated_submission_time': 30258.5070476532, 'accumulated_eval_time': 674.4321019649506, 'accumulated_logging_time': 0.8986501693725586}
I1011 05:14:37.584702 140114986481408 logging_writer.py:48] [33591] accumulated_eval_time=674.432102, accumulated_logging_time=0.898650, accumulated_submission_time=30258.507048, global_step=33591, preemption_count=0, score=30258.507048, test/ctc_loss=0.230000, test/num_examples=2472, test/wer=0.071639, total_duration=30973.290828, train/ctc_loss=0.206840, train/wer=0.070349, validation/ctc_loss=0.419464, validation/num_examples=5348, validation/wer=0.119529
I1011 05:20:48.740038 140114986481408 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.647508, loss=1.105607
I1011 05:20:48.747171 140143741998912 pytorch_submission_base.py:86] 34000) loss = 1.106, grad_norm = 1.648
I1011 05:28:17.895368 140114978088704 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.925747, loss=1.172735
I1011 05:28:17.928019 140143741998912 pytorch_submission_base.py:86] 34500) loss = 1.173, grad_norm = 1.926
I1011 05:35:46.983437 140114986481408 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.560772, loss=1.200455
I1011 05:35:46.991991 140143741998912 pytorch_submission_base.py:86] 35000) loss = 1.200, grad_norm = 2.561
I1011 05:38:38.264731 140143741998912 spec.py:321] Evaluating on the training split.
I1011 05:38:51.678786 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 05:39:02.866881 140143741998912 spec.py:349] Evaluating on the test split.
I1011 05:39:08.802477 140143741998912 submission_runner.py:393] Time since start: 32444.54s, 	Step: 35189, 	{'train/ctc_loss': 0.2015170457704009, 'train/wer': 0.0685981045829326, 'validation/ctc_loss': 0.41637553252775267, 'validation/wer': 0.11860184425240188, 'validation/num_examples': 5348, 'test/ctc_loss': 0.22769525957319828, 'test/wer': 0.07107021713078626, 'test/num_examples': 2472, 'score': 31697.269169807434, 'total_duration': 32444.542278528214, 'accumulated_submission_time': 31697.269169807434, 'accumulated_eval_time': 704.9697449207306, 'accumulated_logging_time': 0.9428112506866455}
I1011 05:39:08.834636 140114986481408 logging_writer.py:48] [35189] accumulated_eval_time=704.969745, accumulated_logging_time=0.942811, accumulated_submission_time=31697.269170, global_step=35189, preemption_count=0, score=31697.269170, test/ctc_loss=0.227695, test/num_examples=2472, test/wer=0.071070, total_duration=32444.542279, train/ctc_loss=0.201517, train/wer=0.068598, validation/ctc_loss=0.416376, validation/num_examples=5348, validation/wer=0.118602
I1011 05:43:49.585454 140114978088704 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.121277, loss=1.165866
I1011 05:43:49.591124 140143741998912 pytorch_submission_base.py:86] 35500) loss = 1.166, grad_norm = 2.121
I1011 05:51:18.324127 140143741998912 spec.py:321] Evaluating on the training split.
I1011 05:51:31.286748 140143741998912 spec.py:333] Evaluating on the validation split.
I1011 05:51:42.209845 140143741998912 spec.py:349] Evaluating on the test split.
I1011 05:51:48.220751 140143741998912 submission_runner.py:393] Time since start: 33203.96s, 	Step: 36000, 	{'train/ctc_loss': 0.20129743411676712, 'train/wer': 0.06839278574438885, 'validation/ctc_loss': 0.4169288590798925, 'validation/wer': 0.11860184425240188, 'validation/num_examples': 5348, 'test/ctc_loss': 0.22741094183495708, 'test/wer': 0.07104990555115472, 'test/num_examples': 2472, 'score': 32425.433014154434, 'total_duration': 33203.96063351631, 'accumulated_submission_time': 32425.433014154434, 'accumulated_eval_time': 734.8661799430847, 'accumulated_logging_time': 0.9848320484161377}
I1011 05:51:48.250553 140114986481408 logging_writer.py:48] [36000] accumulated_eval_time=734.866180, accumulated_logging_time=0.984832, accumulated_submission_time=32425.433014, global_step=36000, preemption_count=0, score=32425.433014, test/ctc_loss=0.227411, test/num_examples=2472, test/wer=0.071050, total_duration=33203.960634, train/ctc_loss=0.201297, train/wer=0.068393, validation/ctc_loss=0.416929, validation/num_examples=5348, validation/wer=0.118602
I1011 05:51:48.816534 140114978088704 logging_writer.py:48] [36000] global_step=36000, preemption_count=0, score=32425.433014
I1011 05:51:49.222932 140143741998912 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_deepspeech/nadamw_run_0/librispeech_deepspeech_pytorch/trial_1/checkpoint_36000.
I1011 05:51:49.328868 140143741998912 submission_runner.py:646] Final librispeech_deepspeech score: 32425.433014154434
