python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_19 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-05-2023-22-26-53.log
2023-10-05 22:26:57.984895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1005 22:27:15.480520 140708414486336 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_19/criteo1tb_jax.
I1005 22:27:17.183332 140708414486336 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I1005 22:27:17.184196 140708414486336 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1005 22:27:17.184332 140708414486336 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1005 22:27:17.189786 140708414486336 submission_runner.py:507] Using RNG seed 377480999
I1005 22:27:23.075134 140708414486336 submission_runner.py:516] --- Tuning run 1/1 ---
I1005 22:27:23.075399 140708414486336 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_19/criteo1tb_jax/trial_1.
I1005 22:27:23.075689 140708414486336 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_19/criteo1tb_jax/trial_1/hparams.json.
I1005 22:27:23.258619 140708414486336 submission_runner.py:191] Initializing dataset.
I1005 22:27:23.258873 140708414486336 submission_runner.py:198] Initializing model.
I1005 22:27:28.868801 140708414486336 submission_runner.py:232] Initializing optimizer.
I1005 22:27:32.060858 140708414486336 submission_runner.py:239] Initializing metrics bundle.
I1005 22:27:32.061069 140708414486336 submission_runner.py:257] Initializing checkpoint and logger.
I1005 22:27:32.062207 140708414486336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_19/criteo1tb_jax/trial_1 with prefix checkpoint_
I1005 22:27:32.062350 140708414486336 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_19/criteo1tb_jax/trial_1/meta_data_0.json.
I1005 22:27:32.062557 140708414486336 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1005 22:27:32.062619 140708414486336 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1005 22:27:32.920078 140708414486336 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_19/criteo1tb_jax/trial_1/flags_0.json.
I1005 22:27:33.008168 140708414486336 submission_runner.py:290] Starting training loop.
I1005 22:28:00.563626 140544869127936 logging_writer.py:48] [0] global_step=0, grad_norm=4.569020748138428, loss=0.39126285910606384
I1005 22:28:00.575108 140708414486336 spec.py:321] Evaluating on the training split.
I1005 22:32:18.456035 140708414486336 spec.py:333] Evaluating on the validation split.
I1005 22:36:36.104315 140708414486336 spec.py:349] Evaluating on the test split.
I1005 22:41:21.174288 140708414486336 submission_runner.py:381] Time since start: 828.17s, 	Step: 1, 	{'train/loss': 0.39487716386903005, 'validation/loss': 0.39041853763949763, 'validation/num_examples': 83274637, 'test/loss': 0.3943082105263158, 'test/num_examples': 95000000, 'score': 27.56691861152649, 'total_duration': 828.1660182476044, 'accumulated_submission_time': 27.56691861152649, 'accumulated_eval_time': 800.5990645885468, 'accumulated_logging_time': 0}
I1005 22:41:21.196635 140525894104832 logging_writer.py:48] [1] accumulated_eval_time=800.599065, accumulated_logging_time=0, accumulated_submission_time=27.566919, global_step=1, preemption_count=0, score=27.566919, test/loss=0.394308, test/num_examples=95000000, total_duration=828.166018, train/loss=0.394877, validation/loss=0.390419, validation/num_examples=83274637
I1005 22:41:21.311233 140525885712128 logging_writer.py:48] [1] global_step=1, grad_norm=4.568385601043701, loss=0.39102691411972046
I1005 22:41:21.419926 140525894104832 logging_writer.py:48] [2] global_step=2, grad_norm=3.685270071029663, loss=0.3407381474971771
I1005 22:41:21.525002 140525885712128 logging_writer.py:48] [3] global_step=3, grad_norm=2.4482216835021973, loss=0.27950096130371094
I1005 22:41:21.628917 140525894104832 logging_writer.py:48] [4] global_step=4, grad_norm=1.5516964197158813, loss=0.2335999459028244
I1005 22:41:21.732018 140525885712128 logging_writer.py:48] [5] global_step=5, grad_norm=1.0932788848876953, loss=0.19789168238639832
I1005 22:41:21.834956 140525894104832 logging_writer.py:48] [6] global_step=6, grad_norm=0.7309775948524475, loss=0.17143061757087708
I1005 22:41:21.938032 140525885712128 logging_writer.py:48] [7] global_step=7, grad_norm=0.4599771797657013, loss=0.1561232954263687
I1005 22:41:22.041265 140525894104832 logging_writer.py:48] [8] global_step=8, grad_norm=0.2500699758529663, loss=0.14793892204761505
I1005 22:41:22.145935 140525885712128 logging_writer.py:48] [9] global_step=9, grad_norm=0.07937699556350708, loss=0.14204619824886322
I1005 22:41:22.251528 140525894104832 logging_writer.py:48] [10] global_step=10, grad_norm=0.16789184510707855, loss=0.14467844367027283
I1005 22:41:22.355846 140525885712128 logging_writer.py:48] [11] global_step=11, grad_norm=0.2925322651863098, loss=0.14602945744991302
I1005 22:41:22.459690 140525894104832 logging_writer.py:48] [12] global_step=12, grad_norm=0.3487376868724823, loss=0.1462467461824417
I1005 22:41:22.563744 140525885712128 logging_writer.py:48] [13] global_step=13, grad_norm=0.3871408998966217, loss=0.14972609281539917
I1005 22:41:22.667658 140525894104832 logging_writer.py:48] [14] global_step=14, grad_norm=0.35077497363090515, loss=0.1450142115354538
I1005 22:41:22.773058 140525885712128 logging_writer.py:48] [15] global_step=15, grad_norm=0.33114543557167053, loss=0.14757663011550903
I1005 22:41:22.877713 140525894104832 logging_writer.py:48] [16] global_step=16, grad_norm=0.2458336353302002, loss=0.14101175963878632
I1005 22:41:22.983696 140525885712128 logging_writer.py:48] [17] global_step=17, grad_norm=0.17087484896183014, loss=0.1384413242340088
I1005 22:41:23.088730 140525894104832 logging_writer.py:48] [18] global_step=18, grad_norm=0.08830630779266357, loss=0.1364363431930542
I1005 22:41:23.193788 140525885712128 logging_writer.py:48] [19] global_step=19, grad_norm=0.08539152890443802, loss=0.15069237351417542
I1005 22:41:23.298320 140525894104832 logging_writer.py:48] [20] global_step=20, grad_norm=0.03953568637371063, loss=0.150166854262352
I1005 22:41:23.402508 140525885712128 logging_writer.py:48] [21] global_step=21, grad_norm=0.07297151535749435, loss=0.15114517509937286
I1005 22:41:23.506750 140525894104832 logging_writer.py:48] [22] global_step=22, grad_norm=0.0825117975473404, loss=0.14808757603168488
I1005 22:41:23.617735 140525885712128 logging_writer.py:48] [23] global_step=23, grad_norm=0.026642315089702606, loss=0.1491139829158783
I1005 22:41:23.724663 140525894104832 logging_writer.py:48] [24] global_step=24, grad_norm=0.02261306345462799, loss=0.14590127766132355
I1005 22:41:23.829580 140525885712128 logging_writer.py:48] [25] global_step=25, grad_norm=0.023288030177354813, loss=0.14420510828495026
I1005 22:41:23.936399 140525894104832 logging_writer.py:48] [26] global_step=26, grad_norm=0.03402286767959595, loss=0.1483854055404663
I1005 22:41:24.041726 140525885712128 logging_writer.py:48] [27] global_step=27, grad_norm=0.025392504408955574, loss=0.14671772718429565
I1005 22:41:24.657377 140525894104832 logging_writer.py:48] [28] global_step=28, grad_norm=0.01885128766298294, loss=0.14644384384155273
I1005 22:41:25.384930 140525885712128 logging_writer.py:48] [29] global_step=29, grad_norm=0.01805463619530201, loss=0.14569059014320374
I1005 22:41:26.137733 140525894104832 logging_writer.py:48] [30] global_step=30, grad_norm=0.019501615315675735, loss=0.14604182541370392
I1005 22:41:27.012263 140525885712128 logging_writer.py:48] [31] global_step=31, grad_norm=0.014772516675293446, loss=0.1453419178724289
I1005 22:41:27.865537 140525894104832 logging_writer.py:48] [32] global_step=32, grad_norm=0.02672331966459751, loss=0.14597642421722412
I1005 22:41:28.641574 140525885712128 logging_writer.py:48] [33] global_step=33, grad_norm=0.028353828936815262, loss=0.14594346284866333
I1005 22:41:29.516219 140525894104832 logging_writer.py:48] [34] global_step=34, grad_norm=0.057877860963344574, loss=0.14529870450496674
I1005 22:41:30.370013 140525885712128 logging_writer.py:48] [35] global_step=35, grad_norm=0.09082995355129242, loss=0.14538010954856873
I1005 22:41:31.039458 140525894104832 logging_writer.py:48] [36] global_step=36, grad_norm=0.12216633558273315, loss=0.1469316929578781
I1005 22:41:31.774211 140525885712128 logging_writer.py:48] [37] global_step=37, grad_norm=0.09907402098178864, loss=0.14635850489139557
I1005 22:41:32.518933 140525894104832 logging_writer.py:48] [38] global_step=38, grad_norm=0.04962005093693733, loss=0.14189183712005615
I1005 22:41:33.386240 140525885712128 logging_writer.py:48] [39] global_step=39, grad_norm=0.028346383944153786, loss=0.13896970450878143
I1005 22:41:34.246156 140525894104832 logging_writer.py:48] [40] global_step=40, grad_norm=0.054932743310928345, loss=0.14086133241653442
I1005 22:41:34.950773 140525885712128 logging_writer.py:48] [41] global_step=41, grad_norm=0.06437186151742935, loss=0.13880832493305206
I1005 22:41:35.734219 140525894104832 logging_writer.py:48] [42] global_step=42, grad_norm=0.09523283690214157, loss=0.13813969492912292
I1005 22:41:36.581635 140525885712128 logging_writer.py:48] [43] global_step=43, grad_norm=0.12798000872135162, loss=0.1388273388147354
I1005 22:41:37.287176 140525894104832 logging_writer.py:48] [44] global_step=44, grad_norm=0.14705932140350342, loss=0.13693824410438538
I1005 22:41:38.078307 140525885712128 logging_writer.py:48] [45] global_step=45, grad_norm=0.11994514614343643, loss=0.1371917426586151
I1005 22:41:39.128680 140525894104832 logging_writer.py:48] [46] global_step=46, grad_norm=0.0797869935631752, loss=0.13479574024677277
I1005 22:41:39.803643 140525885712128 logging_writer.py:48] [47] global_step=47, grad_norm=0.05273805558681488, loss=0.13377393782138824
I1005 22:41:40.542721 140525894104832 logging_writer.py:48] [48] global_step=48, grad_norm=0.04729977622628212, loss=0.1339467614889145
I1005 22:41:41.412322 140525885712128 logging_writer.py:48] [49] global_step=49, grad_norm=0.037876758724451065, loss=0.13821786642074585
I1005 22:41:42.353208 140525894104832 logging_writer.py:48] [50] global_step=50, grad_norm=0.01502944901585579, loss=0.1323813498020172
I1005 22:41:43.069911 140525885712128 logging_writer.py:48] [51] global_step=51, grad_norm=0.016575759276747704, loss=0.13136456906795502
I1005 22:41:43.930613 140525894104832 logging_writer.py:48] [52] global_step=52, grad_norm=0.017884789034724236, loss=0.1321481466293335
I1005 22:41:44.805964 140525885712128 logging_writer.py:48] [53] global_step=53, grad_norm=0.03617182746529579, loss=0.13176247477531433
I1005 22:41:45.421268 140525894104832 logging_writer.py:48] [54] global_step=54, grad_norm=0.07638249546289444, loss=0.13246342539787292
I1005 22:41:46.322512 140525885712128 logging_writer.py:48] [55] global_step=55, grad_norm=0.14114637672901154, loss=0.13297906517982483
I1005 22:41:47.139169 140525894104832 logging_writer.py:48] [56] global_step=56, grad_norm=0.19911202788352966, loss=0.133641317486763
I1005 22:41:47.984590 140525885712128 logging_writer.py:48] [57] global_step=57, grad_norm=0.17798152565956116, loss=0.138201043009758
I1005 22:41:48.892693 140525894104832 logging_writer.py:48] [58] global_step=58, grad_norm=0.11366720497608185, loss=0.1344425082206726
I1005 22:41:49.575151 140525885712128 logging_writer.py:48] [59] global_step=59, grad_norm=0.07616216689348221, loss=0.13250143826007843
I1005 22:41:50.345264 140525894104832 logging_writer.py:48] [60] global_step=60, grad_norm=0.033857252448797226, loss=0.1299506574869156
I1005 22:41:51.210699 140525885712128 logging_writer.py:48] [61] global_step=61, grad_norm=0.006713468115776777, loss=0.13276362419128418
I1005 22:41:52.132927 140525894104832 logging_writer.py:48] [62] global_step=62, grad_norm=0.008481127209961414, loss=0.13353188335895538
I1005 22:41:52.710886 140525885712128 logging_writer.py:48] [63] global_step=63, grad_norm=0.015429808758199215, loss=0.1312791258096695
I1005 22:41:53.542965 140525894104832 logging_writer.py:48] [64] global_step=64, grad_norm=0.03409413620829582, loss=0.13296476006507874
I1005 22:41:54.319071 140525885712128 logging_writer.py:48] [65] global_step=65, grad_norm=0.03157617151737213, loss=0.13372650742530823
I1005 22:41:55.027657 140525894104832 logging_writer.py:48] [66] global_step=66, grad_norm=0.021906398236751556, loss=0.1320456713438034
I1005 22:41:55.859934 140525885712128 logging_writer.py:48] [67] global_step=67, grad_norm=0.03434066101908684, loss=0.12971679866313934
I1005 22:41:56.632056 140525894104832 logging_writer.py:48] [68] global_step=68, grad_norm=0.07094592601060867, loss=0.1315084993839264
I1005 22:41:57.353495 140525885712128 logging_writer.py:48] [69] global_step=69, grad_norm=0.09771868586540222, loss=0.12939243018627167
I1005 22:41:58.133908 140525894104832 logging_writer.py:48] [70] global_step=70, grad_norm=0.12312869727611542, loss=0.13265621662139893
I1005 22:41:58.932317 140525885712128 logging_writer.py:48] [71] global_step=71, grad_norm=0.12461322546005249, loss=0.12893366813659668
I1005 22:41:59.704199 140525894104832 logging_writer.py:48] [72] global_step=72, grad_norm=0.12468753010034561, loss=0.13293175399303436
I1005 22:42:00.566883 140525885712128 logging_writer.py:48] [73] global_step=73, grad_norm=0.09650222957134247, loss=0.1317356377840042
I1005 22:42:01.246531 140525894104832 logging_writer.py:48] [74] global_step=74, grad_norm=0.06856261193752289, loss=0.13303516805171967
I1005 22:42:02.204824 140525885712128 logging_writer.py:48] [75] global_step=75, grad_norm=0.04956215247511864, loss=0.13275864720344543
I1005 22:42:02.933521 140525894104832 logging_writer.py:48] [76] global_step=76, grad_norm=0.012338579632341862, loss=0.128930002450943
I1005 22:42:03.667231 140525885712128 logging_writer.py:48] [77] global_step=77, grad_norm=0.03397739306092262, loss=0.13013063371181488
I1005 22:42:04.504810 140525894104832 logging_writer.py:48] [78] global_step=78, grad_norm=0.02782616578042507, loss=0.12862557172775269
I1005 22:42:05.271167 140525885712128 logging_writer.py:48] [79] global_step=79, grad_norm=0.02698853239417076, loss=0.12875935435295105
I1005 22:42:06.016370 140525894104832 logging_writer.py:48] [80] global_step=80, grad_norm=0.02018648572266102, loss=0.12820059061050415
I1005 22:42:06.855417 140525885712128 logging_writer.py:48] [81] global_step=81, grad_norm=0.017037564888596535, loss=0.12877993285655975
I1005 22:42:07.517268 140525894104832 logging_writer.py:48] [82] global_step=82, grad_norm=0.01913166232407093, loss=0.12799173593521118
I1005 22:42:08.311100 140525885712128 logging_writer.py:48] [83] global_step=83, grad_norm=0.022382065653800964, loss=0.1292332410812378
I1005 22:42:09.069429 140525894104832 logging_writer.py:48] [84] global_step=84, grad_norm=0.020867671817541122, loss=0.13031765818595886
I1005 22:42:09.877346 140525885712128 logging_writer.py:48] [85] global_step=85, grad_norm=0.019802924245595932, loss=0.13021302223205566
I1005 22:42:10.653733 140525894104832 logging_writer.py:48] [86] global_step=86, grad_norm=0.038490600883960724, loss=0.12724356353282928
I1005 22:42:11.427915 140525885712128 logging_writer.py:48] [87] global_step=87, grad_norm=0.0698266327381134, loss=0.13012386858463287
I1005 22:42:12.234908 140525894104832 logging_writer.py:48] [88] global_step=88, grad_norm=0.0806310623884201, loss=0.1301795095205307
I1005 22:42:13.039474 140525885712128 logging_writer.py:48] [89] global_step=89, grad_norm=0.095794178545475, loss=0.13015906512737274
I1005 22:42:13.853453 140525894104832 logging_writer.py:48] [90] global_step=90, grad_norm=0.11898130923509598, loss=0.12974140048027039
I1005 22:42:14.858897 140525885712128 logging_writer.py:48] [91] global_step=91, grad_norm=0.13446496427059174, loss=0.12929683923721313
I1005 22:42:15.504330 140525894104832 logging_writer.py:48] [92] global_step=92, grad_norm=0.13435301184654236, loss=0.12954096496105194
I1005 22:42:16.358030 140525885712128 logging_writer.py:48] [93] global_step=93, grad_norm=0.12490386515855789, loss=0.12879960238933563
I1005 22:42:17.223625 140525894104832 logging_writer.py:48] [94] global_step=94, grad_norm=0.10812433809041977, loss=0.12888434529304504
I1005 22:42:18.016066 140525885712128 logging_writer.py:48] [95] global_step=95, grad_norm=0.08337067067623138, loss=0.1367013156414032
I1005 22:42:18.774113 140525894104832 logging_writer.py:48] [96] global_step=96, grad_norm=0.06856579333543777, loss=0.14173412322998047
I1005 22:42:19.631190 140525885712128 logging_writer.py:48] [97] global_step=97, grad_norm=0.07768222689628601, loss=0.14412102103233337
I1005 22:42:20.451656 140525894104832 logging_writer.py:48] [98] global_step=98, grad_norm=0.09170722961425781, loss=0.14117172360420227
I1005 22:42:21.275418 140525885712128 logging_writer.py:48] [99] global_step=99, grad_norm=0.0871012955904007, loss=0.14070475101470947
I1005 22:42:22.085561 140525894104832 logging_writer.py:48] [100] global_step=100, grad_norm=0.07745493203401566, loss=0.13947246968746185
I1005 22:47:35.271856 140525885712128 logging_writer.py:48] [500] global_step=500, grad_norm=0.01112721674144268, loss=0.1249871551990509
I1005 22:54:07.088610 140525894104832 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.030812779441475868, loss=0.12214486300945282
I1005 23:00:44.413060 140525885712128 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.012892035767436028, loss=0.12973807752132416
I1005 23:01:21.823091 140708414486336 spec.py:321] Evaluating on the training split.
I1005 23:04:50.488087 140708414486336 spec.py:333] Evaluating on the validation split.
I1005 23:07:37.966530 140708414486336 spec.py:349] Evaluating on the test split.
I1005 23:10:57.595292 140708414486336 submission_runner.py:381] Time since start: 2604.59s, 	Step: 1547, 	{'train/loss': 0.12365761043140723, 'validation/loss': 0.12536015017393592, 'validation/num_examples': 83274637, 'test/loss': 0.12768637894736842, 'test/num_examples': 95000000, 'score': 1228.1637744903564, 'total_duration': 2604.58704996109, 'accumulated_submission_time': 1228.1637744903564, 'accumulated_eval_time': 1376.3711988925934, 'accumulated_logging_time': 0.03062272071838379}
I1005 23:10:57.610777 140525894104832 logging_writer.py:48] [1547] accumulated_eval_time=1376.371199, accumulated_logging_time=0.030623, accumulated_submission_time=1228.163774, global_step=1547, preemption_count=0, score=1228.163774, test/loss=0.127686, test/num_examples=95000000, total_duration=2604.587050, train/loss=0.123658, validation/loss=0.125360, validation/num_examples=83274637
I1005 23:16:36.073484 140525885712128 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0075503322295844555, loss=0.1203906387090683
I1005 23:23:13.509416 140525894104832 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.006514555308967829, loss=0.12231170386075974
I1005 23:29:46.538589 140525885712128 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0043960087932646275, loss=0.12149177491664886
I1005 23:30:57.985116 140708414486336 spec.py:321] Evaluating on the training split.
I1005 23:34:29.513211 140708414486336 spec.py:333] Evaluating on the validation split.
I1005 23:37:28.433303 140708414486336 spec.py:349] Evaluating on the test split.
I1005 23:41:09.714710 140708414486336 submission_runner.py:381] Time since start: 4416.71s, 	Step: 3090, 	{'train/loss': 0.1249671792084316, 'validation/loss': 0.12490362461742102, 'validation/num_examples': 83274637, 'test/loss': 0.12720966315789473, 'test/num_examples': 95000000, 'score': 2428.5099585056305, 'total_duration': 4416.706413507462, 'accumulated_submission_time': 2428.5099585056305, 'accumulated_eval_time': 1988.1006798744202, 'accumulated_logging_time': 0.053696393966674805}
I1005 23:41:09.739764 140525894104832 logging_writer.py:48] [3090] accumulated_eval_time=1988.100680, accumulated_logging_time=0.053696, accumulated_submission_time=2428.509959, global_step=3090, preemption_count=0, score=2428.509959, test/loss=0.127210, test/num_examples=95000000, total_duration=4416.706414, train/loss=0.124967, validation/loss=0.124904, validation/num_examples=83274637
I1005 23:46:19.223806 140525885712128 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.017940456047654152, loss=0.12693792581558228
I1005 23:52:54.865039 140525894104832 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.008858350105583668, loss=0.12042824923992157
I1005 23:59:33.790826 140525885712128 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.013907921500504017, loss=0.11584401875734329
I1006 00:01:10.590164 140708414486336 spec.py:321] Evaluating on the training split.
I1006 00:04:30.508129 140708414486336 spec.py:333] Evaluating on the validation split.
I1006 00:07:30.518117 140708414486336 spec.py:349] Evaluating on the test split.
I1006 00:10:50.870513 140708414486336 submission_runner.py:381] Time since start: 6197.86s, 	Step: 4623, 	{'train/loss': 0.12141249314794, 'validation/loss': 0.12425704119250619, 'validation/num_examples': 83274637, 'test/loss': 0.12650668421052633, 'test/num_examples': 95000000, 'score': 3629.330596923828, 'total_duration': 6197.862279653549, 'accumulated_submission_time': 3629.330596923828, 'accumulated_eval_time': 2568.381001472473, 'accumulated_logging_time': 0.08730316162109375}
I1006 00:10:50.891837 140525894104832 logging_writer.py:48] [4623] accumulated_eval_time=2568.381001, accumulated_logging_time=0.087303, accumulated_submission_time=3629.330597, global_step=4623, preemption_count=0, score=3629.330597, test/loss=0.126507, test/num_examples=95000000, total_duration=6197.862280, train/loss=0.121412, validation/loss=0.124257, validation/num_examples=83274637
I1006 00:15:38.418393 140525885712128 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.004987796302884817, loss=0.12415912002325058
I1006 00:22:14.775063 140525894104832 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.005092507228255272, loss=0.11916116625070572
I1006 00:28:49.891000 140525885712128 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.003964279778301716, loss=0.13055069744586945
I1006 00:30:50.899405 140708414486336 spec.py:321] Evaluating on the training split.
I1006 00:34:00.812203 140708414486336 spec.py:333] Evaluating on the validation split.
I1006 00:36:34.545635 140708414486336 spec.py:349] Evaluating on the test split.
I1006 00:39:28.632908 140708414486336 submission_runner.py:381] Time since start: 7915.62s, 	Step: 6154, 	{'train/loss': 0.12324135258512676, 'validation/loss': 0.12396254576288336, 'validation/num_examples': 83274637, 'test/loss': 0.12623605263157894, 'test/num_examples': 95000000, 'score': 4829.310063123703, 'total_duration': 7915.624661922455, 'accumulated_submission_time': 4829.310063123703, 'accumulated_eval_time': 3086.114464521408, 'accumulated_logging_time': 0.11639094352722168}
I1006 00:39:28.654764 140525894104832 logging_writer.py:48] [6154] accumulated_eval_time=3086.114465, accumulated_logging_time=0.116391, accumulated_submission_time=4829.310063, global_step=6154, preemption_count=0, score=4829.310063, test/loss=0.126236, test/num_examples=95000000, total_duration=7915.624662, train/loss=0.123241, validation/loss=0.123963, validation/num_examples=83274637
I1006 00:43:43.746671 140525885712128 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.02207326889038086, loss=0.12065453827381134
I1006 00:50:18.481915 140525894104832 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.004080250859260559, loss=0.12027905136346817
I1006 00:57:00.539569 140525885712128 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.015251192264258862, loss=0.12261976301670074
I1006 00:59:29.212594 140708414486336 spec.py:321] Evaluating on the training split.
I1006 01:01:58.967356 140708414486336 spec.py:333] Evaluating on the validation split.
I1006 01:04:09.474972 140708414486336 spec.py:349] Evaluating on the test split.
I1006 01:06:47.007646 140708414486336 submission_runner.py:381] Time since start: 9554.00s, 	Step: 7686, 	{'train/loss': 0.12040294191372469, 'validation/loss': 0.12378149423815561, 'validation/num_examples': 83274637, 'test/loss': 0.12609863157894738, 'test/num_examples': 95000000, 'score': 6029.836069822311, 'total_duration': 9553.999418973923, 'accumulated_submission_time': 6029.836069822311, 'accumulated_eval_time': 3523.909481525421, 'accumulated_logging_time': 0.14920830726623535}
I1006 01:06:47.025855 140525894104832 logging_writer.py:48] [7686] accumulated_eval_time=3523.909482, accumulated_logging_time=0.149208, accumulated_submission_time=6029.836070, global_step=7686, preemption_count=0, score=6029.836070, test/loss=0.126099, test/num_examples=95000000, total_duration=9553.999419, train/loss=0.120403, validation/loss=0.123781, validation/num_examples=83274637
I1006 01:10:41.147193 140708414486336 spec.py:321] Evaluating on the training split.
I1006 01:12:34.310701 140708414486336 spec.py:333] Evaluating on the validation split.
I1006 01:14:03.960555 140708414486336 spec.py:349] Evaluating on the test split.
I1006 01:16:05.601037 140708414486336 submission_runner.py:381] Time since start: 10112.59s, 	Step: 8000, 	{'train/loss': 0.12189160952777983, 'validation/loss': 0.12376138007062103, 'validation/num_examples': 83274637, 'test/loss': 0.12605418947368421, 'test/num_examples': 95000000, 'score': 6263.941518783569, 'total_duration': 10112.59280371666, 'accumulated_submission_time': 6263.941518783569, 'accumulated_eval_time': 3848.3632860183716, 'accumulated_logging_time': 0.17885160446166992}
I1006 01:16:05.617725 140525885712128 logging_writer.py:48] [8000] accumulated_eval_time=3848.363286, accumulated_logging_time=0.178852, accumulated_submission_time=6263.941519, global_step=8000, preemption_count=0, score=6263.941519, test/loss=0.126054, test/num_examples=95000000, total_duration=10112.592804, train/loss=0.121892, validation/loss=0.123761, validation/num_examples=83274637
I1006 01:16:05.631149 140525894104832 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=6263.941519
I1006 01:16:11.667699 140708414486336 checkpoints.py:490] Saving checkpoint at step: 8000
I1006 01:16:45.855142 140708414486336 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_19/criteo1tb_jax/trial_1/checkpoint_8000
I1006 01:16:46.166133 140708414486336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_19/criteo1tb_jax/trial_1/checkpoint_8000.
I1006 01:16:46.563551 140708414486336 submission_runner.py:549] Tuning trial 1/1
I1006 01:16:46.563819 140708414486336 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1006 01:16:46.565122 140708414486336 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.39487716386903005, 'validation/loss': 0.39041853763949763, 'validation/num_examples': 83274637, 'test/loss': 0.3943082105263158, 'test/num_examples': 95000000, 'score': 27.56691861152649, 'total_duration': 828.1660182476044, 'accumulated_submission_time': 27.56691861152649, 'accumulated_eval_time': 800.5990645885468, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1547, {'train/loss': 0.12365761043140723, 'validation/loss': 0.12536015017393592, 'validation/num_examples': 83274637, 'test/loss': 0.12768637894736842, 'test/num_examples': 95000000, 'score': 1228.1637744903564, 'total_duration': 2604.58704996109, 'accumulated_submission_time': 1228.1637744903564, 'accumulated_eval_time': 1376.3711988925934, 'accumulated_logging_time': 0.03062272071838379, 'global_step': 1547, 'preemption_count': 0}), (3090, {'train/loss': 0.1249671792084316, 'validation/loss': 0.12490362461742102, 'validation/num_examples': 83274637, 'test/loss': 0.12720966315789473, 'test/num_examples': 95000000, 'score': 2428.5099585056305, 'total_duration': 4416.706413507462, 'accumulated_submission_time': 2428.5099585056305, 'accumulated_eval_time': 1988.1006798744202, 'accumulated_logging_time': 0.053696393966674805, 'global_step': 3090, 'preemption_count': 0}), (4623, {'train/loss': 0.12141249314794, 'validation/loss': 0.12425704119250619, 'validation/num_examples': 83274637, 'test/loss': 0.12650668421052633, 'test/num_examples': 95000000, 'score': 3629.330596923828, 'total_duration': 6197.862279653549, 'accumulated_submission_time': 3629.330596923828, 'accumulated_eval_time': 2568.381001472473, 'accumulated_logging_time': 0.08730316162109375, 'global_step': 4623, 'preemption_count': 0}), (6154, {'train/loss': 0.12324135258512676, 'validation/loss': 0.12396254576288336, 'validation/num_examples': 83274637, 'test/loss': 0.12623605263157894, 'test/num_examples': 95000000, 'score': 4829.310063123703, 'total_duration': 7915.624661922455, 'accumulated_submission_time': 4829.310063123703, 'accumulated_eval_time': 3086.114464521408, 'accumulated_logging_time': 0.11639094352722168, 'global_step': 6154, 'preemption_count': 0}), (7686, {'train/loss': 0.12040294191372469, 'validation/loss': 0.12378149423815561, 'validation/num_examples': 83274637, 'test/loss': 0.12609863157894738, 'test/num_examples': 95000000, 'score': 6029.836069822311, 'total_duration': 9553.999418973923, 'accumulated_submission_time': 6029.836069822311, 'accumulated_eval_time': 3523.909481525421, 'accumulated_logging_time': 0.14920830726623535, 'global_step': 7686, 'preemption_count': 0}), (8000, {'train/loss': 0.12189160952777983, 'validation/loss': 0.12376138007062103, 'validation/num_examples': 83274637, 'test/loss': 0.12605418947368421, 'test/num_examples': 95000000, 'score': 6263.941518783569, 'total_duration': 10112.59280371666, 'accumulated_submission_time': 6263.941518783569, 'accumulated_eval_time': 3848.3632860183716, 'accumulated_logging_time': 0.17885160446166992, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1006 01:16:46.565267 140708414486336 submission_runner.py:552] Timing: 6263.941518783569
I1006 01:16:46.565320 140708414486336 submission_runner.py:554] Total number of evals: 7
I1006 01:16:46.565370 140708414486336 submission_runner.py:555] ====================
I1006 01:16:46.565474 140708414486336 submission_runner.py:625] Final criteo1tb score: 6263.941518783569
