python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_16 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-05-2023-14-40-25.log
2023-10-05 14:40:30.589016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1005 14:40:47.509256 140428241033024 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_16/criteo1tb_jax.
I1005 14:40:49.165258 140428241033024 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I1005 14:40:49.166034 140428241033024 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1005 14:40:49.166242 140428241033024 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1005 14:40:49.171678 140428241033024 submission_runner.py:507] Using RNG seed 2021597379
I1005 14:40:54.862869 140428241033024 submission_runner.py:516] --- Tuning run 1/1 ---
I1005 14:40:54.863119 140428241033024 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_16/criteo1tb_jax/trial_1.
I1005 14:40:54.863459 140428241033024 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_16/criteo1tb_jax/trial_1/hparams.json.
I1005 14:40:55.049889 140428241033024 submission_runner.py:191] Initializing dataset.
I1005 14:40:55.050133 140428241033024 submission_runner.py:198] Initializing model.
I1005 14:41:01.055913 140428241033024 submission_runner.py:232] Initializing optimizer.
I1005 14:41:04.203054 140428241033024 submission_runner.py:239] Initializing metrics bundle.
I1005 14:41:04.203362 140428241033024 submission_runner.py:257] Initializing checkpoint and logger.
I1005 14:41:04.204930 140428241033024 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_16/criteo1tb_jax/trial_1 with prefix checkpoint_
I1005 14:41:04.205109 140428241033024 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_16/criteo1tb_jax/trial_1/meta_data_0.json.
I1005 14:41:04.205363 140428241033024 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1005 14:41:04.205445 140428241033024 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1005 14:41:05.101086 140428241033024 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_16/criteo1tb_jax/trial_1/flags_0.json.
I1005 14:41:05.201840 140428241033024 submission_runner.py:290] Starting training loop.
I1005 14:41:32.573573 140264454739712 logging_writer.py:48] [0] global_step=0, grad_norm=2.69710373878479, loss=0.2985074818134308
I1005 14:41:32.584293 140428241033024 spec.py:321] Evaluating on the training split.
I1005 14:45:21.665245 140428241033024 spec.py:333] Evaluating on the validation split.
I1005 14:49:18.297549 140428241033024 spec.py:349] Evaluating on the test split.
I1005 14:53:39.574321 140428241033024 submission_runner.py:381] Time since start: 754.37s, 	Step: 1, 	{'train/loss': 0.2988469825600678, 'validation/loss': 0.29946842037870425, 'validation/num_examples': 83274637, 'test/loss': 0.3004103157894737, 'test/num_examples': 95000000, 'score': 27.382452726364136, 'total_duration': 754.3724067211151, 'accumulated_submission_time': 27.382452726364136, 'accumulated_eval_time': 726.9899196624756, 'accumulated_logging_time': 0}
I1005 14:53:39.593715 140246469547776 logging_writer.py:48] [1] accumulated_eval_time=726.989920, accumulated_logging_time=0, accumulated_submission_time=27.382453, global_step=1, preemption_count=0, score=27.382453, test/loss=0.300410, test/num_examples=95000000, total_duration=754.372407, train/loss=0.298847, validation/loss=0.299468, validation/num_examples=83274637
I1005 14:53:39.708259 140245915924224 logging_writer.py:48] [1] global_step=1, grad_norm=2.694498300552368, loss=0.29850852489471436
I1005 14:53:39.815883 140246469547776 logging_writer.py:48] [2] global_step=2, grad_norm=2.2854344844818115, loss=0.2682182788848877
I1005 14:53:39.921396 140245915924224 logging_writer.py:48] [3] global_step=3, grad_norm=1.750363826751709, loss=0.22579649090766907
I1005 14:53:40.025862 140246469547776 logging_writer.py:48] [4] global_step=4, grad_norm=1.1450132131576538, loss=0.18438422679901123
I1005 14:53:40.129739 140245915924224 logging_writer.py:48] [5] global_step=5, grad_norm=0.550030529499054, loss=0.15953904390335083
I1005 14:53:40.233591 140246469547776 logging_writer.py:48] [6] global_step=6, grad_norm=0.12393932044506073, loss=0.14319366216659546
I1005 14:53:40.336981 140245915924224 logging_writer.py:48] [7] global_step=7, grad_norm=0.29673874378204346, loss=0.14899790287017822
I1005 14:53:40.441786 140246469547776 logging_writer.py:48] [8] global_step=8, grad_norm=0.5652155876159668, loss=0.16123223304748535
I1005 14:53:40.546039 140245915924224 logging_writer.py:48] [9] global_step=9, grad_norm=0.6474987864494324, loss=0.16599537432193756
I1005 14:53:40.650362 140246469547776 logging_writer.py:48] [10] global_step=10, grad_norm=0.6874946355819702, loss=0.1700192540884018
I1005 14:53:40.755246 140245915924224 logging_writer.py:48] [11] global_step=11, grad_norm=0.6476266384124756, loss=0.1677028387784958
I1005 14:53:40.859060 140246469547776 logging_writer.py:48] [12] global_step=12, grad_norm=0.5432172417640686, loss=0.15954726934432983
I1005 14:53:40.963577 140245915924224 logging_writer.py:48] [13] global_step=13, grad_norm=0.41766056418418884, loss=0.15335626900196075
I1005 14:53:41.067424 140246469547776 logging_writer.py:48] [14] global_step=14, grad_norm=0.25202664732933044, loss=0.1475486159324646
I1005 14:53:41.171962 140245915924224 logging_writer.py:48] [15] global_step=15, grad_norm=0.06114674732089043, loss=0.14190645515918732
I1005 14:53:41.277997 140246469547776 logging_writer.py:48] [16] global_step=16, grad_norm=0.0959836021065712, loss=0.1432436853647232
I1005 14:53:41.383769 140245915924224 logging_writer.py:48] [17] global_step=17, grad_norm=0.14821475744247437, loss=0.14326336979866028
I1005 14:53:41.489760 140246469547776 logging_writer.py:48] [18] global_step=18, grad_norm=0.08925185352563858, loss=0.14028817415237427
I1005 14:53:41.594243 140245915924224 logging_writer.py:48] [19] global_step=19, grad_norm=0.028789255768060684, loss=0.1383901685476303
I1005 14:53:41.699818 140246469547776 logging_writer.py:48] [20] global_step=20, grad_norm=0.05352919548749924, loss=0.13786061108112335
I1005 14:53:41.805735 140245915924224 logging_writer.py:48] [21] global_step=21, grad_norm=0.020151738077402115, loss=0.13745279610157013
I1005 14:53:41.913154 140246469547776 logging_writer.py:48] [22] global_step=22, grad_norm=0.016545085236430168, loss=0.13503868877887726
I1005 14:53:42.018334 140245915924224 logging_writer.py:48] [23] global_step=23, grad_norm=0.028183819726109505, loss=0.13559812307357788
I1005 14:53:42.123499 140246469547776 logging_writer.py:48] [24] global_step=24, grad_norm=0.025363564491271973, loss=0.13531994819641113
I1005 14:53:42.228607 140245915924224 logging_writer.py:48] [25] global_step=25, grad_norm=0.032604511827230453, loss=0.1358918994665146
I1005 14:53:42.335093 140246469547776 logging_writer.py:48] [26] global_step=26, grad_norm=0.04658052697777748, loss=0.135549396276474
I1005 14:53:42.439671 140245915924224 logging_writer.py:48] [27] global_step=27, grad_norm=0.033591967076063156, loss=0.13309404253959656
I1005 14:53:42.596543 140246469547776 logging_writer.py:48] [28] global_step=28, grad_norm=0.02152423933148384, loss=0.13564318418502808
I1005 14:53:43.375494 140245915924224 logging_writer.py:48] [29] global_step=29, grad_norm=0.01678732968866825, loss=0.1345873922109604
I1005 14:53:44.029183 140246469547776 logging_writer.py:48] [30] global_step=30, grad_norm=0.02589876763522625, loss=0.1338292956352234
I1005 14:53:44.719229 140245915924224 logging_writer.py:48] [31] global_step=31, grad_norm=0.06412366032600403, loss=0.13510704040527344
I1005 14:53:45.447263 140246469547776 logging_writer.py:48] [32] global_step=32, grad_norm=0.15925239026546478, loss=0.13357271254062653
I1005 14:53:46.278263 140245915924224 logging_writer.py:48] [33] global_step=33, grad_norm=0.21347451210021973, loss=0.13861192762851715
I1005 14:53:46.987944 140246469547776 logging_writer.py:48] [34] global_step=34, grad_norm=0.14658759534358978, loss=0.1360321044921875
I1005 14:53:47.707894 140245915924224 logging_writer.py:48] [35] global_step=35, grad_norm=0.05153970420360565, loss=0.13554657995700836
I1005 14:53:48.484881 140246469547776 logging_writer.py:48] [36] global_step=36, grad_norm=0.021532990038394928, loss=0.13452793657779694
I1005 14:53:49.066402 140245915924224 logging_writer.py:48] [37] global_step=37, grad_norm=0.008955039083957672, loss=0.13258445262908936
I1005 14:53:49.831021 140246469547776 logging_writer.py:48] [38] global_step=38, grad_norm=0.06523073464632034, loss=0.14959405362606049
I1005 14:53:50.568017 140245915924224 logging_writer.py:48] [39] global_step=39, grad_norm=0.09284225851297379, loss=0.15119673311710358
I1005 14:53:51.178088 140246469547776 logging_writer.py:48] [40] global_step=40, grad_norm=0.10601439327001572, loss=0.15099874138832092
I1005 14:53:51.918859 140245915924224 logging_writer.py:48] [41] global_step=41, grad_norm=0.1363154500722885, loss=0.15067769587039948
I1005 14:53:52.605473 140246469547776 logging_writer.py:48] [42] global_step=42, grad_norm=0.18936608731746674, loss=0.15164628624916077
I1005 14:53:53.313479 140245915924224 logging_writer.py:48] [43] global_step=43, grad_norm=0.24490374326705933, loss=0.15194259583950043
I1005 14:53:54.035526 140246469547776 logging_writer.py:48] [44] global_step=44, grad_norm=0.23148009181022644, loss=0.15030115842819214
I1005 14:53:54.983267 140245915924224 logging_writer.py:48] [45] global_step=45, grad_norm=0.16139580309391022, loss=0.14648975431919098
I1005 14:53:55.660379 140246469547776 logging_writer.py:48] [46] global_step=46, grad_norm=0.10763821750879288, loss=0.145600825548172
I1005 14:53:56.358309 140245915924224 logging_writer.py:48] [47] global_step=47, grad_norm=0.05353454127907753, loss=0.1455736607313156
I1005 14:53:56.976682 140246469547776 logging_writer.py:48] [48] global_step=48, grad_norm=0.05729219689965248, loss=0.14702019095420837
I1005 14:53:57.750934 140245915924224 logging_writer.py:48] [49] global_step=49, grad_norm=0.07659205049276352, loss=0.14333383738994598
I1005 14:53:58.621441 140246469547776 logging_writer.py:48] [50] global_step=50, grad_norm=0.09977021813392639, loss=0.1407983899116516
I1005 14:53:59.305739 140245915924224 logging_writer.py:48] [51] global_step=51, grad_norm=0.14722700417041779, loss=0.14144131541252136
I1005 14:54:00.073239 140246469547776 logging_writer.py:48] [52] global_step=52, grad_norm=0.18782679736614227, loss=0.14108821749687195
I1005 14:54:01.036642 140245915924224 logging_writer.py:48] [53] global_step=53, grad_norm=0.18362535536289215, loss=0.14147092401981354
I1005 14:54:01.609082 140246469547776 logging_writer.py:48] [54] global_step=54, grad_norm=0.14022324979305267, loss=0.14151324331760406
I1005 14:54:02.630017 140245915924224 logging_writer.py:48] [55] global_step=55, grad_norm=0.0891001746058464, loss=0.140717551112175
I1005 14:54:03.187801 140246469547776 logging_writer.py:48] [56] global_step=56, grad_norm=0.03966551646590233, loss=0.1389942467212677
I1005 14:54:04.065721 140245915924224 logging_writer.py:48] [57] global_step=57, grad_norm=0.05961081013083458, loss=0.13085421919822693
I1005 14:54:04.719847 140246469547776 logging_writer.py:48] [58] global_step=58, grad_norm=0.03671596571803093, loss=0.12763188779354095
I1005 14:54:05.375207 140245915924224 logging_writer.py:48] [59] global_step=59, grad_norm=0.010647992603480816, loss=0.1282852292060852
I1005 14:54:06.150888 140246469547776 logging_writer.py:48] [60] global_step=60, grad_norm=0.008280989713966846, loss=0.12828224897384644
I1005 14:54:06.878921 140245915924224 logging_writer.py:48] [61] global_step=61, grad_norm=0.006835087202489376, loss=0.12757529318332672
I1005 14:54:07.684401 140246469547776 logging_writer.py:48] [62] global_step=62, grad_norm=0.005973142106086016, loss=0.12763355672359467
I1005 14:54:08.446529 140245915924224 logging_writer.py:48] [63] global_step=63, grad_norm=0.005029947031289339, loss=0.12736108899116516
I1005 14:54:09.063218 140246469547776 logging_writer.py:48] [64] global_step=64, grad_norm=0.012098507955670357, loss=0.12492954730987549
I1005 14:54:09.802292 140245915924224 logging_writer.py:48] [65] global_step=65, grad_norm=0.03023376874625683, loss=0.1257748007774353
I1005 14:54:10.615453 140246469547776 logging_writer.py:48] [66] global_step=66, grad_norm=0.0839204490184784, loss=0.12192845344543457
I1005 14:54:11.300059 140245915924224 logging_writer.py:48] [67] global_step=67, grad_norm=0.13795681297779083, loss=0.12723512947559357
I1005 14:54:12.069777 140246469547776 logging_writer.py:48] [68] global_step=68, grad_norm=0.12122058123350143, loss=0.12697350978851318
I1005 14:54:12.662198 140245915924224 logging_writer.py:48] [69] global_step=69, grad_norm=0.07895336300134659, loss=0.12657876312732697
I1005 14:54:13.566996 140246469547776 logging_writer.py:48] [70] global_step=70, grad_norm=0.062358658760786057, loss=0.12355390936136246
I1005 14:54:14.173377 140245915924224 logging_writer.py:48] [71] global_step=71, grad_norm=0.05455053597688675, loss=0.12617693841457367
I1005 14:54:14.833016 140246469547776 logging_writer.py:48] [72] global_step=72, grad_norm=0.028942422941327095, loss=0.12756916880607605
I1005 14:54:15.454381 140245915924224 logging_writer.py:48] [73] global_step=73, grad_norm=0.008790706284344196, loss=0.12793679535388947
I1005 14:54:16.291141 140246469547776 logging_writer.py:48] [74] global_step=74, grad_norm=0.009570607915520668, loss=0.12681034207344055
I1005 14:54:16.922664 140245915924224 logging_writer.py:48] [75] global_step=75, grad_norm=0.013014557771384716, loss=0.12748056650161743
I1005 14:54:17.424932 140246469547776 logging_writer.py:48] [76] global_step=76, grad_norm=0.007068132050335407, loss=0.13755598664283752
I1005 14:54:18.333394 140245915924224 logging_writer.py:48] [77] global_step=77, grad_norm=0.007197588682174683, loss=0.14336669445037842
I1005 14:54:18.976351 140246469547776 logging_writer.py:48] [78] global_step=78, grad_norm=0.015083969570696354, loss=0.14196030795574188
I1005 14:54:19.827916 140245915924224 logging_writer.py:48] [79] global_step=79, grad_norm=0.03714318573474884, loss=0.14392541348934174
I1005 14:54:20.483041 140246469547776 logging_writer.py:48] [80] global_step=80, grad_norm=0.07421884685754776, loss=0.14245691895484924
I1005 14:54:21.213887 140245915924224 logging_writer.py:48] [81] global_step=81, grad_norm=0.09948133677244186, loss=0.1421968787908554
I1005 14:54:21.905940 140246469547776 logging_writer.py:48] [82] global_step=82, grad_norm=0.11477486044168472, loss=0.1425808221101761
I1005 14:54:22.718007 140245915924224 logging_writer.py:48] [83] global_step=83, grad_norm=0.12567691504955292, loss=0.1419595181941986
I1005 14:54:23.323946 140246469547776 logging_writer.py:48] [84] global_step=84, grad_norm=0.12118447571992874, loss=0.14261391758918762
I1005 14:54:24.030462 140245915924224 logging_writer.py:48] [85] global_step=85, grad_norm=0.11118196696043015, loss=0.14195014536380768
I1005 14:54:24.802749 140246469547776 logging_writer.py:48] [86] global_step=86, grad_norm=0.10014670342206955, loss=0.13954810798168182
I1005 14:54:25.567688 140245915924224 logging_writer.py:48] [87] global_step=87, grad_norm=0.08552531898021698, loss=0.13918493688106537
I1005 14:54:26.254039 140246469547776 logging_writer.py:48] [88] global_step=88, grad_norm=0.05760546773672104, loss=0.13899612426757812
I1005 14:54:27.014397 140245915924224 logging_writer.py:48] [89] global_step=89, grad_norm=0.041663914918899536, loss=0.13928742706775665
I1005 14:54:27.669238 140246469547776 logging_writer.py:48] [90] global_step=90, grad_norm=0.029126731678843498, loss=0.1398327350616455
I1005 14:54:28.566926 140245915924224 logging_writer.py:48] [91] global_step=91, grad_norm=0.01410767063498497, loss=0.13862130045890808
I1005 14:54:29.074835 140246469547776 logging_writer.py:48] [92] global_step=92, grad_norm=0.00480789877474308, loss=0.14030148088932037
I1005 14:54:29.829644 140245915924224 logging_writer.py:48] [93] global_step=93, grad_norm=0.016813401132822037, loss=0.13850370049476624
I1005 14:54:30.430446 140246469547776 logging_writer.py:48] [94] global_step=94, grad_norm=0.015541750937700272, loss=0.13719609379768372
I1005 14:54:31.262626 140245915924224 logging_writer.py:48] [95] global_step=95, grad_norm=0.03966497629880905, loss=0.13127492368221283
I1005 14:54:31.831228 140246469547776 logging_writer.py:48] [96] global_step=96, grad_norm=0.036679212003946304, loss=0.12799812853336334
I1005 14:54:32.426608 140245915924224 logging_writer.py:48] [97] global_step=97, grad_norm=0.011351271532475948, loss=0.12931422889232635
I1005 14:54:33.295816 140246469547776 logging_writer.py:48] [98] global_step=98, grad_norm=0.004843613598495722, loss=0.12691137194633484
I1005 14:54:34.017537 140245915924224 logging_writer.py:48] [99] global_step=99, grad_norm=0.01125264447182417, loss=0.1280132234096527
I1005 14:54:34.671937 140246469547776 logging_writer.py:48] [100] global_step=100, grad_norm=0.013590377755463123, loss=0.12849776446819305
I1005 14:59:12.538540 140245915924224 logging_writer.py:48] [500] global_step=500, grad_norm=0.011998195201158524, loss=0.12986049056053162
I1005 15:05:16.673302 140246469547776 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.020264634862542152, loss=0.12278582900762558
I1005 15:11:17.440289 140245915924224 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.01791679859161377, loss=0.1184309795498848
I1005 15:13:40.128727 140428241033024 spec.py:321] Evaluating on the training split.
I1005 15:16:49.015180 140428241033024 spec.py:333] Evaluating on the validation split.
I1005 15:20:00.353797 140428241033024 spec.py:349] Evaluating on the test split.
I1005 15:23:22.004843 140428241033024 submission_runner.py:381] Time since start: 2536.80s, 	Step: 1705, 	{'train/loss': 0.12487145189969044, 'validation/loss': 0.12538211364403787, 'validation/num_examples': 83274637, 'test/loss': 0.12789006315789472, 'test/num_examples': 95000000, 'score': 1227.885491847992, 'total_duration': 2536.8029403686523, 'accumulated_submission_time': 1227.885491847992, 'accumulated_eval_time': 1308.8659811019897, 'accumulated_logging_time': 0.027512550354003906}
I1005 15:23:22.020746 140246469547776 logging_writer.py:48] [1705] accumulated_eval_time=1308.865981, accumulated_logging_time=0.027513, accumulated_submission_time=1227.885492, global_step=1705, preemption_count=0, score=1227.885492, test/loss=0.127890, test/num_examples=95000000, total_duration=2536.802940, train/loss=0.124871, validation/loss=0.125382, validation/num_examples=83274637
I1005 15:26:38.448961 140245915924224 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.008112688548862934, loss=0.130132257938385
I1005 15:32:44.779104 140246469547776 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.006481905002146959, loss=0.11818327754735947
I1005 15:38:41.725981 140245915924224 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0038839669432491064, loss=0.11791884899139404
I1005 15:43:22.718934 140428241033024 spec.py:321] Evaluating on the training split.
I1005 15:46:29.409953 140428241033024 spec.py:333] Evaluating on the validation split.
I1005 15:49:07.659484 140428241033024 spec.py:349] Evaluating on the test split.
I1005 15:52:11.416330 140428241033024 submission_runner.py:381] Time since start: 4266.21s, 	Step: 3388, 	{'train/loss': 0.1254888210656508, 'validation/loss': 0.12464857697308246, 'validation/num_examples': 83274637, 'test/loss': 0.12707753684210527, 'test/num_examples': 95000000, 'score': 2428.552515268326, 'total_duration': 4266.214435100555, 'accumulated_submission_time': 2428.552515268326, 'accumulated_eval_time': 1837.563336133957, 'accumulated_logging_time': 0.050937652587890625}
I1005 15:52:11.434984 140246469547776 logging_writer.py:48] [3388] accumulated_eval_time=1837.563336, accumulated_logging_time=0.050938, accumulated_submission_time=2428.552515, global_step=3388, preemption_count=0, score=2428.552515, test/loss=0.127078, test/num_examples=95000000, total_duration=4266.214435, train/loss=0.125489, validation/loss=0.124649, validation/num_examples=83274637
I1005 15:53:15.595672 140245915924224 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.004787404555827379, loss=0.11951645463705063
I1005 15:59:07.049140 140246469547776 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.017220482230186462, loss=0.13436807692050934
I1005 16:05:02.478477 140245915924224 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.01320680696517229, loss=0.1181967556476593
I1005 16:11:03.058849 140246469547776 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.004008408635854721, loss=0.12212185561656952
I1005 16:12:11.992233 140428241033024 spec.py:321] Evaluating on the training split.
I1005 16:15:08.628479 140428241033024 spec.py:333] Evaluating on the validation split.
I1005 16:17:34.216069 140428241033024 spec.py:349] Evaluating on the test split.
I1005 16:20:28.794928 140428241033024 submission_runner.py:381] Time since start: 5963.59s, 	Step: 5098, 	{'train/loss': 0.12332490405196664, 'validation/loss': 0.12398370466628393, 'validation/num_examples': 83274637, 'test/loss': 0.12629629473684212, 'test/num_examples': 95000000, 'score': 3629.0783865451813, 'total_duration': 5963.593027830124, 'accumulated_submission_time': 3629.0783865451813, 'accumulated_eval_time': 2334.365970849991, 'accumulated_logging_time': 0.07754063606262207}
I1005 16:20:28.814186 140245915924224 logging_writer.py:48] [5098] accumulated_eval_time=2334.365971, accumulated_logging_time=0.077541, accumulated_submission_time=3629.078387, global_step=5098, preemption_count=0, score=3629.078387, test/loss=0.126296, test/num_examples=95000000, total_duration=5963.593028, train/loss=0.123325, validation/loss=0.123984, validation/num_examples=83274637
I1005 16:25:02.674344 140246469547776 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.004172871354967356, loss=0.11674722284078598
I1005 16:31:05.818423 140245915924224 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.004923081491142511, loss=0.11692199110984802
I1005 16:37:11.483113 140246469547776 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.003570183180272579, loss=0.12149260938167572
I1005 16:40:28.795209 140428241033024 spec.py:321] Evaluating on the training split.
I1005 16:43:14.467955 140428241033024 spec.py:333] Evaluating on the validation split.
I1005 16:45:34.967022 140428241033024 spec.py:349] Evaluating on the test split.
I1005 16:48:11.970530 140428241033024 submission_runner.py:381] Time since start: 7626.77s, 	Step: 6777, 	{'train/loss': 0.12195483993434306, 'validation/loss': 0.12386526524276534, 'validation/num_examples': 83274637, 'test/loss': 0.12624582105263157, 'test/num_examples': 95000000, 'score': 4829.028816699982, 'total_duration': 7626.7686467170715, 'accumulated_submission_time': 4829.028816699982, 'accumulated_eval_time': 2797.541258096695, 'accumulated_logging_time': 0.1041574478149414}
I1005 16:48:11.984771 140245915924224 logging_writer.py:48] [6777] accumulated_eval_time=2797.541258, accumulated_logging_time=0.104157, accumulated_submission_time=4829.028817, global_step=6777, preemption_count=0, score=4829.028817, test/loss=0.126246, test/num_examples=95000000, total_duration=7626.768647, train/loss=0.121955, validation/loss=0.123865, validation/num_examples=83274637
I1005 16:50:33.956198 140246469547776 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0030472581274807453, loss=0.12063241004943848
I1005 16:56:37.161472 140245915924224 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.004150238819420338, loss=0.12385081499814987
I1005 17:02:33.320082 140428241033024 spec.py:321] Evaluating on the training split.
I1005 17:04:55.791905 140428241033024 spec.py:333] Evaluating on the validation split.
I1005 17:06:51.477528 140428241033024 spec.py:349] Evaluating on the test split.
I1005 17:09:12.111130 140428241033024 submission_runner.py:381] Time since start: 8886.91s, 	Step: 8000, 	{'train/loss': 0.1257087899453985, 'validation/loss': 0.12370735401704604, 'validation/num_examples': 83274637, 'test/loss': 0.12599171578947368, 'test/num_examples': 95000000, 'score': 5690.339685916901, 'total_duration': 8886.909214496613, 'accumulated_submission_time': 5690.339685916901, 'accumulated_eval_time': 3196.332253932953, 'accumulated_logging_time': 0.12581896781921387}
I1005 17:09:12.128695 140246469547776 logging_writer.py:48] [8000] accumulated_eval_time=3196.332254, accumulated_logging_time=0.125819, accumulated_submission_time=5690.339686, global_step=8000, preemption_count=0, score=5690.339686, test/loss=0.125992, test/num_examples=95000000, total_duration=8886.909214, train/loss=0.125709, validation/loss=0.123707, validation/num_examples=83274637
I1005 17:09:12.149997 140245915924224 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5690.339686
I1005 17:09:19.614819 140428241033024 checkpoints.py:490] Saving checkpoint at step: 8000
I1005 17:10:02.783473 140428241033024 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_16/criteo1tb_jax/trial_1/checkpoint_8000
I1005 17:10:03.123197 140428241033024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_16/criteo1tb_jax/trial_1/checkpoint_8000.
I1005 17:10:03.538082 140428241033024 submission_runner.py:549] Tuning trial 1/1
I1005 17:10:03.538442 140428241033024 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1005 17:10:03.538948 140428241033024 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.2988469825600678, 'validation/loss': 0.29946842037870425, 'validation/num_examples': 83274637, 'test/loss': 0.3004103157894737, 'test/num_examples': 95000000, 'score': 27.382452726364136, 'total_duration': 754.3724067211151, 'accumulated_submission_time': 27.382452726364136, 'accumulated_eval_time': 726.9899196624756, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1705, {'train/loss': 0.12487145189969044, 'validation/loss': 0.12538211364403787, 'validation/num_examples': 83274637, 'test/loss': 0.12789006315789472, 'test/num_examples': 95000000, 'score': 1227.885491847992, 'total_duration': 2536.8029403686523, 'accumulated_submission_time': 1227.885491847992, 'accumulated_eval_time': 1308.8659811019897, 'accumulated_logging_time': 0.027512550354003906, 'global_step': 1705, 'preemption_count': 0}), (3388, {'train/loss': 0.1254888210656508, 'validation/loss': 0.12464857697308246, 'validation/num_examples': 83274637, 'test/loss': 0.12707753684210527, 'test/num_examples': 95000000, 'score': 2428.552515268326, 'total_duration': 4266.214435100555, 'accumulated_submission_time': 2428.552515268326, 'accumulated_eval_time': 1837.563336133957, 'accumulated_logging_time': 0.050937652587890625, 'global_step': 3388, 'preemption_count': 0}), (5098, {'train/loss': 0.12332490405196664, 'validation/loss': 0.12398370466628393, 'validation/num_examples': 83274637, 'test/loss': 0.12629629473684212, 'test/num_examples': 95000000, 'score': 3629.0783865451813, 'total_duration': 5963.593027830124, 'accumulated_submission_time': 3629.0783865451813, 'accumulated_eval_time': 2334.365970849991, 'accumulated_logging_time': 0.07754063606262207, 'global_step': 5098, 'preemption_count': 0}), (6777, {'train/loss': 0.12195483993434306, 'validation/loss': 0.12386526524276534, 'validation/num_examples': 83274637, 'test/loss': 0.12624582105263157, 'test/num_examples': 95000000, 'score': 4829.028816699982, 'total_duration': 7626.7686467170715, 'accumulated_submission_time': 4829.028816699982, 'accumulated_eval_time': 2797.541258096695, 'accumulated_logging_time': 0.1041574478149414, 'global_step': 6777, 'preemption_count': 0}), (8000, {'train/loss': 0.1257087899453985, 'validation/loss': 0.12370735401704604, 'validation/num_examples': 83274637, 'test/loss': 0.12599171578947368, 'test/num_examples': 95000000, 'score': 5690.339685916901, 'total_duration': 8886.909214496613, 'accumulated_submission_time': 5690.339685916901, 'accumulated_eval_time': 3196.332253932953, 'accumulated_logging_time': 0.12581896781921387, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1005 17:10:03.539083 140428241033024 submission_runner.py:552] Timing: 5690.339685916901
I1005 17:10:03.539142 140428241033024 submission_runner.py:554] Total number of evals: 6
I1005 17:10:03.539211 140428241033024 submission_runner.py:555] ====================
I1005 17:10:03.539404 140428241033024 submission_runner.py:625] Final criteo1tb score: 5690.339685916901
