python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_2 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-04-2023-01-13-37.log
2023-10-04 01:13:42.575079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1004 01:13:59.336772 140453294417728 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_2/criteo1tb_jax.
I1004 01:14:01.086878 140453294417728 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I1004 01:14:01.087708 140453294417728 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1004 01:14:01.087862 140453294417728 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1004 01:14:01.094167 140453294417728 submission_runner.py:507] Using RNG seed 82158366
I1004 01:14:06.897749 140453294417728 submission_runner.py:516] --- Tuning run 1/1 ---
I1004 01:14:06.897975 140453294417728 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_2/criteo1tb_jax/trial_1.
I1004 01:14:06.898278 140453294417728 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_2/criteo1tb_jax/trial_1/hparams.json.
I1004 01:14:07.080033 140453294417728 submission_runner.py:191] Initializing dataset.
I1004 01:14:07.080463 140453294417728 submission_runner.py:198] Initializing model.
I1004 01:14:12.652021 140453294417728 submission_runner.py:232] Initializing optimizer.
I1004 01:14:15.775453 140453294417728 submission_runner.py:239] Initializing metrics bundle.
I1004 01:14:15.775672 140453294417728 submission_runner.py:257] Initializing checkpoint and logger.
I1004 01:14:15.776835 140453294417728 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_2/criteo1tb_jax/trial_1 with prefix checkpoint_
I1004 01:14:15.776998 140453294417728 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_2/criteo1tb_jax/trial_1/meta_data_0.json.
I1004 01:14:15.777199 140453294417728 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1004 01:14:15.777259 140453294417728 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1004 01:14:16.571333 140453294417728 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_2/criteo1tb_jax/trial_1/flags_0.json.
I1004 01:14:16.661275 140453294417728 submission_runner.py:290] Starting training loop.
I1004 01:14:41.704121 140289084741376 logging_writer.py:48] [0] global_step=0, grad_norm=1.530428171157837, loss=0.2149205505847931
I1004 01:14:41.716537 140453294417728 spec.py:321] Evaluating on the training split.
I1004 01:18:39.658900 140453294417728 spec.py:333] Evaluating on the validation split.
I1004 01:22:40.993717 140453294417728 spec.py:349] Evaluating on the test split.
I1004 01:27:13.917446 140453294417728 submission_runner.py:381] Time since start: 777.26s, 	Step: 1, 	{'train/loss': 0.21765304661396914, 'validation/loss': 0.21864044871189292, 'validation/num_examples': 83274637, 'test/loss': 0.21941926315789473, 'test/num_examples': 95000000, 'score': 25.0552339553833, 'total_duration': 777.2560930252075, 'accumulated_submission_time': 25.0552339553833, 'accumulated_eval_time': 752.2008173465729, 'accumulated_logging_time': 0}
I1004 01:27:13.937502 140269538244352 logging_writer.py:48] [1] accumulated_eval_time=752.200817, accumulated_logging_time=0, accumulated_submission_time=25.055234, global_step=1, preemption_count=0, score=25.055234, test/loss=0.219419, test/num_examples=95000000, total_duration=777.256093, train/loss=0.217653, validation/loss=0.218640, validation/num_examples=83274637
I1004 01:27:14.051643 140269529851648 logging_writer.py:48] [1] global_step=1, grad_norm=1.5340425968170166, loss=0.21457751095294952
I1004 01:27:14.157434 140269538244352 logging_writer.py:48] [2] global_step=2, grad_norm=1.2795217037200928, loss=0.1989998072385788
I1004 01:27:14.262314 140269529851648 logging_writer.py:48] [3] global_step=3, grad_norm=0.904512882232666, loss=0.17584985494613647
I1004 01:27:14.365671 140269538244352 logging_writer.py:48] [4] global_step=4, grad_norm=0.49964436888694763, loss=0.15457594394683838
I1004 01:27:14.469570 140269529851648 logging_writer.py:48] [5] global_step=5, grad_norm=0.12160345911979675, loss=0.14654375612735748
I1004 01:27:14.572494 140269538244352 logging_writer.py:48] [6] global_step=6, grad_norm=0.23462532460689545, loss=0.14836464822292328
I1004 01:27:14.675960 140269529851648 logging_writer.py:48] [7] global_step=7, grad_norm=0.4537326693534851, loss=0.15707673132419586
I1004 01:27:14.781014 140269538244352 logging_writer.py:48] [8] global_step=8, grad_norm=0.5409236550331116, loss=0.1596173495054245
I1004 01:27:14.885108 140269529851648 logging_writer.py:48] [9] global_step=9, grad_norm=0.5791057348251343, loss=0.16344961524009705
I1004 01:27:14.988629 140269538244352 logging_writer.py:48] [10] global_step=10, grad_norm=0.552085816860199, loss=0.16101056337356567
I1004 01:27:15.092874 140269529851648 logging_writer.py:48] [11] global_step=11, grad_norm=0.4632681608200073, loss=0.15484261512756348
I1004 01:27:15.197187 140269538244352 logging_writer.py:48] [12] global_step=12, grad_norm=0.3484812080860138, loss=0.1490681916475296
I1004 01:27:15.301356 140269529851648 logging_writer.py:48] [13] global_step=13, grad_norm=0.20001523196697235, loss=0.141895592212677
I1004 01:27:15.404756 140269538244352 logging_writer.py:48] [14] global_step=14, grad_norm=0.07515515387058258, loss=0.14113962650299072
I1004 01:27:15.509002 140269529851648 logging_writer.py:48] [15] global_step=15, grad_norm=0.06593486666679382, loss=0.13910576701164246
I1004 01:27:15.613980 140269538244352 logging_writer.py:48] [16] global_step=16, grad_norm=0.0971141904592514, loss=0.13859353959560394
I1004 01:27:15.717229 140269529851648 logging_writer.py:48] [17] global_step=17, grad_norm=0.05671343207359314, loss=0.13713228702545166
I1004 01:27:15.821482 140269538244352 logging_writer.py:48] [18] global_step=18, grad_norm=0.021028798073530197, loss=0.1347644329071045
I1004 01:27:15.927833 140269529851648 logging_writer.py:48] [19] global_step=19, grad_norm=0.04498232528567314, loss=0.1358468234539032
I1004 01:27:16.032975 140269538244352 logging_writer.py:48] [20] global_step=20, grad_norm=0.020352676510810852, loss=0.13633280992507935
I1004 01:27:16.137608 140269529851648 logging_writer.py:48] [21] global_step=21, grad_norm=0.017083175480365753, loss=0.13696859776973724
I1004 01:27:16.242846 140269538244352 logging_writer.py:48] [22] global_step=22, grad_norm=0.017074741423130035, loss=0.1385868638753891
I1004 01:27:16.348185 140269529851648 logging_writer.py:48] [23] global_step=23, grad_norm=0.02876833640038967, loss=0.13453145325183868
I1004 01:27:16.453667 140269538244352 logging_writer.py:48] [24] global_step=24, grad_norm=0.027376655489206314, loss=0.13593082129955292
I1004 01:27:16.558878 140269529851648 logging_writer.py:48] [25] global_step=25, grad_norm=0.022043079137802124, loss=0.1348065882921219
I1004 01:27:16.664212 140269538244352 logging_writer.py:48] [26] global_step=26, grad_norm=0.02952854335308075, loss=0.13782356679439545
I1004 01:27:16.769845 140269529851648 logging_writer.py:48] [27] global_step=27, grad_norm=0.05187051743268967, loss=0.13465933501720428
I1004 01:27:16.876277 140269538244352 logging_writer.py:48] [28] global_step=28, grad_norm=0.04792020469903946, loss=0.13567157089710236
I1004 01:27:17.620767 140269529851648 logging_writer.py:48] [29] global_step=29, grad_norm=0.0432044081389904, loss=0.1341974139213562
I1004 01:27:18.319670 140269538244352 logging_writer.py:48] [30] global_step=30, grad_norm=0.04326054826378822, loss=0.13473768532276154
I1004 01:27:19.246408 140269529851648 logging_writer.py:48] [31] global_step=31, grad_norm=0.05123985931277275, loss=0.13278906047344208
I1004 01:27:20.057706 140269538244352 logging_writer.py:48] [32] global_step=32, grad_norm=0.0743710994720459, loss=0.1358707994222641
I1004 01:27:20.815320 140269529851648 logging_writer.py:48] [33] global_step=33, grad_norm=0.0945887640118599, loss=0.13464121520519257
I1004 01:27:21.523413 140269538244352 logging_writer.py:48] [34] global_step=34, grad_norm=0.10283394157886505, loss=0.13581159710884094
I1004 01:27:22.339480 140269529851648 logging_writer.py:48] [35] global_step=35, grad_norm=0.09943991154432297, loss=0.13408690690994263
I1004 01:27:23.127585 140269538244352 logging_writer.py:48] [36] global_step=36, grad_norm=0.08719959855079651, loss=0.13416536152362823
I1004 01:27:23.925600 140269529851648 logging_writer.py:48] [37] global_step=37, grad_norm=0.0765872672200203, loss=0.13233929872512817
I1004 01:27:24.918716 140269538244352 logging_writer.py:48] [38] global_step=38, grad_norm=0.11075006425380707, loss=0.14567306637763977
I1004 01:27:25.592164 140269529851648 logging_writer.py:48] [39] global_step=39, grad_norm=0.1564573347568512, loss=0.1485670506954193
I1004 01:27:26.482159 140269538244352 logging_writer.py:48] [40] global_step=40, grad_norm=0.16414965689182281, loss=0.14779464900493622
I1004 01:27:27.282800 140269529851648 logging_writer.py:48] [41] global_step=41, grad_norm=0.14378710091114044, loss=0.1467759758234024
I1004 01:27:28.075186 140269538244352 logging_writer.py:48] [42] global_step=42, grad_norm=0.11193914711475372, loss=0.14630794525146484
I1004 01:27:28.858687 140269529851648 logging_writer.py:48] [43] global_step=43, grad_norm=0.08914162218570709, loss=0.1446497142314911
I1004 01:27:29.573912 140269538244352 logging_writer.py:48] [44] global_step=44, grad_norm=0.08397965878248215, loss=0.14470124244689941
I1004 01:27:30.294028 140269529851648 logging_writer.py:48] [45] global_step=45, grad_norm=0.09562075138092041, loss=0.14229677617549896
I1004 01:27:31.143288 140269538244352 logging_writer.py:48] [46] global_step=46, grad_norm=0.1285027712583542, loss=0.1451679766178131
I1004 01:27:31.888790 140269529851648 logging_writer.py:48] [47] global_step=47, grad_norm=0.16171903908252716, loss=0.14321263134479523
I1004 01:27:32.609936 140269538244352 logging_writer.py:48] [48] global_step=48, grad_norm=0.18856285512447357, loss=0.14478854835033417
I1004 01:27:33.326529 140269529851648 logging_writer.py:48] [49] global_step=49, grad_norm=0.15247413516044617, loss=0.14413611590862274
I1004 01:27:34.027348 140269538244352 logging_writer.py:48] [50] global_step=50, grad_norm=0.11004689335823059, loss=0.14407406747341156
I1004 01:27:34.766905 140269529851648 logging_writer.py:48] [51] global_step=51, grad_norm=0.07355775684118271, loss=0.14420588314533234
I1004 01:27:35.521781 140269538244352 logging_writer.py:48] [52] global_step=52, grad_norm=0.0442918986082077, loss=0.1437966674566269
I1004 01:27:36.210953 140269529851648 logging_writer.py:48] [53] global_step=53, grad_norm=0.037337251007556915, loss=0.14344459772109985
I1004 01:27:36.967139 140269538244352 logging_writer.py:48] [54] global_step=54, grad_norm=0.021448997780680656, loss=0.14099054038524628
I1004 01:27:37.688490 140269529851648 logging_writer.py:48] [55] global_step=55, grad_norm=0.007888669148087502, loss=0.14222347736358643
I1004 01:27:38.452605 140269538244352 logging_writer.py:48] [56] global_step=56, grad_norm=0.008360184729099274, loss=0.14238256216049194
I1004 01:27:39.294000 140269529851648 logging_writer.py:48] [57] global_step=57, grad_norm=0.021439986303448677, loss=0.13066983222961426
I1004 01:27:40.064960 140269538244352 logging_writer.py:48] [58] global_step=58, grad_norm=0.029811622574925423, loss=0.12858232855796814
I1004 01:27:40.790583 140269529851648 logging_writer.py:48] [59] global_step=59, grad_norm=0.03225807845592499, loss=0.12721309065818787
I1004 01:27:41.535459 140269538244352 logging_writer.py:48] [60] global_step=60, grad_norm=0.021052664145827293, loss=0.12451045960187912
I1004 01:27:42.332382 140269529851648 logging_writer.py:48] [61] global_step=61, grad_norm=0.005887656006962061, loss=0.1266736388206482
I1004 01:27:43.216595 140269538244352 logging_writer.py:48] [62] global_step=62, grad_norm=0.008450699970126152, loss=0.12733933329582214
I1004 01:27:43.990092 140269529851648 logging_writer.py:48] [63] global_step=63, grad_norm=0.017804905772209167, loss=0.12858806550502777
I1004 01:27:44.782904 140269538244352 logging_writer.py:48] [64] global_step=64, grad_norm=0.043412256985902786, loss=0.12662871181964874
I1004 01:27:45.520871 140269529851648 logging_writer.py:48] [65] global_step=65, grad_norm=0.06192963942885399, loss=0.12477289885282516
I1004 01:27:46.366115 140269538244352 logging_writer.py:48] [66] global_step=66, grad_norm=0.05395527929067612, loss=0.12689155340194702
I1004 01:27:47.149582 140269529851648 logging_writer.py:48] [67] global_step=67, grad_norm=0.0614902563393116, loss=0.12852536141872406
I1004 01:27:47.987998 140269538244352 logging_writer.py:48] [68] global_step=68, grad_norm=0.08156648278236389, loss=0.12801259756088257
I1004 01:27:48.719072 140269529851648 logging_writer.py:48] [69] global_step=69, grad_norm=0.0784955769777298, loss=0.12635667622089386
I1004 01:27:49.539284 140269538244352 logging_writer.py:48] [70] global_step=70, grad_norm=0.06760559231042862, loss=0.1273777037858963
I1004 01:27:50.340379 140269529851648 logging_writer.py:48] [71] global_step=71, grad_norm=0.05548890307545662, loss=0.12608319520950317
I1004 01:27:51.041417 140269538244352 logging_writer.py:48] [72] global_step=72, grad_norm=0.037937186658382416, loss=0.12804746627807617
I1004 01:27:51.801558 140269529851648 logging_writer.py:48] [73] global_step=73, grad_norm=0.028414536267518997, loss=0.12694014608860016
I1004 01:27:52.589828 140269538244352 logging_writer.py:48] [74] global_step=74, grad_norm=0.03149544447660446, loss=0.12631861865520477
I1004 01:27:53.342374 140269529851648 logging_writer.py:48] [75] global_step=75, grad_norm=0.034983038902282715, loss=0.12568287551403046
I1004 01:27:54.102456 140269538244352 logging_writer.py:48] [76] global_step=76, grad_norm=0.03525632992386818, loss=0.13191479444503784
I1004 01:27:54.893558 140269529851648 logging_writer.py:48] [77] global_step=77, grad_norm=0.04694107174873352, loss=0.13655546307563782
I1004 01:27:55.660456 140269538244352 logging_writer.py:48] [78] global_step=78, grad_norm=0.06970394402742386, loss=0.13677549362182617
I1004 01:27:56.480073 140269529851648 logging_writer.py:48] [79] global_step=79, grad_norm=0.09902409464120865, loss=0.13876964151859283
I1004 01:27:57.223329 140269538244352 logging_writer.py:48] [80] global_step=80, grad_norm=0.13062499463558197, loss=0.13862520456314087
I1004 01:27:57.960503 140269529851648 logging_writer.py:48] [81] global_step=81, grad_norm=0.1400437206029892, loss=0.13630232214927673
I1004 01:27:58.743479 140269538244352 logging_writer.py:48] [82] global_step=82, grad_norm=0.12561017274856567, loss=0.13617104291915894
I1004 01:27:59.504439 140269529851648 logging_writer.py:48] [83] global_step=83, grad_norm=0.11116792261600494, loss=0.13828110694885254
I1004 01:28:00.288172 140269538244352 logging_writer.py:48] [84] global_step=84, grad_norm=0.09562622755765915, loss=0.1350465714931488
I1004 01:28:01.229675 140269529851648 logging_writer.py:48] [85] global_step=85, grad_norm=0.06812407076358795, loss=0.13573499023914337
I1004 01:28:02.121282 140269538244352 logging_writer.py:48] [86] global_step=86, grad_norm=0.044932663440704346, loss=0.13331453502178192
I1004 01:28:02.737789 140269529851648 logging_writer.py:48] [87] global_step=87, grad_norm=0.031533777713775635, loss=0.13462264835834503
I1004 01:28:03.508747 140269538244352 logging_writer.py:48] [88] global_step=88, grad_norm=0.01127352099865675, loss=0.1374034434556961
I1004 01:28:04.308768 140269529851648 logging_writer.py:48] [89] global_step=89, grad_norm=0.01207659300416708, loss=0.1334155797958374
I1004 01:28:05.016501 140269538244352 logging_writer.py:48] [90] global_step=90, grad_norm=0.01757200062274933, loss=0.1347033977508545
I1004 01:28:05.897962 140269529851648 logging_writer.py:48] [91] global_step=91, grad_norm=0.013347725383937359, loss=0.13373267650604248
I1004 01:28:06.647934 140269538244352 logging_writer.py:48] [92] global_step=92, grad_norm=0.004476621281355619, loss=0.13295798003673553
I1004 01:28:07.377644 140269529851648 logging_writer.py:48] [93] global_step=93, grad_norm=0.016677331179380417, loss=0.13594385981559753
I1004 01:28:08.165548 140269538244352 logging_writer.py:48] [94] global_step=94, grad_norm=0.022383345291018486, loss=0.13543111085891724
I1004 01:28:08.952312 140269529851648 logging_writer.py:48] [95] global_step=95, grad_norm=0.0061626797541975975, loss=0.1276758760213852
I1004 01:28:09.676091 140269538244352 logging_writer.py:48] [96] global_step=96, grad_norm=0.021171992644667625, loss=0.1287066638469696
I1004 01:28:10.444087 140269529851648 logging_writer.py:48] [97] global_step=97, grad_norm=0.02980106510221958, loss=0.1265428364276886
I1004 01:28:11.285149 140269538244352 logging_writer.py:48] [98] global_step=98, grad_norm=0.044936228543519974, loss=0.12691251933574677
I1004 01:28:12.028049 140269529851648 logging_writer.py:48] [99] global_step=99, grad_norm=0.044466324150562286, loss=0.12831871211528778
I1004 01:28:12.719500 140269538244352 logging_writer.py:48] [100] global_step=100, grad_norm=0.04207853600382805, loss=0.126413956284523
I1004 01:33:15.291553 140269529851648 logging_writer.py:48] [500] global_step=500, grad_norm=0.004732715897262096, loss=0.1219678670167923
I1004 01:39:30.282899 140269538244352 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.004128343891352415, loss=0.13043338060379028
I1004 01:45:45.294802 140269529851648 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.012864351272583008, loss=0.11800715327262878
I1004 01:47:14.071464 140453294417728 spec.py:321] Evaluating on the training split.
I1004 01:50:33.874192 140453294417728 spec.py:333] Evaluating on the validation split.
I1004 01:53:33.962022 140453294417728 spec.py:349] Evaluating on the test split.
I1004 01:56:53.370094 140453294417728 submission_runner.py:381] Time since start: 2556.71s, 	Step: 1620, 	{'train/loss': 0.1264975205907282, 'validation/loss': 0.12613019255790933, 'validation/num_examples': 83274637, 'test/loss': 0.1297762947368421, 'test/num_examples': 95000000, 'score': 1225.157609462738, 'total_duration': 2556.7087519168854, 'accumulated_submission_time': 1225.157609462738, 'accumulated_eval_time': 1331.4994020462036, 'accumulated_logging_time': 0.027962207794189453}
I1004 01:56:53.385445 140269538244352 logging_writer.py:48] [1620] accumulated_eval_time=1331.499402, accumulated_logging_time=0.027962, accumulated_submission_time=1225.157609, global_step=1620, preemption_count=0, score=1225.157609, test/loss=0.129776, test/num_examples=95000000, total_duration=2556.708752, train/loss=0.126498, validation/loss=0.126130, validation/num_examples=83274637
I1004 02:01:18.653782 140269529851648 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.004609334748238325, loss=0.12008551508188248
I1004 02:07:32.234908 140269538244352 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.012845308519899845, loss=0.12319689989089966
I1004 02:13:50.265724 140269529851648 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.012947767972946167, loss=0.123883917927742
I1004 02:16:53.458065 140453294417728 spec.py:321] Evaluating on the training split.
I1004 02:20:04.197237 140453294417728 spec.py:333] Evaluating on the validation split.
I1004 02:23:11.212101 140453294417728 spec.py:349] Evaluating on the test split.
I1004 02:26:33.921535 140453294417728 submission_runner.py:381] Time since start: 4337.26s, 	Step: 3250, 	{'train/loss': 0.12344378345417527, 'validation/loss': 0.12471875440297626, 'validation/num_examples': 83274637, 'test/loss': 0.12709486315789473, 'test/num_examples': 95000000, 'score': 2425.198889017105, 'total_duration': 4337.260200500488, 'accumulated_submission_time': 2425.198889017105, 'accumulated_eval_time': 1911.9628460407257, 'accumulated_logging_time': 0.05051231384277344}
I1004 02:26:33.934708 140269538244352 logging_writer.py:48] [3250] accumulated_eval_time=1911.962846, accumulated_logging_time=0.050512, accumulated_submission_time=2425.198889, global_step=3250, preemption_count=0, score=2425.198889, test/loss=0.127095, test/num_examples=95000000, total_duration=4337.260201, train/loss=0.123444, validation/loss=0.124719, validation/num_examples=83274637
I1004 02:29:23.908842 140269529851648 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.012225130572915077, loss=0.12452860921621323
I1004 02:35:38.185257 140269538244352 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.006615357473492622, loss=0.12338750809431076
I1004 02:41:53.995836 140269529851648 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.005815747193992138, loss=0.12149260938167572
I1004 02:46:34.579594 140453294417728 spec.py:321] Evaluating on the training split.
I1004 02:49:43.977386 140453294417728 spec.py:333] Evaluating on the validation split.
I1004 02:52:42.121104 140453294417728 spec.py:349] Evaluating on the test split.
I1004 02:56:17.062374 140453294417728 submission_runner.py:381] Time since start: 6120.40s, 	Step: 4870, 	{'train/loss': 0.12390645345052083, 'validation/loss': 0.12434744086605866, 'validation/num_examples': 83274637, 'test/loss': 0.1268364, 'test/num_examples': 95000000, 'score': 3625.8127720355988, 'total_duration': 6120.401033878326, 'accumulated_submission_time': 3625.8127720355988, 'accumulated_eval_time': 2494.445590019226, 'accumulated_logging_time': 0.07065129280090332}
I1004 02:56:17.079037 140269538244352 logging_writer.py:48] [4870] accumulated_eval_time=2494.445590, accumulated_logging_time=0.070651, accumulated_submission_time=3625.812772, global_step=4870, preemption_count=0, score=3625.812772, test/loss=0.126836, test/num_examples=95000000, total_duration=6120.401034, train/loss=0.123906, validation/loss=0.124347, validation/num_examples=83274637
I1004 02:57:37.850157 140269529851648 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.008406700566411018, loss=0.12111742049455643
I1004 03:03:54.488183 140269538244352 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.00502821896225214, loss=0.11970911920070648
I1004 03:10:04.896583 140269529851648 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.008988659828901291, loss=0.12722299993038177
I1004 03:16:16.884378 140269538244352 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.006103475112468004, loss=0.1282009780406952
I1004 03:16:17.525414 140453294417728 spec.py:321] Evaluating on the training split.
I1004 03:19:07.786587 140453294417728 spec.py:333] Evaluating on the validation split.
I1004 03:21:53.052359 140453294417728 spec.py:349] Evaluating on the test split.
I1004 03:25:15.001885 140453294417728 submission_runner.py:381] Time since start: 7858.34s, 	Step: 6502, 	{'train/loss': 0.12035669770630651, 'validation/loss': 0.12390424469817864, 'validation/num_examples': 83274637, 'test/loss': 0.12629908421052632, 'test/num_examples': 95000000, 'score': 4826.228189945221, 'total_duration': 7858.340544939041, 'accumulated_submission_time': 4826.228189945221, 'accumulated_eval_time': 3031.921991586685, 'accumulated_logging_time': 0.09451866149902344}
I1004 03:25:15.019027 140269529851648 logging_writer.py:48] [6502] accumulated_eval_time=3031.921992, accumulated_logging_time=0.094519, accumulated_submission_time=4826.228190, global_step=6502, preemption_count=0, score=4826.228190, test/loss=0.126299, test/num_examples=95000000, total_duration=7858.340545, train/loss=0.120357, validation/loss=0.123904, validation/num_examples=83274637
I1004 03:31:07.629122 140269538244352 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.006721837911754847, loss=0.1231197714805603
I1004 03:37:25.135048 140269529851648 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.008709426037967205, loss=0.11671601235866547
I1004 03:43:34.165926 140453294417728 spec.py:321] Evaluating on the training split.
I1004 03:46:09.395638 140453294417728 spec.py:333] Evaluating on the validation split.
I1004 03:48:22.630983 140453294417728 spec.py:349] Evaluating on the test split.
I1004 03:50:47.837964 140453294417728 submission_runner.py:381] Time since start: 9391.18s, 	Step: 8000, 	{'train/loss': 0.12156195010779039, 'validation/loss': 0.12372189625996208, 'validation/num_examples': 83274637, 'test/loss': 0.12604096842105264, 'test/num_examples': 95000000, 'score': 5925.340941429138, 'total_duration': 9391.176621437073, 'accumulated_submission_time': 5925.340941429138, 'accumulated_eval_time': 3465.593977212906, 'accumulated_logging_time': 0.1237952709197998}
I1004 03:50:47.857481 140269538244352 logging_writer.py:48] [8000] accumulated_eval_time=3465.593977, accumulated_logging_time=0.123795, accumulated_submission_time=5925.340941, global_step=8000, preemption_count=0, score=5925.340941, test/loss=0.126041, test/num_examples=95000000, total_duration=9391.176621, train/loss=0.121562, validation/loss=0.123722, validation/num_examples=83274637
I1004 03:50:47.876913 140269529851648 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5925.340941
I1004 03:50:53.396723 140453294417728 checkpoints.py:490] Saving checkpoint at step: 8000
I1004 03:51:28.895970 140453294417728 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_2/criteo1tb_jax/trial_1/checkpoint_8000
I1004 03:51:29.236343 140453294417728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_2/criteo1tb_jax/trial_1/checkpoint_8000.
I1004 03:51:29.622431 140453294417728 submission_runner.py:549] Tuning trial 1/1
I1004 03:51:29.622689 140453294417728 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1004 03:51:29.623830 140453294417728 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.21765304661396914, 'validation/loss': 0.21864044871189292, 'validation/num_examples': 83274637, 'test/loss': 0.21941926315789473, 'test/num_examples': 95000000, 'score': 25.0552339553833, 'total_duration': 777.2560930252075, 'accumulated_submission_time': 25.0552339553833, 'accumulated_eval_time': 752.2008173465729, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1620, {'train/loss': 0.1264975205907282, 'validation/loss': 0.12613019255790933, 'validation/num_examples': 83274637, 'test/loss': 0.1297762947368421, 'test/num_examples': 95000000, 'score': 1225.157609462738, 'total_duration': 2556.7087519168854, 'accumulated_submission_time': 1225.157609462738, 'accumulated_eval_time': 1331.4994020462036, 'accumulated_logging_time': 0.027962207794189453, 'global_step': 1620, 'preemption_count': 0}), (3250, {'train/loss': 0.12344378345417527, 'validation/loss': 0.12471875440297626, 'validation/num_examples': 83274637, 'test/loss': 0.12709486315789473, 'test/num_examples': 95000000, 'score': 2425.198889017105, 'total_duration': 4337.260200500488, 'accumulated_submission_time': 2425.198889017105, 'accumulated_eval_time': 1911.9628460407257, 'accumulated_logging_time': 0.05051231384277344, 'global_step': 3250, 'preemption_count': 0}), (4870, {'train/loss': 0.12390645345052083, 'validation/loss': 0.12434744086605866, 'validation/num_examples': 83274637, 'test/loss': 0.1268364, 'test/num_examples': 95000000, 'score': 3625.8127720355988, 'total_duration': 6120.401033878326, 'accumulated_submission_time': 3625.8127720355988, 'accumulated_eval_time': 2494.445590019226, 'accumulated_logging_time': 0.07065129280090332, 'global_step': 4870, 'preemption_count': 0}), (6502, {'train/loss': 0.12035669770630651, 'validation/loss': 0.12390424469817864, 'validation/num_examples': 83274637, 'test/loss': 0.12629908421052632, 'test/num_examples': 95000000, 'score': 4826.228189945221, 'total_duration': 7858.340544939041, 'accumulated_submission_time': 4826.228189945221, 'accumulated_eval_time': 3031.921991586685, 'accumulated_logging_time': 0.09451866149902344, 'global_step': 6502, 'preemption_count': 0}), (8000, {'train/loss': 0.12156195010779039, 'validation/loss': 0.12372189625996208, 'validation/num_examples': 83274637, 'test/loss': 0.12604096842105264, 'test/num_examples': 95000000, 'score': 5925.340941429138, 'total_duration': 9391.176621437073, 'accumulated_submission_time': 5925.340941429138, 'accumulated_eval_time': 3465.593977212906, 'accumulated_logging_time': 0.1237952709197998, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1004 03:51:29.623977 140453294417728 submission_runner.py:552] Timing: 5925.340941429138
I1004 03:51:29.624029 140453294417728 submission_runner.py:554] Total number of evals: 6
I1004 03:51:29.624077 140453294417728 submission_runner.py:555] ====================
I1004 03:51:29.624185 140453294417728 submission_runner.py:625] Final criteo1tb score: 5925.340941429138
