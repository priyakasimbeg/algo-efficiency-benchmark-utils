python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_1 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-03-2023-22-38-08.log
2023-10-03 22:38:13.137975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1003 22:38:29.797005 139783731337024 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_1/criteo1tb_jax.
I1003 22:38:31.392361 139783731337024 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1003 22:38:31.393189 139783731337024 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1003 22:38:31.393467 139783731337024 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1003 22:38:31.398872 139783731337024 submission_runner.py:507] Using RNG seed 3228868382
I1003 22:38:36.988869 139783731337024 submission_runner.py:516] --- Tuning run 1/1 ---
I1003 22:38:36.989259 139783731337024 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_1/criteo1tb_jax/trial_1.
I1003 22:38:36.989713 139783731337024 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_1/criteo1tb_jax/trial_1/hparams.json.
I1003 22:38:37.175131 139783731337024 submission_runner.py:191] Initializing dataset.
I1003 22:38:37.175389 139783731337024 submission_runner.py:198] Initializing model.
I1003 22:38:43.088776 139783731337024 submission_runner.py:232] Initializing optimizer.
I1003 22:38:46.154774 139783731337024 submission_runner.py:239] Initializing metrics bundle.
I1003 22:38:46.155072 139783731337024 submission_runner.py:257] Initializing checkpoint and logger.
I1003 22:38:46.156538 139783731337024 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_1/criteo1tb_jax/trial_1 with prefix checkpoint_
I1003 22:38:46.156728 139783731337024 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_1/criteo1tb_jax/trial_1/meta_data_0.json.
I1003 22:38:46.156988 139783731337024 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1003 22:38:46.157069 139783731337024 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1003 22:38:46.950871 139783731337024 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_1/criteo1tb_jax/trial_1/flags_0.json.
I1003 22:38:47.047094 139783731337024 submission_runner.py:290] Starting training loop.
I1003 22:39:14.411561 139619413780224 logging_writer.py:48] [0] global_step=0, grad_norm=5.0534987449646, loss=0.38632485270500183
I1003 22:39:14.423593 139783731337024 spec.py:321] Evaluating on the training split.
I1003 22:43:03.702869 139783731337024 spec.py:333] Evaluating on the validation split.
I1003 22:46:59.195645 139783731337024 spec.py:349] Evaluating on the test split.
I1003 22:51:16.578967 139783731337024 submission_runner.py:381] Time since start: 749.53s, 	Step: 1, 	{'train/loss': 0.3849641811922661, 'validation/loss': 0.38433416407447085, 'validation/num_examples': 83274637, 'test/loss': 0.38555654736842104, 'test/num_examples': 95000000, 'score': 27.37646770477295, 'total_duration': 749.5318112373352, 'accumulated_submission_time': 27.37646770477295, 'accumulated_eval_time': 722.1553020477295, 'accumulated_logging_time': 0}
I1003 22:51:16.601035 139597711406848 logging_writer.py:48] [1] accumulated_eval_time=722.155302, accumulated_logging_time=0, accumulated_submission_time=27.376468, global_step=1, preemption_count=0, score=27.376468, test/loss=0.385557, test/num_examples=95000000, total_duration=749.531811, train/loss=0.384964, validation/loss=0.384334, validation/num_examples=83274637
I1003 22:51:16.714306 139597703014144 logging_writer.py:48] [1] global_step=1, grad_norm=5.039124488830566, loss=0.3867447078227997
I1003 22:51:16.819144 139597711406848 logging_writer.py:48] [2] global_step=2, grad_norm=3.9856250286102295, loss=0.3323694169521332
I1003 22:51:16.923276 139597703014144 logging_writer.py:48] [3] global_step=3, grad_norm=2.697694778442383, loss=0.2672979235649109
I1003 22:51:17.026255 139597711406848 logging_writer.py:48] [4] global_step=4, grad_norm=1.6550003290176392, loss=0.21212805807590485
I1003 22:51:17.128480 139597703014144 logging_writer.py:48] [5] global_step=5, grad_norm=0.8514480590820312, loss=0.1776224672794342
I1003 22:51:17.231200 139597711406848 logging_writer.py:48] [6] global_step=6, grad_norm=0.32644912600517273, loss=0.15923617780208588
I1003 22:51:17.334422 139597703014144 logging_writer.py:48] [7] global_step=7, grad_norm=0.1594121754169464, loss=0.1582089364528656
I1003 22:51:17.438668 139597711406848 logging_writer.py:48] [8] global_step=8, grad_norm=0.4383605420589447, loss=0.1620095670223236
I1003 22:51:17.542321 139597703014144 logging_writer.py:48] [9] global_step=9, grad_norm=0.6190726161003113, loss=0.16940544545650482
I1003 22:51:17.645787 139597711406848 logging_writer.py:48] [10] global_step=10, grad_norm=0.7295931577682495, loss=0.17719107866287231
I1003 22:51:17.748349 139597703014144 logging_writer.py:48] [11] global_step=11, grad_norm=0.7244188189506531, loss=0.17612962424755096
I1003 22:51:17.850932 139597711406848 logging_writer.py:48] [12] global_step=12, grad_norm=0.6647006869316101, loss=0.17109669744968414
I1003 22:51:17.953537 139597703014144 logging_writer.py:48] [13] global_step=13, grad_norm=0.5955788493156433, loss=0.16798312962055206
I1003 22:51:18.056626 139597711406848 logging_writer.py:48] [14] global_step=14, grad_norm=0.4521912932395935, loss=0.15847989916801453
I1003 22:51:18.160569 139597703014144 logging_writer.py:48] [15] global_step=15, grad_norm=0.316379576921463, loss=0.15566708147525787
I1003 22:51:18.264149 139597711406848 logging_writer.py:48] [16] global_step=16, grad_norm=0.1446419358253479, loss=0.1503811776638031
I1003 22:51:18.367590 139597703014144 logging_writer.py:48] [17] global_step=17, grad_norm=0.060020849108695984, loss=0.1463460475206375
I1003 22:51:18.470486 139597711406848 logging_writer.py:48] [18] global_step=18, grad_norm=0.11931132525205612, loss=0.14835238456726074
I1003 22:51:18.574550 139597703014144 logging_writer.py:48] [19] global_step=19, grad_norm=0.14670653641223907, loss=0.14460937678813934
I1003 22:51:18.680645 139597711406848 logging_writer.py:48] [20] global_step=20, grad_norm=0.09354463964700699, loss=0.14032305777072906
I1003 22:51:18.785343 139597703014144 logging_writer.py:48] [21] global_step=21, grad_norm=0.04221726581454277, loss=0.14288626611232758
I1003 22:51:18.889985 139597711406848 logging_writer.py:48] [22] global_step=22, grad_norm=0.06595556437969208, loss=0.1408662348985672
I1003 22:51:18.995251 139597703014144 logging_writer.py:48] [23] global_step=23, grad_norm=0.039003632962703705, loss=0.1391829550266266
I1003 22:51:19.104521 139597711406848 logging_writer.py:48] [24] global_step=24, grad_norm=0.02575068362057209, loss=0.13946495950222015
I1003 22:51:19.208711 139597703014144 logging_writer.py:48] [25] global_step=25, grad_norm=0.01888318359851837, loss=0.1364360749721527
I1003 22:51:19.313057 139597711406848 logging_writer.py:48] [26] global_step=26, grad_norm=0.0282698106020689, loss=0.13842834532260895
I1003 22:51:19.419160 139597703014144 logging_writer.py:48] [27] global_step=27, grad_norm=0.015154991298913956, loss=0.14030598104000092
I1003 22:51:19.526505 139597711406848 logging_writer.py:48] [28] global_step=28, grad_norm=0.015112345106899738, loss=0.13900217413902283
I1003 22:51:20.254982 139597703014144 logging_writer.py:48] [29] global_step=29, grad_norm=0.018760710954666138, loss=0.13896667957305908
I1003 22:51:20.907799 139597711406848 logging_writer.py:48] [30] global_step=30, grad_norm=0.021297017112374306, loss=0.13746975362300873
I1003 22:51:21.573323 139597703014144 logging_writer.py:48] [31] global_step=31, grad_norm=0.042147260159254074, loss=0.13777709007263184
I1003 22:51:22.440219 139597711406848 logging_writer.py:48] [32] global_step=32, grad_norm=0.06773725897073746, loss=0.1352546364068985
I1003 22:51:23.035894 139597703014144 logging_writer.py:48] [33] global_step=33, grad_norm=0.08209196478128433, loss=0.13474728167057037
I1003 22:51:23.785910 139597711406848 logging_writer.py:48] [34] global_step=34, grad_norm=0.05133707821369171, loss=0.13803640007972717
I1003 22:51:24.568641 139597703014144 logging_writer.py:48] [35] global_step=35, grad_norm=0.029521672055125237, loss=0.13556192815303802
I1003 22:51:25.294105 139597711406848 logging_writer.py:48] [36] global_step=36, grad_norm=0.014922644942998886, loss=0.13772420585155487
I1003 22:51:26.032791 139597703014144 logging_writer.py:48] [37] global_step=37, grad_norm=0.01084077451378107, loss=0.13463044166564941
I1003 22:51:26.814010 139597711406848 logging_writer.py:48] [38] global_step=38, grad_norm=0.010777365416288376, loss=0.13146239519119263
I1003 22:51:27.548410 139597703014144 logging_writer.py:48] [39] global_step=39, grad_norm=0.012156668119132519, loss=0.13558441400527954
I1003 22:51:28.419706 139597711406848 logging_writer.py:48] [40] global_step=40, grad_norm=0.02975848689675331, loss=0.1385694146156311
I1003 22:51:29.026893 139597703014144 logging_writer.py:48] [41] global_step=41, grad_norm=0.11430829763412476, loss=0.14030048251152039
I1003 22:51:29.855516 139597711406848 logging_writer.py:48] [42] global_step=42, grad_norm=0.388752818107605, loss=0.14684104919433594
I1003 22:51:30.654085 139597703014144 logging_writer.py:48] [43] global_step=43, grad_norm=0.4253702759742737, loss=0.14957112073898315
I1003 22:51:31.308343 139597711406848 logging_writer.py:48] [44] global_step=44, grad_norm=0.10178437829017639, loss=0.1407860368490219
I1003 22:51:32.162829 139597703014144 logging_writer.py:48] [45] global_step=45, grad_norm=0.03657097741961479, loss=0.14045904576778412
I1003 22:51:32.840663 139597711406848 logging_writer.py:48] [46] global_step=46, grad_norm=0.011088771745562553, loss=0.1396384984254837
I1003 22:51:33.590647 139597703014144 logging_writer.py:48] [47] global_step=47, grad_norm=0.010993903502821922, loss=0.13913387060165405
I1003 22:51:34.365303 139597711406848 logging_writer.py:48] [48] global_step=48, grad_norm=0.011427857913076878, loss=0.1391625702381134
I1003 22:51:35.136644 139597703014144 logging_writer.py:48] [49] global_step=49, grad_norm=0.00885017029941082, loss=0.1398521214723587
I1003 22:51:35.847720 139597711406848 logging_writer.py:48] [50] global_step=50, grad_norm=0.013269237242639065, loss=0.13932780921459198
I1003 22:51:36.768594 139597703014144 logging_writer.py:48] [51] global_step=51, grad_norm=0.01743871532380581, loss=0.1383841335773468
I1003 22:51:37.328159 139597711406848 logging_writer.py:48] [52] global_step=52, grad_norm=0.027525940909981728, loss=0.14011834561824799
I1003 22:51:38.086390 139597703014144 logging_writer.py:48] [53] global_step=53, grad_norm=0.044452402740716934, loss=0.13787941634655
I1003 22:51:38.888371 139597711406848 logging_writer.py:48] [54] global_step=54, grad_norm=0.09308930486440659, loss=0.13475444912910461
I1003 22:51:39.592743 139597703014144 logging_writer.py:48] [55] global_step=55, grad_norm=0.1479625701904297, loss=0.1391262412071228
I1003 22:51:40.260754 139597711406848 logging_writer.py:48] [56] global_step=56, grad_norm=0.1243797168135643, loss=0.13846223056316376
I1003 22:51:40.948339 139597703014144 logging_writer.py:48] [57] global_step=57, grad_norm=0.06977935880422592, loss=0.13132649660110474
I1003 22:51:41.668456 139597711406848 logging_writer.py:48] [58] global_step=58, grad_norm=0.037513941526412964, loss=0.1288333237171173
I1003 22:51:42.339749 139597703014144 logging_writer.py:48] [59] global_step=59, grad_norm=0.031459011137485504, loss=0.12864524126052856
I1003 22:51:43.229658 139597711406848 logging_writer.py:48] [60] global_step=60, grad_norm=0.021997720003128052, loss=0.12711919844150543
I1003 22:51:43.836724 139597703014144 logging_writer.py:48] [61] global_step=61, grad_norm=0.014545129612088203, loss=0.1287824958562851
I1003 22:51:44.591888 139597711406848 logging_writer.py:48] [62] global_step=62, grad_norm=0.009708280675113201, loss=0.1268409937620163
I1003 22:51:45.306817 139597703014144 logging_writer.py:48] [63] global_step=63, grad_norm=0.016036460176110268, loss=0.12752459943294525
I1003 22:51:45.975344 139597711406848 logging_writer.py:48] [64] global_step=64, grad_norm=0.033911511301994324, loss=0.12550541758537292
I1003 22:51:46.592363 139597703014144 logging_writer.py:48] [65] global_step=65, grad_norm=0.04565270617604256, loss=0.1263626366853714
I1003 22:51:47.284048 139597711406848 logging_writer.py:48] [66] global_step=66, grad_norm=0.05013907700777054, loss=0.12452942132949829
I1003 22:51:48.055626 139597703014144 logging_writer.py:48] [67] global_step=67, grad_norm=0.053236301988363266, loss=0.12473040819168091
I1003 22:51:48.703548 139597711406848 logging_writer.py:48] [68] global_step=68, grad_norm=0.05021275579929352, loss=0.12533783912658691
I1003 22:51:49.364635 139597703014144 logging_writer.py:48] [69] global_step=69, grad_norm=0.054558951407670975, loss=0.12557193636894226
I1003 22:51:50.095012 139597711406848 logging_writer.py:48] [70] global_step=70, grad_norm=0.056877944618463516, loss=0.12549449503421783
I1003 22:51:50.900280 139597703014144 logging_writer.py:48] [71] global_step=71, grad_norm=0.06541579961776733, loss=0.12728962302207947
I1003 22:51:51.624899 139597711406848 logging_writer.py:48] [72] global_step=72, grad_norm=0.08599290251731873, loss=0.12448514997959137
I1003 22:51:52.299818 139597703014144 logging_writer.py:48] [73] global_step=73, grad_norm=0.10159936547279358, loss=0.1260758638381958
I1003 22:51:53.052042 139597711406848 logging_writer.py:48] [74] global_step=74, grad_norm=0.09498385339975357, loss=0.12588049471378326
I1003 22:51:53.698014 139597703014144 logging_writer.py:48] [75] global_step=75, grad_norm=0.08142327517271042, loss=0.12604567408561707
I1003 22:51:54.522646 139597711406848 logging_writer.py:48] [76] global_step=76, grad_norm=0.06460757553577423, loss=0.12919503450393677
I1003 22:51:55.139446 139597703014144 logging_writer.py:48] [77] global_step=77, grad_norm=0.04868554323911667, loss=0.12975366413593292
I1003 22:51:55.895339 139597711406848 logging_writer.py:48] [78] global_step=78, grad_norm=0.03979867324233055, loss=0.1279514580965042
I1003 22:51:56.602613 139597703014144 logging_writer.py:48] [79] global_step=79, grad_norm=0.033396705985069275, loss=0.127915620803833
I1003 22:51:57.239934 139597711406848 logging_writer.py:48] [80] global_step=80, grad_norm=0.033750392496585846, loss=0.12602576613426208
I1003 22:51:57.924333 139597703014144 logging_writer.py:48] [81] global_step=81, grad_norm=0.031098825857043266, loss=0.12577298283576965
I1003 22:51:58.683164 139597711406848 logging_writer.py:48] [82] global_step=82, grad_norm=0.015071879141032696, loss=0.12811057269573212
I1003 22:51:59.382095 139597703014144 logging_writer.py:48] [83] global_step=83, grad_norm=0.007702821400016546, loss=0.1277548372745514
I1003 22:52:00.069741 139597711406848 logging_writer.py:48] [84] global_step=84, grad_norm=0.012797229923307896, loss=0.12595078349113464
I1003 22:52:00.803775 139597703014144 logging_writer.py:48] [85] global_step=85, grad_norm=0.045300472527742386, loss=0.1297391951084137
I1003 22:52:01.556610 139597711406848 logging_writer.py:48] [86] global_step=86, grad_norm=0.08713259547948837, loss=0.12787897884845734
I1003 22:52:02.378633 139597703014144 logging_writer.py:48] [87] global_step=87, grad_norm=0.12473469972610474, loss=0.12904581427574158
I1003 22:52:03.064690 139597711406848 logging_writer.py:48] [88] global_step=88, grad_norm=0.15075775980949402, loss=0.12880191206932068
I1003 22:52:03.677405 139597703014144 logging_writer.py:48] [89] global_step=89, grad_norm=0.16067646443843842, loss=0.12960226833820343
I1003 22:52:04.390988 139597711406848 logging_writer.py:48] [90] global_step=90, grad_norm=0.14029577374458313, loss=0.13002678751945496
I1003 22:52:05.107040 139597703014144 logging_writer.py:48] [91] global_step=91, grad_norm=0.10481678694486618, loss=0.12762518227100372
I1003 22:52:05.837058 139597711406848 logging_writer.py:48] [92] global_step=92, grad_norm=0.0708235427737236, loss=0.12778335809707642
I1003 22:52:06.545062 139597703014144 logging_writer.py:48] [93] global_step=93, grad_norm=0.052708983421325684, loss=0.12780830264091492
I1003 22:52:07.204510 139597711406848 logging_writer.py:48] [94] global_step=94, grad_norm=0.0407293364405632, loss=0.12766890227794647
I1003 22:52:07.933909 139597703014144 logging_writer.py:48] [95] global_step=95, grad_norm=0.025220070034265518, loss=0.1324998438358307
I1003 22:52:08.696354 139597711406848 logging_writer.py:48] [96] global_step=96, grad_norm=0.01657920517027378, loss=0.13492444157600403
I1003 22:52:09.383563 139597703014144 logging_writer.py:48] [97] global_step=97, grad_norm=0.019941505044698715, loss=0.13536429405212402
I1003 22:52:10.069024 139597711406848 logging_writer.py:48] [98] global_step=98, grad_norm=0.03554593399167061, loss=0.1317603439092636
I1003 22:52:10.811908 139597703014144 logging_writer.py:48] [99] global_step=99, grad_norm=0.04450756683945656, loss=0.1332046389579773
I1003 22:52:11.567551 139597711406848 logging_writer.py:48] [100] global_step=100, grad_norm=0.05668923631310463, loss=0.13126592338085175
I1003 22:56:57.047033 139597703014144 logging_writer.py:48] [500] global_step=500, grad_norm=0.0043134624138474464, loss=0.12178481370210648
I1003 23:02:45.541237 139597711406848 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.018214326351881027, loss=0.12080585211515427
I1003 23:08:34.846923 139597703014144 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.02163565531373024, loss=0.11967600882053375
I1003 23:11:17.062726 139783731337024 spec.py:321] Evaluating on the training split.
I1003 23:14:20.449756 139783731337024 spec.py:333] Evaluating on the validation split.
I1003 23:17:28.502579 139783731337024 spec.py:349] Evaluating on the test split.
I1003 23:21:07.479856 139783731337024 submission_runner.py:381] Time since start: 2540.43s, 	Step: 1733, 	{'train/loss': 0.1250518942778965, 'validation/loss': 0.12556500246287475, 'validation/num_examples': 83274637, 'test/loss': 0.12790066315789475, 'test/num_examples': 95000000, 'score': 1227.8081755638123, 'total_duration': 2540.432702064514, 'accumulated_submission_time': 1227.8081755638123, 'accumulated_eval_time': 1312.5723872184753, 'accumulated_logging_time': 0.029822587966918945}
I1003 23:21:07.496350 139597711406848 logging_writer.py:48] [1733] accumulated_eval_time=1312.572387, accumulated_logging_time=0.029823, accumulated_submission_time=1227.808176, global_step=1733, preemption_count=0, score=1227.808176, test/loss=0.127901, test/num_examples=95000000, total_duration=2540.432702, train/loss=0.125052, validation/loss=0.125565, validation/num_examples=83274637
I1003 23:24:01.597280 139597703014144 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.014652900397777557, loss=0.12149707973003387
I1003 23:29:57.231305 139597711406848 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.005597215611487627, loss=0.1199793666601181
I1003 23:35:51.700148 139597703014144 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.019279373809695244, loss=0.11586127430200577
I1003 23:41:07.983992 139783731337024 spec.py:321] Evaluating on the training split.
I1003 23:44:06.057558 139783731337024 spec.py:333] Evaluating on the validation split.
I1003 23:47:11.944655 139783731337024 spec.py:349] Evaluating on the test split.
I1003 23:50:47.433339 139783731337024 submission_runner.py:381] Time since start: 4320.39s, 	Step: 3442, 	{'train/loss': 0.1242570457218578, 'validation/loss': 0.12462983176978604, 'validation/num_examples': 83274637, 'test/loss': 0.12700197894736842, 'test/num_examples': 95000000, 'score': 2428.2655460834503, 'total_duration': 4320.386169672012, 'accumulated_submission_time': 2428.2655460834503, 'accumulated_eval_time': 1892.0216739177704, 'accumulated_logging_time': 0.05432391166687012}
I1003 23:50:47.447450 139597711406848 logging_writer.py:48] [3442] accumulated_eval_time=1892.021674, accumulated_logging_time=0.054324, accumulated_submission_time=2428.265546, global_step=3442, preemption_count=0, score=2428.265546, test/loss=0.127002, test/num_examples=95000000, total_duration=4320.386170, train/loss=0.124257, validation/loss=0.124630, validation/num_examples=83274637
I1003 23:51:13.023375 139597703014144 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.019588053226470947, loss=0.11908979713916779
I1003 23:57:07.189431 139597711406848 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.004332310985773802, loss=0.1218516007065773
I1004 00:03:04.002432 139597703014144 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0043373932130634785, loss=0.1198437511920929
I1004 00:08:59.043247 139597711406848 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.017520828172564507, loss=0.11859579384326935
I1004 00:10:48.031593 139783731337024 spec.py:321] Evaluating on the training split.
I1004 00:13:43.247315 139783731337024 spec.py:333] Evaluating on the validation split.
I1004 00:16:43.864119 139783731337024 spec.py:349] Evaluating on the test split.
I1004 00:20:14.047564 139783731337024 submission_runner.py:381] Time since start: 6087.00s, 	Step: 5155, 	{'train/loss': 0.12209507204451651, 'validation/loss': 0.12417712490298817, 'validation/num_examples': 83274637, 'test/loss': 0.12659117894736843, 'test/num_examples': 95000000, 'score': 3628.8210110664368, 'total_duration': 6087.000399589539, 'accumulated_submission_time': 3628.8210110664368, 'accumulated_eval_time': 2458.037598848343, 'accumulated_logging_time': 0.07557845115661621}
I1004 00:20:14.066690 139597703014144 logging_writer.py:48] [5155] accumulated_eval_time=2458.037599, accumulated_logging_time=0.075578, accumulated_submission_time=3628.821011, global_step=5155, preemption_count=0, score=3628.821011, test/loss=0.126591, test/num_examples=95000000, total_duration=6087.000400, train/loss=0.122095, validation/loss=0.124177, validation/num_examples=83274637
I1004 00:24:06.650548 139597711406848 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.003599386429414153, loss=0.11435669660568237
I1004 00:30:04.134784 139597703014144 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0037807063199579716, loss=0.12044248729944229
I1004 00:35:56.349798 139597711406848 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.011054917238652706, loss=0.11989292502403259
I1004 00:40:14.311469 139783731337024 spec.py:321] Evaluating on the training split.
I1004 00:42:59.558046 139783731337024 spec.py:333] Evaluating on the validation split.
I1004 00:45:50.161557 139783731337024 spec.py:349] Evaluating on the test split.
I1004 00:49:10.229261 139783731337024 submission_runner.py:381] Time since start: 7823.18s, 	Step: 6865, 	{'train/loss': 0.12266618500715532, 'validation/loss': 0.12375369465735407, 'validation/num_examples': 83274637, 'test/loss': 0.12614064210526316, 'test/num_examples': 95000000, 'score': 4829.036646127701, 'total_duration': 7823.182115316391, 'accumulated_submission_time': 4829.036646127701, 'accumulated_eval_time': 2993.955353498459, 'accumulated_logging_time': 0.10243535041809082}
I1004 00:49:10.245101 139597703014144 logging_writer.py:48] [6865] accumulated_eval_time=2993.955353, accumulated_logging_time=0.102435, accumulated_submission_time=4829.036646, global_step=6865, preemption_count=0, score=4829.036646, test/loss=0.126141, test/num_examples=95000000, total_duration=7823.182115, train/loss=0.122666, validation/loss=0.123754, validation/num_examples=83274637
I1004 00:50:31.698976 139597711406848 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0046958038583397865, loss=0.12206815183162689
I1004 00:56:20.829034 139597703014144 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.005776022560894489, loss=0.13225434720516205
I1004 01:02:06.033515 139783731337024 spec.py:321] Evaluating on the training split.
I1004 01:04:28.522819 139783731337024 spec.py:333] Evaluating on the validation split.
I1004 01:06:32.204560 139783731337024 spec.py:349] Evaluating on the test split.
I1004 01:09:30.936966 139783731337024 submission_runner.py:381] Time since start: 9043.89s, 	Step: 8000, 	{'train/loss': 0.1221395768459488, 'validation/loss': 0.12368367333741725, 'validation/num_examples': 83274637, 'test/loss': 0.126021, 'test/num_examples': 95000000, 'score': 5604.8036942481995, 'total_duration': 9043.889823436737, 'accumulated_submission_time': 5604.8036942481995, 'accumulated_eval_time': 3438.8587675094604, 'accumulated_logging_time': 0.125288724899292}
I1004 01:09:30.954060 139597711406848 logging_writer.py:48] [8000] accumulated_eval_time=3438.858768, accumulated_logging_time=0.125289, accumulated_submission_time=5604.803694, global_step=8000, preemption_count=0, score=5604.803694, test/loss=0.126021, test/num_examples=95000000, total_duration=9043.889823, train/loss=0.122140, validation/loss=0.123684, validation/num_examples=83274637
I1004 01:09:30.966977 139597703014144 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5604.803694
I1004 01:09:36.335319 139783731337024 checkpoints.py:490] Saving checkpoint at step: 8000
I1004 01:10:08.710439 139783731337024 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_1/criteo1tb_jax/trial_1/checkpoint_8000
I1004 01:10:08.996928 139783731337024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_1/criteo1tb_jax/trial_1/checkpoint_8000.
I1004 01:10:09.324178 139783731337024 submission_runner.py:549] Tuning trial 1/1
I1004 01:10:09.324434 139783731337024 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1004 01:10:09.325625 139783731337024 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.3849641811922661, 'validation/loss': 0.38433416407447085, 'validation/num_examples': 83274637, 'test/loss': 0.38555654736842104, 'test/num_examples': 95000000, 'score': 27.37646770477295, 'total_duration': 749.5318112373352, 'accumulated_submission_time': 27.37646770477295, 'accumulated_eval_time': 722.1553020477295, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1733, {'train/loss': 0.1250518942778965, 'validation/loss': 0.12556500246287475, 'validation/num_examples': 83274637, 'test/loss': 0.12790066315789475, 'test/num_examples': 95000000, 'score': 1227.8081755638123, 'total_duration': 2540.432702064514, 'accumulated_submission_time': 1227.8081755638123, 'accumulated_eval_time': 1312.5723872184753, 'accumulated_logging_time': 0.029822587966918945, 'global_step': 1733, 'preemption_count': 0}), (3442, {'train/loss': 0.1242570457218578, 'validation/loss': 0.12462983176978604, 'validation/num_examples': 83274637, 'test/loss': 0.12700197894736842, 'test/num_examples': 95000000, 'score': 2428.2655460834503, 'total_duration': 4320.386169672012, 'accumulated_submission_time': 2428.2655460834503, 'accumulated_eval_time': 1892.0216739177704, 'accumulated_logging_time': 0.05432391166687012, 'global_step': 3442, 'preemption_count': 0}), (5155, {'train/loss': 0.12209507204451651, 'validation/loss': 0.12417712490298817, 'validation/num_examples': 83274637, 'test/loss': 0.12659117894736843, 'test/num_examples': 95000000, 'score': 3628.8210110664368, 'total_duration': 6087.000399589539, 'accumulated_submission_time': 3628.8210110664368, 'accumulated_eval_time': 2458.037598848343, 'accumulated_logging_time': 0.07557845115661621, 'global_step': 5155, 'preemption_count': 0}), (6865, {'train/loss': 0.12266618500715532, 'validation/loss': 0.12375369465735407, 'validation/num_examples': 83274637, 'test/loss': 0.12614064210526316, 'test/num_examples': 95000000, 'score': 4829.036646127701, 'total_duration': 7823.182115316391, 'accumulated_submission_time': 4829.036646127701, 'accumulated_eval_time': 2993.955353498459, 'accumulated_logging_time': 0.10243535041809082, 'global_step': 6865, 'preemption_count': 0}), (8000, {'train/loss': 0.1221395768459488, 'validation/loss': 0.12368367333741725, 'validation/num_examples': 83274637, 'test/loss': 0.126021, 'test/num_examples': 95000000, 'score': 5604.8036942481995, 'total_duration': 9043.889823436737, 'accumulated_submission_time': 5604.8036942481995, 'accumulated_eval_time': 3438.8587675094604, 'accumulated_logging_time': 0.125288724899292, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1004 01:10:09.325777 139783731337024 submission_runner.py:552] Timing: 5604.8036942481995
I1004 01:10:09.325830 139783731337024 submission_runner.py:554] Total number of evals: 6
I1004 01:10:09.325884 139783731337024 submission_runner.py:555] ====================
I1004 01:10:09.326020 139783731337024 submission_runner.py:625] Final criteo1tb score: 5604.8036942481995
