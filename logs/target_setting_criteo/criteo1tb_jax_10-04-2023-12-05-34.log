python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_6 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-04-2023-12-05-34.log
2023-10-04 12:05:39.529317: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1004 12:05:56.458003 140127368927040 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_6/criteo1tb_jax.
I1004 12:05:58.042013 140127368927040 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I1004 12:05:58.042826 140127368927040 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1004 12:05:58.042985 140127368927040 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1004 12:05:58.048425 140127368927040 submission_runner.py:507] Using RNG seed 3892382253
I1004 12:06:03.946022 140127368927040 submission_runner.py:516] --- Tuning run 1/1 ---
I1004 12:06:03.946274 140127368927040 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_6/criteo1tb_jax/trial_1.
I1004 12:06:03.946549 140127368927040 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_6/criteo1tb_jax/trial_1/hparams.json.
I1004 12:06:04.130089 140127368927040 submission_runner.py:191] Initializing dataset.
I1004 12:06:04.130340 140127368927040 submission_runner.py:198] Initializing model.
I1004 12:06:09.743634 140127368927040 submission_runner.py:232] Initializing optimizer.
I1004 12:06:12.840107 140127368927040 submission_runner.py:239] Initializing metrics bundle.
I1004 12:06:12.840320 140127368927040 submission_runner.py:257] Initializing checkpoint and logger.
I1004 12:06:12.841501 140127368927040 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_6/criteo1tb_jax/trial_1 with prefix checkpoint_
I1004 12:06:12.841645 140127368927040 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_6/criteo1tb_jax/trial_1/meta_data_0.json.
I1004 12:06:12.841844 140127368927040 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1004 12:06:12.841905 140127368927040 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1004 12:06:13.671523 140127368927040 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_6/criteo1tb_jax/trial_1/flags_0.json.
I1004 12:06:13.762070 140127368927040 submission_runner.py:290] Starting training loop.
I1004 12:06:40.731750 139963397031680 logging_writer.py:48] [0] global_step=0, grad_norm=10.226539611816406, loss=1.1393920183181763
I1004 12:06:40.742253 140127368927040 spec.py:321] Evaluating on the training split.
I1004 12:10:37.109396 140127368927040 spec.py:333] Evaluating on the validation split.
I1004 12:14:34.751634 140127368927040 spec.py:349] Evaluating on the test split.
I1004 12:19:00.111162 140127368927040 submission_runner.py:381] Time since start: 766.35s, 	Step: 1, 	{'train/loss': 1.1411611688961774, 'validation/loss': 1.1343283789997187, 'validation/num_examples': 83274637, 'test/loss': 1.137678147368421, 'test/num_examples': 95000000, 'score': 26.980109930038452, 'total_duration': 766.3490214347839, 'accumulated_submission_time': 26.980109930038452, 'accumulated_eval_time': 739.3688399791718, 'accumulated_logging_time': 0}
I1004 12:19:00.133805 139944475469568 logging_writer.py:48] [1] accumulated_eval_time=739.368840, accumulated_logging_time=0, accumulated_submission_time=26.980110, global_step=1, preemption_count=0, score=26.980110, test/loss=1.137678, test/num_examples=95000000, total_duration=766.349021, train/loss=1.141161, validation/loss=1.134328, validation/num_examples=83274637
I1004 12:19:00.249632 139944261580544 logging_writer.py:48] [1] global_step=1, grad_norm=10.274273872375488, loss=1.1405029296875
I1004 12:19:00.359065 139944475469568 logging_writer.py:48] [2] global_step=2, grad_norm=9.356157302856445, loss=1.020612359046936
I1004 12:19:00.464547 139944261580544 logging_writer.py:48] [3] global_step=3, grad_norm=7.927635192871094, loss=0.8384630680084229
I1004 12:19:00.569473 139944475469568 logging_writer.py:48] [4] global_step=4, grad_norm=6.287729263305664, loss=0.6404331922531128
I1004 12:19:00.672961 139944261580544 logging_writer.py:48] [5] global_step=5, grad_norm=4.5380964279174805, loss=0.46396589279174805
I1004 12:19:00.777031 139944475469568 logging_writer.py:48] [6] global_step=6, grad_norm=3.007220983505249, loss=0.3364998698234558
I1004 12:19:00.880924 139944261580544 logging_writer.py:48] [7] global_step=7, grad_norm=1.9867061376571655, loss=0.2541154623031616
I1004 12:19:00.985610 139944475469568 logging_writer.py:48] [8] global_step=8, grad_norm=1.0997151136398315, loss=0.20440563559532166
I1004 12:19:01.091526 139944261580544 logging_writer.py:48] [9] global_step=9, grad_norm=0.349997878074646, loss=0.17986242473125458
I1004 12:19:01.197606 139944475469568 logging_writer.py:48] [10] global_step=10, grad_norm=0.2984768748283386, loss=0.17448754608631134
I1004 12:19:01.302509 139944261580544 logging_writer.py:48] [11] global_step=11, grad_norm=0.7132798433303833, loss=0.1857813596725464
I1004 12:19:01.407422 139944475469568 logging_writer.py:48] [12] global_step=12, grad_norm=1.0622845888137817, loss=0.2082817256450653
I1004 12:19:01.514538 139944261580544 logging_writer.py:48] [13] global_step=13, grad_norm=1.2115836143493652, loss=0.21762298047542572
I1004 12:19:01.618607 139944475469568 logging_writer.py:48] [14] global_step=14, grad_norm=1.3369369506835938, loss=0.22979465126991272
I1004 12:19:01.722100 139944261580544 logging_writer.py:48] [15] global_step=15, grad_norm=1.3529895544052124, loss=0.22910454869270325
I1004 12:19:01.824794 139944475469568 logging_writer.py:48] [16] global_step=16, grad_norm=1.3657000064849854, loss=0.229969784617424
I1004 12:19:01.929548 139944261580544 logging_writer.py:48] [17] global_step=17, grad_norm=1.3252930641174316, loss=0.2241516262292862
I1004 12:19:02.038445 139944475469568 logging_writer.py:48] [18] global_step=18, grad_norm=1.2000999450683594, loss=0.2091103494167328
I1004 12:19:02.145229 139944261580544 logging_writer.py:48] [19] global_step=19, grad_norm=0.949309229850769, loss=0.18291622400283813
I1004 12:19:02.253587 139944475469568 logging_writer.py:48] [20] global_step=20, grad_norm=0.7298403978347778, loss=0.16730552911758423
I1004 12:19:02.361772 139944261580544 logging_writer.py:48] [21] global_step=21, grad_norm=0.48510313034057617, loss=0.1579717993736267
I1004 12:19:02.471326 139944475469568 logging_writer.py:48] [22] global_step=22, grad_norm=0.17884989082813263, loss=0.1503286361694336
I1004 12:19:02.576705 139944261580544 logging_writer.py:48] [23] global_step=23, grad_norm=0.15570522844791412, loss=0.14748866856098175
I1004 12:19:02.681822 139944475469568 logging_writer.py:48] [24] global_step=24, grad_norm=0.2798413038253784, loss=0.14569152891635895
I1004 12:19:02.785792 139944261580544 logging_writer.py:48] [25] global_step=25, grad_norm=0.227757066488266, loss=0.14647918939590454
I1004 12:19:02.888587 139944475469568 logging_writer.py:48] [26] global_step=26, grad_norm=0.11623784899711609, loss=0.14231006801128387
I1004 12:19:02.991313 139944261580544 logging_writer.py:48] [27] global_step=27, grad_norm=0.03824586048722267, loss=0.13928760588169098
I1004 12:19:03.293849 139944475469568 logging_writer.py:48] [28] global_step=28, grad_norm=0.13927744328975677, loss=0.1420375108718872
I1004 12:19:03.870149 139944261580544 logging_writer.py:48] [29] global_step=29, grad_norm=0.09511101245880127, loss=0.1418379694223404
I1004 12:19:04.778686 139944475469568 logging_writer.py:48] [30] global_step=30, grad_norm=0.044240474700927734, loss=0.14276012778282166
I1004 12:19:05.487421 139944261580544 logging_writer.py:48] [31] global_step=31, grad_norm=0.03824222832918167, loss=0.14247238636016846
I1004 12:19:06.267880 139944475469568 logging_writer.py:48] [32] global_step=32, grad_norm=0.056441910564899445, loss=0.1397116482257843
I1004 12:19:06.995489 139944261580544 logging_writer.py:48] [33] global_step=33, grad_norm=0.03333582729101181, loss=0.13952398300170898
I1004 12:19:07.709009 139944475469568 logging_writer.py:48] [34] global_step=34, grad_norm=0.034103937447071075, loss=0.1407492458820343
I1004 12:19:08.384950 139944261580544 logging_writer.py:48] [35] global_step=35, grad_norm=0.03605694696307182, loss=0.1396307796239853
I1004 12:19:09.176087 139944475469568 logging_writer.py:48] [36] global_step=36, grad_norm=0.031058184802532196, loss=0.13651558756828308
I1004 12:19:09.979295 139944261580544 logging_writer.py:48] [37] global_step=37, grad_norm=0.038330480456352234, loss=0.139465793967247
I1004 12:19:10.671844 139944475469568 logging_writer.py:48] [38] global_step=38, grad_norm=0.031171169131994247, loss=0.14104993641376495
I1004 12:19:11.539861 139944261580544 logging_writer.py:48] [39] global_step=39, grad_norm=0.019920513033866882, loss=0.1389254629611969
I1004 12:19:12.387173 139944475469568 logging_writer.py:48] [40] global_step=40, grad_norm=0.013178384862840176, loss=0.1378215253353119
I1004 12:19:13.237018 139944261580544 logging_writer.py:48] [41] global_step=41, grad_norm=0.023576831445097923, loss=0.14087438583374023
I1004 12:19:13.887598 139944475469568 logging_writer.py:48] [42] global_step=42, grad_norm=0.06904813647270203, loss=0.13869185745716095
I1004 12:19:14.688475 139944261580544 logging_writer.py:48] [43] global_step=43, grad_norm=0.1465665102005005, loss=0.13927093148231506
I1004 12:19:15.586164 139944475469568 logging_writer.py:48] [44] global_step=44, grad_norm=0.1947992444038391, loss=0.1379130482673645
I1004 12:19:16.226307 139944261580544 logging_writer.py:48] [45] global_step=45, grad_norm=0.16503635048866272, loss=0.13653407990932465
I1004 12:19:16.992724 139944475469568 logging_writer.py:48] [46] global_step=46, grad_norm=0.07447585463523865, loss=0.13553234934806824
I1004 12:19:17.822644 139944261580544 logging_writer.py:48] [47] global_step=47, grad_norm=0.05056629329919815, loss=0.1352246105670929
I1004 12:19:18.583820 139944475469568 logging_writer.py:48] [48] global_step=48, grad_norm=0.022541919723153114, loss=0.13660459220409393
I1004 12:19:19.242350 139944261580544 logging_writer.py:48] [49] global_step=49, grad_norm=0.021476179361343384, loss=0.13605986535549164
I1004 12:19:20.036241 139944475469568 logging_writer.py:48] [50] global_step=50, grad_norm=0.030770113691687584, loss=0.13334983587265015
I1004 12:19:20.750797 139944261580544 logging_writer.py:48] [51] global_step=51, grad_norm=0.03754312917590141, loss=0.12933459877967834
I1004 12:19:21.343667 139944475469568 logging_writer.py:48] [52] global_step=52, grad_norm=0.017494788393378258, loss=0.1327853947877884
I1004 12:19:22.165767 139944261580544 logging_writer.py:48] [53] global_step=53, grad_norm=0.02465192787349224, loss=0.13221625983715057
I1004 12:19:22.973581 139944475469568 logging_writer.py:48] [54] global_step=54, grad_norm=0.0712360069155693, loss=0.1324010193347931
I1004 12:19:23.741219 139944261580544 logging_writer.py:48] [55] global_step=55, grad_norm=0.14644189178943634, loss=0.1318158060312271
I1004 12:19:24.600717 139944475469568 logging_writer.py:48] [56] global_step=56, grad_norm=0.3275594413280487, loss=0.13442927598953247
I1004 12:19:25.380480 139944261580544 logging_writer.py:48] [57] global_step=57, grad_norm=0.35399147868156433, loss=0.13064295053482056
I1004 12:19:26.203478 139944475469568 logging_writer.py:48] [58] global_step=58, grad_norm=0.1782415360212326, loss=0.1246151328086853
I1004 12:19:26.964091 139944261580544 logging_writer.py:48] [59] global_step=59, grad_norm=0.06939901411533356, loss=0.12311116605997086
I1004 12:19:27.725996 139944475469568 logging_writer.py:48] [60] global_step=60, grad_norm=0.051651641726493835, loss=0.1224043220281601
I1004 12:19:28.461031 139944261580544 logging_writer.py:48] [61] global_step=61, grad_norm=0.0269217137247324, loss=0.12249808013439178
I1004 12:19:29.261860 139944475469568 logging_writer.py:48] [62] global_step=62, grad_norm=0.012194132432341576, loss=0.12418292462825775
I1004 12:19:29.920872 139944261580544 logging_writer.py:48] [63] global_step=63, grad_norm=0.015611584298312664, loss=0.12312901020050049
I1004 12:19:30.783157 139944475469568 logging_writer.py:48] [64] global_step=64, grad_norm=0.009925372898578644, loss=0.12261273711919785
I1004 12:19:31.473469 139944261580544 logging_writer.py:48] [65] global_step=65, grad_norm=0.009228900074958801, loss=0.12170527875423431
I1004 12:19:32.264323 139944475469568 logging_writer.py:48] [66] global_step=66, grad_norm=0.01436096802353859, loss=0.12241364270448685
I1004 12:19:32.995618 139944261580544 logging_writer.py:48] [67] global_step=67, grad_norm=0.007388752419501543, loss=0.12415941804647446
I1004 12:19:33.766595 139944475469568 logging_writer.py:48] [68] global_step=68, grad_norm=0.008642447181046009, loss=0.12373200058937073
I1004 12:19:34.541534 139944261580544 logging_writer.py:48] [69] global_step=69, grad_norm=0.008822264149785042, loss=0.12271109223365784
I1004 12:19:35.265911 139944475469568 logging_writer.py:48] [70] global_step=70, grad_norm=0.008232646621763706, loss=0.12505438923835754
I1004 12:19:35.963922 139944261580544 logging_writer.py:48] [71] global_step=71, grad_norm=0.03341533616185188, loss=0.12091460078954697
I1004 12:19:36.681772 139944475469568 logging_writer.py:48] [72] global_step=72, grad_norm=0.07852725684642792, loss=0.123002789914608
I1004 12:19:37.528128 139944261580544 logging_writer.py:48] [73] global_step=73, grad_norm=0.13828375935554504, loss=0.12210571020841599
I1004 12:19:38.306824 139944475469568 logging_writer.py:48] [74] global_step=74, grad_norm=0.1981414407491684, loss=0.12428604811429977
I1004 12:19:38.949519 139944261580544 logging_writer.py:48] [75] global_step=75, grad_norm=0.23765580356121063, loss=0.12376895546913147
I1004 12:19:39.643337 139944475469568 logging_writer.py:48] [76] global_step=76, grad_norm=0.22994177043437958, loss=0.1289592981338501
I1004 12:19:40.386050 139944261580544 logging_writer.py:48] [77] global_step=77, grad_norm=0.19632379710674286, loss=0.1294962465763092
I1004 12:19:41.169825 139944475469568 logging_writer.py:48] [78] global_step=78, grad_norm=0.18250638246536255, loss=0.13271822035312653
I1004 12:19:41.857157 139944261580544 logging_writer.py:48] [79] global_step=79, grad_norm=0.15801209211349487, loss=0.1318959891796112
I1004 12:19:42.641577 139944475469568 logging_writer.py:48] [80] global_step=80, grad_norm=0.11017894744873047, loss=0.1291862279176712
I1004 12:19:43.438302 139944261580544 logging_writer.py:48] [81] global_step=81, grad_norm=0.07248838990926743, loss=0.13066674768924713
I1004 12:19:44.113692 139944475469568 logging_writer.py:48] [82] global_step=82, grad_norm=0.03839066997170448, loss=0.12809403240680695
I1004 12:19:44.866118 139944261580544 logging_writer.py:48] [83] global_step=83, grad_norm=0.0200530756264925, loss=0.1281387358903885
I1004 12:19:45.530654 139944475469568 logging_writer.py:48] [84] global_step=84, grad_norm=0.03279450163245201, loss=0.13094399869441986
I1004 12:19:46.208940 139944261580544 logging_writer.py:48] [85] global_step=85, grad_norm=0.054485004395246506, loss=0.13019299507141113
I1004 12:19:46.962066 139944475469568 logging_writer.py:48] [86] global_step=86, grad_norm=0.04462170600891113, loss=0.1293604075908661
I1004 12:19:47.695814 139944261580544 logging_writer.py:48] [87] global_step=87, grad_norm=0.05125702545046806, loss=0.1291639357805252
I1004 12:19:48.415504 139944475469568 logging_writer.py:48] [88] global_step=88, grad_norm=0.06760483235120773, loss=0.12924793362617493
I1004 12:19:49.096338 139944261580544 logging_writer.py:48] [89] global_step=89, grad_norm=0.07124083489179611, loss=0.12943772971630096
I1004 12:19:50.006876 139944475469568 logging_writer.py:48] [90] global_step=90, grad_norm=0.05907538905739784, loss=0.12636829912662506
I1004 12:19:50.571599 139944261580544 logging_writer.py:48] [91] global_step=91, grad_norm=0.05608968436717987, loss=0.12699180841445923
I1004 12:19:51.338881 139944475469568 logging_writer.py:48] [92] global_step=92, grad_norm=0.07410738617181778, loss=0.12824556231498718
I1004 12:19:52.077871 139944261580544 logging_writer.py:48] [93] global_step=93, grad_norm=0.0836760401725769, loss=0.1298293173313141
I1004 12:19:52.833082 139944475469568 logging_writer.py:48] [94] global_step=94, grad_norm=0.08655533194541931, loss=0.12710875272750854
I1004 12:19:53.416685 139944261580544 logging_writer.py:48] [95] global_step=95, grad_norm=0.11750458925962448, loss=0.13070273399353027
I1004 12:19:54.215627 139944475469568 logging_writer.py:48] [96] global_step=96, grad_norm=0.1516171544790268, loss=0.1335705816745758
I1004 12:19:54.828223 139944261580544 logging_writer.py:48] [97] global_step=97, grad_norm=0.17490343749523163, loss=0.13536213338375092
I1004 12:19:55.526998 139944475469568 logging_writer.py:48] [98] global_step=98, grad_norm=0.20858336985111237, loss=0.1348249912261963
I1004 12:19:56.203871 139944261580544 logging_writer.py:48] [99] global_step=99, grad_norm=0.2346053272485733, loss=0.1346648782491684
I1004 12:19:56.917369 139944475469568 logging_writer.py:48] [100] global_step=100, grad_norm=0.25426992774009705, loss=0.13652420043945312
I1004 12:24:39.950294 139944261580544 logging_writer.py:48] [500] global_step=500, grad_norm=0.07638128846883774, loss=0.12334764748811722
I1004 12:30:41.740275 139944475469568 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.03515831381082535, loss=0.12202975898981094
I1004 12:36:50.401518 139944261580544 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.014643327333033085, loss=0.11547723412513733
I1004 12:39:00.690910 140127368927040 spec.py:321] Evaluating on the training split.
I1004 12:42:01.464126 140127368927040 spec.py:333] Evaluating on the validation split.
I1004 12:45:16.586601 140127368927040 spec.py:349] Evaluating on the test split.
I1004 12:49:03.586132 140127368927040 submission_runner.py:381] Time since start: 2569.82s, 	Step: 1679, 	{'train/loss': 0.12466756952633648, 'validation/loss': 0.12553175104203695, 'validation/num_examples': 83274637, 'test/loss': 0.12804274736842106, 'test/num_examples': 95000000, 'score': 1227.5045328140259, 'total_duration': 2569.82399225235, 'accumulated_submission_time': 1227.5045328140259, 'accumulated_eval_time': 1342.2640297412872, 'accumulated_logging_time': 0.03238844871520996}
I1004 12:49:03.602816 139944475469568 logging_writer.py:48] [1679] accumulated_eval_time=1342.264030, accumulated_logging_time=0.032388, accumulated_submission_time=1227.504533, global_step=1679, preemption_count=0, score=1227.504533, test/loss=0.128043, test/num_examples=95000000, total_duration=2569.823992, train/loss=0.124668, validation/loss=0.125532, validation/num_examples=83274637
I1004 12:52:43.829174 139944261580544 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.00401671277359128, loss=0.12050644308328629
I1004 12:58:43.214335 139944475469568 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.023440375924110413, loss=0.12243156135082245
I1004 13:04:47.014479 139944261580544 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.023132866248488426, loss=0.11597780883312225
I1004 13:09:03.898551 140127368927040 spec.py:321] Evaluating on the training split.
I1004 13:12:13.073323 140127368927040 spec.py:333] Evaluating on the validation split.
I1004 13:14:36.014862 140127368927040 spec.py:349] Evaluating on the test split.
I1004 13:17:26.947448 140127368927040 submission_runner.py:381] Time since start: 4273.19s, 	Step: 3356, 	{'train/loss': 0.12281289490513832, 'validation/loss': 0.12483588490454783, 'validation/num_examples': 83274637, 'test/loss': 0.12727182105263157, 'test/num_examples': 95000000, 'score': 2427.7666840553284, 'total_duration': 4273.185286998749, 'accumulated_submission_time': 2427.7666840553284, 'accumulated_eval_time': 1845.312887430191, 'accumulated_logging_time': 0.060332536697387695}
I1004 13:17:26.964020 139944475469568 logging_writer.py:48] [3356] accumulated_eval_time=1845.312887, accumulated_logging_time=0.060333, accumulated_submission_time=2427.766684, global_step=3356, preemption_count=0, score=2427.766684, test/loss=0.127272, test/num_examples=95000000, total_duration=4273.185287, train/loss=0.122813, validation/loss=0.124836, validation/num_examples=83274637
I1004 13:18:55.591667 139944261580544 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.011221781373023987, loss=0.13320735096931458
I1004 13:24:58.437685 139944475469568 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.00476486561819911, loss=0.12692098319530487
I1004 13:31:02.406028 139944261580544 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.005090083461254835, loss=0.12380900979042053
I1004 13:37:01.524859 139944475469568 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.006214501801878214, loss=0.1224786788225174
I1004 13:37:27.226731 140127368927040 spec.py:321] Evaluating on the training split.
I1004 13:40:28.022365 140127368927040 spec.py:333] Evaluating on the validation split.
I1004 13:42:53.303432 140127368927040 spec.py:349] Evaluating on the test split.
I1004 13:45:36.763713 140127368927040 submission_runner.py:381] Time since start: 5963.00s, 	Step: 5038, 	{'train/loss': 0.1219370260178668, 'validation/loss': 0.12432802318910138, 'validation/num_examples': 83274637, 'test/loss': 0.12680393684210525, 'test/num_examples': 95000000, 'score': 3627.9945871829987, 'total_duration': 5963.001584291458, 'accumulated_submission_time': 3627.9945871829987, 'accumulated_eval_time': 2334.849837064743, 'accumulated_logging_time': 0.08937978744506836}
I1004 13:45:36.783042 139944261580544 logging_writer.py:48] [5038] accumulated_eval_time=2334.849837, accumulated_logging_time=0.089380, accumulated_submission_time=3627.994587, global_step=5038, preemption_count=0, score=3627.994587, test/loss=0.126804, test/num_examples=95000000, total_duration=5963.001584, train/loss=0.121937, validation/loss=0.124328, validation/num_examples=83274637
I1004 13:50:53.222014 139944475469568 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.004788989666849375, loss=0.12387345731258392
I1004 13:56:47.504232 139944261580544 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.00963982567191124, loss=0.1258959025144577
I1004 14:02:55.021927 139944475469568 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.005168733187019825, loss=0.12600959837436676
I1004 14:05:37.101024 140127368927040 spec.py:321] Evaluating on the training split.
I1004 14:08:24.055951 140127368927040 spec.py:333] Evaluating on the validation split.
I1004 14:10:42.803518 140127368927040 spec.py:349] Evaluating on the test split.
I1004 14:13:17.823542 140127368927040 submission_runner.py:381] Time since start: 7624.06s, 	Step: 6724, 	{'train/loss': 0.12114313713409616, 'validation/loss': 0.1240290486045589, 'validation/num_examples': 83274637, 'test/loss': 0.12638387368421053, 'test/num_examples': 95000000, 'score': 4828.28324341774, 'total_duration': 7624.061403751373, 'accumulated_submission_time': 4828.28324341774, 'accumulated_eval_time': 2795.572328567505, 'accumulated_logging_time': 0.11588811874389648}
I1004 14:13:17.837757 139944261580544 logging_writer.py:48] [6724] accumulated_eval_time=2795.572329, accumulated_logging_time=0.115888, accumulated_submission_time=4828.283243, global_step=6724, preemption_count=0, score=4828.283243, test/loss=0.126384, test/num_examples=95000000, total_duration=7624.061404, train/loss=0.121143, validation/loss=0.124029, validation/num_examples=83274637
I1004 14:16:23.988123 139944475469568 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.004730468150228262, loss=0.12131397426128387
I1004 14:22:21.444431 139944261580544 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.004561981651932001, loss=0.1249517872929573
I1004 14:28:26.449856 140127368927040 spec.py:321] Evaluating on the training split.
I1004 14:30:50.893891 140127368927040 spec.py:333] Evaluating on the validation split.
I1004 14:32:45.940329 140127368927040 spec.py:349] Evaluating on the test split.
I1004 14:35:05.431665 140127368927040 submission_runner.py:381] Time since start: 8931.67s, 	Step: 8000, 	{'train/loss': 0.12162564835458431, 'validation/loss': 0.12380540307849075, 'validation/num_examples': 83274637, 'test/loss': 0.12616841052631578, 'test/num_examples': 95000000, 'score': 5736.871240615845, 'total_duration': 8931.669537782669, 'accumulated_submission_time': 5736.871240615845, 'accumulated_eval_time': 3194.554102897644, 'accumulated_logging_time': 0.13722777366638184}
I1004 14:35:05.449680 139944475469568 logging_writer.py:48] [8000] accumulated_eval_time=3194.554103, accumulated_logging_time=0.137228, accumulated_submission_time=5736.871241, global_step=8000, preemption_count=0, score=5736.871241, test/loss=0.126168, test/num_examples=95000000, total_duration=8931.669538, train/loss=0.121626, validation/loss=0.123805, validation/num_examples=83274637
I1004 14:35:05.464625 139944261580544 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5736.871241
I1004 14:35:11.292650 140127368927040 checkpoints.py:490] Saving checkpoint at step: 8000
I1004 14:35:46.720730 140127368927040 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_6/criteo1tb_jax/trial_1/checkpoint_8000
I1004 14:35:47.038768 140127368927040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_6/criteo1tb_jax/trial_1/checkpoint_8000.
I1004 14:35:47.417453 140127368927040 submission_runner.py:549] Tuning trial 1/1
I1004 14:35:47.417751 140127368927040 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1004 14:35:47.419352 140127368927040 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 1.1411611688961774, 'validation/loss': 1.1343283789997187, 'validation/num_examples': 83274637, 'test/loss': 1.137678147368421, 'test/num_examples': 95000000, 'score': 26.980109930038452, 'total_duration': 766.3490214347839, 'accumulated_submission_time': 26.980109930038452, 'accumulated_eval_time': 739.3688399791718, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1679, {'train/loss': 0.12466756952633648, 'validation/loss': 0.12553175104203695, 'validation/num_examples': 83274637, 'test/loss': 0.12804274736842106, 'test/num_examples': 95000000, 'score': 1227.5045328140259, 'total_duration': 2569.82399225235, 'accumulated_submission_time': 1227.5045328140259, 'accumulated_eval_time': 1342.2640297412872, 'accumulated_logging_time': 0.03238844871520996, 'global_step': 1679, 'preemption_count': 0}), (3356, {'train/loss': 0.12281289490513832, 'validation/loss': 0.12483588490454783, 'validation/num_examples': 83274637, 'test/loss': 0.12727182105263157, 'test/num_examples': 95000000, 'score': 2427.7666840553284, 'total_duration': 4273.185286998749, 'accumulated_submission_time': 2427.7666840553284, 'accumulated_eval_time': 1845.312887430191, 'accumulated_logging_time': 0.060332536697387695, 'global_step': 3356, 'preemption_count': 0}), (5038, {'train/loss': 0.1219370260178668, 'validation/loss': 0.12432802318910138, 'validation/num_examples': 83274637, 'test/loss': 0.12680393684210525, 'test/num_examples': 95000000, 'score': 3627.9945871829987, 'total_duration': 5963.001584291458, 'accumulated_submission_time': 3627.9945871829987, 'accumulated_eval_time': 2334.849837064743, 'accumulated_logging_time': 0.08937978744506836, 'global_step': 5038, 'preemption_count': 0}), (6724, {'train/loss': 0.12114313713409616, 'validation/loss': 0.1240290486045589, 'validation/num_examples': 83274637, 'test/loss': 0.12638387368421053, 'test/num_examples': 95000000, 'score': 4828.28324341774, 'total_duration': 7624.061403751373, 'accumulated_submission_time': 4828.28324341774, 'accumulated_eval_time': 2795.572328567505, 'accumulated_logging_time': 0.11588811874389648, 'global_step': 6724, 'preemption_count': 0}), (8000, {'train/loss': 0.12162564835458431, 'validation/loss': 0.12380540307849075, 'validation/num_examples': 83274637, 'test/loss': 0.12616841052631578, 'test/num_examples': 95000000, 'score': 5736.871240615845, 'total_duration': 8931.669537782669, 'accumulated_submission_time': 5736.871240615845, 'accumulated_eval_time': 3194.554102897644, 'accumulated_logging_time': 0.13722777366638184, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1004 14:35:47.419520 140127368927040 submission_runner.py:552] Timing: 5736.871240615845
I1004 14:35:47.419586 140127368927040 submission_runner.py:554] Total number of evals: 6
I1004 14:35:47.419643 140127368927040 submission_runner.py:555] ====================
I1004 14:35:47.419772 140127368927040 submission_runner.py:625] Final criteo1tb score: 5736.871240615845
