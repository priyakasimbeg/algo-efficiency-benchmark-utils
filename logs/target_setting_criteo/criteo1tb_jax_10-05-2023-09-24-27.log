python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_14 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-05-2023-09-24-27.log
2023-10-05 09:24:32.933096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1005 09:24:50.227685 139673521522496 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_14/criteo1tb_jax.
I1005 09:24:51.903574 139673521522496 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1005 09:24:51.904324 139673521522496 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1005 09:24:51.904456 139673521522496 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1005 09:24:51.909821 139673521522496 submission_runner.py:507] Using RNG seed 797332219
I1005 09:24:57.697182 139673521522496 submission_runner.py:516] --- Tuning run 1/1 ---
I1005 09:24:57.697381 139673521522496 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_14/criteo1tb_jax/trial_1.
I1005 09:24:57.697660 139673521522496 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_14/criteo1tb_jax/trial_1/hparams.json.
I1005 09:24:57.880703 139673521522496 submission_runner.py:191] Initializing dataset.
I1005 09:24:57.880955 139673521522496 submission_runner.py:198] Initializing model.
I1005 09:25:03.430031 139673521522496 submission_runner.py:232] Initializing optimizer.
I1005 09:25:06.591780 139673521522496 submission_runner.py:239] Initializing metrics bundle.
I1005 09:25:06.591989 139673521522496 submission_runner.py:257] Initializing checkpoint and logger.
I1005 09:25:06.593066 139673521522496 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_14/criteo1tb_jax/trial_1 with prefix checkpoint_
I1005 09:25:06.593211 139673521522496 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_14/criteo1tb_jax/trial_1/meta_data_0.json.
I1005 09:25:06.593420 139673521522496 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1005 09:25:06.593482 139673521522496 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1005 09:25:07.452888 139673521522496 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_14/criteo1tb_jax/trial_1/flags_0.json.
I1005 09:25:07.548591 139673521522496 submission_runner.py:290] Starting training loop.
I1005 09:25:34.394660 139509279741696 logging_writer.py:48] [0] global_step=0, grad_norm=6.64976167678833, loss=0.5091068148612976
I1005 09:25:34.405560 139673521522496 spec.py:321] Evaluating on the training split.
I1005 09:29:25.395066 139673521522496 spec.py:333] Evaluating on the validation split.
I1005 09:33:23.416378 139673521522496 spec.py:349] Evaluating on the test split.
I1005 09:37:44.389243 139673521522496 submission_runner.py:381] Time since start: 756.84s, 	Step: 1, 	{'train/loss': 0.5088812870049627, 'validation/loss': 0.5070665633763135, 'validation/num_examples': 83274637, 'test/loss': 0.5088895578947369, 'test/num_examples': 95000000, 'score': 26.856948614120483, 'total_duration': 756.8405702114105, 'accumulated_submission_time': 26.856948614120483, 'accumulated_eval_time': 729.9835860729218, 'accumulated_logging_time': 0}
I1005 09:37:44.409171 139490622412544 logging_writer.py:48] [1] accumulated_eval_time=729.983586, accumulated_logging_time=0, accumulated_submission_time=26.856949, global_step=1, preemption_count=0, score=26.856949, test/loss=0.508890, test/num_examples=95000000, total_duration=756.840570, train/loss=0.508881, validation/loss=0.507067, validation/num_examples=83274637
I1005 09:37:44.524408 139490068788992 logging_writer.py:48] [1] global_step=1, grad_norm=6.654105186462402, loss=0.5092591643333435
I1005 09:37:44.632213 139490622412544 logging_writer.py:48] [2] global_step=2, grad_norm=5.576148986816406, loss=0.4381336569786072
I1005 09:37:44.737383 139490068788992 logging_writer.py:48] [3] global_step=3, grad_norm=4.100656032562256, loss=0.3411094546318054
I1005 09:37:44.840995 139490622412544 logging_writer.py:48] [4] global_step=4, grad_norm=2.491448402404785, loss=0.25312381982803345
I1005 09:37:44.944673 139490068788992 logging_writer.py:48] [5] global_step=5, grad_norm=1.280053734779358, loss=0.19465544819831848
I1005 09:37:45.048188 139490622412544 logging_writer.py:48] [6] global_step=6, grad_norm=0.4933052361011505, loss=0.1651495099067688
I1005 09:37:45.151979 139490068788992 logging_writer.py:48] [7] global_step=7, grad_norm=0.189003586769104, loss=0.1597205102443695
I1005 09:37:45.256788 139490622412544 logging_writer.py:48] [8] global_step=8, grad_norm=0.5631381273269653, loss=0.16899070143699646
I1005 09:37:45.361268 139490068788992 logging_writer.py:48] [9] global_step=9, grad_norm=0.7617873549461365, loss=0.17651371657848358
I1005 09:37:45.465753 139490622412544 logging_writer.py:48] [10] global_step=10, grad_norm=0.8585805892944336, loss=0.18182115256786346
I1005 09:37:45.571222 139490068788992 logging_writer.py:48] [11] global_step=11, grad_norm=0.8652745485305786, loss=0.18331444263458252
I1005 09:37:45.677021 139490622412544 logging_writer.py:48] [12] global_step=12, grad_norm=0.8523470163345337, loss=0.18216489255428314
I1005 09:37:45.781357 139490068788992 logging_writer.py:48] [13] global_step=13, grad_norm=0.7137683033943176, loss=0.17023323476314545
I1005 09:37:45.885012 139490622412544 logging_writer.py:48] [14] global_step=14, grad_norm=0.572665810585022, loss=0.1614554077386856
I1005 09:37:45.988594 139490068788992 logging_writer.py:48] [15] global_step=15, grad_norm=0.4063155949115753, loss=0.15640990436077118
I1005 09:37:46.093271 139490622412544 logging_writer.py:48] [16] global_step=16, grad_norm=0.19214318692684174, loss=0.14932295680046082
I1005 09:37:46.198677 139490068788992 logging_writer.py:48] [17] global_step=17, grad_norm=0.06614986062049866, loss=0.1470140516757965
I1005 09:37:46.302737 139490622412544 logging_writer.py:48] [18] global_step=18, grad_norm=0.16796338558197021, loss=0.1462951898574829
I1005 09:37:46.406885 139490068788992 logging_writer.py:48] [19] global_step=19, grad_norm=0.22298607230186462, loss=0.14047713577747345
I1005 09:37:46.511626 139490622412544 logging_writer.py:48] [20] global_step=20, grad_norm=0.11121408641338348, loss=0.1382819563150406
I1005 09:37:46.616477 139490068788992 logging_writer.py:48] [21] global_step=21, grad_norm=0.04480571299791336, loss=0.13718865811824799
I1005 09:37:46.720322 139490622412544 logging_writer.py:48] [22] global_step=22, grad_norm=0.1004071831703186, loss=0.13480758666992188
I1005 09:37:46.823849 139490068788992 logging_writer.py:48] [23] global_step=23, grad_norm=0.09288939833641052, loss=0.13442720472812653
I1005 09:37:46.928447 139490622412544 logging_writer.py:48] [24] global_step=24, grad_norm=0.045336101204156876, loss=0.13251739740371704
I1005 09:37:47.033232 139490068788992 logging_writer.py:48] [25] global_step=25, grad_norm=0.027944844216108322, loss=0.13406682014465332
I1005 09:37:47.137962 139490622412544 logging_writer.py:48] [26] global_step=26, grad_norm=0.026996077969670296, loss=0.13606473803520203
I1005 09:37:47.242095 139490068788992 logging_writer.py:48] [27] global_step=27, grad_norm=0.03520442172884941, loss=0.13214299082756042
I1005 09:37:47.565539 139490622412544 logging_writer.py:48] [28] global_step=28, grad_norm=0.05965052917599678, loss=0.13393567502498627
I1005 09:37:48.380429 139490068788992 logging_writer.py:48] [29] global_step=29, grad_norm=0.050049591809511185, loss=0.1334339827299118
I1005 09:37:49.179482 139490622412544 logging_writer.py:48] [30] global_step=30, grad_norm=0.033532559871673584, loss=0.13255807757377625
I1005 09:37:49.828373 139490068788992 logging_writer.py:48] [31] global_step=31, grad_norm=0.028644239529967308, loss=0.1323634386062622
I1005 09:37:50.603901 139490622412544 logging_writer.py:48] [32] global_step=32, grad_norm=0.03355181962251663, loss=0.13345101475715637
I1005 09:37:51.412306 139490068788992 logging_writer.py:48] [33] global_step=33, grad_norm=0.05813496932387352, loss=0.1315472424030304
I1005 09:37:52.171694 139490622412544 logging_writer.py:48] [34] global_step=34, grad_norm=0.089957594871521, loss=0.13319814205169678
I1005 09:37:52.827184 139490068788992 logging_writer.py:48] [35] global_step=35, grad_norm=0.13295471668243408, loss=0.1324409395456314
I1005 09:37:53.572879 139490622412544 logging_writer.py:48] [36] global_step=36, grad_norm=0.1442631185054779, loss=0.13480448722839355
I1005 09:37:54.216683 139490068788992 logging_writer.py:48] [37] global_step=37, grad_norm=0.1364288032054901, loss=0.13287188112735748
I1005 09:37:54.962735 139490622412544 logging_writer.py:48] [38] global_step=38, grad_norm=0.07449944317340851, loss=0.13231968879699707
I1005 09:37:55.800565 139490068788992 logging_writer.py:48] [39] global_step=39, grad_norm=0.03831074759364128, loss=0.13187280297279358
I1005 09:37:56.434137 139490622412544 logging_writer.py:48] [40] global_step=40, grad_norm=0.056304506957530975, loss=0.1343216747045517
I1005 09:37:57.225180 139490068788992 logging_writer.py:48] [41] global_step=41, grad_norm=0.07160249352455139, loss=0.1337958127260208
I1005 09:37:58.078631 139490622412544 logging_writer.py:48] [42] global_step=42, grad_norm=0.07427527755498886, loss=0.13077767193317413
I1005 09:37:58.764323 139490068788992 logging_writer.py:48] [43] global_step=43, grad_norm=0.08978596329689026, loss=0.12991242110729218
I1005 09:37:59.541295 139490622412544 logging_writer.py:48] [44] global_step=44, grad_norm=0.1575642079114914, loss=0.13241766393184662
I1005 09:38:00.241601 139490068788992 logging_writer.py:48] [45] global_step=45, grad_norm=0.23198699951171875, loss=0.13438068330287933
I1005 09:38:01.098008 139490622412544 logging_writer.py:48] [46] global_step=46, grad_norm=0.24916091561317444, loss=0.13120779395103455
I1005 09:38:01.912182 139490068788992 logging_writer.py:48] [47] global_step=47, grad_norm=0.22203807532787323, loss=0.12932339310646057
I1005 09:38:02.683115 139490622412544 logging_writer.py:48] [48] global_step=48, grad_norm=0.1753288209438324, loss=0.12904635071754456
I1005 09:38:03.582799 139490068788992 logging_writer.py:48] [49] global_step=49, grad_norm=0.10169568657875061, loss=0.1293160617351532
I1005 09:38:04.208102 139490622412544 logging_writer.py:48] [50] global_step=50, grad_norm=0.05518150329589844, loss=0.12885455787181854
I1005 09:38:04.972037 139490068788992 logging_writer.py:48] [51] global_step=51, grad_norm=0.052730608731508255, loss=0.12806683778762817
I1005 09:38:05.837304 139490622412544 logging_writer.py:48] [52] global_step=52, grad_norm=0.03533182293176651, loss=0.12512904405593872
I1005 09:38:06.529841 139490068788992 logging_writer.py:48] [53] global_step=53, grad_norm=0.017886631190776825, loss=0.1263895481824875
I1005 09:38:07.399368 139490622412544 logging_writer.py:48] [54] global_step=54, grad_norm=0.021351151168346405, loss=0.12722179293632507
I1005 09:38:08.216415 139490068788992 logging_writer.py:48] [55] global_step=55, grad_norm=0.049570679664611816, loss=0.12617012858390808
I1005 09:38:09.170188 139490622412544 logging_writer.py:48] [56] global_step=56, grad_norm=0.07804150879383087, loss=0.1267656683921814
I1005 09:38:09.917415 139490068788992 logging_writer.py:48] [57] global_step=57, grad_norm=0.09728486090898514, loss=0.12610070407390594
I1005 09:38:10.555716 139490622412544 logging_writer.py:48] [58] global_step=58, grad_norm=0.13328438997268677, loss=0.12791167199611664
I1005 09:38:11.324868 139490068788992 logging_writer.py:48] [59] global_step=59, grad_norm=0.16829076409339905, loss=0.12600192427635193
I1005 09:38:12.036876 139490622412544 logging_writer.py:48] [60] global_step=60, grad_norm=0.17706619203090668, loss=0.12677200138568878
I1005 09:38:12.867966 139490068788992 logging_writer.py:48] [61] global_step=61, grad_norm=0.15036843717098236, loss=0.12789662182331085
I1005 09:38:13.720988 139490622412544 logging_writer.py:48] [62] global_step=62, grad_norm=0.11392098665237427, loss=0.1264127492904663
I1005 09:38:14.453181 139490068788992 logging_writer.py:48] [63] global_step=63, grad_norm=0.08444461971521378, loss=0.12528903782367706
I1005 09:38:15.112121 139490622412544 logging_writer.py:48] [64] global_step=64, grad_norm=0.04635188728570938, loss=0.12369223684072495
I1005 09:38:15.843495 139490068788992 logging_writer.py:48] [65] global_step=65, grad_norm=0.015819724649190903, loss=0.12473347783088684
I1005 09:38:16.704090 139490622412544 logging_writer.py:48] [66] global_step=66, grad_norm=0.005639099981635809, loss=0.12452393770217896
I1005 09:38:17.388955 139490068788992 logging_writer.py:48] [67] global_step=67, grad_norm=0.019111094996333122, loss=0.12340793013572693
I1005 09:38:18.183134 139490622412544 logging_writer.py:48] [68] global_step=68, grad_norm=0.05206171050667763, loss=0.12623098492622375
I1005 09:38:18.846861 139490068788992 logging_writer.py:48] [69] global_step=69, grad_norm=0.07466565817594528, loss=0.12439510971307755
I1005 09:38:19.721762 139490622412544 logging_writer.py:48] [70] global_step=70, grad_norm=0.08632539212703705, loss=0.12558837234973907
I1005 09:38:20.316652 139490068788992 logging_writer.py:48] [71] global_step=71, grad_norm=0.08184913545846939, loss=0.12648293375968933
I1005 09:38:20.929246 139490622412544 logging_writer.py:48] [72] global_step=72, grad_norm=0.070440374314785, loss=0.12515848875045776
I1005 09:38:21.705107 139490068788992 logging_writer.py:48] [73] global_step=73, grad_norm=0.06527769565582275, loss=0.1255713850259781
I1005 09:38:22.408262 139490622412544 logging_writer.py:48] [74] global_step=74, grad_norm=0.07051677256822586, loss=0.12752757966518402
I1005 09:38:23.130448 139490068788992 logging_writer.py:48] [75] global_step=75, grad_norm=0.09426618367433548, loss=0.12514497339725494
I1005 09:38:23.844870 139490622412544 logging_writer.py:48] [76] global_step=76, grad_norm=0.10311741381883621, loss=0.12646308541297913
I1005 09:38:24.482581 139490068788992 logging_writer.py:48] [77] global_step=77, grad_norm=0.11486940085887909, loss=0.1275322437286377
I1005 09:38:25.279538 139490622412544 logging_writer.py:48] [78] global_step=78, grad_norm=0.12925292551517487, loss=0.126916766166687
I1005 09:38:26.008993 139490068788992 logging_writer.py:48] [79] global_step=79, grad_norm=0.14106687903404236, loss=0.12659290432929993
I1005 09:38:26.742512 139490622412544 logging_writer.py:48] [80] global_step=80, grad_norm=0.1526648849248886, loss=0.1279100775718689
I1005 09:38:27.355502 139490068788992 logging_writer.py:48] [81] global_step=81, grad_norm=0.15698274970054626, loss=0.12794852256774902
I1005 09:38:28.184361 139490622412544 logging_writer.py:48] [82] global_step=82, grad_norm=0.17062242329120636, loss=0.13038481771945953
I1005 09:38:28.834221 139490068788992 logging_writer.py:48] [83] global_step=83, grad_norm=0.17379213869571686, loss=0.12931430339813232
I1005 09:38:29.571489 139490622412544 logging_writer.py:48] [84] global_step=84, grad_norm=0.15332786738872528, loss=0.12948113679885864
I1005 09:38:30.359023 139490068788992 logging_writer.py:48] [85] global_step=85, grad_norm=0.14641635119915009, loss=0.12550947070121765
I1005 09:38:31.098808 139490622412544 logging_writer.py:48] [86] global_step=86, grad_norm=0.13368496298789978, loss=0.12741224467754364
I1005 09:38:31.730223 139490068788992 logging_writer.py:48] [87] global_step=87, grad_norm=0.09716781228780746, loss=0.1260284036397934
I1005 09:38:32.544881 139490622412544 logging_writer.py:48] [88] global_step=88, grad_norm=0.0662083774805069, loss=0.12693624198436737
I1005 09:38:33.187309 139490068788992 logging_writer.py:48] [89] global_step=89, grad_norm=0.042833272367715836, loss=0.12824711203575134
I1005 09:38:33.912343 139490622412544 logging_writer.py:48] [90] global_step=90, grad_norm=0.01708964630961418, loss=0.12643714249134064
I1005 09:38:34.735414 139490068788992 logging_writer.py:48] [91] global_step=91, grad_norm=0.021329523995518684, loss=0.12443029135465622
I1005 09:38:35.476328 139490622412544 logging_writer.py:48] [92] global_step=92, grad_norm=0.028783660382032394, loss=0.12557461857795715
I1005 09:38:36.104947 139490068788992 logging_writer.py:48] [93] global_step=93, grad_norm=0.025034233927726746, loss=0.12475325167179108
I1005 09:38:36.905920 139490622412544 logging_writer.py:48] [94] global_step=94, grad_norm=0.033485885709524155, loss=0.1271507441997528
I1005 09:38:37.577665 139490068788992 logging_writer.py:48] [95] global_step=95, grad_norm=0.05546049773693085, loss=0.12510564923286438
I1005 09:38:38.281215 139490622412544 logging_writer.py:48] [96] global_step=96, grad_norm=0.06375496834516525, loss=0.1255711168050766
I1005 09:38:39.099279 139490068788992 logging_writer.py:48] [97] global_step=97, grad_norm=0.056470658630132675, loss=0.12520575523376465
I1005 09:38:39.746189 139490622412544 logging_writer.py:48] [98] global_step=98, grad_norm=0.06781768798828125, loss=0.1271488517522812
I1005 09:38:40.468558 139490068788992 logging_writer.py:48] [99] global_step=99, grad_norm=0.08946756273508072, loss=0.12546610832214355
I1005 09:38:41.192118 139490622412544 logging_writer.py:48] [100] global_step=100, grad_norm=0.10427261143922806, loss=0.12510812282562256
I1005 09:43:21.610961 139490068788992 logging_writer.py:48] [500] global_step=500, grad_norm=0.020798636600375175, loss=0.13352392613887787
I1005 09:49:18.923347 139490622412544 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.004614837933331728, loss=0.12865689396858215
I1005 09:55:11.563128 139490068788992 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.02164315991103649, loss=0.12494996190071106
I1005 09:57:44.937934 139673521522496 spec.py:321] Evaluating on the training split.
I1005 10:00:46.924432 139673521522496 spec.py:333] Evaluating on the validation split.
I1005 10:03:56.230273 139673521522496 spec.py:349] Evaluating on the test split.
I1005 10:07:42.031528 139673521522496 submission_runner.py:381] Time since start: 2554.48s, 	Step: 1724, 	{'train/loss': 0.12477393720135, 'validation/loss': 0.125583531513923, 'validation/num_examples': 83274637, 'test/loss': 0.12850673684210526, 'test/num_examples': 95000000, 'score': 1227.353055715561, 'total_duration': 2554.482857465744, 'accumulated_submission_time': 1227.353055715561, 'accumulated_eval_time': 1327.077137708664, 'accumulated_logging_time': 0.02844381332397461}
I1005 10:07:42.047528 139490622412544 logging_writer.py:48] [1724] accumulated_eval_time=1327.077138, accumulated_logging_time=0.028444, accumulated_submission_time=1227.353056, global_step=1724, preemption_count=0, score=1227.353056, test/loss=0.128507, test/num_examples=95000000, total_duration=2554.482857, train/loss=0.124774, validation/loss=0.125584, validation/num_examples=83274637
I1005 10:10:43.208500 139490068788992 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.012192734517157078, loss=0.12132163345813751
I1005 10:16:43.175154 139490622412544 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.006342634093016386, loss=0.13151252269744873
I1005 10:22:39.375463 139490068788992 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.012970362789928913, loss=0.12609922885894775
I1005 10:27:42.228367 139673521522496 spec.py:321] Evaluating on the training split.
I1005 10:30:49.400056 139673521522496 spec.py:333] Evaluating on the validation split.
I1005 10:33:58.672273 139673521522496 spec.py:349] Evaluating on the test split.
I1005 10:37:25.135122 139673521522496 submission_runner.py:381] Time since start: 4337.59s, 	Step: 3432, 	{'train/loss': 0.1247721498117507, 'validation/loss': 0.12463166906389517, 'validation/num_examples': 83274637, 'test/loss': 0.12700495789473684, 'test/num_examples': 95000000, 'score': 2427.5022280216217, 'total_duration': 4337.586449623108, 'accumulated_submission_time': 2427.5022280216217, 'accumulated_eval_time': 1909.9838390350342, 'accumulated_logging_time': 0.05309724807739258}
I1005 10:37:25.155760 139490622412544 logging_writer.py:48] [3432] accumulated_eval_time=1909.983839, accumulated_logging_time=0.053097, accumulated_submission_time=2427.502228, global_step=3432, preemption_count=0, score=2427.502228, test/loss=0.127005, test/num_examples=95000000, total_duration=4337.586450, train/loss=0.124772, validation/loss=0.124632, validation/num_examples=83274637
I1005 10:37:58.049191 139490068788992 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.004615035839378834, loss=0.12026461213827133
I1005 10:43:58.129091 139490622412544 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.009796440601348877, loss=0.12249511480331421
I1005 10:49:52.409523 139490068788992 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.00719195231795311, loss=0.11549527943134308
I1005 10:55:47.185523 139490622412544 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.005565264727920294, loss=0.11783958971500397
I1005 10:57:25.779192 139673521522496 spec.py:321] Evaluating on the training split.
I1005 11:00:24.544308 139673521522496 spec.py:333] Evaluating on the validation split.
I1005 11:03:20.865264 139673521522496 spec.py:349] Evaluating on the test split.
I1005 11:06:51.374115 139673521522496 submission_runner.py:381] Time since start: 6103.83s, 	Step: 5143, 	{'train/loss': 0.1215496303150489, 'validation/loss': 0.12413258553141457, 'validation/num_examples': 83274637, 'test/loss': 0.12655961052631579, 'test/num_examples': 95000000, 'score': 3628.0940811634064, 'total_duration': 6103.825448036194, 'accumulated_submission_time': 3628.0940811634064, 'accumulated_eval_time': 2475.578716993332, 'accumulated_logging_time': 0.08221197128295898}
I1005 11:06:51.392850 139490068788992 logging_writer.py:48] [5143] accumulated_eval_time=2475.578717, accumulated_logging_time=0.082212, accumulated_submission_time=3628.094081, global_step=5143, preemption_count=0, score=3628.094081, test/loss=0.126560, test/num_examples=95000000, total_duration=6103.825448, train/loss=0.121550, validation/loss=0.124133, validation/num_examples=83274637
I1005 11:10:48.969849 139490622412544 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.005272199399769306, loss=0.12915071845054626
I1005 11:16:45.817544 139490068788992 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.007732931058853865, loss=0.12638665735721588
I1005 11:22:42.934561 139490622412544 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.005149362608790398, loss=0.1202586442232132
I1005 11:26:51.681972 139673521522496 spec.py:321] Evaluating on the training split.
I1005 11:29:41.714903 139673521522496 spec.py:333] Evaluating on the validation split.
I1005 11:32:32.226269 139673521522496 spec.py:349] Evaluating on the test split.
I1005 11:35:57.354253 139673521522496 submission_runner.py:381] Time since start: 7849.81s, 	Step: 6855, 	{'train/loss': 0.12255992529527196, 'validation/loss': 0.12381139529914732, 'validation/num_examples': 83274637, 'test/loss': 0.12617367368421054, 'test/num_examples': 95000000, 'score': 4828.352328777313, 'total_duration': 7849.805579900742, 'accumulated_submission_time': 4828.352328777313, 'accumulated_eval_time': 3021.250950574875, 'accumulated_logging_time': 0.10839366912841797}
I1005 11:35:57.370725 139490068788992 logging_writer.py:48] [6855] accumulated_eval_time=3021.250951, accumulated_logging_time=0.108394, accumulated_submission_time=4828.352329, global_step=6855, preemption_count=0, score=4828.352329, test/loss=0.126174, test/num_examples=95000000, total_duration=7849.805580, train/loss=0.122560, validation/loss=0.123811, validation/num_examples=83274637
I1005 11:37:28.640450 139490622412544 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.006965969689190388, loss=0.12295167148113251
I1005 11:43:24.920950 139490068788992 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.005384271033108234, loss=0.13016939163208008
I1005 11:49:20.908563 139673521522496 spec.py:321] Evaluating on the training split.
I1005 11:51:47.241614 139673521522496 spec.py:333] Evaluating on the validation split.
I1005 11:53:42.444804 139673521522496 spec.py:349] Evaluating on the test split.
I1005 11:56:23.658010 139673521522496 submission_runner.py:381] Time since start: 9076.11s, 	Step: 8000, 	{'train/loss': 0.12223078169912663, 'validation/loss': 0.1237364264944199, 'validation/num_examples': 83274637, 'test/loss': 0.1260378210526316, 'test/num_examples': 95000000, 'score': 5631.866857051849, 'total_duration': 9076.109323978424, 'accumulated_submission_time': 5631.866857051849, 'accumulated_eval_time': 3444.000330686569, 'accumulated_logging_time': 0.13224267959594727}
I1005 11:56:23.676401 139490622412544 logging_writer.py:48] [8000] accumulated_eval_time=3444.000331, accumulated_logging_time=0.132243, accumulated_submission_time=5631.866857, global_step=8000, preemption_count=0, score=5631.866857, test/loss=0.126038, test/num_examples=95000000, total_duration=9076.109324, train/loss=0.122231, validation/loss=0.123736, validation/num_examples=83274637
I1005 11:56:23.690009 139490068788992 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5631.866857
I1005 11:56:29.815669 139673521522496 checkpoints.py:490] Saving checkpoint at step: 8000
I1005 11:57:05.112246 139673521522496 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_14/criteo1tb_jax/trial_1/checkpoint_8000
I1005 11:57:05.432539 139673521522496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_14/criteo1tb_jax/trial_1/checkpoint_8000.
I1005 11:57:05.784544 139673521522496 submission_runner.py:549] Tuning trial 1/1
I1005 11:57:05.784789 139673521522496 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1005 11:57:05.785910 139673521522496 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.5088812870049627, 'validation/loss': 0.5070665633763135, 'validation/num_examples': 83274637, 'test/loss': 0.5088895578947369, 'test/num_examples': 95000000, 'score': 26.856948614120483, 'total_duration': 756.8405702114105, 'accumulated_submission_time': 26.856948614120483, 'accumulated_eval_time': 729.9835860729218, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1724, {'train/loss': 0.12477393720135, 'validation/loss': 0.125583531513923, 'validation/num_examples': 83274637, 'test/loss': 0.12850673684210526, 'test/num_examples': 95000000, 'score': 1227.353055715561, 'total_duration': 2554.482857465744, 'accumulated_submission_time': 1227.353055715561, 'accumulated_eval_time': 1327.077137708664, 'accumulated_logging_time': 0.02844381332397461, 'global_step': 1724, 'preemption_count': 0}), (3432, {'train/loss': 0.1247721498117507, 'validation/loss': 0.12463166906389517, 'validation/num_examples': 83274637, 'test/loss': 0.12700495789473684, 'test/num_examples': 95000000, 'score': 2427.5022280216217, 'total_duration': 4337.586449623108, 'accumulated_submission_time': 2427.5022280216217, 'accumulated_eval_time': 1909.9838390350342, 'accumulated_logging_time': 0.05309724807739258, 'global_step': 3432, 'preemption_count': 0}), (5143, {'train/loss': 0.1215496303150489, 'validation/loss': 0.12413258553141457, 'validation/num_examples': 83274637, 'test/loss': 0.12655961052631579, 'test/num_examples': 95000000, 'score': 3628.0940811634064, 'total_duration': 6103.825448036194, 'accumulated_submission_time': 3628.0940811634064, 'accumulated_eval_time': 2475.578716993332, 'accumulated_logging_time': 0.08221197128295898, 'global_step': 5143, 'preemption_count': 0}), (6855, {'train/loss': 0.12255992529527196, 'validation/loss': 0.12381139529914732, 'validation/num_examples': 83274637, 'test/loss': 0.12617367368421054, 'test/num_examples': 95000000, 'score': 4828.352328777313, 'total_duration': 7849.805579900742, 'accumulated_submission_time': 4828.352328777313, 'accumulated_eval_time': 3021.250950574875, 'accumulated_logging_time': 0.10839366912841797, 'global_step': 6855, 'preemption_count': 0}), (8000, {'train/loss': 0.12223078169912663, 'validation/loss': 0.1237364264944199, 'validation/num_examples': 83274637, 'test/loss': 0.1260378210526316, 'test/num_examples': 95000000, 'score': 5631.866857051849, 'total_duration': 9076.109323978424, 'accumulated_submission_time': 5631.866857051849, 'accumulated_eval_time': 3444.000330686569, 'accumulated_logging_time': 0.13224267959594727, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1005 11:57:05.786021 139673521522496 submission_runner.py:552] Timing: 5631.866857051849
I1005 11:57:05.786083 139673521522496 submission_runner.py:554] Total number of evals: 6
I1005 11:57:05.786134 139673521522496 submission_runner.py:555] ====================
I1005 11:57:05.786311 139673521522496 submission_runner.py:625] Final criteo1tb score: 5631.866857051849
