python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_4 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-04-2023-06-54-36.log
2023-10-04 06:54:41.269167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1004 06:54:57.948065 140628089960256 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_4/criteo1tb_jax.
I1004 06:54:59.544897 140628089960256 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I1004 06:54:59.545639 140628089960256 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1004 06:54:59.545777 140628089960256 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1004 06:54:59.551182 140628089960256 submission_runner.py:507] Using RNG seed 555004624
I1004 06:55:05.470318 140628089960256 submission_runner.py:516] --- Tuning run 1/1 ---
I1004 06:55:05.470519 140628089960256 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_4/criteo1tb_jax/trial_1.
I1004 06:55:05.470792 140628089960256 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_4/criteo1tb_jax/trial_1/hparams.json.
I1004 06:55:05.650685 140628089960256 submission_runner.py:191] Initializing dataset.
I1004 06:55:05.650912 140628089960256 submission_runner.py:198] Initializing model.
I1004 06:55:11.210511 140628089960256 submission_runner.py:232] Initializing optimizer.
I1004 06:55:14.226530 140628089960256 submission_runner.py:239] Initializing metrics bundle.
I1004 06:55:14.226727 140628089960256 submission_runner.py:257] Initializing checkpoint and logger.
I1004 06:55:14.227772 140628089960256 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_4/criteo1tb_jax/trial_1 with prefix checkpoint_
I1004 06:55:14.227919 140628089960256 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_4/criteo1tb_jax/trial_1/meta_data_0.json.
I1004 06:55:14.228117 140628089960256 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1004 06:55:14.228178 140628089960256 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1004 06:55:15.030356 140628089960256 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_4/criteo1tb_jax/trial_1/flags_0.json.
I1004 06:55:15.118265 140628089960256 submission_runner.py:290] Starting training loop.
I1004 06:55:41.986425 140464364701440 logging_writer.py:48] [0] global_step=0, grad_norm=3.151413917541504, loss=0.34154629707336426
I1004 06:55:41.996822 140628089960256 spec.py:321] Evaluating on the training split.
I1004 06:59:32.271694 140628089960256 spec.py:333] Evaluating on the validation split.
I1004 07:03:27.405511 140628089960256 spec.py:349] Evaluating on the test split.
I1004 07:07:54.655894 140628089960256 submission_runner.py:381] Time since start: 759.54s, 	Step: 1, 	{'train/loss': 0.3445540614098123, 'validation/loss': 0.3420507014638803, 'validation/num_examples': 83274637, 'test/loss': 0.3450269052631579, 'test/num_examples': 95000000, 'score': 26.878530979156494, 'total_duration': 759.5375635623932, 'accumulated_submission_time': 26.878530979156494, 'accumulated_eval_time': 732.6589949131012, 'accumulated_logging_time': 0}
I1004 07:07:54.681468 140444742702848 logging_writer.py:48] [1] accumulated_eval_time=732.658995, accumulated_logging_time=0, accumulated_submission_time=26.878531, global_step=1, preemption_count=0, score=26.878531, test/loss=0.345027, test/num_examples=95000000, total_duration=759.537564, train/loss=0.344554, validation/loss=0.342051, validation/num_examples=83274637
I1004 07:07:54.795433 140444734310144 logging_writer.py:48] [1] global_step=1, grad_norm=3.153306484222412, loss=0.3412535488605499
I1004 07:07:54.903842 140444742702848 logging_writer.py:48] [2] global_step=2, grad_norm=2.55843186378479, loss=0.306776225566864
I1004 07:07:55.008905 140444734310144 logging_writer.py:48] [3] global_step=3, grad_norm=1.874993085861206, loss=0.2623479962348938
I1004 07:07:55.114252 140444742702848 logging_writer.py:48] [4] global_step=4, grad_norm=1.3423762321472168, loss=0.22074970602989197
I1004 07:07:55.217798 140444734310144 logging_writer.py:48] [5] global_step=5, grad_norm=0.9313974380493164, loss=0.18720194697380066
I1004 07:07:55.321688 140444742702848 logging_writer.py:48] [6] global_step=6, grad_norm=0.5971012115478516, loss=0.16268675029277802
I1004 07:07:55.427691 140444734310144 logging_writer.py:48] [7] global_step=7, grad_norm=0.3322150707244873, loss=0.14553605020046234
I1004 07:07:55.533080 140444742702848 logging_writer.py:48] [8] global_step=8, grad_norm=0.09211231023073196, loss=0.13814450800418854
I1004 07:07:55.637781 140444734310144 logging_writer.py:48] [9] global_step=9, grad_norm=0.18173570930957794, loss=0.14125585556030273
I1004 07:07:55.742137 140444742702848 logging_writer.py:48] [10] global_step=10, grad_norm=0.3346090614795685, loss=0.1459759920835495
I1004 07:07:55.846247 140444734310144 logging_writer.py:48] [11] global_step=11, grad_norm=0.4187062680721283, loss=0.14783865213394165
I1004 07:07:55.950988 140444742702848 logging_writer.py:48] [12] global_step=12, grad_norm=0.4652233421802521, loss=0.15130126476287842
I1004 07:07:56.055661 140444734310144 logging_writer.py:48] [13] global_step=13, grad_norm=0.44143810868263245, loss=0.14826005697250366
I1004 07:07:56.160831 140444742702848 logging_writer.py:48] [14] global_step=14, grad_norm=0.3731135129928589, loss=0.14286382496356964
I1004 07:07:56.265276 140444734310144 logging_writer.py:48] [15] global_step=15, grad_norm=0.28637489676475525, loss=0.13905055820941925
I1004 07:07:56.370513 140444742702848 logging_writer.py:48] [16] global_step=16, grad_norm=0.18364834785461426, loss=0.1362193524837494
I1004 07:07:56.474776 140444734310144 logging_writer.py:48] [17] global_step=17, grad_norm=0.06336770206689835, loss=0.1342647820711136
I1004 07:07:56.580125 140444742702848 logging_writer.py:48] [18] global_step=18, grad_norm=0.05758325010538101, loss=0.13436359167099
I1004 07:07:56.685820 140444734310144 logging_writer.py:48] [19] global_step=19, grad_norm=0.11079486459493637, loss=0.14058628678321838
I1004 07:07:56.790591 140444742702848 logging_writer.py:48] [20] global_step=20, grad_norm=0.1096334382891655, loss=0.14271986484527588
I1004 07:07:56.895175 140444734310144 logging_writer.py:48] [21] global_step=21, grad_norm=0.09581219404935837, loss=0.13909494876861572
I1004 07:07:56.999893 140444742702848 logging_writer.py:48] [22] global_step=22, grad_norm=0.03533231094479561, loss=0.138625368475914
I1004 07:07:57.104030 140444734310144 logging_writer.py:48] [23] global_step=23, grad_norm=0.034474555402994156, loss=0.14091235399246216
I1004 07:07:57.208786 140444742702848 logging_writer.py:48] [24] global_step=24, grad_norm=0.02911200001835823, loss=0.1373511701822281
I1004 07:07:57.312600 140444734310144 logging_writer.py:48] [25] global_step=25, grad_norm=0.03750244155526161, loss=0.14025551080703735
I1004 07:07:57.416313 140444742702848 logging_writer.py:48] [26] global_step=26, grad_norm=0.01780489832162857, loss=0.13736309111118317
I1004 07:07:57.521414 140444734310144 logging_writer.py:48] [27] global_step=27, grad_norm=0.01714850589632988, loss=0.13860461115837097
I1004 07:07:57.634649 140444742702848 logging_writer.py:48] [28] global_step=28, grad_norm=0.014811305329203606, loss=0.14061132073402405
I1004 07:07:58.232990 140444734310144 logging_writer.py:48] [29] global_step=29, grad_norm=0.019158389419317245, loss=0.13942140340805054
I1004 07:07:59.007949 140444742702848 logging_writer.py:48] [30] global_step=30, grad_norm=0.010234006680548191, loss=0.138071671128273
I1004 07:07:59.642939 140444734310144 logging_writer.py:48] [31] global_step=31, grad_norm=0.011402320116758347, loss=0.13797223567962646
I1004 07:08:00.434492 140444742702848 logging_writer.py:48] [32] global_step=32, grad_norm=0.007748706266283989, loss=0.13806863129138947
I1004 07:08:01.391647 140444734310144 logging_writer.py:48] [33] global_step=33, grad_norm=0.015087698586285114, loss=0.1395651400089264
I1004 07:08:02.199179 140444742702848 logging_writer.py:48] [34] global_step=34, grad_norm=0.03273197263479233, loss=0.13742750883102417
I1004 07:08:02.988024 140444734310144 logging_writer.py:48] [35] global_step=35, grad_norm=0.07613428682088852, loss=0.13764286041259766
I1004 07:08:03.787209 140444742702848 logging_writer.py:48] [36] global_step=36, grad_norm=0.05945144221186638, loss=0.13868382573127747
I1004 07:08:04.532194 140444734310144 logging_writer.py:48] [37] global_step=37, grad_norm=0.03818345442414284, loss=0.1348356008529663
I1004 07:08:05.241192 140444742702848 logging_writer.py:48] [38] global_step=38, grad_norm=0.03567444905638695, loss=0.13993287086486816
I1004 07:08:05.956700 140444734310144 logging_writer.py:48] [39] global_step=39, grad_norm=0.06052456796169281, loss=0.1388382464647293
I1004 07:08:06.759107 140444742702848 logging_writer.py:48] [40] global_step=40, grad_norm=0.06236834079027176, loss=0.13986223936080933
I1004 07:08:07.434369 140444734310144 logging_writer.py:48] [41] global_step=41, grad_norm=0.06950195133686066, loss=0.13999629020690918
I1004 07:08:08.164690 140444742702848 logging_writer.py:48] [42] global_step=42, grad_norm=0.06486386060714722, loss=0.13827723264694214
I1004 07:08:08.894808 140444734310144 logging_writer.py:48] [43] global_step=43, grad_norm=0.05607938393950462, loss=0.13845711946487427
I1004 07:08:09.658401 140444742702848 logging_writer.py:48] [44] global_step=44, grad_norm=0.07502085715532303, loss=0.13896337151527405
I1004 07:08:10.453635 140444734310144 logging_writer.py:48] [45] global_step=45, grad_norm=0.09246379137039185, loss=0.1399550586938858
I1004 07:08:11.155156 140444742702848 logging_writer.py:48] [46] global_step=46, grad_norm=0.1104334145784378, loss=0.13319624960422516
I1004 07:08:11.949869 140444734310144 logging_writer.py:48] [47] global_step=47, grad_norm=0.12426035851240158, loss=0.12809652090072632
I1004 07:08:12.668581 140444742702848 logging_writer.py:48] [48] global_step=48, grad_norm=0.1320711225271225, loss=0.12784568965435028
I1004 07:08:13.439187 140444734310144 logging_writer.py:48] [49] global_step=49, grad_norm=0.09373478591442108, loss=0.1256675273180008
I1004 07:08:14.372735 140444742702848 logging_writer.py:48] [50] global_step=50, grad_norm=0.07386773824691772, loss=0.12628436088562012
I1004 07:08:14.935446 140444734310144 logging_writer.py:48] [51] global_step=51, grad_norm=0.060546115040779114, loss=0.1260610818862915
I1004 07:08:15.691404 140444742702848 logging_writer.py:48] [52] global_step=52, grad_norm=0.04319148138165474, loss=0.12584850192070007
I1004 07:08:16.386950 140444734310144 logging_writer.py:48] [53] global_step=53, grad_norm=0.044719159603118896, loss=0.125929594039917
I1004 07:08:17.230941 140444742702848 logging_writer.py:48] [54] global_step=54, grad_norm=0.030980022624135017, loss=0.12307284027338028
I1004 07:08:18.024845 140444734310144 logging_writer.py:48] [55] global_step=55, grad_norm=0.022242842242121696, loss=0.12305564433336258
I1004 07:08:19.038382 140444742702848 logging_writer.py:48] [56] global_step=56, grad_norm=0.036052156239748, loss=0.12659180164337158
I1004 07:08:19.674738 140444734310144 logging_writer.py:48] [57] global_step=57, grad_norm=0.07090185582637787, loss=0.12261423468589783
I1004 07:08:20.438915 140444742702848 logging_writer.py:48] [58] global_step=58, grad_norm=0.08783213049173355, loss=0.12272873520851135
I1004 07:08:21.166161 140444734310144 logging_writer.py:48] [59] global_step=59, grad_norm=0.07043222337961197, loss=0.12418657541275024
I1004 07:08:21.891186 140444742702848 logging_writer.py:48] [60] global_step=60, grad_norm=0.05141574516892433, loss=0.12301504611968994
I1004 07:08:22.760056 140444734310144 logging_writer.py:48] [61] global_step=61, grad_norm=0.05198312923312187, loss=0.1224622055888176
I1004 07:08:23.555534 140444742702848 logging_writer.py:48] [62] global_step=62, grad_norm=0.06004411354660988, loss=0.12280969321727753
I1004 07:08:24.315930 140444734310144 logging_writer.py:48] [63] global_step=63, grad_norm=0.05791017413139343, loss=0.1221829503774643
I1004 07:08:25.084959 140444742702848 logging_writer.py:48] [64] global_step=64, grad_norm=0.0487542562186718, loss=0.12145079672336578
I1004 07:08:25.752796 140444734310144 logging_writer.py:48] [65] global_step=65, grad_norm=0.019287846982479095, loss=0.13313762843608856
I1004 07:08:26.482391 140444742702848 logging_writer.py:48] [66] global_step=66, grad_norm=0.01853795163333416, loss=0.13645078241825104
I1004 07:08:27.183500 140444734310144 logging_writer.py:48] [67] global_step=67, grad_norm=0.01967642828822136, loss=0.13696852326393127
I1004 07:08:27.995720 140444742702848 logging_writer.py:48] [68] global_step=68, grad_norm=0.040258269757032394, loss=0.13500162959098816
I1004 07:08:28.704239 140444734310144 logging_writer.py:48] [69] global_step=69, grad_norm=0.06499706953763962, loss=0.1358027309179306
I1004 07:08:29.388008 140444742702848 logging_writer.py:48] [70] global_step=70, grad_norm=0.10836993157863617, loss=0.13493669033050537
I1004 07:08:30.054473 140444734310144 logging_writer.py:48] [71] global_step=71, grad_norm=0.17476743459701538, loss=0.13664090633392334
I1004 07:08:30.914507 140444742702848 logging_writer.py:48] [72] global_step=72, grad_norm=0.1938898265361786, loss=0.138563871383667
I1004 07:08:31.433669 140444734310144 logging_writer.py:48] [73] global_step=73, grad_norm=0.1961393654346466, loss=0.1403512954711914
I1004 07:08:32.063336 140444742702848 logging_writer.py:48] [74] global_step=74, grad_norm=0.1668700873851776, loss=0.13366234302520752
I1004 07:08:32.828476 140444734310144 logging_writer.py:48] [75] global_step=75, grad_norm=0.11110515892505646, loss=0.1361587643623352
I1004 07:08:33.486112 140444742702848 logging_writer.py:48] [76] global_step=76, grad_norm=0.05244411900639534, loss=0.13434845209121704
I1004 07:08:34.231833 140444734310144 logging_writer.py:48] [77] global_step=77, grad_norm=0.01924062892794609, loss=0.13420113921165466
I1004 07:08:34.816977 140444742702848 logging_writer.py:48] [78] global_step=78, grad_norm=0.010844743810594082, loss=0.1342460960149765
I1004 07:08:35.495629 140444734310144 logging_writer.py:48] [79] global_step=79, grad_norm=0.0062892017886042595, loss=0.13483838737010956
I1004 07:08:36.235175 140444742702848 logging_writer.py:48] [80] global_step=80, grad_norm=0.0068948883563280106, loss=0.13420510292053223
I1004 07:08:36.830470 140444734310144 logging_writer.py:48] [81] global_step=81, grad_norm=0.006715628318488598, loss=0.1332932561635971
I1004 07:08:37.538540 140444742702848 logging_writer.py:48] [82] global_step=82, grad_norm=0.020797234028577805, loss=0.13718882203102112
I1004 07:08:38.266926 140444734310144 logging_writer.py:48] [83] global_step=83, grad_norm=0.03377273306250572, loss=0.13464143872261047
I1004 07:08:38.938108 140444742702848 logging_writer.py:48] [84] global_step=84, grad_norm=0.02989579364657402, loss=0.13054074347019196
I1004 07:08:39.775958 140444734310144 logging_writer.py:48] [85] global_step=85, grad_norm=0.014349707402288914, loss=0.12903745472431183
I1004 07:08:40.454354 140444742702848 logging_writer.py:48] [86] global_step=86, grad_norm=0.006101076025515795, loss=0.1276659220457077
I1004 07:08:41.024086 140444734310144 logging_writer.py:48] [87] global_step=87, grad_norm=0.010065394453704357, loss=0.12817321717739105
I1004 07:08:41.675798 140444742702848 logging_writer.py:48] [88] global_step=88, grad_norm=0.00646670488640666, loss=0.12968386709690094
I1004 07:08:42.366921 140444734310144 logging_writer.py:48] [89] global_step=89, grad_norm=0.015938762575387955, loss=0.12669160962104797
I1004 07:08:43.075665 140444742702848 logging_writer.py:48] [90] global_step=90, grad_norm=0.038145892322063446, loss=0.1303907036781311
I1004 07:08:43.816853 140444734310144 logging_writer.py:48] [91] global_step=91, grad_norm=0.04494502395391464, loss=0.12993782758712769
I1004 07:08:44.555233 140444742702848 logging_writer.py:48] [92] global_step=92, grad_norm=0.03724588826298714, loss=0.12960748374462128
I1004 07:08:45.252200 140444734310144 logging_writer.py:48] [93] global_step=93, grad_norm=0.029599957168102264, loss=0.1300283670425415
I1004 07:08:45.921008 140444742702848 logging_writer.py:48] [94] global_step=94, grad_norm=0.023988019675016403, loss=0.1300128698348999
I1004 07:08:46.651575 140444734310144 logging_writer.py:48] [95] global_step=95, grad_norm=0.029301123693585396, loss=0.12861202657222748
I1004 07:08:47.309127 140444742702848 logging_writer.py:48] [96] global_step=96, grad_norm=0.030864326283335686, loss=0.12782157957553864
I1004 07:08:48.187707 140444734310144 logging_writer.py:48] [97] global_step=97, grad_norm=0.03232256695628166, loss=0.1253715455532074
I1004 07:08:48.779224 140444742702848 logging_writer.py:48] [98] global_step=98, grad_norm=0.04569406434893608, loss=0.1268550604581833
I1004 07:08:49.611891 140444734310144 logging_writer.py:48] [99] global_step=99, grad_norm=0.06373966485261917, loss=0.1256798654794693
I1004 07:08:50.280745 140444742702848 logging_writer.py:48] [100] global_step=100, grad_norm=0.09172944724559784, loss=0.12708380818367004
I1004 07:13:37.657703 140444734310144 logging_writer.py:48] [500] global_step=500, grad_norm=0.020550549030303955, loss=0.1277637481689453
I1004 07:19:33.546941 140444742702848 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.022762300446629524, loss=0.12326030433177948
I1004 07:25:28.120225 140444734310144 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.004371520597487688, loss=0.1286981701850891
I1004 07:27:55.125441 140628089960256 spec.py:321] Evaluating on the training split.
I1004 07:30:57.973219 140628089960256 spec.py:333] Evaluating on the validation split.
I1004 07:34:08.033442 140628089960256 spec.py:349] Evaluating on the test split.
I1004 07:37:45.319712 140628089960256 submission_runner.py:381] Time since start: 2550.20s, 	Step: 1710, 	{'train/loss': 0.12240077564551395, 'validation/loss': 0.12503333998321722, 'validation/num_examples': 83274637, 'test/loss': 0.1273194105263158, 'test/num_examples': 95000000, 'score': 1227.2907950878143, 'total_duration': 2550.2013709545135, 'accumulated_submission_time': 1227.2907950878143, 'accumulated_eval_time': 1322.8532423973083, 'accumulated_logging_time': 0.03348255157470703}
I1004 07:37:45.337634 140444742702848 logging_writer.py:48] [1710] accumulated_eval_time=1322.853242, accumulated_logging_time=0.033483, accumulated_submission_time=1227.290795, global_step=1710, preemption_count=0, score=1227.290795, test/loss=0.127319, test/num_examples=95000000, total_duration=2550.201371, train/loss=0.122401, validation/loss=0.125033, validation/num_examples=83274637
I1004 07:40:56.079143 140444734310144 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.004752879962325096, loss=0.12407927960157394
I1004 07:46:55.414027 140444742702848 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.003861498087644577, loss=0.12361396104097366
I1004 07:52:46.273579 140444734310144 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.006111638154834509, loss=0.1339406818151474
I1004 07:57:45.904157 140628089960256 spec.py:321] Evaluating on the training split.
I1004 08:00:53.053065 140628089960256 spec.py:333] Evaluating on the validation split.
I1004 08:03:35.104256 140628089960256 spec.py:349] Evaluating on the test split.
I1004 08:06:21.672927 140628089960256 submission_runner.py:381] Time since start: 4266.55s, 	Step: 3420, 	{'train/loss': 0.12144299873016165, 'validation/loss': 0.12465018610648522, 'validation/num_examples': 83274637, 'test/loss': 0.1270739157894737, 'test/num_examples': 95000000, 'score': 2427.8257179260254, 'total_duration': 4266.55459022522, 'accumulated_submission_time': 2427.8257179260254, 'accumulated_eval_time': 1838.6219639778137, 'accumulated_logging_time': 0.05963611602783203}
I1004 08:06:21.696815 140444742702848 logging_writer.py:48] [3420] accumulated_eval_time=1838.621964, accumulated_logging_time=0.059636, accumulated_submission_time=2427.825718, global_step=3420, preemption_count=0, score=2427.825718, test/loss=0.127074, test/num_examples=95000000, total_duration=4266.554590, train/loss=0.121443, validation/loss=0.124650, validation/num_examples=83274637
I1004 08:07:03.756108 140444734310144 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.00322907161898911, loss=0.12569524347782135
I1004 08:12:58.507448 140444742702848 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.013928748667240143, loss=0.12173406034708023
I1004 08:19:00.945528 140444734310144 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.004952282644808292, loss=0.11992644518613815
I1004 08:24:59.334189 140444742702848 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.007473386358469725, loss=0.12472812831401825
I1004 08:26:22.168287 140628089960256 spec.py:321] Evaluating on the training split.
I1004 08:29:19.548691 140628089960256 spec.py:333] Evaluating on the validation split.
I1004 08:31:47.025488 140628089960256 spec.py:349] Evaluating on the test split.
I1004 08:34:28.704906 140628089960256 submission_runner.py:381] Time since start: 5953.59s, 	Step: 5120, 	{'train/loss': 0.12285524044396742, 'validation/loss': 0.12399308327216124, 'validation/num_examples': 83274637, 'test/loss': 0.12633754736842107, 'test/num_examples': 95000000, 'score': 3628.2634518146515, 'total_duration': 5953.58656835556, 'accumulated_submission_time': 3628.2634518146515, 'accumulated_eval_time': 2325.1585314273834, 'accumulated_logging_time': 0.09411931037902832}
I1004 08:34:28.723687 140444734310144 logging_writer.py:48] [5120] accumulated_eval_time=2325.158531, accumulated_logging_time=0.094119, accumulated_submission_time=3628.263452, global_step=5120, preemption_count=0, score=3628.263452, test/loss=0.126338, test/num_examples=95000000, total_duration=5953.586568, train/loss=0.122855, validation/loss=0.123993, validation/num_examples=83274637
I1004 08:38:46.367439 140444742702848 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.013459947891533375, loss=0.11590282618999481
I1004 08:44:42.852330 140444734310144 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.007921450771391392, loss=0.13014473021030426
I1004 08:50:43.831346 140444742702848 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.005370528437197208, loss=0.12461768090724945
I1004 08:54:29.362057 140628089960256 spec.py:321] Evaluating on the training split.
I1004 08:57:12.272796 140628089960256 spec.py:333] Evaluating on the validation split.
I1004 08:59:23.549495 140628089960256 spec.py:349] Evaluating on the test split.
I1004 09:02:02.925312 140628089960256 submission_runner.py:381] Time since start: 7607.81s, 	Step: 6817, 	{'train/loss': 0.1213660809978749, 'validation/loss': 0.12373445710727025, 'validation/num_examples': 83274637, 'test/loss': 0.12600627368421052, 'test/num_examples': 95000000, 'score': 4828.869834184647, 'total_duration': 7607.806978940964, 'accumulated_submission_time': 4828.869834184647, 'accumulated_eval_time': 2778.721745967865, 'accumulated_logging_time': 0.12146472930908203}
I1004 09:02:02.946028 140444734310144 logging_writer.py:48] [6817] accumulated_eval_time=2778.721746, accumulated_logging_time=0.121465, accumulated_submission_time=4828.869834, global_step=6817, preemption_count=0, score=4828.869834, test/loss=0.126006, test/num_examples=95000000, total_duration=7607.806979, train/loss=0.121366, validation/loss=0.123734, validation/num_examples=83274637
I1004 09:03:58.351310 140444742702848 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.005358266178518534, loss=0.12198083102703094
I1004 09:09:55.033292 140444734310144 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.011885207146406174, loss=0.11817283183336258
I1004 09:15:49.999379 140628089960256 spec.py:321] Evaluating on the training split.
I1004 09:18:12.099652 140628089960256 spec.py:333] Evaluating on the validation split.
I1004 09:20:06.816250 140628089960256 spec.py:349] Evaluating on the test split.
I1004 09:22:26.489251 140628089960256 submission_runner.py:381] Time since start: 8831.37s, 	Step: 8000, 	{'train/loss': 0.12075428992697278, 'validation/loss': 0.12367137667619013, 'validation/num_examples': 83274637, 'test/loss': 0.12598022105263157, 'test/num_examples': 95000000, 'score': 5655.899785041809, 'total_duration': 8831.370918989182, 'accumulated_submission_time': 5655.899785041809, 'accumulated_eval_time': 3175.211587190628, 'accumulated_logging_time': 0.14918017387390137}
I1004 09:22:26.504491 140444742702848 logging_writer.py:48] [8000] accumulated_eval_time=3175.211587, accumulated_logging_time=0.149180, accumulated_submission_time=5655.899785, global_step=8000, preemption_count=0, score=5655.899785, test/loss=0.125980, test/num_examples=95000000, total_duration=8831.370919, train/loss=0.120754, validation/loss=0.123671, validation/num_examples=83274637
I1004 09:22:26.520399 140444734310144 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5655.899785
I1004 09:22:32.487443 140628089960256 checkpoints.py:490] Saving checkpoint at step: 8000
I1004 09:23:07.351624 140628089960256 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_4/criteo1tb_jax/trial_1/checkpoint_8000
I1004 09:23:07.655435 140628089960256 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_4/criteo1tb_jax/trial_1/checkpoint_8000.
I1004 09:23:08.049349 140628089960256 submission_runner.py:549] Tuning trial 1/1
I1004 09:23:08.049577 140628089960256 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1004 09:23:08.050581 140628089960256 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.3445540614098123, 'validation/loss': 0.3420507014638803, 'validation/num_examples': 83274637, 'test/loss': 0.3450269052631579, 'test/num_examples': 95000000, 'score': 26.878530979156494, 'total_duration': 759.5375635623932, 'accumulated_submission_time': 26.878530979156494, 'accumulated_eval_time': 732.6589949131012, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1710, {'train/loss': 0.12240077564551395, 'validation/loss': 0.12503333998321722, 'validation/num_examples': 83274637, 'test/loss': 0.1273194105263158, 'test/num_examples': 95000000, 'score': 1227.2907950878143, 'total_duration': 2550.2013709545135, 'accumulated_submission_time': 1227.2907950878143, 'accumulated_eval_time': 1322.8532423973083, 'accumulated_logging_time': 0.03348255157470703, 'global_step': 1710, 'preemption_count': 0}), (3420, {'train/loss': 0.12144299873016165, 'validation/loss': 0.12465018610648522, 'validation/num_examples': 83274637, 'test/loss': 0.1270739157894737, 'test/num_examples': 95000000, 'score': 2427.8257179260254, 'total_duration': 4266.55459022522, 'accumulated_submission_time': 2427.8257179260254, 'accumulated_eval_time': 1838.6219639778137, 'accumulated_logging_time': 0.05963611602783203, 'global_step': 3420, 'preemption_count': 0}), (5120, {'train/loss': 0.12285524044396742, 'validation/loss': 0.12399308327216124, 'validation/num_examples': 83274637, 'test/loss': 0.12633754736842107, 'test/num_examples': 95000000, 'score': 3628.2634518146515, 'total_duration': 5953.58656835556, 'accumulated_submission_time': 3628.2634518146515, 'accumulated_eval_time': 2325.1585314273834, 'accumulated_logging_time': 0.09411931037902832, 'global_step': 5120, 'preemption_count': 0}), (6817, {'train/loss': 0.1213660809978749, 'validation/loss': 0.12373445710727025, 'validation/num_examples': 83274637, 'test/loss': 0.12600627368421052, 'test/num_examples': 95000000, 'score': 4828.869834184647, 'total_duration': 7607.806978940964, 'accumulated_submission_time': 4828.869834184647, 'accumulated_eval_time': 2778.721745967865, 'accumulated_logging_time': 0.12146472930908203, 'global_step': 6817, 'preemption_count': 0}), (8000, {'train/loss': 0.12075428992697278, 'validation/loss': 0.12367137667619013, 'validation/num_examples': 83274637, 'test/loss': 0.12598022105263157, 'test/num_examples': 95000000, 'score': 5655.899785041809, 'total_duration': 8831.370918989182, 'accumulated_submission_time': 5655.899785041809, 'accumulated_eval_time': 3175.211587190628, 'accumulated_logging_time': 0.14918017387390137, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1004 09:23:08.050721 140628089960256 submission_runner.py:552] Timing: 5655.899785041809
I1004 09:23:08.050778 140628089960256 submission_runner.py:554] Total number of evals: 6
I1004 09:23:08.050832 140628089960256 submission_runner.py:555] ====================
I1004 09:23:08.050945 140628089960256 submission_runner.py:625] Final criteo1tb score: 5655.899785041809
