python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_17 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-05-2023-17-15-55.log
2023-10-05 17:16:00.230648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1005 17:16:17.293844 140707629066048 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_17/criteo1tb_jax.
I1005 17:16:18.890239 140707629066048 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1005 17:16:18.891122 140707629066048 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1005 17:16:18.891263 140707629066048 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1005 17:16:18.896779 140707629066048 submission_runner.py:507] Using RNG seed 60677413
I1005 17:16:24.469615 140707629066048 submission_runner.py:516] --- Tuning run 1/1 ---
I1005 17:16:24.469879 140707629066048 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_17/criteo1tb_jax/trial_1.
I1005 17:16:24.470237 140707629066048 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_17/criteo1tb_jax/trial_1/hparams.json.
I1005 17:16:24.658509 140707629066048 submission_runner.py:191] Initializing dataset.
I1005 17:16:24.658803 140707629066048 submission_runner.py:198] Initializing model.
I1005 17:16:30.692681 140707629066048 submission_runner.py:232] Initializing optimizer.
I1005 17:16:33.940459 140707629066048 submission_runner.py:239] Initializing metrics bundle.
I1005 17:16:33.940732 140707629066048 submission_runner.py:257] Initializing checkpoint and logger.
I1005 17:16:33.942171 140707629066048 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_17/criteo1tb_jax/trial_1 with prefix checkpoint_
I1005 17:16:33.942350 140707629066048 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_17/criteo1tb_jax/trial_1/meta_data_0.json.
I1005 17:16:33.942598 140707629066048 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1005 17:16:33.942678 140707629066048 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1005 17:16:34.808893 140707629066048 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_17/criteo1tb_jax/trial_1/flags_0.json.
I1005 17:16:34.901174 140707629066048 submission_runner.py:290] Starting training loop.
I1005 17:17:00.883901 140543486052096 logging_writer.py:48] [0] global_step=0, grad_norm=7.834481716156006, loss=1.3742557764053345
I1005 17:17:00.896167 140707629066048 spec.py:321] Evaluating on the training split.
I1005 17:20:53.979860 140707629066048 spec.py:333] Evaluating on the validation split.
I1005 17:24:43.844374 140707629066048 spec.py:349] Evaluating on the test split.
I1005 17:29:04.265857 140707629066048 submission_runner.py:381] Time since start: 749.36s, 	Step: 1, 	{'train/loss': 1.3757441298766706, 'validation/loss': 1.3720028584453632, 'validation/num_examples': 83274637, 'test/loss': 1.3729342315789475, 'test/num_examples': 95000000, 'score': 25.994945526123047, 'total_duration': 749.3646326065063, 'accumulated_submission_time': 25.994945526123047, 'accumulated_eval_time': 723.3696386814117, 'accumulated_logging_time': 0}
I1005 17:29:04.288758 140524392531712 logging_writer.py:48] [1] accumulated_eval_time=723.369639, accumulated_logging_time=0, accumulated_submission_time=25.994946, global_step=1, preemption_count=0, score=25.994946, test/loss=1.372934, test/num_examples=95000000, total_duration=749.364633, train/loss=1.375744, validation/loss=1.372003, validation/num_examples=83274637
I1005 17:29:04.402637 140524384139008 logging_writer.py:48] [1] global_step=1, grad_norm=7.826981544494629, loss=1.374162197113037
I1005 17:29:04.511265 140524392531712 logging_writer.py:48] [2] global_step=2, grad_norm=7.100758075714111, loss=1.2833929061889648
I1005 17:29:04.615442 140524384139008 logging_writer.py:48] [3] global_step=3, grad_norm=6.195780277252197, loss=1.1440532207489014
I1005 17:29:04.719582 140524392531712 logging_writer.py:48] [4] global_step=4, grad_norm=5.44587516784668, loss=0.9837565422058105
I1005 17:29:04.822540 140524384139008 logging_writer.py:48] [5] global_step=5, grad_norm=5.140377044677734, loss=0.8092446327209473
I1005 17:29:04.925480 140524392531712 logging_writer.py:48] [6] global_step=6, grad_norm=4.912602424621582, loss=0.6200697422027588
I1005 17:29:05.028104 140524384139008 logging_writer.py:48] [7] global_step=7, grad_norm=4.131179332733154, loss=0.43504494428634644
I1005 17:29:05.132283 140524392531712 logging_writer.py:48] [8] global_step=8, grad_norm=2.7576098442077637, loss=0.2827409505844116
I1005 17:29:05.235946 140524384139008 logging_writer.py:48] [9] global_step=9, grad_norm=1.1321742534637451, loss=0.1960192322731018
I1005 17:29:05.338944 140524392531712 logging_writer.py:48] [10] global_step=10, grad_norm=0.1581195592880249, loss=0.1710321605205536
I1005 17:29:05.441594 140524384139008 logging_writer.py:48] [11] global_step=11, grad_norm=0.8083573579788208, loss=0.18449151515960693
I1005 17:29:05.544374 140524392531712 logging_writer.py:48] [12] global_step=12, grad_norm=1.2796134948730469, loss=0.21882116794586182
I1005 17:29:05.647599 140524384139008 logging_writer.py:48] [13] global_step=13, grad_norm=1.6425927877426147, loss=0.26156532764434814
I1005 17:29:05.750535 140524392531712 logging_writer.py:48] [14] global_step=14, grad_norm=1.8437236547470093, loss=0.29234203696250916
I1005 17:29:05.853962 140524384139008 logging_writer.py:48] [15] global_step=15, grad_norm=1.9870319366455078, loss=0.31437256932258606
I1005 17:29:05.958585 140524392531712 logging_writer.py:48] [16] global_step=16, grad_norm=2.1217639446258545, loss=0.3341442346572876
I1005 17:29:06.063174 140524384139008 logging_writer.py:48] [17] global_step=17, grad_norm=2.0504472255706787, loss=0.3224182426929474
I1005 17:29:06.168891 140524392531712 logging_writer.py:48] [18] global_step=18, grad_norm=1.967860221862793, loss=0.3087734878063202
I1005 17:29:06.272350 140524384139008 logging_writer.py:48] [19] global_step=19, grad_norm=1.801946759223938, loss=0.27997684478759766
I1005 17:29:06.375810 140524392531712 logging_writer.py:48] [20] global_step=20, grad_norm=1.6064811944961548, loss=0.24995139241218567
I1005 17:29:06.480672 140524384139008 logging_writer.py:48] [21] global_step=21, grad_norm=1.3540526628494263, loss=0.216908797621727
I1005 17:29:06.586660 140524392531712 logging_writer.py:48] [22] global_step=22, grad_norm=1.0216474533081055, loss=0.18577447533607483
I1005 17:29:06.691296 140524384139008 logging_writer.py:48] [23] global_step=23, grad_norm=0.6061937212944031, loss=0.1647753119468689
I1005 17:29:06.796440 140524392531712 logging_writer.py:48] [24] global_step=24, grad_norm=0.1370900273323059, loss=0.1543385088443756
I1005 17:29:06.900214 140524384139008 logging_writer.py:48] [25] global_step=25, grad_norm=0.46245959401130676, loss=0.15265464782714844
I1005 17:29:07.003090 140524392531712 logging_writer.py:48] [26] global_step=26, grad_norm=0.49557527899742126, loss=0.1531388908624649
I1005 17:29:07.106268 140524384139008 logging_writer.py:48] [27] global_step=27, grad_norm=0.2505318522453308, loss=0.14978355169296265
I1005 17:29:07.474958 140524392531712 logging_writer.py:48] [28] global_step=28, grad_norm=0.06433065980672836, loss=0.14097535610198975
I1005 17:29:08.212703 140524384139008 logging_writer.py:48] [29] global_step=29, grad_norm=0.25504758954048157, loss=0.14504554867744446
I1005 17:29:08.948313 140524392531712 logging_writer.py:48] [30] global_step=30, grad_norm=0.14506888389587402, loss=0.13934655487537384
I1005 17:29:09.591964 140524384139008 logging_writer.py:48] [31] global_step=31, grad_norm=0.10103555768728256, loss=0.14016538858413696
I1005 17:29:10.275726 140524392531712 logging_writer.py:48] [32] global_step=32, grad_norm=0.025520339608192444, loss=0.1361437737941742
I1005 17:29:10.980806 140524384139008 logging_writer.py:48] [33] global_step=33, grad_norm=0.0671372264623642, loss=0.13859760761260986
I1005 17:29:11.678352 140524392531712 logging_writer.py:48] [34] global_step=34, grad_norm=0.025943638756871223, loss=0.14175459742546082
I1005 17:29:12.403782 140524384139008 logging_writer.py:48] [35] global_step=35, grad_norm=0.04273609817028046, loss=0.1398129165172577
I1005 17:29:13.094588 140524392531712 logging_writer.py:48] [36] global_step=36, grad_norm=0.04124331474304199, loss=0.1395769715309143
I1005 17:29:13.800411 140524384139008 logging_writer.py:48] [37] global_step=37, grad_norm=0.04983266443014145, loss=0.14006011188030243
I1005 17:29:14.610679 140524392531712 logging_writer.py:48] [38] global_step=38, grad_norm=0.08835262060165405, loss=0.15019473433494568
I1005 17:29:15.403522 140524384139008 logging_writer.py:48] [39] global_step=39, grad_norm=0.1887730211019516, loss=0.1513959914445877
I1005 17:29:16.075587 140524392531712 logging_writer.py:48] [40] global_step=40, grad_norm=0.16874435544013977, loss=0.1520135998725891
I1005 17:29:16.784857 140524384139008 logging_writer.py:48] [41] global_step=41, grad_norm=0.16950730979442596, loss=0.15364469587802887
I1005 17:29:17.453570 140524392531712 logging_writer.py:48] [42] global_step=42, grad_norm=0.08511185646057129, loss=0.15233033895492554
I1005 17:29:18.254084 140524384139008 logging_writer.py:48] [43] global_step=43, grad_norm=0.04786943271756172, loss=0.15184922516345978
I1005 17:29:18.920953 140524392531712 logging_writer.py:48] [44] global_step=44, grad_norm=0.03831002488732338, loss=0.14989537000656128
I1005 17:29:19.658613 140524384139008 logging_writer.py:48] [45] global_step=45, grad_norm=0.025226907804608345, loss=0.14758147299289703
I1005 17:29:20.353297 140524392531712 logging_writer.py:48] [46] global_step=46, grad_norm=0.04126683250069618, loss=0.14556777477264404
I1005 17:29:21.010234 140524384139008 logging_writer.py:48] [47] global_step=47, grad_norm=0.05240995064377785, loss=0.14476308226585388
I1005 17:29:21.726397 140524392531712 logging_writer.py:48] [48] global_step=48, grad_norm=0.11157577484846115, loss=0.1458728015422821
I1005 17:29:22.423241 140524384139008 logging_writer.py:48] [49] global_step=49, grad_norm=0.2440546303987503, loss=0.1458810269832611
I1005 17:29:23.205177 140524392531712 logging_writer.py:48] [50] global_step=50, grad_norm=0.31675541400909424, loss=0.1477423757314682
I1005 17:29:23.845119 140524384139008 logging_writer.py:48] [51] global_step=51, grad_norm=0.3513239026069641, loss=0.14598411321640015
I1005 17:29:24.555738 140524392531712 logging_writer.py:48] [52] global_step=52, grad_norm=0.23741334676742554, loss=0.14370575547218323
I1005 17:29:25.387957 140524384139008 logging_writer.py:48] [53] global_step=53, grad_norm=0.1344297230243683, loss=0.14226025342941284
I1005 17:29:26.137303 140524392531712 logging_writer.py:48] [54] global_step=54, grad_norm=0.05743308737874031, loss=0.14089831709861755
I1005 17:29:26.890587 140524384139008 logging_writer.py:48] [55] global_step=55, grad_norm=0.030599188059568405, loss=0.14188161492347717
I1005 17:29:27.589171 140524392531712 logging_writer.py:48] [56] global_step=56, grad_norm=0.01455956231802702, loss=0.1423702985048294
I1005 17:29:28.293768 140524384139008 logging_writer.py:48] [57] global_step=57, grad_norm=0.014089493080973625, loss=0.13708342611789703
I1005 17:29:29.067602 140524392531712 logging_writer.py:48] [58] global_step=58, grad_norm=0.025170927867293358, loss=0.134622722864151
I1005 17:29:29.743820 140524384139008 logging_writer.py:48] [59] global_step=59, grad_norm=0.02404671348631382, loss=0.13544028997421265
I1005 17:29:30.381985 140524392531712 logging_writer.py:48] [60] global_step=60, grad_norm=0.05423754081130028, loss=0.1341332495212555
I1005 17:29:31.088197 140524384139008 logging_writer.py:48] [61] global_step=61, grad_norm=0.04965166002511978, loss=0.13398578763008118
I1005 17:29:31.829991 140524392531712 logging_writer.py:48] [62] global_step=62, grad_norm=0.017579151317477226, loss=0.1362549215555191
I1005 17:29:32.524178 140524384139008 logging_writer.py:48] [63] global_step=63, grad_norm=0.01742090843617916, loss=0.1343660205602646
I1005 17:29:33.247964 140524392531712 logging_writer.py:48] [64] global_step=64, grad_norm=0.030452260747551918, loss=0.1338759958744049
I1005 17:29:33.936774 140524384139008 logging_writer.py:48] [65] global_step=65, grad_norm=0.06108493730425835, loss=0.13364112377166748
I1005 17:29:34.706564 140524392531712 logging_writer.py:48] [66] global_step=66, grad_norm=0.13846620917320251, loss=0.13519267737865448
I1005 17:29:35.471622 140524384139008 logging_writer.py:48] [67] global_step=67, grad_norm=0.2497396320104599, loss=0.1364271640777588
I1005 17:29:36.225594 140524392531712 logging_writer.py:48] [68] global_step=68, grad_norm=0.46471717953681946, loss=0.13785961270332336
I1005 17:29:37.087413 140524384139008 logging_writer.py:48] [69] global_step=69, grad_norm=0.5097834467887878, loss=0.14039888978004456
I1005 17:29:37.604199 140524392531712 logging_writer.py:48] [70] global_step=70, grad_norm=0.2936179041862488, loss=0.13391339778900146
I1005 17:29:38.431173 140524384139008 logging_writer.py:48] [71] global_step=71, grad_norm=0.1738063246011734, loss=0.13268159329891205
I1005 17:29:39.110711 140524392531712 logging_writer.py:48] [72] global_step=72, grad_norm=0.13382166624069214, loss=0.13158853352069855
I1005 17:29:39.900207 140524384139008 logging_writer.py:48] [73] global_step=73, grad_norm=0.0948663055896759, loss=0.1332065910100937
I1005 17:29:40.466578 140524392531712 logging_writer.py:48] [74] global_step=74, grad_norm=0.08752278983592987, loss=0.13116377592086792
I1005 17:29:41.156683 140524384139008 logging_writer.py:48] [75] global_step=75, grad_norm=0.06277358531951904, loss=0.13047592341899872
I1005 17:29:41.858301 140524392531712 logging_writer.py:48] [76] global_step=76, grad_norm=0.060527872294187546, loss=0.13196295499801636
I1005 17:29:42.603342 140524384139008 logging_writer.py:48] [77] global_step=77, grad_norm=0.05509332939982414, loss=0.13273561000823975
I1005 17:29:43.241444 140524392531712 logging_writer.py:48] [78] global_step=78, grad_norm=0.030676623806357384, loss=0.13380098342895508
I1005 17:29:43.919970 140524384139008 logging_writer.py:48] [79] global_step=79, grad_norm=0.021303417161107063, loss=0.13293243944644928
I1005 17:29:44.560194 140524392531712 logging_writer.py:48] [80] global_step=80, grad_norm=0.039127200841903687, loss=0.1306811273097992
I1005 17:29:45.308438 140524384139008 logging_writer.py:48] [81] global_step=81, grad_norm=0.03984108194708824, loss=0.1295284479856491
I1005 17:29:45.927524 140524392531712 logging_writer.py:48] [82] global_step=82, grad_norm=0.04271308332681656, loss=0.129435196518898
I1005 17:29:46.602567 140524384139008 logging_writer.py:48] [83] global_step=83, grad_norm=0.05859130993485451, loss=0.13056711852550507
I1005 17:29:47.284716 140524392531712 logging_writer.py:48] [84] global_step=84, grad_norm=0.06954719871282578, loss=0.13012966513633728
I1005 17:29:47.987822 140524384139008 logging_writer.py:48] [85] global_step=85, grad_norm=0.09702733159065247, loss=0.13327622413635254
I1005 17:29:48.621346 140524392531712 logging_writer.py:48] [86] global_step=86, grad_norm=0.12990252673625946, loss=0.13131096959114075
I1005 17:29:49.302689 140524384139008 logging_writer.py:48] [87] global_step=87, grad_norm=0.1549784541130066, loss=0.13446742296218872
I1005 17:29:49.974463 140524392531712 logging_writer.py:48] [88] global_step=88, grad_norm=0.19289815425872803, loss=0.13169656693935394
I1005 17:29:50.624443 140524384139008 logging_writer.py:48] [89] global_step=89, grad_norm=0.18926271796226501, loss=0.13219928741455078
I1005 17:29:51.302309 140524392531712 logging_writer.py:48] [90] global_step=90, grad_norm=0.14726947247982025, loss=0.13311178982257843
I1005 17:29:51.974567 140524384139008 logging_writer.py:48] [91] global_step=91, grad_norm=0.1167188212275505, loss=0.1319328248500824
I1005 17:29:52.648129 140524392531712 logging_writer.py:48] [92] global_step=92, grad_norm=0.08506788313388824, loss=0.13246296346187592
I1005 17:29:53.319963 140524384139008 logging_writer.py:48] [93] global_step=93, grad_norm=0.07235169410705566, loss=0.13356517255306244
I1005 17:29:54.104460 140524392531712 logging_writer.py:48] [94] global_step=94, grad_norm=0.08435029536485672, loss=0.1324184387922287
I1005 17:29:54.593536 140524384139008 logging_writer.py:48] [95] global_step=95, grad_norm=0.04701554402709007, loss=0.12349007278680801
I1005 17:29:55.312576 140524392531712 logging_writer.py:48] [96] global_step=96, grad_norm=0.015441995114088058, loss=0.12322133034467697
I1005 17:29:56.022598 140524384139008 logging_writer.py:48] [97] global_step=97, grad_norm=0.02253265492618084, loss=0.12502525746822357
I1005 17:29:56.622931 140524392531712 logging_writer.py:48] [98] global_step=98, grad_norm=0.024422764778137207, loss=0.12656691670417786
I1005 17:29:57.340461 140524384139008 logging_writer.py:48] [99] global_step=99, grad_norm=0.040345389395952225, loss=0.12727148830890656
I1005 17:29:57.978936 140524392531712 logging_writer.py:48] [100] global_step=100, grad_norm=0.08184866607189178, loss=0.12758640944957733
I1005 17:34:40.970519 140524384139008 logging_writer.py:48] [500] global_step=500, grad_norm=0.011117765679955482, loss=0.1210411861538887
I1005 17:40:29.944088 140524392531712 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.02299617789685726, loss=0.12223559617996216
I1005 17:46:17.958716 140524384139008 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.015568793751299381, loss=0.1189853847026825
I1005 17:49:04.833017 140707629066048 spec.py:321] Evaluating on the training split.
I1005 17:52:00.468633 140707629066048 spec.py:333] Evaluating on the validation split.
I1005 17:55:06.372464 140707629066048 spec.py:349] Evaluating on the test split.
I1005 17:58:08.844542 140707629066048 submission_runner.py:381] Time since start: 2493.94s, 	Step: 1736, 	{'train/loss': 0.12457028275015969, 'validation/loss': 0.12532621427097904, 'validation/num_examples': 83274637, 'test/loss': 0.1277333052631579, 'test/num_examples': 95000000, 'score': 1226.5064070224762, 'total_duration': 2493.9433102607727, 'accumulated_submission_time': 1226.5064070224762, 'accumulated_eval_time': 1267.3811326026917, 'accumulated_logging_time': 0.03059983253479004}
I1005 17:58:08.863400 140524392531712 logging_writer.py:48] [1736] accumulated_eval_time=1267.381133, accumulated_logging_time=0.030600, accumulated_submission_time=1226.506407, global_step=1736, preemption_count=0, score=1226.506407, test/loss=0.127733, test/num_examples=95000000, total_duration=2493.943310, train/loss=0.124570, validation/loss=0.125326, validation/num_examples=83274637
I1005 18:01:04.567032 140524384139008 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.03417633846402168, loss=0.12953098118305206
I1005 18:06:55.324112 140524392531712 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.005399513989686966, loss=0.12694332003593445
I1005 18:12:49.187243 140524384139008 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.01767887733876705, loss=0.13312062621116638
I1005 18:18:08.980598 140707629066048 spec.py:321] Evaluating on the training split.
I1005 18:21:13.518718 140707629066048 spec.py:333] Evaluating on the validation split.
I1005 18:23:57.160643 140707629066048 spec.py:349] Evaluating on the test split.
I1005 18:26:48.209400 140707629066048 submission_runner.py:381] Time since start: 4213.31s, 	Step: 3449, 	{'train/loss': 0.12302141969308913, 'validation/loss': 0.12483195813870675, 'validation/num_examples': 83274637, 'test/loss': 0.1270222, 'test/num_examples': 95000000, 'score': 2426.5926032066345, 'total_duration': 4213.308148860931, 'accumulated_submission_time': 2426.5926032066345, 'accumulated_eval_time': 1786.6098747253418, 'accumulated_logging_time': 0.05692577362060547}
I1005 18:26:48.228437 140524392531712 logging_writer.py:48] [3449] accumulated_eval_time=1786.609875, accumulated_logging_time=0.056926, accumulated_submission_time=2426.592603, global_step=3449, preemption_count=0, score=2426.592603, test/loss=0.127022, test/num_examples=95000000, total_duration=4213.308149, train/loss=0.123021, validation/loss=0.124832, validation/num_examples=83274637
I1005 18:27:08.398472 140524384139008 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.010321292094886303, loss=0.12042932957410812
I1005 18:33:00.926880 140524392531712 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0061745052225887775, loss=0.12543657422065735
I1005 18:38:54.015860 140524384139008 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.00852367002516985, loss=0.12146793305873871
I1005 18:44:43.266892 140524392531712 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.005370356608182192, loss=0.12861567735671997
I1005 18:46:48.335372 140707629066048 spec.py:321] Evaluating on the training split.
I1005 18:49:38.268993 140707629066048 spec.py:333] Evaluating on the validation split.
I1005 18:52:14.144397 140707629066048 spec.py:349] Evaluating on the test split.
I1005 18:54:55.618882 140707629066048 submission_runner.py:381] Time since start: 5900.72s, 	Step: 5183, 	{'train/loss': 0.1223884438568691, 'validation/loss': 0.1241572989384511, 'validation/num_examples': 83274637, 'test/loss': 0.126548, 'test/num_examples': 95000000, 'score': 3626.6669471263885, 'total_duration': 5900.71764421463, 'accumulated_submission_time': 3626.6669471263885, 'accumulated_eval_time': 2273.8933658599854, 'accumulated_logging_time': 0.08497381210327148}
I1005 18:54:55.639338 140524384139008 logging_writer.py:48] [5183] accumulated_eval_time=2273.893366, accumulated_logging_time=0.084974, accumulated_submission_time=3626.666947, global_step=5183, preemption_count=0, score=3626.666947, test/loss=0.126548, test/num_examples=95000000, total_duration=5900.717644, train/loss=0.122388, validation/loss=0.124157, validation/num_examples=83274637
I1005 18:58:22.188945 140524392531712 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.007894884794950485, loss=0.12216998636722565
I1005 19:04:17.683916 140524384139008 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.010040019638836384, loss=0.12213895469903946
I1005 19:10:10.057068 140524392531712 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.005996523424983025, loss=0.11810968816280365
I1005 19:14:55.817168 140707629066048 spec.py:321] Evaluating on the training split.
I1005 19:17:41.899803 140707629066048 spec.py:333] Evaluating on the validation split.
I1005 19:19:56.299235 140707629066048 spec.py:349] Evaluating on the test split.
I1005 19:22:30.617905 140707629066048 submission_runner.py:381] Time since start: 7555.72s, 	Step: 6902, 	{'train/loss': 0.12182548810850899, 'validation/loss': 0.12387679336266576, 'validation/num_examples': 83274637, 'test/loss': 0.12611536842105264, 'test/num_examples': 95000000, 'score': 4826.814220666885, 'total_duration': 7555.716665506363, 'accumulated_submission_time': 4826.814220666885, 'accumulated_eval_time': 2728.6940603256226, 'accumulated_logging_time': 0.11271905899047852}
I1005 19:22:30.637540 140524384139008 logging_writer.py:48] [6902] accumulated_eval_time=2728.694060, accumulated_logging_time=0.112719, accumulated_submission_time=4826.814221, global_step=6902, preemption_count=0, score=4826.814221, test/loss=0.126115, test/num_examples=95000000, total_duration=7555.716666, train/loss=0.121825, validation/loss=0.123877, validation/num_examples=83274637
I1005 19:23:26.019692 140524392531712 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.010039500892162323, loss=0.12067170441150665
I1005 19:29:16.095863 140524384139008 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0046615805476903915, loss=0.1227743923664093
I1005 19:35:07.027814 140707629066048 spec.py:321] Evaluating on the training split.
I1005 19:37:35.383831 140707629066048 spec.py:333] Evaluating on the validation split.
I1005 19:39:31.053927 140707629066048 spec.py:349] Evaluating on the test split.
I1005 19:41:52.529783 140707629066048 submission_runner.py:381] Time since start: 8717.63s, 	Step: 8000, 	{'train/loss': 0.12133540747300633, 'validation/loss': 0.12376808079031314, 'validation/num_examples': 83274637, 'test/loss': 0.12604931578947368, 'test/num_examples': 95000000, 'score': 5583.181339263916, 'total_duration': 8717.628555297852, 'accumulated_submission_time': 5583.181339263916, 'accumulated_eval_time': 3134.195985555649, 'accumulated_logging_time': 0.14020156860351562}
I1005 19:41:52.546930 140524392531712 logging_writer.py:48] [8000] accumulated_eval_time=3134.195986, accumulated_logging_time=0.140202, accumulated_submission_time=5583.181339, global_step=8000, preemption_count=0, score=5583.181339, test/loss=0.126049, test/num_examples=95000000, total_duration=8717.628555, train/loss=0.121335, validation/loss=0.123768, validation/num_examples=83274637
I1005 19:41:52.560684 140524384139008 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5583.181339
I1005 19:41:58.593136 140707629066048 checkpoints.py:490] Saving checkpoint at step: 8000
I1005 19:42:34.307708 140707629066048 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_17/criteo1tb_jax/trial_1/checkpoint_8000
I1005 19:42:34.636407 140707629066048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_17/criteo1tb_jax/trial_1/checkpoint_8000.
I1005 19:42:35.032154 140707629066048 submission_runner.py:549] Tuning trial 1/1
I1005 19:42:35.032456 140707629066048 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1005 19:42:35.033915 140707629066048 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 1.3757441298766706, 'validation/loss': 1.3720028584453632, 'validation/num_examples': 83274637, 'test/loss': 1.3729342315789475, 'test/num_examples': 95000000, 'score': 25.994945526123047, 'total_duration': 749.3646326065063, 'accumulated_submission_time': 25.994945526123047, 'accumulated_eval_time': 723.3696386814117, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1736, {'train/loss': 0.12457028275015969, 'validation/loss': 0.12532621427097904, 'validation/num_examples': 83274637, 'test/loss': 0.1277333052631579, 'test/num_examples': 95000000, 'score': 1226.5064070224762, 'total_duration': 2493.9433102607727, 'accumulated_submission_time': 1226.5064070224762, 'accumulated_eval_time': 1267.3811326026917, 'accumulated_logging_time': 0.03059983253479004, 'global_step': 1736, 'preemption_count': 0}), (3449, {'train/loss': 0.12302141969308913, 'validation/loss': 0.12483195813870675, 'validation/num_examples': 83274637, 'test/loss': 0.1270222, 'test/num_examples': 95000000, 'score': 2426.5926032066345, 'total_duration': 4213.308148860931, 'accumulated_submission_time': 2426.5926032066345, 'accumulated_eval_time': 1786.6098747253418, 'accumulated_logging_time': 0.05692577362060547, 'global_step': 3449, 'preemption_count': 0}), (5183, {'train/loss': 0.1223884438568691, 'validation/loss': 0.1241572989384511, 'validation/num_examples': 83274637, 'test/loss': 0.126548, 'test/num_examples': 95000000, 'score': 3626.6669471263885, 'total_duration': 5900.71764421463, 'accumulated_submission_time': 3626.6669471263885, 'accumulated_eval_time': 2273.8933658599854, 'accumulated_logging_time': 0.08497381210327148, 'global_step': 5183, 'preemption_count': 0}), (6902, {'train/loss': 0.12182548810850899, 'validation/loss': 0.12387679336266576, 'validation/num_examples': 83274637, 'test/loss': 0.12611536842105264, 'test/num_examples': 95000000, 'score': 4826.814220666885, 'total_duration': 7555.716665506363, 'accumulated_submission_time': 4826.814220666885, 'accumulated_eval_time': 2728.6940603256226, 'accumulated_logging_time': 0.11271905899047852, 'global_step': 6902, 'preemption_count': 0}), (8000, {'train/loss': 0.12133540747300633, 'validation/loss': 0.12376808079031314, 'validation/num_examples': 83274637, 'test/loss': 0.12604931578947368, 'test/num_examples': 95000000, 'score': 5583.181339263916, 'total_duration': 8717.628555297852, 'accumulated_submission_time': 5583.181339263916, 'accumulated_eval_time': 3134.195985555649, 'accumulated_logging_time': 0.14020156860351562, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1005 19:42:35.034088 140707629066048 submission_runner.py:552] Timing: 5583.181339263916
I1005 19:42:35.034197 140707629066048 submission_runner.py:554] Total number of evals: 6
I1005 19:42:35.034270 140707629066048 submission_runner.py:555] ====================
I1005 19:42:35.034425 140707629066048 submission_runner.py:625] Final criteo1tb score: 5583.181339263916
