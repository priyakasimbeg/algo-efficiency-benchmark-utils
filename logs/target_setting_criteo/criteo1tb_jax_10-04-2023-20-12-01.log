python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_9 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-04-2023-20-12-01.log
2023-10-04 20:12:06.572305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1004 20:12:23.249152 140565235881792 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_9/criteo1tb_jax.
I1004 20:12:24.868903 140565235881792 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I1004 20:12:24.870205 140565235881792 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1004 20:12:24.870349 140565235881792 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1004 20:12:24.875811 140565235881792 submission_runner.py:507] Using RNG seed 1088447404
I1004 20:12:30.416357 140565235881792 submission_runner.py:516] --- Tuning run 1/1 ---
I1004 20:12:30.416608 140565235881792 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_9/criteo1tb_jax/trial_1.
I1004 20:12:30.416936 140565235881792 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_9/criteo1tb_jax/trial_1/hparams.json.
I1004 20:12:30.607342 140565235881792 submission_runner.py:191] Initializing dataset.
I1004 20:12:30.607656 140565235881792 submission_runner.py:198] Initializing model.
I1004 20:12:36.661033 140565235881792 submission_runner.py:232] Initializing optimizer.
I1004 20:12:39.877043 140565235881792 submission_runner.py:239] Initializing metrics bundle.
I1004 20:12:39.877336 140565235881792 submission_runner.py:257] Initializing checkpoint and logger.
I1004 20:12:39.878793 140565235881792 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_9/criteo1tb_jax/trial_1 with prefix checkpoint_
I1004 20:12:39.878979 140565235881792 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_9/criteo1tb_jax/trial_1/meta_data_0.json.
I1004 20:12:39.879240 140565235881792 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1004 20:12:39.879320 140565235881792 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1004 20:12:40.728258 140565235881792 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_9/criteo1tb_jax/trial_1/flags_0.json.
I1004 20:12:40.818886 140565235881792 submission_runner.py:290] Starting training loop.
I1004 20:13:06.208518 140401491040000 logging_writer.py:48] [0] global_step=0, grad_norm=2.9909071922302246, loss=0.37813276052474976
I1004 20:13:06.220172 140565235881792 spec.py:321] Evaluating on the training split.
I1004 20:16:52.619050 140565235881792 spec.py:333] Evaluating on the validation split.
I1004 20:20:42.821834 140565235881792 spec.py:349] Evaluating on the test split.
I1004 20:25:04.072905 140565235881792 submission_runner.py:381] Time since start: 743.25s, 	Step: 1, 	{'train/loss': 0.37617974911095964, 'validation/loss': 0.37911396719747936, 'validation/num_examples': 83274637, 'test/loss': 0.37799124210526314, 'test/num_examples': 95000000, 'score': 25.401227235794067, 'total_duration': 743.2539710998535, 'accumulated_submission_time': 25.401227235794067, 'accumulated_eval_time': 717.8526861667633, 'accumulated_logging_time': 0}
I1004 20:25:04.093489 140383254198016 logging_writer.py:48] [1] accumulated_eval_time=717.852686, accumulated_logging_time=0, accumulated_submission_time=25.401227, global_step=1, preemption_count=0, score=25.401227, test/loss=0.377991, test/num_examples=95000000, total_duration=743.253971, train/loss=0.376180, validation/loss=0.379114, validation/num_examples=83274637
I1004 20:25:04.207456 140383245805312 logging_writer.py:48] [1] global_step=1, grad_norm=2.983628273010254, loss=0.3786194324493408
I1004 20:25:04.313133 140383254198016 logging_writer.py:48] [2] global_step=2, grad_norm=2.72426700592041, loss=0.34252026677131653
I1004 20:25:04.418752 140383245805312 logging_writer.py:48] [3] global_step=3, grad_norm=2.2343060970306396, loss=0.2890361547470093
I1004 20:25:04.522888 140383254198016 logging_writer.py:48] [4] global_step=4, grad_norm=1.6310596466064453, loss=0.23395177721977234
I1004 20:25:04.626611 140383245805312 logging_writer.py:48] [5] global_step=5, grad_norm=0.9452674984931946, loss=0.18940511345863342
I1004 20:25:04.730702 140383254198016 logging_writer.py:48] [6] global_step=6, grad_norm=0.2744470536708832, loss=0.16752544045448303
I1004 20:25:04.834946 140383245805312 logging_writer.py:48] [7] global_step=7, grad_norm=0.35784611105918884, loss=0.17075330018997192
I1004 20:25:04.940427 140383254198016 logging_writer.py:48] [8] global_step=8, grad_norm=0.7262993454933167, loss=0.18238383531570435
I1004 20:25:05.045023 140383245805312 logging_writer.py:48] [9] global_step=9, grad_norm=0.9845336079597473, loss=0.2008824646472931
I1004 20:25:05.149341 140383254198016 logging_writer.py:48] [10] global_step=10, grad_norm=1.1072601079940796, loss=0.2133127599954605
I1004 20:25:05.253910 140383245805312 logging_writer.py:48] [11] global_step=11, grad_norm=1.1150959730148315, loss=0.21220718324184418
I1004 20:25:05.357267 140383254198016 logging_writer.py:48] [12] global_step=12, grad_norm=1.0726864337921143, loss=0.20701667666435242
I1004 20:25:05.461784 140383245805312 logging_writer.py:48] [13] global_step=13, grad_norm=0.9894216656684875, loss=0.19951178133487701
I1004 20:25:05.567124 140383254198016 logging_writer.py:48] [14] global_step=14, grad_norm=0.8085317611694336, loss=0.18499000370502472
I1004 20:25:05.672811 140383245805312 logging_writer.py:48] [15] global_step=15, grad_norm=0.5746696591377258, loss=0.1709415465593338
I1004 20:25:05.777509 140383254198016 logging_writer.py:48] [16] global_step=16, grad_norm=0.27869799733161926, loss=0.15728864073753357
I1004 20:25:05.881451 140383245805312 logging_writer.py:48] [17] global_step=17, grad_norm=0.0632534846663475, loss=0.15543381869792938
I1004 20:25:05.986063 140383254198016 logging_writer.py:48] [18] global_step=18, grad_norm=0.1828044354915619, loss=0.1589975655078888
I1004 20:25:06.090513 140383245805312 logging_writer.py:48] [19] global_step=19, grad_norm=0.28955313563346863, loss=0.14420881867408752
I1004 20:25:06.195048 140383254198016 logging_writer.py:48] [20] global_step=20, grad_norm=0.1390894204378128, loss=0.13811536133289337
I1004 20:25:06.304837 140383245805312 logging_writer.py:48] [21] global_step=21, grad_norm=0.052779532968997955, loss=0.13645219802856445
I1004 20:25:06.411665 140383254198016 logging_writer.py:48] [22] global_step=22, grad_norm=0.1118028461933136, loss=0.13633283972740173
I1004 20:25:06.516844 140383245805312 logging_writer.py:48] [23] global_step=23, grad_norm=0.0850413516163826, loss=0.13716883957386017
I1004 20:25:06.621294 140383254198016 logging_writer.py:48] [24] global_step=24, grad_norm=0.030195852741599083, loss=0.13746050000190735
I1004 20:25:06.726807 140383245805312 logging_writer.py:48] [25] global_step=25, grad_norm=0.022940820083022118, loss=0.13725873827934265
I1004 20:25:06.832874 140383254198016 logging_writer.py:48] [26] global_step=26, grad_norm=0.02821638621389866, loss=0.1347126066684723
I1004 20:25:06.938584 140383245805312 logging_writer.py:48] [27] global_step=27, grad_norm=0.03395550698041916, loss=0.1381811499595642
I1004 20:25:07.042480 140383254198016 logging_writer.py:48] [28] global_step=28, grad_norm=0.02937653288245201, loss=0.13682714104652405
I1004 20:25:07.754470 140383245805312 logging_writer.py:48] [29] global_step=29, grad_norm=0.019296668469905853, loss=0.1370360404253006
I1004 20:25:08.357100 140383254198016 logging_writer.py:48] [30] global_step=30, grad_norm=0.02026713639497757, loss=0.13498619198799133
I1004 20:25:09.053561 140383245805312 logging_writer.py:48] [31] global_step=31, grad_norm=0.022603029385209084, loss=0.13529197871685028
I1004 20:25:10.052072 140383254198016 logging_writer.py:48] [32] global_step=32, grad_norm=0.025462890043854713, loss=0.13476699590682983
I1004 20:25:10.611504 140383245805312 logging_writer.py:48] [33] global_step=33, grad_norm=0.02694789133965969, loss=0.13439320027828217
I1004 20:25:11.370118 140383254198016 logging_writer.py:48] [34] global_step=34, grad_norm=0.040026310831308365, loss=0.1343897134065628
I1004 20:25:12.039281 140383245805312 logging_writer.py:48] [35] global_step=35, grad_norm=0.05397015064954758, loss=0.13210991024971008
I1004 20:25:13.021618 140383254198016 logging_writer.py:48] [36] global_step=36, grad_norm=0.06499380618333817, loss=0.13568846881389618
I1004 20:25:13.666128 140383245805312 logging_writer.py:48] [37] global_step=37, grad_norm=0.0941191166639328, loss=0.13408410549163818
I1004 20:25:14.184478 140383254198016 logging_writer.py:48] [38] global_step=38, grad_norm=0.16226930916309357, loss=0.13318215310573578
I1004 20:25:14.852621 140383245805312 logging_writer.py:48] [39] global_step=39, grad_norm=0.17859502136707306, loss=0.1348177045583725
I1004 20:25:15.588578 140383254198016 logging_writer.py:48] [40] global_step=40, grad_norm=0.10047562420368195, loss=0.13351795077323914
I1004 20:25:16.289399 140383245805312 logging_writer.py:48] [41] global_step=41, grad_norm=0.04740648716688156, loss=0.13248075544834137
I1004 20:25:17.021434 140383254198016 logging_writer.py:48] [42] global_step=42, grad_norm=0.01796139031648636, loss=0.1338927298784256
I1004 20:25:17.720904 140383245805312 logging_writer.py:48] [43] global_step=43, grad_norm=0.008908206596970558, loss=0.1304594874382019
I1004 20:25:18.524292 140383254198016 logging_writer.py:48] [44] global_step=44, grad_norm=0.012420355342328548, loss=0.1292598396539688
I1004 20:25:19.236263 140383245805312 logging_writer.py:48] [45] global_step=45, grad_norm=0.00821214821189642, loss=0.13022734224796295
I1004 20:25:19.967125 140383254198016 logging_writer.py:48] [46] global_step=46, grad_norm=0.015872500836849213, loss=0.12873396277427673
I1004 20:25:20.672089 140383245805312 logging_writer.py:48] [47] global_step=47, grad_norm=0.03431856632232666, loss=0.12711764872074127
I1004 20:25:21.452981 140383254198016 logging_writer.py:48] [48] global_step=48, grad_norm=0.09273095428943634, loss=0.12987494468688965
I1004 20:25:22.157052 140383245805312 logging_writer.py:48] [49] global_step=49, grad_norm=0.19289584457874298, loss=0.12947994470596313
I1004 20:25:22.766494 140383254198016 logging_writer.py:48] [50] global_step=50, grad_norm=0.33287313580513, loss=0.13426345586776733
I1004 20:25:23.553114 140383245805312 logging_writer.py:48] [51] global_step=51, grad_norm=0.2944091558456421, loss=0.13062715530395508
I1004 20:25:24.228190 140383254198016 logging_writer.py:48] [52] global_step=52, grad_norm=0.108728788793087, loss=0.12567520141601562
I1004 20:25:25.039794 140383245805312 logging_writer.py:48] [53] global_step=53, grad_norm=0.022658398374915123, loss=0.1255451738834381
I1004 20:25:25.753410 140383254198016 logging_writer.py:48] [54] global_step=54, grad_norm=0.008279935456812382, loss=0.12770166993141174
I1004 20:25:26.500245 140383245805312 logging_writer.py:48] [55] global_step=55, grad_norm=0.035580504685640335, loss=0.12604019045829773
I1004 20:25:27.106057 140383254198016 logging_writer.py:48] [56] global_step=56, grad_norm=0.03178130090236664, loss=0.12656719982624054
I1004 20:25:27.809473 140383245805312 logging_writer.py:48] [57] global_step=57, grad_norm=0.013249075040221214, loss=0.12869074940681458
I1004 20:25:28.494893 140383254198016 logging_writer.py:48] [58] global_step=58, grad_norm=0.009569147601723671, loss=0.1290532648563385
I1004 20:25:29.254687 140383245805312 logging_writer.py:48] [59] global_step=59, grad_norm=0.015307961031794548, loss=0.1324431151151657
I1004 20:25:29.974619 140383254198016 logging_writer.py:48] [60] global_step=60, grad_norm=0.03108958713710308, loss=0.13036863505840302
I1004 20:25:30.710941 140383245805312 logging_writer.py:48] [61] global_step=61, grad_norm=0.030209921300411224, loss=0.12940174341201782
I1004 20:25:31.609891 140383254198016 logging_writer.py:48] [62] global_step=62, grad_norm=0.016622327268123627, loss=0.1307787448167801
I1004 20:25:32.205129 140383245805312 logging_writer.py:48] [63] global_step=63, grad_norm=0.009090278297662735, loss=0.12893719971179962
I1004 20:25:33.064148 140383254198016 logging_writer.py:48] [64] global_step=64, grad_norm=0.006046709138900042, loss=0.12971945106983185
I1004 20:25:33.729676 140383245805312 logging_writer.py:48] [65] global_step=65, grad_norm=0.006396114360541105, loss=0.13093028962612152
I1004 20:25:34.491443 140383254198016 logging_writer.py:48] [66] global_step=66, grad_norm=0.019473569467663765, loss=0.13032788038253784
I1004 20:25:35.142511 140383245805312 logging_writer.py:48] [67] global_step=67, grad_norm=0.025362113490700722, loss=0.12981802225112915
I1004 20:25:35.790848 140383254198016 logging_writer.py:48] [68] global_step=68, grad_norm=0.037003688514232635, loss=0.12983821332454681
I1004 20:25:36.685568 140383245805312 logging_writer.py:48] [69] global_step=69, grad_norm=0.06858575344085693, loss=0.13017642498016357
I1004 20:25:37.353218 140383254198016 logging_writer.py:48] [70] global_step=70, grad_norm=0.10391687601804733, loss=0.1295035034418106
I1004 20:25:38.091044 140383245805312 logging_writer.py:48] [71] global_step=71, grad_norm=0.10734990239143372, loss=0.12773945927619934
I1004 20:25:38.760429 140383254198016 logging_writer.py:48] [72] global_step=72, grad_norm=0.08908642828464508, loss=0.1284642517566681
I1004 20:25:39.489025 140383245805312 logging_writer.py:48] [73] global_step=73, grad_norm=0.07716573029756546, loss=0.12859748303890228
I1004 20:25:40.201389 140383254198016 logging_writer.py:48] [74] global_step=74, grad_norm=0.05779043585062027, loss=0.1297260820865631
I1004 20:25:40.932453 140383245805312 logging_writer.py:48] [75] global_step=75, grad_norm=0.03324146941304207, loss=0.12697431445121765
I1004 20:25:41.756224 140383254198016 logging_writer.py:48] [76] global_step=76, grad_norm=0.006947687827050686, loss=0.13389644026756287
I1004 20:25:42.492778 140383245805312 logging_writer.py:48] [77] global_step=77, grad_norm=0.01451386883854866, loss=0.13713538646697998
I1004 20:25:43.062647 140383254198016 logging_writer.py:48] [78] global_step=78, grad_norm=0.013237960636615753, loss=0.137551948428154
I1004 20:25:43.937827 140383245805312 logging_writer.py:48] [79] global_step=79, grad_norm=0.020033542066812515, loss=0.1376645267009735
I1004 20:25:44.524447 140383254198016 logging_writer.py:48] [80] global_step=80, grad_norm=0.031079838052392006, loss=0.13908956944942474
I1004 20:25:45.428783 140383245805312 logging_writer.py:48] [81] global_step=81, grad_norm=0.04808096960186958, loss=0.13787494599819183
I1004 20:25:46.030984 140383254198016 logging_writer.py:48] [82] global_step=82, grad_norm=0.0572776161134243, loss=0.13579317927360535
I1004 20:25:46.762543 140383245805312 logging_writer.py:48] [83] global_step=83, grad_norm=0.061684638261795044, loss=0.13791373372077942
I1004 20:25:47.495559 140383254198016 logging_writer.py:48] [84] global_step=84, grad_norm=0.07937654107809067, loss=0.1378566324710846
I1004 20:25:48.302970 140383245805312 logging_writer.py:48] [85] global_step=85, grad_norm=0.11015443503856659, loss=0.13728030025959015
I1004 20:25:48.894894 140383254198016 logging_writer.py:48] [86] global_step=86, grad_norm=0.13367699086666107, loss=0.1375609040260315
I1004 20:25:49.566344 140383245805312 logging_writer.py:48] [87] global_step=87, grad_norm=0.15460865199565887, loss=0.13641206920146942
I1004 20:25:50.217997 140383254198016 logging_writer.py:48] [88] global_step=88, grad_norm=0.15123243629932404, loss=0.13647551834583282
I1004 20:25:50.864554 140383245805312 logging_writer.py:48] [89] global_step=89, grad_norm=0.11575700342655182, loss=0.1371222287416458
I1004 20:25:51.523214 140383254198016 logging_writer.py:48] [90] global_step=90, grad_norm=0.07252576947212219, loss=0.13444775342941284
I1004 20:25:52.189119 140383245805312 logging_writer.py:48] [91] global_step=91, grad_norm=0.04111222177743912, loss=0.1349899172782898
I1004 20:25:52.864087 140383254198016 logging_writer.py:48] [92] global_step=92, grad_norm=0.02544606849551201, loss=0.13632890582084656
I1004 20:25:53.576478 140383245805312 logging_writer.py:48] [93] global_step=93, grad_norm=0.016029348596930504, loss=0.13681861758232117
I1004 20:25:54.279632 140383254198016 logging_writer.py:48] [94] global_step=94, grad_norm=0.006605579052120447, loss=0.13492874801158905
I1004 20:25:54.958098 140383245805312 logging_writer.py:48] [95] global_step=95, grad_norm=0.007222404237836599, loss=0.13327279686927795
I1004 20:25:55.702023 140383254198016 logging_writer.py:48] [96] global_step=96, grad_norm=0.013039451092481613, loss=0.1309332251548767
I1004 20:25:56.535311 140383245805312 logging_writer.py:48] [97] global_step=97, grad_norm=0.02011636272072792, loss=0.13119974732398987
I1004 20:25:57.117974 140383254198016 logging_writer.py:48] [98] global_step=98, grad_norm=0.023621704429388046, loss=0.13005632162094116
I1004 20:25:57.826210 140383245805312 logging_writer.py:48] [99] global_step=99, grad_norm=0.032901495695114136, loss=0.1297057867050171
I1004 20:25:58.553926 140383254198016 logging_writer.py:48] [100] global_step=100, grad_norm=0.02526979148387909, loss=0.13010504841804504
I1004 20:30:39.349317 140383245805312 logging_writer.py:48] [500] global_step=500, grad_norm=0.006059967912733555, loss=0.12346428632736206
I1004 20:36:32.486651 140383254198016 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.006706410553306341, loss=0.12424246966838837
I1004 20:42:23.903254 140383245805312 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.004686566069722176, loss=0.11742652952671051
I1004 20:45:04.444107 140565235881792 spec.py:321] Evaluating on the training split.
I1004 20:48:12.887472 140565235881792 spec.py:333] Evaluating on the validation split.
I1004 20:51:23.269886 140565235881792 spec.py:349] Evaluating on the test split.
I1004 20:55:04.345606 140565235881792 submission_runner.py:381] Time since start: 2543.53s, 	Step: 1730, 	{'train/loss': 0.12394317291067831, 'validation/loss': 0.12534417892449054, 'validation/num_examples': 83274637, 'test/loss': 0.12784354736842105, 'test/num_examples': 95000000, 'score': 1225.7206552028656, 'total_duration': 2543.5266573429108, 'accumulated_submission_time': 1225.7206552028656, 'accumulated_eval_time': 1317.7541477680206, 'accumulated_logging_time': 0.02829575538635254}
I1004 20:55:04.362845 140383254198016 logging_writer.py:48] [1730] accumulated_eval_time=1317.754148, accumulated_logging_time=0.028296, accumulated_submission_time=1225.720655, global_step=1730, preemption_count=0, score=1225.720655, test/loss=0.127844, test/num_examples=95000000, total_duration=2543.526657, train/loss=0.123943, validation/loss=0.125344, validation/num_examples=83274637
I1004 20:58:01.386219 140383245805312 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.010809162631630898, loss=0.12185446918010712
I1004 21:03:55.608023 140383254198016 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.00567649956792593, loss=0.11735236644744873
I1004 21:09:46.495999 140383245805312 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.006363372318446636, loss=0.12097227573394775
I1004 21:15:05.017448 140565235881792 spec.py:321] Evaluating on the training split.
I1004 21:18:00.855666 140565235881792 spec.py:333] Evaluating on the validation split.
I1004 21:21:00.998559 140565235881792 spec.py:349] Evaluating on the test split.
I1004 21:23:56.415099 140565235881792 submission_runner.py:381] Time since start: 4275.60s, 	Step: 3455, 	{'train/loss': 0.1252915124473332, 'validation/loss': 0.12462692572289448, 'validation/num_examples': 83274637, 'test/loss': 0.1270489052631579, 'test/num_examples': 95000000, 'score': 2426.344469308853, 'total_duration': 4275.596145629883, 'accumulated_submission_time': 2426.344469308853, 'accumulated_eval_time': 1849.1517560482025, 'accumulated_logging_time': 0.053259849548339844}
I1004 21:23:56.434698 140383254198016 logging_writer.py:48] [3455] accumulated_eval_time=1849.151756, accumulated_logging_time=0.053260, accumulated_submission_time=2426.344469, global_step=3455, preemption_count=0, score=2426.344469, test/loss=0.127049, test/num_examples=95000000, total_duration=4275.596146, train/loss=0.125292, validation/loss=0.124627, validation/num_examples=83274637
I1004 21:24:12.198218 140383245805312 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.005708875600248575, loss=0.12146314978599548
I1004 21:30:05.373605 140383254198016 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.005624155979603529, loss=0.12460870295763016
I1004 21:35:58.262467 140383245805312 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.003625758457928896, loss=0.11939304322004318
I1004 21:41:53.987619 140383254198016 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.020882155746221542, loss=0.1261066198348999
I1004 21:43:56.692011 140565235881792 spec.py:321] Evaluating on the training split.
I1004 21:46:53.522904 140565235881792 spec.py:333] Evaluating on the validation split.
I1004 21:49:28.247313 140565235881792 spec.py:349] Evaluating on the test split.
I1004 21:53:01.390371 140565235881792 submission_runner.py:381] Time since start: 6020.57s, 	Step: 5176, 	{'train/loss': 0.12214810593323137, 'validation/loss': 0.12417491534667392, 'validation/num_examples': 83274637, 'test/loss': 0.1265168, 'test/num_examples': 95000000, 'score': 3626.570088624954, 'total_duration': 6020.571435689926, 'accumulated_submission_time': 3626.570088624954, 'accumulated_eval_time': 2393.850077867508, 'accumulated_logging_time': 0.08091497421264648}
I1004 21:53:01.408775 140383245805312 logging_writer.py:48] [5176] accumulated_eval_time=2393.850078, accumulated_logging_time=0.080915, accumulated_submission_time=3626.570089, global_step=5176, preemption_count=0, score=3626.570089, test/loss=0.126517, test/num_examples=95000000, total_duration=6020.571436, train/loss=0.122148, validation/loss=0.124175, validation/num_examples=83274637
I1004 21:56:36.600636 140383254198016 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.006560268346220255, loss=0.12288238108158112
I1004 22:02:28.588723 140383245805312 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.006858399137854576, loss=0.11085180938243866
I1004 22:08:24.895260 140383254198016 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.008655245415866375, loss=0.11947096139192581
I1004 22:13:01.595185 140565235881792 spec.py:321] Evaluating on the training split.
I1004 22:15:53.721951 140565235881792 spec.py:333] Evaluating on the validation split.
I1004 22:18:10.543277 140565235881792 spec.py:349] Evaluating on the test split.
I1004 22:21:33.723958 140565235881792 submission_runner.py:381] Time since start: 7732.91s, 	Step: 6892, 	{'train/loss': 0.12084553076786066, 'validation/loss': 0.1238156823187353, 'validation/num_examples': 83274637, 'test/loss': 0.1261455157894737, 'test/num_examples': 95000000, 'score': 4826.7261509895325, 'total_duration': 7732.905031442642, 'accumulated_submission_time': 4826.7261509895325, 'accumulated_eval_time': 2905.9788336753845, 'accumulated_logging_time': 0.1067049503326416}
I1004 22:21:33.743692 140383245805312 logging_writer.py:48] [6892] accumulated_eval_time=2905.978834, accumulated_logging_time=0.106705, accumulated_submission_time=4826.726151, global_step=6892, preemption_count=0, score=4826.726151, test/loss=0.126146, test/num_examples=95000000, total_duration=7732.905031, train/loss=0.120846, validation/loss=0.123816, validation/num_examples=83274637
I1004 22:22:36.418318 140383254198016 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.008129405789077282, loss=0.11970661580562592
I1004 22:28:36.788336 140383245805312 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.014499149285256863, loss=0.12452803552150726
I1004 22:34:35.004179 140565235881792 spec.py:321] Evaluating on the training split.
I1004 22:37:00.860702 140565235881792 spec.py:333] Evaluating on the validation split.
I1004 22:38:57.642971 140565235881792 spec.py:349] Evaluating on the test split.
I1004 22:41:50.849831 140565235881792 submission_runner.py:381] Time since start: 8950.03s, 	Step: 8000, 	{'train/loss': 0.12209711134808618, 'validation/loss': 0.12372899325877577, 'validation/num_examples': 83274637, 'test/loss': 0.12603075789473683, 'test/num_examples': 95000000, 'score': 5607.964383840561, 'total_duration': 8950.030904531479, 'accumulated_submission_time': 5607.964383840561, 'accumulated_eval_time': 3341.8244540691376, 'accumulated_logging_time': 0.1338210105895996}
I1004 22:41:50.867929 140383254198016 logging_writer.py:48] [8000] accumulated_eval_time=3341.824454, accumulated_logging_time=0.133821, accumulated_submission_time=5607.964384, global_step=8000, preemption_count=0, score=5607.964384, test/loss=0.126031, test/num_examples=95000000, total_duration=8950.030905, train/loss=0.122097, validation/loss=0.123729, validation/num_examples=83274637
I1004 22:41:50.883924 140383245805312 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5607.964384
I1004 22:41:56.683291 140565235881792 checkpoints.py:490] Saving checkpoint at step: 8000
I1004 22:42:32.065475 140565235881792 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_9/criteo1tb_jax/trial_1/checkpoint_8000
I1004 22:42:32.387613 140565235881792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_9/criteo1tb_jax/trial_1/checkpoint_8000.
I1004 22:42:32.758075 140565235881792 submission_runner.py:549] Tuning trial 1/1
I1004 22:42:32.758378 140565235881792 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1004 22:42:32.759632 140565235881792 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.37617974911095964, 'validation/loss': 0.37911396719747936, 'validation/num_examples': 83274637, 'test/loss': 0.37799124210526314, 'test/num_examples': 95000000, 'score': 25.401227235794067, 'total_duration': 743.2539710998535, 'accumulated_submission_time': 25.401227235794067, 'accumulated_eval_time': 717.8526861667633, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1730, {'train/loss': 0.12394317291067831, 'validation/loss': 0.12534417892449054, 'validation/num_examples': 83274637, 'test/loss': 0.12784354736842105, 'test/num_examples': 95000000, 'score': 1225.7206552028656, 'total_duration': 2543.5266573429108, 'accumulated_submission_time': 1225.7206552028656, 'accumulated_eval_time': 1317.7541477680206, 'accumulated_logging_time': 0.02829575538635254, 'global_step': 1730, 'preemption_count': 0}), (3455, {'train/loss': 0.1252915124473332, 'validation/loss': 0.12462692572289448, 'validation/num_examples': 83274637, 'test/loss': 0.1270489052631579, 'test/num_examples': 95000000, 'score': 2426.344469308853, 'total_duration': 4275.596145629883, 'accumulated_submission_time': 2426.344469308853, 'accumulated_eval_time': 1849.1517560482025, 'accumulated_logging_time': 0.053259849548339844, 'global_step': 3455, 'preemption_count': 0}), (5176, {'train/loss': 0.12214810593323137, 'validation/loss': 0.12417491534667392, 'validation/num_examples': 83274637, 'test/loss': 0.1265168, 'test/num_examples': 95000000, 'score': 3626.570088624954, 'total_duration': 6020.571435689926, 'accumulated_submission_time': 3626.570088624954, 'accumulated_eval_time': 2393.850077867508, 'accumulated_logging_time': 0.08091497421264648, 'global_step': 5176, 'preemption_count': 0}), (6892, {'train/loss': 0.12084553076786066, 'validation/loss': 0.1238156823187353, 'validation/num_examples': 83274637, 'test/loss': 0.1261455157894737, 'test/num_examples': 95000000, 'score': 4826.7261509895325, 'total_duration': 7732.905031442642, 'accumulated_submission_time': 4826.7261509895325, 'accumulated_eval_time': 2905.9788336753845, 'accumulated_logging_time': 0.1067049503326416, 'global_step': 6892, 'preemption_count': 0}), (8000, {'train/loss': 0.12209711134808618, 'validation/loss': 0.12372899325877577, 'validation/num_examples': 83274637, 'test/loss': 0.12603075789473683, 'test/num_examples': 95000000, 'score': 5607.964383840561, 'total_duration': 8950.030904531479, 'accumulated_submission_time': 5607.964383840561, 'accumulated_eval_time': 3341.8244540691376, 'accumulated_logging_time': 0.1338210105895996, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1004 22:42:32.759762 140565235881792 submission_runner.py:552] Timing: 5607.964383840561
I1004 22:42:32.759822 140565235881792 submission_runner.py:554] Total number of evals: 6
I1004 22:42:32.759879 140565235881792 submission_runner.py:555] ====================
I1004 22:42:32.759991 140565235881792 submission_runner.py:625] Final criteo1tb score: 5607.964383840561
