python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-03-2023-20-02-40.log
2023-10-03 20:02:45.138179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1003 20:03:02.559031 140513024403264 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_0/criteo1tb_jax.
I1003 20:03:04.162532 140513024403264 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1003 20:03:04.163603 140513024403264 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1003 20:03:04.163756 140513024403264 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1003 20:03:04.168902 140513024403264 submission_runner.py:507] Using RNG seed 312348788
I1003 20:03:09.849899 140513024403264 submission_runner.py:516] --- Tuning run 1/1 ---
I1003 20:03:09.850100 140513024403264 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_0/criteo1tb_jax/trial_1.
I1003 20:03:09.850399 140513024403264 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_0/criteo1tb_jax/trial_1/hparams.json.
I1003 20:03:10.031441 140513024403264 submission_runner.py:191] Initializing dataset.
I1003 20:03:10.031669 140513024403264 submission_runner.py:198] Initializing model.
I1003 20:03:15.502229 140513024403264 submission_runner.py:232] Initializing optimizer.
I1003 20:03:18.511540 140513024403264 submission_runner.py:239] Initializing metrics bundle.
I1003 20:03:18.511754 140513024403264 submission_runner.py:257] Initializing checkpoint and logger.
I1003 20:03:18.512863 140513024403264 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_0/criteo1tb_jax/trial_1 with prefix checkpoint_
I1003 20:03:18.513015 140513024403264 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_0/criteo1tb_jax/trial_1/meta_data_0.json.
I1003 20:03:18.513226 140513024403264 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1003 20:03:18.513286 140513024403264 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1003 20:03:19.222606 140513024403264 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_0/criteo1tb_jax/trial_1/flags_0.json.
I1003 20:03:19.314434 140513024403264 submission_runner.py:290] Starting training loop.
I1003 20:03:46.045992 140349138781952 logging_writer.py:48] [0] global_step=0, grad_norm=8.274335861206055, loss=0.7609832882881165
I1003 20:03:46.056027 140513024403264 spec.py:321] Evaluating on the training split.
I1003 20:07:32.798029 140513024403264 spec.py:333] Evaluating on the validation split.
I1003 20:11:23.851976 140513024403264 spec.py:349] Evaluating on the test split.
I1003 20:15:45.301671 140513024403264 submission_runner.py:381] Time since start: 745.99s, 	Step: 1, 	{'train/loss': 0.762693777024371, 'validation/loss': 0.7624101921933325, 'validation/num_examples': 83274637, 'test/loss': 0.7618393263157894, 'test/num_examples': 95000000, 'score': 26.741571187973022, 'total_duration': 745.987146615982, 'accumulated_submission_time': 26.741571187973022, 'accumulated_eval_time': 719.2455406188965, 'accumulated_logging_time': 0}
I1003 20:15:45.322296 140329147668224 logging_writer.py:48] [1] accumulated_eval_time=719.245541, accumulated_logging_time=0, accumulated_submission_time=26.741571, global_step=1, preemption_count=0, score=26.741571, test/loss=0.761839, test/num_examples=95000000, total_duration=745.987147, train/loss=0.762694, validation/loss=0.762410, validation/num_examples=83274637
I1003 20:15:45.435648 140329063806720 logging_writer.py:48] [1] global_step=1, grad_norm=8.26555061340332, loss=0.7609802484512329
I1003 20:15:45.543038 140329147668224 logging_writer.py:48] [2] global_step=2, grad_norm=7.37358283996582, loss=0.6693174839019775
I1003 20:15:45.647784 140329063806720 logging_writer.py:48] [3] global_step=3, grad_norm=6.0431227684021, loss=0.5332698822021484
I1003 20:15:45.751249 140329147668224 logging_writer.py:48] [4] global_step=4, grad_norm=4.234263896942139, loss=0.390501469373703
I1003 20:15:45.853990 140329063806720 logging_writer.py:48] [5] global_step=5, grad_norm=2.6975226402282715, loss=0.2810521423816681
I1003 20:15:45.958857 140329147668224 logging_writer.py:48] [6] global_step=6, grad_norm=1.3744802474975586, loss=0.21049381792545319
I1003 20:15:46.063405 140329063806720 logging_writer.py:48] [7] global_step=7, grad_norm=0.34342294931411743, loss=0.1795281618833542
I1003 20:15:46.167845 140329147668224 logging_writer.py:48] [8] global_step=8, grad_norm=0.4684726893901825, loss=0.18340946733951569
I1003 20:15:46.271555 140329063806720 logging_writer.py:48] [9] global_step=9, grad_norm=0.8665149807929993, loss=0.19564419984817505
I1003 20:15:46.373801 140329147668224 logging_writer.py:48] [10] global_step=10, grad_norm=1.1516060829162598, loss=0.21538084745407104
I1003 20:15:46.477020 140329063806720 logging_writer.py:48] [11] global_step=11, grad_norm=1.3397966623306274, loss=0.23377946019172668
I1003 20:15:46.581612 140329147668224 logging_writer.py:48] [12] global_step=12, grad_norm=1.3896723985671997, loss=0.23840777575969696
I1003 20:15:46.686640 140329063806720 logging_writer.py:48] [13] global_step=13, grad_norm=1.3838250637054443, loss=0.23837651312351227
I1003 20:15:46.790246 140329147668224 logging_writer.py:48] [14] global_step=14, grad_norm=1.2880929708480835, loss=0.2265414595603943
I1003 20:15:46.892769 140329063806720 logging_writer.py:48] [15] global_step=15, grad_norm=1.1208574771881104, loss=0.20845983922481537
I1003 20:15:46.995970 140329147668224 logging_writer.py:48] [16] global_step=16, grad_norm=0.9375412464141846, loss=0.19298633933067322
I1003 20:15:47.098988 140329063806720 logging_writer.py:48] [17] global_step=17, grad_norm=0.7362641096115112, loss=0.18100382387638092
I1003 20:15:47.202733 140329147668224 logging_writer.py:48] [18] global_step=18, grad_norm=0.44853848218917847, loss=0.1694687455892563
I1003 20:15:47.308925 140329063806720 logging_writer.py:48] [19] global_step=19, grad_norm=0.13393829762935638, loss=0.16051295399665833
I1003 20:15:47.413716 140329147668224 logging_writer.py:48] [20] global_step=20, grad_norm=0.2085835188627243, loss=0.15742060542106628
I1003 20:15:47.518228 140329063806720 logging_writer.py:48] [21] global_step=21, grad_norm=0.28925254940986633, loss=0.1555575430393219
I1003 20:15:47.621679 140329147668224 logging_writer.py:48] [22] global_step=22, grad_norm=0.2149684578180313, loss=0.15267565846443176
I1003 20:15:47.726341 140329063806720 logging_writer.py:48] [23] global_step=23, grad_norm=0.08226073533296585, loss=0.15002837777137756
I1003 20:15:47.830419 140329147668224 logging_writer.py:48] [24] global_step=24, grad_norm=0.07532493770122528, loss=0.14724522829055786
I1003 20:15:47.935168 140329063806720 logging_writer.py:48] [25] global_step=25, grad_norm=0.14340487122535706, loss=0.14816468954086304
I1003 20:15:48.040573 140329147668224 logging_writer.py:48] [26] global_step=26, grad_norm=0.10669630020856857, loss=0.1471480429172516
I1003 20:15:48.143874 140329063806720 logging_writer.py:48] [27] global_step=27, grad_norm=0.05536722391843796, loss=0.1462147831916809
I1003 20:15:48.246831 140329147668224 logging_writer.py:48] [28] global_step=28, grad_norm=0.03991403058171272, loss=0.14799782633781433
I1003 20:15:48.872200 140329063806720 logging_writer.py:48] [29] global_step=29, grad_norm=0.03683997690677643, loss=0.1468949317932129
I1003 20:15:49.555081 140329147668224 logging_writer.py:48] [30] global_step=30, grad_norm=0.037415046244859695, loss=0.14903509616851807
I1003 20:15:50.308699 140329063806720 logging_writer.py:48] [31] global_step=31, grad_norm=0.03843897953629494, loss=0.14741790294647217
I1003 20:15:51.147356 140329147668224 logging_writer.py:48] [32] global_step=32, grad_norm=0.03376984968781471, loss=0.14705219864845276
I1003 20:15:51.829711 140329063806720 logging_writer.py:48] [33] global_step=33, grad_norm=0.031141597777605057, loss=0.14700287580490112
I1003 20:15:52.550415 140329147668224 logging_writer.py:48] [34] global_step=34, grad_norm=0.026080455631017685, loss=0.14256547391414642
I1003 20:15:53.340137 140329063806720 logging_writer.py:48] [35] global_step=35, grad_norm=0.031475894153118134, loss=0.1421668827533722
I1003 20:15:54.034369 140329147668224 logging_writer.py:48] [36] global_step=36, grad_norm=0.048436373472213745, loss=0.14118920266628265
I1003 20:15:54.861556 140329063806720 logging_writer.py:48] [37] global_step=37, grad_norm=0.06453472375869751, loss=0.1386760026216507
I1003 20:15:55.566852 140329147668224 logging_writer.py:48] [38] global_step=38, grad_norm=0.07242614775896072, loss=0.14123812317848206
I1003 20:15:56.370580 140329063806720 logging_writer.py:48] [39] global_step=39, grad_norm=0.0949491411447525, loss=0.14327608048915863
I1003 20:15:57.096912 140329147668224 logging_writer.py:48] [40] global_step=40, grad_norm=0.16773667931556702, loss=0.14048631489276886
I1003 20:15:57.834921 140329063806720 logging_writer.py:48] [41] global_step=41, grad_norm=0.16092605888843536, loss=0.14109724760055542
I1003 20:15:58.556982 140329147668224 logging_writer.py:48] [42] global_step=42, grad_norm=0.10084910690784454, loss=0.14126025140285492
I1003 20:15:59.491238 140329063806720 logging_writer.py:48] [43] global_step=43, grad_norm=0.059717293828725815, loss=0.14065024256706238
I1003 20:16:00.179541 140329147668224 logging_writer.py:48] [44] global_step=44, grad_norm=0.05440050736069679, loss=0.13947026431560516
I1003 20:16:00.839514 140329063806720 logging_writer.py:48] [45] global_step=45, grad_norm=0.05229898914694786, loss=0.13872894644737244
I1003 20:16:01.543637 140329147668224 logging_writer.py:48] [46] global_step=46, grad_norm=0.058563482016325, loss=0.13680782914161682
I1003 20:16:02.607806 140329063806720 logging_writer.py:48] [47] global_step=47, grad_norm=0.09105291962623596, loss=0.13804003596305847
I1003 20:16:03.314156 140329147668224 logging_writer.py:48] [48] global_step=48, grad_norm=0.13517723977565765, loss=0.13678207993507385
I1003 20:16:04.157252 140329063806720 logging_writer.py:48] [49] global_step=49, grad_norm=0.19697465002536774, loss=0.13794873654842377
I1003 20:16:04.737423 140329147668224 logging_writer.py:48] [50] global_step=50, grad_norm=0.23711082339286804, loss=0.1400948315858841
I1003 20:16:05.551441 140329063806720 logging_writer.py:48] [51] global_step=51, grad_norm=0.218534916639328, loss=0.13695979118347168
I1003 20:16:06.254087 140329147668224 logging_writer.py:48] [52] global_step=52, grad_norm=0.16562515497207642, loss=0.13675816357135773
I1003 20:16:07.206489 140329063806720 logging_writer.py:48] [53] global_step=53, grad_norm=0.1239173635840416, loss=0.13562637567520142
I1003 20:16:07.864160 140329147668224 logging_writer.py:48] [54] global_step=54, grad_norm=0.0981258973479271, loss=0.13503262400627136
I1003 20:16:08.525810 140329063806720 logging_writer.py:48] [55] global_step=55, grad_norm=0.08929109573364258, loss=0.13728006184101105
I1003 20:16:09.256769 140329147668224 logging_writer.py:48] [56] global_step=56, grad_norm=0.10217411071062088, loss=0.13506773114204407
I1003 20:16:10.028443 140329063806720 logging_writer.py:48] [57] global_step=57, grad_norm=0.08461814373731613, loss=0.1302167922258377
I1003 20:16:10.720117 140329147668224 logging_writer.py:48] [58] global_step=58, grad_norm=0.057296235114336014, loss=0.13087521493434906
I1003 20:16:11.528743 140329063806720 logging_writer.py:48] [59] global_step=59, grad_norm=0.04126991704106331, loss=0.1313043236732483
I1003 20:16:12.427607 140329147668224 logging_writer.py:48] [60] global_step=60, grad_norm=0.05115598440170288, loss=0.1282506287097931
I1003 20:16:13.006835 140329063806720 logging_writer.py:48] [61] global_step=61, grad_norm=0.05113338306546211, loss=0.12840400636196136
I1003 20:16:13.733039 140329147668224 logging_writer.py:48] [62] global_step=62, grad_norm=0.02313726581633091, loss=0.13165283203125
I1003 20:16:14.568336 140329063806720 logging_writer.py:48] [63] global_step=63, grad_norm=0.008318204432725906, loss=0.1305966079235077
I1003 20:16:15.271270 140329147668224 logging_writer.py:48] [64] global_step=64, grad_norm=0.011751381680369377, loss=0.12961332499980927
I1003 20:16:15.976161 140329063806720 logging_writer.py:48] [65] global_step=65, grad_norm=0.02515002153813839, loss=0.13147372007369995
I1003 20:16:16.861319 140329147668224 logging_writer.py:48] [66] global_step=66, grad_norm=0.059342823922634125, loss=0.12820057570934296
I1003 20:16:17.519585 140329063806720 logging_writer.py:48] [67] global_step=67, grad_norm=0.09291619062423706, loss=0.12971451878547668
I1003 20:16:18.273139 140329147668224 logging_writer.py:48] [68] global_step=68, grad_norm=0.12369824200868607, loss=0.12886634469032288
I1003 20:16:18.967885 140329063806720 logging_writer.py:48] [69] global_step=69, grad_norm=0.1732325702905655, loss=0.13278503715991974
I1003 20:16:19.698614 140329147668224 logging_writer.py:48] [70] global_step=70, grad_norm=0.19848079979419708, loss=0.13384756445884705
I1003 20:16:20.445653 140329063806720 logging_writer.py:48] [71] global_step=71, grad_norm=0.18627260625362396, loss=0.13166962563991547
I1003 20:16:21.070050 140329147668224 logging_writer.py:48] [72] global_step=72, grad_norm=0.1383127123117447, loss=0.13347600400447845
I1003 20:16:21.708655 140329063806720 logging_writer.py:48] [73] global_step=73, grad_norm=0.10619477182626724, loss=0.1318248212337494
I1003 20:16:22.478087 140329147668224 logging_writer.py:48] [74] global_step=74, grad_norm=0.11243883520364761, loss=0.1285231113433838
I1003 20:16:23.129129 140329063806720 logging_writer.py:48] [75] global_step=75, grad_norm=0.10635416209697723, loss=0.12972889840602875
I1003 20:16:23.835833 140329147668224 logging_writer.py:48] [76] global_step=76, grad_norm=0.08323264867067337, loss=0.12878066301345825
I1003 20:16:24.451684 140329063806720 logging_writer.py:48] [77] global_step=77, grad_norm=0.05105467885732651, loss=0.12469770014286041
I1003 20:16:25.079013 140329147668224 logging_writer.py:48] [78] global_step=78, grad_norm=0.026617584750056267, loss=0.12465225160121918
I1003 20:16:25.795321 140329063806720 logging_writer.py:48] [79] global_step=79, grad_norm=0.03209928050637245, loss=0.12643937766551971
I1003 20:16:26.540113 140329147668224 logging_writer.py:48] [80] global_step=80, grad_norm=0.041225966066122055, loss=0.124249666929245
I1003 20:16:27.252535 140329063806720 logging_writer.py:48] [81] global_step=81, grad_norm=0.05013862997293472, loss=0.12633347511291504
I1003 20:16:27.964912 140329147668224 logging_writer.py:48] [82] global_step=82, grad_norm=0.04385508596897125, loss=0.12713702023029327
I1003 20:16:28.684806 140329063806720 logging_writer.py:48] [83] global_step=83, grad_norm=0.02753395587205887, loss=0.12479981780052185
I1003 20:16:29.394737 140329147668224 logging_writer.py:48] [84] global_step=84, grad_norm=0.024399571120738983, loss=0.12524059414863586
I1003 20:16:30.090950 140329063806720 logging_writer.py:48] [85] global_step=85, grad_norm=0.019696438685059547, loss=0.12565256655216217
I1003 20:16:30.779500 140329147668224 logging_writer.py:48] [86] global_step=86, grad_norm=0.025197530165314674, loss=0.12497243285179138
I1003 20:16:31.522102 140329063806720 logging_writer.py:48] [87] global_step=87, grad_norm=0.03854256495833397, loss=0.12705408036708832
I1003 20:16:32.255434 140329147668224 logging_writer.py:48] [88] global_step=88, grad_norm=0.04819454252719879, loss=0.12760119140148163
I1003 20:16:32.818750 140329063806720 logging_writer.py:48] [89] global_step=89, grad_norm=0.06496082991361618, loss=0.12753131985664368
I1003 20:16:33.530599 140329147668224 logging_writer.py:48] [90] global_step=90, grad_norm=0.09169117361307144, loss=0.12590019404888153
I1003 20:16:34.178945 140329063806720 logging_writer.py:48] [91] global_step=91, grad_norm=0.1091088280081749, loss=0.12608297169208527
I1003 20:16:35.006415 140329147668224 logging_writer.py:48] [92] global_step=92, grad_norm=0.12877361476421356, loss=0.12543228268623352
I1003 20:16:35.614295 140329063806720 logging_writer.py:48] [93] global_step=93, grad_norm=0.15052559971809387, loss=0.1266583800315857
I1003 20:16:36.202563 140329147668224 logging_writer.py:48] [94] global_step=94, grad_norm=0.14682219922542572, loss=0.12782952189445496
I1003 20:16:36.859811 140329063806720 logging_writer.py:48] [95] global_step=95, grad_norm=0.164414644241333, loss=0.13798905909061432
I1003 20:16:37.513589 140329147668224 logging_writer.py:48] [96] global_step=96, grad_norm=0.195962056517601, loss=0.1432989239692688
I1003 20:16:38.209865 140329063806720 logging_writer.py:48] [97] global_step=97, grad_norm=0.1899145543575287, loss=0.1400463730096817
I1003 20:16:38.891446 140329147668224 logging_writer.py:48] [98] global_step=98, grad_norm=0.17609819769859314, loss=0.14017491042613983
I1003 20:16:39.514035 140329063806720 logging_writer.py:48] [99] global_step=99, grad_norm=0.15716782212257385, loss=0.1405133455991745
I1003 20:16:40.306693 140329147668224 logging_writer.py:48] [100] global_step=100, grad_norm=0.12582069635391235, loss=0.14091157913208008
I1003 20:21:21.867475 140329063806720 logging_writer.py:48] [500] global_step=500, grad_norm=0.01692764274775982, loss=0.12467437982559204
I1003 20:27:09.499771 140329147668224 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.019594021141529083, loss=0.12022969126701355
I1003 20:33:03.142314 140329063806720 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.03086279332637787, loss=0.13168716430664062
I1003 20:35:46.066200 140513024403264 spec.py:321] Evaluating on the training split.
I1003 20:38:48.481534 140513024403264 spec.py:333] Evaluating on the validation split.
I1003 20:42:01.097138 140513024403264 spec.py:349] Evaluating on the test split.
I1003 20:45:31.153886 140513024403264 submission_runner.py:381] Time since start: 2531.84s, 	Step: 1733, 	{'train/loss': 0.1243338314992077, 'validation/loss': 0.12549568964197347, 'validation/num_examples': 83274637, 'test/loss': 0.12780922105263157, 'test/num_examples': 95000000, 'score': 1227.4552011489868, 'total_duration': 2531.8393790721893, 'accumulated_submission_time': 1227.4552011489868, 'accumulated_eval_time': 1304.3331997394562, 'accumulated_logging_time': 0.028359651565551758}
I1003 20:45:31.169445 140329147668224 logging_writer.py:48] [1733] accumulated_eval_time=1304.333200, accumulated_logging_time=0.028360, accumulated_submission_time=1227.455201, global_step=1733, preemption_count=0, score=1227.455201, test/loss=0.127809, test/num_examples=95000000, total_duration=2531.839379, train/loss=0.124334, validation/loss=0.125496, validation/num_examples=83274637
I1003 20:48:24.097108 140329063806720 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0051004886627197266, loss=0.11979535222053528
I1003 20:54:17.139082 140329147668224 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.015151577070355415, loss=0.11854148656129837
I1003 21:00:09.599356 140329063806720 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.04429202899336815, loss=0.1275894045829773
I1003 21:05:31.293787 140513024403264 spec.py:321] Evaluating on the training split.
I1003 21:08:32.857243 140513024403264 spec.py:333] Evaluating on the validation split.
I1003 21:11:11.926193 140513024403264 spec.py:349] Evaluating on the test split.
I1003 21:14:08.142635 140513024403264 submission_runner.py:381] Time since start: 4248.83s, 	Step: 3450, 	{'train/loss': 0.12295438658516362, 'validation/loss': 0.12458174990303471, 'validation/num_examples': 83274637, 'test/loss': 0.12701844210526317, 'test/num_examples': 95000000, 'score': 2427.5503962039948, 'total_duration': 4248.828116416931, 'accumulated_submission_time': 2427.5503962039948, 'accumulated_eval_time': 1821.181999206543, 'accumulated_logging_time': 0.05131840705871582}
I1003 21:14:08.160008 140329147668224 logging_writer.py:48] [3450] accumulated_eval_time=1821.181999, accumulated_logging_time=0.051318, accumulated_submission_time=2427.550396, global_step=3450, preemption_count=0, score=2427.550396, test/loss=0.127018, test/num_examples=95000000, total_duration=4248.828116, train/loss=0.122954, validation/loss=0.124582, validation/num_examples=83274637
I1003 21:14:27.866456 140329063806720 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.003712422214448452, loss=0.12201814353466034
I1003 21:20:16.886142 140329147668224 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.015087444335222244, loss=0.12616515159606934
I1003 21:26:05.480835 140329063806720 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.011235333047807217, loss=0.11778149753808975
I1003 21:31:58.268263 140329147668224 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.004037222359329462, loss=0.120102658867836
I1003 21:34:08.169026 140513024403264 spec.py:321] Evaluating on the training split.
I1003 21:37:05.046009 140513024403264 spec.py:333] Evaluating on the validation split.
I1003 21:39:44.536936 140513024403264 spec.py:349] Evaluating on the test split.
I1003 21:43:13.434834 140513024403264 submission_runner.py:381] Time since start: 5994.12s, 	Step: 5184, 	{'train/loss': 0.12401935889286066, 'validation/loss': 0.12407450061895797, 'validation/num_examples': 83274637, 'test/loss': 0.12649892631578946, 'test/num_examples': 95000000, 'score': 3627.5294320583344, 'total_duration': 5994.1203355789185, 'accumulated_submission_time': 3627.5294320583344, 'accumulated_eval_time': 2366.4477667808533, 'accumulated_logging_time': 0.07649087905883789}
I1003 21:43:13.452071 140329063806720 logging_writer.py:48] [5184] accumulated_eval_time=2366.447767, accumulated_logging_time=0.076491, accumulated_submission_time=3627.529432, global_step=5184, preemption_count=0, score=3627.529432, test/loss=0.126499, test/num_examples=95000000, total_duration=5994.120336, train/loss=0.124019, validation/loss=0.124075, validation/num_examples=83274637
I1003 21:46:43.447774 140329147668224 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.004869246855378151, loss=0.11879150569438934
I1003 21:52:34.081761 140329063806720 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.00491763511672616, loss=0.1305743008852005
I1003 21:58:26.533819 140329147668224 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.009142674505710602, loss=0.12354647368192673
I1003 22:03:14.170618 140513024403264 spec.py:321] Evaluating on the training split.
I1003 22:05:57.141685 140513024403264 spec.py:333] Evaluating on the validation split.
I1003 22:08:23.089125 140513024403264 spec.py:349] Evaluating on the test split.
I1003 22:11:43.782553 140513024403264 submission_runner.py:381] Time since start: 7704.47s, 	Step: 6906, 	{'train/loss': 0.12161993830458923, 'validation/loss': 0.12376394988068216, 'validation/num_examples': 83274637, 'test/loss': 0.12607676842105264, 'test/num_examples': 95000000, 'score': 4828.2184681892395, 'total_duration': 7704.468060970306, 'accumulated_submission_time': 4828.2184681892395, 'accumulated_eval_time': 2876.0596690177917, 'accumulated_logging_time': 0.10171318054199219}
I1003 22:11:43.798507 140329063806720 logging_writer.py:48] [6906] accumulated_eval_time=2876.059669, accumulated_logging_time=0.101713, accumulated_submission_time=4828.218468, global_step=6906, preemption_count=0, score=4828.218468, test/loss=0.126077, test/num_examples=95000000, total_duration=7704.468061, train/loss=0.121620, validation/loss=0.123764, validation/num_examples=83274637
I1003 22:12:35.909461 140329147668224 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0039856997318565845, loss=0.12242519855499268
I1003 22:18:26.747810 140329063806720 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.004974426236003637, loss=0.11482216417789459
I1003 22:24:16.527381 140513024403264 spec.py:321] Evaluating on the training split.
I1003 22:26:35.506891 140513024403264 spec.py:333] Evaluating on the validation split.
I1003 22:28:30.208474 140513024403264 spec.py:349] Evaluating on the test split.
I1003 22:30:49.064103 140513024403264 submission_runner.py:381] Time since start: 8849.75s, 	Step: 8000, 	{'train/loss': 0.1213508222088124, 'validation/loss': 0.12365463688541807, 'validation/num_examples': 83274637, 'test/loss': 0.1259714, 'test/num_examples': 95000000, 'score': 5580.92550611496, 'total_duration': 8849.749608516693, 'accumulated_submission_time': 5580.92550611496, 'accumulated_eval_time': 3268.5963644981384, 'accumulated_logging_time': 0.12497353553771973}
I1003 22:30:49.077591 140329147668224 logging_writer.py:48] [8000] accumulated_eval_time=3268.596364, accumulated_logging_time=0.124974, accumulated_submission_time=5580.925506, global_step=8000, preemption_count=0, score=5580.925506, test/loss=0.125971, test/num_examples=95000000, total_duration=8849.749609, train/loss=0.121351, validation/loss=0.123655, validation/num_examples=83274637
I1003 22:30:49.093039 140329063806720 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5580.925506
I1003 22:30:54.384481 140513024403264 checkpoints.py:490] Saving checkpoint at step: 8000
I1003 22:31:35.546416 140513024403264 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_0/criteo1tb_jax/trial_1/checkpoint_8000
I1003 22:31:35.859923 140513024403264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_0/criteo1tb_jax/trial_1/checkpoint_8000.
I1003 22:31:36.194669 140513024403264 submission_runner.py:549] Tuning trial 1/1
I1003 22:31:36.194958 140513024403264 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1003 22:31:36.196075 140513024403264 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.762693777024371, 'validation/loss': 0.7624101921933325, 'validation/num_examples': 83274637, 'test/loss': 0.7618393263157894, 'test/num_examples': 95000000, 'score': 26.741571187973022, 'total_duration': 745.987146615982, 'accumulated_submission_time': 26.741571187973022, 'accumulated_eval_time': 719.2455406188965, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1733, {'train/loss': 0.1243338314992077, 'validation/loss': 0.12549568964197347, 'validation/num_examples': 83274637, 'test/loss': 0.12780922105263157, 'test/num_examples': 95000000, 'score': 1227.4552011489868, 'total_duration': 2531.8393790721893, 'accumulated_submission_time': 1227.4552011489868, 'accumulated_eval_time': 1304.3331997394562, 'accumulated_logging_time': 0.028359651565551758, 'global_step': 1733, 'preemption_count': 0}), (3450, {'train/loss': 0.12295438658516362, 'validation/loss': 0.12458174990303471, 'validation/num_examples': 83274637, 'test/loss': 0.12701844210526317, 'test/num_examples': 95000000, 'score': 2427.5503962039948, 'total_duration': 4248.828116416931, 'accumulated_submission_time': 2427.5503962039948, 'accumulated_eval_time': 1821.181999206543, 'accumulated_logging_time': 0.05131840705871582, 'global_step': 3450, 'preemption_count': 0}), (5184, {'train/loss': 0.12401935889286066, 'validation/loss': 0.12407450061895797, 'validation/num_examples': 83274637, 'test/loss': 0.12649892631578946, 'test/num_examples': 95000000, 'score': 3627.5294320583344, 'total_duration': 5994.1203355789185, 'accumulated_submission_time': 3627.5294320583344, 'accumulated_eval_time': 2366.4477667808533, 'accumulated_logging_time': 0.07649087905883789, 'global_step': 5184, 'preemption_count': 0}), (6906, {'train/loss': 0.12161993830458923, 'validation/loss': 0.12376394988068216, 'validation/num_examples': 83274637, 'test/loss': 0.12607676842105264, 'test/num_examples': 95000000, 'score': 4828.2184681892395, 'total_duration': 7704.468060970306, 'accumulated_submission_time': 4828.2184681892395, 'accumulated_eval_time': 2876.0596690177917, 'accumulated_logging_time': 0.10171318054199219, 'global_step': 6906, 'preemption_count': 0}), (8000, {'train/loss': 0.1213508222088124, 'validation/loss': 0.12365463688541807, 'validation/num_examples': 83274637, 'test/loss': 0.1259714, 'test/num_examples': 95000000, 'score': 5580.92550611496, 'total_duration': 8849.749608516693, 'accumulated_submission_time': 5580.92550611496, 'accumulated_eval_time': 3268.5963644981384, 'accumulated_logging_time': 0.12497353553771973, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1003 22:31:36.196206 140513024403264 submission_runner.py:552] Timing: 5580.92550611496
I1003 22:31:36.196265 140513024403264 submission_runner.py:554] Total number of evals: 6
I1003 22:31:36.196314 140513024403264 submission_runner.py:555] ====================
I1003 22:31:36.196437 140513024403264 submission_runner.py:625] Final criteo1tb score: 5580.92550611496
