python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_5 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-04-2023-09-30-06.log
2023-10-04 09:30:10.937409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1004 09:30:27.812588 140655693051712 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_5/criteo1tb_jax.
I1004 09:30:29.446644 140655693051712 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I1004 09:30:29.447519 140655693051712 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1004 09:30:29.447679 140655693051712 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1004 09:30:29.452884 140655693051712 submission_runner.py:507] Using RNG seed 938526487
I1004 09:30:35.117740 140655693051712 submission_runner.py:516] --- Tuning run 1/1 ---
I1004 09:30:35.117951 140655693051712 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_5/criteo1tb_jax/trial_1.
I1004 09:30:35.118272 140655693051712 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_5/criteo1tb_jax/trial_1/hparams.json.
I1004 09:30:35.306599 140655693051712 submission_runner.py:191] Initializing dataset.
I1004 09:30:35.306819 140655693051712 submission_runner.py:198] Initializing model.
I1004 09:30:40.984342 140655693051712 submission_runner.py:232] Initializing optimizer.
I1004 09:30:44.145823 140655693051712 submission_runner.py:239] Initializing metrics bundle.
I1004 09:30:44.146028 140655693051712 submission_runner.py:257] Initializing checkpoint and logger.
I1004 09:30:44.147046 140655693051712 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_5/criteo1tb_jax/trial_1 with prefix checkpoint_
I1004 09:30:44.147190 140655693051712 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_5/criteo1tb_jax/trial_1/meta_data_0.json.
I1004 09:30:44.147405 140655693051712 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1004 09:30:44.147470 140655693051712 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1004 09:30:44.972182 140655693051712 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_5/criteo1tb_jax/trial_1/flags_0.json.
I1004 09:30:45.063348 140655693051712 submission_runner.py:290] Starting training loop.
I1004 09:31:10.581813 140491610900224 logging_writer.py:48] [0] global_step=0, grad_norm=13.191818237304688, loss=1.2735271453857422
I1004 09:31:10.593529 140655693051712 spec.py:321] Evaluating on the training split.
I1004 09:34:56.020409 140655693051712 spec.py:333] Evaluating on the validation split.
I1004 09:38:48.955849 140655693051712 spec.py:349] Evaluating on the test split.
I1004 09:43:06.565341 140655693051712 submission_runner.py:381] Time since start: 741.50s, 	Step: 1, 	{'train/loss': 1.2732552582362913, 'validation/loss': 1.2764677917479244, 'validation/num_examples': 83274637, 'test/loss': 1.2748633263157896, 'test/num_examples': 95000000, 'score': 25.530160665512085, 'total_duration': 741.5019369125366, 'accumulated_submission_time': 25.530160665512085, 'accumulated_eval_time': 715.9717352390289, 'accumulated_logging_time': 0}
I1004 09:43:06.585687 140469430372096 logging_writer.py:48] [1] accumulated_eval_time=715.971735, accumulated_logging_time=0, accumulated_submission_time=25.530161, global_step=1, preemption_count=0, score=25.530161, test/loss=1.274863, test/num_examples=95000000, total_duration=741.501937, train/loss=1.273255, validation/loss=1.276468, validation/num_examples=83274637
I1004 09:43:06.699856 140469421979392 logging_writer.py:48] [1] global_step=1, grad_norm=13.16947078704834, loss=1.273310899734497
I1004 09:43:06.805905 140469430372096 logging_writer.py:48] [2] global_step=2, grad_norm=12.377657890319824, loss=1.1149688959121704
I1004 09:43:06.910875 140469421979392 logging_writer.py:48] [3] global_step=3, grad_norm=10.698382377624512, loss=0.8634944558143616
I1004 09:43:07.014553 140469430372096 logging_writer.py:48] [4] global_step=4, grad_norm=8.002134323120117, loss=0.5858942866325378
I1004 09:43:07.117509 140469421979392 logging_writer.py:48] [5] global_step=5, grad_norm=4.731669902801514, loss=0.35720548033714294
I1004 09:43:07.220032 140469430372096 logging_writer.py:48] [6] global_step=6, grad_norm=2.0124802589416504, loss=0.23140761256217957
I1004 09:43:07.323202 140469421979392 logging_writer.py:48] [7] global_step=7, grad_norm=0.32937300205230713, loss=0.1877427101135254
I1004 09:43:07.428082 140469430372096 logging_writer.py:48] [8] global_step=8, grad_norm=0.7500945329666138, loss=0.19222958385944366
I1004 09:43:07.533071 140469421979392 logging_writer.py:48] [9] global_step=9, grad_norm=1.361940860748291, loss=0.22828829288482666
I1004 09:43:07.637861 140469430372096 logging_writer.py:48] [10] global_step=10, grad_norm=1.6523878574371338, loss=0.2557079792022705
I1004 09:43:07.741304 140469421979392 logging_writer.py:48] [11] global_step=11, grad_norm=1.932159423828125, loss=0.29014235734939575
I1004 09:43:07.845028 140469430372096 logging_writer.py:48] [12] global_step=12, grad_norm=2.0847625732421875, loss=0.3126366138458252
I1004 09:43:07.948721 140469421979392 logging_writer.py:48] [13] global_step=13, grad_norm=2.1690783500671387, loss=0.326875776052475
I1004 09:43:08.052050 140469430372096 logging_writer.py:48] [14] global_step=14, grad_norm=2.144885540008545, loss=0.32302165031433105
I1004 09:43:08.155624 140469421979392 logging_writer.py:48] [15] global_step=15, grad_norm=2.0667433738708496, loss=0.30922964215278625
I1004 09:43:08.259016 140469430372096 logging_writer.py:48] [16] global_step=16, grad_norm=1.9677120447158813, loss=0.2908169627189636
I1004 09:43:08.362411 140469421979392 logging_writer.py:48] [17] global_step=17, grad_norm=1.8123232126235962, loss=0.26649773120880127
I1004 09:43:08.465631 140469430372096 logging_writer.py:48] [18] global_step=18, grad_norm=1.5057474374771118, loss=0.2284536212682724
I1004 09:43:08.570256 140469421979392 logging_writer.py:48] [19] global_step=19, grad_norm=1.191832184791565, loss=0.20413285493850708
I1004 09:43:08.676170 140469430372096 logging_writer.py:48] [20] global_step=20, grad_norm=0.6278690695762634, loss=0.17576582729816437
I1004 09:43:08.780308 140469421979392 logging_writer.py:48] [21] global_step=21, grad_norm=0.20846813917160034, loss=0.16830521821975708
I1004 09:43:08.885359 140469430372096 logging_writer.py:48] [22] global_step=22, grad_norm=0.7112834453582764, loss=0.17190471291542053
I1004 09:43:08.989383 140469421979392 logging_writer.py:48] [23] global_step=23, grad_norm=0.7362961173057556, loss=0.169106125831604
I1004 09:43:09.093305 140469430372096 logging_writer.py:48] [24] global_step=24, grad_norm=0.32308390736579895, loss=0.1623430848121643
I1004 09:43:09.196547 140469421979392 logging_writer.py:48] [25] global_step=25, grad_norm=0.10794730484485626, loss=0.15355081856250763
I1004 09:43:09.300880 140469430372096 logging_writer.py:48] [26] global_step=26, grad_norm=0.3322788178920746, loss=0.15351372957229614
I1004 09:43:09.405711 140469421979392 logging_writer.py:48] [27] global_step=27, grad_norm=0.2452455312013626, loss=0.1522621512413025
I1004 09:43:09.509490 140469430372096 logging_writer.py:48] [28] global_step=28, grad_norm=0.12242648005485535, loss=0.15192396938800812
I1004 09:43:10.242455 140469421979392 logging_writer.py:48] [29] global_step=29, grad_norm=0.05970774218440056, loss=0.14602838456630707
I1004 09:43:11.027866 140469430372096 logging_writer.py:48] [30] global_step=30, grad_norm=0.1346234530210495, loss=0.14813676476478577
I1004 09:43:11.711591 140469421979392 logging_writer.py:48] [31] global_step=31, grad_norm=0.053694549947977066, loss=0.14625121653079987
I1004 09:43:12.555624 140469430372096 logging_writer.py:48] [32] global_step=32, grad_norm=0.1032814010977745, loss=0.14556054770946503
I1004 09:43:13.261322 140469421979392 logging_writer.py:48] [33] global_step=33, grad_norm=0.10171175748109818, loss=0.14473623037338257
I1004 09:43:13.951703 140469430372096 logging_writer.py:48] [34] global_step=34, grad_norm=0.10587216913700104, loss=0.1394260823726654
I1004 09:43:14.740718 140469421979392 logging_writer.py:48] [35] global_step=35, grad_norm=0.1125120148062706, loss=0.14185692369937897
I1004 09:43:15.518096 140469430372096 logging_writer.py:48] [36] global_step=36, grad_norm=0.12006810307502747, loss=0.14016717672348022
I1004 09:43:16.330127 140469421979392 logging_writer.py:48] [37] global_step=37, grad_norm=0.2032579779624939, loss=0.14045584201812744
I1004 09:43:17.069813 140469430372096 logging_writer.py:48] [38] global_step=38, grad_norm=0.29694437980651855, loss=0.14482912421226501
I1004 09:43:17.831759 140469421979392 logging_writer.py:48] [39] global_step=39, grad_norm=0.47176364064216614, loss=0.145447239279747
I1004 09:43:18.621461 140469430372096 logging_writer.py:48] [40] global_step=40, grad_norm=0.39360710978507996, loss=0.1476241797208786
I1004 09:43:19.452442 140469421979392 logging_writer.py:48] [41] global_step=41, grad_norm=0.30644091963768005, loss=0.1434708684682846
I1004 09:43:20.048063 140469430372096 logging_writer.py:48] [42] global_step=42, grad_norm=0.193487748503685, loss=0.14271515607833862
I1004 09:43:21.039095 140469421979392 logging_writer.py:48] [43] global_step=43, grad_norm=0.1460413634777069, loss=0.14250560104846954
I1004 09:43:21.561965 140469430372096 logging_writer.py:48] [44] global_step=44, grad_norm=0.0903901681303978, loss=0.1398722380399704
I1004 09:43:22.321058 140469421979392 logging_writer.py:48] [45] global_step=45, grad_norm=0.069620281457901, loss=0.13983698189258575
I1004 09:43:23.183336 140469430372096 logging_writer.py:48] [46] global_step=46, grad_norm=0.09147423505783081, loss=0.13965800404548645
I1004 09:43:23.958420 140469421979392 logging_writer.py:48] [47] global_step=47, grad_norm=0.13604891300201416, loss=0.13879716396331787
I1004 09:43:24.539801 140469430372096 logging_writer.py:48] [48] global_step=48, grad_norm=0.20680025219917297, loss=0.1396794617176056
I1004 09:43:25.370469 140469421979392 logging_writer.py:48] [49] global_step=49, grad_norm=0.3677363693714142, loss=0.1369408220052719
I1004 09:43:25.986682 140469430372096 logging_writer.py:48] [50] global_step=50, grad_norm=0.5186651945114136, loss=0.14294403791427612
I1004 09:43:26.801723 140469421979392 logging_writer.py:48] [51] global_step=51, grad_norm=0.6666010022163391, loss=0.14343778789043427
I1004 09:43:27.540598 140469430372096 logging_writer.py:48] [52] global_step=52, grad_norm=0.6037202477455139, loss=0.14300087094306946
I1004 09:43:28.328807 140469421979392 logging_writer.py:48] [53] global_step=53, grad_norm=0.3620838224887848, loss=0.13795354962348938
I1004 09:43:29.053693 140469430372096 logging_writer.py:48] [54] global_step=54, grad_norm=0.19837753474712372, loss=0.13368366658687592
I1004 09:43:29.894940 140469421979392 logging_writer.py:48] [55] global_step=55, grad_norm=0.12186088413000107, loss=0.13499903678894043
I1004 09:43:30.561943 140469430372096 logging_writer.py:48] [56] global_step=56, grad_norm=0.06108924001455307, loss=0.13441580533981323
I1004 09:43:31.200354 140469421979392 logging_writer.py:48] [57] global_step=57, grad_norm=0.10225431621074677, loss=0.1334090530872345
I1004 09:43:32.028286 140469430372096 logging_writer.py:48] [58] global_step=58, grad_norm=0.0630270317196846, loss=0.12946102023124695
I1004 09:43:32.814782 140469421979392 logging_writer.py:48] [59] global_step=59, grad_norm=0.024367541074752808, loss=0.13127700984477997
I1004 09:43:33.473783 140469430372096 logging_writer.py:48] [60] global_step=60, grad_norm=0.026255004107952118, loss=0.13095533847808838
I1004 09:43:34.283235 140469421979392 logging_writer.py:48] [61] global_step=61, grad_norm=0.024888021871447563, loss=0.12944373488426208
I1004 09:43:35.146952 140469430372096 logging_writer.py:48] [62] global_step=62, grad_norm=0.030268007889389992, loss=0.12911728024482727
I1004 09:43:35.661324 140469421979392 logging_writer.py:48] [63] global_step=63, grad_norm=0.056469861418008804, loss=0.12860462069511414
I1004 09:43:36.448096 140469430372096 logging_writer.py:48] [64] global_step=64, grad_norm=0.06909584999084473, loss=0.1309364289045334
I1004 09:43:37.147759 140469421979392 logging_writer.py:48] [65] global_step=65, grad_norm=0.07307972759008408, loss=0.12913790345191956
I1004 09:43:37.832596 140469430372096 logging_writer.py:48] [66] global_step=66, grad_norm=0.08496861159801483, loss=0.12761561572551727
I1004 09:43:38.521772 140469421979392 logging_writer.py:48] [67] global_step=67, grad_norm=0.1158716157078743, loss=0.12958747148513794
I1004 09:43:39.133818 140469430372096 logging_writer.py:48] [68] global_step=68, grad_norm=0.1601409912109375, loss=0.12825791537761688
I1004 09:43:39.826037 140469421979392 logging_writer.py:48] [69] global_step=69, grad_norm=0.19999125599861145, loss=0.13081581890583038
I1004 09:43:40.598447 140469430372096 logging_writer.py:48] [70] global_step=70, grad_norm=0.2473132461309433, loss=0.12848787009716034
I1004 09:43:41.385711 140469421979392 logging_writer.py:48] [71] global_step=71, grad_norm=0.27641916275024414, loss=0.13158543407917023
I1004 09:43:42.114352 140469430372096 logging_writer.py:48] [72] global_step=72, grad_norm=0.2780810594558716, loss=0.13103967905044556
I1004 09:43:42.732947 140469421979392 logging_writer.py:48] [73] global_step=73, grad_norm=0.22902712225914001, loss=0.1286928355693817
I1004 09:43:43.420539 140469430372096 logging_writer.py:48] [74] global_step=74, grad_norm=0.1677308827638626, loss=0.12795193493366241
I1004 09:43:44.159646 140469421979392 logging_writer.py:48] [75] global_step=75, grad_norm=0.12151993066072464, loss=0.12609489262104034
I1004 09:43:44.824130 140469430372096 logging_writer.py:48] [76] global_step=76, grad_norm=0.05420101061463356, loss=0.13391661643981934
I1004 09:43:45.590264 140469421979392 logging_writer.py:48] [77] global_step=77, grad_norm=0.012255988083779812, loss=0.13468003273010254
I1004 09:43:46.296017 140469430372096 logging_writer.py:48] [78] global_step=78, grad_norm=0.007621878292411566, loss=0.13614021241664886
I1004 09:43:47.141307 140469421979392 logging_writer.py:48] [79] global_step=79, grad_norm=0.018595704808831215, loss=0.13503405451774597
I1004 09:43:47.795469 140469430372096 logging_writer.py:48] [80] global_step=80, grad_norm=0.012778563424944878, loss=0.13504377007484436
I1004 09:43:48.633074 140469421979392 logging_writer.py:48] [81] global_step=81, grad_norm=0.008736364543437958, loss=0.13638600707054138
I1004 09:43:49.267321 140469430372096 logging_writer.py:48] [82] global_step=82, grad_norm=0.01665688119828701, loss=0.13624697923660278
I1004 09:43:49.953377 140469421979392 logging_writer.py:48] [83] global_step=83, grad_norm=0.04360359162092209, loss=0.13333769142627716
I1004 09:43:50.691127 140469430372096 logging_writer.py:48] [84] global_step=84, grad_norm=0.10486042499542236, loss=0.13563284277915955
I1004 09:43:51.375110 140469421979392 logging_writer.py:48] [85] global_step=85, grad_norm=0.1822393536567688, loss=0.13832110166549683
I1004 09:43:52.281181 140469430372096 logging_writer.py:48] [86] global_step=86, grad_norm=0.24565134942531586, loss=0.13716378808021545
I1004 09:43:52.956367 140469421979392 logging_writer.py:48] [87] global_step=87, grad_norm=0.31857559084892273, loss=0.13833269476890564
I1004 09:43:53.654330 140469430372096 logging_writer.py:48] [88] global_step=88, grad_norm=0.36619916558265686, loss=0.13904665410518646
I1004 09:43:54.429138 140469421979392 logging_writer.py:48] [89] global_step=89, grad_norm=0.38818931579589844, loss=0.13667690753936768
I1004 09:43:55.057440 140469430372096 logging_writer.py:48] [90] global_step=90, grad_norm=0.3309502601623535, loss=0.13385772705078125
I1004 09:43:55.949931 140469421979392 logging_writer.py:48] [91] global_step=91, grad_norm=0.17310182750225067, loss=0.13555297255516052
I1004 09:43:56.707860 140469430372096 logging_writer.py:48] [92] global_step=92, grad_norm=0.08626130223274231, loss=0.13444696366786957
I1004 09:43:57.472386 140469421979392 logging_writer.py:48] [93] global_step=93, grad_norm=0.06642866134643555, loss=0.13509729504585266
I1004 09:43:58.168785 140469430372096 logging_writer.py:48] [94] global_step=94, grad_norm=0.055194079875946045, loss=0.1379992961883545
I1004 09:43:58.937565 140469421979392 logging_writer.py:48] [95] global_step=95, grad_norm=0.08073332160711288, loss=0.13644781708717346
I1004 09:43:59.680753 140469430372096 logging_writer.py:48] [96] global_step=96, grad_norm=0.06102210283279419, loss=0.1368289589881897
I1004 09:44:00.498419 140469421979392 logging_writer.py:48] [97] global_step=97, grad_norm=0.045153308659791946, loss=0.13571476936340332
I1004 09:44:01.231228 140469430372096 logging_writer.py:48] [98] global_step=98, grad_norm=0.051196567714214325, loss=0.1366049349308014
I1004 09:44:02.196882 140469421979392 logging_writer.py:48] [99] global_step=99, grad_norm=0.06456111371517181, loss=0.13499867916107178
I1004 09:44:02.899305 140469430372096 logging_writer.py:48] [100] global_step=100, grad_norm=0.07831519842147827, loss=0.13794222474098206
I1004 09:48:45.668832 140469421979392 logging_writer.py:48] [500] global_step=500, grad_norm=0.07028985023498535, loss=0.13351786136627197
I1004 09:54:38.002968 140469430372096 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.007237167563289404, loss=0.1267137974500656
I1004 10:00:31.133121 140469421979392 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.040749721229076385, loss=0.12772807478904724
I1004 10:03:06.822345 140655693051712 spec.py:321] Evaluating on the training split.
I1004 10:06:04.959508 140655693051712 spec.py:333] Evaluating on the validation split.
I1004 10:08:51.724424 140655693051712 spec.py:349] Evaluating on the test split.
I1004 10:11:50.005319 140655693051712 submission_runner.py:381] Time since start: 2464.94s, 	Step: 1717, 	{'train/loss': 0.12267932052132469, 'validation/loss': 0.1253280275481717, 'validation/num_examples': 83274637, 'test/loss': 0.12765314736842107, 'test/num_examples': 95000000, 'score': 1225.7348670959473, 'total_duration': 2464.9419045448303, 'accumulated_submission_time': 1225.7348670959473, 'accumulated_eval_time': 1239.1546547412872, 'accumulated_logging_time': 0.028185606002807617}
I1004 10:11:50.022881 140469430372096 logging_writer.py:48] [1717] accumulated_eval_time=1239.154655, accumulated_logging_time=0.028186, accumulated_submission_time=1225.734867, global_step=1717, preemption_count=0, score=1225.734867, test/loss=0.127653, test/num_examples=95000000, total_duration=2464.941905, train/loss=0.122679, validation/loss=0.125328, validation/num_examples=83274637
I1004 10:14:56.397584 140469421979392 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.014791595749557018, loss=0.12369928508996964
I1004 10:20:46.361037 140469430372096 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.008456229232251644, loss=0.11319921165704727
I1004 10:26:41.950984 140469421979392 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.013532239943742752, loss=0.1312643587589264
I1004 10:31:50.375553 140655693051712 spec.py:321] Evaluating on the training split.
I1004 10:34:40.335809 140655693051712 spec.py:333] Evaluating on the validation split.
I1004 10:37:41.543212 140655693051712 spec.py:349] Evaluating on the test split.
I1004 10:41:01.943414 140655693051712 submission_runner.py:381] Time since start: 4216.88s, 	Step: 3441, 	{'train/loss': 0.12474754621397774, 'validation/loss': 0.1248542458371809, 'validation/num_examples': 83274637, 'test/loss': 0.12708248421052631, 'test/num_examples': 95000000, 'score': 2426.0547959804535, 'total_duration': 4216.8799929618835, 'accumulated_submission_time': 2426.0547959804535, 'accumulated_eval_time': 1790.7224497795105, 'accumulated_logging_time': 0.05450773239135742}
I1004 10:41:01.961941 140469430372096 logging_writer.py:48] [3441] accumulated_eval_time=1790.722450, accumulated_logging_time=0.054508, accumulated_submission_time=2426.054796, global_step=3441, preemption_count=0, score=2426.054796, test/loss=0.127082, test/num_examples=95000000, total_duration=4216.879993, train/loss=0.124748, validation/loss=0.124854, validation/num_examples=83274637
I1004 10:41:28.345542 140469421979392 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.004969444125890732, loss=0.11706560850143433
I1004 10:47:23.061642 140469430372096 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.03951706737279892, loss=0.12272106111049652
I1004 10:53:12.355327 140469421979392 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.011409296654164791, loss=0.122925765812397
I1004 10:59:02.979735 140469430372096 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.016184750944375992, loss=0.12772788107395172
I1004 11:01:02.160422 140655693051712 spec.py:321] Evaluating on the training split.
I1004 11:03:57.868307 140655693051712 spec.py:333] Evaluating on the validation split.
I1004 11:06:57.109308 140655693051712 spec.py:349] Evaluating on the test split.
I1004 11:10:29.552176 140655693051712 submission_runner.py:381] Time since start: 5984.49s, 	Step: 5171, 	{'train/loss': 0.12462282480683716, 'validation/loss': 0.12444260789752827, 'validation/num_examples': 83274637, 'test/loss': 0.12725268421052632, 'test/num_examples': 95000000, 'score': 3626.217294692993, 'total_duration': 5984.488785028458, 'accumulated_submission_time': 3626.217294692993, 'accumulated_eval_time': 2358.114166736603, 'accumulated_logging_time': 0.08519554138183594}
I1004 11:10:29.569652 140469421979392 logging_writer.py:48] [5171] accumulated_eval_time=2358.114167, accumulated_logging_time=0.085196, accumulated_submission_time=3626.217295, global_step=5171, preemption_count=0, score=3626.217295, test/loss=0.127253, test/num_examples=95000000, total_duration=5984.488785, train/loss=0.124623, validation/loss=0.124443, validation/num_examples=83274637
I1004 11:14:10.728834 140469430372096 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.008450846187770367, loss=0.11647690832614899
I1004 11:20:05.231852 140469421979392 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.005128361284732819, loss=0.1294151246547699
I1004 11:26:00.191984 140469430372096 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.004267130047082901, loss=0.11620534211397171
I1004 11:30:30.248515 140655693051712 spec.py:321] Evaluating on the training split.
I1004 11:33:14.678205 140655693051712 spec.py:333] Evaluating on the validation split.
I1004 11:36:08.557183 140655693051712 spec.py:349] Evaluating on the test split.
I1004 11:39:30.392068 140655693051712 submission_runner.py:381] Time since start: 7725.33s, 	Step: 6881, 	{'train/loss': 0.12190860772282823, 'validation/loss': 0.12386889179715067, 'validation/num_examples': 83274637, 'test/loss': 0.1261884105263158, 'test/num_examples': 95000000, 'score': 4826.864942550659, 'total_duration': 7725.328666210175, 'accumulated_submission_time': 4826.864942550659, 'accumulated_eval_time': 2898.257682323456, 'accumulated_logging_time': 0.11003494262695312}
I1004 11:39:30.409056 140469421979392 logging_writer.py:48] [6881] accumulated_eval_time=2898.257682, accumulated_logging_time=0.110035, accumulated_submission_time=4826.864943, global_step=6881, preemption_count=0, score=4826.864943, test/loss=0.126188, test/num_examples=95000000, total_duration=7725.328666, train/loss=0.121909, validation/loss=0.123869, validation/num_examples=83274637
I1004 11:40:40.231137 140469430372096 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.007468110881745815, loss=0.11829376965761185
I1004 11:46:31.822415 140469421979392 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.005411647260189056, loss=0.12092238664627075
I1004 11:52:17.222252 140655693051712 spec.py:321] Evaluating on the training split.
I1004 11:54:40.378089 140655693051712 spec.py:333] Evaluating on the validation split.
I1004 11:56:34.454442 140655693051712 spec.py:349] Evaluating on the test split.
I1004 11:59:10.263307 140655693051712 submission_runner.py:381] Time since start: 8905.20s, 	Step: 8000, 	{'train/loss': 0.12246157088369694, 'validation/loss': 0.12382838726754222, 'validation/num_examples': 83274637, 'test/loss': 0.12614123157894735, 'test/num_examples': 95000000, 'score': 5593.655275583267, 'total_duration': 8905.199909687042, 'accumulated_submission_time': 5593.655275583267, 'accumulated_eval_time': 3311.298754930496, 'accumulated_logging_time': 0.1343674659729004}
I1004 11:59:10.279994 140469430372096 logging_writer.py:48] [8000] accumulated_eval_time=3311.298755, accumulated_logging_time=0.134367, accumulated_submission_time=5593.655276, global_step=8000, preemption_count=0, score=5593.655276, test/loss=0.126141, test/num_examples=95000000, total_duration=8905.199910, train/loss=0.122462, validation/loss=0.123828, validation/num_examples=83274637
I1004 11:59:10.293604 140469421979392 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5593.655276
I1004 11:59:16.049603 140655693051712 checkpoints.py:490] Saving checkpoint at step: 8000
I1004 11:59:50.246218 140655693051712 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_5/criteo1tb_jax/trial_1/checkpoint_8000
I1004 11:59:50.602053 140655693051712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_5/criteo1tb_jax/trial_1/checkpoint_8000.
I1004 11:59:50.952205 140655693051712 submission_runner.py:549] Tuning trial 1/1
I1004 11:59:50.952455 140655693051712 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1004 11:59:50.953434 140655693051712 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 1.2732552582362913, 'validation/loss': 1.2764677917479244, 'validation/num_examples': 83274637, 'test/loss': 1.2748633263157896, 'test/num_examples': 95000000, 'score': 25.530160665512085, 'total_duration': 741.5019369125366, 'accumulated_submission_time': 25.530160665512085, 'accumulated_eval_time': 715.9717352390289, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1717, {'train/loss': 0.12267932052132469, 'validation/loss': 0.1253280275481717, 'validation/num_examples': 83274637, 'test/loss': 0.12765314736842107, 'test/num_examples': 95000000, 'score': 1225.7348670959473, 'total_duration': 2464.9419045448303, 'accumulated_submission_time': 1225.7348670959473, 'accumulated_eval_time': 1239.1546547412872, 'accumulated_logging_time': 0.028185606002807617, 'global_step': 1717, 'preemption_count': 0}), (3441, {'train/loss': 0.12474754621397774, 'validation/loss': 0.1248542458371809, 'validation/num_examples': 83274637, 'test/loss': 0.12708248421052631, 'test/num_examples': 95000000, 'score': 2426.0547959804535, 'total_duration': 4216.8799929618835, 'accumulated_submission_time': 2426.0547959804535, 'accumulated_eval_time': 1790.7224497795105, 'accumulated_logging_time': 0.05450773239135742, 'global_step': 3441, 'preemption_count': 0}), (5171, {'train/loss': 0.12462282480683716, 'validation/loss': 0.12444260789752827, 'validation/num_examples': 83274637, 'test/loss': 0.12725268421052632, 'test/num_examples': 95000000, 'score': 3626.217294692993, 'total_duration': 5984.488785028458, 'accumulated_submission_time': 3626.217294692993, 'accumulated_eval_time': 2358.114166736603, 'accumulated_logging_time': 0.08519554138183594, 'global_step': 5171, 'preemption_count': 0}), (6881, {'train/loss': 0.12190860772282823, 'validation/loss': 0.12386889179715067, 'validation/num_examples': 83274637, 'test/loss': 0.1261884105263158, 'test/num_examples': 95000000, 'score': 4826.864942550659, 'total_duration': 7725.328666210175, 'accumulated_submission_time': 4826.864942550659, 'accumulated_eval_time': 2898.257682323456, 'accumulated_logging_time': 0.11003494262695312, 'global_step': 6881, 'preemption_count': 0}), (8000, {'train/loss': 0.12246157088369694, 'validation/loss': 0.12382838726754222, 'validation/num_examples': 83274637, 'test/loss': 0.12614123157894735, 'test/num_examples': 95000000, 'score': 5593.655275583267, 'total_duration': 8905.199909687042, 'accumulated_submission_time': 5593.655275583267, 'accumulated_eval_time': 3311.298754930496, 'accumulated_logging_time': 0.1343674659729004, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1004 11:59:50.953547 140655693051712 submission_runner.py:552] Timing: 5593.655275583267
I1004 11:59:50.953606 140655693051712 submission_runner.py:554] Total number of evals: 6
I1004 11:59:50.953655 140655693051712 submission_runner.py:555] ====================
I1004 11:59:50.953766 140655693051712 submission_runner.py:625] Final criteo1tb score: 5593.655275583267
