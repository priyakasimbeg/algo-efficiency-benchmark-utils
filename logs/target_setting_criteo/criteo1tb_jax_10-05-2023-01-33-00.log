python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_11 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-05-2023-01-33-00.log
2023-10-05 01:33:05.114550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1005 01:33:21.976554 139715226584896 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_11/criteo1tb_jax.
I1005 01:33:23.654750 139715226584896 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1005 01:33:23.655718 139715226584896 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1005 01:33:23.655895 139715226584896 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1005 01:33:23.661322 139715226584896 submission_runner.py:507] Using RNG seed 3597495590
I1005 01:33:29.251209 139715226584896 submission_runner.py:516] --- Tuning run 1/1 ---
I1005 01:33:29.251465 139715226584896 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_11/criteo1tb_jax/trial_1.
I1005 01:33:29.251788 139715226584896 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_11/criteo1tb_jax/trial_1/hparams.json.
I1005 01:33:29.436518 139715226584896 submission_runner.py:191] Initializing dataset.
I1005 01:33:29.436782 139715226584896 submission_runner.py:198] Initializing model.
I1005 01:33:35.437527 139715226584896 submission_runner.py:232] Initializing optimizer.
I1005 01:33:38.640153 139715226584896 submission_runner.py:239] Initializing metrics bundle.
I1005 01:33:38.640441 139715226584896 submission_runner.py:257] Initializing checkpoint and logger.
I1005 01:33:38.641914 139715226584896 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_11/criteo1tb_jax/trial_1 with prefix checkpoint_
I1005 01:33:38.642097 139715226584896 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_11/criteo1tb_jax/trial_1/meta_data_0.json.
I1005 01:33:38.642409 139715226584896 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1005 01:33:38.642483 139715226584896 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1005 01:33:39.503259 139715226584896 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_11/criteo1tb_jax/trial_1/flags_0.json.
I1005 01:33:39.595606 139715226584896 submission_runner.py:290] Starting training loop.
I1005 01:34:04.699041 139551021455104 logging_writer.py:48] [0] global_step=0, grad_norm=11.935284614562988, loss=1.466156005859375
I1005 01:34:04.711726 139715226584896 spec.py:321] Evaluating on the training split.
I1005 01:37:51.829763 139715226584896 spec.py:333] Evaluating on the validation split.
I1005 01:41:44.755788 139715226584896 spec.py:349] Evaluating on the test split.
I1005 01:46:07.397455 139715226584896 submission_runner.py:381] Time since start: 747.80s, 	Step: 1, 	{'train/loss': 1.4639851312217473, 'validation/loss': 1.4677188205575726, 'validation/num_examples': 83274637, 'test/loss': 1.4643166315789473, 'test/num_examples': 95000000, 'score': 25.116081476211548, 'total_duration': 747.8017973899841, 'accumulated_submission_time': 25.116081476211548, 'accumulated_eval_time': 722.6856603622437, 'accumulated_logging_time': 0}
I1005 01:46:07.418310 139530057279232 logging_writer.py:48] [1] accumulated_eval_time=722.685660, accumulated_logging_time=0, accumulated_submission_time=25.116081, global_step=1, preemption_count=0, score=25.116081, test/loss=1.464317, test/num_examples=95000000, total_duration=747.801797, train/loss=1.463985, validation/loss=1.467719, validation/num_examples=83274637
I1005 01:46:07.535040 139530048886528 logging_writer.py:48] [1] global_step=1, grad_norm=11.919755935668945, loss=1.4651882648468018
I1005 01:46:07.642526 139530057279232 logging_writer.py:48] [2] global_step=2, grad_norm=10.788902282714844, loss=1.3311870098114014
I1005 01:46:07.749155 139530048886528 logging_writer.py:48] [3] global_step=3, grad_norm=9.966279983520508, loss=1.11722731590271
I1005 01:46:07.854250 139530057279232 logging_writer.py:48] [4] global_step=4, grad_norm=8.634698867797852, loss=0.8687127232551575
I1005 01:46:07.959358 139530048886528 logging_writer.py:48] [5] global_step=5, grad_norm=6.960019588470459, loss=0.6149077415466309
I1005 01:46:08.064557 139530057279232 logging_writer.py:48] [6] global_step=6, grad_norm=4.64030647277832, loss=0.3933574855327606
I1005 01:46:08.177070 139530048886528 logging_writer.py:48] [7] global_step=7, grad_norm=2.184514045715332, loss=0.2543697953224182
I1005 01:46:08.286477 139530057279232 logging_writer.py:48] [8] global_step=8, grad_norm=0.5522656440734863, loss=0.2023419886827469
I1005 01:46:08.392218 139530048886528 logging_writer.py:48] [9] global_step=9, grad_norm=0.6246769428253174, loss=0.20724360644817352
I1005 01:46:08.498820 139530057279232 logging_writer.py:48] [10] global_step=10, grad_norm=1.2184637784957886, loss=0.23233729600906372
I1005 01:46:08.605785 139530048886528 logging_writer.py:48] [11] global_step=11, grad_norm=1.620242953300476, loss=0.2685319781303406
I1005 01:46:08.712131 139530057279232 logging_writer.py:48] [12] global_step=12, grad_norm=1.9316377639770508, loss=0.3094119429588318
I1005 01:46:08.818186 139530048886528 logging_writer.py:48] [13] global_step=13, grad_norm=2.1805622577667236, loss=0.3457796573638916
I1005 01:46:08.923571 139530057279232 logging_writer.py:48] [14] global_step=14, grad_norm=2.316206455230713, loss=0.3674840033054352
I1005 01:46:09.029397 139530048886528 logging_writer.py:48] [15] global_step=15, grad_norm=2.3981080055236816, loss=0.3808758556842804
I1005 01:46:09.135791 139530057279232 logging_writer.py:48] [16] global_step=16, grad_norm=2.3746097087860107, loss=0.3756951093673706
I1005 01:46:09.241517 139530048886528 logging_writer.py:48] [17] global_step=17, grad_norm=2.2343063354492188, loss=0.3499990403652191
I1005 01:46:09.346831 139530057279232 logging_writer.py:48] [18] global_step=18, grad_norm=2.1454455852508545, loss=0.3303891718387604
I1005 01:46:09.455248 139530048886528 logging_writer.py:48] [19] global_step=19, grad_norm=1.6859798431396484, loss=0.2649308741092682
I1005 01:46:09.562369 139530057279232 logging_writer.py:48] [20] global_step=20, grad_norm=1.4038890600204468, loss=0.23120570182800293
I1005 01:46:09.670500 139530048886528 logging_writer.py:48] [21] global_step=21, grad_norm=1.1042174100875854, loss=0.2008371502161026
I1005 01:46:09.778298 139530057279232 logging_writer.py:48] [22] global_step=22, grad_norm=0.7042945623397827, loss=0.1771145462989807
I1005 01:46:09.883211 139530048886528 logging_writer.py:48] [23] global_step=23, grad_norm=0.20592060685157776, loss=0.16385942697525024
I1005 01:46:09.987786 139530057279232 logging_writer.py:48] [24] global_step=24, grad_norm=0.7155815362930298, loss=0.16227981448173523
I1005 01:46:10.092165 139530048886528 logging_writer.py:48] [25] global_step=25, grad_norm=0.8531348705291748, loss=0.16057220101356506
I1005 01:46:10.196917 139530057279232 logging_writer.py:48] [26] global_step=26, grad_norm=0.5307213664054871, loss=0.15067562460899353
I1005 01:46:10.301209 139530048886528 logging_writer.py:48] [27] global_step=27, grad_norm=0.12333326786756516, loss=0.14327216148376465
I1005 01:46:10.404926 139530057279232 logging_writer.py:48] [28] global_step=28, grad_norm=0.2743839621543884, loss=0.1381867676973343
I1005 01:46:11.084261 139530048886528 logging_writer.py:48] [29] global_step=29, grad_norm=0.36731454730033875, loss=0.1370248943567276
I1005 01:46:11.802936 139530057279232 logging_writer.py:48] [30] global_step=30, grad_norm=0.2664739489555359, loss=0.13401052355766296
I1005 01:46:12.542874 139530048886528 logging_writer.py:48] [31] global_step=31, grad_norm=0.11539552360773087, loss=0.13364344835281372
I1005 01:46:13.335579 139530057279232 logging_writer.py:48] [32] global_step=32, grad_norm=0.04087186977267265, loss=0.1322619616985321
I1005 01:46:14.014473 139530048886528 logging_writer.py:48] [33] global_step=33, grad_norm=0.036504555493593216, loss=0.12799331545829773
I1005 01:46:14.727226 139530057279232 logging_writer.py:48] [34] global_step=34, grad_norm=0.07543262094259262, loss=0.12776099145412445
I1005 01:46:15.506610 139530048886528 logging_writer.py:48] [35] global_step=35, grad_norm=0.04757889360189438, loss=0.12955491244792938
I1005 01:46:16.268693 139530057279232 logging_writer.py:48] [36] global_step=36, grad_norm=0.05193639174103737, loss=0.12639100849628448
I1005 01:46:16.998842 139530048886528 logging_writer.py:48] [37] global_step=37, grad_norm=0.04864247143268585, loss=0.12677247822284698
I1005 01:46:17.657031 139530057279232 logging_writer.py:48] [38] global_step=38, grad_norm=0.1284107267856598, loss=0.14271408319473267
I1005 01:46:18.434223 139530048886528 logging_writer.py:48] [39] global_step=39, grad_norm=0.4321785867214203, loss=0.15006256103515625
I1005 01:46:19.263246 139530057279232 logging_writer.py:48] [40] global_step=40, grad_norm=0.6070358753204346, loss=0.15266180038452148
I1005 01:46:19.838665 139530048886528 logging_writer.py:48] [41] global_step=41, grad_norm=0.8867026567459106, loss=0.1546022593975067
I1005 01:46:20.561354 139530057279232 logging_writer.py:48] [42] global_step=42, grad_norm=0.5884592533111572, loss=0.1508704125881195
I1005 01:46:21.369664 139530048886528 logging_writer.py:48] [43] global_step=43, grad_norm=0.16507688164710999, loss=0.14549526572227478
I1005 01:46:22.062206 139530057279232 logging_writer.py:48] [44] global_step=44, grad_norm=0.030549371615052223, loss=0.1456221044063568
I1005 01:46:23.055931 139530048886528 logging_writer.py:48] [45] global_step=45, grad_norm=0.026546843349933624, loss=0.1453636884689331
I1005 01:46:23.572526 139530057279232 logging_writer.py:48] [46] global_step=46, grad_norm=0.019969264045357704, loss=0.143437460064888
I1005 01:46:24.252050 139530048886528 logging_writer.py:48] [47] global_step=47, grad_norm=0.04895782098174095, loss=0.14271609485149384
I1005 01:46:24.897740 139530057279232 logging_writer.py:48] [48] global_step=48, grad_norm=0.06414920836687088, loss=0.14321690797805786
I1005 01:46:25.736136 139530048886528 logging_writer.py:48] [49] global_step=49, grad_norm=0.05927426367998123, loss=0.14061076939105988
I1005 01:46:26.313097 139530057279232 logging_writer.py:48] [50] global_step=50, grad_norm=0.07341259717941284, loss=0.14029204845428467
I1005 01:46:27.090588 139530048886528 logging_writer.py:48] [51] global_step=51, grad_norm=0.13937540352344513, loss=0.1411570906639099
I1005 01:46:27.810697 139530057279232 logging_writer.py:48] [52] global_step=52, grad_norm=0.29069554805755615, loss=0.14099614322185516
I1005 01:46:28.436776 139530048886528 logging_writer.py:48] [53] global_step=53, grad_norm=0.4322283864021301, loss=0.14369043707847595
I1005 01:46:29.359522 139530057279232 logging_writer.py:48] [54] global_step=54, grad_norm=0.7282599806785583, loss=0.14241069555282593
I1005 01:46:30.005201 139530048886528 logging_writer.py:48] [55] global_step=55, grad_norm=0.7426177859306335, loss=0.14946265518665314
I1005 01:46:30.841526 139530057279232 logging_writer.py:48] [56] global_step=56, grad_norm=0.37513405084609985, loss=0.14152711629867554
I1005 01:46:31.549118 139530048886528 logging_writer.py:48] [57] global_step=57, grad_norm=0.0950467586517334, loss=0.130263090133667
I1005 01:46:32.232772 139530057279232 logging_writer.py:48] [58] global_step=58, grad_norm=0.046027395874261856, loss=0.12984821200370789
I1005 01:46:33.051574 139530048886528 logging_writer.py:48] [59] global_step=59, grad_norm=0.010094132274389267, loss=0.12936745584011078
I1005 01:46:33.666033 139530057279232 logging_writer.py:48] [60] global_step=60, grad_norm=0.01249574963003397, loss=0.12999853491783142
I1005 01:46:34.383012 139530048886528 logging_writer.py:48] [61] global_step=61, grad_norm=0.03008064068853855, loss=0.12825651466846466
I1005 01:46:35.232289 139530057279232 logging_writer.py:48] [62] global_step=62, grad_norm=0.03441173583269119, loss=0.12849737703800201
I1005 01:46:35.892051 139530048886528 logging_writer.py:48] [63] global_step=63, grad_norm=0.025170698761940002, loss=0.12767072021961212
I1005 01:46:36.629158 139530057279232 logging_writer.py:48] [64] global_step=64, grad_norm=0.01987701840698719, loss=0.1282707303762436
I1005 01:46:37.153216 139530048886528 logging_writer.py:48] [65] global_step=65, grad_norm=0.011114150285720825, loss=0.1287938803434372
I1005 01:46:37.939103 139530057279232 logging_writer.py:48] [66] global_step=66, grad_norm=0.012961141765117645, loss=0.1293407827615738
I1005 01:46:38.689861 139530048886528 logging_writer.py:48] [67] global_step=67, grad_norm=0.029146015644073486, loss=0.12706933915615082
I1005 01:46:39.518643 139530057279232 logging_writer.py:48] [68] global_step=68, grad_norm=0.04079453647136688, loss=0.12577581405639648
I1005 01:46:40.141098 139530048886528 logging_writer.py:48] [69] global_step=69, grad_norm=0.020042363554239273, loss=0.1291169971227646
I1005 01:46:41.001197 139530057279232 logging_writer.py:48] [70] global_step=70, grad_norm=0.00791733805090189, loss=0.12826065719127655
I1005 01:46:41.545428 139530048886528 logging_writer.py:48] [71] global_step=71, grad_norm=0.014891373924911022, loss=0.12839668989181519
I1005 01:46:42.159232 139530057279232 logging_writer.py:48] [72] global_step=72, grad_norm=0.023501072078943253, loss=0.12803512811660767
I1005 01:46:42.831499 139530048886528 logging_writer.py:48] [73] global_step=73, grad_norm=0.04047958552837372, loss=0.12647907435894012
I1005 01:46:43.642297 139530057279232 logging_writer.py:48] [74] global_step=74, grad_norm=0.07514871656894684, loss=0.12637905776500702
I1005 01:46:44.215099 139530048886528 logging_writer.py:48] [75] global_step=75, grad_norm=0.1416703313589096, loss=0.1270778328180313
I1005 01:46:44.890940 139530057279232 logging_writer.py:48] [76] global_step=76, grad_norm=0.1869843453168869, loss=0.1263076663017273
I1005 01:46:45.728265 139530048886528 logging_writer.py:48] [77] global_step=77, grad_norm=0.2018042951822281, loss=0.12273836880922318
I1005 01:46:46.273399 139530057279232 logging_writer.py:48] [78] global_step=78, grad_norm=0.2587558627128601, loss=0.12179148197174072
I1005 01:46:46.905540 139530048886528 logging_writer.py:48] [79] global_step=79, grad_norm=0.2865644097328186, loss=0.12302026152610779
I1005 01:46:47.611601 139530057279232 logging_writer.py:48] [80] global_step=80, grad_norm=0.299411416053772, loss=0.12105293571949005
I1005 01:46:48.276077 139530048886528 logging_writer.py:48] [81] global_step=81, grad_norm=0.30138731002807617, loss=0.12399289011955261
I1005 01:46:49.097524 139530057279232 logging_writer.py:48] [82] global_step=82, grad_norm=0.27583548426628113, loss=0.12253237515687943
I1005 01:46:49.694360 139530048886528 logging_writer.py:48] [83] global_step=83, grad_norm=0.21423016488552094, loss=0.12084348499774933
I1005 01:46:50.286380 139530057279232 logging_writer.py:48] [84] global_step=84, grad_norm=0.1361531913280487, loss=0.12136029452085495
I1005 01:46:50.937551 139530048886528 logging_writer.py:48] [85] global_step=85, grad_norm=0.1038651093840599, loss=0.12264982610940933
I1005 01:46:51.699959 139530057279232 logging_writer.py:48] [86] global_step=86, grad_norm=0.08462831377983093, loss=0.12245193868875504
I1005 01:46:52.334595 139530048886528 logging_writer.py:48] [87] global_step=87, grad_norm=0.0629788264632225, loss=0.12140185385942459
I1005 01:46:52.982770 139530057279232 logging_writer.py:48] [88] global_step=88, grad_norm=0.05462973937392235, loss=0.12215058505535126
I1005 01:46:53.696685 139530048886528 logging_writer.py:48] [89] global_step=89, grad_norm=0.02869405224919319, loss=0.12027844786643982
I1005 01:46:54.304702 139530057279232 logging_writer.py:48] [90] global_step=90, grad_norm=0.010421170853078365, loss=0.12084762752056122
I1005 01:46:55.055824 139530048886528 logging_writer.py:48] [91] global_step=91, grad_norm=0.016803715378046036, loss=0.12110334634780884
I1005 01:46:55.807965 139530057279232 logging_writer.py:48] [92] global_step=92, grad_norm=0.03215309977531433, loss=0.12094487249851227
I1005 01:46:56.329566 139530048886528 logging_writer.py:48] [93] global_step=93, grad_norm=0.04521237313747406, loss=0.12200386077165604
I1005 01:46:57.072810 139530057279232 logging_writer.py:48] [94] global_step=94, grad_norm=0.07440462708473206, loss=0.11978568136692047
I1005 01:46:57.731835 139530048886528 logging_writer.py:48] [95] global_step=95, grad_norm=0.10168606787919998, loss=0.1269577294588089
I1005 01:46:58.497126 139530057279232 logging_writer.py:48] [96] global_step=96, grad_norm=0.13929349184036255, loss=0.1301548033952713
I1005 01:46:59.179549 139530048886528 logging_writer.py:48] [97] global_step=97, grad_norm=0.16521666944026947, loss=0.12830765545368195
I1005 01:46:59.849964 139530057279232 logging_writer.py:48] [98] global_step=98, grad_norm=0.16997432708740234, loss=0.13265755772590637
I1005 01:47:00.573247 139530048886528 logging_writer.py:48] [99] global_step=99, grad_norm=0.18306215107440948, loss=0.1311480849981308
I1005 01:47:01.281531 139530057279232 logging_writer.py:48] [100] global_step=100, grad_norm=0.2163029909133911, loss=0.12973588705062866
I1005 01:51:42.647355 139530048886528 logging_writer.py:48] [500] global_step=500, grad_norm=0.00997211690992117, loss=0.13019630312919617
I1005 01:57:45.442034 139530057279232 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.12123088538646698, loss=0.13379992544651031
I1005 02:03:51.496470 139530048886528 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0544794462621212, loss=0.12372972071170807
I1005 02:06:08.008329 139715226584896 spec.py:321] Evaluating on the training split.
I1005 02:09:14.058883 139715226584896 spec.py:333] Evaluating on the validation split.
I1005 02:12:01.961525 139715226584896 spec.py:349] Evaluating on the test split.
I1005 02:14:55.018006 139715226584896 submission_runner.py:381] Time since start: 2475.42s, 	Step: 1690, 	{'train/loss': 0.12373438421285378, 'validation/loss': 0.12560455832428305, 'validation/num_examples': 83274637, 'test/loss': 0.12798494736842106, 'test/num_examples': 95000000, 'score': 1225.674236536026, 'total_duration': 2475.422340154648, 'accumulated_submission_time': 1225.674236536026, 'accumulated_eval_time': 1249.695288658142, 'accumulated_logging_time': 0.029331445693969727}
I1005 02:14:55.033925 139530057279232 logging_writer.py:48] [1690] accumulated_eval_time=1249.695289, accumulated_logging_time=0.029331, accumulated_submission_time=1225.674237, global_step=1690, preemption_count=0, score=1225.674237, test/loss=0.127985, test/num_examples=95000000, total_duration=2475.422340, train/loss=0.123734, validation/loss=0.125605, validation/num_examples=83274637
I1005 02:18:22.722561 139530048886528 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.08113224804401398, loss=0.12472636252641678
I1005 02:24:19.269029 139530057279232 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.018525579944252968, loss=0.12622961401939392
I1005 02:30:25.450511 139530048886528 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.015160410664975643, loss=0.12538515031337738
I1005 02:34:55.362167 139715226584896 spec.py:321] Evaluating on the training split.
I1005 02:37:58.720114 139715226584896 spec.py:333] Evaluating on the validation split.
I1005 02:40:58.479404 139715226584896 spec.py:349] Evaluating on the test split.
I1005 02:44:14.468469 139715226584896 submission_runner.py:381] Time since start: 4234.87s, 	Step: 3376, 	{'train/loss': 0.1218409148402184, 'validation/loss': 0.12473437740713297, 'validation/num_examples': 83274637, 'test/loss': 0.12725248421052632, 'test/num_examples': 95000000, 'score': 2425.9706761837006, 'total_duration': 4234.872818946838, 'accumulated_submission_time': 2425.9706761837006, 'accumulated_eval_time': 1808.801582813263, 'accumulated_logging_time': 0.05394864082336426}
I1005 02:44:14.485432 139530057279232 logging_writer.py:48] [3376] accumulated_eval_time=1808.801583, accumulated_logging_time=0.053949, accumulated_submission_time=2425.970676, global_step=3376, preemption_count=0, score=2425.970676, test/loss=0.127252, test/num_examples=95000000, total_duration=4234.872819, train/loss=0.121841, validation/loss=0.124734, validation/num_examples=83274637
I1005 02:45:28.161271 139530048886528 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.005512769799679518, loss=0.11892855167388916
I1005 02:51:27.555719 139530057279232 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.013663817197084427, loss=0.1255851686000824
I1005 02:57:22.801697 139530048886528 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.005315104499459267, loss=0.12041991949081421
I1005 03:03:21.699569 139530057279232 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.016197863966226578, loss=0.1237616315484047
I1005 03:04:14.956243 139715226584896 spec.py:321] Evaluating on the training split.
I1005 03:07:10.840027 139715226584896 spec.py:333] Evaluating on the validation split.
I1005 03:10:08.024127 139715226584896 spec.py:349] Evaluating on the test split.
I1005 03:13:42.621291 139715226584896 submission_runner.py:381] Time since start: 6003.03s, 	Step: 5076, 	{'train/loss': 0.12393519563494988, 'validation/loss': 0.12432455274467302, 'validation/num_examples': 83274637, 'test/loss': 0.12665413684210527, 'test/num_examples': 95000000, 'score': 3626.408106803894, 'total_duration': 6003.025599002838, 'accumulated_submission_time': 3626.408106803894, 'accumulated_eval_time': 2376.466544866562, 'accumulated_logging_time': 0.08091998100280762}
I1005 03:13:42.635271 139530048886528 logging_writer.py:48] [5076] accumulated_eval_time=2376.466545, accumulated_logging_time=0.080920, accumulated_submission_time=3626.408107, global_step=5076, preemption_count=0, score=3626.408107, test/loss=0.126654, test/num_examples=95000000, total_duration=6003.025599, train/loss=0.123935, validation/loss=0.124325, validation/num_examples=83274637
I1005 03:18:33.668998 139530057279232 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.008007997646927834, loss=0.12089143693447113
I1005 03:24:30.862435 139530048886528 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.005254571326076984, loss=0.13070878386497498
I1005 03:30:20.386975 139530057279232 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.011312227696180344, loss=0.1179206594824791
I1005 03:33:43.115357 139715226584896 spec.py:321] Evaluating on the training split.
I1005 03:36:31.381993 139715226584896 spec.py:333] Evaluating on the validation split.
I1005 03:39:01.669411 139715226584896 spec.py:349] Evaluating on the test split.
I1005 03:42:19.534554 139715226584896 submission_runner.py:381] Time since start: 7719.94s, 	Step: 6782, 	{'train/loss': 0.1227980079890797, 'validation/loss': 0.12382634582964318, 'validation/num_examples': 83274637, 'test/loss': 0.1261001157894737, 'test/num_examples': 95000000, 'score': 4826.8575484752655, 'total_duration': 7719.938907146454, 'accumulated_submission_time': 4826.8575484752655, 'accumulated_eval_time': 2892.8857254981995, 'accumulated_logging_time': 0.10201859474182129}
I1005 03:42:19.554247 139530048886528 logging_writer.py:48] [6782] accumulated_eval_time=2892.885725, accumulated_logging_time=0.102019, accumulated_submission_time=4826.857548, global_step=6782, preemption_count=0, score=4826.857548, test/loss=0.126100, test/num_examples=95000000, total_duration=7719.938907, train/loss=0.122798, validation/loss=0.123826, validation/num_examples=83274637
I1005 03:44:40.388474 139530057279232 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.004071378149092197, loss=0.12010356038808823
I1005 03:50:40.096063 139530048886528 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.006221281364560127, loss=0.12491506338119507
I1005 03:56:43.385720 139715226584896 spec.py:321] Evaluating on the training split.
I1005 03:59:01.562264 139715226584896 spec.py:333] Evaluating on the validation split.
I1005 04:00:58.822932 139715226584896 spec.py:349] Evaluating on the test split.
I1005 04:03:54.373839 139715226584896 submission_runner.py:381] Time since start: 9014.78s, 	Step: 8000, 	{'train/loss': 0.12119096480075668, 'validation/loss': 0.1237339527520246, 'validation/num_examples': 83274637, 'test/loss': 0.1259997894736842, 'test/num_examples': 95000000, 'score': 5690.6648325920105, 'total_duration': 9014.778187513351, 'accumulated_submission_time': 5690.6648325920105, 'accumulated_eval_time': 3323.8738219738007, 'accumulated_logging_time': 0.12918639183044434}
I1005 04:03:54.393560 139530057279232 logging_writer.py:48] [8000] accumulated_eval_time=3323.873822, accumulated_logging_time=0.129186, accumulated_submission_time=5690.664833, global_step=8000, preemption_count=0, score=5690.664833, test/loss=0.126000, test/num_examples=95000000, total_duration=9014.778188, train/loss=0.121191, validation/loss=0.123734, validation/num_examples=83274637
I1005 04:03:54.407295 139530048886528 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5690.664833
I1005 04:04:00.013226 139715226584896 checkpoints.py:490] Saving checkpoint at step: 8000
I1005 04:04:35.653249 139715226584896 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_11/criteo1tb_jax/trial_1/checkpoint_8000
I1005 04:04:35.990525 139715226584896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_11/criteo1tb_jax/trial_1/checkpoint_8000.
I1005 04:04:36.287462 139715226584896 submission_runner.py:549] Tuning trial 1/1
I1005 04:04:36.287705 139715226584896 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1005 04:04:36.288759 139715226584896 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 1.4639851312217473, 'validation/loss': 1.4677188205575726, 'validation/num_examples': 83274637, 'test/loss': 1.4643166315789473, 'test/num_examples': 95000000, 'score': 25.116081476211548, 'total_duration': 747.8017973899841, 'accumulated_submission_time': 25.116081476211548, 'accumulated_eval_time': 722.6856603622437, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1690, {'train/loss': 0.12373438421285378, 'validation/loss': 0.12560455832428305, 'validation/num_examples': 83274637, 'test/loss': 0.12798494736842106, 'test/num_examples': 95000000, 'score': 1225.674236536026, 'total_duration': 2475.422340154648, 'accumulated_submission_time': 1225.674236536026, 'accumulated_eval_time': 1249.695288658142, 'accumulated_logging_time': 0.029331445693969727, 'global_step': 1690, 'preemption_count': 0}), (3376, {'train/loss': 0.1218409148402184, 'validation/loss': 0.12473437740713297, 'validation/num_examples': 83274637, 'test/loss': 0.12725248421052632, 'test/num_examples': 95000000, 'score': 2425.9706761837006, 'total_duration': 4234.872818946838, 'accumulated_submission_time': 2425.9706761837006, 'accumulated_eval_time': 1808.801582813263, 'accumulated_logging_time': 0.05394864082336426, 'global_step': 3376, 'preemption_count': 0}), (5076, {'train/loss': 0.12393519563494988, 'validation/loss': 0.12432455274467302, 'validation/num_examples': 83274637, 'test/loss': 0.12665413684210527, 'test/num_examples': 95000000, 'score': 3626.408106803894, 'total_duration': 6003.025599002838, 'accumulated_submission_time': 3626.408106803894, 'accumulated_eval_time': 2376.466544866562, 'accumulated_logging_time': 0.08091998100280762, 'global_step': 5076, 'preemption_count': 0}), (6782, {'train/loss': 0.1227980079890797, 'validation/loss': 0.12382634582964318, 'validation/num_examples': 83274637, 'test/loss': 0.1261001157894737, 'test/num_examples': 95000000, 'score': 4826.8575484752655, 'total_duration': 7719.938907146454, 'accumulated_submission_time': 4826.8575484752655, 'accumulated_eval_time': 2892.8857254981995, 'accumulated_logging_time': 0.10201859474182129, 'global_step': 6782, 'preemption_count': 0}), (8000, {'train/loss': 0.12119096480075668, 'validation/loss': 0.1237339527520246, 'validation/num_examples': 83274637, 'test/loss': 0.1259997894736842, 'test/num_examples': 95000000, 'score': 5690.6648325920105, 'total_duration': 9014.778187513351, 'accumulated_submission_time': 5690.6648325920105, 'accumulated_eval_time': 3323.8738219738007, 'accumulated_logging_time': 0.12918639183044434, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1005 04:04:36.288885 139715226584896 submission_runner.py:552] Timing: 5690.6648325920105
I1005 04:04:36.288949 139715226584896 submission_runner.py:554] Total number of evals: 6
I1005 04:04:36.289007 139715226584896 submission_runner.py:555] ====================
I1005 04:04:36.289117 139715226584896 submission_runner.py:625] Final criteo1tb score: 5690.6648325920105
