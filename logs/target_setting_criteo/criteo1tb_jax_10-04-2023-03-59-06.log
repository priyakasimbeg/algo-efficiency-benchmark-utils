python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_3 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-04-2023-03-59-06.log
2023-10-04 03:59:11.633933: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1004 03:59:28.784017 139698279835456 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_3/criteo1tb_jax.
I1004 03:59:30.429089 139698279835456 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1004 03:59:30.429961 139698279835456 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1004 03:59:30.430099 139698279835456 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1004 03:59:30.435495 139698279835456 submission_runner.py:507] Using RNG seed 2459803252
I1004 03:59:36.252983 139698279835456 submission_runner.py:516] --- Tuning run 1/1 ---
I1004 03:59:36.253247 139698279835456 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_3/criteo1tb_jax/trial_1.
I1004 03:59:36.253584 139698279835456 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_3/criteo1tb_jax/trial_1/hparams.json.
I1004 03:59:36.446936 139698279835456 submission_runner.py:191] Initializing dataset.
I1004 03:59:36.447243 139698279835456 submission_runner.py:198] Initializing model.
I1004 03:59:42.524937 139698279835456 submission_runner.py:232] Initializing optimizer.
I1004 03:59:45.647919 139698279835456 submission_runner.py:239] Initializing metrics bundle.
I1004 03:59:45.648188 139698279835456 submission_runner.py:257] Initializing checkpoint and logger.
I1004 03:59:45.649566 139698279835456 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_3/criteo1tb_jax/trial_1 with prefix checkpoint_
I1004 03:59:45.649747 139698279835456 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_3/criteo1tb_jax/trial_1/meta_data_0.json.
I1004 03:59:45.649998 139698279835456 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1004 03:59:45.650068 139698279835456 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1004 03:59:46.471138 139698279835456 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_3/criteo1tb_jax/trial_1/flags_0.json.
I1004 03:59:46.568554 139698279835456 submission_runner.py:290] Starting training loop.
I1004 04:00:13.902612 139534177130240 logging_writer.py:48] [0] global_step=0, grad_norm=2.5150911808013916, loss=0.30282020568847656
I1004 04:00:13.913901 139698279835456 spec.py:321] Evaluating on the training split.
I1004 04:04:25.568587 139698279835456 spec.py:333] Evaluating on the validation split.
I1004 04:08:41.581185 139698279835456 spec.py:349] Evaluating on the test split.
I1004 04:13:29.746326 139698279835456 submission_runner.py:381] Time since start: 823.18s, 	Step: 1, 	{'train/loss': 0.3022634158344389, 'validation/loss': 0.30157506420592384, 'validation/num_examples': 83274637, 'test/loss': 0.30287703157894735, 'test/num_examples': 95000000, 'score': 27.345337390899658, 'total_duration': 823.1777153015137, 'accumulated_submission_time': 27.345337390899658, 'accumulated_eval_time': 795.8323402404785, 'accumulated_logging_time': 0}
I1004 04:13:29.764949 139513170990848 logging_writer.py:48] [1] accumulated_eval_time=795.832340, accumulated_logging_time=0, accumulated_submission_time=27.345337, global_step=1, preemption_count=0, score=27.345337, test/loss=0.302877, test/num_examples=95000000, total_duration=823.177715, train/loss=0.302263, validation/loss=0.301575, validation/num_examples=83274637
I1004 04:13:29.878834 139511610734336 logging_writer.py:48] [1] global_step=1, grad_norm=2.518901824951172, loss=0.30254995822906494
I1004 04:13:29.986380 139513170990848 logging_writer.py:48] [2] global_step=2, grad_norm=2.144052267074585, loss=0.2750059962272644
I1004 04:13:30.091219 139511610734336 logging_writer.py:48] [3] global_step=3, grad_norm=1.7210801839828491, loss=0.23437359929084778
I1004 04:13:30.194546 139513170990848 logging_writer.py:48] [4] global_step=4, grad_norm=1.1381683349609375, loss=0.19539222121238708
I1004 04:13:30.297967 139511610734336 logging_writer.py:48] [5] global_step=5, grad_norm=0.6286855340003967, loss=0.1665746420621872
I1004 04:13:30.401272 139513170990848 logging_writer.py:48] [6] global_step=6, grad_norm=0.20716366171836853, loss=0.15051083266735077
I1004 04:13:30.504613 139511610734336 logging_writer.py:48] [7] global_step=7, grad_norm=0.1373598277568817, loss=0.14964525401592255
I1004 04:13:30.609160 139513170990848 logging_writer.py:48] [8] global_step=8, grad_norm=0.3841414153575897, loss=0.15902075171470642
I1004 04:13:30.715221 139511610734336 logging_writer.py:48] [9] global_step=9, grad_norm=0.4971860647201538, loss=0.1621187925338745
I1004 04:13:30.822512 139513170990848 logging_writer.py:48] [10] global_step=10, grad_norm=0.5288602113723755, loss=0.1628638356924057
I1004 04:13:30.925771 139511610734336 logging_writer.py:48] [11] global_step=11, grad_norm=0.5313171744346619, loss=0.16411197185516357
I1004 04:13:31.029376 139513170990848 logging_writer.py:48] [12] global_step=12, grad_norm=0.4582134485244751, loss=0.1591087132692337
I1004 04:13:31.134820 139511610734336 logging_writer.py:48] [13] global_step=13, grad_norm=0.36833763122558594, loss=0.15520282089710236
I1004 04:13:31.238927 139513170990848 logging_writer.py:48] [14] global_step=14, grad_norm=0.22697077691555023, loss=0.14838820695877075
I1004 04:13:31.343596 139511610734336 logging_writer.py:48] [15] global_step=15, grad_norm=0.10689778625965118, loss=0.1474144011735916
I1004 04:13:31.447097 139513170990848 logging_writer.py:48] [16] global_step=16, grad_norm=0.049133770167827606, loss=0.14435376226902008
I1004 04:13:31.552292 139511610734336 logging_writer.py:48] [17] global_step=17, grad_norm=0.12174052000045776, loss=0.14361292123794556
I1004 04:13:31.656421 139513170990848 logging_writer.py:48] [18] global_step=18, grad_norm=0.11395198106765747, loss=0.14459368586540222
I1004 04:13:31.760285 139511610734336 logging_writer.py:48] [19] global_step=19, grad_norm=0.09425102919340134, loss=0.13792751729488373
I1004 04:13:31.867149 139513170990848 logging_writer.py:48] [20] global_step=20, grad_norm=0.02491702511906624, loss=0.13245688378810883
I1004 04:13:31.971841 139511610734336 logging_writer.py:48] [21] global_step=21, grad_norm=0.07969702780246735, loss=0.13486269116401672
I1004 04:13:32.078518 139513170990848 logging_writer.py:48] [22] global_step=22, grad_norm=0.05032392963767052, loss=0.13523255288600922
I1004 04:13:32.183236 139511610734336 logging_writer.py:48] [23] global_step=23, grad_norm=0.013391694985330105, loss=0.13162481784820557
I1004 04:13:32.287581 139513170990848 logging_writer.py:48] [24] global_step=24, grad_norm=0.015727071091532707, loss=0.1342429220676422
I1004 04:13:32.392391 139511610734336 logging_writer.py:48] [25] global_step=25, grad_norm=0.01330716535449028, loss=0.13398689031600952
I1004 04:13:32.496856 139513170990848 logging_writer.py:48] [26] global_step=26, grad_norm=0.010628651827573776, loss=0.13278977572917938
I1004 04:13:32.600651 139511610734336 logging_writer.py:48] [27] global_step=27, grad_norm=0.009761635214090347, loss=0.13044096529483795
I1004 04:13:33.083062 139513170990848 logging_writer.py:48] [28] global_step=28, grad_norm=0.009931928478181362, loss=0.13148987293243408
I1004 04:13:33.844954 139511610734336 logging_writer.py:48] [29] global_step=29, grad_norm=0.015173043124377728, loss=0.13083592057228088
I1004 04:13:34.665988 139513170990848 logging_writer.py:48] [30] global_step=30, grad_norm=0.034040726721286774, loss=0.13269230723381042
I1004 04:13:35.534847 139511610734336 logging_writer.py:48] [31] global_step=31, grad_norm=0.07610318809747696, loss=0.13187797367572784
I1004 04:13:36.303707 139513170990848 logging_writer.py:48] [32] global_step=32, grad_norm=0.09266126900911331, loss=0.13174518942832947
I1004 04:13:37.109370 139511610734336 logging_writer.py:48] [33] global_step=33, grad_norm=0.07313709706068039, loss=0.13145223259925842
I1004 04:13:37.933151 139513170990848 logging_writer.py:48] [34] global_step=34, grad_norm=0.04726598411798477, loss=0.1316097229719162
I1004 04:13:38.788139 139511610734336 logging_writer.py:48] [35] global_step=35, grad_norm=0.036836836487054825, loss=0.13119076192378998
I1004 04:13:39.665027 139513170990848 logging_writer.py:48] [36] global_step=36, grad_norm=0.037904638797044754, loss=0.13024333119392395
I1004 04:13:40.483219 139511610734336 logging_writer.py:48] [37] global_step=37, grad_norm=0.03212114796042442, loss=0.12994419038295746
I1004 04:13:41.318356 139513170990848 logging_writer.py:48] [38] global_step=38, grad_norm=0.07206638157367706, loss=0.13841021060943604
I1004 04:13:42.375547 139511610734336 logging_writer.py:48] [39] global_step=39, grad_norm=0.14202173054218292, loss=0.13886013627052307
I1004 04:13:43.038770 139513170990848 logging_writer.py:48] [40] global_step=40, grad_norm=0.207734614610672, loss=0.14283493161201477
I1004 04:13:43.814667 139511610734336 logging_writer.py:48] [41] global_step=41, grad_norm=0.21784119307994843, loss=0.13970695436000824
I1004 04:13:44.801124 139513170990848 logging_writer.py:48] [42] global_step=42, grad_norm=0.14861567318439484, loss=0.1413191258907318
I1004 04:13:45.664703 139511610734336 logging_writer.py:48] [43] global_step=43, grad_norm=0.05582223832607269, loss=0.14164794981479645
I1004 04:13:46.421212 139513170990848 logging_writer.py:48] [44] global_step=44, grad_norm=0.018601369112730026, loss=0.1401515007019043
I1004 04:13:47.207473 139511610734336 logging_writer.py:48] [45] global_step=45, grad_norm=0.011248637922108173, loss=0.14045369625091553
I1004 04:13:47.958335 139513170990848 logging_writer.py:48] [46] global_step=46, grad_norm=0.014759945683181286, loss=0.138901948928833
I1004 04:13:48.893429 139511610734336 logging_writer.py:48] [47] global_step=47, grad_norm=0.03658292815089226, loss=0.13913370668888092
I1004 04:13:49.582818 139513170990848 logging_writer.py:48] [48] global_step=48, grad_norm=0.05041106417775154, loss=0.13924410939216614
I1004 04:13:50.481141 139511610734336 logging_writer.py:48] [49] global_step=49, grad_norm=0.09226514399051666, loss=0.13824553787708282
I1004 04:13:51.312568 139513170990848 logging_writer.py:48] [50] global_step=50, grad_norm=0.2051304280757904, loss=0.13904398679733276
I1004 04:13:52.125212 139511610734336 logging_writer.py:48] [51] global_step=51, grad_norm=0.31609344482421875, loss=0.1437242031097412
I1004 04:13:53.111530 139513170990848 logging_writer.py:48] [52] global_step=52, grad_norm=0.2919463515281677, loss=0.14203853905200958
I1004 04:13:54.033153 139511610734336 logging_writer.py:48] [53] global_step=53, grad_norm=0.16274143755435944, loss=0.13913220167160034
I1004 04:13:54.774822 139513170990848 logging_writer.py:48] [54] global_step=54, grad_norm=0.05003122240304947, loss=0.13679002225399017
I1004 04:13:55.519541 139511610734336 logging_writer.py:48] [55] global_step=55, grad_norm=0.01235335972160101, loss=0.13864511251449585
I1004 04:13:56.292376 139513170990848 logging_writer.py:48] [56] global_step=56, grad_norm=0.009522708132863045, loss=0.13855084776878357
I1004 04:13:57.206665 139511610734336 logging_writer.py:48] [57] global_step=57, grad_norm=0.011092211119830608, loss=0.13192902505397797
I1004 04:13:58.132624 139513170990848 logging_writer.py:48] [58] global_step=58, grad_norm=0.01406613364815712, loss=0.1275114119052887
I1004 04:13:58.957178 139511610734336 logging_writer.py:48] [59] global_step=59, grad_norm=0.007780252955853939, loss=0.12465020269155502
I1004 04:13:59.724789 139513170990848 logging_writer.py:48] [60] global_step=60, grad_norm=0.025733496993780136, loss=0.12805451452732086
I1004 04:14:00.568230 139511610734336 logging_writer.py:48] [61] global_step=61, grad_norm=0.01345797162503004, loss=0.12943868339061737
I1004 04:14:01.568502 139513170990848 logging_writer.py:48] [62] global_step=62, grad_norm=0.01025195699185133, loss=0.12831608951091766
I1004 04:14:02.221958 139511610734336 logging_writer.py:48] [63] global_step=63, grad_norm=0.006359683349728584, loss=0.1274084746837616
I1004 04:14:03.057427 139513170990848 logging_writer.py:48] [64] global_step=64, grad_norm=0.010332685895264149, loss=0.1282150000333786
I1004 04:14:03.905161 139511610734336 logging_writer.py:48] [65] global_step=65, grad_norm=0.0053251804783940315, loss=0.12749168276786804
I1004 04:14:04.828744 139513170990848 logging_writer.py:48] [66] global_step=66, grad_norm=0.008885972201824188, loss=0.12900207936763763
I1004 04:14:05.712247 139511610734336 logging_writer.py:48] [67] global_step=67, grad_norm=0.03803849592804909, loss=0.1245477944612503
I1004 04:14:06.474449 139513170990848 logging_writer.py:48] [68] global_step=68, grad_norm=0.0736447349190712, loss=0.1281311959028244
I1004 04:14:07.304620 139511610734336 logging_writer.py:48] [69] global_step=69, grad_norm=0.056586481630802155, loss=0.12852004170417786
I1004 04:14:08.142350 139513170990848 logging_writer.py:48] [70] global_step=70, grad_norm=0.02717103250324726, loss=0.12767261266708374
I1004 04:14:08.831087 139511610734336 logging_writer.py:48] [71] global_step=71, grad_norm=0.026179414242506027, loss=0.12656037509441376
I1004 04:14:09.580797 139513170990848 logging_writer.py:48] [72] global_step=72, grad_norm=0.011881678365170956, loss=0.12617497146129608
I1004 04:14:10.375850 139511610734336 logging_writer.py:48] [73] global_step=73, grad_norm=0.005281940568238497, loss=0.12580153346061707
I1004 04:14:11.380659 139513170990848 logging_writer.py:48] [74] global_step=74, grad_norm=0.012266796082258224, loss=0.12142950296401978
I1004 04:14:11.958896 139511610734336 logging_writer.py:48] [75] global_step=75, grad_norm=0.011784572154283524, loss=0.12331990152597427
I1004 04:14:12.668237 139513170990848 logging_writer.py:48] [76] global_step=76, grad_norm=0.01867467351257801, loss=0.12274740636348724
I1004 04:14:13.403865 139511610734336 logging_writer.py:48] [77] global_step=77, grad_norm=0.01932843029499054, loss=0.12383924424648285
I1004 04:14:14.130135 139513170990848 logging_writer.py:48] [78] global_step=78, grad_norm=0.013212786056101322, loss=0.1265103667974472
I1004 04:14:15.018096 139511610734336 logging_writer.py:48] [79] global_step=79, grad_norm=0.010036547668278217, loss=0.12525887787342072
I1004 04:14:15.748255 139513170990848 logging_writer.py:48] [80] global_step=80, grad_norm=0.01332768052816391, loss=0.12955428659915924
I1004 04:14:16.494642 139511610734336 logging_writer.py:48] [81] global_step=81, grad_norm=0.027605341747403145, loss=0.1313019096851349
I1004 04:14:17.176514 139513170990848 logging_writer.py:48] [82] global_step=82, grad_norm=0.03814131021499634, loss=0.131340891122818
I1004 04:14:18.043192 139511610734336 logging_writer.py:48] [83] global_step=83, grad_norm=0.07380714267492294, loss=0.1283780187368393
I1004 04:14:18.755005 139513170990848 logging_writer.py:48] [84] global_step=84, grad_norm=0.11178696900606155, loss=0.13121400773525238
I1004 04:14:19.432246 139511610734336 logging_writer.py:48] [85] global_step=85, grad_norm=0.1178310215473175, loss=0.13140840828418732
I1004 04:14:20.167475 139513170990848 logging_writer.py:48] [86] global_step=86, grad_norm=0.11512850970029831, loss=0.13282987475395203
I1004 04:14:20.997437 139511610734336 logging_writer.py:48] [87] global_step=87, grad_norm=0.09123463928699493, loss=0.13143843412399292
I1004 04:14:21.792839 139513170990848 logging_writer.py:48] [88] global_step=88, grad_norm=0.05291382968425751, loss=0.13019397854804993
I1004 04:14:22.546182 139511610734336 logging_writer.py:48] [89] global_step=89, grad_norm=0.031168635934591293, loss=0.1290930211544037
I1004 04:14:23.390562 139513170990848 logging_writer.py:48] [90] global_step=90, grad_norm=0.029879294335842133, loss=0.1314762383699417
I1004 04:14:24.148735 139511610734336 logging_writer.py:48] [91] global_step=91, grad_norm=0.019363034516572952, loss=0.1319626122713089
I1004 04:14:24.903275 139513170990848 logging_writer.py:48] [92] global_step=92, grad_norm=0.02089059352874756, loss=0.13270381093025208
I1004 04:14:25.713661 139511610734336 logging_writer.py:48] [93] global_step=93, grad_norm=0.04259104281663895, loss=0.12915192544460297
I1004 04:14:26.505860 139513170990848 logging_writer.py:48] [94] global_step=94, grad_norm=0.05514742434024811, loss=0.13228510320186615
I1004 04:14:27.034913 139511610734336 logging_writer.py:48] [95] global_step=95, grad_norm=0.03591853007674217, loss=0.13651107251644135
I1004 04:14:27.695938 139513170990848 logging_writer.py:48] [96] global_step=96, grad_norm=0.025904780253767967, loss=0.13876010477542877
I1004 04:14:28.269425 139511610734336 logging_writer.py:48] [97] global_step=97, grad_norm=0.024511825293302536, loss=0.14147146046161652
I1004 04:14:29.016119 139513170990848 logging_writer.py:48] [98] global_step=98, grad_norm=0.009152269922196865, loss=0.13870950043201447
I1004 04:14:29.485435 139511610734336 logging_writer.py:48] [99] global_step=99, grad_norm=0.008256077766418457, loss=0.14131280779838562
I1004 04:14:30.260222 139513170990848 logging_writer.py:48] [100] global_step=100, grad_norm=0.015369262546300888, loss=0.1410159170627594
I1004 04:19:44.436313 139511610734336 logging_writer.py:48] [500] global_step=500, grad_norm=0.030833285301923752, loss=0.1276257187128067
I1004 04:26:11.555595 139513170990848 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.005827181972563267, loss=0.1254780888557434
I1004 04:32:41.896218 139511610734336 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.023969165980815887, loss=0.12906835973262787
I1004 04:33:29.995239 139698279835456 spec.py:321] Evaluating on the training split.
I1004 04:36:56.526539 139698279835456 spec.py:333] Evaluating on the validation split.
I1004 04:40:22.382028 139698279835456 spec.py:349] Evaluating on the test split.
I1004 04:44:25.912732 139698279835456 submission_runner.py:381] Time since start: 2679.34s, 	Step: 1563, 	{'train/loss': 0.12403293825545401, 'validation/loss': 0.1255404091404205, 'validation/num_examples': 83274637, 'test/loss': 0.12787607368421053, 'test/num_examples': 95000000, 'score': 1227.5463552474976, 'total_duration': 2679.3441350460052, 'accumulated_submission_time': 1227.5463552474976, 'accumulated_eval_time': 1451.7497844696045, 'accumulated_logging_time': 0.026823997497558594}
I1004 04:44:25.933832 139513170990848 logging_writer.py:48] [1563] accumulated_eval_time=1451.749784, accumulated_logging_time=0.026824, accumulated_submission_time=1227.546355, global_step=1563, preemption_count=0, score=1227.546355, test/loss=0.127876, test/num_examples=95000000, total_duration=2679.344135, train/loss=0.124033, validation/loss=0.125540, validation/num_examples=83274637
I1004 04:49:50.382974 139511610734336 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.012671698816120625, loss=0.118614062666893
I1004 04:56:24.849852 139513170990848 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.004958679899573326, loss=0.11942338943481445
I1004 05:02:59.672364 139511610734336 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.007304232567548752, loss=0.12616537511348724
I1004 05:04:26.330631 139698279835456 spec.py:321] Evaluating on the training split.
I1004 05:07:46.679685 139698279835456 spec.py:333] Evaluating on the validation split.
I1004 05:10:26.810398 139698279835456 spec.py:349] Evaluating on the test split.
I1004 05:13:39.070800 139698279835456 submission_runner.py:381] Time since start: 4432.50s, 	Step: 3112, 	{'train/loss': 0.12314235039477078, 'validation/loss': 0.12459073222979045, 'validation/num_examples': 83274637, 'test/loss': 0.1269571052631579, 'test/num_examples': 95000000, 'score': 2427.9148066043854, 'total_duration': 4432.502191781998, 'accumulated_submission_time': 2427.9148066043854, 'accumulated_eval_time': 2004.4899036884308, 'accumulated_logging_time': 0.05591559410095215}
I1004 05:13:39.088057 139513170990848 logging_writer.py:48] [3112] accumulated_eval_time=2004.489904, accumulated_logging_time=0.055916, accumulated_submission_time=2427.914807, global_step=3112, preemption_count=0, score=2427.914807, test/loss=0.126957, test/num_examples=95000000, total_duration=4432.502192, train/loss=0.123142, validation/loss=0.124591, validation/num_examples=83274637
I1004 05:18:21.989813 139511610734336 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.007240976206958294, loss=0.13041332364082336
I1004 05:24:51.933387 139513170990848 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.003831936279311776, loss=0.11821332573890686
I1004 05:31:19.173880 139511610734336 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.007445964030921459, loss=0.12228716164827347
I1004 05:33:39.504191 139698279835456 spec.py:321] Evaluating on the training split.
I1004 05:36:56.528726 139698279835456 spec.py:333] Evaluating on the validation split.
I1004 05:39:55.455480 139698279835456 spec.py:349] Evaluating on the test split.
I1004 05:43:45.388557 139698279835456 submission_runner.py:381] Time since start: 6238.82s, 	Step: 4677, 	{'train/loss': 0.1209629346739571, 'validation/loss': 0.12415177504766547, 'validation/num_examples': 83274637, 'test/loss': 0.12652415789473684, 'test/num_examples': 95000000, 'score': 3628.299048423767, 'total_duration': 6238.81994342804, 'accumulated_submission_time': 3628.299048423767, 'accumulated_eval_time': 2610.3742232322693, 'accumulated_logging_time': 0.0846400260925293}
I1004 05:43:45.406970 139513170990848 logging_writer.py:48] [4677] accumulated_eval_time=2610.374223, accumulated_logging_time=0.084640, accumulated_submission_time=3628.299048, global_step=4677, preemption_count=0, score=3628.299048, test/loss=0.126524, test/num_examples=95000000, total_duration=6238.819943, train/loss=0.120963, validation/loss=0.124152, validation/num_examples=83274637
I1004 05:47:40.850660 139511610734336 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.008084386587142944, loss=0.1207275390625
I1004 05:54:17.983369 139513170990848 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.004793655592948198, loss=0.1264246702194214
I1004 06:00:52.224161 139511610734336 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.005187840200960636, loss=0.12136810272932053
I1004 06:03:45.444326 139698279835456 spec.py:321] Evaluating on the training split.
I1004 06:06:43.225752 139698279835456 spec.py:333] Evaluating on the validation split.
I1004 06:09:34.250338 139698279835456 spec.py:349] Evaluating on the test split.
I1004 06:13:16.913244 139698279835456 submission_runner.py:381] Time since start: 8010.34s, 	Step: 6217, 	{'train/loss': 0.12261555029911066, 'validation/loss': 0.12384835733357805, 'validation/num_examples': 83274637, 'test/loss': 0.12620305263157894, 'test/num_examples': 95000000, 'score': 4828.303719043732, 'total_duration': 8010.344632387161, 'accumulated_submission_time': 4828.303719043732, 'accumulated_eval_time': 3181.843104839325, 'accumulated_logging_time': 0.11554217338562012}
I1004 06:13:16.929322 139513170990848 logging_writer.py:48] [6217] accumulated_eval_time=3181.843105, accumulated_logging_time=0.115542, accumulated_submission_time=4828.303719, global_step=6217, preemption_count=0, score=4828.303719, test/loss=0.126203, test/num_examples=95000000, total_duration=8010.344632, train/loss=0.122616, validation/loss=0.123848, validation/num_examples=83274637
I1004 06:16:44.314646 139511610734336 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.00422888295724988, loss=0.12236768007278442
I1004 06:23:10.923097 139513170990848 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.003514989512041211, loss=0.12117874622344971
I1004 06:29:41.076136 139511610734336 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.006774686276912689, loss=0.12651261687278748
I1004 06:33:17.156405 139698279835456 spec.py:321] Evaluating on the training split.
I1004 06:35:57.280974 139698279835456 spec.py:333] Evaluating on the validation split.
I1004 06:38:21.174024 139698279835456 spec.py:349] Evaluating on the test split.
I1004 06:40:57.613466 139698279835456 submission_runner.py:381] Time since start: 9671.04s, 	Step: 7772, 	{'train/loss': 0.12159875353927133, 'validation/loss': 0.12375724916099004, 'validation/num_examples': 83274637, 'test/loss': 0.1260564, 'test/num_examples': 95000000, 'score': 6028.4999079704285, 'total_duration': 9671.044870853424, 'accumulated_submission_time': 6028.4999079704285, 'accumulated_eval_time': 3642.3001370429993, 'accumulated_logging_time': 0.14206218719482422}
I1004 06:40:57.634768 139513170990848 logging_writer.py:48] [7772] accumulated_eval_time=3642.300137, accumulated_logging_time=0.142062, accumulated_submission_time=6028.499908, global_step=7772, preemption_count=0, score=6028.499908, test/loss=0.126056, test/num_examples=95000000, total_duration=9671.044871, train/loss=0.121599, validation/loss=0.123757, validation/num_examples=83274637
I1004 06:43:38.291221 139698279835456 spec.py:321] Evaluating on the training split.
I1004 06:45:20.879424 139698279835456 spec.py:333] Evaluating on the validation split.
I1004 06:46:50.983356 139698279835456 spec.py:349] Evaluating on the test split.
I1004 06:48:51.716615 139698279835456 submission_runner.py:381] Time since start: 10145.15s, 	Step: 8000, 	{'train/loss': 0.12176544861223712, 'validation/loss': 0.12378849516930347, 'validation/num_examples': 83274637, 'test/loss': 0.12609822105263158, 'test/num_examples': 95000000, 'score': 6189.145237445831, 'total_duration': 10145.148000240326, 'accumulated_submission_time': 6189.145237445831, 'accumulated_eval_time': 3955.7254769802094, 'accumulated_logging_time': 0.1711864471435547}
I1004 06:48:51.730087 139511610734336 logging_writer.py:48] [8000] accumulated_eval_time=3955.725477, accumulated_logging_time=0.171186, accumulated_submission_time=6189.145237, global_step=8000, preemption_count=0, score=6189.145237, test/loss=0.126098, test/num_examples=95000000, total_duration=10145.148000, train/loss=0.121765, validation/loss=0.123788, validation/num_examples=83274637
I1004 06:48:51.743021 139513170990848 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=6189.145237
I1004 06:48:57.576707 139698279835456 checkpoints.py:490] Saving checkpoint at step: 8000
I1004 06:49:30.403051 139698279835456 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_3/criteo1tb_jax/trial_1/checkpoint_8000
I1004 06:49:30.696925 139698279835456 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_3/criteo1tb_jax/trial_1/checkpoint_8000.
I1004 06:49:31.096022 139698279835456 submission_runner.py:549] Tuning trial 1/1
I1004 06:49:31.096336 139698279835456 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1004 06:49:31.098072 139698279835456 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.3022634158344389, 'validation/loss': 0.30157506420592384, 'validation/num_examples': 83274637, 'test/loss': 0.30287703157894735, 'test/num_examples': 95000000, 'score': 27.345337390899658, 'total_duration': 823.1777153015137, 'accumulated_submission_time': 27.345337390899658, 'accumulated_eval_time': 795.8323402404785, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1563, {'train/loss': 0.12403293825545401, 'validation/loss': 0.1255404091404205, 'validation/num_examples': 83274637, 'test/loss': 0.12787607368421053, 'test/num_examples': 95000000, 'score': 1227.5463552474976, 'total_duration': 2679.3441350460052, 'accumulated_submission_time': 1227.5463552474976, 'accumulated_eval_time': 1451.7497844696045, 'accumulated_logging_time': 0.026823997497558594, 'global_step': 1563, 'preemption_count': 0}), (3112, {'train/loss': 0.12314235039477078, 'validation/loss': 0.12459073222979045, 'validation/num_examples': 83274637, 'test/loss': 0.1269571052631579, 'test/num_examples': 95000000, 'score': 2427.9148066043854, 'total_duration': 4432.502191781998, 'accumulated_submission_time': 2427.9148066043854, 'accumulated_eval_time': 2004.4899036884308, 'accumulated_logging_time': 0.05591559410095215, 'global_step': 3112, 'preemption_count': 0}), (4677, {'train/loss': 0.1209629346739571, 'validation/loss': 0.12415177504766547, 'validation/num_examples': 83274637, 'test/loss': 0.12652415789473684, 'test/num_examples': 95000000, 'score': 3628.299048423767, 'total_duration': 6238.81994342804, 'accumulated_submission_time': 3628.299048423767, 'accumulated_eval_time': 2610.3742232322693, 'accumulated_logging_time': 0.0846400260925293, 'global_step': 4677, 'preemption_count': 0}), (6217, {'train/loss': 0.12261555029911066, 'validation/loss': 0.12384835733357805, 'validation/num_examples': 83274637, 'test/loss': 0.12620305263157894, 'test/num_examples': 95000000, 'score': 4828.303719043732, 'total_duration': 8010.344632387161, 'accumulated_submission_time': 4828.303719043732, 'accumulated_eval_time': 3181.843104839325, 'accumulated_logging_time': 0.11554217338562012, 'global_step': 6217, 'preemption_count': 0}), (7772, {'train/loss': 0.12159875353927133, 'validation/loss': 0.12375724916099004, 'validation/num_examples': 83274637, 'test/loss': 0.1260564, 'test/num_examples': 95000000, 'score': 6028.4999079704285, 'total_duration': 9671.044870853424, 'accumulated_submission_time': 6028.4999079704285, 'accumulated_eval_time': 3642.3001370429993, 'accumulated_logging_time': 0.14206218719482422, 'global_step': 7772, 'preemption_count': 0}), (8000, {'train/loss': 0.12176544861223712, 'validation/loss': 0.12378849516930347, 'validation/num_examples': 83274637, 'test/loss': 0.12609822105263158, 'test/num_examples': 95000000, 'score': 6189.145237445831, 'total_duration': 10145.148000240326, 'accumulated_submission_time': 6189.145237445831, 'accumulated_eval_time': 3955.7254769802094, 'accumulated_logging_time': 0.1711864471435547, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1004 06:49:31.098299 139698279835456 submission_runner.py:552] Timing: 6189.145237445831
I1004 06:49:31.098377 139698279835456 submission_runner.py:554] Total number of evals: 7
I1004 06:49:31.098440 139698279835456 submission_runner.py:555] ====================
I1004 06:49:31.098585 139698279835456 submission_runner.py:625] Final criteo1tb score: 6189.145237445831
