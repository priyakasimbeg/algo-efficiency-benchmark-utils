python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_8 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-04-2023-17-36-32.log
2023-10-04 17:36:37.680651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1004 17:36:54.281324 140070686779200 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_8/criteo1tb_jax.
I1004 17:36:55.913707 140070686779200 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1004 17:36:55.914427 140070686779200 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1004 17:36:55.914558 140070686779200 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1004 17:36:55.920347 140070686779200 submission_runner.py:507] Using RNG seed 2096080066
I1004 17:37:01.586510 140070686779200 submission_runner.py:516] --- Tuning run 1/1 ---
I1004 17:37:01.586713 140070686779200 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_8/criteo1tb_jax/trial_1.
I1004 17:37:01.587001 140070686779200 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_8/criteo1tb_jax/trial_1/hparams.json.
I1004 17:37:01.767271 140070686779200 submission_runner.py:191] Initializing dataset.
I1004 17:37:01.767531 140070686779200 submission_runner.py:198] Initializing model.
I1004 17:37:07.483552 140070686779200 submission_runner.py:232] Initializing optimizer.
I1004 17:37:10.614361 140070686779200 submission_runner.py:239] Initializing metrics bundle.
I1004 17:37:10.614578 140070686779200 submission_runner.py:257] Initializing checkpoint and logger.
I1004 17:37:10.615708 140070686779200 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_8/criteo1tb_jax/trial_1 with prefix checkpoint_
I1004 17:37:10.615855 140070686779200 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_8/criteo1tb_jax/trial_1/meta_data_0.json.
I1004 17:37:10.616057 140070686779200 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1004 17:37:10.616120 140070686779200 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1004 17:37:11.455296 140070686779200 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_8/criteo1tb_jax/trial_1/flags_0.json.
I1004 17:37:11.541965 140070686779200 submission_runner.py:290] Starting training loop.
I1004 17:37:38.728926 139906572609280 logging_writer.py:48] [0] global_step=0, grad_norm=8.516694068908691, loss=1.2685973644256592
I1004 17:37:38.739294 140070686779200 spec.py:321] Evaluating on the training split.
I1004 17:41:27.154655 140070686779200 spec.py:333] Evaluating on the validation split.
I1004 17:45:24.202363 140070686779200 spec.py:349] Evaluating on the test split.
I1004 17:49:46.044065 140070686779200 submission_runner.py:381] Time since start: 754.50s, 	Step: 1, 	{'train/loss': 1.2716726626989976, 'validation/loss': 1.2701357797572868, 'validation/num_examples': 83274637, 'test/loss': 1.270348547368421, 'test/num_examples': 95000000, 'score': 27.197319269180298, 'total_duration': 754.5020382404327, 'accumulated_submission_time': 27.197319269180298, 'accumulated_eval_time': 727.3046836853027, 'accumulated_logging_time': 0}
I1004 17:49:46.064271 139884702447360 logging_writer.py:48] [1] accumulated_eval_time=727.304684, accumulated_logging_time=0, accumulated_submission_time=27.197319, global_step=1, preemption_count=0, score=27.197319, test/loss=1.270349, test/num_examples=95000000, total_duration=754.502038, train/loss=1.271673, validation/loss=1.270136, validation/num_examples=83274637
I1004 17:49:46.178734 139884694054656 logging_writer.py:48] [1] global_step=1, grad_norm=8.482047080993652, loss=1.2685370445251465
I1004 17:49:46.286619 139884702447360 logging_writer.py:48] [2] global_step=2, grad_norm=7.76570463180542, loss=1.1730384826660156
I1004 17:49:46.391359 139884694054656 logging_writer.py:48] [3] global_step=3, grad_norm=7.047117710113525, loss=1.0199846029281616
I1004 17:49:46.496158 139884702447360 logging_writer.py:48] [4] global_step=4, grad_norm=5.797060966491699, loss=0.8468168377876282
I1004 17:49:46.599863 139884694054656 logging_writer.py:48] [5] global_step=5, grad_norm=4.951369285583496, loss=0.6828144192695618
I1004 17:49:46.702942 139884702447360 logging_writer.py:48] [6] global_step=6, grad_norm=4.564065456390381, loss=0.5205845832824707
I1004 17:49:46.807304 139884694054656 logging_writer.py:48] [7] global_step=7, grad_norm=3.429323196411133, loss=0.366265207529068
I1004 17:49:46.914383 139884702447360 logging_writer.py:48] [8] global_step=8, grad_norm=1.9167513847351074, loss=0.2569258511066437
I1004 17:49:47.018736 139884694054656 logging_writer.py:48] [9] global_step=9, grad_norm=0.5600413084030151, loss=0.20603889226913452
I1004 17:49:47.125214 139884702447360 logging_writer.py:48] [10] global_step=10, grad_norm=0.5041507482528687, loss=0.20207364857196808
I1004 17:49:47.229800 139884694054656 logging_writer.py:48] [11] global_step=11, grad_norm=1.0452998876571655, loss=0.2201874703168869
I1004 17:49:47.332737 139884702447360 logging_writer.py:48] [12] global_step=12, grad_norm=1.4311059713363647, loss=0.24954164028167725
I1004 17:49:47.436747 139884694054656 logging_writer.py:48] [13] global_step=13, grad_norm=1.7386759519577026, loss=0.28475478291511536
I1004 17:49:47.541485 139884702447360 logging_writer.py:48] [14] global_step=14, grad_norm=1.8874279260635376, loss=0.30397042632102966
I1004 17:49:47.648124 139884694054656 logging_writer.py:48] [15] global_step=15, grad_norm=1.9586355686187744, loss=0.3147217333316803
I1004 17:49:47.752703 139884702447360 logging_writer.py:48] [16] global_step=16, grad_norm=2.030547618865967, loss=0.3233147859573364
I1004 17:49:47.856238 139884694054656 logging_writer.py:48] [17] global_step=17, grad_norm=1.919405221939087, loss=0.3062037527561188
I1004 17:49:47.960980 139884702447360 logging_writer.py:48] [18] global_step=18, grad_norm=1.8120979070663452, loss=0.2875024080276489
I1004 17:49:48.067927 139884694054656 logging_writer.py:48] [19] global_step=19, grad_norm=1.555517315864563, loss=0.24982157349586487
I1004 17:49:48.173314 139884702447360 logging_writer.py:48] [20] global_step=20, grad_norm=1.3525573015213013, loss=0.22630701959133148
I1004 17:49:48.277516 139884694054656 logging_writer.py:48] [21] global_step=21, grad_norm=0.9897863864898682, loss=0.1943291425704956
I1004 17:49:48.382989 139884702447360 logging_writer.py:48] [22] global_step=22, grad_norm=0.5147824883460999, loss=0.17138315737247467
I1004 17:49:48.487204 139884694054656 logging_writer.py:48] [23] global_step=23, grad_norm=0.1996549814939499, loss=0.16513611376285553
I1004 17:49:48.590857 139884702447360 logging_writer.py:48] [24] global_step=24, grad_norm=0.6358353495597839, loss=0.16522881388664246
I1004 17:49:48.695350 139884694054656 logging_writer.py:48] [25] global_step=25, grad_norm=0.6336565017700195, loss=0.16444948315620422
I1004 17:49:48.799472 139884702447360 logging_writer.py:48] [26] global_step=26, grad_norm=0.335645467042923, loss=0.15615929663181305
I1004 17:49:48.903445 139884694054656 logging_writer.py:48] [27] global_step=27, grad_norm=0.11799350380897522, loss=0.15601107478141785
I1004 17:49:49.010880 139884702447360 logging_writer.py:48] [28] global_step=28, grad_norm=0.24141523241996765, loss=0.1524861454963684
I1004 17:49:49.612586 139884694054656 logging_writer.py:48] [29] global_step=29, grad_norm=0.22888241708278656, loss=0.15047764778137207
I1004 17:49:50.359494 139884702447360 logging_writer.py:48] [30] global_step=30, grad_norm=0.12034337967634201, loss=0.14922015368938446
I1004 17:49:51.079065 139884694054656 logging_writer.py:48] [31] global_step=31, grad_norm=0.03938382863998413, loss=0.1451980173587799
I1004 17:49:51.775936 139884702447360 logging_writer.py:48] [32] global_step=32, grad_norm=0.07526888698339462, loss=0.14339879155158997
I1004 17:49:52.516925 139884694054656 logging_writer.py:48] [33] global_step=33, grad_norm=0.05345168337225914, loss=0.1444467008113861
I1004 17:49:53.293767 139884702447360 logging_writer.py:48] [34] global_step=34, grad_norm=0.05538893863558769, loss=0.14731188118457794
I1004 17:49:54.009834 139884694054656 logging_writer.py:48] [35] global_step=35, grad_norm=0.11249826103448868, loss=0.14241282641887665
I1004 17:49:54.728573 139884702447360 logging_writer.py:48] [36] global_step=36, grad_norm=0.29656168818473816, loss=0.1444476693868637
I1004 17:49:55.531108 139884694054656 logging_writer.py:48] [37] global_step=37, grad_norm=0.6885164976119995, loss=0.14652542769908905
I1004 17:49:56.357278 139884702447360 logging_writer.py:48] [38] global_step=38, grad_norm=0.45708194375038147, loss=0.14010871946811676
I1004 17:49:57.090139 139884694054656 logging_writer.py:48] [39] global_step=39, grad_norm=0.10912927240133286, loss=0.13213838636875153
I1004 17:49:57.770318 139884702447360 logging_writer.py:48] [40] global_step=40, grad_norm=0.028350695967674255, loss=0.1346999704837799
I1004 17:49:58.673680 139884694054656 logging_writer.py:48] [41] global_step=41, grad_norm=0.04100111126899719, loss=0.13256515562534332
I1004 17:49:59.299318 139884702447360 logging_writer.py:48] [42] global_step=42, grad_norm=0.059690218418836594, loss=0.13192889094352722
I1004 17:50:00.102928 139884694054656 logging_writer.py:48] [43] global_step=43, grad_norm=0.11861346662044525, loss=0.13167648017406464
I1004 17:50:00.838905 139884702447360 logging_writer.py:48] [44] global_step=44, grad_norm=0.10981114208698273, loss=0.12983472645282745
I1004 17:50:01.607992 139884694054656 logging_writer.py:48] [45] global_step=45, grad_norm=0.09786106646060944, loss=0.13022348284721375
I1004 17:50:02.470367 139884702447360 logging_writer.py:48] [46] global_step=46, grad_norm=0.07172266393899918, loss=0.1277344971895218
I1004 17:50:03.198778 139884694054656 logging_writer.py:48] [47] global_step=47, grad_norm=0.0493052676320076, loss=0.1279723048210144
I1004 17:50:04.006027 139884702447360 logging_writer.py:48] [48] global_step=48, grad_norm=0.051804907619953156, loss=0.12716367840766907
I1004 17:50:04.752466 139884694054656 logging_writer.py:48] [49] global_step=49, grad_norm=0.0402718260884285, loss=0.12978920340538025
I1004 17:50:05.421647 139884702447360 logging_writer.py:48] [50] global_step=50, grad_norm=0.01870042271912098, loss=0.12662935256958008
I1004 17:50:06.282823 139884694054656 logging_writer.py:48] [51] global_step=51, grad_norm=0.027658239006996155, loss=0.12767736613750458
I1004 17:50:06.962985 139884702447360 logging_writer.py:48] [52] global_step=52, grad_norm=0.04808519408106804, loss=0.12782955169677734
I1004 17:50:07.633431 139884694054656 logging_writer.py:48] [53] global_step=53, grad_norm=0.16489149630069733, loss=0.12642347812652588
I1004 17:50:08.349611 139884702447360 logging_writer.py:48] [54] global_step=54, grad_norm=0.3063287138938904, loss=0.12684468924999237
I1004 17:50:09.122505 139884694054656 logging_writer.py:48] [55] global_step=55, grad_norm=0.6329935789108276, loss=0.12917755544185638
I1004 17:50:09.851594 139884702447360 logging_writer.py:48] [56] global_step=56, grad_norm=0.6594139933586121, loss=0.13675183057785034
I1004 17:50:10.659893 139884694054656 logging_writer.py:48] [57] global_step=57, grad_norm=0.23294171690940857, loss=0.12625983357429504
I1004 17:50:11.469130 139884702447360 logging_writer.py:48] [58] global_step=58, grad_norm=0.042715903371572495, loss=0.12565100193023682
I1004 17:50:12.111271 139884694054656 logging_writer.py:48] [59] global_step=59, grad_norm=0.08700110763311386, loss=0.12372268736362457
I1004 17:50:12.885080 139884702447360 logging_writer.py:48] [60] global_step=60, grad_norm=0.06442253291606903, loss=0.1265963762998581
I1004 17:50:13.641164 139884694054656 logging_writer.py:48] [61] global_step=61, grad_norm=0.045359570533037186, loss=0.12598180770874023
I1004 17:50:14.499775 139884702447360 logging_writer.py:48] [62] global_step=62, grad_norm=0.03540881723165512, loss=0.1256377249956131
I1004 17:50:15.184163 139884694054656 logging_writer.py:48] [63] global_step=63, grad_norm=0.027035821229219437, loss=0.1247541680932045
I1004 17:50:15.948298 139884702447360 logging_writer.py:48] [64] global_step=64, grad_norm=0.018166152760386467, loss=0.12354430556297302
I1004 17:50:16.661838 139884694054656 logging_writer.py:48] [65] global_step=65, grad_norm=0.01849382370710373, loss=0.12526820600032806
I1004 17:50:17.340298 139884702447360 logging_writer.py:48] [66] global_step=66, grad_norm=0.029793081805109978, loss=0.12417060136795044
I1004 17:50:17.909942 139884694054656 logging_writer.py:48] [67] global_step=67, grad_norm=0.05579549819231033, loss=0.12433752417564392
I1004 17:50:18.775570 139884702447360 logging_writer.py:48] [68] global_step=68, grad_norm=0.08742934465408325, loss=0.12381307780742645
I1004 17:50:19.467356 139884694054656 logging_writer.py:48] [69] global_step=69, grad_norm=0.1141129806637764, loss=0.12441440671682358
I1004 17:50:20.037207 139884702447360 logging_writer.py:48] [70] global_step=70, grad_norm=0.15244846045970917, loss=0.1242702454328537
I1004 17:50:20.747550 139884694054656 logging_writer.py:48] [71] global_step=71, grad_norm=0.18482662737369537, loss=0.12694081664085388
I1004 17:50:21.513833 139884702447360 logging_writer.py:48] [72] global_step=72, grad_norm=0.24065342545509338, loss=0.12478026002645493
I1004 17:50:22.174506 139884694054656 logging_writer.py:48] [73] global_step=73, grad_norm=0.2579345107078552, loss=0.12579455971717834
I1004 17:50:22.846782 139884702447360 logging_writer.py:48] [74] global_step=74, grad_norm=0.24950051307678223, loss=0.12606707215309143
I1004 17:50:23.551479 139884694054656 logging_writer.py:48] [75] global_step=75, grad_norm=0.23276746273040771, loss=0.1251407265663147
I1004 17:50:24.118088 139884702447360 logging_writer.py:48] [76] global_step=76, grad_norm=0.18763911724090576, loss=0.1355324387550354
I1004 17:50:24.750164 139884694054656 logging_writer.py:48] [77] global_step=77, grad_norm=0.18042784929275513, loss=0.13973647356033325
I1004 17:50:25.493012 139884702447360 logging_writer.py:48] [78] global_step=78, grad_norm=0.21458294987678528, loss=0.13979379832744598
I1004 17:50:26.178076 139884694054656 logging_writer.py:48] [79] global_step=79, grad_norm=0.2089209407567978, loss=0.13806958496570587
I1004 17:50:26.742381 139884702447360 logging_writer.py:48] [80] global_step=80, grad_norm=0.1923597902059555, loss=0.13812106847763062
I1004 17:50:27.593940 139884694054656 logging_writer.py:48] [81] global_step=81, grad_norm=0.1922825574874878, loss=0.13860076665878296
I1004 17:50:28.104050 139884702447360 logging_writer.py:48] [82] global_step=82, grad_norm=0.1773526519536972, loss=0.1390896737575531
I1004 17:50:28.796896 139884694054656 logging_writer.py:48] [83] global_step=83, grad_norm=0.16481025516986847, loss=0.13803020119667053
I1004 17:50:29.523322 139884702447360 logging_writer.py:48] [84] global_step=84, grad_norm=0.17224779725074768, loss=0.13607217371463776
I1004 17:50:30.231657 139884694054656 logging_writer.py:48] [85] global_step=85, grad_norm=0.1741684377193451, loss=0.13599097728729248
I1004 17:50:30.842566 139884702447360 logging_writer.py:48] [86] global_step=86, grad_norm=0.16006246209144592, loss=0.13517269492149353
I1004 17:50:31.595770 139884694054656 logging_writer.py:48] [87] global_step=87, grad_norm=0.14439739286899567, loss=0.13508833944797516
I1004 17:50:32.216706 139884702447360 logging_writer.py:48] [88] global_step=88, grad_norm=0.12615324556827545, loss=0.13429361581802368
I1004 17:50:32.863511 139884694054656 logging_writer.py:48] [89] global_step=89, grad_norm=0.117105633020401, loss=0.13535919785499573
I1004 17:50:33.675874 139884702447360 logging_writer.py:48] [90] global_step=90, grad_norm=0.12683317065238953, loss=0.1344313770532608
I1004 17:50:34.404393 139884694054656 logging_writer.py:48] [91] global_step=91, grad_norm=0.1409255564212799, loss=0.135346457362175
I1004 17:50:35.027066 139884702447360 logging_writer.py:48] [92] global_step=92, grad_norm=0.17485272884368896, loss=0.13494709134101868
I1004 17:50:35.747137 139884694054656 logging_writer.py:48] [93] global_step=93, grad_norm=0.19782371819019318, loss=0.13382241129875183
I1004 17:50:36.480491 139884702447360 logging_writer.py:48] [94] global_step=94, grad_norm=0.18589283525943756, loss=0.13608689606189728
I1004 17:50:37.229588 139884694054656 logging_writer.py:48] [95] global_step=95, grad_norm=0.15259121358394623, loss=0.13217060267925262
I1004 17:50:38.032260 139884702447360 logging_writer.py:48] [96] global_step=96, grad_norm=0.15312737226486206, loss=0.1291569471359253
I1004 17:50:38.671202 139884694054656 logging_writer.py:48] [97] global_step=97, grad_norm=0.1341376155614853, loss=0.12717871367931366
I1004 17:50:39.426253 139884702447360 logging_writer.py:48] [98] global_step=98, grad_norm=0.09245023131370544, loss=0.12882670760154724
I1004 17:50:40.117909 139884694054656 logging_writer.py:48] [99] global_step=99, grad_norm=0.05686965212225914, loss=0.12634322047233582
I1004 17:50:40.902413 139884702447360 logging_writer.py:48] [100] global_step=100, grad_norm=0.01446702890098095, loss=0.1290155053138733
I1004 17:55:23.867447 139884694054656 logging_writer.py:48] [500] global_step=500, grad_norm=0.015663055703043938, loss=0.12931731343269348
I1004 18:01:13.547154 139884702447360 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.020449863746762276, loss=0.12220633029937744
I1004 18:07:05.252812 139884694054656 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.006646628957241774, loss=0.12223200500011444
I1004 18:09:46.739620 140070686779200 spec.py:321] Evaluating on the training split.
I1004 18:12:51.978457 140070686779200 spec.py:333] Evaluating on the validation split.
I1004 18:15:42.648451 140070686779200 spec.py:349] Evaluating on the test split.
I1004 18:18:48.241555 140070686779200 submission_runner.py:381] Time since start: 2496.70s, 	Step: 1728, 	{'train/loss': 0.12414301266460298, 'validation/loss': 0.12525223015982645, 'validation/num_examples': 83274637, 'test/loss': 0.1275699052631579, 'test/num_examples': 95000000, 'score': 1227.8405332565308, 'total_duration': 2496.699513196945, 'accumulated_submission_time': 1227.8405332565308, 'accumulated_eval_time': 1268.806548833847, 'accumulated_logging_time': 0.028219938278198242}
I1004 18:18:48.257507 139884702447360 logging_writer.py:48] [1728] accumulated_eval_time=1268.806549, accumulated_logging_time=0.028220, accumulated_submission_time=1227.840533, global_step=1728, preemption_count=0, score=1227.840533, test/loss=0.127570, test/num_examples=95000000, total_duration=2496.699513, train/loss=0.124143, validation/loss=0.125252, validation/num_examples=83274637
I1004 18:21:43.432562 139884694054656 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.01801910810172558, loss=0.1248273104429245
I1004 18:27:43.319147 139884702447360 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.013378093019127846, loss=0.11715836822986603
I1004 18:33:36.204218 139884694054656 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.005499149207025766, loss=0.11766356229782104
I1004 18:38:49.015359 140070686779200 spec.py:321] Evaluating on the training split.
I1004 18:41:48.065566 140070686779200 spec.py:333] Evaluating on the validation split.
I1004 18:44:51.570789 140070686779200 spec.py:349] Evaluating on the test split.
I1004 18:48:17.459417 140070686779200 submission_runner.py:381] Time since start: 4265.92s, 	Step: 3443, 	{'train/loss': 0.12274535796927206, 'validation/loss': 0.12444966887096728, 'validation/num_examples': 83274637, 'test/loss': 0.1268665157894737, 'test/num_examples': 95000000, 'score': 2428.567491531372, 'total_duration': 4265.917397737503, 'accumulated_submission_time': 2428.567491531372, 'accumulated_eval_time': 1837.250565290451, 'accumulated_logging_time': 0.05152249336242676}
I1004 18:48:17.476068 139884702447360 logging_writer.py:48] [3443] accumulated_eval_time=1837.250565, accumulated_logging_time=0.051522, accumulated_submission_time=2428.567492, global_step=3443, preemption_count=0, score=2428.567492, test/loss=0.126867, test/num_examples=95000000, total_duration=4265.917398, train/loss=0.122745, validation/loss=0.124450, validation/num_examples=83274637
I1004 18:48:42.731169 139884694054656 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.014330108650028706, loss=0.12904411554336548
I1004 18:54:34.459569 139884702447360 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.014329065568745136, loss=0.12983396649360657
I1004 19:00:27.659549 139884694054656 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.013805817812681198, loss=0.1217925101518631
I1004 19:06:23.419476 139884702447360 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.009951230138540268, loss=0.12536150217056274
I1004 19:08:17.901208 140070686779200 spec.py:321] Evaluating on the training split.
I1004 19:11:18.681012 140070686779200 spec.py:333] Evaluating on the validation split.
I1004 19:14:19.015163 140070686779200 spec.py:349] Evaluating on the test split.
I1004 19:17:49.904083 140070686779200 submission_runner.py:381] Time since start: 6038.36s, 	Step: 5162, 	{'train/loss': 0.12320837584681481, 'validation/loss': 0.12428281134386691, 'validation/num_examples': 83274637, 'test/loss': 0.126584, 'test/num_examples': 95000000, 'score': 3628.9602406024933, 'total_duration': 6038.3620755672455, 'accumulated_submission_time': 3628.9602406024933, 'accumulated_eval_time': 2409.253444671631, 'accumulated_logging_time': 0.07707953453063965}
I1004 19:17:49.923658 139884694054656 logging_writer.py:48] [5162] accumulated_eval_time=2409.253445, accumulated_logging_time=0.077080, accumulated_submission_time=3628.960241, global_step=5162, preemption_count=0, score=3628.960241, test/loss=0.126584, test/num_examples=95000000, total_duration=6038.362076, train/loss=0.123208, validation/loss=0.124283, validation/num_examples=83274637
I1004 19:21:37.406841 139884702447360 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0053321016021072865, loss=0.12258436530828476
I1004 19:27:32.041464 139884694054656 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.005907086655497551, loss=0.11727015674114227
I1004 19:33:26.706161 139884702447360 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.004212948959320784, loss=0.12042924016714096
I1004 19:37:49.999979 140070686779200 spec.py:321] Evaluating on the training split.
I1004 19:40:29.651905 140070686779200 spec.py:333] Evaluating on the validation split.
I1004 19:43:20.154212 140070686779200 spec.py:349] Evaluating on the test split.
I1004 19:46:35.315456 140070686779200 submission_runner.py:381] Time since start: 7763.77s, 	Step: 6875, 	{'train/loss': 0.12316163980735922, 'validation/loss': 0.12382723445555217, 'validation/num_examples': 83274637, 'test/loss': 0.12616613684210526, 'test/num_examples': 95000000, 'score': 4829.005361318588, 'total_duration': 7763.773419380188, 'accumulated_submission_time': 4829.005361318588, 'accumulated_eval_time': 2934.568858861923, 'accumulated_logging_time': 0.10419487953186035}
I1004 19:46:35.332477 139884694054656 logging_writer.py:48] [6875] accumulated_eval_time=2934.568859, accumulated_logging_time=0.104195, accumulated_submission_time=4829.005361, global_step=6875, preemption_count=0, score=4829.005361, test/loss=0.126166, test/num_examples=95000000, total_duration=7763.773419, train/loss=0.123162, validation/loss=0.123827, validation/num_examples=83274637
I1004 19:47:50.184660 139884702447360 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.015061000362038612, loss=0.12400210648775101
I1004 19:53:41.177949 139884694054656 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.019068585708737373, loss=0.11457778513431549
I1004 19:59:37.556909 140070686779200 spec.py:321] Evaluating on the training split.
I1004 20:01:59.034046 140070686779200 spec.py:333] Evaluating on the validation split.
I1004 20:03:54.167977 140070686779200 spec.py:349] Evaluating on the test split.
I1004 20:06:41.887653 140070686779200 submission_runner.py:381] Time since start: 8970.35s, 	Step: 8000, 	{'train/loss': 0.12218886801281816, 'validation/loss': 0.12377125102328576, 'validation/num_examples': 83274637, 'test/loss': 0.12607583157894736, 'test/num_examples': 95000000, 'score': 5611.205914258957, 'total_duration': 8970.345626592636, 'accumulated_submission_time': 5611.205914258957, 'accumulated_eval_time': 3358.899565935135, 'accumulated_logging_time': 0.12925028800964355}
I1004 20:06:41.904347 139884702447360 logging_writer.py:48] [8000] accumulated_eval_time=3358.899566, accumulated_logging_time=0.129250, accumulated_submission_time=5611.205914, global_step=8000, preemption_count=0, score=5611.205914, test/loss=0.126076, test/num_examples=95000000, total_duration=8970.345627, train/loss=0.122189, validation/loss=0.123771, validation/num_examples=83274637
I1004 20:06:41.921523 139884694054656 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5611.205914
I1004 20:06:47.658860 140070686779200 checkpoints.py:490] Saving checkpoint at step: 8000
I1004 20:07:22.905216 140070686779200 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_8/criteo1tb_jax/trial_1/checkpoint_8000
I1004 20:07:23.195267 140070686779200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_8/criteo1tb_jax/trial_1/checkpoint_8000.
I1004 20:07:23.558991 140070686779200 submission_runner.py:549] Tuning trial 1/1
I1004 20:07:23.559248 140070686779200 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1004 20:07:23.560158 140070686779200 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 1.2716726626989976, 'validation/loss': 1.2701357797572868, 'validation/num_examples': 83274637, 'test/loss': 1.270348547368421, 'test/num_examples': 95000000, 'score': 27.197319269180298, 'total_duration': 754.5020382404327, 'accumulated_submission_time': 27.197319269180298, 'accumulated_eval_time': 727.3046836853027, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1728, {'train/loss': 0.12414301266460298, 'validation/loss': 0.12525223015982645, 'validation/num_examples': 83274637, 'test/loss': 0.1275699052631579, 'test/num_examples': 95000000, 'score': 1227.8405332565308, 'total_duration': 2496.699513196945, 'accumulated_submission_time': 1227.8405332565308, 'accumulated_eval_time': 1268.806548833847, 'accumulated_logging_time': 0.028219938278198242, 'global_step': 1728, 'preemption_count': 0}), (3443, {'train/loss': 0.12274535796927206, 'validation/loss': 0.12444966887096728, 'validation/num_examples': 83274637, 'test/loss': 0.1268665157894737, 'test/num_examples': 95000000, 'score': 2428.567491531372, 'total_duration': 4265.917397737503, 'accumulated_submission_time': 2428.567491531372, 'accumulated_eval_time': 1837.250565290451, 'accumulated_logging_time': 0.05152249336242676, 'global_step': 3443, 'preemption_count': 0}), (5162, {'train/loss': 0.12320837584681481, 'validation/loss': 0.12428281134386691, 'validation/num_examples': 83274637, 'test/loss': 0.126584, 'test/num_examples': 95000000, 'score': 3628.9602406024933, 'total_duration': 6038.3620755672455, 'accumulated_submission_time': 3628.9602406024933, 'accumulated_eval_time': 2409.253444671631, 'accumulated_logging_time': 0.07707953453063965, 'global_step': 5162, 'preemption_count': 0}), (6875, {'train/loss': 0.12316163980735922, 'validation/loss': 0.12382723445555217, 'validation/num_examples': 83274637, 'test/loss': 0.12616613684210526, 'test/num_examples': 95000000, 'score': 4829.005361318588, 'total_duration': 7763.773419380188, 'accumulated_submission_time': 4829.005361318588, 'accumulated_eval_time': 2934.568858861923, 'accumulated_logging_time': 0.10419487953186035, 'global_step': 6875, 'preemption_count': 0}), (8000, {'train/loss': 0.12218886801281816, 'validation/loss': 0.12377125102328576, 'validation/num_examples': 83274637, 'test/loss': 0.12607583157894736, 'test/num_examples': 95000000, 'score': 5611.205914258957, 'total_duration': 8970.345626592636, 'accumulated_submission_time': 5611.205914258957, 'accumulated_eval_time': 3358.899565935135, 'accumulated_logging_time': 0.12925028800964355, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1004 20:07:23.560268 140070686779200 submission_runner.py:552] Timing: 5611.205914258957
I1004 20:07:23.560324 140070686779200 submission_runner.py:554] Total number of evals: 6
I1004 20:07:23.560372 140070686779200 submission_runner.py:555] ====================
I1004 20:07:23.560477 140070686779200 submission_runner.py:625] Final criteo1tb score: 5611.205914258957
