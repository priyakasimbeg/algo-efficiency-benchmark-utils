python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_7 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-04-2023-14-41-03.log
2023-10-04 14:41:08.429587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1004 14:41:25.547850 140235179677504 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_7/criteo1tb_jax.
I1004 14:41:27.229073 140235179677504 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1004 14:41:27.230133 140235179677504 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1004 14:41:27.230301 140235179677504 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1004 14:41:27.235949 140235179677504 submission_runner.py:507] Using RNG seed 194695665
I1004 14:41:33.042347 140235179677504 submission_runner.py:516] --- Tuning run 1/1 ---
I1004 14:41:33.042647 140235179677504 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_7/criteo1tb_jax/trial_1.
I1004 14:41:33.042998 140235179677504 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_7/criteo1tb_jax/trial_1/hparams.json.
I1004 14:41:33.235918 140235179677504 submission_runner.py:191] Initializing dataset.
I1004 14:41:33.236241 140235179677504 submission_runner.py:198] Initializing model.
I1004 14:41:39.259255 140235179677504 submission_runner.py:232] Initializing optimizer.
I1004 14:41:42.449991 140235179677504 submission_runner.py:239] Initializing metrics bundle.
I1004 14:41:42.450311 140235179677504 submission_runner.py:257] Initializing checkpoint and logger.
I1004 14:41:42.451713 140235179677504 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_7/criteo1tb_jax/trial_1 with prefix checkpoint_
I1004 14:41:42.451904 140235179677504 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_7/criteo1tb_jax/trial_1/meta_data_0.json.
I1004 14:41:42.452154 140235179677504 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1004 14:41:42.452225 140235179677504 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1004 14:41:43.342457 140235179677504 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_7/criteo1tb_jax/trial_1/flags_0.json.
I1004 14:41:43.444269 140235179677504 submission_runner.py:290] Starting training loop.
I1004 14:42:09.400403 140071115151104 logging_writer.py:48] [0] global_step=0, grad_norm=5.530930042266846, loss=0.5009625554084778
I1004 14:42:09.411805 140235179677504 spec.py:321] Evaluating on the training split.
I1004 14:46:21.367812 140235179677504 spec.py:333] Evaluating on the validation split.
I1004 14:50:38.327863 140235179677504 spec.py:349] Evaluating on the test split.
I1004 14:55:26.219093 140235179677504 submission_runner.py:381] Time since start: 822.77s, 	Step: 1, 	{'train/loss': 0.500283679122445, 'validation/loss': 0.5051477558527214, 'validation/num_examples': 83274637, 'test/loss': 0.5026625684210526, 'test/num_examples': 95000000, 'score': 25.96751356124878, 'total_duration': 822.7747693061829, 'accumulated_submission_time': 25.96751356124878, 'accumulated_eval_time': 796.8072137832642, 'accumulated_logging_time': 0}
I1004 14:55:26.239980 140053447702272 logging_writer.py:48] [1] accumulated_eval_time=796.807214, accumulated_logging_time=0, accumulated_submission_time=25.967514, global_step=1, preemption_count=0, score=25.967514, test/loss=0.502663, test/num_examples=95000000, total_duration=822.774769, train/loss=0.500284, validation/loss=0.505148, validation/num_examples=83274637
I1004 14:55:26.359262 140053439309568 logging_writer.py:48] [1] global_step=1, grad_norm=5.528039455413818, loss=0.5010671615600586
I1004 14:55:26.468924 140053447702272 logging_writer.py:48] [2] global_step=2, grad_norm=4.794717788696289, loss=0.43987706303596497
I1004 14:55:26.575068 140053439309568 logging_writer.py:48] [3] global_step=3, grad_norm=3.7353103160858154, loss=0.352267861366272
I1004 14:55:26.679100 140053447702272 logging_writer.py:48] [4] global_step=4, grad_norm=2.549896240234375, loss=0.2640054225921631
I1004 14:55:26.782864 140053439309568 logging_writer.py:48] [5] global_step=5, grad_norm=1.4436383247375488, loss=0.20174668729305267
I1004 14:55:26.886760 140053447702272 logging_writer.py:48] [6] global_step=6, grad_norm=0.5505800843238831, loss=0.16519799828529358
I1004 14:55:26.991903 140053439309568 logging_writer.py:48] [7] global_step=7, grad_norm=0.15772192180156708, loss=0.15348322689533234
I1004 14:55:27.096822 140053447702272 logging_writer.py:48] [8] global_step=8, grad_norm=0.5791817307472229, loss=0.16379806399345398
I1004 14:55:27.207314 140053439309568 logging_writer.py:48] [9] global_step=9, grad_norm=0.8986058235168457, loss=0.18497714400291443
I1004 14:55:27.312080 140053447702272 logging_writer.py:48] [10] global_step=10, grad_norm=1.0618761777877808, loss=0.19989600777626038
I1004 14:55:27.416931 140053439309568 logging_writer.py:48] [11] global_step=11, grad_norm=1.1672403812408447, loss=0.21248120069503784
I1004 14:55:27.522469 140053447702272 logging_writer.py:48] [12] global_step=12, grad_norm=1.206581473350525, loss=0.21676331758499146
I1004 14:55:27.628427 140053439309568 logging_writer.py:48] [13] global_step=13, grad_norm=1.1638544797897339, loss=0.21123942732810974
I1004 14:55:27.733820 140053447702272 logging_writer.py:48] [14] global_step=14, grad_norm=1.120035171508789, loss=0.20410865545272827
I1004 14:55:27.838499 140053439309568 logging_writer.py:48] [15] global_step=15, grad_norm=0.9880922436714172, loss=0.18912170827388763
I1004 14:55:27.943590 140053447702272 logging_writer.py:48] [16] global_step=16, grad_norm=0.8670487999916077, loss=0.17943879961967468
I1004 14:55:28.048727 140053439309568 logging_writer.py:48] [17] global_step=17, grad_norm=0.6404845118522644, loss=0.16179430484771729
I1004 14:55:28.154649 140053447702272 logging_writer.py:48] [18] global_step=18, grad_norm=0.3763061761856079, loss=0.14965268969535828
I1004 14:55:28.260811 140053439309568 logging_writer.py:48] [19] global_step=19, grad_norm=0.16718162596225739, loss=0.15220968425273895
I1004 14:55:28.366419 140053447702272 logging_writer.py:48] [20] global_step=20, grad_norm=0.17425289750099182, loss=0.15280644595623016
I1004 14:55:28.473503 140053439309568 logging_writer.py:48] [21] global_step=21, grad_norm=0.26788976788520813, loss=0.1525615155696869
I1004 14:55:28.584354 140053447702272 logging_writer.py:48] [22] global_step=22, grad_norm=0.16033455729484558, loss=0.14969073235988617
I1004 14:55:28.691336 140053439309568 logging_writer.py:48] [23] global_step=23, grad_norm=0.046967633068561554, loss=0.14800214767456055
I1004 14:55:28.796895 140053447702272 logging_writer.py:48] [24] global_step=24, grad_norm=0.09981130063533783, loss=0.14638635516166687
I1004 14:55:28.901847 140053439309568 logging_writer.py:48] [25] global_step=25, grad_norm=0.081015445291996, loss=0.14473894238471985
I1004 14:55:29.008164 140053447702272 logging_writer.py:48] [26] global_step=26, grad_norm=0.041616156697273254, loss=0.145362988114357
I1004 14:55:29.113228 140053439309568 logging_writer.py:48] [27] global_step=27, grad_norm=0.03380059078335762, loss=0.14780494570732117
I1004 14:55:29.371267 140053447702272 logging_writer.py:48] [28] global_step=28, grad_norm=0.02918403409421444, loss=0.14661042392253876
I1004 14:55:30.179775 140053439309568 logging_writer.py:48] [29] global_step=29, grad_norm=0.03765890747308731, loss=0.14534315466880798
I1004 14:55:31.025619 140053447702272 logging_writer.py:48] [30] global_step=30, grad_norm=0.04854770004749298, loss=0.14517441391944885
I1004 14:55:31.800847 140053439309568 logging_writer.py:48] [31] global_step=31, grad_norm=0.05844489857554436, loss=0.14351128041744232
I1004 14:55:32.613303 140053447702272 logging_writer.py:48] [32] global_step=32, grad_norm=0.11400053650140762, loss=0.14466574788093567
I1004 14:55:33.512467 140053439309568 logging_writer.py:48] [33] global_step=33, grad_norm=0.16348060965538025, loss=0.14485111832618713
I1004 14:55:34.308303 140053447702272 logging_writer.py:48] [34] global_step=34, grad_norm=0.29245835542678833, loss=0.1447102427482605
I1004 14:55:35.279538 140053439309568 logging_writer.py:48] [35] global_step=35, grad_norm=0.22896751761436462, loss=0.14465536177158356
I1004 14:55:36.208879 140053447702272 logging_writer.py:48] [36] global_step=36, grad_norm=0.14865636825561523, loss=0.14234988391399384
I1004 14:55:36.906379 140053439309568 logging_writer.py:48] [37] global_step=37, grad_norm=0.05164485052227974, loss=0.13916870951652527
I1004 14:55:37.744235 140053447702272 logging_writer.py:48] [38] global_step=38, grad_norm=0.07399387657642365, loss=0.13222995400428772
I1004 14:55:38.444727 140053439309568 logging_writer.py:48] [39] global_step=39, grad_norm=0.07056217640638351, loss=0.12893471121788025
I1004 14:55:39.231813 140053447702272 logging_writer.py:48] [40] global_step=40, grad_norm=0.021171223372220993, loss=0.12803590297698975
I1004 14:55:40.068502 140053439309568 logging_writer.py:48] [41] global_step=41, grad_norm=0.027921708300709724, loss=0.1270487904548645
I1004 14:55:40.789344 140053447702272 logging_writer.py:48] [42] global_step=42, grad_norm=0.015929115936160088, loss=0.12922239303588867
I1004 14:55:41.558209 140053439309568 logging_writer.py:48] [43] global_step=43, grad_norm=0.035561759024858475, loss=0.1261896789073944
I1004 14:55:42.324727 140053447702272 logging_writer.py:48] [44] global_step=44, grad_norm=0.08841007947921753, loss=0.12748461961746216
I1004 14:55:43.181807 140053439309568 logging_writer.py:48] [45] global_step=45, grad_norm=0.18965096771717072, loss=0.12635812163352966
I1004 14:55:44.061424 140053447702272 logging_writer.py:48] [46] global_step=46, grad_norm=0.29582804441452026, loss=0.12518753111362457
I1004 14:55:44.812538 140053439309568 logging_writer.py:48] [47] global_step=47, grad_norm=0.43776482343673706, loss=0.12911227345466614
I1004 14:55:45.586984 140053447702272 logging_writer.py:48] [48] global_step=48, grad_norm=0.4256764054298401, loss=0.13133275508880615
I1004 14:55:46.411335 140053439309568 logging_writer.py:48] [49] global_step=49, grad_norm=0.25384441018104553, loss=0.12419688701629639
I1004 14:55:47.327071 140053447702272 logging_writer.py:48] [50] global_step=50, grad_norm=0.12372631579637527, loss=0.12446316331624985
I1004 14:55:48.276285 140053439309568 logging_writer.py:48] [51] global_step=51, grad_norm=0.08461448550224304, loss=0.1227109283208847
I1004 14:55:49.016722 140053447702272 logging_writer.py:48] [52] global_step=52, grad_norm=0.02217063494026661, loss=0.12053055316209793
I1004 14:55:49.804775 140053439309568 logging_writer.py:48] [53] global_step=53, grad_norm=0.006874963641166687, loss=0.1212008148431778
I1004 14:55:50.789530 140053447702272 logging_writer.py:48] [54] global_step=54, grad_norm=0.006986334454268217, loss=0.12011832743883133
I1004 14:55:51.836382 140053439309568 logging_writer.py:48] [55] global_step=55, grad_norm=0.006700259633362293, loss=0.11953520029783249
I1004 14:55:52.444648 140053447702272 logging_writer.py:48] [56] global_step=56, grad_norm=0.016082536429166794, loss=0.12182488292455673
I1004 14:55:53.398221 140053439309568 logging_writer.py:48] [57] global_step=57, grad_norm=0.10720811784267426, loss=0.1345602422952652
I1004 14:55:54.226754 140053447702272 logging_writer.py:48] [58] global_step=58, grad_norm=0.14800965785980225, loss=0.13747672736644745
I1004 14:55:54.939628 140053439309568 logging_writer.py:48] [59] global_step=59, grad_norm=0.060157302767038345, loss=0.13450169563293457
I1004 14:55:55.801693 140053447702272 logging_writer.py:48] [60] global_step=60, grad_norm=0.04751024395227432, loss=0.13500584661960602
I1004 14:55:56.668540 140053439309568 logging_writer.py:48] [61] global_step=61, grad_norm=0.019261879846453667, loss=0.13267545402050018
I1004 14:55:57.466973 140053447702272 logging_writer.py:48] [62] global_step=62, grad_norm=0.021097376942634583, loss=0.13556845486164093
I1004 14:55:58.385942 140053439309568 logging_writer.py:48] [63] global_step=63, grad_norm=0.04137638583779335, loss=0.13479818403720856
I1004 14:55:59.227786 140053447702272 logging_writer.py:48] [64] global_step=64, grad_norm=0.0664178878068924, loss=0.13577798008918762
I1004 14:55:59.959131 140053439309568 logging_writer.py:48] [65] global_step=65, grad_norm=0.06549554318189621, loss=0.1331624984741211
I1004 14:56:00.898092 140053447702272 logging_writer.py:48] [66] global_step=66, grad_norm=0.08212744444608688, loss=0.13435615599155426
I1004 14:56:01.621575 140053439309568 logging_writer.py:48] [67] global_step=67, grad_norm=0.08657731115818024, loss=0.13452458381652832
I1004 14:56:02.567157 140053447702272 logging_writer.py:48] [68] global_step=68, grad_norm=0.08173945546150208, loss=0.13463017344474792
I1004 14:56:03.405386 140053439309568 logging_writer.py:48] [69] global_step=69, grad_norm=0.08500228077173233, loss=0.13293328881263733
I1004 14:56:04.126385 140053447702272 logging_writer.py:48] [70] global_step=70, grad_norm=0.09662448614835739, loss=0.13386525213718414
I1004 14:56:04.972074 140053439309568 logging_writer.py:48] [71] global_step=71, grad_norm=0.1053214967250824, loss=0.13373753428459167
I1004 14:56:05.775732 140053447702272 logging_writer.py:48] [72] global_step=72, grad_norm=0.09689783304929733, loss=0.13081172108650208
I1004 14:56:06.616850 140053439309568 logging_writer.py:48] [73] global_step=73, grad_norm=0.08064771443605423, loss=0.1318598985671997
I1004 14:56:07.424967 140053447702272 logging_writer.py:48] [74] global_step=74, grad_norm=0.07754030078649521, loss=0.13320860266685486
I1004 14:56:08.104115 140053439309568 logging_writer.py:48] [75] global_step=75, grad_norm=0.11522067338228226, loss=0.1317170411348343
I1004 14:56:08.941654 140053447702272 logging_writer.py:48] [76] global_step=76, grad_norm=0.09930385649204254, loss=0.12851457297801971
I1004 14:56:09.747596 140053439309568 logging_writer.py:48] [77] global_step=77, grad_norm=0.09266901761293411, loss=0.1268034428358078
I1004 14:56:10.356168 140053447702272 logging_writer.py:48] [78] global_step=78, grad_norm=0.0991620421409607, loss=0.12651224434375763
I1004 14:56:11.195813 140053439309568 logging_writer.py:48] [79] global_step=79, grad_norm=0.07393176853656769, loss=0.12654344737529755
I1004 14:56:11.863300 140053447702272 logging_writer.py:48] [80] global_step=80, grad_norm=0.07603272050619125, loss=0.12831264734268188
I1004 14:56:12.776731 140053439309568 logging_writer.py:48] [81] global_step=81, grad_norm=0.08201432228088379, loss=0.12883257865905762
I1004 14:56:13.466399 140053447702272 logging_writer.py:48] [82] global_step=82, grad_norm=0.06722336262464523, loss=0.1261533498764038
I1004 14:56:14.103975 140053439309568 logging_writer.py:48] [83] global_step=83, grad_norm=0.06141303852200508, loss=0.1252688467502594
I1004 14:56:14.863897 140053447702272 logging_writer.py:48] [84] global_step=84, grad_norm=0.06659353524446487, loss=0.12509360909461975
I1004 14:56:15.622297 140053439309568 logging_writer.py:48] [85] global_step=85, grad_norm=0.05263610929250717, loss=0.12736691534519196
I1004 14:56:16.388890 140053447702272 logging_writer.py:48] [86] global_step=86, grad_norm=0.06197961047291756, loss=0.12788867950439453
I1004 14:56:17.126611 140053439309568 logging_writer.py:48] [87] global_step=87, grad_norm=0.08678325265645981, loss=0.12660449743270874
I1004 14:56:17.878628 140053447702272 logging_writer.py:48] [88] global_step=88, grad_norm=0.11379782110452652, loss=0.12665815651416779
I1004 14:56:18.629501 140053439309568 logging_writer.py:48] [89] global_step=89, grad_norm=0.1456701010465622, loss=0.12731045484542847
I1004 14:56:19.549745 140053447702272 logging_writer.py:48] [90] global_step=90, grad_norm=0.17620684206485748, loss=0.1290128231048584
I1004 14:56:20.145515 140053439309568 logging_writer.py:48] [91] global_step=91, grad_norm=0.21853016316890717, loss=0.1261914223432541
I1004 14:56:20.984611 140053447702272 logging_writer.py:48] [92] global_step=92, grad_norm=0.21137240529060364, loss=0.12697230279445648
I1004 14:56:21.809023 140053439309568 logging_writer.py:48] [93] global_step=93, grad_norm=0.1452721357345581, loss=0.12826213240623474
I1004 14:56:22.646616 140053447702272 logging_writer.py:48] [94] global_step=94, grad_norm=0.08320631831884384, loss=0.1252617985010147
I1004 14:56:23.310499 140053439309568 logging_writer.py:48] [95] global_step=95, grad_norm=0.061073221266269684, loss=0.12956613302230835
I1004 14:56:24.194036 140053447702272 logging_writer.py:48] [96] global_step=96, grad_norm=0.05887778848409653, loss=0.13470734655857086
I1004 14:56:24.853094 140053439309568 logging_writer.py:48] [97] global_step=97, grad_norm=0.05728812888264656, loss=0.13352926075458527
I1004 14:56:25.618535 140053447702272 logging_writer.py:48] [98] global_step=98, grad_norm=0.053198911249637604, loss=0.1342819631099701
I1004 14:56:26.367495 140053439309568 logging_writer.py:48] [99] global_step=99, grad_norm=0.049849264323711395, loss=0.13361622393131256
I1004 14:56:27.214406 140053447702272 logging_writer.py:48] [100] global_step=100, grad_norm=0.025067038834095, loss=0.12942415475845337
I1004 15:01:37.267261 140053439309568 logging_writer.py:48] [500] global_step=500, grad_norm=0.01645420305430889, loss=0.13989150524139404
I1004 15:08:12.739271 140053447702272 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.03130579739809036, loss=0.12467366456985474
I1004 15:14:42.807353 140053439309568 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0077196448110044, loss=0.12404918670654297
I1004 15:15:26.830539 140235179677504 spec.py:321] Evaluating on the training split.
I1004 15:18:48.980778 140235179677504 spec.py:333] Evaluating on the validation split.
I1004 15:22:04.692562 140235179677504 spec.py:349] Evaluating on the test split.
I1004 15:25:21.785426 140235179677504 submission_runner.py:381] Time since start: 2618.34s, 	Step: 1560, 	{'train/loss': 0.12265261764046531, 'validation/loss': 0.1251835778041278, 'validation/num_examples': 83274637, 'test/loss': 0.12756808421052632, 'test/num_examples': 95000000, 'score': 1226.5286693572998, 'total_duration': 2618.341100215912, 'accumulated_submission_time': 1226.5286693572998, 'accumulated_eval_time': 1391.762050628662, 'accumulated_logging_time': 0.028968095779418945}
I1004 15:25:21.800994 140053447702272 logging_writer.py:48] [1560] accumulated_eval_time=1391.762051, accumulated_logging_time=0.028968, accumulated_submission_time=1226.528669, global_step=1560, preemption_count=0, score=1226.528669, test/loss=0.127568, test/num_examples=95000000, total_duration=2618.341100, train/loss=0.122653, validation/loss=0.125184, validation/num_examples=83274637
I1004 15:30:50.664848 140053439309568 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.004933979362249374, loss=0.12457163631916046
I1004 15:37:15.485770 140053447702272 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.036126673221588135, loss=0.11807204782962799
I1004 15:43:46.002026 140053439309568 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.003801091341301799, loss=0.1155889630317688
I1004 15:45:22.086421 140235179677504 spec.py:321] Evaluating on the training split.
I1004 15:48:42.061551 140235179677504 spec.py:333] Evaluating on the validation split.
I1004 15:51:22.453080 140235179677504 spec.py:349] Evaluating on the test split.
I1004 15:54:34.709079 140235179677504 submission_runner.py:381] Time since start: 4371.26s, 	Step: 3122, 	{'train/loss': 0.12230931887836577, 'validation/loss': 0.12450627674306164, 'validation/num_examples': 83274637, 'test/loss': 0.12686991578947368, 'test/num_examples': 95000000, 'score': 2426.785194158554, 'total_duration': 4371.264741897583, 'accumulated_submission_time': 2426.785194158554, 'accumulated_eval_time': 1944.3846473693848, 'accumulated_logging_time': 0.05216693878173828}
I1004 15:54:34.727415 140053447702272 logging_writer.py:48] [3122] accumulated_eval_time=1944.384647, accumulated_logging_time=0.052167, accumulated_submission_time=2426.785194, global_step=3122, preemption_count=0, score=2426.785194, test/loss=0.126870, test/num_examples=95000000, total_duration=4371.264742, train/loss=0.122309, validation/loss=0.124506, validation/num_examples=83274637
I1004 15:59:13.547453 140053439309568 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.006916351150721312, loss=0.12193913757801056
I1004 16:05:38.968921 140053447702272 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.01431910041719675, loss=0.12163535505533218
I1004 16:12:23.494577 140053439309568 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.004499043338000774, loss=0.12699417769908905
I1004 16:14:35.024518 140235179677504 spec.py:321] Evaluating on the training split.
I1004 16:17:57.969808 140235179677504 spec.py:333] Evaluating on the validation split.
I1004 16:20:51.852192 140235179677504 spec.py:349] Evaluating on the test split.
I1004 16:24:26.525967 140235179677504 submission_runner.py:381] Time since start: 6163.08s, 	Step: 4671, 	{'train/loss': 0.12152262753660574, 'validation/loss': 0.12426840119399139, 'validation/num_examples': 83274637, 'test/loss': 0.1265166210526316, 'test/num_examples': 95000000, 'score': 3627.0529556274414, 'total_duration': 6163.0816469192505, 'accumulated_submission_time': 3627.0529556274414, 'accumulated_eval_time': 2535.886058330536, 'accumulated_logging_time': 0.07884645462036133}
I1004 16:24:26.543788 140053447702272 logging_writer.py:48] [4671] accumulated_eval_time=2535.886058, accumulated_logging_time=0.078846, accumulated_submission_time=3627.052956, global_step=4671, preemption_count=0, score=3627.052956, test/loss=0.126517, test/num_examples=95000000, total_duration=6163.081647, train/loss=0.121523, validation/loss=0.124268, validation/num_examples=83274637
I1004 16:28:22.829710 140053439309568 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.01806866005063057, loss=0.12250316143035889
I1004 16:34:55.936666 140053447702272 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.005578687880188227, loss=0.1289294958114624
I1004 16:41:24.730882 140053439309568 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.014111164025962353, loss=0.11773267388343811
I1004 16:44:26.747634 140235179677504 spec.py:321] Evaluating on the training split.
I1004 16:47:19.008615 140235179677504 spec.py:333] Evaluating on the validation split.
I1004 16:50:05.184314 140235179677504 spec.py:349] Evaluating on the test split.
I1004 16:53:29.398755 140235179677504 submission_runner.py:381] Time since start: 7905.95s, 	Step: 6235, 	{'train/loss': 0.12229800674150575, 'validation/loss': 0.12378036544308203, 'validation/num_examples': 83274637, 'test/loss': 0.1260933894736842, 'test/num_examples': 95000000, 'score': 4827.22793507576, 'total_duration': 7905.954438686371, 'accumulated_submission_time': 4827.22793507576, 'accumulated_eval_time': 3078.537145614624, 'accumulated_logging_time': 0.10457921028137207}
I1004 16:53:29.415628 140053447702272 logging_writer.py:48] [6235] accumulated_eval_time=3078.537146, accumulated_logging_time=0.104579, accumulated_submission_time=4827.227935, global_step=6235, preemption_count=0, score=4827.227935, test/loss=0.126093, test/num_examples=95000000, total_duration=7905.954439, train/loss=0.122298, validation/loss=0.123780, validation/num_examples=83274637
I1004 16:56:39.330267 140053439309568 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0075758714228868484, loss=0.11756637692451477
I1004 17:03:06.284657 140053447702272 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.010792328044772148, loss=0.12278075516223907
I1004 17:09:27.999945 140053439309568 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.006356615107506514, loss=0.12240266799926758
I1004 17:13:29.905267 140235179677504 spec.py:321] Evaluating on the training split.
I1004 17:16:09.874268 140235179677504 spec.py:333] Evaluating on the validation split.
I1004 17:18:30.363464 140235179677504 spec.py:349] Evaluating on the test split.
I1004 17:21:08.248124 140235179677504 submission_runner.py:381] Time since start: 9564.80s, 	Step: 7806, 	{'train/loss': 0.12442500036467546, 'validation/loss': 0.12373696687504024, 'validation/num_examples': 83274637, 'test/loss': 0.1260810105263158, 'test/num_examples': 95000000, 'score': 6027.684932470322, 'total_duration': 9564.803795099258, 'accumulated_submission_time': 6027.684932470322, 'accumulated_eval_time': 3536.8799612522125, 'accumulated_logging_time': 0.13281846046447754}
I1004 17:21:08.264864 140053447702272 logging_writer.py:48] [7806] accumulated_eval_time=3536.879961, accumulated_logging_time=0.132818, accumulated_submission_time=6027.684932, global_step=7806, preemption_count=0, score=6027.684932, test/loss=0.126081, test/num_examples=95000000, total_duration=9564.803795, train/loss=0.124425, validation/loss=0.123737, validation/num_examples=83274637
I1004 17:23:19.155939 140235179677504 spec.py:321] Evaluating on the training split.
I1004 17:25:08.194737 140235179677504 spec.py:333] Evaluating on the validation split.
I1004 17:26:36.795186 140235179677504 spec.py:349] Evaluating on the test split.
I1004 17:28:39.473022 140235179677504 submission_runner.py:381] Time since start: 10016.03s, 	Step: 8000, 	{'train/loss': 0.12084789396082081, 'validation/loss': 0.1237034512681214, 'validation/num_examples': 83274637, 'test/loss': 0.12603090526315788, 'test/num_examples': 95000000, 'score': 6158.565259695053, 'total_duration': 10016.028698682785, 'accumulated_submission_time': 6158.565259695053, 'accumulated_eval_time': 3857.1970043182373, 'accumulated_logging_time': 0.15737318992614746}
I1004 17:28:39.489225 140053439309568 logging_writer.py:48] [8000] accumulated_eval_time=3857.197004, accumulated_logging_time=0.157373, accumulated_submission_time=6158.565260, global_step=8000, preemption_count=0, score=6158.565260, test/loss=0.126031, test/num_examples=95000000, total_duration=10016.028699, train/loss=0.120848, validation/loss=0.123703, validation/num_examples=83274637
I1004 17:28:39.503060 140053447702272 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=6158.565260
I1004 17:28:45.162080 140235179677504 checkpoints.py:490] Saving checkpoint at step: 8000
I1004 17:29:19.939319 140235179677504 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_7/criteo1tb_jax/trial_1/checkpoint_8000
I1004 17:29:20.205096 140235179677504 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_7/criteo1tb_jax/trial_1/checkpoint_8000.
I1004 17:29:20.558479 140235179677504 submission_runner.py:549] Tuning trial 1/1
I1004 17:29:20.558738 140235179677504 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1004 17:29:20.559860 140235179677504 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.500283679122445, 'validation/loss': 0.5051477558527214, 'validation/num_examples': 83274637, 'test/loss': 0.5026625684210526, 'test/num_examples': 95000000, 'score': 25.96751356124878, 'total_duration': 822.7747693061829, 'accumulated_submission_time': 25.96751356124878, 'accumulated_eval_time': 796.8072137832642, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1560, {'train/loss': 0.12265261764046531, 'validation/loss': 0.1251835778041278, 'validation/num_examples': 83274637, 'test/loss': 0.12756808421052632, 'test/num_examples': 95000000, 'score': 1226.5286693572998, 'total_duration': 2618.341100215912, 'accumulated_submission_time': 1226.5286693572998, 'accumulated_eval_time': 1391.762050628662, 'accumulated_logging_time': 0.028968095779418945, 'global_step': 1560, 'preemption_count': 0}), (3122, {'train/loss': 0.12230931887836577, 'validation/loss': 0.12450627674306164, 'validation/num_examples': 83274637, 'test/loss': 0.12686991578947368, 'test/num_examples': 95000000, 'score': 2426.785194158554, 'total_duration': 4371.264741897583, 'accumulated_submission_time': 2426.785194158554, 'accumulated_eval_time': 1944.3846473693848, 'accumulated_logging_time': 0.05216693878173828, 'global_step': 3122, 'preemption_count': 0}), (4671, {'train/loss': 0.12152262753660574, 'validation/loss': 0.12426840119399139, 'validation/num_examples': 83274637, 'test/loss': 0.1265166210526316, 'test/num_examples': 95000000, 'score': 3627.0529556274414, 'total_duration': 6163.0816469192505, 'accumulated_submission_time': 3627.0529556274414, 'accumulated_eval_time': 2535.886058330536, 'accumulated_logging_time': 0.07884645462036133, 'global_step': 4671, 'preemption_count': 0}), (6235, {'train/loss': 0.12229800674150575, 'validation/loss': 0.12378036544308203, 'validation/num_examples': 83274637, 'test/loss': 0.1260933894736842, 'test/num_examples': 95000000, 'score': 4827.22793507576, 'total_duration': 7905.954438686371, 'accumulated_submission_time': 4827.22793507576, 'accumulated_eval_time': 3078.537145614624, 'accumulated_logging_time': 0.10457921028137207, 'global_step': 6235, 'preemption_count': 0}), (7806, {'train/loss': 0.12442500036467546, 'validation/loss': 0.12373696687504024, 'validation/num_examples': 83274637, 'test/loss': 0.1260810105263158, 'test/num_examples': 95000000, 'score': 6027.684932470322, 'total_duration': 9564.803795099258, 'accumulated_submission_time': 6027.684932470322, 'accumulated_eval_time': 3536.8799612522125, 'accumulated_logging_time': 0.13281846046447754, 'global_step': 7806, 'preemption_count': 0}), (8000, {'train/loss': 0.12084789396082081, 'validation/loss': 0.1237034512681214, 'validation/num_examples': 83274637, 'test/loss': 0.12603090526315788, 'test/num_examples': 95000000, 'score': 6158.565259695053, 'total_duration': 10016.028698682785, 'accumulated_submission_time': 6158.565259695053, 'accumulated_eval_time': 3857.1970043182373, 'accumulated_logging_time': 0.15737318992614746, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1004 17:29:20.560000 140235179677504 submission_runner.py:552] Timing: 6158.565259695053
I1004 17:29:20.560051 140235179677504 submission_runner.py:554] Total number of evals: 7
I1004 17:29:20.560111 140235179677504 submission_runner.py:555] ====================
I1004 17:29:20.560224 140235179677504 submission_runner.py:625] Final criteo1tb score: 6158.565259695053
