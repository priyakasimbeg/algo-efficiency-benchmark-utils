python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_15 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-05-2023-12-04-56.log
2023-10-05 12:05:01.662547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1005 12:05:18.955610 140146950829888 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_15/criteo1tb_jax.
I1005 12:05:20.644359 140146950829888 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1005 12:05:20.645351 140146950829888 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1005 12:05:20.645527 140146950829888 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1005 12:05:20.651221 140146950829888 submission_runner.py:507] Using RNG seed 1277969209
I1005 12:05:26.402371 140146950829888 submission_runner.py:516] --- Tuning run 1/1 ---
I1005 12:05:26.402630 140146950829888 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_15/criteo1tb_jax/trial_1.
I1005 12:05:26.402988 140146950829888 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_15/criteo1tb_jax/trial_1/hparams.json.
I1005 12:05:26.593663 140146950829888 submission_runner.py:191] Initializing dataset.
I1005 12:05:26.593960 140146950829888 submission_runner.py:198] Initializing model.
I1005 12:05:32.543097 140146950829888 submission_runner.py:232] Initializing optimizer.
I1005 12:05:35.701412 140146950829888 submission_runner.py:239] Initializing metrics bundle.
I1005 12:05:35.701683 140146950829888 submission_runner.py:257] Initializing checkpoint and logger.
I1005 12:05:35.703061 140146950829888 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_15/criteo1tb_jax/trial_1 with prefix checkpoint_
I1005 12:05:35.703257 140146950829888 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_15/criteo1tb_jax/trial_1/meta_data_0.json.
I1005 12:05:35.703519 140146950829888 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1005 12:05:35.703611 140146950829888 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1005 12:05:36.546387 140146950829888 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_15/criteo1tb_jax/trial_1/flags_0.json.
I1005 12:05:36.640242 140146950829888 submission_runner.py:290] Starting training loop.
I1005 12:06:02.227354 139982934103808 logging_writer.py:48] [0] global_step=0, grad_norm=4.8241496086120605, loss=0.6604136824607849
I1005 12:06:02.239597 140146950829888 spec.py:321] Evaluating on the training split.
I1005 12:09:52.488209 140146950829888 spec.py:333] Evaluating on the validation split.
I1005 12:13:46.182176 140146950829888 spec.py:349] Evaluating on the test split.
I1005 12:18:07.001742 140146950829888 submission_runner.py:381] Time since start: 750.36s, 	Step: 1, 	{'train/loss': 0.658306673637726, 'validation/loss': 0.6540939950299633, 'validation/num_examples': 83274637, 'test/loss': 0.6565128421052632, 'test/num_examples': 95000000, 'score': 25.599350690841675, 'total_duration': 750.3614554405212, 'accumulated_submission_time': 25.599350690841675, 'accumulated_eval_time': 724.7620601654053, 'accumulated_logging_time': 0}
I1005 12:18:07.023025 139961508558592 logging_writer.py:48] [1] accumulated_eval_time=724.762060, accumulated_logging_time=0, accumulated_submission_time=25.599351, global_step=1, preemption_count=0, score=25.599351, test/loss=0.656513, test/num_examples=95000000, total_duration=750.361455, train/loss=0.658307, validation/loss=0.654094, validation/num_examples=83274637
I1005 12:18:07.142771 139961500165888 logging_writer.py:48] [1] global_step=1, grad_norm=4.8175249099731445, loss=0.6605612635612488
I1005 12:18:07.250101 139961508558592 logging_writer.py:48] [2] global_step=2, grad_norm=4.108120441436768, loss=0.6039950847625732
I1005 12:18:07.354948 139961500165888 logging_writer.py:48] [3] global_step=3, grad_norm=3.274608612060547, loss=0.5264867544174194
I1005 12:18:07.458357 139961508558592 logging_writer.py:48] [4] global_step=4, grad_norm=2.6401400566101074, loss=0.44651150703430176
I1005 12:18:07.561507 139961500165888 logging_writer.py:48] [5] global_step=5, grad_norm=2.1265082359313965, loss=0.37429964542388916
I1005 12:18:07.664541 139961508558592 logging_writer.py:48] [6] global_step=6, grad_norm=1.8151158094406128, loss=0.30840057134628296
I1005 12:18:07.768601 139961500165888 logging_writer.py:48] [7] global_step=7, grad_norm=1.4890042543411255, loss=0.2510443925857544
I1005 12:18:07.872241 139961508558592 logging_writer.py:48] [8] global_step=8, grad_norm=1.0818486213684082, loss=0.20217172801494598
I1005 12:18:07.975519 139961500165888 logging_writer.py:48] [9] global_step=9, grad_norm=0.5587096810340881, loss=0.17087525129318237
I1005 12:18:08.078715 139961508558592 logging_writer.py:48] [10] global_step=10, grad_norm=0.11274715512990952, loss=0.15955881774425507
I1005 12:18:08.182025 139961500165888 logging_writer.py:48] [11] global_step=11, grad_norm=0.4168323278427124, loss=0.16131740808486938
I1005 12:18:08.284709 139961508558592 logging_writer.py:48] [12] global_step=12, grad_norm=0.7286205291748047, loss=0.17834866046905518
I1005 12:18:08.387731 139961500165888 logging_writer.py:48] [13] global_step=13, grad_norm=0.8942771553993225, loss=0.190945103764534
I1005 12:18:08.491359 139961508558592 logging_writer.py:48] [14] global_step=14, grad_norm=0.9748519659042358, loss=0.19762983918190002
I1005 12:18:08.595441 139961500165888 logging_writer.py:48] [15] global_step=15, grad_norm=0.963423490524292, loss=0.19560429453849792
I1005 12:18:08.700313 139961508558592 logging_writer.py:48] [16] global_step=16, grad_norm=0.9285032153129578, loss=0.19263127446174622
I1005 12:18:08.803684 139961500165888 logging_writer.py:48] [17] global_step=17, grad_norm=0.8130214810371399, loss=0.1818493902683258
I1005 12:18:08.907023 139961508558592 logging_writer.py:48] [18] global_step=18, grad_norm=0.6559533476829529, loss=0.16961798071861267
I1005 12:18:09.010541 139961500165888 logging_writer.py:48] [19] global_step=19, grad_norm=0.41739246249198914, loss=0.1516471952199936
I1005 12:18:09.114819 139961508558592 logging_writer.py:48] [20] global_step=20, grad_norm=0.2078840434551239, loss=0.14435704052448273
I1005 12:18:09.218787 139961500165888 logging_writer.py:48] [21] global_step=21, grad_norm=0.06715463846921921, loss=0.1401146948337555
I1005 12:18:09.322019 139961508558592 logging_writer.py:48] [22] global_step=22, grad_norm=0.16964250802993774, loss=0.14234408736228943
I1005 12:18:09.425640 139961500165888 logging_writer.py:48] [23] global_step=23, grad_norm=0.238102525472641, loss=0.14180469512939453
I1005 12:18:09.530184 139961508558592 logging_writer.py:48] [24] global_step=24, grad_norm=0.22268782556056976, loss=0.14038711786270142
I1005 12:18:09.633511 139961500165888 logging_writer.py:48] [25] global_step=25, grad_norm=0.14922480285167694, loss=0.13629382848739624
I1005 12:18:09.737798 139961508558592 logging_writer.py:48] [26] global_step=26, grad_norm=0.03967549279332161, loss=0.13446970283985138
I1005 12:18:09.841776 139961500165888 logging_writer.py:48] [27] global_step=27, grad_norm=0.07889517396688461, loss=0.13519248366355896
I1005 12:18:09.945274 139961508558592 logging_writer.py:48] [28] global_step=28, grad_norm=0.10552109777927399, loss=0.13344386219978333
I1005 12:18:10.683874 139961500165888 logging_writer.py:48] [29] global_step=29, grad_norm=0.09544508904218674, loss=0.13351498544216156
I1005 12:18:11.531015 139961508558592 logging_writer.py:48] [30] global_step=30, grad_norm=0.04569900408387184, loss=0.13174012303352356
I1005 12:18:12.294286 139961500165888 logging_writer.py:48] [31] global_step=31, grad_norm=0.025103211402893066, loss=0.13271406292915344
I1005 12:18:13.042815 139961508558592 logging_writer.py:48] [32] global_step=32, grad_norm=0.02271142415702343, loss=0.1320723295211792
I1005 12:18:13.888649 139961500165888 logging_writer.py:48] [33] global_step=33, grad_norm=0.020727278664708138, loss=0.13220210373401642
I1005 12:18:14.659466 139961508558592 logging_writer.py:48] [34] global_step=34, grad_norm=0.01801219768822193, loss=0.13092313706874847
I1005 12:18:15.329961 139961500165888 logging_writer.py:48] [35] global_step=35, grad_norm=0.017997989431023598, loss=0.1307794749736786
I1005 12:18:16.009692 139961508558592 logging_writer.py:48] [36] global_step=36, grad_norm=0.01834125630557537, loss=0.13217946887016296
I1005 12:18:16.726238 139961500165888 logging_writer.py:48] [37] global_step=37, grad_norm=0.015616467222571373, loss=0.13180801272392273
I1005 12:18:17.548903 139961508558592 logging_writer.py:48] [38] global_step=38, grad_norm=0.06382947415113449, loss=0.15237551927566528
I1005 12:18:18.183609 139961500165888 logging_writer.py:48] [39] global_step=39, grad_norm=0.055267706513404846, loss=0.15553109347820282
I1005 12:18:19.006385 139961508558592 logging_writer.py:48] [40] global_step=40, grad_norm=0.027795061469078064, loss=0.15085990726947784
I1005 12:18:19.648398 139961500165888 logging_writer.py:48] [41] global_step=41, grad_norm=0.0349583700299263, loss=0.15310509502887726
I1005 12:18:20.391942 139961508558592 logging_writer.py:48] [42] global_step=42, grad_norm=0.026818353682756424, loss=0.15143491327762604
I1005 12:18:21.259420 139961500165888 logging_writer.py:48] [43] global_step=43, grad_norm=0.03481630980968475, loss=0.15334552526474
I1005 12:18:22.120255 139961508558592 logging_writer.py:48] [44] global_step=44, grad_norm=0.029904961585998535, loss=0.1554364562034607
I1005 12:18:22.723305 139961500165888 logging_writer.py:48] [45] global_step=45, grad_norm=0.04270545020699501, loss=0.15263979136943817
I1005 12:18:23.426794 139961508558592 logging_writer.py:48] [46] global_step=46, grad_norm=0.04981865733861923, loss=0.15087704360485077
I1005 12:18:24.218234 139961500165888 logging_writer.py:48] [47] global_step=47, grad_norm=0.11143753677606583, loss=0.1498706191778183
I1005 12:18:24.911418 139961508558592 logging_writer.py:48] [48] global_step=48, grad_norm=0.1903412640094757, loss=0.15106201171875
I1005 12:18:25.672603 139961500165888 logging_writer.py:48] [49] global_step=49, grad_norm=0.19436296820640564, loss=0.14826633036136627
I1005 12:18:26.520655 139961508558592 logging_writer.py:48] [50] global_step=50, grad_norm=0.13040503859519958, loss=0.1471427083015442
I1005 12:18:27.308730 139961500165888 logging_writer.py:48] [51] global_step=51, grad_norm=0.08118685334920883, loss=0.14595045149326324
I1005 12:18:28.076860 139961508558592 logging_writer.py:48] [52] global_step=52, grad_norm=0.05544079467654228, loss=0.14523933827877045
I1005 12:18:28.909338 139961500165888 logging_writer.py:48] [53] global_step=53, grad_norm=0.03764095902442932, loss=0.1427881419658661
I1005 12:18:29.587556 139961508558592 logging_writer.py:48] [54] global_step=54, grad_norm=0.02763093262910843, loss=0.14540421962738037
I1005 12:18:30.336905 139961500165888 logging_writer.py:48] [55] global_step=55, grad_norm=0.015996910631656647, loss=0.1422380805015564
I1005 12:18:31.122870 139961508558592 logging_writer.py:48] [56] global_step=56, grad_norm=0.028010420501232147, loss=0.14446446299552917
I1005 12:18:31.905259 139961500165888 logging_writer.py:48] [57] global_step=57, grad_norm=0.04211903363466263, loss=0.1304229497909546
I1005 12:18:32.643022 139961508558592 logging_writer.py:48] [58] global_step=58, grad_norm=0.062470145523548126, loss=0.125376895070076
I1005 12:18:33.396739 139961500165888 logging_writer.py:48] [59] global_step=59, grad_norm=0.08510685712099075, loss=0.12504225969314575
I1005 12:18:34.114316 139961508558592 logging_writer.py:48] [60] global_step=60, grad_norm=0.12190064787864685, loss=0.12303811311721802
I1005 12:18:34.893331 139961500165888 logging_writer.py:48] [61] global_step=61, grad_norm=0.1718929409980774, loss=0.1268632411956787
I1005 12:18:35.581562 139961508558592 logging_writer.py:48] [62] global_step=62, grad_norm=0.2058078944683075, loss=0.12575408816337585
I1005 12:18:36.431253 139961500165888 logging_writer.py:48] [63] global_step=63, grad_norm=0.1910422295331955, loss=0.12695443630218506
I1005 12:18:37.175602 139961508558592 logging_writer.py:48] [64] global_step=64, grad_norm=0.13863390684127808, loss=0.12372364103794098
I1005 12:18:37.910795 139961500165888 logging_writer.py:48] [65] global_step=65, grad_norm=0.0893838107585907, loss=0.12342336773872375
I1005 12:18:38.541708 139961508558592 logging_writer.py:48] [66] global_step=66, grad_norm=0.05089423432946205, loss=0.12358354032039642
I1005 12:18:39.308974 139961500165888 logging_writer.py:48] [67] global_step=67, grad_norm=0.03696918115019798, loss=0.12408052384853363
I1005 12:18:40.063490 139961508558592 logging_writer.py:48] [68] global_step=68, grad_norm=0.03284100815653801, loss=0.1221756637096405
I1005 12:18:40.923931 139961500165888 logging_writer.py:48] [69] global_step=69, grad_norm=0.03720274567604065, loss=0.12445610016584396
I1005 12:18:41.640055 139961508558592 logging_writer.py:48] [70] global_step=70, grad_norm=0.03761003538966179, loss=0.12462660670280457
I1005 12:18:42.325214 139961500165888 logging_writer.py:48] [71] global_step=71, grad_norm=0.030753910541534424, loss=0.12364596128463745
I1005 12:18:43.061761 139961508558592 logging_writer.py:48] [72] global_step=72, grad_norm=0.034260354936122894, loss=0.12200713157653809
I1005 12:18:43.742730 139961500165888 logging_writer.py:48] [73] global_step=73, grad_norm=0.045937392860651016, loss=0.12357074022293091
I1005 12:18:44.533356 139961508558592 logging_writer.py:48] [74] global_step=74, grad_norm=0.06292008608579636, loss=0.121891550719738
I1005 12:18:45.290597 139961500165888 logging_writer.py:48] [75] global_step=75, grad_norm=0.06644349545240402, loss=0.12148052453994751
I1005 12:18:46.017455 139961508558592 logging_writer.py:48] [76] global_step=76, grad_norm=0.04239564761519432, loss=0.13073086738586426
I1005 12:18:46.722342 139961500165888 logging_writer.py:48] [77] global_step=77, grad_norm=0.03582707419991493, loss=0.12939703464508057
I1005 12:18:47.672897 139961508558592 logging_writer.py:48] [78] global_step=78, grad_norm=0.04424483701586723, loss=0.1299414187669754
I1005 12:18:48.357884 139961500165888 logging_writer.py:48] [79] global_step=79, grad_norm=0.04612307623028755, loss=0.1289261430501938
I1005 12:18:48.889929 139961508558592 logging_writer.py:48] [80] global_step=80, grad_norm=0.05866490676999092, loss=0.12976640462875366
I1005 12:18:49.640490 139961500165888 logging_writer.py:48] [81] global_step=81, grad_norm=0.07686296850442886, loss=0.12985509634017944
I1005 12:18:50.392460 139961508558592 logging_writer.py:48] [82] global_step=82, grad_norm=0.11219021677970886, loss=0.1301347315311432
I1005 12:18:51.067023 139961500165888 logging_writer.py:48] [83] global_step=83, grad_norm=0.1527157872915268, loss=0.12924057245254517
I1005 12:18:51.669578 139961508558592 logging_writer.py:48] [84] global_step=84, grad_norm=0.16167917847633362, loss=0.13200877606868744
I1005 12:18:52.337265 139961500165888 logging_writer.py:48] [85] global_step=85, grad_norm=0.17179875075817108, loss=0.132147416472435
I1005 12:18:53.125141 139961508558592 logging_writer.py:48] [86] global_step=86, grad_norm=0.1985834538936615, loss=0.12881386280059814
I1005 12:18:53.607392 139961500165888 logging_writer.py:48] [87] global_step=87, grad_norm=0.20471259951591492, loss=0.13160136342048645
I1005 12:18:54.370511 139961508558592 logging_writer.py:48] [88] global_step=88, grad_norm=0.1599886417388916, loss=0.12957757711410522
I1005 12:18:55.070423 139961500165888 logging_writer.py:48] [89] global_step=89, grad_norm=0.11325329542160034, loss=0.1292044222354889
I1005 12:18:55.622365 139961508558592 logging_writer.py:48] [90] global_step=90, grad_norm=0.07786395400762558, loss=0.12773458659648895
I1005 12:18:56.453573 139961500165888 logging_writer.py:48] [91] global_step=91, grad_norm=0.0514182448387146, loss=0.1291428506374359
I1005 12:18:56.997448 139961508558592 logging_writer.py:48] [92] global_step=92, grad_norm=0.03412343189120293, loss=0.12920457124710083
I1005 12:18:57.691217 139961500165888 logging_writer.py:48] [93] global_step=93, grad_norm=0.013017667457461357, loss=0.12768836319446564
I1005 12:18:58.480343 139961508558592 logging_writer.py:48] [94] global_step=94, grad_norm=0.015391231514513493, loss=0.12507247924804688
I1005 12:18:59.181345 139961500165888 logging_writer.py:48] [95] global_step=95, grad_norm=0.04043317213654518, loss=0.1324482411146164
I1005 12:18:59.840636 139961508558592 logging_writer.py:48] [96] global_step=96, grad_norm=0.045555755496025085, loss=0.13302043080329895
I1005 12:19:00.675021 139961500165888 logging_writer.py:48] [97] global_step=97, grad_norm=0.03077060543000698, loss=0.13172335922718048
I1005 12:19:01.154211 139961508558592 logging_writer.py:48] [98] global_step=98, grad_norm=0.022179540246725082, loss=0.13243919610977173
I1005 12:19:02.014694 139961500165888 logging_writer.py:48] [99] global_step=99, grad_norm=0.013531734235584736, loss=0.13211803138256073
I1005 12:19:02.761053 139961508558592 logging_writer.py:48] [100] global_step=100, grad_norm=0.00658783782273531, loss=0.1325012892484665
I1005 12:23:48.455528 139961500165888 logging_writer.py:48] [500] global_step=500, grad_norm=0.004943470936268568, loss=0.12470060586929321
I1005 12:29:46.250663 139961508558592 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.03492351248860359, loss=0.12861143052577972
I1005 12:35:41.796730 139961500165888 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.011922219768166542, loss=0.12212451547384262
I1005 12:38:07.197979 140146950829888 spec.py:321] Evaluating on the training split.
I1005 12:41:03.623360 140146950829888 spec.py:333] Evaluating on the validation split.
I1005 12:44:14.801238 140146950829888 spec.py:349] Evaluating on the test split.
I1005 12:47:56.860201 140146950829888 submission_runner.py:381] Time since start: 2540.22s, 	Step: 1706, 	{'train/loss': 0.12512125459107212, 'validation/loss': 0.1255893676246226, 'validation/num_examples': 83274637, 'test/loss': 0.128187, 'test/num_examples': 95000000, 'score': 1225.7430458068848, 'total_duration': 2540.2199127674103, 'accumulated_submission_time': 1225.7430458068848, 'accumulated_eval_time': 1314.4242329597473, 'accumulated_logging_time': 0.029237747192382812}
I1005 12:47:56.879685 139961508558592 logging_writer.py:48] [1706] accumulated_eval_time=1314.424233, accumulated_logging_time=0.029238, accumulated_submission_time=1225.743046, global_step=1706, preemption_count=0, score=1225.743046, test/loss=0.128187, test/num_examples=95000000, total_duration=2540.219913, train/loss=0.125121, validation/loss=0.125589, validation/num_examples=83274637
I1005 12:51:12.724891 139961500165888 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.014374175108969212, loss=0.11972742527723312
I1005 12:57:07.672603 139961508558592 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.033824723213911057, loss=0.13213060796260834
I1005 13:02:57.331040 139961500165888 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.005665198899805546, loss=0.12215323001146317
I1005 13:07:57.295107 140146950829888 spec.py:321] Evaluating on the training split.
I1005 13:11:01.204403 140146950829888 spec.py:333] Evaluating on the validation split.
I1005 13:13:27.797779 140146950829888 spec.py:349] Evaluating on the test split.
I1005 13:16:17.376790 140146950829888 submission_runner.py:381] Time since start: 4240.74s, 	Step: 3418, 	{'train/loss': 0.12452726544074293, 'validation/loss': 0.12448470955208127, 'validation/num_examples': 83274637, 'test/loss': 0.12674165263157894, 'test/num_examples': 95000000, 'score': 2426.1257026195526, 'total_duration': 4240.7365164756775, 'accumulated_submission_time': 2426.1257026195526, 'accumulated_eval_time': 1814.5058813095093, 'accumulated_logging_time': 0.057662248611450195}
I1005 13:16:17.393676 139961508558592 logging_writer.py:48] [3418] accumulated_eval_time=1814.505881, accumulated_logging_time=0.057662, accumulated_submission_time=2426.125703, global_step=3418, preemption_count=0, score=2426.125703, test/loss=0.126742, test/num_examples=95000000, total_duration=4240.736516, train/loss=0.124527, validation/loss=0.124485, validation/num_examples=83274637
I1005 13:17:00.938733 139961500165888 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.010646446608006954, loss=0.12174874544143677
I1005 13:22:57.418716 139961508558592 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.016117623075842857, loss=0.1302284598350525
I1005 13:28:51.132629 139961500165888 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.010415478609502316, loss=0.11951775848865509
I1005 13:34:45.040888 139961508558592 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.003779152175411582, loss=0.12060829997062683
I1005 13:36:17.531355 140146950829888 spec.py:321] Evaluating on the training split.
I1005 13:39:18.459098 140146950829888 spec.py:333] Evaluating on the validation split.
I1005 13:41:40.411099 140146950829888 spec.py:349] Evaluating on the test split.
I1005 13:44:22.013209 140146950829888 submission_runner.py:381] Time since start: 5925.37s, 	Step: 5128, 	{'train/loss': 0.12188743495341367, 'validation/loss': 0.12405263321652185, 'validation/num_examples': 83274637, 'test/loss': 0.12632571578947369, 'test/num_examples': 95000000, 'score': 3626.2306106090546, 'total_duration': 5925.372926950455, 'accumulated_submission_time': 3626.2306106090546, 'accumulated_eval_time': 2298.987685918808, 'accumulated_logging_time': 0.08374595642089844}
I1005 13:44:22.031517 139961500165888 logging_writer.py:48] [5128] accumulated_eval_time=2298.987686, accumulated_logging_time=0.083746, accumulated_submission_time=3626.230611, global_step=5128, preemption_count=0, score=3626.230611, test/loss=0.126326, test/num_examples=95000000, total_duration=5925.372927, train/loss=0.121887, validation/loss=0.124053, validation/num_examples=83274637
I1005 13:48:29.299939 139961508558592 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.005431674886494875, loss=0.12145810574293137
I1005 13:54:25.088043 139961500165888 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.004345570225268602, loss=0.13051974773406982
I1005 14:00:17.943292 139961508558592 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0048534017987549305, loss=0.12208086252212524
I1005 14:04:22.209864 140146950829888 spec.py:321] Evaluating on the training split.
I1005 14:07:10.204099 140146950829888 spec.py:333] Evaluating on the validation split.
I1005 14:09:25.007446 140146950829888 spec.py:349] Evaluating on the test split.
I1005 14:11:57.339807 140146950829888 submission_runner.py:381] Time since start: 7580.70s, 	Step: 6844, 	{'train/loss': 0.11974962102542133, 'validation/loss': 0.12376428611751258, 'validation/num_examples': 83274637, 'test/loss': 0.12602176842105264, 'test/num_examples': 95000000, 'score': 4826.376331329346, 'total_duration': 7580.699512720108, 'accumulated_submission_time': 4826.376331329346, 'accumulated_eval_time': 2754.1175725460052, 'accumulated_logging_time': 0.11096525192260742}
I1005 14:11:57.361879 139961500165888 logging_writer.py:48] [6844] accumulated_eval_time=2754.117573, accumulated_logging_time=0.110965, accumulated_submission_time=4826.376331, global_step=6844, preemption_count=0, score=4826.376331, test/loss=0.126022, test/num_examples=95000000, total_duration=7580.699513, train/loss=0.119750, validation/loss=0.123764, validation/num_examples=83274637
I1005 14:13:35.263384 139961508558592 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.028285356238484383, loss=0.12274818122386932
I1005 14:19:32.815234 139961500165888 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.005525793880224228, loss=0.12736043334007263
I1005 14:25:24.896865 140146950829888 spec.py:321] Evaluating on the training split.
I1005 14:27:46.883226 140146950829888 spec.py:333] Evaluating on the validation split.
I1005 14:29:44.689753 140146950829888 spec.py:349] Evaluating on the test split.
I1005 14:32:02.502241 140146950829888 submission_runner.py:381] Time since start: 8785.86s, 	Step: 8000, 	{'train/loss': 0.12264604388542895, 'validation/loss': 0.12366526437095127, 'validation/num_examples': 83274637, 'test/loss': 0.12594873684210525, 'test/num_examples': 95000000, 'score': 5633.886581420898, 'total_duration': 8785.86192727089, 'accumulated_submission_time': 5633.886581420898, 'accumulated_eval_time': 3151.722886800766, 'accumulated_logging_time': 0.14141392707824707}
I1005 14:32:02.522741 139961508558592 logging_writer.py:48] [8000] accumulated_eval_time=3151.722887, accumulated_logging_time=0.141414, accumulated_submission_time=5633.886581, global_step=8000, preemption_count=0, score=5633.886581, test/loss=0.125949, test/num_examples=95000000, total_duration=8785.861927, train/loss=0.122646, validation/loss=0.123665, validation/num_examples=83274637
I1005 14:32:02.539881 139961500165888 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5633.886581
I1005 14:32:09.982217 140146950829888 checkpoints.py:490] Saving checkpoint at step: 8000
I1005 14:32:53.092384 140146950829888 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_15/criteo1tb_jax/trial_1/checkpoint_8000
I1005 14:32:53.436004 140146950829888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_15/criteo1tb_jax/trial_1/checkpoint_8000.
I1005 14:32:53.827023 140146950829888 submission_runner.py:549] Tuning trial 1/1
I1005 14:32:53.827316 140146950829888 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1005 14:32:53.828346 140146950829888 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.658306673637726, 'validation/loss': 0.6540939950299633, 'validation/num_examples': 83274637, 'test/loss': 0.6565128421052632, 'test/num_examples': 95000000, 'score': 25.599350690841675, 'total_duration': 750.3614554405212, 'accumulated_submission_time': 25.599350690841675, 'accumulated_eval_time': 724.7620601654053, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1706, {'train/loss': 0.12512125459107212, 'validation/loss': 0.1255893676246226, 'validation/num_examples': 83274637, 'test/loss': 0.128187, 'test/num_examples': 95000000, 'score': 1225.7430458068848, 'total_duration': 2540.2199127674103, 'accumulated_submission_time': 1225.7430458068848, 'accumulated_eval_time': 1314.4242329597473, 'accumulated_logging_time': 0.029237747192382812, 'global_step': 1706, 'preemption_count': 0}), (3418, {'train/loss': 0.12452726544074293, 'validation/loss': 0.12448470955208127, 'validation/num_examples': 83274637, 'test/loss': 0.12674165263157894, 'test/num_examples': 95000000, 'score': 2426.1257026195526, 'total_duration': 4240.7365164756775, 'accumulated_submission_time': 2426.1257026195526, 'accumulated_eval_time': 1814.5058813095093, 'accumulated_logging_time': 0.057662248611450195, 'global_step': 3418, 'preemption_count': 0}), (5128, {'train/loss': 0.12188743495341367, 'validation/loss': 0.12405263321652185, 'validation/num_examples': 83274637, 'test/loss': 0.12632571578947369, 'test/num_examples': 95000000, 'score': 3626.2306106090546, 'total_duration': 5925.372926950455, 'accumulated_submission_time': 3626.2306106090546, 'accumulated_eval_time': 2298.987685918808, 'accumulated_logging_time': 0.08374595642089844, 'global_step': 5128, 'preemption_count': 0}), (6844, {'train/loss': 0.11974962102542133, 'validation/loss': 0.12376428611751258, 'validation/num_examples': 83274637, 'test/loss': 0.12602176842105264, 'test/num_examples': 95000000, 'score': 4826.376331329346, 'total_duration': 7580.699512720108, 'accumulated_submission_time': 4826.376331329346, 'accumulated_eval_time': 2754.1175725460052, 'accumulated_logging_time': 0.11096525192260742, 'global_step': 6844, 'preemption_count': 0}), (8000, {'train/loss': 0.12264604388542895, 'validation/loss': 0.12366526437095127, 'validation/num_examples': 83274637, 'test/loss': 0.12594873684210525, 'test/num_examples': 95000000, 'score': 5633.886581420898, 'total_duration': 8785.86192727089, 'accumulated_submission_time': 5633.886581420898, 'accumulated_eval_time': 3151.722886800766, 'accumulated_logging_time': 0.14141392707824707, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1005 14:32:53.828501 140146950829888 submission_runner.py:552] Timing: 5633.886581420898
I1005 14:32:53.828557 140146950829888 submission_runner.py:554] Total number of evals: 6
I1005 14:32:53.828635 140146950829888 submission_runner.py:555] ====================
I1005 14:32:53.828788 140146950829888 submission_runner.py:625] Final criteo1tb score: 5633.886581420898
