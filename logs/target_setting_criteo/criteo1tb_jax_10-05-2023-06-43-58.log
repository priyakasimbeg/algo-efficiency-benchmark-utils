python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_13 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-05-2023-06-43-58.log
2023-10-05 06:44:03.953357: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1005 06:44:21.045331 139683934209856 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_13/criteo1tb_jax.
I1005 06:44:22.707720 139683934209856 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I1005 06:44:22.708576 139683934209856 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1005 06:44:22.708715 139683934209856 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1005 06:44:22.713831 139683934209856 submission_runner.py:507] Using RNG seed 1020254534
I1005 06:44:28.464562 139683934209856 submission_runner.py:516] --- Tuning run 1/1 ---
I1005 06:44:28.464801 139683934209856 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_13/criteo1tb_jax/trial_1.
I1005 06:44:28.465082 139683934209856 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_13/criteo1tb_jax/trial_1/hparams.json.
I1005 06:44:28.648243 139683934209856 submission_runner.py:191] Initializing dataset.
I1005 06:44:28.648507 139683934209856 submission_runner.py:198] Initializing model.
I1005 06:44:34.310985 139683934209856 submission_runner.py:232] Initializing optimizer.
I1005 06:44:37.519706 139683934209856 submission_runner.py:239] Initializing metrics bundle.
I1005 06:44:37.519926 139683934209856 submission_runner.py:257] Initializing checkpoint and logger.
I1005 06:44:37.521210 139683934209856 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_13/criteo1tb_jax/trial_1 with prefix checkpoint_
I1005 06:44:37.521392 139683934209856 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_13/criteo1tb_jax/trial_1/meta_data_0.json.
I1005 06:44:37.521603 139683934209856 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1005 06:44:37.521669 139683934209856 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1005 06:44:38.373932 139683934209856 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_13/criteo1tb_jax/trial_1/flags_0.json.
I1005 06:44:38.462548 139683934209856 submission_runner.py:290] Starting training loop.
I1005 06:45:05.807845 139519471896320 logging_writer.py:48] [0] global_step=0, grad_norm=8.2957181930542, loss=1.7937672138214111
I1005 06:45:05.818852 139683934209856 spec.py:321] Evaluating on the training split.
I1005 06:49:09.112078 139683934209856 spec.py:333] Evaluating on the validation split.
I1005 06:53:13.681721 139683934209856 spec.py:349] Evaluating on the test split.
I1005 06:57:43.078857 139683934209856 submission_runner.py:381] Time since start: 784.62s, 	Step: 1, 	{'train/loss': 1.794940300707547, 'validation/loss': 1.797035800948613, 'validation/num_examples': 83274637, 'test/loss': 1.794517389473684, 'test/num_examples': 95000000, 'score': 27.356279373168945, 'total_duration': 784.6162300109863, 'accumulated_submission_time': 27.356279373168945, 'accumulated_eval_time': 757.2599132061005, 'accumulated_logging_time': 0}
I1005 06:57:43.099658 139498893588224 logging_writer.py:48] [1] accumulated_eval_time=757.259913, accumulated_logging_time=0, accumulated_submission_time=27.356279, global_step=1, preemption_count=0, score=27.356279, test/loss=1.794517, test/num_examples=95000000, total_duration=784.616230, train/loss=1.794940, validation/loss=1.797036, validation/num_examples=83274637
I1005 06:57:43.216754 139498885195520 logging_writer.py:48] [1] global_step=1, grad_norm=8.301424026489258, loss=1.794978380203247
I1005 06:57:43.327315 139498893588224 logging_writer.py:48] [2] global_step=2, grad_norm=8.208545684814453, loss=1.6922738552093506
I1005 06:57:43.433346 139498885195520 logging_writer.py:48] [3] global_step=3, grad_norm=8.139775276184082, loss=1.512599229812622
I1005 06:57:43.538419 139498893588224 logging_writer.py:48] [4] global_step=4, grad_norm=8.293630599975586, loss=1.2682472467422485
I1005 06:57:43.642691 139498885195520 logging_writer.py:48] [5] global_step=5, grad_norm=7.952104568481445, loss=0.9766207933425903
I1005 06:57:43.747271 139498893588224 logging_writer.py:48] [6] global_step=6, grad_norm=6.999520778656006, loss=0.6741639375686646
I1005 06:57:43.852092 139498885195520 logging_writer.py:48] [7] global_step=7, grad_norm=4.744137287139893, loss=0.4074461758136749
I1005 06:57:43.958792 139498893588224 logging_writer.py:48] [8] global_step=8, grad_norm=2.0469462871551514, loss=0.252937912940979
I1005 06:57:44.066265 139498885195520 logging_writer.py:48] [9] global_step=9, grad_norm=0.28616029024124146, loss=0.2016095668077469
I1005 06:57:44.172482 139498893588224 logging_writer.py:48] [10] global_step=10, grad_norm=0.9999096393585205, loss=0.21697795391082764
I1005 06:57:44.277243 139498885195520 logging_writer.py:48] [11] global_step=11, grad_norm=1.6620739698410034, loss=0.2615937888622284
I1005 06:57:44.381190 139498893588224 logging_writer.py:48] [12] global_step=12, grad_norm=2.074662446975708, loss=0.3097383677959442
I1005 06:57:44.484706 139498885195520 logging_writer.py:48] [13] global_step=13, grad_norm=2.4544453620910645, loss=0.36574622988700867
I1005 06:57:44.588098 139498893588224 logging_writer.py:48] [14] global_step=14, grad_norm=2.6852657794952393, loss=0.40384969115257263
I1005 06:57:44.692080 139498885195520 logging_writer.py:48] [15] global_step=15, grad_norm=2.8857529163360596, loss=0.4373733401298523
I1005 06:57:44.796940 139498893588224 logging_writer.py:48] [16] global_step=16, grad_norm=2.881516456604004, loss=0.43912091851234436
I1005 06:57:44.900716 139498885195520 logging_writer.py:48] [17] global_step=17, grad_norm=2.8606741428375244, loss=0.4327659010887146
I1005 06:57:45.004575 139498893588224 logging_writer.py:48] [18] global_step=18, grad_norm=2.774726629257202, loss=0.417923241853714
I1005 06:57:45.111051 139498885195520 logging_writer.py:48] [19] global_step=19, grad_norm=2.307898759841919, loss=0.35088154673576355
I1005 06:57:45.217898 139498893588224 logging_writer.py:48] [20] global_step=20, grad_norm=2.047520399093628, loss=0.3080829083919525
I1005 06:57:45.323898 139498885195520 logging_writer.py:48] [21] global_step=21, grad_norm=1.7875117063522339, loss=0.2682393789291382
I1005 06:57:45.428810 139498893588224 logging_writer.py:48] [22] global_step=22, grad_norm=1.421080470085144, loss=0.2245299518108368
I1005 06:57:45.533116 139498885195520 logging_writer.py:48] [23] global_step=23, grad_norm=0.9403208494186401, loss=0.18788306415081024
I1005 06:57:45.637560 139498893588224 logging_writer.py:48] [24] global_step=24, grad_norm=0.32108408212661743, loss=0.16997016966342926
I1005 06:57:45.743110 139498885195520 logging_writer.py:48] [25] global_step=25, grad_norm=0.8157297968864441, loss=0.1694229245185852
I1005 06:57:45.848716 139498893588224 logging_writer.py:48] [26] global_step=26, grad_norm=0.6490067839622498, loss=0.16000568866729736
I1005 06:57:45.953966 139498885195520 logging_writer.py:48] [27] global_step=27, grad_norm=0.19321711361408234, loss=0.14788924157619476
I1005 06:57:46.089316 139498893588224 logging_writer.py:48] [28] global_step=28, grad_norm=0.3707803189754486, loss=0.14436718821525574
I1005 06:57:47.026041 139498885195520 logging_writer.py:48] [29] global_step=29, grad_norm=0.28586918115615845, loss=0.13874097168445587
I1005 06:57:47.786468 139498893588224 logging_writer.py:48] [30] global_step=30, grad_norm=0.12240827828645706, loss=0.13426239788532257
I1005 06:57:48.483119 139498885195520 logging_writer.py:48] [31] global_step=31, grad_norm=0.11252546310424805, loss=0.13066942989826202
I1005 06:57:49.282397 139498893588224 logging_writer.py:48] [32] global_step=32, grad_norm=0.11225037276744843, loss=0.13323089480400085
I1005 06:57:50.048602 139498885195520 logging_writer.py:48] [33] global_step=33, grad_norm=0.1283717304468155, loss=0.13285066187381744
I1005 06:57:50.886884 139498893588224 logging_writer.py:48] [34] global_step=34, grad_norm=0.459568053483963, loss=0.1314390003681183
I1005 06:57:51.571858 139498885195520 logging_writer.py:48] [35] global_step=35, grad_norm=2.1724677085876465, loss=0.14806899428367615
I1005 06:57:52.445526 139498893588224 logging_writer.py:48] [36] global_step=36, grad_norm=1.429980993270874, loss=0.16366195678710938
I1005 06:57:53.240263 139498885195520 logging_writer.py:48] [37] global_step=37, grad_norm=0.7134217619895935, loss=0.13275982439517975
I1005 06:57:53.798994 139498893588224 logging_writer.py:48] [38] global_step=38, grad_norm=0.6858763694763184, loss=0.13820818066596985
I1005 06:57:54.558166 139498885195520 logging_writer.py:48] [39] global_step=39, grad_norm=0.04282531142234802, loss=0.13703510165214539
I1005 06:57:55.237345 139498893588224 logging_writer.py:48] [40] global_step=40, grad_norm=0.1701485514640808, loss=0.13301074504852295
I1005 06:57:56.083362 139498885195520 logging_writer.py:48] [41] global_step=41, grad_norm=0.09661384671926498, loss=0.13435868918895721
I1005 06:57:56.870121 139498893588224 logging_writer.py:48] [42] global_step=42, grad_norm=0.06897395104169846, loss=0.1336323320865631
I1005 06:57:57.639981 139498885195520 logging_writer.py:48] [43] global_step=43, grad_norm=0.026030931621789932, loss=0.13280688226222992
I1005 06:57:58.296340 139498893588224 logging_writer.py:48] [44] global_step=44, grad_norm=0.0317424051463604, loss=0.13472086191177368
I1005 06:57:59.175119 139498885195520 logging_writer.py:48] [45] global_step=45, grad_norm=0.03601102530956268, loss=0.1340489536523819
I1005 06:57:59.961738 139498893588224 logging_writer.py:48] [46] global_step=46, grad_norm=0.026249432936310768, loss=0.13328856229782104
I1005 06:58:00.765164 139498885195520 logging_writer.py:48] [47] global_step=47, grad_norm=0.025465872138738632, loss=0.13185787200927734
I1005 06:58:01.646506 139498893588224 logging_writer.py:48] [48] global_step=48, grad_norm=0.0549871101975441, loss=0.13197943568229675
I1005 06:58:02.649530 139498885195520 logging_writer.py:48] [49] global_step=49, grad_norm=0.028439097106456757, loss=0.12979759275913239
I1005 06:58:03.305941 139498893588224 logging_writer.py:48] [50] global_step=50, grad_norm=0.1082417443394661, loss=0.1318373829126358
I1005 06:58:04.066869 139498885195520 logging_writer.py:48] [51] global_step=51, grad_norm=0.21732768416404724, loss=0.13215389847755432
I1005 06:58:04.786648 139498893588224 logging_writer.py:48] [52] global_step=52, grad_norm=0.34352579712867737, loss=0.1344265341758728
I1005 06:58:05.601601 139498885195520 logging_writer.py:48] [53] global_step=53, grad_norm=0.6868507266044617, loss=0.13505497574806213
I1005 06:58:06.403828 139498893588224 logging_writer.py:48] [54] global_step=54, grad_norm=0.7830522656440735, loss=0.13714538514614105
I1005 06:58:07.202586 139498885195520 logging_writer.py:48] [55] global_step=55, grad_norm=0.6295455098152161, loss=0.13235220313072205
I1005 06:58:07.969413 139498893588224 logging_writer.py:48] [56] global_step=56, grad_norm=0.5361661314964294, loss=0.13121479749679565
I1005 06:58:08.850281 139498885195520 logging_writer.py:48] [57] global_step=57, grad_norm=0.4280746877193451, loss=0.1349484622478485
I1005 06:58:09.669783 139498893588224 logging_writer.py:48] [58] global_step=58, grad_norm=0.35163915157318115, loss=0.13657423853874207
I1005 06:58:10.542438 139498885195520 logging_writer.py:48] [59] global_step=59, grad_norm=0.3780214488506317, loss=0.1367708444595337
I1005 06:58:11.268708 139498893588224 logging_writer.py:48] [60] global_step=60, grad_norm=0.3423418700695038, loss=0.1372910439968109
I1005 06:58:12.050313 139498885195520 logging_writer.py:48] [61] global_step=61, grad_norm=0.33711403608322144, loss=0.13620133697986603
I1005 06:58:12.935422 139498893588224 logging_writer.py:48] [62] global_step=62, grad_norm=0.31910672783851624, loss=0.1361529529094696
I1005 06:58:13.756152 139498885195520 logging_writer.py:48] [63] global_step=63, grad_norm=0.29511603713035583, loss=0.13557258248329163
I1005 06:58:14.538077 139498893588224 logging_writer.py:48] [64] global_step=64, grad_norm=0.2837519347667694, loss=0.13635966181755066
I1005 06:58:15.466821 139498885195520 logging_writer.py:48] [65] global_step=65, grad_norm=0.2843170464038849, loss=0.13517636060714722
I1005 06:58:16.395093 139498893588224 logging_writer.py:48] [66] global_step=66, grad_norm=0.2758994996547699, loss=0.1358097344636917
I1005 06:58:17.115829 139498885195520 logging_writer.py:48] [67] global_step=67, grad_norm=0.2509460747241974, loss=0.13612134754657745
I1005 06:58:17.863349 139498893588224 logging_writer.py:48] [68] global_step=68, grad_norm=0.2576163411140442, loss=0.13665974140167236
I1005 06:58:18.590739 139498885195520 logging_writer.py:48] [69] global_step=69, grad_norm=0.27653130888938904, loss=0.1355624496936798
I1005 06:58:19.322298 139498893588224 logging_writer.py:48] [70] global_step=70, grad_norm=0.26809069514274597, loss=0.13597369194030762
I1005 06:58:20.114645 139498885195520 logging_writer.py:48] [71] global_step=71, grad_norm=0.2911142110824585, loss=0.1286265105009079
I1005 06:58:20.853227 139498893588224 logging_writer.py:48] [72] global_step=72, grad_norm=0.2798313498497009, loss=0.12920311093330383
I1005 06:58:21.656401 139498885195520 logging_writer.py:48] [73] global_step=73, grad_norm=0.21315130591392517, loss=0.13044512271881104
I1005 06:58:22.590466 139498893588224 logging_writer.py:48] [74] global_step=74, grad_norm=0.15310929715633392, loss=0.1288873851299286
I1005 06:58:23.331844 139498885195520 logging_writer.py:48] [75] global_step=75, grad_norm=0.10451126098632812, loss=0.13080942630767822
I1005 06:58:23.895342 139498893588224 logging_writer.py:48] [76] global_step=76, grad_norm=0.03963073343038559, loss=0.12605728209018707
I1005 06:58:24.507937 139498885195520 logging_writer.py:48] [77] global_step=77, grad_norm=0.07574687153100967, loss=0.12393809854984283
I1005 06:58:25.131707 139498893588224 logging_writer.py:48] [78] global_step=78, grad_norm=0.010460314340889454, loss=0.12474502623081207
I1005 06:58:25.737710 139498885195520 logging_writer.py:48] [79] global_step=79, grad_norm=0.014814174734055996, loss=0.12267382442951202
I1005 06:58:26.377631 139498893588224 logging_writer.py:48] [80] global_step=80, grad_norm=0.02353871800005436, loss=0.1249236986041069
I1005 06:58:26.892429 139498885195520 logging_writer.py:48] [81] global_step=81, grad_norm=0.011315761134028435, loss=0.12715576589107513
I1005 06:58:27.685118 139498893588224 logging_writer.py:48] [82] global_step=82, grad_norm=0.05012775957584381, loss=0.12879274785518646
I1005 06:58:28.227390 139498885195520 logging_writer.py:48] [83] global_step=83, grad_norm=0.17436860501766205, loss=0.1281067132949829
I1005 06:58:28.908867 139498893588224 logging_writer.py:48] [84] global_step=84, grad_norm=0.2538003623485565, loss=0.12636719644069672
I1005 06:58:29.462574 139498885195520 logging_writer.py:48] [85] global_step=85, grad_norm=0.3424586355686188, loss=0.1296916902065277
I1005 06:58:30.143373 139498893588224 logging_writer.py:48] [86] global_step=86, grad_norm=0.38558876514434814, loss=0.12873435020446777
I1005 06:58:30.768298 139498885195520 logging_writer.py:48] [87] global_step=87, grad_norm=0.43470460176467896, loss=0.12778271734714508
I1005 06:58:31.403486 139498893588224 logging_writer.py:48] [88] global_step=88, grad_norm=0.4507696032524109, loss=0.13104330003261566
I1005 06:58:32.030171 139498885195520 logging_writer.py:48] [89] global_step=89, grad_norm=0.43622714281082153, loss=0.12723270058631897
I1005 06:58:32.596894 139498893588224 logging_writer.py:48] [90] global_step=90, grad_norm=0.40067264437675476, loss=0.1304032802581787
I1005 06:58:33.135987 139498885195520 logging_writer.py:48] [91] global_step=91, grad_norm=0.29717692732810974, loss=0.12829235196113586
I1005 06:58:33.855396 139498893588224 logging_writer.py:48] [92] global_step=92, grad_norm=0.21804818511009216, loss=0.12753994762897491
I1005 06:58:34.392087 139498885195520 logging_writer.py:48] [93] global_step=93, grad_norm=0.1488468497991562, loss=0.12706665694713593
I1005 06:58:35.026055 139498893588224 logging_writer.py:48] [94] global_step=94, grad_norm=0.08307130634784698, loss=0.12452490627765656
I1005 06:58:35.809928 139498885195520 logging_writer.py:48] [95] global_step=95, grad_norm=0.008777391165494919, loss=0.13799089193344116
I1005 06:58:36.551555 139498893588224 logging_writer.py:48] [96] global_step=96, grad_norm=0.03728979825973511, loss=0.14235460758209229
I1005 06:58:37.245098 139498885195520 logging_writer.py:48] [97] global_step=97, grad_norm=0.021976351737976074, loss=0.14292526245117188
I1005 06:58:38.061116 139498893588224 logging_writer.py:48] [98] global_step=98, grad_norm=0.025630086660385132, loss=0.14280377328395844
I1005 06:58:38.814977 139498885195520 logging_writer.py:48] [99] global_step=99, grad_norm=0.025171738117933273, loss=0.14481648802757263
I1005 06:58:39.594320 139498893588224 logging_writer.py:48] [100] global_step=100, grad_norm=0.05351521447300911, loss=0.14303967356681824
I1005 07:03:36.914552 139498885195520 logging_writer.py:48] [500] global_step=500, grad_norm=0.04696367308497429, loss=0.12789449095726013
I1005 07:09:54.308525 139498893588224 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.012048834934830666, loss=0.1283872127532959
I1005 07:16:08.658753 139498885195520 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.025863876566290855, loss=0.12419205904006958
I1005 07:17:43.152162 139683934209856 spec.py:321] Evaluating on the training split.
I1005 07:20:52.738770 139683934209856 spec.py:333] Evaluating on the validation split.
I1005 07:24:11.058540 139683934209856 spec.py:349] Evaluating on the test split.
I1005 07:27:24.972801 139683934209856 submission_runner.py:381] Time since start: 2566.51s, 	Step: 1624, 	{'train/loss': 0.12428431241017468, 'validation/loss': 0.12550225826862504, 'validation/num_examples': 83274637, 'test/loss': 0.12788692631578946, 'test/num_examples': 95000000, 'score': 1227.3764970302582, 'total_duration': 2566.5101749897003, 'accumulated_submission_time': 1227.3764970302582, 'accumulated_eval_time': 1339.0805222988129, 'accumulated_logging_time': 0.031224727630615234}
I1005 07:27:24.988842 139498893588224 logging_writer.py:48] [1624] accumulated_eval_time=1339.080522, accumulated_logging_time=0.031225, accumulated_submission_time=1227.376497, global_step=1624, preemption_count=0, score=1227.376497, test/loss=0.127887, test/num_examples=95000000, total_duration=2566.510175, train/loss=0.124284, validation/loss=0.125502, validation/num_examples=83274637
I1005 07:31:55.641346 139498885195520 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.005974166560918093, loss=0.11811510473489761
I1005 07:38:11.180592 139498893588224 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0057663340121507645, loss=0.12958239018917084
I1005 07:44:31.337705 139498885195520 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.005588681902736425, loss=0.11864469200372696
I1005 07:47:25.660303 139683934209856 spec.py:321] Evaluating on the training split.
I1005 07:50:42.121353 139683934209856 spec.py:333] Evaluating on the validation split.
I1005 07:53:16.181984 139683934209856 spec.py:349] Evaluating on the test split.
I1005 07:56:13.596512 139683934209856 submission_runner.py:381] Time since start: 4295.13s, 	Step: 3231, 	{'train/loss': 0.12503697137412784, 'validation/loss': 0.12519329264683554, 'validation/num_examples': 83274637, 'test/loss': 0.12762284210526315, 'test/num_examples': 95000000, 'score': 2428.019098997116, 'total_duration': 4295.133886575699, 'accumulated_submission_time': 2428.019098997116, 'accumulated_eval_time': 1867.016678571701, 'accumulated_logging_time': 0.054378509521484375}
I1005 07:56:13.614473 139498893588224 logging_writer.py:48] [3231] accumulated_eval_time=1867.016679, accumulated_logging_time=0.054379, accumulated_submission_time=2428.019099, global_step=3231, preemption_count=0, score=2428.019099, test/loss=0.127623, test/num_examples=95000000, total_duration=4295.133887, train/loss=0.125037, validation/loss=0.125193, validation/num_examples=83274637
I1005 07:59:22.271547 139498885195520 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.025279484689235687, loss=0.12698052823543549
I1005 08:05:39.600449 139498893588224 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.008240967057645321, loss=0.12776592373847961
I1005 08:12:02.041905 139498885195520 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.007422431837767363, loss=0.13322624564170837
I1005 08:16:13.794046 139683934209856 spec.py:321] Evaluating on the training split.
I1005 08:19:21.306234 139683934209856 spec.py:333] Evaluating on the validation split.
I1005 08:22:03.890627 139683934209856 spec.py:349] Evaluating on the test split.
I1005 08:24:54.975880 139683934209856 submission_runner.py:381] Time since start: 6016.51s, 	Step: 4831, 	{'train/loss': 0.12356317868022798, 'validation/loss': 0.12423259197155072, 'validation/num_examples': 83274637, 'test/loss': 0.1265671052631579, 'test/num_examples': 95000000, 'score': 3628.1703431606293, 'total_duration': 6016.513259410858, 'accumulated_submission_time': 3628.1703431606293, 'accumulated_eval_time': 2388.198466062546, 'accumulated_logging_time': 0.07952880859375}
I1005 08:24:54.998265 139498893588224 logging_writer.py:48] [4831] accumulated_eval_time=2388.198466, accumulated_logging_time=0.079529, accumulated_submission_time=3628.170343, global_step=4831, preemption_count=0, score=3628.170343, test/loss=0.126567, test/num_examples=95000000, total_duration=6016.513259, train/loss=0.123563, validation/loss=0.124233, validation/num_examples=83274637
I1005 08:26:48.194389 139498885195520 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.005036066751927137, loss=0.11802113056182861
I1005 08:33:00.671052 139498893588224 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.012698504142463207, loss=0.1246006190776825
I1005 08:39:18.625880 139498885195520 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.008838942274451256, loss=0.11959514021873474
I1005 08:44:54.977068 139683934209856 spec.py:321] Evaluating on the training split.
I1005 08:47:45.622569 139683934209856 spec.py:333] Evaluating on the validation split.
I1005 08:50:18.640483 139683934209856 spec.py:349] Evaluating on the test split.
I1005 08:53:08.538530 139683934209856 submission_runner.py:381] Time since start: 7710.08s, 	Step: 6444, 	{'train/loss': 0.12283182444062622, 'validation/loss': 0.12407876362162948, 'validation/num_examples': 83274637, 'test/loss': 0.12640425263157895, 'test/num_examples': 95000000, 'score': 4828.120344161987, 'total_duration': 7710.075885772705, 'accumulated_submission_time': 4828.120344161987, 'accumulated_eval_time': 2881.759857416153, 'accumulated_logging_time': 0.10914182662963867}
I1005 08:53:08.557762 139498893588224 logging_writer.py:48] [6444] accumulated_eval_time=2881.759857, accumulated_logging_time=0.109142, accumulated_submission_time=4828.120344, global_step=6444, preemption_count=0, score=4828.120344, test/loss=0.126404, test/num_examples=95000000, total_duration=7710.075886, train/loss=0.122832, validation/loss=0.124079, validation/num_examples=83274637
I1005 08:53:33.732710 139498885195520 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.013258389197289944, loss=0.12057743966579437
I1005 08:59:51.366095 139498893588224 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.006446346640586853, loss=0.12554730474948883
I1005 09:06:14.127437 139498885195520 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.005742711015045643, loss=0.11967114359140396
I1005 09:12:32.253186 139683934209856 spec.py:321] Evaluating on the training split.
I1005 09:14:56.127867 139683934209856 spec.py:333] Evaluating on the validation split.
I1005 09:16:57.785821 139683934209856 spec.py:349] Evaluating on the test split.
I1005 09:19:27.456366 139683934209856 submission_runner.py:381] Time since start: 9288.99s, 	Step: 8000, 	{'train/loss': 0.12293043076617163, 'validation/loss': 0.12381510591274028, 'validation/num_examples': 83274637, 'test/loss': 0.12610651578947368, 'test/num_examples': 95000000, 'score': 5991.781370401382, 'total_duration': 9288.993755817413, 'accumulated_submission_time': 5991.781370401382, 'accumulated_eval_time': 3296.963000059128, 'accumulated_logging_time': 0.1418161392211914}
I1005 09:19:27.473029 139498893588224 logging_writer.py:48] [8000] accumulated_eval_time=3296.963000, accumulated_logging_time=0.141816, accumulated_submission_time=5991.781370, global_step=8000, preemption_count=0, score=5991.781370, test/loss=0.126107, test/num_examples=95000000, total_duration=9288.993756, train/loss=0.122930, validation/loss=0.123815, validation/num_examples=83274637
I1005 09:19:27.490139 139498885195520 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5991.781370
I1005 09:19:33.404393 139683934209856 checkpoints.py:490] Saving checkpoint at step: 8000
I1005 09:20:08.179615 139683934209856 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_13/criteo1tb_jax/trial_1/checkpoint_8000
I1005 09:20:08.498111 139683934209856 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_13/criteo1tb_jax/trial_1/checkpoint_8000.
I1005 09:20:08.880100 139683934209856 submission_runner.py:549] Tuning trial 1/1
I1005 09:20:08.880435 139683934209856 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1005 09:20:08.881040 139683934209856 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 1.794940300707547, 'validation/loss': 1.797035800948613, 'validation/num_examples': 83274637, 'test/loss': 1.794517389473684, 'test/num_examples': 95000000, 'score': 27.356279373168945, 'total_duration': 784.6162300109863, 'accumulated_submission_time': 27.356279373168945, 'accumulated_eval_time': 757.2599132061005, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1624, {'train/loss': 0.12428431241017468, 'validation/loss': 0.12550225826862504, 'validation/num_examples': 83274637, 'test/loss': 0.12788692631578946, 'test/num_examples': 95000000, 'score': 1227.3764970302582, 'total_duration': 2566.5101749897003, 'accumulated_submission_time': 1227.3764970302582, 'accumulated_eval_time': 1339.0805222988129, 'accumulated_logging_time': 0.031224727630615234, 'global_step': 1624, 'preemption_count': 0}), (3231, {'train/loss': 0.12503697137412784, 'validation/loss': 0.12519329264683554, 'validation/num_examples': 83274637, 'test/loss': 0.12762284210526315, 'test/num_examples': 95000000, 'score': 2428.019098997116, 'total_duration': 4295.133886575699, 'accumulated_submission_time': 2428.019098997116, 'accumulated_eval_time': 1867.016678571701, 'accumulated_logging_time': 0.054378509521484375, 'global_step': 3231, 'preemption_count': 0}), (4831, {'train/loss': 0.12356317868022798, 'validation/loss': 0.12423259197155072, 'validation/num_examples': 83274637, 'test/loss': 0.1265671052631579, 'test/num_examples': 95000000, 'score': 3628.1703431606293, 'total_duration': 6016.513259410858, 'accumulated_submission_time': 3628.1703431606293, 'accumulated_eval_time': 2388.198466062546, 'accumulated_logging_time': 0.07952880859375, 'global_step': 4831, 'preemption_count': 0}), (6444, {'train/loss': 0.12283182444062622, 'validation/loss': 0.12407876362162948, 'validation/num_examples': 83274637, 'test/loss': 0.12640425263157895, 'test/num_examples': 95000000, 'score': 4828.120344161987, 'total_duration': 7710.075885772705, 'accumulated_submission_time': 4828.120344161987, 'accumulated_eval_time': 2881.759857416153, 'accumulated_logging_time': 0.10914182662963867, 'global_step': 6444, 'preemption_count': 0}), (8000, {'train/loss': 0.12293043076617163, 'validation/loss': 0.12381510591274028, 'validation/num_examples': 83274637, 'test/loss': 0.12610651578947368, 'test/num_examples': 95000000, 'score': 5991.781370401382, 'total_duration': 9288.993755817413, 'accumulated_submission_time': 5991.781370401382, 'accumulated_eval_time': 3296.963000059128, 'accumulated_logging_time': 0.1418161392211914, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1005 09:20:08.881207 139683934209856 submission_runner.py:552] Timing: 5991.781370401382
I1005 09:20:08.881295 139683934209856 submission_runner.py:554] Total number of evals: 6
I1005 09:20:08.881375 139683934209856 submission_runner.py:555] ====================
I1005 09:20:08.881531 139683934209856 submission_runner.py:625] Final criteo1tb score: 5991.781370401382
