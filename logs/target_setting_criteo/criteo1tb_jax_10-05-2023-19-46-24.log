python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_18 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-05-2023-19-46-24.log
2023-10-05 19:46:29.142586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1005 19:46:46.088299 139859622512448 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_18/criteo1tb_jax.
I1005 19:46:47.680665 139859622512448 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1005 19:46:47.681618 139859622512448 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1005 19:46:47.681818 139859622512448 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1005 19:46:47.687593 139859622512448 submission_runner.py:507] Using RNG seed 2777391419
I1005 19:46:53.371614 139859622512448 submission_runner.py:516] --- Tuning run 1/1 ---
I1005 19:46:53.371859 139859622512448 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_18/criteo1tb_jax/trial_1.
I1005 19:46:53.372201 139859622512448 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_18/criteo1tb_jax/trial_1/hparams.json.
I1005 19:46:53.560110 139859622512448 submission_runner.py:191] Initializing dataset.
I1005 19:46:53.560399 139859622512448 submission_runner.py:198] Initializing model.
I1005 19:46:59.545065 139859622512448 submission_runner.py:232] Initializing optimizer.
I1005 19:47:02.790045 139859622512448 submission_runner.py:239] Initializing metrics bundle.
I1005 19:47:02.790360 139859622512448 submission_runner.py:257] Initializing checkpoint and logger.
I1005 19:47:02.791914 139859622512448 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_18/criteo1tb_jax/trial_1 with prefix checkpoint_
I1005 19:47:02.792092 139859622512448 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_18/criteo1tb_jax/trial_1/meta_data_0.json.
I1005 19:47:02.792329 139859622512448 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1005 19:47:02.792394 139859622512448 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1005 19:47:03.680131 139859622512448 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_18/criteo1tb_jax/trial_1/flags_0.json.
I1005 19:47:03.774929 139859622512448 submission_runner.py:290] Starting training loop.
I1005 19:47:31.437148 139695766882048 logging_writer.py:48] [0] global_step=0, grad_norm=7.174249172210693, loss=0.7552672624588013
I1005 19:47:31.447715 139859622512448 spec.py:321] Evaluating on the training split.
I1005 19:51:34.166918 139859622512448 spec.py:333] Evaluating on the validation split.
I1005 19:55:35.964700 139859622512448 spec.py:349] Evaluating on the test split.
I1005 20:00:05.866830 139859622512448 submission_runner.py:381] Time since start: 782.09s, 	Step: 1, 	{'train/loss': 0.7563688170235112, 'validation/loss': 0.7599762698455234, 'validation/num_examples': 83274637, 'test/loss': 0.7573673263157895, 'test/num_examples': 95000000, 'score': 27.67276692390442, 'total_duration': 782.0918402671814, 'accumulated_submission_time': 27.67276692390442, 'accumulated_eval_time': 754.4190254211426, 'accumulated_logging_time': 0}
I1005 20:00:05.885677 139678292367104 logging_writer.py:48] [1] accumulated_eval_time=754.419025, accumulated_logging_time=0, accumulated_submission_time=27.672767, global_step=1, preemption_count=0, score=27.672767, test/loss=0.757367, test/num_examples=95000000, total_duration=782.091840, train/loss=0.756369, validation/loss=0.759976, validation/num_examples=83274637
I1005 20:00:06.001278 139678283974400 logging_writer.py:48] [1] global_step=1, grad_norm=7.162508964538574, loss=0.7553117275238037
I1005 20:00:06.108562 139678292367104 logging_writer.py:48] [2] global_step=2, grad_norm=6.6395263671875, loss=0.6716408729553223
I1005 20:00:06.215196 139678283974400 logging_writer.py:48] [3] global_step=3, grad_norm=5.726387023925781, loss=0.5386462807655334
I1005 20:00:06.318930 139678292367104 logging_writer.py:48] [4] global_step=4, grad_norm=4.281249523162842, loss=0.3926011919975281
I1005 20:00:06.423467 139678283974400 logging_writer.py:48] [5] global_step=5, grad_norm=2.6399621963500977, loss=0.27356377243995667
I1005 20:00:06.528510 139678292367104 logging_writer.py:48] [6] global_step=6, grad_norm=1.2305477857589722, loss=0.20413018763065338
I1005 20:00:06.631669 139678283974400 logging_writer.py:48] [7] global_step=7, grad_norm=0.2632448077201843, loss=0.176116943359375
I1005 20:00:06.736648 139678292367104 logging_writer.py:48] [8] global_step=8, grad_norm=0.5132812261581421, loss=0.17746491730213165
I1005 20:00:06.840941 139678283974400 logging_writer.py:48] [9] global_step=9, grad_norm=1.0094887018203735, loss=0.20296576619148254
I1005 20:00:06.944017 139678292367104 logging_writer.py:48] [10] global_step=10, grad_norm=1.2608380317687988, loss=0.22017790377140045
I1005 20:00:07.047612 139678283974400 logging_writer.py:48] [11] global_step=11, grad_norm=1.4650163650512695, loss=0.2426348477602005
I1005 20:00:07.150921 139678292367104 logging_writer.py:48] [12] global_step=12, grad_norm=1.5635656118392944, loss=0.2558625340461731
I1005 20:00:07.254364 139678283974400 logging_writer.py:48] [13] global_step=13, grad_norm=1.6151907444000244, loss=0.2636305093765259
I1005 20:00:07.358059 139678292367104 logging_writer.py:48] [14] global_step=14, grad_norm=1.5420771837234497, loss=0.25352922081947327
I1005 20:00:07.462198 139678283974400 logging_writer.py:48] [15] global_step=15, grad_norm=1.4466127157211304, loss=0.2391572743654251
I1005 20:00:07.567001 139678292367104 logging_writer.py:48] [16] global_step=16, grad_norm=1.3242688179016113, loss=0.22333388030529022
I1005 20:00:07.671169 139678283974400 logging_writer.py:48] [17] global_step=17, grad_norm=1.1452215909957886, loss=0.2052750289440155
I1005 20:00:07.775407 139678292367104 logging_writer.py:48] [18] global_step=18, grad_norm=0.8392099142074585, loss=0.18004344403743744
I1005 20:00:07.880988 139678283974400 logging_writer.py:48] [19] global_step=19, grad_norm=0.38134029507637024, loss=0.15651968121528625
I1005 20:00:07.985839 139678292367104 logging_writer.py:48] [20] global_step=20, grad_norm=0.1361466348171234, loss=0.15006983280181885
I1005 20:00:08.090683 139678283974400 logging_writer.py:48] [21] global_step=21, grad_norm=0.34290799498558044, loss=0.1503155380487442
I1005 20:00:08.195380 139678292367104 logging_writer.py:48] [22] global_step=22, grad_norm=0.34480583667755127, loss=0.14696700870990753
I1005 20:00:08.300953 139678283974400 logging_writer.py:48] [23] global_step=23, grad_norm=0.1389741450548172, loss=0.14304248988628387
I1005 20:00:08.406193 139678292367104 logging_writer.py:48] [24] global_step=24, grad_norm=0.10790172964334488, loss=0.14188000559806824
I1005 20:00:08.510831 139678283974400 logging_writer.py:48] [25] global_step=25, grad_norm=0.13541167974472046, loss=0.14017778635025024
I1005 20:00:08.615153 139678292367104 logging_writer.py:48] [26] global_step=26, grad_norm=0.08537036180496216, loss=0.13710954785346985
I1005 20:00:08.719364 139678283974400 logging_writer.py:48] [27] global_step=27, grad_norm=0.06763078272342682, loss=0.1385630965232849
I1005 20:00:08.993449 139678292367104 logging_writer.py:48] [28] global_step=28, grad_norm=0.031003985553979874, loss=0.13611836731433868
I1005 20:00:09.637718 139678283974400 logging_writer.py:48] [29] global_step=29, grad_norm=0.05778941139578819, loss=0.13845376670360565
I1005 20:00:10.424118 139678292367104 logging_writer.py:48] [30] global_step=30, grad_norm=0.059708621352910995, loss=0.13500753045082092
I1005 20:00:11.248239 139678283974400 logging_writer.py:48] [31] global_step=31, grad_norm=0.12691913545131683, loss=0.13802114129066467
I1005 20:00:12.066807 139678292367104 logging_writer.py:48] [32] global_step=32, grad_norm=0.21660563349723816, loss=0.1366521716117859
I1005 20:00:12.823789 139678283974400 logging_writer.py:48] [33] global_step=33, grad_norm=0.19496199488639832, loss=0.13712427020072937
I1005 20:00:13.703968 139678292367104 logging_writer.py:48] [34] global_step=34, grad_norm=0.17439395189285278, loss=0.13633625209331512
I1005 20:00:14.517824 139678283974400 logging_writer.py:48] [35] global_step=35, grad_norm=0.10798558592796326, loss=0.13527937233448029
I1005 20:00:15.247476 139678292367104 logging_writer.py:48] [36] global_step=36, grad_norm=0.09565995633602142, loss=0.13455577194690704
I1005 20:00:16.134290 139678283974400 logging_writer.py:48] [37] global_step=37, grad_norm=0.0615866556763649, loss=0.13224218785762787
I1005 20:00:16.967359 139678292367104 logging_writer.py:48] [38] global_step=38, grad_norm=0.04007803276181221, loss=0.14131605625152588
I1005 20:00:17.764676 139678283974400 logging_writer.py:48] [39] global_step=39, grad_norm=0.046001989394426346, loss=0.14266051352024078
I1005 20:00:18.505865 139678292367104 logging_writer.py:48] [40] global_step=40, grad_norm=0.11629704385995865, loss=0.1415880173444748
I1005 20:00:19.314954 139678283974400 logging_writer.py:48] [41] global_step=41, grad_norm=0.2068597972393036, loss=0.14094512164592743
I1005 20:00:20.187003 139678292367104 logging_writer.py:48] [42] global_step=42, grad_norm=0.43048906326293945, loss=0.14331158995628357
I1005 20:00:21.017886 139678283974400 logging_writer.py:48] [43] global_step=43, grad_norm=0.5415489077568054, loss=0.14801852405071259
I1005 20:00:21.697412 139678292367104 logging_writer.py:48] [44] global_step=44, grad_norm=0.4108879566192627, loss=0.14063134789466858
I1005 20:00:22.533553 139678283974400 logging_writer.py:48] [45] global_step=45, grad_norm=0.25938597321510315, loss=0.14021700620651245
I1005 20:00:23.328864 139678292367104 logging_writer.py:48] [46] global_step=46, grad_norm=0.14822092652320862, loss=0.1408119648694992
I1005 20:00:24.100888 139678283974400 logging_writer.py:48] [47] global_step=47, grad_norm=0.05794654041528702, loss=0.13690191507339478
I1005 20:00:24.859687 139678292367104 logging_writer.py:48] [48] global_step=48, grad_norm=0.03536345437169075, loss=0.13670271635055542
I1005 20:00:25.691996 139678283974400 logging_writer.py:48] [49] global_step=49, grad_norm=0.07278269529342651, loss=0.13816596567630768
I1005 20:00:26.589579 139678292367104 logging_writer.py:48] [50] global_step=50, grad_norm=0.11967470496892929, loss=0.13745199143886566
I1005 20:00:27.362756 139678283974400 logging_writer.py:48] [51] global_step=51, grad_norm=0.1446719914674759, loss=0.13494911789894104
I1005 20:00:28.245828 139678292367104 logging_writer.py:48] [52] global_step=52, grad_norm=0.20716592669487, loss=0.1352682113647461
I1005 20:00:29.021477 139678283974400 logging_writer.py:48] [53] global_step=53, grad_norm=0.29524973034858704, loss=0.13812515139579773
I1005 20:00:29.819801 139678292367104 logging_writer.py:48] [54] global_step=54, grad_norm=0.40955010056495667, loss=0.1387873888015747
I1005 20:00:30.574016 139678283974400 logging_writer.py:48] [55] global_step=55, grad_norm=0.43632909655570984, loss=0.14206957817077637
I1005 20:00:31.334967 139678292367104 logging_writer.py:48] [56] global_step=56, grad_norm=0.3782082796096802, loss=0.13630664348602295
I1005 20:00:32.200654 139678283974400 logging_writer.py:48] [57] global_step=57, grad_norm=0.2787313163280487, loss=0.13491074740886688
I1005 20:00:32.944610 139678292367104 logging_writer.py:48] [58] global_step=58, grad_norm=0.16055992245674133, loss=0.13152951002120972
I1005 20:00:33.673604 139678283974400 logging_writer.py:48] [59] global_step=59, grad_norm=0.08390755951404572, loss=0.13352032005786896
I1005 20:00:34.353130 139678292367104 logging_writer.py:48] [60] global_step=60, grad_norm=0.051804039627313614, loss=0.13285474479198456
I1005 20:00:35.168517 139678283974400 logging_writer.py:48] [61] global_step=61, grad_norm=0.04863507300615311, loss=0.13409483432769775
I1005 20:00:35.885250 139678292367104 logging_writer.py:48] [62] global_step=62, grad_norm=0.04013741388916969, loss=0.13327094912528992
I1005 20:00:36.677146 139678283974400 logging_writer.py:48] [63] global_step=63, grad_norm=0.03075205720961094, loss=0.13331328332424164
I1005 20:00:37.531149 139678292367104 logging_writer.py:48] [64] global_step=64, grad_norm=0.01735381968319416, loss=0.1333707869052887
I1005 20:00:38.353396 139678283974400 logging_writer.py:48] [65] global_step=65, grad_norm=0.015613487921655178, loss=0.13028261065483093
I1005 20:00:39.210304 139678292367104 logging_writer.py:48] [66] global_step=66, grad_norm=0.022496601566672325, loss=0.1289414018392563
I1005 20:00:40.064406 139678283974400 logging_writer.py:48] [67] global_step=67, grad_norm=0.0072346823289990425, loss=0.13089996576309204
I1005 20:00:40.819140 139678292367104 logging_writer.py:48] [68] global_step=68, grad_norm=0.014077266678214073, loss=0.1317908614873886
I1005 20:00:41.789032 139678283974400 logging_writer.py:48] [69] global_step=69, grad_norm=0.029349680989980698, loss=0.13194122910499573
I1005 20:00:42.662284 139678292367104 logging_writer.py:48] [70] global_step=70, grad_norm=0.03299489617347717, loss=0.1303487867116928
I1005 20:00:43.395098 139678283974400 logging_writer.py:48] [71] global_step=71, grad_norm=0.03219200670719147, loss=0.13176435232162476
I1005 20:00:44.136253 139678292367104 logging_writer.py:48] [72] global_step=72, grad_norm=0.03119690530002117, loss=0.1301918923854828
I1005 20:00:45.061969 139678283974400 logging_writer.py:48] [73] global_step=73, grad_norm=0.0327092744410038, loss=0.13017383217811584
I1005 20:00:45.813037 139678292367104 logging_writer.py:48] [74] global_step=74, grad_norm=0.038364559412002563, loss=0.12909221649169922
I1005 20:00:46.691548 139678283974400 logging_writer.py:48] [75] global_step=75, grad_norm=0.04809410870075226, loss=0.13005614280700684
I1005 20:00:47.530279 139678292367104 logging_writer.py:48] [76] global_step=76, grad_norm=0.046125177294015884, loss=0.12566950917243958
I1005 20:00:48.357049 139678283974400 logging_writer.py:48] [77] global_step=77, grad_norm=0.051493123173713684, loss=0.12486454844474792
I1005 20:00:49.103677 139678292367104 logging_writer.py:48] [78] global_step=78, grad_norm=0.08924166858196259, loss=0.12689527869224548
I1005 20:00:49.879408 139678283974400 logging_writer.py:48] [79] global_step=79, grad_norm=0.1265811175107956, loss=0.1278592199087143
I1005 20:00:50.689885 139678292367104 logging_writer.py:48] [80] global_step=80, grad_norm=0.12588998675346375, loss=0.12790493667125702
I1005 20:00:51.503069 139678283974400 logging_writer.py:48] [81] global_step=81, grad_norm=0.1402667909860611, loss=0.12534552812576294
I1005 20:00:52.323291 139678292367104 logging_writer.py:48] [82] global_step=82, grad_norm=0.13641294836997986, loss=0.1268291175365448
I1005 20:00:53.099430 139678283974400 logging_writer.py:48] [83] global_step=83, grad_norm=0.11506067961454391, loss=0.1262979358434677
I1005 20:00:53.878221 139678292367104 logging_writer.py:48] [84] global_step=84, grad_norm=0.0894232913851738, loss=0.12556564807891846
I1005 20:00:54.694214 139678283974400 logging_writer.py:48] [85] global_step=85, grad_norm=0.06313920021057129, loss=0.12562669813632965
I1005 20:00:55.531599 139678292367104 logging_writer.py:48] [86] global_step=86, grad_norm=0.040834374725818634, loss=0.12336411327123642
I1005 20:00:56.255925 139678283974400 logging_writer.py:48] [87] global_step=87, grad_norm=0.01234088372439146, loss=0.12484434992074966
I1005 20:00:57.097355 139678292367104 logging_writer.py:48] [88] global_step=88, grad_norm=0.0215217974036932, loss=0.1278160959482193
I1005 20:00:57.947902 139678283974400 logging_writer.py:48] [89] global_step=89, grad_norm=0.07277106493711472, loss=0.12591731548309326
I1005 20:00:58.701377 139678292367104 logging_writer.py:48] [90] global_step=90, grad_norm=0.1006602942943573, loss=0.12424484640359879
I1005 20:00:59.540425 139678283974400 logging_writer.py:48] [91] global_step=91, grad_norm=0.11127813905477524, loss=0.1251257061958313
I1005 20:01:00.329070 139678292367104 logging_writer.py:48] [92] global_step=92, grad_norm=0.13445577025413513, loss=0.12514758110046387
I1005 20:01:00.997734 139678283974400 logging_writer.py:48] [93] global_step=93, grad_norm=0.139823317527771, loss=0.1252724975347519
I1005 20:01:02.008020 139678292367104 logging_writer.py:48] [94] global_step=94, grad_norm=0.14261867105960846, loss=0.12647704780101776
I1005 20:01:02.791271 139678283974400 logging_writer.py:48] [95] global_step=95, grad_norm=0.16190044581890106, loss=0.13467741012573242
I1005 20:01:03.411740 139678292367104 logging_writer.py:48] [96] global_step=96, grad_norm=0.1990746259689331, loss=0.14207567274570465
I1005 20:01:04.299739 139678283974400 logging_writer.py:48] [97] global_step=97, grad_norm=0.25685322284698486, loss=0.1415351927280426
I1005 20:01:05.089402 139678292367104 logging_writer.py:48] [98] global_step=98, grad_norm=0.2816215753555298, loss=0.14159300923347473
I1005 20:01:05.854867 139678283974400 logging_writer.py:48] [99] global_step=99, grad_norm=0.23708172142505646, loss=0.14053946733474731
I1005 20:01:06.669404 139678292367104 logging_writer.py:48] [100] global_step=100, grad_norm=0.18943361937999725, loss=0.13755421340465546
I1005 20:06:09.790705 139678283974400 logging_writer.py:48] [500] global_step=500, grad_norm=0.06278694421052933, loss=0.12408497184515
I1005 20:12:27.645190 139678292367104 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.018162082880735397, loss=0.12209254503250122
I1005 20:18:46.512219 139678283974400 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.015113258734345436, loss=0.12266198545694351
I1005 20:20:06.247137 139859622512448 spec.py:321] Evaluating on the training split.
I1005 20:23:26.586769 139859622512448 spec.py:333] Evaluating on the validation split.
I1005 20:26:43.519646 139859622512448 spec.py:349] Evaluating on the test split.
I1005 20:30:04.556363 139859622512448 submission_runner.py:381] Time since start: 2580.78s, 	Step: 1609, 	{'train/loss': 0.12281913277488085, 'validation/loss': 0.1254450019397863, 'validation/num_examples': 83274637, 'test/loss': 0.12774282105263157, 'test/num_examples': 95000000, 'score': 1228.0031430721283, 'total_duration': 2580.7813634872437, 'accumulated_submission_time': 1228.0031430721283, 'accumulated_eval_time': 1352.7281999588013, 'accumulated_logging_time': 0.0278170108795166}
I1005 20:30:04.574577 139678292367104 logging_writer.py:48] [1609] accumulated_eval_time=1352.728200, accumulated_logging_time=0.027817, accumulated_submission_time=1228.003143, global_step=1609, preemption_count=0, score=1228.003143, test/loss=0.127743, test/num_examples=95000000, total_duration=2580.781363, train/loss=0.122819, validation/loss=0.125445, validation/num_examples=83274637
I1005 20:34:42.663447 139678283974400 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.06909465044736862, loss=0.13581286370754242
I1005 20:40:56.975270 139678292367104 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.008650027215480804, loss=0.12280328571796417
I1005 20:47:15.578892 139678283974400 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.008009800687432289, loss=0.11916809529066086
I1005 20:50:05.079314 139859622512448 spec.py:321] Evaluating on the training split.
I1005 20:53:17.914937 139859622512448 spec.py:333] Evaluating on the validation split.
I1005 20:56:02.679373 139859622512448 spec.py:349] Evaluating on the test split.
I1005 20:59:02.343595 139859622512448 submission_runner.py:381] Time since start: 4318.57s, 	Step: 3230, 	{'train/loss': 0.12306942729829992, 'validation/loss': 0.12466107777809947, 'validation/num_examples': 83274637, 'test/loss': 0.1270133052631579, 'test/num_examples': 95000000, 'score': 2428.47815489769, 'total_duration': 4318.568609952927, 'accumulated_submission_time': 2428.47815489769, 'accumulated_eval_time': 1889.9924383163452, 'accumulated_logging_time': 0.053708791732788086}
I1005 20:59:02.361949 139678292367104 logging_writer.py:48] [3230] accumulated_eval_time=1889.992438, accumulated_logging_time=0.053709, accumulated_submission_time=2428.478155, global_step=3230, preemption_count=0, score=2428.478155, test/loss=0.127013, test/num_examples=95000000, total_duration=4318.568610, train/loss=0.123069, validation/loss=0.124661, validation/num_examples=83274637
I1005 21:02:08.731459 139678283974400 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.008266117423772812, loss=0.12573528289794922
I1005 21:08:23.991114 139678292367104 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.018973136320710182, loss=0.12099918723106384
I1005 21:14:33.932173 139678283974400 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.006901897024363279, loss=0.1337902843952179
I1005 21:19:02.474247 139859622512448 spec.py:321] Evaluating on the training split.
I1005 21:22:05.036955 139859622512448 spec.py:333] Evaluating on the validation split.
I1005 21:24:35.878164 139859622512448 spec.py:349] Evaluating on the test split.
I1005 21:27:38.773573 139859622512448 submission_runner.py:381] Time since start: 6035.00s, 	Step: 4864, 	{'train/loss': 0.12382498927086404, 'validation/loss': 0.12430601168516651, 'validation/num_examples': 83274637, 'test/loss': 0.12670932631578946, 'test/num_examples': 95000000, 'score': 3628.560966730118, 'total_duration': 6034.998579740524, 'accumulated_submission_time': 3628.560966730118, 'accumulated_eval_time': 2406.291713953018, 'accumulated_logging_time': 0.07950925827026367}
I1005 21:27:38.789420 139678292367104 logging_writer.py:48] [4864] accumulated_eval_time=2406.291714, accumulated_logging_time=0.079509, accumulated_submission_time=3628.560967, global_step=4864, preemption_count=0, score=3628.560967, test/loss=0.126709, test/num_examples=95000000, total_duration=6034.998580, train/loss=0.123825, validation/loss=0.124306, validation/num_examples=83274637
I1005 21:29:05.639880 139678283974400 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.022519301623106003, loss=0.12277626991271973
I1005 21:35:16.552242 139678292367104 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.011570194736123085, loss=0.12088988721370697
I1005 21:41:37.290951 139678283974400 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.005034514237195253, loss=0.12628917396068573
I1005 21:47:39.153141 139859622512448 spec.py:321] Evaluating on the training split.
I1005 21:50:31.460803 139859622512448 spec.py:333] Evaluating on the validation split.
I1005 21:52:58.566440 139859622512448 spec.py:349] Evaluating on the test split.
I1005 21:55:46.462471 139859622512448 submission_runner.py:381] Time since start: 7722.69s, 	Step: 6485, 	{'train/loss': 0.12192281087239583, 'validation/loss': 0.12386604579255026, 'validation/num_examples': 83274637, 'test/loss': 0.12614091578947367, 'test/num_examples': 95000000, 'score': 4828.89098906517, 'total_duration': 7722.68746304512, 'accumulated_submission_time': 4828.89098906517, 'accumulated_eval_time': 2893.600976705551, 'accumulated_logging_time': 0.10667586326599121}
I1005 21:55:46.480448 139678292367104 logging_writer.py:48] [6485] accumulated_eval_time=2893.600977, accumulated_logging_time=0.106676, accumulated_submission_time=4828.890989, global_step=6485, preemption_count=0, score=4828.890989, test/loss=0.126141, test/num_examples=95000000, total_duration=7722.687463, train/loss=0.121923, validation/loss=0.123866, validation/num_examples=83274637
I1005 21:55:48.024962 139678283974400 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0040069203823804855, loss=0.12457520514726639
I1005 22:01:58.576362 139678292367104 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.009451501071453094, loss=0.12064279615879059
I1005 22:08:20.953321 139678283974400 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.006094717886298895, loss=0.12225014716386795
I1005 22:14:35.545072 139859622512448 spec.py:321] Evaluating on the training split.
I1005 22:17:08.894484 139859622512448 spec.py:333] Evaluating on the validation split.
I1005 22:19:11.614735 139859622512448 spec.py:349] Evaluating on the test split.
I1005 22:21:41.932833 139859622512448 submission_runner.py:381] Time since start: 9278.16s, 	Step: 8000, 	{'train/loss': 0.12235325987234055, 'validation/loss': 0.12373945262589377, 'validation/num_examples': 83274637, 'test/loss': 0.12606724210526316, 'test/num_examples': 95000000, 'score': 5957.923988342285, 'total_duration': 9278.157832145691, 'accumulated_submission_time': 5957.923988342285, 'accumulated_eval_time': 3319.9887013435364, 'accumulated_logging_time': 0.13557958602905273}
I1005 22:21:41.950902 139678292367104 logging_writer.py:48] [8000] accumulated_eval_time=3319.988701, accumulated_logging_time=0.135580, accumulated_submission_time=5957.923988, global_step=8000, preemption_count=0, score=5957.923988, test/loss=0.126067, test/num_examples=95000000, total_duration=9278.157832, train/loss=0.122353, validation/loss=0.123739, validation/num_examples=83274637
I1005 22:21:41.968394 139678283974400 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5957.923988
I1005 22:21:47.998700 139859622512448 checkpoints.py:490] Saving checkpoint at step: 8000
I1005 22:22:23.572417 139859622512448 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_18/criteo1tb_jax/trial_1/checkpoint_8000
I1005 22:22:23.866530 139859622512448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_18/criteo1tb_jax/trial_1/checkpoint_8000.
I1005 22:22:24.302840 139859622512448 submission_runner.py:549] Tuning trial 1/1
I1005 22:22:24.303107 139859622512448 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1005 22:22:24.304154 139859622512448 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.7563688170235112, 'validation/loss': 0.7599762698455234, 'validation/num_examples': 83274637, 'test/loss': 0.7573673263157895, 'test/num_examples': 95000000, 'score': 27.67276692390442, 'total_duration': 782.0918402671814, 'accumulated_submission_time': 27.67276692390442, 'accumulated_eval_time': 754.4190254211426, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1609, {'train/loss': 0.12281913277488085, 'validation/loss': 0.1254450019397863, 'validation/num_examples': 83274637, 'test/loss': 0.12774282105263157, 'test/num_examples': 95000000, 'score': 1228.0031430721283, 'total_duration': 2580.7813634872437, 'accumulated_submission_time': 1228.0031430721283, 'accumulated_eval_time': 1352.7281999588013, 'accumulated_logging_time': 0.0278170108795166, 'global_step': 1609, 'preemption_count': 0}), (3230, {'train/loss': 0.12306942729829992, 'validation/loss': 0.12466107777809947, 'validation/num_examples': 83274637, 'test/loss': 0.1270133052631579, 'test/num_examples': 95000000, 'score': 2428.47815489769, 'total_duration': 4318.568609952927, 'accumulated_submission_time': 2428.47815489769, 'accumulated_eval_time': 1889.9924383163452, 'accumulated_logging_time': 0.053708791732788086, 'global_step': 3230, 'preemption_count': 0}), (4864, {'train/loss': 0.12382498927086404, 'validation/loss': 0.12430601168516651, 'validation/num_examples': 83274637, 'test/loss': 0.12670932631578946, 'test/num_examples': 95000000, 'score': 3628.560966730118, 'total_duration': 6034.998579740524, 'accumulated_submission_time': 3628.560966730118, 'accumulated_eval_time': 2406.291713953018, 'accumulated_logging_time': 0.07950925827026367, 'global_step': 4864, 'preemption_count': 0}), (6485, {'train/loss': 0.12192281087239583, 'validation/loss': 0.12386604579255026, 'validation/num_examples': 83274637, 'test/loss': 0.12614091578947367, 'test/num_examples': 95000000, 'score': 4828.89098906517, 'total_duration': 7722.68746304512, 'accumulated_submission_time': 4828.89098906517, 'accumulated_eval_time': 2893.600976705551, 'accumulated_logging_time': 0.10667586326599121, 'global_step': 6485, 'preemption_count': 0}), (8000, {'train/loss': 0.12235325987234055, 'validation/loss': 0.12373945262589377, 'validation/num_examples': 83274637, 'test/loss': 0.12606724210526316, 'test/num_examples': 95000000, 'score': 5957.923988342285, 'total_duration': 9278.157832145691, 'accumulated_submission_time': 5957.923988342285, 'accumulated_eval_time': 3319.9887013435364, 'accumulated_logging_time': 0.13557958602905273, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1005 22:22:24.304278 139859622512448 submission_runner.py:552] Timing: 5957.923988342285
I1005 22:22:24.304329 139859622512448 submission_runner.py:554] Total number of evals: 6
I1005 22:22:24.304378 139859622512448 submission_runner.py:555] ====================
I1005 22:22:24.304487 139859622512448 submission_runner.py:625] Final criteo1tb score: 5957.923988342285
