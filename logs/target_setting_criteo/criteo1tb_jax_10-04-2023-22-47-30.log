python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_10 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-04-2023-22-47-30.log
2023-10-04 22:47:35.469129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1004 22:47:52.110919 140611127494464 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_10/criteo1tb_jax.
I1004 22:47:53.800589 140611127494464 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I1004 22:47:53.801615 140611127494464 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1004 22:47:53.801781 140611127494464 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1004 22:47:53.807057 140611127494464 submission_runner.py:507] Using RNG seed 1670741792
I1004 22:47:59.312186 140611127494464 submission_runner.py:516] --- Tuning run 1/1 ---
I1004 22:47:59.312388 140611127494464 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_10/criteo1tb_jax/trial_1.
I1004 22:47:59.312669 140611127494464 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_10/criteo1tb_jax/trial_1/hparams.json.
I1004 22:47:59.500331 140611127494464 submission_runner.py:191] Initializing dataset.
I1004 22:47:59.500553 140611127494464 submission_runner.py:198] Initializing model.
I1004 22:48:05.337700 140611127494464 submission_runner.py:232] Initializing optimizer.
I1004 22:48:08.517132 140611127494464 submission_runner.py:239] Initializing metrics bundle.
I1004 22:48:08.517339 140611127494464 submission_runner.py:257] Initializing checkpoint and logger.
I1004 22:48:08.518495 140611127494464 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_10/criteo1tb_jax/trial_1 with prefix checkpoint_
I1004 22:48:08.518640 140611127494464 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_10/criteo1tb_jax/trial_1/meta_data_0.json.
I1004 22:48:08.518831 140611127494464 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1004 22:48:08.518885 140611127494464 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1004 22:48:09.378129 140611127494464 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_10/criteo1tb_jax/trial_1/flags_0.json.
I1004 22:48:09.468932 140611127494464 submission_runner.py:290] Starting training loop.
I1004 22:48:36.103816 140446463420160 logging_writer.py:48] [0] global_step=0, grad_norm=4.095562934875488, loss=0.39810022711753845
I1004 22:48:36.114451 140611127494464 spec.py:321] Evaluating on the training split.
I1004 22:52:39.075091 140611127494464 spec.py:333] Evaluating on the validation split.
I1004 22:56:44.726153 140611127494464 spec.py:349] Evaluating on the test split.
I1004 23:01:20.190284 140611127494464 submission_runner.py:381] Time since start: 790.72s, 	Step: 1, 	{'train/loss': 0.3974790033304466, 'validation/loss': 0.3974434857038164, 'validation/num_examples': 83274637, 'test/loss': 0.39825115789473686, 'test/num_examples': 95000000, 'score': 26.645501136779785, 'total_duration': 790.7212767601013, 'accumulated_submission_time': 26.645501136779785, 'accumulated_eval_time': 764.0757405757904, 'accumulated_logging_time': 0}
I1004 23:01:20.211632 140428359747328 logging_writer.py:48] [1] accumulated_eval_time=764.075741, accumulated_logging_time=0, accumulated_submission_time=26.645501, global_step=1, preemption_count=0, score=26.645501, test/loss=0.398251, test/num_examples=95000000, total_duration=790.721277, train/loss=0.397479, validation/loss=0.397443, validation/num_examples=83274637
I1004 23:01:20.325975 140428351354624 logging_writer.py:48] [1] global_step=1, grad_norm=4.111227512359619, loss=0.3971390128135681
I1004 23:01:20.433297 140428359747328 logging_writer.py:48] [2] global_step=2, grad_norm=3.503133773803711, loss=0.35091865062713623
I1004 23:01:20.538585 140428351354624 logging_writer.py:48] [3] global_step=3, grad_norm=2.548581123352051, loss=0.2866143584251404
I1004 23:01:20.642204 140428359747328 logging_writer.py:48] [4] global_step=4, grad_norm=1.7142504453659058, loss=0.22877272963523865
I1004 23:01:20.745758 140428351354624 logging_writer.py:48] [5] global_step=5, grad_norm=0.9954729080200195, loss=0.18621571362018585
I1004 23:01:20.849349 140428359747328 logging_writer.py:48] [6] global_step=6, grad_norm=0.3987880349159241, loss=0.16052518784999847
I1004 23:01:20.953743 140428351354624 logging_writer.py:48] [7] global_step=7, grad_norm=0.14739461243152618, loss=0.15509705245494843
I1004 23:01:21.058329 140428359747328 logging_writer.py:48] [8] global_step=8, grad_norm=0.4893507659435272, loss=0.16455544531345367
I1004 23:01:21.164205 140428351354624 logging_writer.py:48] [9] global_step=9, grad_norm=0.6850659251213074, loss=0.17378699779510498
I1004 23:01:21.269299 140428359747328 logging_writer.py:48] [10] global_step=10, grad_norm=0.7852885723114014, loss=0.18153324723243713
I1004 23:01:21.375487 140428351354624 logging_writer.py:48] [11] global_step=11, grad_norm=0.8012060523033142, loss=0.18254053592681885
I1004 23:01:21.480449 140428359747328 logging_writer.py:48] [12] global_step=12, grad_norm=0.7706000208854675, loss=0.18102869391441345
I1004 23:01:21.583658 140428351354624 logging_writer.py:48] [13] global_step=13, grad_norm=0.6596022248268127, loss=0.169015571475029
I1004 23:01:21.687349 140428359747328 logging_writer.py:48] [14] global_step=14, grad_norm=0.5506947040557861, loss=0.16300299763679504
I1004 23:01:21.791464 140428351354624 logging_writer.py:48] [15] global_step=15, grad_norm=0.35903024673461914, loss=0.15139269828796387
I1004 23:01:21.895912 140428359747328 logging_writer.py:48] [16] global_step=16, grad_norm=0.19588391482830048, loss=0.14958208799362183
I1004 23:01:22.000029 140428351354624 logging_writer.py:48] [17] global_step=17, grad_norm=0.056593481451272964, loss=0.14300665259361267
I1004 23:01:22.103783 140428359747328 logging_writer.py:48] [18] global_step=18, grad_norm=0.16175228357315063, loss=0.14259135723114014
I1004 23:01:22.208181 140428351354624 logging_writer.py:48] [19] global_step=19, grad_norm=0.17909367382526398, loss=0.14021292328834534
I1004 23:01:22.318523 140428359747328 logging_writer.py:48] [20] global_step=20, grad_norm=0.13526000082492828, loss=0.13807523250579834
I1004 23:01:22.425320 140428351354624 logging_writer.py:48] [21] global_step=21, grad_norm=0.04637759551405907, loss=0.13741958141326904
I1004 23:01:22.532172 140428359747328 logging_writer.py:48] [22] global_step=22, grad_norm=0.05334465578198433, loss=0.13690482079982758
I1004 23:01:22.636783 140428351354624 logging_writer.py:48] [23] global_step=23, grad_norm=0.07986583560705185, loss=0.13705849647521973
I1004 23:01:22.741469 140428359747328 logging_writer.py:48] [24] global_step=24, grad_norm=0.04515703395009041, loss=0.13417012989521027
I1004 23:01:22.845267 140428351354624 logging_writer.py:48] [25] global_step=25, grad_norm=0.03607327863574028, loss=0.13631711900234222
I1004 23:01:22.950348 140428359747328 logging_writer.py:48] [26] global_step=26, grad_norm=0.024994932115077972, loss=0.13480892777442932
I1004 23:01:23.056392 140428351354624 logging_writer.py:48] [27] global_step=27, grad_norm=0.022747647017240524, loss=0.13383176922798157
I1004 23:01:23.161066 140428359747328 logging_writer.py:48] [28] global_step=28, grad_norm=0.020962873473763466, loss=0.13549508154392242
I1004 23:01:23.955519 140428351354624 logging_writer.py:48] [29] global_step=29, grad_norm=0.01794576831161976, loss=0.1351592242717743
I1004 23:01:24.822788 140428359747328 logging_writer.py:48] [30] global_step=30, grad_norm=0.01596340537071228, loss=0.134283646941185
I1004 23:01:25.603581 140428351354624 logging_writer.py:48] [31] global_step=31, grad_norm=0.020230544731020927, loss=0.13604287803173065
I1004 23:01:26.301947 140428359747328 logging_writer.py:48] [32] global_step=32, grad_norm=0.030073408037424088, loss=0.13469836115837097
I1004 23:01:27.220648 140428351354624 logging_writer.py:48] [33] global_step=33, grad_norm=0.03591739758849144, loss=0.13389968872070312
I1004 23:01:27.723496 140428359747328 logging_writer.py:48] [34] global_step=34, grad_norm=0.03760434314608574, loss=0.13429506123065948
I1004 23:01:28.396256 140428351354624 logging_writer.py:48] [35] global_step=35, grad_norm=0.06300705671310425, loss=0.13589826226234436
I1004 23:01:29.170277 140428359747328 logging_writer.py:48] [36] global_step=36, grad_norm=0.10188654065132141, loss=0.13557857275009155
I1004 23:01:29.964390 140428351354624 logging_writer.py:48] [37] global_step=37, grad_norm=0.11923616379499435, loss=0.13564112782478333
I1004 23:01:30.626239 140428359747328 logging_writer.py:48] [38] global_step=38, grad_norm=0.10249935835599899, loss=0.13868747651576996
I1004 23:01:31.385764 140428351354624 logging_writer.py:48] [39] global_step=39, grad_norm=0.09357423335313797, loss=0.13920386135578156
I1004 23:01:32.139270 140428359747328 logging_writer.py:48] [40] global_step=40, grad_norm=0.08950366079807281, loss=0.13999395072460175
I1004 23:01:32.959893 140428351354624 logging_writer.py:48] [41] global_step=41, grad_norm=0.08479178696870804, loss=0.13771598041057587
I1004 23:01:33.796043 140428359747328 logging_writer.py:48] [42] global_step=42, grad_norm=0.07607129961252213, loss=0.141084223985672
I1004 23:01:34.534213 140428351354624 logging_writer.py:48] [43] global_step=43, grad_norm=0.06592514365911484, loss=0.13714146614074707
I1004 23:01:35.310668 140428359747328 logging_writer.py:48] [44] global_step=44, grad_norm=0.0904819592833519, loss=0.13538333773612976
I1004 23:01:35.980933 140428351354624 logging_writer.py:48] [45] global_step=45, grad_norm=0.17395716905593872, loss=0.13582837581634521
I1004 23:01:36.625646 140428359747328 logging_writer.py:48] [46] global_step=46, grad_norm=0.32592883706092834, loss=0.13627001643180847
I1004 23:01:37.451227 140428351354624 logging_writer.py:48] [47] global_step=47, grad_norm=0.3453298509120941, loss=0.13624390959739685
I1004 23:01:38.189384 140428359747328 logging_writer.py:48] [48] global_step=48, grad_norm=0.14938271045684814, loss=0.13198496401309967
I1004 23:01:39.035336 140428351354624 logging_writer.py:48] [49] global_step=49, grad_norm=0.03760417178273201, loss=0.13085423409938812
I1004 23:01:39.888829 140428359747328 logging_writer.py:48] [50] global_step=50, grad_norm=0.016118695959448814, loss=0.13223490118980408
I1004 23:01:40.828998 140428351354624 logging_writer.py:48] [51] global_step=51, grad_norm=0.01616715081036091, loss=0.13077035546302795
I1004 23:01:41.590620 140428359747328 logging_writer.py:48] [52] global_step=52, grad_norm=0.029893167316913605, loss=0.13210183382034302
I1004 23:01:42.410517 140428351354624 logging_writer.py:48] [53] global_step=53, grad_norm=0.04627778381109238, loss=0.130204975605011
I1004 23:01:43.177158 140428359747328 logging_writer.py:48] [54] global_step=54, grad_norm=0.0726383626461029, loss=0.13296061754226685
I1004 23:01:43.996958 140428351354624 logging_writer.py:48] [55] global_step=55, grad_norm=0.08804624527692795, loss=0.13186942040920258
I1004 23:01:44.601226 140428359747328 logging_writer.py:48] [56] global_step=56, grad_norm=0.09240885078907013, loss=0.13278786838054657
I1004 23:01:45.285708 140428351354624 logging_writer.py:48] [57] global_step=57, grad_norm=0.09177681803703308, loss=0.13251850008964539
I1004 23:01:46.146289 140428359747328 logging_writer.py:48] [58] global_step=58, grad_norm=0.07895132154226303, loss=0.1313873827457428
I1004 23:01:46.942931 140428351354624 logging_writer.py:48] [59] global_step=59, grad_norm=0.06038925424218178, loss=0.13139541447162628
I1004 23:01:47.678485 140428359747328 logging_writer.py:48] [60] global_step=60, grad_norm=0.041381582617759705, loss=0.13193310797214508
I1004 23:01:48.320565 140428351354624 logging_writer.py:48] [61] global_step=61, grad_norm=0.02181510254740715, loss=0.13421404361724854
I1004 23:01:49.117304 140428359747328 logging_writer.py:48] [62] global_step=62, grad_norm=0.019648490473628044, loss=0.13394947350025177
I1004 23:01:49.937884 140428351354624 logging_writer.py:48] [63] global_step=63, grad_norm=0.016290437430143356, loss=0.1336013227701187
I1004 23:01:50.787685 140428359747328 logging_writer.py:48] [64] global_step=64, grad_norm=0.009407500736415386, loss=0.13343726098537445
I1004 23:01:51.690700 140428351354624 logging_writer.py:48] [65] global_step=65, grad_norm=0.028402959927916527, loss=0.13007044792175293
I1004 23:01:52.409291 140428359747328 logging_writer.py:48] [66] global_step=66, grad_norm=0.047783393412828445, loss=0.13117404282093048
I1004 23:01:53.212541 140428351354624 logging_writer.py:48] [67] global_step=67, grad_norm=0.06024196743965149, loss=0.13157103955745697
I1004 23:01:53.960057 140428359747328 logging_writer.py:48] [68] global_step=68, grad_norm=0.07542668282985687, loss=0.13144369423389435
I1004 23:01:54.708830 140428351354624 logging_writer.py:48] [69] global_step=69, grad_norm=0.07570397853851318, loss=0.13148647546768188
I1004 23:01:55.352300 140428359747328 logging_writer.py:48] [70] global_step=70, grad_norm=0.06731284409761429, loss=0.13064950704574585
I1004 23:01:55.988953 140428351354624 logging_writer.py:48] [71] global_step=71, grad_norm=0.04902922734618187, loss=0.13375575840473175
I1004 23:01:56.835529 140428359747328 logging_writer.py:48] [72] global_step=72, grad_norm=0.03021116927266121, loss=0.13045237958431244
I1004 23:01:57.475274 140428351354624 logging_writer.py:48] [73] global_step=73, grad_norm=0.03253352269530296, loss=0.13311149179935455
I1004 23:01:58.281234 140428359747328 logging_writer.py:48] [74] global_step=74, grad_norm=0.048447370529174805, loss=0.1331632137298584
I1004 23:01:59.035575 140428351354624 logging_writer.py:48] [75] global_step=75, grad_norm=0.06887007504701614, loss=0.13198980689048767
I1004 23:01:59.693743 140428359747328 logging_writer.py:48] [76] global_step=76, grad_norm=0.09060584753751755, loss=0.13428950309753418
I1004 23:02:00.443077 140428351354624 logging_writer.py:48] [77] global_step=77, grad_norm=0.13799235224723816, loss=0.1295543760061264
I1004 23:02:01.082063 140428359747328 logging_writer.py:48] [78] global_step=78, grad_norm=0.16278617084026337, loss=0.132727712392807
I1004 23:02:01.896090 140428351354624 logging_writer.py:48] [79] global_step=79, grad_norm=0.11959002912044525, loss=0.13351774215698242
I1004 23:02:02.558387 140428359747328 logging_writer.py:48] [80] global_step=80, grad_norm=0.04479534924030304, loss=0.12715111672878265
I1004 23:02:03.361068 140428351354624 logging_writer.py:48] [81] global_step=81, grad_norm=0.02077360637485981, loss=0.12494897097349167
I1004 23:02:04.070939 140428359747328 logging_writer.py:48] [82] global_step=82, grad_norm=0.011798943392932415, loss=0.12408845871686935
I1004 23:02:04.966737 140428351354624 logging_writer.py:48] [83] global_step=83, grad_norm=0.0054901945404708385, loss=0.12278999388217926
I1004 23:02:05.637037 140428359747328 logging_writer.py:48] [84] global_step=84, grad_norm=0.008575222454965115, loss=0.11976507306098938
I1004 23:02:06.448848 140428351354624 logging_writer.py:48] [85] global_step=85, grad_norm=0.030852897092700005, loss=0.12182333320379257
I1004 23:02:07.194098 140428359747328 logging_writer.py:48] [86] global_step=86, grad_norm=0.012713097035884857, loss=0.12482117861509323
I1004 23:02:07.990374 140428351354624 logging_writer.py:48] [87] global_step=87, grad_norm=0.01463931892067194, loss=0.12215837091207504
I1004 23:02:08.641607 140428359747328 logging_writer.py:48] [88] global_step=88, grad_norm=0.01875128224492073, loss=0.12230537086725235
I1004 23:02:09.518066 140428351354624 logging_writer.py:48] [89] global_step=89, grad_norm=0.00996025837957859, loss=0.12479795515537262
I1004 23:02:10.205934 140428359747328 logging_writer.py:48] [90] global_step=90, grad_norm=0.005846426356583834, loss=0.12325436621904373
I1004 23:02:10.966789 140428351354624 logging_writer.py:48] [91] global_step=91, grad_norm=0.005200270097702742, loss=0.12252382934093475
I1004 23:02:11.801177 140428359747328 logging_writer.py:48] [92] global_step=92, grad_norm=0.0053659831173717976, loss=0.12262377142906189
I1004 23:02:12.655252 140428351354624 logging_writer.py:48] [93] global_step=93, grad_norm=0.004768176469951868, loss=0.12038356065750122
I1004 23:02:13.268761 140428359747328 logging_writer.py:48] [94] global_step=94, grad_norm=0.012883688323199749, loss=0.1226387545466423
I1004 23:02:14.021538 140428351354624 logging_writer.py:48] [95] global_step=95, grad_norm=0.034289147704839706, loss=0.12318696081638336
I1004 23:02:14.780801 140428359747328 logging_writer.py:48] [96] global_step=96, grad_norm=0.05087773874402046, loss=0.12118767946958542
I1004 23:02:15.581658 140428351354624 logging_writer.py:48] [97] global_step=97, grad_norm=0.08331063389778137, loss=0.12022832036018372
I1004 23:02:16.283795 140428359747328 logging_writer.py:48] [98] global_step=98, grad_norm=0.1361977756023407, loss=0.12303075939416885
I1004 23:02:17.122205 140428351354624 logging_writer.py:48] [99] global_step=99, grad_norm=0.1732383370399475, loss=0.12597817182540894
I1004 23:02:17.804244 140428359747328 logging_writer.py:48] [100] global_step=100, grad_norm=0.185918390750885, loss=0.1286749243736267
I1004 23:07:08.873459 140428351354624 logging_writer.py:48] [500] global_step=500, grad_norm=0.007663751486688852, loss=0.11971621960401535
I1004 23:13:28.727197 140428359747328 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.01797771267592907, loss=0.12914209067821503
I1004 23:19:41.639229 140428351354624 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.02426580712199211, loss=0.12164601683616638
I1004 23:21:20.657203 140611127494464 spec.py:321] Evaluating on the training split.
I1004 23:24:25.132704 140611127494464 spec.py:333] Evaluating on the validation split.
I1004 23:27:45.280395 140611127494464 spec.py:349] Evaluating on the test split.
I1004 23:31:09.146972 140611127494464 submission_runner.py:381] Time since start: 2579.68s, 	Step: 1637, 	{'train/loss': 0.1243074525077388, 'validation/loss': 0.12536494154876954, 'validation/num_examples': 83274637, 'test/loss': 0.12783746315789474, 'test/num_examples': 95000000, 'score': 1227.0588779449463, 'total_duration': 2579.6779749393463, 'accumulated_submission_time': 1227.0588779449463, 'accumulated_eval_time': 1352.5654754638672, 'accumulated_logging_time': 0.02910900115966797}
I1004 23:31:09.166793 140428359747328 logging_writer.py:48] [1637] accumulated_eval_time=1352.565475, accumulated_logging_time=0.029109, accumulated_submission_time=1227.058878, global_step=1637, preemption_count=0, score=1227.058878, test/loss=0.127837, test/num_examples=95000000, total_duration=2579.677975, train/loss=0.124307, validation/loss=0.125365, validation/num_examples=83274637
I1004 23:35:27.092195 140428351354624 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.007559995166957378, loss=0.12177979201078415
I1004 23:41:40.921923 140428359747328 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.007336027920246124, loss=0.11482034623622894
I1004 23:48:01.226632 140428351354624 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.03615853190422058, loss=0.12134388834238052
I1004 23:51:09.621796 140611127494464 spec.py:321] Evaluating on the training split.
I1004 23:54:23.070806 140611127494464 spec.py:333] Evaluating on the validation split.
I1004 23:56:54.073764 140611127494464 spec.py:349] Evaluating on the test split.
I1004 23:59:57.100604 140611127494464 submission_runner.py:381] Time since start: 4307.63s, 	Step: 3252, 	{'train/loss': 0.12422132192167847, 'validation/loss': 0.1246248242427043, 'validation/num_examples': 83274637, 'test/loss': 0.1269549052631579, 'test/num_examples': 95000000, 'score': 2427.4830598831177, 'total_duration': 4307.631606578827, 'accumulated_submission_time': 2427.4830598831177, 'accumulated_eval_time': 1880.0442481040955, 'accumulated_logging_time': 0.05672740936279297}
I1004 23:59:57.121953 140428359747328 logging_writer.py:48] [3252] accumulated_eval_time=1880.044248, accumulated_logging_time=0.056727, accumulated_submission_time=2427.483060, global_step=3252, preemption_count=0, score=2427.483060, test/loss=0.126955, test/num_examples=95000000, total_duration=4307.631607, train/loss=0.124221, validation/loss=0.124625, validation/num_examples=83274637
I1005 00:02:43.749314 140428351354624 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0067992969416081905, loss=0.12817230820655823
I1005 00:09:00.218206 140428359747328 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.019490433856844902, loss=0.12017122656106949
I1005 00:15:09.438759 140428351354624 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.006808493286371231, loss=0.1266854852437973
I1005 00:19:57.272273 140611127494464 spec.py:321] Evaluating on the training split.
I1005 00:23:02.740509 140611127494464 spec.py:333] Evaluating on the validation split.
I1005 00:26:08.577366 140611127494464 spec.py:349] Evaluating on the test split.
I1005 00:29:11.112148 140611127494464 submission_runner.py:381] Time since start: 6061.64s, 	Step: 4877, 	{'train/loss': 0.1229723444524801, 'validation/loss': 0.12421451924191516, 'validation/num_examples': 83274637, 'test/loss': 0.12658232631578947, 'test/num_examples': 95000000, 'score': 3627.602769613266, 'total_duration': 6061.643148183823, 'accumulated_submission_time': 3627.602769613266, 'accumulated_eval_time': 2433.884082555771, 'accumulated_logging_time': 0.08572936058044434}
I1005 00:29:11.131192 140428359747328 logging_writer.py:48] [4877] accumulated_eval_time=2433.884083, accumulated_logging_time=0.085729, accumulated_submission_time=3627.602770, global_step=4877, preemption_count=0, score=3627.602770, test/loss=0.126582, test/num_examples=95000000, total_duration=6061.643148, train/loss=0.122972, validation/loss=0.124215, validation/num_examples=83274637
I1005 00:30:27.772188 140428351354624 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.003365002805367112, loss=0.1341412216424942
I1005 00:36:45.097561 140428359747328 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.00426453398540616, loss=0.11777724325656891
I1005 00:43:00.296631 140428351354624 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.012151701375842094, loss=0.12077445536851883
I1005 00:49:11.159496 140611127494464 spec.py:321] Evaluating on the training split.
I1005 00:52:05.222956 140611127494464 spec.py:333] Evaluating on the validation split.
I1005 00:55:08.134670 140611127494464 spec.py:349] Evaluating on the test split.
I1005 00:58:07.555640 140611127494464 submission_runner.py:381] Time since start: 7798.09s, 	Step: 6491, 	{'train/loss': 0.12046029432764593, 'validation/loss': 0.12369330412091739, 'validation/num_examples': 83274637, 'test/loss': 0.12600663157894737, 'test/num_examples': 95000000, 'score': 4827.595700979233, 'total_duration': 7798.086642742157, 'accumulated_submission_time': 4827.595700979233, 'accumulated_eval_time': 2970.280199289322, 'accumulated_logging_time': 0.11702227592468262}
I1005 00:58:07.572958 140428359747328 logging_writer.py:48] [6491] accumulated_eval_time=2970.280199, accumulated_logging_time=0.117022, accumulated_submission_time=4827.595701, global_step=6491, preemption_count=0, score=4827.595701, test/loss=0.126007, test/num_examples=95000000, total_duration=7798.086643, train/loss=0.120460, validation/loss=0.123693, validation/num_examples=83274637
I1005 00:58:08.547874 140428351354624 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0034038699232041836, loss=0.12043657898902893
I1005 01:04:17.363845 140428359747328 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.004555267747491598, loss=0.1276930272579193
I1005 01:10:34.179426 140428351354624 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.010422605089843273, loss=0.12972140312194824
I1005 01:16:53.968299 140611127494464 spec.py:321] Evaluating on the training split.
I1005 01:19:27.659441 140611127494464 spec.py:333] Evaluating on the validation split.
I1005 01:21:32.217504 140611127494464 spec.py:349] Evaluating on the test split.
I1005 01:23:59.873367 140611127494464 submission_runner.py:381] Time since start: 9350.40s, 	Step: 8000, 	{'train/loss': 0.12208582320303288, 'validation/loss': 0.12363413844722013, 'validation/num_examples': 83274637, 'test/loss': 0.12594647368421052, 'test/num_examples': 95000000, 'score': 5953.959239959717, 'total_duration': 9350.404375553131, 'accumulated_submission_time': 5953.959239959717, 'accumulated_eval_time': 3396.1852462291718, 'accumulated_logging_time': 0.1444094181060791}
I1005 01:23:59.889556 140428359747328 logging_writer.py:48] [8000] accumulated_eval_time=3396.185246, accumulated_logging_time=0.144409, accumulated_submission_time=5953.959240, global_step=8000, preemption_count=0, score=5953.959240, test/loss=0.125946, test/num_examples=95000000, total_duration=9350.404376, train/loss=0.122086, validation/loss=0.123634, validation/num_examples=83274637
I1005 01:23:59.905785 140428351354624 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5953.959240
I1005 01:24:06.269988 140611127494464 checkpoints.py:490] Saving checkpoint at step: 8000
I1005 01:24:42.170319 140611127494464 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_10/criteo1tb_jax/trial_1/checkpoint_8000
I1005 01:24:42.504521 140611127494464 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_10/criteo1tb_jax/trial_1/checkpoint_8000.
I1005 01:24:42.907717 140611127494464 submission_runner.py:549] Tuning trial 1/1
I1005 01:24:42.907959 140611127494464 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1005 01:24:42.909083 140611127494464 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 0.3974790033304466, 'validation/loss': 0.3974434857038164, 'validation/num_examples': 83274637, 'test/loss': 0.39825115789473686, 'test/num_examples': 95000000, 'score': 26.645501136779785, 'total_duration': 790.7212767601013, 'accumulated_submission_time': 26.645501136779785, 'accumulated_eval_time': 764.0757405757904, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1637, {'train/loss': 0.1243074525077388, 'validation/loss': 0.12536494154876954, 'validation/num_examples': 83274637, 'test/loss': 0.12783746315789474, 'test/num_examples': 95000000, 'score': 1227.0588779449463, 'total_duration': 2579.6779749393463, 'accumulated_submission_time': 1227.0588779449463, 'accumulated_eval_time': 1352.5654754638672, 'accumulated_logging_time': 0.02910900115966797, 'global_step': 1637, 'preemption_count': 0}), (3252, {'train/loss': 0.12422132192167847, 'validation/loss': 0.1246248242427043, 'validation/num_examples': 83274637, 'test/loss': 0.1269549052631579, 'test/num_examples': 95000000, 'score': 2427.4830598831177, 'total_duration': 4307.631606578827, 'accumulated_submission_time': 2427.4830598831177, 'accumulated_eval_time': 1880.0442481040955, 'accumulated_logging_time': 0.05672740936279297, 'global_step': 3252, 'preemption_count': 0}), (4877, {'train/loss': 0.1229723444524801, 'validation/loss': 0.12421451924191516, 'validation/num_examples': 83274637, 'test/loss': 0.12658232631578947, 'test/num_examples': 95000000, 'score': 3627.602769613266, 'total_duration': 6061.643148183823, 'accumulated_submission_time': 3627.602769613266, 'accumulated_eval_time': 2433.884082555771, 'accumulated_logging_time': 0.08572936058044434, 'global_step': 4877, 'preemption_count': 0}), (6491, {'train/loss': 0.12046029432764593, 'validation/loss': 0.12369330412091739, 'validation/num_examples': 83274637, 'test/loss': 0.12600663157894737, 'test/num_examples': 95000000, 'score': 4827.595700979233, 'total_duration': 7798.086642742157, 'accumulated_submission_time': 4827.595700979233, 'accumulated_eval_time': 2970.280199289322, 'accumulated_logging_time': 0.11702227592468262, 'global_step': 6491, 'preemption_count': 0}), (8000, {'train/loss': 0.12208582320303288, 'validation/loss': 0.12363413844722013, 'validation/num_examples': 83274637, 'test/loss': 0.12594647368421052, 'test/num_examples': 95000000, 'score': 5953.959239959717, 'total_duration': 9350.404375553131, 'accumulated_submission_time': 5953.959239959717, 'accumulated_eval_time': 3396.1852462291718, 'accumulated_logging_time': 0.1444094181060791, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1005 01:24:42.909243 140611127494464 submission_runner.py:552] Timing: 5953.959239959717
I1005 01:24:42.909312 140611127494464 submission_runner.py:554] Total number of evals: 6
I1005 01:24:42.909384 140611127494464 submission_runner.py:555] ====================
I1005 01:24:42.909511 140611127494464 submission_runner.py:625] Final criteo1tb score: 5953.959239959717
