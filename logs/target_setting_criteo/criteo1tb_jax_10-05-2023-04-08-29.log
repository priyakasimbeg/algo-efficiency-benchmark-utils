python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/criteo1tb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=criteo_target_resetting/nadamw_run_12 --overwrite=true --save_checkpoints=false --max_global_steps=8000 2>&1 | tee -a /logs/criteo1tb_jax_10-05-2023-04-08-29.log
2023-10-05 04:08:34.718531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1005 04:08:52.036231 139777607395136 logger_utils.py:76] Creating experiment directory at /experiment_runs/criteo_target_resetting/nadamw_run_12/criteo1tb_jax.
I1005 04:08:53.704874 139777607395136 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1005 04:08:53.705765 139777607395136 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1005 04:08:53.706014 139777607395136 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1005 04:08:53.711364 139777607395136 submission_runner.py:507] Using RNG seed 1498440633
I1005 04:08:59.538623 139777607395136 submission_runner.py:516] --- Tuning run 1/1 ---
I1005 04:08:59.538828 139777607395136 submission_runner.py:521] Creating tuning directory at /experiment_runs/criteo_target_resetting/nadamw_run_12/criteo1tb_jax/trial_1.
I1005 04:08:59.539101 139777607395136 logger_utils.py:92] Saving hparams to /experiment_runs/criteo_target_resetting/nadamw_run_12/criteo1tb_jax/trial_1/hparams.json.
I1005 04:08:59.724457 139777607395136 submission_runner.py:191] Initializing dataset.
I1005 04:08:59.724690 139777607395136 submission_runner.py:198] Initializing model.
I1005 04:09:05.474860 139777607395136 submission_runner.py:232] Initializing optimizer.
I1005 04:09:08.629472 139777607395136 submission_runner.py:239] Initializing metrics bundle.
I1005 04:09:08.629675 139777607395136 submission_runner.py:257] Initializing checkpoint and logger.
I1005 04:09:08.630780 139777607395136 checkpoints.py:915] Found no checkpoint files in /experiment_runs/criteo_target_resetting/nadamw_run_12/criteo1tb_jax/trial_1 with prefix checkpoint_
I1005 04:09:08.630929 139777607395136 submission_runner.py:277] Saving meta data to /experiment_runs/criteo_target_resetting/nadamw_run_12/criteo1tb_jax/trial_1/meta_data_0.json.
I1005 04:09:08.631139 139777607395136 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1005 04:09:08.631204 139777607395136 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1005 04:09:09.590454 139777607395136 submission_runner.py:280] Saving flags to /experiment_runs/criteo_target_resetting/nadamw_run_12/criteo1tb_jax/trial_1/flags_0.json.
I1005 04:09:09.689040 139777607395136 submission_runner.py:290] Starting training loop.
I1005 04:09:35.633157 139613709526784 logging_writer.py:48] [0] global_step=0, grad_norm=7.453252792358398, loss=1.1117818355560303
I1005 04:09:35.644727 139777607395136 spec.py:321] Evaluating on the training split.
I1005 04:13:26.062630 139777607395136 spec.py:333] Evaluating on the validation split.
I1005 04:17:21.648985 139777607395136 spec.py:349] Evaluating on the test split.
I1005 04:21:47.463688 139777607395136 submission_runner.py:381] Time since start: 757.77s, 	Step: 1, 	{'train/loss': 1.1085744413939662, 'validation/loss': 1.1118760205463278, 'validation/num_examples': 83274637, 'test/loss': 1.1083299368421053, 'test/num_examples': 95000000, 'score': 25.955660104751587, 'total_duration': 757.7745907306671, 'accumulated_submission_time': 25.955660104751587, 'accumulated_eval_time': 731.8188879489899, 'accumulated_logging_time': 0}
I1005 04:21:47.483708 139595798787840 logging_writer.py:48] [1] accumulated_eval_time=731.818888, accumulated_logging_time=0, accumulated_submission_time=25.955660, global_step=1, preemption_count=0, score=25.955660, test/loss=1.108330, test/num_examples=95000000, total_duration=757.774591, train/loss=1.108574, validation/loss=1.111876, validation/num_examples=83274637
I1005 04:21:47.598279 139595790395136 logging_writer.py:48] [1] global_step=1, grad_norm=7.4537224769592285, loss=1.1112666130065918
I1005 04:21:47.704462 139595798787840 logging_writer.py:48] [2] global_step=2, grad_norm=7.030213832855225, loss=1.0230666399002075
I1005 04:21:47.809997 139595790395136 logging_writer.py:48] [3] global_step=3, grad_norm=6.5428032875061035, loss=0.8775051832199097
I1005 04:21:47.913664 139595798787840 logging_writer.py:48] [4] global_step=4, grad_norm=6.013815402984619, loss=0.6969354152679443
I1005 04:21:48.017550 139595790395136 logging_writer.py:48] [5] global_step=5, grad_norm=5.08465051651001, loss=0.49635836482048035
I1005 04:21:48.121562 139595798787840 logging_writer.py:48] [6] global_step=6, grad_norm=3.3711512088775635, loss=0.32477372884750366
I1005 04:21:48.225162 139595790395136 logging_writer.py:48] [7] global_step=7, grad_norm=1.5997182130813599, loss=0.22035712003707886
I1005 04:21:48.330019 139595798787840 logging_writer.py:48] [8] global_step=8, grad_norm=0.34810635447502136, loss=0.17984923720359802
I1005 04:21:48.434667 139595790395136 logging_writer.py:48] [9] global_step=9, grad_norm=0.6144062280654907, loss=0.18636223673820496
I1005 04:21:48.539479 139595798787840 logging_writer.py:48] [10] global_step=10, grad_norm=1.0976189374923706, loss=0.21077606081962585
I1005 04:21:48.643871 139595790395136 logging_writer.py:48] [11] global_step=11, grad_norm=1.4636976718902588, loss=0.24619998037815094
I1005 04:21:48.748688 139595798787840 logging_writer.py:48] [12] global_step=12, grad_norm=1.6627668142318726, loss=0.26913002133369446
I1005 04:21:48.854137 139595790395136 logging_writer.py:48] [13] global_step=13, grad_norm=1.9105545282363892, loss=0.303799033164978
I1005 04:21:48.959345 139595798787840 logging_writer.py:48] [14] global_step=14, grad_norm=1.9292298555374146, loss=0.308157354593277
I1005 04:21:49.063943 139595790395136 logging_writer.py:48] [15] global_step=15, grad_norm=1.901439905166626, loss=0.3041760325431824
I1005 04:21:49.168830 139595798787840 logging_writer.py:48] [16] global_step=16, grad_norm=1.873827576637268, loss=0.2988678216934204
I1005 04:21:49.273284 139595790395136 logging_writer.py:48] [17] global_step=17, grad_norm=1.7879360914230347, loss=0.283721387386322
I1005 04:21:49.378739 139595798787840 logging_writer.py:48] [18] global_step=18, grad_norm=1.5979334115982056, loss=0.2535979151725769
I1005 04:21:49.484439 139595790395136 logging_writer.py:48] [19] global_step=19, grad_norm=1.4928480386734009, loss=0.23452052474021912
I1005 04:21:49.589807 139595798787840 logging_writer.py:48] [20] global_step=20, grad_norm=1.1601158380508423, loss=0.19951379299163818
I1005 04:21:49.696019 139595790395136 logging_writer.py:48] [21] global_step=21, grad_norm=0.702853798866272, loss=0.17131593823432922
I1005 04:21:49.801587 139595798787840 logging_writer.py:48] [22] global_step=22, grad_norm=0.16208484768867493, loss=0.15560494363307953
I1005 04:21:49.905416 139595790395136 logging_writer.py:48] [23] global_step=23, grad_norm=0.5988492369651794, loss=0.15964753925800323
I1005 04:21:50.010289 139595798787840 logging_writer.py:48] [24] global_step=24, grad_norm=0.6277710199356079, loss=0.15557004511356354
I1005 04:21:50.114418 139595790395136 logging_writer.py:48] [25] global_step=25, grad_norm=0.26180073618888855, loss=0.14892011880874634
I1005 04:21:50.219684 139595798787840 logging_writer.py:48] [26] global_step=26, grad_norm=0.11473076790571213, loss=0.1454184353351593
I1005 04:21:50.324330 139595790395136 logging_writer.py:48] [27] global_step=27, grad_norm=0.21925511956214905, loss=0.14498785138130188
I1005 04:21:50.428511 139595798787840 logging_writer.py:48] [28] global_step=28, grad_norm=0.1621997207403183, loss=0.14222735166549683
I1005 04:21:51.346548 139595790395136 logging_writer.py:48] [29] global_step=29, grad_norm=0.08729631453752518, loss=0.13990916311740875
I1005 04:21:51.985383 139595798787840 logging_writer.py:48] [30] global_step=30, grad_norm=0.06161082535982132, loss=0.13831932842731476
I1005 04:21:52.651925 139595790395136 logging_writer.py:48] [31] global_step=31, grad_norm=0.03295595943927765, loss=0.1363985538482666
I1005 04:21:53.293084 139595798787840 logging_writer.py:48] [32] global_step=32, grad_norm=0.05451154708862305, loss=0.1366954743862152
I1005 04:21:53.961881 139595790395136 logging_writer.py:48] [33] global_step=33, grad_norm=0.036077868193387985, loss=0.13742858171463013
I1005 04:21:54.694997 139595798787840 logging_writer.py:48] [34] global_step=34, grad_norm=0.039974529296159744, loss=0.13274168968200684
I1005 04:21:55.509365 139595790395136 logging_writer.py:48] [35] global_step=35, grad_norm=0.02823825739324093, loss=0.13528360426425934
I1005 04:21:56.170661 139595798787840 logging_writer.py:48] [36] global_step=36, grad_norm=0.027687357738614082, loss=0.1341378092765808
I1005 04:21:57.020533 139595790395136 logging_writer.py:48] [37] global_step=37, grad_norm=0.023930829018354416, loss=0.13270199298858643
I1005 04:21:57.727464 139595798787840 logging_writer.py:48] [38] global_step=38, grad_norm=0.018879689276218414, loss=0.1280941218137741
I1005 04:21:58.552083 139595790395136 logging_writer.py:48] [39] global_step=39, grad_norm=0.04889044538140297, loss=0.12646488845348358
I1005 04:21:59.460676 139595798787840 logging_writer.py:48] [40] global_step=40, grad_norm=0.1554785817861557, loss=0.12640468776226044
I1005 04:22:00.097249 139595790395136 logging_writer.py:48] [41] global_step=41, grad_norm=0.6193408370018005, loss=0.1309502273797989
I1005 04:22:00.907003 139595798787840 logging_writer.py:48] [42] global_step=42, grad_norm=0.6390601396560669, loss=0.1358514428138733
I1005 04:22:01.886876 139595790395136 logging_writer.py:48] [43] global_step=43, grad_norm=0.18061701953411102, loss=0.12622950971126556
I1005 04:22:02.592221 139595798787840 logging_writer.py:48] [44] global_step=44, grad_norm=0.027341607958078384, loss=0.12817789614200592
I1005 04:22:03.300223 139595790395136 logging_writer.py:48] [45] global_step=45, grad_norm=0.08164998888969421, loss=0.1255304217338562
I1005 04:22:04.050136 139595798787840 logging_writer.py:48] [46] global_step=46, grad_norm=0.04665997996926308, loss=0.12389889359474182
I1005 04:22:04.847283 139595790395136 logging_writer.py:48] [47] global_step=47, grad_norm=0.018696581944823265, loss=0.12421923130750656
I1005 04:22:05.523124 139595798787840 logging_writer.py:48] [48] global_step=48, grad_norm=0.014717711135745049, loss=0.12346690893173218
I1005 04:22:06.270169 139595790395136 logging_writer.py:48] [49] global_step=49, grad_norm=0.013148775324225426, loss=0.12393481284379959
I1005 04:22:07.185692 139595798787840 logging_writer.py:48] [50] global_step=50, grad_norm=0.014625814743340015, loss=0.12217509746551514
I1005 04:22:07.817922 139595790395136 logging_writer.py:48] [51] global_step=51, grad_norm=0.051356878131628036, loss=0.12473781406879425
I1005 04:22:08.655730 139595798787840 logging_writer.py:48] [52] global_step=52, grad_norm=0.09790870547294617, loss=0.1256132274866104
I1005 04:22:09.523425 139595790395136 logging_writer.py:48] [53] global_step=53, grad_norm=0.12409400194883347, loss=0.12165101617574692
I1005 04:22:10.133512 139595798787840 logging_writer.py:48] [54] global_step=54, grad_norm=0.21694153547286987, loss=0.12284363061189651
I1005 04:22:10.920017 139595790395136 logging_writer.py:48] [55] global_step=55, grad_norm=0.3245093822479248, loss=0.12467525154352188
I1005 04:22:11.804285 139595798787840 logging_writer.py:48] [56] global_step=56, grad_norm=0.5812281966209412, loss=0.12402302026748657
I1005 04:22:12.563942 139595790395136 logging_writer.py:48] [57] global_step=57, grad_norm=0.5787557363510132, loss=0.14233656227588654
I1005 04:22:13.307333 139595798787840 logging_writer.py:48] [58] global_step=58, grad_norm=0.416623055934906, loss=0.14292475581169128
I1005 04:22:14.086292 139595790395136 logging_writer.py:48] [59] global_step=59, grad_norm=0.366800993680954, loss=0.14481201767921448
I1005 04:22:14.832363 139595798787840 logging_writer.py:48] [60] global_step=60, grad_norm=0.31136053800582886, loss=0.14195819199085236
I1005 04:22:15.620226 139595790395136 logging_writer.py:48] [61] global_step=61, grad_norm=0.2264828085899353, loss=0.1410866677761078
I1005 04:22:16.359254 139595798787840 logging_writer.py:48] [62] global_step=62, grad_norm=0.12767666578292847, loss=0.14022375643253326
I1005 04:22:17.127239 139595790395136 logging_writer.py:48] [63] global_step=63, grad_norm=0.09620854258537292, loss=0.14179207384586334
I1005 04:22:17.924798 139595798787840 logging_writer.py:48] [64] global_step=64, grad_norm=0.08235331624746323, loss=0.14019590616226196
I1005 04:22:18.715492 139595790395136 logging_writer.py:48] [65] global_step=65, grad_norm=0.05847161263227463, loss=0.1404668092727661
I1005 04:22:19.447823 139595798787840 logging_writer.py:48] [66] global_step=66, grad_norm=0.026182053610682487, loss=0.14113783836364746
I1005 04:22:20.264678 139595790395136 logging_writer.py:48] [67] global_step=67, grad_norm=0.011994163505733013, loss=0.13843896985054016
I1005 04:22:21.057360 139595798787840 logging_writer.py:48] [68] global_step=68, grad_norm=0.015499310567975044, loss=0.13780006766319275
I1005 04:22:21.716519 139595790395136 logging_writer.py:48] [69] global_step=69, grad_norm=0.02043043076992035, loss=0.13587354123592377
I1005 04:22:22.493403 139595798787840 logging_writer.py:48] [70] global_step=70, grad_norm=0.05104052647948265, loss=0.13816052675247192
I1005 04:22:23.262328 139595790395136 logging_writer.py:48] [71] global_step=71, grad_norm=0.07452283799648285, loss=0.13767722249031067
I1005 04:22:23.996562 139595798787840 logging_writer.py:48] [72] global_step=72, grad_norm=0.08374591171741486, loss=0.13696065545082092
I1005 04:22:24.812059 139595790395136 logging_writer.py:48] [73] global_step=73, grad_norm=0.10358522832393646, loss=0.13721443712711334
I1005 04:22:25.518811 139595798787840 logging_writer.py:48] [74] global_step=74, grad_norm=0.1114937961101532, loss=0.13615381717681885
I1005 04:22:26.300577 139595790395136 logging_writer.py:48] [75] global_step=75, grad_norm=0.11448401212692261, loss=0.1355699747800827
I1005 04:22:27.016187 139595798787840 logging_writer.py:48] [76] global_step=76, grad_norm=0.09204328805208206, loss=0.13194601237773895
I1005 04:22:27.685950 139595790395136 logging_writer.py:48] [77] global_step=77, grad_norm=0.08014597743749619, loss=0.13077117502689362
I1005 04:22:28.397862 139595798787840 logging_writer.py:48] [78] global_step=78, grad_norm=0.11333141475915909, loss=0.13335353136062622
I1005 04:22:29.103510 139595790395136 logging_writer.py:48] [79] global_step=79, grad_norm=0.13725441694259644, loss=0.13259658217430115
I1005 04:22:29.848304 139595798787840 logging_writer.py:48] [80] global_step=80, grad_norm=0.13578204810619354, loss=0.13171949982643127
I1005 04:22:30.584872 139595790395136 logging_writer.py:48] [81] global_step=81, grad_norm=0.13389994204044342, loss=0.1317804902791977
I1005 04:22:31.305665 139595798787840 logging_writer.py:48] [82] global_step=82, grad_norm=0.12836270034313202, loss=0.1307077407836914
I1005 04:22:32.053875 139595790395136 logging_writer.py:48] [83] global_step=83, grad_norm=0.12271875888109207, loss=0.13142842054367065
I1005 04:22:32.777178 139595798787840 logging_writer.py:48] [84] global_step=84, grad_norm=0.12267991155385971, loss=0.13088275492191315
I1005 04:22:33.628704 139595790395136 logging_writer.py:48] [85] global_step=85, grad_norm=0.14258338510990143, loss=0.12864258885383606
I1005 04:22:34.387888 139595798787840 logging_writer.py:48] [86] global_step=86, grad_norm=0.17686834931373596, loss=0.1320870816707611
I1005 04:22:35.023824 139595790395136 logging_writer.py:48] [87] global_step=87, grad_norm=0.18692563474178314, loss=0.13199523091316223
I1005 04:22:35.788099 139595798787840 logging_writer.py:48] [88] global_step=88, grad_norm=0.17660988867282867, loss=0.13145899772644043
I1005 04:22:36.555008 139595790395136 logging_writer.py:48] [89] global_step=89, grad_norm=0.19171544909477234, loss=0.12850789725780487
I1005 04:22:37.261907 139595798787840 logging_writer.py:48] [90] global_step=90, grad_norm=0.20965392887592316, loss=0.13099975883960724
I1005 04:22:37.917068 139595790395136 logging_writer.py:48] [91] global_step=91, grad_norm=0.19038480520248413, loss=0.1309782862663269
I1005 04:22:38.634749 139595798787840 logging_writer.py:48] [92] global_step=92, grad_norm=0.1625751107931137, loss=0.1305403709411621
I1005 04:22:39.341846 139595790395136 logging_writer.py:48] [93] global_step=93, grad_norm=0.14952704310417175, loss=0.1282120794057846
I1005 04:22:40.066610 139595798787840 logging_writer.py:48] [94] global_step=94, grad_norm=0.12768173217773438, loss=0.1275091916322708
I1005 04:22:40.815562 139595790395136 logging_writer.py:48] [95] global_step=95, grad_norm=0.10345190763473511, loss=0.12741369009017944
I1005 04:22:41.575560 139595798787840 logging_writer.py:48] [96] global_step=96, grad_norm=0.07594523578882217, loss=0.12497107684612274
I1005 04:22:42.259291 139595790395136 logging_writer.py:48] [97] global_step=97, grad_norm=0.058021482080221176, loss=0.12478066235780716
I1005 04:22:43.064838 139595798787840 logging_writer.py:48] [98] global_step=98, grad_norm=0.058396801352500916, loss=0.12597715854644775
I1005 04:22:43.817759 139595790395136 logging_writer.py:48] [99] global_step=99, grad_norm=0.05105112865567207, loss=0.12754309177398682
I1005 04:22:44.497138 139595798787840 logging_writer.py:48] [100] global_step=100, grad_norm=0.03076092153787613, loss=0.12561845779418945
I1005 04:27:30.120234 139595790395136 logging_writer.py:48] [500] global_step=500, grad_norm=0.043021541088819504, loss=0.1314665824174881
I1005 04:33:23.326932 139595798787840 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.020477863028645515, loss=0.12128688395023346
I1005 04:39:18.337766 139595790395136 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.028942896053195, loss=0.12144698947668076
I1005 04:41:47.623306 139777607395136 spec.py:321] Evaluating on the training split.
I1005 04:44:58.093090 139777607395136 spec.py:333] Evaluating on the validation split.
I1005 04:48:06.947585 139777607395136 spec.py:349] Evaluating on the test split.
I1005 04:51:51.876850 139777607395136 submission_runner.py:381] Time since start: 2562.19s, 	Step: 1709, 	{'train/loss': 0.12715855484488625, 'validation/loss': 0.12553643434074652, 'validation/num_examples': 83274637, 'test/loss': 0.12794182105263158, 'test/num_examples': 95000000, 'score': 1226.0618574619293, 'total_duration': 2562.1877615451813, 'accumulated_submission_time': 1226.0618574619293, 'accumulated_eval_time': 1336.0723993778229, 'accumulated_logging_time': 0.028172731399536133}
I1005 04:51:51.896056 139595798787840 logging_writer.py:48] [1709] accumulated_eval_time=1336.072399, accumulated_logging_time=0.028173, accumulated_submission_time=1226.061857, global_step=1709, preemption_count=0, score=1226.061857, test/loss=0.127942, test/num_examples=95000000, total_duration=2562.187762, train/loss=0.127159, validation/loss=0.125536, validation/num_examples=83274637
I1005 04:54:57.773566 139595790395136 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.026512129232287407, loss=0.12158771604299545
I1005 05:00:54.887138 139595798787840 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.012607579119503498, loss=0.11961591988801956
I1005 05:06:55.406889 139595790395136 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.042187951505184174, loss=0.1315871924161911
I1005 05:11:51.972157 139777607395136 spec.py:321] Evaluating on the training split.
I1005 05:14:59.478261 139777607395136 spec.py:333] Evaluating on the validation split.
I1005 05:18:07.033860 139777607395136 spec.py:349] Evaluating on the test split.
I1005 05:21:45.708514 139777607395136 submission_runner.py:381] Time since start: 4356.02s, 	Step: 3415, 	{'train/loss': 0.1236513245780513, 'validation/loss': 0.12481521834793467, 'validation/num_examples': 83274637, 'test/loss': 0.12720096842105263, 'test/num_examples': 95000000, 'score': 2426.1056940555573, 'total_duration': 4356.019387245178, 'accumulated_submission_time': 2426.1056940555573, 'accumulated_eval_time': 1929.8086936473846, 'accumulated_logging_time': 0.05477094650268555}
I1005 05:21:45.728033 139595798787840 logging_writer.py:48] [3415] accumulated_eval_time=1929.808694, accumulated_logging_time=0.054771, accumulated_submission_time=2426.105694, global_step=3415, preemption_count=0, score=2426.105694, test/loss=0.127201, test/num_examples=95000000, total_duration=4356.019387, train/loss=0.123651, validation/loss=0.124815, validation/num_examples=83274637
I1005 05:22:31.574058 139595790395136 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.008113618940114975, loss=0.11998938769102097
I1005 05:28:30.479903 139595798787840 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.028066454455256462, loss=0.12086057662963867
I1005 05:34:30.222101 139595790395136 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.004054462071508169, loss=0.12589779496192932
I1005 05:40:25.973393 139595798787840 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.009813393466174603, loss=0.11992117017507553
I1005 05:41:45.982551 139777607395136 spec.py:321] Evaluating on the training split.
I1005 05:44:37.474960 139777607395136 spec.py:333] Evaluating on the validation split.
I1005 05:47:05.528116 139777607395136 spec.py:349] Evaluating on the test split.
I1005 05:50:11.619216 139777607395136 submission_runner.py:381] Time since start: 6061.93s, 	Step: 5111, 	{'train/loss': 0.12299830358733171, 'validation/loss': 0.12442820975611098, 'validation/num_examples': 83274637, 'test/loss': 0.12677748421052631, 'test/num_examples': 95000000, 'score': 3626.3280532360077, 'total_duration': 6061.930123329163, 'accumulated_submission_time': 3626.3280532360077, 'accumulated_eval_time': 2435.44531917572, 'accumulated_logging_time': 0.0821075439453125}
I1005 05:50:11.636900 139595790395136 logging_writer.py:48] [5111] accumulated_eval_time=2435.445319, accumulated_logging_time=0.082108, accumulated_submission_time=3626.328053, global_step=5111, preemption_count=0, score=3626.328053, test/loss=0.126777, test/num_examples=95000000, total_duration=6061.930123, train/loss=0.122998, validation/loss=0.124428, validation/num_examples=83274637
I1005 05:54:32.012074 139595798787840 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.008071070536971092, loss=0.11489467322826385
I1005 06:00:28.448589 139595790395136 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.012057766318321228, loss=0.1255614161491394
I1005 06:06:26.694297 139595798787840 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0050562117248773575, loss=0.1238461583852768
I1005 06:10:11.754320 139777607395136 spec.py:321] Evaluating on the training split.
I1005 06:13:03.947951 139777607395136 spec.py:333] Evaluating on the validation split.
I1005 06:15:24.723177 139777607395136 spec.py:349] Evaluating on the test split.
I1005 06:18:04.725766 139777607395136 submission_runner.py:381] Time since start: 7735.04s, 	Step: 6814, 	{'train/loss': 0.12124444253789554, 'validation/loss': 0.12392557171999441, 'validation/num_examples': 83274637, 'test/loss': 0.1262302, 'test/num_examples': 95000000, 'score': 4826.4118411540985, 'total_duration': 7735.03666472435, 'accumulated_submission_time': 4826.4118411540985, 'accumulated_eval_time': 2908.416738510132, 'accumulated_logging_time': 0.10840606689453125}
I1005 06:18:04.741737 139595790395136 logging_writer.py:48] [6814] accumulated_eval_time=2908.416739, accumulated_logging_time=0.108406, accumulated_submission_time=4826.411841, global_step=6814, preemption_count=0, score=4826.411841, test/loss=0.126230, test/num_examples=95000000, total_duration=7735.036665, train/loss=0.121244, validation/loss=0.123926, validation/num_examples=83274637
I1005 06:20:03.279684 139595798787840 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.007934044115245342, loss=0.1286892294883728
I1005 06:26:01.966225 139595790395136 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.008641785010695457, loss=0.12247554957866669
I1005 06:32:03.251570 139777607395136 spec.py:321] Evaluating on the training split.
I1005 06:34:28.555828 139777607395136 spec.py:333] Evaluating on the validation split.
I1005 06:36:24.550302 139777607395136 spec.py:349] Evaluating on the test split.
I1005 06:38:46.434948 139777607395136 submission_runner.py:381] Time since start: 8976.75s, 	Step: 8000, 	{'train/loss': 0.12079993433922341, 'validation/loss': 0.12382336773200224, 'validation/num_examples': 83274637, 'test/loss': 0.1261193157894737, 'test/num_examples': 95000000, 'score': 5664.896895170212, 'total_duration': 8976.745817184448, 'accumulated_submission_time': 5664.896895170212, 'accumulated_eval_time': 3311.6000378131866, 'accumulated_logging_time': 0.13183832168579102}
I1005 06:38:46.451077 139595798787840 logging_writer.py:48] [8000] accumulated_eval_time=3311.600038, accumulated_logging_time=0.131838, accumulated_submission_time=5664.896895, global_step=8000, preemption_count=0, score=5664.896895, test/loss=0.126119, test/num_examples=95000000, total_duration=8976.745817, train/loss=0.120800, validation/loss=0.123823, validation/num_examples=83274637
I1005 06:38:46.470283 139595790395136 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=5664.896895
I1005 06:38:53.874720 139777607395136 checkpoints.py:490] Saving checkpoint at step: 8000
I1005 06:39:30.913314 139777607395136 checkpoints.py:422] Saved checkpoint at /experiment_runs/criteo_target_resetting/nadamw_run_12/criteo1tb_jax/trial_1/checkpoint_8000
I1005 06:39:31.231514 139777607395136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/criteo_target_resetting/nadamw_run_12/criteo1tb_jax/trial_1/checkpoint_8000.
I1005 06:39:31.603393 139777607395136 submission_runner.py:549] Tuning trial 1/1
I1005 06:39:31.603678 139777607395136 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I1005 06:39:31.604879 139777607395136 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/loss': 1.1085744413939662, 'validation/loss': 1.1118760205463278, 'validation/num_examples': 83274637, 'test/loss': 1.1083299368421053, 'test/num_examples': 95000000, 'score': 25.955660104751587, 'total_duration': 757.7745907306671, 'accumulated_submission_time': 25.955660104751587, 'accumulated_eval_time': 731.8188879489899, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1709, {'train/loss': 0.12715855484488625, 'validation/loss': 0.12553643434074652, 'validation/num_examples': 83274637, 'test/loss': 0.12794182105263158, 'test/num_examples': 95000000, 'score': 1226.0618574619293, 'total_duration': 2562.1877615451813, 'accumulated_submission_time': 1226.0618574619293, 'accumulated_eval_time': 1336.0723993778229, 'accumulated_logging_time': 0.028172731399536133, 'global_step': 1709, 'preemption_count': 0}), (3415, {'train/loss': 0.1236513245780513, 'validation/loss': 0.12481521834793467, 'validation/num_examples': 83274637, 'test/loss': 0.12720096842105263, 'test/num_examples': 95000000, 'score': 2426.1056940555573, 'total_duration': 4356.019387245178, 'accumulated_submission_time': 2426.1056940555573, 'accumulated_eval_time': 1929.8086936473846, 'accumulated_logging_time': 0.05477094650268555, 'global_step': 3415, 'preemption_count': 0}), (5111, {'train/loss': 0.12299830358733171, 'validation/loss': 0.12442820975611098, 'validation/num_examples': 83274637, 'test/loss': 0.12677748421052631, 'test/num_examples': 95000000, 'score': 3626.3280532360077, 'total_duration': 6061.930123329163, 'accumulated_submission_time': 3626.3280532360077, 'accumulated_eval_time': 2435.44531917572, 'accumulated_logging_time': 0.0821075439453125, 'global_step': 5111, 'preemption_count': 0}), (6814, {'train/loss': 0.12124444253789554, 'validation/loss': 0.12392557171999441, 'validation/num_examples': 83274637, 'test/loss': 0.1262302, 'test/num_examples': 95000000, 'score': 4826.4118411540985, 'total_duration': 7735.03666472435, 'accumulated_submission_time': 4826.4118411540985, 'accumulated_eval_time': 2908.416738510132, 'accumulated_logging_time': 0.10840606689453125, 'global_step': 6814, 'preemption_count': 0}), (8000, {'train/loss': 0.12079993433922341, 'validation/loss': 0.12382336773200224, 'validation/num_examples': 83274637, 'test/loss': 0.1261193157894737, 'test/num_examples': 95000000, 'score': 5664.896895170212, 'total_duration': 8976.745817184448, 'accumulated_submission_time': 5664.896895170212, 'accumulated_eval_time': 3311.6000378131866, 'accumulated_logging_time': 0.13183832168579102, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I1005 06:39:31.605079 139777607395136 submission_runner.py:552] Timing: 5664.896895170212
I1005 06:39:31.605204 139777607395136 submission_runner.py:554] Total number of evals: 6
I1005 06:39:31.605312 139777607395136 submission_runner.py:555] ====================
I1005 06:39:31.605507 139777607395136 submission_runner.py:625] Final criteo1tb score: 5664.896895170212
