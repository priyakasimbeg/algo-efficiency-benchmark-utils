python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_jax_05-02-2023-02-25-22.log
I0502 02:25:43.481874 140016461956928 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_lamb/ogbg_jax.
I0502 02:25:43.556555 140016461956928 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 02:25:44.429911 140016461956928 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0502 02:25:44.430611 140016461956928 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 02:25:44.434530 140016461956928 submission_runner.py:538] Using RNG seed 1083470331
I0502 02:25:47.177917 140016461956928 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 02:25:47.178128 140016461956928 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_lamb/ogbg_jax/trial_1.
I0502 02:25:47.178419 140016461956928 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_lamb/ogbg_jax/trial_1/hparams.json.
I0502 02:25:47.303634 140016461956928 submission_runner.py:241] Initializing dataset.
I0502 02:25:47.542732 140016461956928 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:25:47.548444 140016461956928 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 02:25:47.775928 140016461956928 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:25:47.830775 140016461956928 submission_runner.py:248] Initializing model.
I0502 02:25:55.863111 140016461956928 submission_runner.py:258] Initializing optimizer.
I0502 02:25:56.260544 140016461956928 submission_runner.py:265] Initializing metrics bundle.
I0502 02:25:56.260811 140016461956928 submission_runner.py:282] Initializing checkpoint and logger.
I0502 02:25:56.261908 140016461956928 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_lamb/ogbg_jax/trial_1 with prefix checkpoint_
I0502 02:25:56.262168 140016461956928 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0502 02:25:56.262235 140016461956928 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0502 02:25:57.189148 140016461956928 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_lamb/ogbg_jax/trial_1/meta_data_0.json.
I0502 02:25:57.190221 140016461956928 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_lamb/ogbg_jax/trial_1/flags_0.json.
I0502 02:25:57.196426 140016461956928 submission_runner.py:318] Starting training loop.
I0502 02:26:24.233995 139840511276800 logging_writer.py:48] [0] global_step=0, grad_norm=2.660626173019409, loss=0.7244760394096375
I0502 02:26:24.248420 140016461956928 spec.py:298] Evaluating on the training split.
I0502 02:26:24.256859 140016461956928 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:26:24.260851 140016461956928 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 02:26:24.318794 140016461956928 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:27:57.460521 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 02:27:57.463439 140016461956928 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:27:57.467052 140016461956928 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 02:27:57.520649 140016461956928 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:29:01.800632 140016461956928 spec.py:326] Evaluating on the test split.
I0502 02:29:01.803683 140016461956928 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:29:01.808095 140016461956928 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 02:29:01.872316 140016461956928 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:30:06.702548 140016461956928 submission_runner.py:415] Time since start: 249.51s, 	Step: 1, 	{'train/accuracy': 0.5382710695266724, 'train/loss': 0.7247635722160339, 'train/mean_average_precision': 0.02161470177426399, 'validation/accuracy': 0.5399413108825684, 'validation/loss': 0.7201366424560547, 'validation/mean_average_precision': 0.025543758513285417, 'validation/num_examples': 43793, 'test/accuracy': 0.5411576628684998, 'test/loss': 0.7186667919158936, 'test/mean_average_precision': 0.02664824824004646, 'test/num_examples': 43793, 'score': 27.051802158355713, 'total_duration': 249.50607061386108, 'accumulated_submission_time': 27.051802158355713, 'accumulated_eval_time': 222.45410537719727, 'accumulated_logging_time': 0}
I0502 02:30:06.718456 139830427719424 logging_writer.py:48] [1] accumulated_eval_time=222.454105, accumulated_logging_time=0, accumulated_submission_time=27.051802, global_step=1, preemption_count=0, score=27.051802, test/accuracy=0.541158, test/loss=0.718667, test/mean_average_precision=0.026648, test/num_examples=43793, total_duration=249.506071, train/accuracy=0.538271, train/loss=0.724764, train/mean_average_precision=0.021615, validation/accuracy=0.539941, validation/loss=0.720137, validation/mean_average_precision=0.025544, validation/num_examples=43793
I0502 02:30:31.482113 139830436112128 logging_writer.py:48] [100] global_step=100, grad_norm=1.513038158416748, loss=0.5961819887161255
I0502 02:30:56.138689 139830427719424 logging_writer.py:48] [200] global_step=200, grad_norm=0.7045657634735107, loss=0.4644055962562561
I0502 02:31:20.930909 139830436112128 logging_writer.py:48] [300] global_step=300, grad_norm=0.5468174815177917, loss=0.3775331377983093
I0502 02:31:45.042915 139830427719424 logging_writer.py:48] [400] global_step=400, grad_norm=0.4359402060508728, loss=0.31412869691848755
I0502 02:32:09.410119 139830436112128 logging_writer.py:48] [500] global_step=500, grad_norm=0.3567713499069214, loss=0.24369223415851593
I0502 02:32:33.852158 139830427719424 logging_writer.py:48] [600] global_step=600, grad_norm=0.29103049635887146, loss=0.17591188848018646
I0502 02:32:59.431826 139830436112128 logging_writer.py:48] [700] global_step=700, grad_norm=0.14967504143714905, loss=0.11463048309087753
I0502 02:33:24.170626 139830427719424 logging_writer.py:48] [800] global_step=800, grad_norm=0.12442822009325027, loss=0.07756918668746948
I0502 02:33:48.604543 139830436112128 logging_writer.py:48] [900] global_step=900, grad_norm=0.1425437033176422, loss=0.057699643075466156
I0502 02:34:06.732970 140016461956928 spec.py:298] Evaluating on the training split.
I0502 02:35:21.671549 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 02:35:24.277040 140016461956928 spec.py:326] Evaluating on the test split.
I0502 02:35:26.841750 140016461956928 submission_runner.py:415] Time since start: 569.65s, 	Step: 974, 	{'train/accuracy': 0.9867652058601379, 'train/loss': 0.05646629258990288, 'train/mean_average_precision': 0.04542538524388646, 'validation/accuracy': 0.9841195344924927, 'validation/loss': 0.065689317882061, 'validation/mean_average_precision': 0.046208158081691174, 'validation/num_examples': 43793, 'test/accuracy': 0.983147144317627, 'test/loss': 0.06882942467927933, 'test/mean_average_precision': 0.04802407544387797, 'test/num_examples': 43793, 'score': 267.04665422439575, 'total_duration': 569.6452596187592, 'accumulated_submission_time': 267.04665422439575, 'accumulated_eval_time': 302.56285071372986, 'accumulated_logging_time': 0.026891469955444336}
I0502 02:35:26.849673 139830427719424 logging_writer.py:48] [974] accumulated_eval_time=302.562851, accumulated_logging_time=0.026891, accumulated_submission_time=267.046654, global_step=974, preemption_count=0, score=267.046654, test/accuracy=0.983147, test/loss=0.068829, test/mean_average_precision=0.048024, test/num_examples=43793, total_duration=569.645260, train/accuracy=0.986765, train/loss=0.056466, train/mean_average_precision=0.045425, validation/accuracy=0.984120, validation/loss=0.065689, validation/mean_average_precision=0.046208, validation/num_examples=43793
I0502 02:35:33.910330 139830436112128 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.10816953331232071, loss=0.051708709448575974
I0502 02:35:58.717556 139830427719424 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.07375451177358627, loss=0.05150335282087326
I0502 02:36:23.452026 139830436112128 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.23496726155281067, loss=0.05084184929728508
I0502 02:36:48.194929 139830427719424 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.11972065269947052, loss=0.05809251219034195
I0502 02:37:13.169544 139830436112128 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.2996014654636383, loss=0.04793289676308632
I0502 02:37:37.507585 139830427719424 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.10776260495185852, loss=0.05089351162314415
I0502 02:38:02.361077 139830436112128 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.19453130662441254, loss=0.048525284975767136
I0502 02:38:27.020204 139830427719424 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.09023954719305038, loss=0.04376322403550148
I0502 02:38:51.571395 139830436112128 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.27682578563690186, loss=0.04888835549354553
I0502 02:39:16.200349 139830427719424 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.1894022524356842, loss=0.04577651619911194
I0502 02:39:27.068316 140016461956928 spec.py:298] Evaluating on the training split.
I0502 02:40:41.368427 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 02:40:43.975593 140016461956928 spec.py:326] Evaluating on the test split.
I0502 02:40:46.539352 140016461956928 submission_runner.py:415] Time since start: 889.34s, 	Step: 1945, 	{'train/accuracy': 0.9870240092277527, 'train/loss': 0.0484481155872345, 'train/mean_average_precision': 0.0909829737696722, 'validation/accuracy': 0.9843221306800842, 'validation/loss': 0.05845212936401367, 'validation/mean_average_precision': 0.09142817036587084, 'validation/num_examples': 43793, 'test/accuracy': 0.9833080172538757, 'test/loss': 0.061896536499261856, 'test/mean_average_precision': 0.08979671008062669, 'test/num_examples': 43793, 'score': 507.24751901626587, 'total_duration': 889.3428633213043, 'accumulated_submission_time': 507.24751901626587, 'accumulated_eval_time': 382.0338718891144, 'accumulated_logging_time': 0.04410696029663086}
I0502 02:40:46.547292 139830436112128 logging_writer.py:48] [1945] accumulated_eval_time=382.033872, accumulated_logging_time=0.044107, accumulated_submission_time=507.247519, global_step=1945, preemption_count=0, score=507.247519, test/accuracy=0.983308, test/loss=0.061897, test/mean_average_precision=0.089797, test/num_examples=43793, total_duration=889.342863, train/accuracy=0.987024, train/loss=0.048448, train/mean_average_precision=0.090983, validation/accuracy=0.984322, validation/loss=0.058452, validation/mean_average_precision=0.091428, validation/num_examples=43793
I0502 02:41:00.123571 139830427719424 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.08656256645917892, loss=0.045858994126319885
I0502 02:41:24.829938 139830436112128 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.06781694293022156, loss=0.047392748296260834
I0502 02:41:49.462922 139830427719424 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.15398117899894714, loss=0.051549218595027924
I0502 02:42:14.189264 139830436112128 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.1217627227306366, loss=0.047987665981054306
I0502 02:42:38.567178 139830427719424 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.102590411901474, loss=0.04441201686859131
I0502 02:43:03.054176 139830436112128 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.09072025120258331, loss=0.04486024007201195
I0502 02:43:27.559662 139830427719424 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.176035538315773, loss=0.046909648925065994
I0502 02:43:51.869151 139830436112128 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.10411600768566132, loss=0.04277411848306656
I0502 02:44:16.233575 139830427719424 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.20440639555454254, loss=0.050091907382011414
I0502 02:44:40.617313 139830436112128 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.1710718721151352, loss=0.04319975897669792
I0502 02:44:46.619313 140016461956928 spec.py:298] Evaluating on the training split.
I0502 02:46:00.956963 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 02:46:03.641240 140016461956928 spec.py:326] Evaluating on the test split.
I0502 02:46:06.224944 140016461956928 submission_runner.py:415] Time since start: 1209.03s, 	Step: 2925, 	{'train/accuracy': 0.9876962900161743, 'train/loss': 0.044443048536777496, 'train/mean_average_precision': 0.13600497369939313, 'validation/accuracy': 0.9849521517753601, 'validation/loss': 0.05332230031490326, 'validation/mean_average_precision': 0.13055990526214517, 'validation/num_examples': 43793, 'test/accuracy': 0.9839882254600525, 'test/loss': 0.05628528445959091, 'test/mean_average_precision': 0.1354805088205757, 'test/num_examples': 43793, 'score': 747.3013498783112, 'total_duration': 1209.0284264087677, 'accumulated_submission_time': 747.3013498783112, 'accumulated_eval_time': 461.63943910598755, 'accumulated_logging_time': 0.061554908752441406}
I0502 02:46:06.232592 139830427719424 logging_writer.py:48] [2925] accumulated_eval_time=461.639439, accumulated_logging_time=0.061555, accumulated_submission_time=747.301350, global_step=2925, preemption_count=0, score=747.301350, test/accuracy=0.983988, test/loss=0.056285, test/mean_average_precision=0.135481, test/num_examples=43793, total_duration=1209.028426, train/accuracy=0.987696, train/loss=0.044443, train/mean_average_precision=0.136005, validation/accuracy=0.984952, validation/loss=0.053322, validation/mean_average_precision=0.130560, validation/num_examples=43793
I0502 02:46:24.632097 139830436112128 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.21674370765686035, loss=0.04864269867539406
I0502 02:46:48.969254 139830427719424 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.14169944822788239, loss=0.046383488923311234
I0502 02:47:13.623282 139830436112128 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.12217684090137482, loss=0.04100196063518524
I0502 02:47:38.273877 139830427719424 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.1501268744468689, loss=0.04928106814622879
I0502 02:48:02.879106 139830436112128 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.07053292542695999, loss=0.04636033996939659
I0502 02:48:27.368515 139830427719424 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.09249452501535416, loss=0.04104738309979439
I0502 02:48:52.127508 139830436112128 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.11118333786725998, loss=0.043876588344573975
I0502 02:49:16.621928 139830427719424 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0842316523194313, loss=0.044719621539115906
I0502 02:49:40.895821 139830436112128 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.07206760346889496, loss=0.04270936921238899
I0502 02:50:05.337256 139830427719424 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0673016682267189, loss=0.04558455944061279
I0502 02:50:06.309191 140016461956928 spec.py:298] Evaluating on the training split.
I0502 02:51:19.686112 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 02:51:22.275942 140016461956928 spec.py:326] Evaluating on the test split.
I0502 02:51:24.862178 140016461956928 submission_runner.py:415] Time since start: 1527.67s, 	Step: 3905, 	{'train/accuracy': 0.9880914688110352, 'train/loss': 0.04212948679924011, 'train/mean_average_precision': 0.1726979036151406, 'validation/accuracy': 0.9852420091629028, 'validation/loss': 0.052216753363609314, 'validation/mean_average_precision': 0.15972083335216697, 'validation/num_examples': 43793, 'test/accuracy': 0.9842885732650757, 'test/loss': 0.05533430352807045, 'test/mean_average_precision': 0.15635061477885354, 'test/num_examples': 43793, 'score': 987.359974861145, 'total_duration': 1527.6656908988953, 'accumulated_submission_time': 987.359974861145, 'accumulated_eval_time': 540.1923894882202, 'accumulated_logging_time': 0.07853269577026367}
I0502 02:51:24.870196 139830436112128 logging_writer.py:48] [3905] accumulated_eval_time=540.192389, accumulated_logging_time=0.078533, accumulated_submission_time=987.359975, global_step=3905, preemption_count=0, score=987.359975, test/accuracy=0.984289, test/loss=0.055334, test/mean_average_precision=0.156351, test/num_examples=43793, total_duration=1527.665691, train/accuracy=0.988091, train/loss=0.042129, train/mean_average_precision=0.172698, validation/accuracy=0.985242, validation/loss=0.052217, validation/mean_average_precision=0.159721, validation/num_examples=43793
I0502 02:51:48.476155 139830427719424 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.061510223895311356, loss=0.03953879699110985
I0502 02:52:13.100857 139830436112128 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.06260596960783005, loss=0.04440277814865112
I0502 02:52:37.908699 139830427719424 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.07513277977705002, loss=0.047346025705337524
I0502 02:53:02.979477 139830436112128 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.04501538351178169, loss=0.045985374599695206
I0502 02:53:27.003447 139830427719424 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.07394271343946457, loss=0.041029106825590134
I0502 02:53:51.514491 139830436112128 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.04995911568403244, loss=0.04245981201529503
I0502 02:54:15.680187 139830427719424 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.05368146300315857, loss=0.046533357352018356
I0502 02:54:40.193941 139830436112128 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0526217445731163, loss=0.04587554186582565
I0502 02:55:04.778092 139830427719424 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.07222295552492142, loss=0.04691481590270996
I0502 02:55:25.078531 140016461956928 spec.py:298] Evaluating on the training split.
I0502 02:56:39.522346 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 02:56:42.092469 140016461956928 spec.py:326] Evaluating on the test split.
I0502 02:56:44.610167 140016461956928 submission_runner.py:415] Time since start: 1847.41s, 	Step: 4885, 	{'train/accuracy': 0.9884511828422546, 'train/loss': 0.039487335830926895, 'train/mean_average_precision': 0.221168105531605, 'validation/accuracy': 0.9855582118034363, 'validation/loss': 0.04935424402356148, 'validation/mean_average_precision': 0.18249816776621405, 'validation/num_examples': 43793, 'test/accuracy': 0.9846023321151733, 'test/loss': 0.05213505029678345, 'test/mean_average_precision': 0.18075974949238585, 'test/num_examples': 43793, 'score': 1227.550749540329, 'total_duration': 1847.41366147995, 'accumulated_submission_time': 1227.550749540329, 'accumulated_eval_time': 619.7239730358124, 'accumulated_logging_time': 0.09557342529296875}
I0502 02:56:44.618415 139830436112128 logging_writer.py:48] [4885] accumulated_eval_time=619.723973, accumulated_logging_time=0.095573, accumulated_submission_time=1227.550750, global_step=4885, preemption_count=0, score=1227.550750, test/accuracy=0.984602, test/loss=0.052135, test/mean_average_precision=0.180760, test/num_examples=43793, total_duration=1847.413661, train/accuracy=0.988451, train/loss=0.039487, train/mean_average_precision=0.221168, validation/accuracy=0.985558, validation/loss=0.049354, validation/mean_average_precision=0.182498, validation/num_examples=43793
I0502 02:56:48.625205 139830427719424 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.052718888968229294, loss=0.03971051424741745
I0502 02:57:12.930637 139830436112128 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.03532745689153671, loss=0.04414307698607445
I0502 02:57:37.047393 139830427719424 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.09316190332174301, loss=0.040192604064941406
I0502 02:58:01.235255 139830436112128 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.07723186165094376, loss=0.04490092024207115
I0502 02:58:25.644895 139830427719424 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.035246483981609344, loss=0.041899073868989944
I0502 02:58:50.353394 139830436112128 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.04963609576225281, loss=0.04047892242670059
I0502 02:59:15.037095 139830427719424 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.06972412765026093, loss=0.04063316807150841
I0502 02:59:39.533258 139830436112128 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.05977267771959305, loss=0.042403388768434525
I0502 03:00:04.082096 139830427719424 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.05627844110131264, loss=0.04149729758501053
I0502 03:00:28.439243 139830436112128 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.04028099775314331, loss=0.04471876472234726
I0502 03:00:44.631561 140016461956928 spec.py:298] Evaluating on the training split.
I0502 03:02:00.314403 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 03:02:03.063974 140016461956928 spec.py:326] Evaluating on the test split.
I0502 03:02:05.872660 140016461956928 submission_runner.py:415] Time since start: 2168.68s, 	Step: 5867, 	{'train/accuracy': 0.9889355301856995, 'train/loss': 0.03809946030378342, 'train/mean_average_precision': 0.24073140125080847, 'validation/accuracy': 0.9858906865119934, 'validation/loss': 0.04771829769015312, 'validation/mean_average_precision': 0.20280648132348084, 'validation/num_examples': 43793, 'test/accuracy': 0.9850205779075623, 'test/loss': 0.05028391256928444, 'test/mean_average_precision': 0.20410043645349357, 'test/num_examples': 43793, 'score': 1467.5451378822327, 'total_duration': 2168.6761581897736, 'accumulated_submission_time': 1467.5451378822327, 'accumulated_eval_time': 700.9650220870972, 'accumulated_logging_time': 0.11403393745422363}
I0502 03:02:05.880384 139830427719424 logging_writer.py:48] [5867] accumulated_eval_time=700.965022, accumulated_logging_time=0.114034, accumulated_submission_time=1467.545138, global_step=5867, preemption_count=0, score=1467.545138, test/accuracy=0.985021, test/loss=0.050284, test/mean_average_precision=0.204100, test/num_examples=43793, total_duration=2168.676158, train/accuracy=0.988936, train/loss=0.038099, train/mean_average_precision=0.240731, validation/accuracy=0.985891, validation/loss=0.047718, validation/mean_average_precision=0.202806, validation/num_examples=43793
I0502 03:02:14.054286 139830436112128 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.07283848524093628, loss=0.04629595950245857
I0502 03:02:38.025213 139830427719424 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.04077313095331192, loss=0.04149708151817322
I0502 03:03:02.390361 139830436112128 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.07734128832817078, loss=0.043319616466760635
I0502 03:03:26.693815 139830427719424 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.05701127648353577, loss=0.03880561515688896
I0502 03:03:51.325492 139830436112128 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.04878542572259903, loss=0.04299396649003029
I0502 03:04:15.652810 139830427719424 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.04339693859219551, loss=0.039051856845617294
I0502 03:04:39.714431 139830436112128 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.039905332028865814, loss=0.0388869047164917
I0502 03:05:04.376332 139830427719424 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.08751577883958817, loss=0.04136855900287628
I0502 03:05:29.508526 139830436112128 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.06144537776708603, loss=0.040330592542886734
I0502 03:05:54.472632 139830427719424 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.04057343676686287, loss=0.0409572459757328
I0502 03:06:05.960525 140016461956928 spec.py:298] Evaluating on the training split.
I0502 03:07:21.024665 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 03:07:23.602243 140016461956928 spec.py:326] Evaluating on the test split.
I0502 03:07:26.145331 140016461956928 submission_runner.py:415] Time since start: 2488.95s, 	Step: 6847, 	{'train/accuracy': 0.9891735911369324, 'train/loss': 0.036761123687028885, 'train/mean_average_precision': 0.2711986768694797, 'validation/accuracy': 0.9860624074935913, 'validation/loss': 0.046917904168367386, 'validation/mean_average_precision': 0.21011351662172223, 'validation/num_examples': 43793, 'test/accuracy': 0.9852067828178406, 'test/loss': 0.04956843703985214, 'test/mean_average_precision': 0.21441966426921308, 'test/num_examples': 43793, 'score': 1707.6073698997498, 'total_duration': 2488.9488427639008, 'accumulated_submission_time': 1707.6073698997498, 'accumulated_eval_time': 781.1497938632965, 'accumulated_logging_time': 0.13106989860534668}
I0502 03:07:26.153702 139830436112128 logging_writer.py:48] [6847] accumulated_eval_time=781.149794, accumulated_logging_time=0.131070, accumulated_submission_time=1707.607370, global_step=6847, preemption_count=0, score=1707.607370, test/accuracy=0.985207, test/loss=0.049568, test/mean_average_precision=0.214420, test/num_examples=43793, total_duration=2488.948843, train/accuracy=0.989174, train/loss=0.036761, train/mean_average_precision=0.271199, validation/accuracy=0.986062, validation/loss=0.046918, validation/mean_average_precision=0.210114, validation/num_examples=43793
I0502 03:07:39.623001 139830427719424 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.060128308832645416, loss=0.04065163433551788
I0502 03:08:04.275365 139830436112128 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.04253707826137543, loss=0.03920853137969971
I0502 03:08:28.399998 139830427719424 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.050118621438741684, loss=0.042491525411605835
I0502 03:08:52.738554 139830436112128 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0452374666929245, loss=0.041864994913339615
I0502 03:09:17.542609 139830427719424 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.029691234230995178, loss=0.037656135857105255
I0502 03:09:41.946325 139830436112128 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.048602793365716934, loss=0.03844371438026428
I0502 03:10:05.089231 139830427719424 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.04705425724387169, loss=0.03743164241313934
I0502 03:10:28.222535 139830436112128 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.05074141547083855, loss=0.04019325226545334
I0502 03:10:51.303649 139830427719424 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.059263598173856735, loss=0.03685091808438301
I0502 03:11:14.439391 139830436112128 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.035659875720739365, loss=0.036680225282907486
I0502 03:11:26.335377 140016461956928 spec.py:298] Evaluating on the training split.
I0502 03:12:38.710643 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 03:12:41.238545 140016461956928 spec.py:326] Evaluating on the test split.
I0502 03:12:43.722669 140016461956928 submission_runner.py:415] Time since start: 2806.53s, 	Step: 7853, 	{'train/accuracy': 0.989593505859375, 'train/loss': 0.03558668494224548, 'train/mean_average_precision': 0.29176598340064, 'validation/accuracy': 0.9861817359924316, 'validation/loss': 0.04656646400690079, 'validation/mean_average_precision': 0.22040746597287028, 'validation/num_examples': 43793, 'test/accuracy': 0.985296905040741, 'test/loss': 0.04927525669336319, 'test/mean_average_precision': 0.22198119898760946, 'test/num_examples': 43793, 'score': 1947.7708423137665, 'total_duration': 2806.5261702537537, 'accumulated_submission_time': 1947.7708423137665, 'accumulated_eval_time': 858.5370628833771, 'accumulated_logging_time': 0.14883756637573242}
I0502 03:12:43.736884 139830427719424 logging_writer.py:48] [7853] accumulated_eval_time=858.537063, accumulated_logging_time=0.148838, accumulated_submission_time=1947.770842, global_step=7853, preemption_count=0, score=1947.770842, test/accuracy=0.985297, test/loss=0.049275, test/mean_average_precision=0.221981, test/num_examples=43793, total_duration=2806.526170, train/accuracy=0.989594, train/loss=0.035587, train/mean_average_precision=0.291766, validation/accuracy=0.986182, validation/loss=0.046566, validation/mean_average_precision=0.220407, validation/num_examples=43793
I0502 03:12:54.815077 139830436112128 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.03467114642262459, loss=0.03822881355881691
I0502 03:13:18.178421 139830427719424 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.03927084803581238, loss=0.0361315980553627
I0502 03:13:41.236181 139830436112128 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.07530391961336136, loss=0.0375836081802845
I0502 03:14:04.362690 139830427719424 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0353890135884285, loss=0.040134720504283905
I0502 03:14:27.718625 139830436112128 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.03987984359264374, loss=0.034129198640584946
I0502 03:14:50.994560 139830427719424 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.05401061847805977, loss=0.03672154247760773
I0502 03:15:14.158822 139830436112128 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.052003007382154465, loss=0.0414988175034523
I0502 03:15:37.102887 139830427719424 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0382726714015007, loss=0.03545001521706581
I0502 03:16:00.275463 139830436112128 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.04755936190485954, loss=0.036273397505283356
I0502 03:16:23.381400 139830427719424 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.042357187718153, loss=0.04000852629542351
I0502 03:16:43.970074 140016461956928 spec.py:298] Evaluating on the training split.
I0502 03:17:56.420122 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 03:17:58.930207 140016461956928 spec.py:326] Evaluating on the test split.
I0502 03:18:01.384991 140016461956928 submission_runner.py:415] Time since start: 3124.19s, 	Step: 8890, 	{'train/accuracy': 0.9898359179496765, 'train/loss': 0.034257907420396805, 'train/mean_average_precision': 0.33647031018609763, 'validation/accuracy': 0.9862204790115356, 'validation/loss': 0.04632710665464401, 'validation/mean_average_precision': 0.23348770862249096, 'validation/num_examples': 43793, 'test/accuracy': 0.9853171110153198, 'test/loss': 0.04897666722536087, 'test/mean_average_precision': 0.23489984573031308, 'test/num_examples': 43793, 'score': 2187.9864826202393, 'total_duration': 3124.1885104179382, 'accumulated_submission_time': 2187.9864826202393, 'accumulated_eval_time': 935.9519424438477, 'accumulated_logging_time': 0.17164063453674316}
I0502 03:18:01.393009 139830436112128 logging_writer.py:48] [8890] accumulated_eval_time=935.951942, accumulated_logging_time=0.171641, accumulated_submission_time=2187.986483, global_step=8890, preemption_count=0, score=2187.986483, test/accuracy=0.985317, test/loss=0.048977, test/mean_average_precision=0.234900, test/num_examples=43793, total_duration=3124.188510, train/accuracy=0.989836, train/loss=0.034258, train/mean_average_precision=0.336470, validation/accuracy=0.986220, validation/loss=0.046327, validation/mean_average_precision=0.233488, validation/num_examples=43793
I0502 03:18:04.027558 139830427719424 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.04341696947813034, loss=0.04170018807053566
I0502 03:18:27.237936 139830436112128 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0398704893887043, loss=0.03738773986697197
I0502 03:18:50.431953 139830427719424 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.03171928972005844, loss=0.03919633477926254
I0502 03:19:13.646979 139830436112128 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.04530540853738785, loss=0.043848488479852676
I0502 03:19:37.049595 139830427719424 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.041065048426389694, loss=0.040817152708768845
I0502 03:20:00.243510 139830436112128 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.04157184809446335, loss=0.033436574041843414
I0502 03:20:23.228461 139830427719424 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.05254565551877022, loss=0.03686796501278877
I0502 03:20:46.078537 139830436112128 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.03327077627182007, loss=0.038790300488471985
I0502 03:21:09.144129 139830427719424 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.041896332055330276, loss=0.038258638232946396
I0502 03:21:32.066238 139830436112128 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.05118555948138237, loss=0.03606921061873436
I0502 03:21:55.118770 139830427719424 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.03348028287291527, loss=0.035648297518491745
I0502 03:22:01.406150 140016461956928 spec.py:298] Evaluating on the training split.
I0502 03:23:13.671294 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 03:23:16.170453 140016461956928 spec.py:326] Evaluating on the test split.
I0502 03:23:18.589025 140016461956928 submission_runner.py:415] Time since start: 3441.39s, 	Step: 9928, 	{'train/accuracy': 0.9901276230812073, 'train/loss': 0.03325457125902176, 'train/mean_average_precision': 0.34926035983155845, 'validation/accuracy': 0.9863680601119995, 'validation/loss': 0.04596709832549095, 'validation/mean_average_precision': 0.2404337163780253, 'validation/num_examples': 43793, 'test/accuracy': 0.9855133891105652, 'test/loss': 0.04880305007100105, 'test/mean_average_precision': 0.23969024173979178, 'test/num_examples': 43793, 'score': 2427.980811357498, 'total_duration': 3441.3925454616547, 'accumulated_submission_time': 2427.980811357498, 'accumulated_eval_time': 1013.1348078250885, 'accumulated_logging_time': 0.1896076202392578}
I0502 03:23:18.597131 139830436112128 logging_writer.py:48] [9928] accumulated_eval_time=1013.134808, accumulated_logging_time=0.189608, accumulated_submission_time=2427.980811, global_step=9928, preemption_count=0, score=2427.980811, test/accuracy=0.985513, test/loss=0.048803, test/mean_average_precision=0.239690, test/num_examples=43793, total_duration=3441.392545, train/accuracy=0.990128, train/loss=0.033255, train/mean_average_precision=0.349260, validation/accuracy=0.986368, validation/loss=0.045967, validation/mean_average_precision=0.240434, validation/num_examples=43793
I0502 03:23:35.375887 139830427719424 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.037124212831258774, loss=0.039480071514844894
I0502 03:23:58.487965 139830436112128 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0526832640171051, loss=0.03745277225971222
I0502 03:24:21.290925 139830427719424 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.04763099551200867, loss=0.03465856611728668
I0502 03:24:43.848370 139830436112128 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.031936828047037125, loss=0.03630717471241951
I0502 03:25:06.555802 139830427719424 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.03489205241203308, loss=0.03406025096774101
I0502 03:25:29.044886 139830436112128 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.039189063012599945, loss=0.037905819714069366
I0502 03:25:51.382808 139830427719424 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.07559283822774887, loss=0.04018544405698776
I0502 03:26:13.925770 139830436112128 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.036712948232889175, loss=0.03600819781422615
I0502 03:26:36.370993 139830427719424 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0492531880736351, loss=0.03348921984434128
I0502 03:26:59.073503 139830436112128 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.04186689853668213, loss=0.03832181170582771
I0502 03:27:18.821203 140016461956928 spec.py:298] Evaluating on the training split.
I0502 03:28:28.313774 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 03:28:30.833836 140016461956928 spec.py:326] Evaluating on the test split.
I0502 03:28:33.496864 140016461956928 submission_runner.py:415] Time since start: 3756.30s, 	Step: 10988, 	{'train/accuracy': 0.9901374578475952, 'train/loss': 0.0328630693256855, 'train/mean_average_precision': 0.3613298561420277, 'validation/accuracy': 0.9865052700042725, 'validation/loss': 0.04598310589790344, 'validation/mean_average_precision': 0.24557890857677883, 'validation/num_examples': 43793, 'test/accuracy': 0.9856982827186584, 'test/loss': 0.04875835403800011, 'test/mean_average_precision': 0.2467232486206807, 'test/num_examples': 43793, 'score': 2668.187639951706, 'total_duration': 3756.300361394882, 'accumulated_submission_time': 2668.187639951706, 'accumulated_eval_time': 1087.8104150295258, 'accumulated_logging_time': 0.20604181289672852}
I0502 03:28:33.505602 139830427719424 logging_writer.py:48] [10988] accumulated_eval_time=1087.810415, accumulated_logging_time=0.206042, accumulated_submission_time=2668.187640, global_step=10988, preemption_count=0, score=2668.187640, test/accuracy=0.985698, test/loss=0.048758, test/mean_average_precision=0.246723, test/num_examples=43793, total_duration=3756.300361, train/accuracy=0.990137, train/loss=0.032863, train/mean_average_precision=0.361330, validation/accuracy=0.986505, validation/loss=0.045983, validation/mean_average_precision=0.245579, validation/num_examples=43793
I0502 03:28:36.433963 139830436112128 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.03229794651269913, loss=0.03761424496769905
I0502 03:28:59.199851 139830427719424 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.03594408184289932, loss=0.03595181182026863
I0502 03:29:21.909297 139830436112128 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.04136855900287628, loss=0.03797950968146324
I0502 03:29:44.564769 139830427719424 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.03353343904018402, loss=0.03655444458127022
I0502 03:30:07.244983 139830436112128 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.035394344478845596, loss=0.03785603865981102
I0502 03:30:29.780751 139830427719424 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.044076643884181976, loss=0.03699715435504913
I0502 03:30:52.402711 139830436112128 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.03758696839213371, loss=0.035433512181043625
I0502 03:31:15.053162 139830427719424 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.03466295823454857, loss=0.03201266750693321
I0502 03:31:37.482440 139830436112128 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.044321175664663315, loss=0.0358131006360054
I0502 03:32:00.021558 139830427719424 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.048095617443323135, loss=0.03858871012926102
I0502 03:32:22.696156 140016461956928 spec.py:298] Evaluating on the training split.
I0502 03:33:31.972483 140016461956928 spec.py:310] Evaluating on the validation split.
I0502 03:33:34.613982 140016461956928 spec.py:326] Evaluating on the test split.
I0502 03:33:37.048982 140016461956928 submission_runner.py:415] Time since start: 4059.85s, 	Step: 12000, 	{'train/accuracy': 0.9908209443092346, 'train/loss': 0.03074347786605358, 'train/mean_average_precision': 0.40663552016201665, 'validation/accuracy': 0.9865750670433044, 'validation/loss': 0.04496759548783302, 'validation/mean_average_precision': 0.2557580824516941, 'validation/num_examples': 43793, 'test/accuracy': 0.9857316017150879, 'test/loss': 0.04770534485578537, 'test/mean_average_precision': 0.2536670584972984, 'test/num_examples': 43793, 'score': 2897.361358642578, 'total_duration': 4059.8524990081787, 'accumulated_submission_time': 2897.361358642578, 'accumulated_eval_time': 1162.1632025241852, 'accumulated_logging_time': 0.22313928604125977}
I0502 03:33:37.057212 139830436112128 logging_writer.py:48] [12000] accumulated_eval_time=1162.163203, accumulated_logging_time=0.223139, accumulated_submission_time=2897.361359, global_step=12000, preemption_count=0, score=2897.361359, test/accuracy=0.985732, test/loss=0.047705, test/mean_average_precision=0.253667, test/num_examples=43793, total_duration=4059.852499, train/accuracy=0.990821, train/loss=0.030743, train/mean_average_precision=0.406636, validation/accuracy=0.986575, validation/loss=0.044968, validation/mean_average_precision=0.255758, validation/num_examples=43793
I0502 03:33:37.071811 139830427719424 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2897.361359
I0502 03:33:37.097767 140016461956928 checkpoints.py:356] Saving checkpoint at step: 12000
I0502 03:33:37.189834 140016461956928 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_lamb/ogbg_jax/trial_1/checkpoint_12000
I0502 03:33:37.190255 140016461956928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_lamb/ogbg_jax/trial_1/checkpoint_12000.
I0502 03:33:37.333819 140016461956928 submission_runner.py:578] Tuning trial 1/1
I0502 03:33:37.334029 140016461956928 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0502 03:33:37.335141 140016461956928 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5382710695266724, 'train/loss': 0.7247635722160339, 'train/mean_average_precision': 0.02161470177426399, 'validation/accuracy': 0.5399413108825684, 'validation/loss': 0.7201366424560547, 'validation/mean_average_precision': 0.025543758513285417, 'validation/num_examples': 43793, 'test/accuracy': 0.5411576628684998, 'test/loss': 0.7186667919158936, 'test/mean_average_precision': 0.02664824824004646, 'test/num_examples': 43793, 'score': 27.051802158355713, 'total_duration': 249.50607061386108, 'accumulated_submission_time': 27.051802158355713, 'accumulated_eval_time': 222.45410537719727, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (974, {'train/accuracy': 0.9867652058601379, 'train/loss': 0.05646629258990288, 'train/mean_average_precision': 0.04542538524388646, 'validation/accuracy': 0.9841195344924927, 'validation/loss': 0.065689317882061, 'validation/mean_average_precision': 0.046208158081691174, 'validation/num_examples': 43793, 'test/accuracy': 0.983147144317627, 'test/loss': 0.06882942467927933, 'test/mean_average_precision': 0.04802407544387797, 'test/num_examples': 43793, 'score': 267.04665422439575, 'total_duration': 569.6452596187592, 'accumulated_submission_time': 267.04665422439575, 'accumulated_eval_time': 302.56285071372986, 'accumulated_logging_time': 0.026891469955444336, 'global_step': 974, 'preemption_count': 0}), (1945, {'train/accuracy': 0.9870240092277527, 'train/loss': 0.0484481155872345, 'train/mean_average_precision': 0.0909829737696722, 'validation/accuracy': 0.9843221306800842, 'validation/loss': 0.05845212936401367, 'validation/mean_average_precision': 0.09142817036587084, 'validation/num_examples': 43793, 'test/accuracy': 0.9833080172538757, 'test/loss': 0.061896536499261856, 'test/mean_average_precision': 0.08979671008062669, 'test/num_examples': 43793, 'score': 507.24751901626587, 'total_duration': 889.3428633213043, 'accumulated_submission_time': 507.24751901626587, 'accumulated_eval_time': 382.0338718891144, 'accumulated_logging_time': 0.04410696029663086, 'global_step': 1945, 'preemption_count': 0}), (2925, {'train/accuracy': 0.9876962900161743, 'train/loss': 0.044443048536777496, 'train/mean_average_precision': 0.13600497369939313, 'validation/accuracy': 0.9849521517753601, 'validation/loss': 0.05332230031490326, 'validation/mean_average_precision': 0.13055990526214517, 'validation/num_examples': 43793, 'test/accuracy': 0.9839882254600525, 'test/loss': 0.05628528445959091, 'test/mean_average_precision': 0.1354805088205757, 'test/num_examples': 43793, 'score': 747.3013498783112, 'total_duration': 1209.0284264087677, 'accumulated_submission_time': 747.3013498783112, 'accumulated_eval_time': 461.63943910598755, 'accumulated_logging_time': 0.061554908752441406, 'global_step': 2925, 'preemption_count': 0}), (3905, {'train/accuracy': 0.9880914688110352, 'train/loss': 0.04212948679924011, 'train/mean_average_precision': 0.1726979036151406, 'validation/accuracy': 0.9852420091629028, 'validation/loss': 0.052216753363609314, 'validation/mean_average_precision': 0.15972083335216697, 'validation/num_examples': 43793, 'test/accuracy': 0.9842885732650757, 'test/loss': 0.05533430352807045, 'test/mean_average_precision': 0.15635061477885354, 'test/num_examples': 43793, 'score': 987.359974861145, 'total_duration': 1527.6656908988953, 'accumulated_submission_time': 987.359974861145, 'accumulated_eval_time': 540.1923894882202, 'accumulated_logging_time': 0.07853269577026367, 'global_step': 3905, 'preemption_count': 0}), (4885, {'train/accuracy': 0.9884511828422546, 'train/loss': 0.039487335830926895, 'train/mean_average_precision': 0.221168105531605, 'validation/accuracy': 0.9855582118034363, 'validation/loss': 0.04935424402356148, 'validation/mean_average_precision': 0.18249816776621405, 'validation/num_examples': 43793, 'test/accuracy': 0.9846023321151733, 'test/loss': 0.05213505029678345, 'test/mean_average_precision': 0.18075974949238585, 'test/num_examples': 43793, 'score': 1227.550749540329, 'total_duration': 1847.41366147995, 'accumulated_submission_time': 1227.550749540329, 'accumulated_eval_time': 619.7239730358124, 'accumulated_logging_time': 0.09557342529296875, 'global_step': 4885, 'preemption_count': 0}), (5867, {'train/accuracy': 0.9889355301856995, 'train/loss': 0.03809946030378342, 'train/mean_average_precision': 0.24073140125080847, 'validation/accuracy': 0.9858906865119934, 'validation/loss': 0.04771829769015312, 'validation/mean_average_precision': 0.20280648132348084, 'validation/num_examples': 43793, 'test/accuracy': 0.9850205779075623, 'test/loss': 0.05028391256928444, 'test/mean_average_precision': 0.20410043645349357, 'test/num_examples': 43793, 'score': 1467.5451378822327, 'total_duration': 2168.6761581897736, 'accumulated_submission_time': 1467.5451378822327, 'accumulated_eval_time': 700.9650220870972, 'accumulated_logging_time': 0.11403393745422363, 'global_step': 5867, 'preemption_count': 0}), (6847, {'train/accuracy': 0.9891735911369324, 'train/loss': 0.036761123687028885, 'train/mean_average_precision': 0.2711986768694797, 'validation/accuracy': 0.9860624074935913, 'validation/loss': 0.046917904168367386, 'validation/mean_average_precision': 0.21011351662172223, 'validation/num_examples': 43793, 'test/accuracy': 0.9852067828178406, 'test/loss': 0.04956843703985214, 'test/mean_average_precision': 0.21441966426921308, 'test/num_examples': 43793, 'score': 1707.6073698997498, 'total_duration': 2488.9488427639008, 'accumulated_submission_time': 1707.6073698997498, 'accumulated_eval_time': 781.1497938632965, 'accumulated_logging_time': 0.13106989860534668, 'global_step': 6847, 'preemption_count': 0}), (7853, {'train/accuracy': 0.989593505859375, 'train/loss': 0.03558668494224548, 'train/mean_average_precision': 0.29176598340064, 'validation/accuracy': 0.9861817359924316, 'validation/loss': 0.04656646400690079, 'validation/mean_average_precision': 0.22040746597287028, 'validation/num_examples': 43793, 'test/accuracy': 0.985296905040741, 'test/loss': 0.04927525669336319, 'test/mean_average_precision': 0.22198119898760946, 'test/num_examples': 43793, 'score': 1947.7708423137665, 'total_duration': 2806.5261702537537, 'accumulated_submission_time': 1947.7708423137665, 'accumulated_eval_time': 858.5370628833771, 'accumulated_logging_time': 0.14883756637573242, 'global_step': 7853, 'preemption_count': 0}), (8890, {'train/accuracy': 0.9898359179496765, 'train/loss': 0.034257907420396805, 'train/mean_average_precision': 0.33647031018609763, 'validation/accuracy': 0.9862204790115356, 'validation/loss': 0.04632710665464401, 'validation/mean_average_precision': 0.23348770862249096, 'validation/num_examples': 43793, 'test/accuracy': 0.9853171110153198, 'test/loss': 0.04897666722536087, 'test/mean_average_precision': 0.23489984573031308, 'test/num_examples': 43793, 'score': 2187.9864826202393, 'total_duration': 3124.1885104179382, 'accumulated_submission_time': 2187.9864826202393, 'accumulated_eval_time': 935.9519424438477, 'accumulated_logging_time': 0.17164063453674316, 'global_step': 8890, 'preemption_count': 0}), (9928, {'train/accuracy': 0.9901276230812073, 'train/loss': 0.03325457125902176, 'train/mean_average_precision': 0.34926035983155845, 'validation/accuracy': 0.9863680601119995, 'validation/loss': 0.04596709832549095, 'validation/mean_average_precision': 0.2404337163780253, 'validation/num_examples': 43793, 'test/accuracy': 0.9855133891105652, 'test/loss': 0.04880305007100105, 'test/mean_average_precision': 0.23969024173979178, 'test/num_examples': 43793, 'score': 2427.980811357498, 'total_duration': 3441.3925454616547, 'accumulated_submission_time': 2427.980811357498, 'accumulated_eval_time': 1013.1348078250885, 'accumulated_logging_time': 0.1896076202392578, 'global_step': 9928, 'preemption_count': 0}), (10988, {'train/accuracy': 0.9901374578475952, 'train/loss': 0.0328630693256855, 'train/mean_average_precision': 0.3613298561420277, 'validation/accuracy': 0.9865052700042725, 'validation/loss': 0.04598310589790344, 'validation/mean_average_precision': 0.24557890857677883, 'validation/num_examples': 43793, 'test/accuracy': 0.9856982827186584, 'test/loss': 0.04875835403800011, 'test/mean_average_precision': 0.2467232486206807, 'test/num_examples': 43793, 'score': 2668.187639951706, 'total_duration': 3756.300361394882, 'accumulated_submission_time': 2668.187639951706, 'accumulated_eval_time': 1087.8104150295258, 'accumulated_logging_time': 0.20604181289672852, 'global_step': 10988, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9908209443092346, 'train/loss': 0.03074347786605358, 'train/mean_average_precision': 0.40663552016201665, 'validation/accuracy': 0.9865750670433044, 'validation/loss': 0.04496759548783302, 'validation/mean_average_precision': 0.2557580824516941, 'validation/num_examples': 43793, 'test/accuracy': 0.9857316017150879, 'test/loss': 0.04770534485578537, 'test/mean_average_precision': 0.2536670584972984, 'test/num_examples': 43793, 'score': 2897.361358642578, 'total_duration': 4059.8524990081787, 'accumulated_submission_time': 2897.361358642578, 'accumulated_eval_time': 1162.1632025241852, 'accumulated_logging_time': 0.22313928604125977, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0502 03:33:37.335249 140016461956928 submission_runner.py:581] Timing: 2897.361358642578
I0502 03:33:37.335291 140016461956928 submission_runner.py:582] ====================
I0502 03:33:37.335402 140016461956928 submission_runner.py:645] Final ogbg score: 2897.361358642578
