python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_05-02-2023-13-49-24.log
I0502 13:49:45.093768 139794031630144 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_adafactor/librispeech_conformer_jax.
I0502 13:49:45.168031 139794031630144 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 13:49:45.975125 139794031630144 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0502 13:49:45.975713 139794031630144 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 13:49:45.980893 139794031630144 submission_runner.py:538] Using RNG seed 114294699
I0502 13:49:48.622752 139794031630144 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 13:49:48.622984 139794031630144 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_adafactor/librispeech_conformer_jax/trial_1.
I0502 13:49:48.624646 139794031630144 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_adafactor/librispeech_conformer_jax/trial_1/hparams.json.
I0502 13:49:48.762509 139794031630144 submission_runner.py:241] Initializing dataset.
I0502 13:49:48.762721 139794031630144 submission_runner.py:248] Initializing model.
I0502 13:49:55.138123 139794031630144 submission_runner.py:258] Initializing optimizer.
I0502 13:49:57.482242 139794031630144 submission_runner.py:265] Initializing metrics bundle.
I0502 13:49:57.482447 139794031630144 submission_runner.py:282] Initializing checkpoint and logger.
I0502 13:49:57.483565 139794031630144 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_adafactor/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0502 13:49:57.483850 139794031630144 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0502 13:49:57.483927 139794031630144 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0502 13:49:58.231876 139794031630144 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_adafactor/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0502 13:49:58.232754 139794031630144 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_adafactor/librispeech_conformer_jax/trial_1/flags_0.json.
I0502 13:49:58.239615 139794031630144 submission_runner.py:318] Starting training loop.
I0502 13:49:58.449091 139794031630144 input_pipeline.py:20] Loading split = train-clean-100
I0502 13:49:58.485123 139794031630144 input_pipeline.py:20] Loading split = train-clean-360
I0502 13:49:58.821604 139794031630144 input_pipeline.py:20] Loading split = train-other-500
2023-05-02 13:51:47.117080: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-05-02 13:51:47.588551: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0502 13:51:49.493605 139618699695872 logging_writer.py:48] [0] global_step=0, grad_norm=82.9171142578125, loss=31.65743064880371
I0502 13:51:49.529660 139794031630144 spec.py:298] Evaluating on the training split.
I0502 13:51:49.645379 139794031630144 input_pipeline.py:20] Loading split = train-clean-100
I0502 13:51:49.678687 139794031630144 input_pipeline.py:20] Loading split = train-clean-360
I0502 13:51:49.785627 139794031630144 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0502 13:52:40.532489 139794031630144 spec.py:310] Evaluating on the validation split.
I0502 13:52:40.603195 139794031630144 input_pipeline.py:20] Loading split = dev-clean
I0502 13:52:40.608462 139794031630144 input_pipeline.py:20] Loading split = dev-other
I0502 13:53:23.010002 139794031630144 spec.py:326] Evaluating on the test split.
I0502 13:53:23.080134 139794031630144 input_pipeline.py:20] Loading split = test-clean
I0502 13:53:53.162088 139794031630144 submission_runner.py:415] Time since start: 234.92s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(30.807848, dtype=float32), 'train/wer': 0.9456302188534661, 'validation/ctc_loss': DeviceArray(29.849506, dtype=float32), 'validation/wer': 0.9052861098515181, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.915693, dtype=float32), 'test/wer': 0.908252594804298, 'test/num_examples': 2472, 'score': 111.28984069824219, 'total_duration': 234.92086386680603, 'accumulated_submission_time': 111.28984069824219, 'accumulated_eval_time': 123.63087224960327, 'accumulated_logging_time': 0}
I0502 13:53:53.186448 139615931463424 logging_writer.py:48] [1] accumulated_eval_time=123.630872, accumulated_logging_time=0, accumulated_submission_time=111.289841, global_step=1, preemption_count=0, score=111.289841, test/ctc_loss=29.915693283081055, test/num_examples=2472, test/wer=0.908253, total_duration=234.920864, train/ctc_loss=30.80784797668457, train/wer=0.945630, validation/ctc_loss=29.849506378173828, validation/num_examples=5348, validation/wer=0.905286
I0502 13:56:27.410105 139621042136832 logging_writer.py:48] [100] global_step=100, grad_norm=9.205446243286133, loss=6.404290199279785
I0502 13:57:55.002507 139621050529536 logging_writer.py:48] [200] global_step=200, grad_norm=0.34887591004371643, loss=5.830174922943115
I0502 13:59:22.431463 139621042136832 logging_writer.py:48] [300] global_step=300, grad_norm=0.6384007334709167, loss=5.821770191192627
I0502 14:00:50.934837 139621050529536 logging_writer.py:48] [400] global_step=400, grad_norm=2.128554105758667, loss=5.810830116271973
I0502 14:02:19.737525 139621042136832 logging_writer.py:48] [500] global_step=500, grad_norm=1.313963770866394, loss=5.788608074188232
I0502 14:03:49.239130 139621050529536 logging_writer.py:48] [600] global_step=600, grad_norm=1.6490498781204224, loss=5.7881598472595215
I0502 14:05:18.886573 139621042136832 logging_writer.py:48] [700] global_step=700, grad_norm=2.7854347229003906, loss=5.728577613830566
I0502 14:06:48.442528 139621050529536 logging_writer.py:48] [800] global_step=800, grad_norm=2.475602149963379, loss=5.541701793670654
I0502 14:08:17.659458 139621042136832 logging_writer.py:48] [900] global_step=900, grad_norm=0.636271059513092, loss=5.4468464851379395
I0502 14:09:47.148262 139621050529536 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7826665639877319, loss=5.093632221221924
I0502 14:11:20.699253 139621333726976 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.7806143760681152, loss=4.347283363342285
I0502 14:12:50.550853 139621325334272 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9582788944244385, loss=3.871811866760254
I0502 14:14:19.813388 139621333726976 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0554676055908203, loss=3.573711633682251
I0502 14:15:49.889597 139621325334272 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.0782184600830078, loss=3.329432725906372
I0502 14:17:19.799504 139621333726976 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9855751991271973, loss=3.175281286239624
I0502 14:18:50.053466 139621325334272 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7758563756942749, loss=3.0354228019714355
I0502 14:20:19.981082 139621333726976 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9954627156257629, loss=2.9563305377960205
I0502 14:21:50.319799 139621325334272 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9721936583518982, loss=2.908240556716919
I0502 14:23:20.345479 139621333726976 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9178900122642517, loss=2.7704918384552
I0502 14:24:50.849174 139621325334272 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9322096109390259, loss=2.661681652069092
I0502 14:26:24.731827 139621333726976 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8269645571708679, loss=2.6368660926818848
I0502 14:27:55.258313 139621325334272 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7714064717292786, loss=2.449040412902832
I0502 14:29:25.267392 139621333726976 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7819996476173401, loss=2.4357798099517822
I0502 14:30:55.846692 139621325334272 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.7362711429595947, loss=2.3959200382232666
I0502 14:32:26.343880 139621333726976 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7856718897819519, loss=2.2735369205474854
I0502 14:33:53.675143 139794031630144 spec.py:298] Evaluating on the training split.
I0502 14:34:39.979498 139794031630144 spec.py:310] Evaluating on the validation split.
I0502 14:35:22.014528 139794031630144 spec.py:326] Evaluating on the test split.
I0502 14:35:43.318262 139794031630144 submission_runner.py:415] Time since start: 2745.07s, 	Step: 2597, 	{'train/ctc_loss': DeviceArray(1.6089301, dtype=float32), 'train/wer': 0.4304185289695372, 'validation/ctc_loss': DeviceArray(2.065058, dtype=float32), 'validation/wer': 0.48169302164034383, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.693238, dtype=float32), 'test/wer': 0.4319866756037617, 'test/num_examples': 2472, 'score': 2511.7295060157776, 'total_duration': 2745.0747108459473, 'accumulated_submission_time': 2511.7295060157776, 'accumulated_eval_time': 233.27012181282043, 'accumulated_logging_time': 0.03702521324157715}
I0502 14:35:43.342142 139621333726976 logging_writer.py:48] [2597] accumulated_eval_time=233.270122, accumulated_logging_time=0.037025, accumulated_submission_time=2511.729506, global_step=2597, preemption_count=0, score=2511.729506, test/ctc_loss=1.6932380199432373, test/num_examples=2472, test/wer=0.431987, total_duration=2745.074711, train/ctc_loss=1.6089301109313965, train/wer=0.430419, validation/ctc_loss=2.0650579929351807, validation/num_examples=5348, validation/wer=0.481693
I0502 14:35:46.933285 139621325334272 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7939248085021973, loss=2.302687406539917
I0502 14:37:17.831721 139621333726976 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8349323868751526, loss=2.257720947265625
I0502 14:38:48.577293 139621325334272 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6872414946556091, loss=2.1812222003936768
I0502 14:40:19.211020 139621333726976 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0078966617584229, loss=2.1029582023620605
I0502 14:41:49.649286 139621325334272 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5974404215812683, loss=2.1433589458465576
I0502 14:43:23.484943 139621333726976 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.7644185423851013, loss=2.030341148376465
I0502 14:44:54.656252 139621325334272 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7351266145706177, loss=2.0898189544677734
I0502 14:46:25.175681 139621333726976 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5059599876403809, loss=2.0477724075317383
I0502 14:47:56.564827 139621325334272 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9222326874732971, loss=2.0223238468170166
I0502 14:49:27.972282 139621333726976 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7670664191246033, loss=2.0017950534820557
I0502 14:50:58.745003 139621325334272 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7554590106010437, loss=2.0392799377441406
I0502 14:52:29.574677 139621333726976 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6303724646568298, loss=1.9547983407974243
I0502 14:54:00.722113 139621325334272 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5178220868110657, loss=1.9216067790985107
I0502 14:55:32.346661 139621333726976 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5151201486587524, loss=1.9543185234069824
I0502 14:57:03.509962 139621325334272 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5714766979217529, loss=1.9512290954589844
I0502 14:58:34.417150 139621333726976 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7281535267829895, loss=1.8807775974273682
I0502 15:00:09.402291 139621333726976 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5565873980522156, loss=1.8624625205993652
I0502 15:01:39.886953 139621325334272 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5904663801193237, loss=1.910601019859314
I0502 15:03:11.433491 139621333726976 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8819169402122498, loss=1.8602861166000366
I0502 15:04:43.183197 139621325334272 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5390142202377319, loss=1.866123080253601
I0502 15:06:14.570224 139621333726976 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.616341769695282, loss=1.8425670862197876
I0502 15:07:46.313736 139621325334272 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.595136284828186, loss=1.8320175409317017
I0502 15:09:17.421973 139621333726976 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.545706570148468, loss=1.7725309133529663
I0502 15:10:48.503052 139621325334272 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5727055072784424, loss=1.8486294746398926
I0502 15:12:19.338828 139621333726976 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.44084352254867554, loss=1.8422846794128418
I0502 15:13:50.620860 139621325334272 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.4908004701137543, loss=1.835961937904358
I0502 15:15:25.302926 139621333726976 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.45200586318969727, loss=1.8068881034851074
I0502 15:15:43.675735 139794031630144 spec.py:298] Evaluating on the training split.
I0502 15:16:29.581495 139794031630144 spec.py:310] Evaluating on the validation split.
I0502 15:17:12.948284 139794031630144 spec.py:326] Evaluating on the test split.
I0502 15:17:34.808432 139794031630144 submission_runner.py:415] Time since start: 5256.57s, 	Step: 5221, 	{'train/ctc_loss': DeviceArray(0.55735064, dtype=float32), 'train/wer': 0.19551447673334577, 'validation/ctc_loss': DeviceArray(0.8859576, dtype=float32), 'validation/wer': 0.2606875126629297, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5994949, dtype=float32), 'test/wer': 0.20106432677269312, 'test/num_examples': 2472, 'score': 4912.0085163116455, 'total_duration': 5256.565467834473, 'accumulated_submission_time': 4912.0085163116455, 'accumulated_eval_time': 344.39952969551086, 'accumulated_logging_time': 0.07772970199584961}
I0502 15:17:34.830708 139621333726976 logging_writer.py:48] [5221] accumulated_eval_time=344.399530, accumulated_logging_time=0.077730, accumulated_submission_time=4912.008516, global_step=5221, preemption_count=0, score=4912.008516, test/ctc_loss=0.5994948744773865, test/num_examples=2472, test/wer=0.201064, total_duration=5256.565468, train/ctc_loss=0.5573506355285645, train/wer=0.195514, validation/ctc_loss=0.8859575986862183, validation/num_examples=5348, validation/wer=0.260688
I0502 15:18:47.438922 139621325334272 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.5202800631523132, loss=1.7473418712615967
I0502 15:20:18.313027 139621333726976 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.4734688699245453, loss=1.7971336841583252
I0502 15:21:49.935058 139621325334272 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.46765419840812683, loss=1.8069067001342773
I0502 15:23:20.927656 139621333726976 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5066799521446228, loss=1.7948174476623535
I0502 15:24:51.684154 139621325334272 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5656203627586365, loss=1.7026652097702026
I0502 15:26:23.119033 139621333726976 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6304826140403748, loss=1.771973967552185
I0502 15:27:54.700545 139621325334272 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4217028021812439, loss=1.7434710264205933
I0502 15:29:25.743544 139621333726976 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5178630948066711, loss=1.7518187761306763
I0502 15:30:57.005455 139621325334272 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.37932920455932617, loss=1.7059632539749146
I0502 15:32:32.028389 139621333726976 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.4262526035308838, loss=1.7176095247268677
I0502 15:34:02.990931 139621325334272 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6085333824157715, loss=1.689703345298767
I0502 15:35:33.455170 139621333726976 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.46300071477890015, loss=1.7177423238754272
I0502 15:37:04.506420 139621325334272 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.4165099263191223, loss=1.70528244972229
I0502 15:38:35.579444 139621333726976 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.692101240158081, loss=1.7525485754013062
I0502 15:40:06.785067 139621325334272 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.3981296718120575, loss=1.6596187353134155
I0502 15:41:38.079353 139621333726976 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.44453465938568115, loss=1.7298256158828735
I0502 15:43:09.137579 139621325334272 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6149975657463074, loss=1.7206324338912964
I0502 15:44:40.101222 139621333726976 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4881327748298645, loss=1.7100993394851685
I0502 15:46:11.535465 139621325334272 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5122680068016052, loss=1.7076283693313599
I0502 15:47:42.375236 139621333726976 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.38130292296409607, loss=1.6716351509094238
I0502 15:49:17.595043 139621333726976 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.46134239435195923, loss=1.6001042127609253
I0502 15:50:48.506393 139621325334272 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.37452083826065063, loss=1.625300645828247
I0502 15:52:19.563108 139621333726976 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.36739611625671387, loss=1.5839216709136963
I0502 15:53:50.868448 139621325334272 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.44899141788482666, loss=1.6439579725265503
I0502 15:55:22.320192 139621333726976 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.40272462368011475, loss=1.602708101272583
I0502 15:56:53.831091 139621325334272 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.3567346930503845, loss=1.5910887718200684
I0502 15:57:35.725731 139794031630144 spec.py:298] Evaluating on the training split.
I0502 15:58:22.397511 139794031630144 spec.py:310] Evaluating on the validation split.
I0502 15:59:05.289425 139794031630144 spec.py:326] Evaluating on the test split.
I0502 15:59:27.088907 139794031630144 submission_runner.py:415] Time since start: 7768.85s, 	Step: 7847, 	{'train/ctc_loss': DeviceArray(0.39454228, dtype=float32), 'train/wer': 0.1461486155257207, 'validation/ctc_loss': DeviceArray(0.74232364, dtype=float32), 'validation/wer': 0.22283861879998842, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4764659, dtype=float32), 'test/wer': 0.16100989173928057, 'test/num_examples': 2472, 'score': 7312.852331638336, 'total_duration': 7768.846139669418, 'accumulated_submission_time': 7312.852331638336, 'accumulated_eval_time': 455.7596242427826, 'accumulated_logging_time': 0.11454486846923828}
I0502 15:59:27.112066 139621333726976 logging_writer.py:48] [7847] accumulated_eval_time=455.759624, accumulated_logging_time=0.114545, accumulated_submission_time=7312.852332, global_step=7847, preemption_count=0, score=7312.852332, test/ctc_loss=0.4764659106731415, test/num_examples=2472, test/wer=0.161010, total_duration=7768.846140, train/ctc_loss=0.39454227685928345, train/wer=0.146149, validation/ctc_loss=0.742323637008667, validation/num_examples=5348, validation/wer=0.222839
I0502 16:00:16.294671 139621325334272 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4679543673992157, loss=1.6600534915924072
I0502 16:01:47.734504 139621333726976 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4632120728492737, loss=1.6285871267318726
I0502 16:03:18.890717 139621325334272 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5021744966506958, loss=1.6106946468353271
I0502 16:04:50.314213 139621333726976 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.3419612944126129, loss=1.5779558420181274
I0502 16:06:25.169316 139621333726976 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5358561277389526, loss=1.5114136934280396
I0502 16:07:56.311439 139621325334272 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.4412595331668854, loss=1.6093976497650146
I0502 16:09:27.066593 139621333726976 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.35234537720680237, loss=1.5743380784988403
I0502 16:10:59.002573 139621325334272 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.37919899821281433, loss=1.5556268692016602
I0502 16:12:30.265695 139621333726976 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3412594497203827, loss=1.5555649995803833
I0502 16:14:01.722187 139621325334272 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.39640071988105774, loss=1.5237199068069458
I0502 16:15:33.079096 139621333726976 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.398266077041626, loss=1.6290996074676514
I0502 16:17:04.440531 139621325334272 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.37156131863594055, loss=1.5633715391159058
I0502 16:18:35.575091 139621333726976 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.4013022184371948, loss=1.5144741535186768
I0502 16:20:07.017096 139621325334272 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5193273425102234, loss=1.5362207889556885
I0502 16:21:41.810927 139621333726976 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.4216099679470062, loss=1.5296852588653564
I0502 16:23:13.479621 139621325334272 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.31577223539352417, loss=1.515515685081482
I0502 16:24:44.662906 139621333726976 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.390085369348526, loss=1.561246395111084
I0502 16:26:16.580445 139621325334272 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.33414629101753235, loss=1.500864863395691
I0502 16:27:48.405685 139621333726976 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.36621198058128357, loss=1.51362943649292
I0502 16:29:19.958012 139621325334272 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.38788971304893494, loss=1.5649875402450562
I0502 16:30:51.198662 139621333726976 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.35920000076293945, loss=1.520039677619934
I0502 16:32:22.955281 139621325334272 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3757934272289276, loss=1.4763847589492798
I0502 16:33:54.389436 139621333726976 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.3431865870952606, loss=1.5356875658035278
I0502 16:35:25.941395 139621325334272 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.394302636384964, loss=1.491546392440796
I0502 16:37:00.417058 139621333726976 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.3291624188423157, loss=1.410122275352478
I0502 16:38:32.777673 139621325334272 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.3279407322406769, loss=1.444808006286621
I0502 16:39:27.862088 139794031630144 spec.py:298] Evaluating on the training split.
I0502 16:40:14.778486 139794031630144 spec.py:310] Evaluating on the validation split.
I0502 16:40:58.213790 139794031630144 spec.py:326] Evaluating on the test split.
I0502 16:41:20.117385 139794031630144 submission_runner.py:415] Time since start: 10281.87s, 	Step: 10461, 	{'train/ctc_loss': DeviceArray(0.3153075, dtype=float32), 'train/wer': 0.11664170742179727, 'validation/ctc_loss': DeviceArray(0.62143594, dtype=float32), 'validation/wer': 0.18827967467124623, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38501707, dtype=float32), 'test/wer': 0.1301362906993277, 'test/num_examples': 2472, 'score': 9713.548959493637, 'total_duration': 10281.873970508575, 'accumulated_submission_time': 9713.548959493637, 'accumulated_eval_time': 568.0112118721008, 'accumulated_logging_time': 0.1530444622039795}
I0502 16:41:20.141767 139621333726976 logging_writer.py:48] [10461] accumulated_eval_time=568.011212, accumulated_logging_time=0.153044, accumulated_submission_time=9713.548959, global_step=10461, preemption_count=0, score=9713.548959, test/ctc_loss=0.385017067193985, test/num_examples=2472, test/wer=0.130136, total_duration=10281.873971, train/ctc_loss=0.31530749797821045, train/wer=0.116642, validation/ctc_loss=0.6214359402656555, validation/num_examples=5348, validation/wer=0.188280
I0502 16:41:56.789297 139621325334272 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.35438311100006104, loss=1.4842655658721924
I0502 16:43:28.719174 139621333726976 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.3738310635089874, loss=1.4570940732955933
I0502 16:45:00.542023 139621325334272 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.4176693856716156, loss=1.4608036279678345
I0502 16:46:32.078790 139621333726976 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.3888411819934845, loss=1.519924521446228
I0502 16:48:03.864476 139621325334272 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.4083775579929352, loss=1.4931005239486694
I0502 16:49:35.640776 139621333726976 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.3476407527923584, loss=1.4820984601974487
I0502 16:51:06.937658 139621325334272 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3235401213169098, loss=1.443880558013916
I0502 16:52:38.479698 139621333726976 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.4128008484840393, loss=1.4553662538528442
I0502 16:54:09.567487 139621325334272 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.33942267298698425, loss=1.4729831218719482
I0502 16:55:45.111362 139621333726976 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.33398574590682983, loss=1.4099068641662598
I0502 16:57:15.837373 139621325334272 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.40404006838798523, loss=1.4905164241790771
I0502 16:58:47.041702 139621333726976 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.3709630072116852, loss=1.4228092432022095
I0502 17:00:18.800444 139621325334272 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.37348052859306335, loss=1.4243814945220947
I0502 17:01:50.577104 139621333726976 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4089584946632385, loss=1.4569989442825317
I0502 17:03:22.187230 139621325334272 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.423906534910202, loss=1.4530386924743652
I0502 17:04:53.786085 139621333726976 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.38339051604270935, loss=1.3860399723052979
I0502 17:06:24.963047 139621325334272 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.32529547810554504, loss=1.4336774349212646
I0502 17:07:56.447382 139621333726976 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.3664909601211548, loss=1.4982668161392212
I0502 17:09:27.881876 139621325334272 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2826988399028778, loss=1.454622745513916
I0502 17:11:02.732773 139621333726976 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.4576115310192108, loss=1.4313992261886597
I0502 17:12:34.045568 139621325334272 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3374948501586914, loss=1.4324613809585571
I0502 17:14:05.631153 139621333726976 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.33260229229927063, loss=1.406068205833435
I0502 17:15:37.347928 139621325334272 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.37860873341560364, loss=1.3709741830825806
I0502 17:17:09.312626 139621333726976 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.3318272829055786, loss=1.4448271989822388
I0502 17:18:41.200058 139621325334272 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.3442506492137909, loss=1.4276422262191772
I0502 17:20:12.785280 139621333726976 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6567429900169373, loss=1.3964154720306396
I0502 17:21:20.596331 139794031630144 spec.py:298] Evaluating on the training split.
I0502 17:22:06.481035 139794031630144 spec.py:310] Evaluating on the validation split.
I0502 17:22:49.273944 139794031630144 spec.py:326] Evaluating on the test split.
I0502 17:23:11.037378 139794031630144 submission_runner.py:415] Time since start: 12792.79s, 	Step: 13075, 	{'train/ctc_loss': DeviceArray(0.26769337, dtype=float32), 'train/wer': 0.10257344147513536, 'validation/ctc_loss': DeviceArray(0.559525, dtype=float32), 'validation/wer': 0.1707590039460101, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.33999276, dtype=float32), 'test/wer': 0.11642597444803282, 'test/num_examples': 2472, 'score': 12113.949732542038, 'total_duration': 12792.794283390045, 'accumulated_submission_time': 12113.949732542038, 'accumulated_eval_time': 678.448846578598, 'accumulated_logging_time': 0.19350099563598633}
I0502 17:23:11.064464 139621333726976 logging_writer.py:48] [13075] accumulated_eval_time=678.448847, accumulated_logging_time=0.193501, accumulated_submission_time=12113.949733, global_step=13075, preemption_count=0, score=12113.949733, test/ctc_loss=0.3399927616119385, test/num_examples=2472, test/wer=0.116426, total_duration=12792.794283, train/ctc_loss=0.2676933705806732, train/wer=0.102573, validation/ctc_loss=0.5595250129699707, validation/num_examples=5348, validation/wer=0.170759
I0502 17:23:34.937359 139621325334272 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.57546067237854, loss=1.4535555839538574
I0502 17:25:07.140319 139621333726976 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.3282482326030731, loss=1.4378844499588013
I0502 17:26:38.803911 139621325334272 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.3960074782371521, loss=1.440421462059021
I0502 17:28:13.848293 139621333726976 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.3897523880004883, loss=1.3961632251739502
I0502 17:29:45.681560 139621325334272 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5742567777633667, loss=1.3623794317245483
I0502 17:31:16.603494 139621333726976 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.35020601749420166, loss=1.3870959281921387
I0502 17:32:47.836879 139621325334272 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.36803707480430603, loss=1.3382402658462524
I0502 17:34:20.075845 139621333726976 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.34316501021385193, loss=1.3469871282577515
I0502 17:35:52.187013 139621325334272 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.39149361848831177, loss=1.3589589595794678
I0502 17:37:23.783450 139621333726976 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.44429031014442444, loss=1.3520230054855347
I0502 17:38:55.578470 139621325334272 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.31972169876098633, loss=1.4072521924972534
I0502 17:40:27.344044 139621333726976 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.31152787804603577, loss=1.4205716848373413
I0502 17:41:59.541318 139621325334272 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.5209969878196716, loss=1.3300727605819702
I0502 17:43:31.385094 139621333726976 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.3614359200000763, loss=1.345199704170227
I0502 17:45:07.531503 139621333726976 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.29799404740333557, loss=1.364986777305603
I0502 17:46:38.311863 139621325334272 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.49453601241111755, loss=1.3733795881271362
I0502 17:48:09.581137 139621333726976 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.33037978410720825, loss=1.3554456233978271
I0502 17:49:41.192030 139621325334272 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.4262066185474396, loss=1.3936210870742798
I0502 17:51:12.940006 139621333726976 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3524727523326874, loss=1.414936900138855
I0502 17:52:44.548248 139621325334272 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3187800645828247, loss=1.3559051752090454
I0502 17:54:16.587859 139621333726976 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.34158456325531006, loss=1.3588794469833374
I0502 17:55:48.199199 139621325334272 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.2728019058704376, loss=1.364212989807129
I0502 17:57:20.084395 139621333726976 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.409143328666687, loss=1.3201467990875244
I0502 17:58:51.907962 139621325334272 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.3414570093154907, loss=1.4227741956710815
I0502 18:00:27.323654 139621333726976 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.3337934911251068, loss=1.3455239534378052
I0502 18:01:58.742365 139621325334272 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.3555411100387573, loss=1.3724240064620972
I0502 18:03:11.987055 139794031630144 spec.py:298] Evaluating on the training split.
I0502 18:03:58.357926 139794031630144 spec.py:310] Evaluating on the validation split.
I0502 18:04:41.023036 139794031630144 spec.py:326] Evaluating on the test split.
I0502 18:05:02.973180 139794031630144 submission_runner.py:415] Time since start: 15304.73s, 	Step: 15681, 	{'train/ctc_loss': DeviceArray(0.2610823, dtype=float32), 'train/wer': 0.09628116147686595, 'validation/ctc_loss': DeviceArray(0.5289547, dtype=float32), 'validation/wer': 0.16140049590444674, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.31872362, dtype=float32), 'test/wer': 0.10822009627688746, 'test/num_examples': 2472, 'score': 14514.81673502922, 'total_duration': 15304.73035645485, 'accumulated_submission_time': 14514.81673502922, 'accumulated_eval_time': 789.4318437576294, 'accumulated_logging_time': 0.23786544799804688}
I0502 18:05:02.999383 139621333726976 logging_writer.py:48] [15681] accumulated_eval_time=789.431844, accumulated_logging_time=0.237865, accumulated_submission_time=14514.816735, global_step=15681, preemption_count=0, score=14514.816735, test/ctc_loss=0.3187236189842224, test/num_examples=2472, test/wer=0.108220, total_duration=15304.730356, train/ctc_loss=0.2610822916030884, train/wer=0.096281, validation/ctc_loss=0.5289546847343445, validation/num_examples=5348, validation/wer=0.161400
I0502 18:05:21.328943 139621325334272 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.32233646512031555, loss=1.2992981672286987
I0502 18:06:53.341823 139621333726976 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.3807051479816437, loss=1.3580995798110962
I0502 18:08:25.304763 139621325334272 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.3330169916152954, loss=1.3287997245788574
I0502 18:09:57.143095 139621333726976 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.36447808146476746, loss=1.3719089031219482
I0502 18:11:28.914788 139621325334272 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.3056102693080902, loss=1.3611234426498413
I0502 18:13:00.943852 139621333726976 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.338559091091156, loss=1.3259750604629517
I0502 18:14:32.970447 139621325334272 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.4690324068069458, loss=1.362336277961731
I0502 18:16:04.415483 139621333726976 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.3718682825565338, loss=1.2882890701293945
I0502 18:17:39.432338 139621333726976 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.3189670443534851, loss=1.376769781112671
I0502 18:19:11.023662 139621325334272 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.34094104170799255, loss=1.3169699907302856
I0502 18:20:42.173278 139621333726976 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.4263635575771332, loss=1.3346630334854126
I0502 18:22:13.815348 139621325334272 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.29517605900764465, loss=1.3301773071289062
I0502 18:23:46.080152 139621333726976 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3264450430870056, loss=1.3386634588241577
I0502 18:25:17.857214 139621325334272 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.29068174958229065, loss=1.3883030414581299
I0502 18:26:49.797815 139621333726976 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.33505430817604065, loss=1.3196498155593872
I0502 18:28:21.334896 139621325334272 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3431202173233032, loss=1.3322120904922485
I0502 18:29:53.264913 139621333726976 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.3281903862953186, loss=1.3571851253509521
I0502 18:31:24.683382 139621325334272 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.36022382974624634, loss=1.3002963066101074
I0502 18:32:56.252201 139621333726976 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3626521825790405, loss=1.3588252067565918
I0502 18:34:31.930175 139621333726976 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.43639427423477173, loss=1.3519352674484253
I0502 18:36:03.336888 139621325334272 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.29602739214897156, loss=1.2880867719650269
I0502 18:37:34.759653 139621333726976 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.3849700391292572, loss=1.309382438659668
I0502 18:39:06.624502 139621325334272 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3715716600418091, loss=1.3401079177856445
I0502 18:40:38.178338 139621333726976 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3415394425392151, loss=1.2767343521118164
I0502 18:42:10.268068 139621325334272 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.4308767318725586, loss=1.335855484008789
I0502 18:43:41.931568 139621333726976 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.31376397609710693, loss=1.276349425315857
I0502 18:45:03.609877 139794031630144 spec.py:298] Evaluating on the training split.
I0502 18:45:50.415728 139794031630144 spec.py:310] Evaluating on the validation split.
I0502 18:46:33.304254 139794031630144 spec.py:326] Evaluating on the test split.
I0502 18:46:55.168025 139794031630144 submission_runner.py:415] Time since start: 17816.93s, 	Step: 18290, 	{'train/ctc_loss': DeviceArray(0.21924773, dtype=float32), 'train/wer': 0.08407249058770978, 'validation/ctc_loss': DeviceArray(0.5128597, dtype=float32), 'validation/wer': 0.15524510607917105, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.30312216, dtype=float32), 'test/wer': 0.1032437592671582, 'test/num_examples': 2472, 'score': 16915.29617500305, 'total_duration': 17816.92512845993, 'accumulated_submission_time': 16915.29617500305, 'accumulated_eval_time': 900.9867856502533, 'accumulated_logging_time': 0.3573765754699707}
I0502 18:46:55.191262 139621333726976 logging_writer.py:48] [18290] accumulated_eval_time=900.986786, accumulated_logging_time=0.357377, accumulated_submission_time=16915.296175, global_step=18290, preemption_count=0, score=16915.296175, test/ctc_loss=0.3031221628189087, test/num_examples=2472, test/wer=0.103244, total_duration=17816.925128, train/ctc_loss=0.2192477285861969, train/wer=0.084072, validation/ctc_loss=0.5128597021102905, validation/num_examples=5348, validation/wer=0.155245
I0502 18:47:05.194797 139621325334272 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.3401341736316681, loss=1.312970757484436
I0502 18:48:36.849137 139621333726976 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.4502832591533661, loss=1.358140468597412
I0502 18:50:08.602355 139621325334272 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3702535629272461, loss=1.3182008266448975
I0502 18:51:43.847319 139621333726976 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.27659016847610474, loss=1.3240723609924316
I0502 18:53:15.243930 139621325334272 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.4230518937110901, loss=1.3453240394592285
I0502 18:54:46.827583 139621333726976 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3313307762145996, loss=1.2169357538223267
I0502 18:56:19.133515 139621325334272 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.36186811327934265, loss=1.312784194946289
I0502 18:57:50.990372 139621333726976 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.4222424626350403, loss=1.276772379875183
I0502 18:59:22.447686 139621325334272 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.34870418906211853, loss=1.271622896194458
I0502 19:00:54.100928 139621333726976 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.2827478051185608, loss=1.312786340713501
I0502 19:02:25.507243 139621325334272 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3224308490753174, loss=1.351157307624817
I0502 19:03:57.150555 139621333726976 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.4245082437992096, loss=1.2926437854766846
I0502 19:05:29.135568 139621325334272 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.3780075013637543, loss=1.3035058975219727
I0502 19:07:04.455404 139621333726976 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3009983003139496, loss=1.2392299175262451
I0502 19:08:36.000722 139621325334272 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.31208378076553345, loss=1.260179877281189
I0502 19:10:07.075031 139621333726976 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3072822391986847, loss=1.2518858909606934
I0502 19:11:38.555433 139621325334272 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.378695547580719, loss=1.3302375078201294
I0502 19:13:09.324268 139794031630144 spec.py:298] Evaluating on the training split.
I0502 19:13:56.769471 139794031630144 spec.py:310] Evaluating on the validation split.
I0502 19:14:40.191567 139794031630144 spec.py:326] Evaluating on the test split.
I0502 19:15:02.161597 139794031630144 submission_runner.py:415] Time since start: 19503.92s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(0.1869362, dtype=float32), 'train/wer': 0.07404248680412907, 'validation/ctc_loss': DeviceArray(0.49759445, dtype=float32), 'validation/wer': 0.14854943125355768, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.28804332, dtype=float32), 'test/wer': 0.09863303069079682, 'test/num_examples': 2472, 'score': 18489.389750242233, 'total_duration': 19503.91836452484, 'accumulated_submission_time': 18489.389750242233, 'accumulated_eval_time': 1013.8205723762512, 'accumulated_logging_time': 0.3958742618560791}
I0502 19:15:02.185969 139621333726976 logging_writer.py:48] [20000] accumulated_eval_time=1013.820572, accumulated_logging_time=0.395874, accumulated_submission_time=18489.389750, global_step=20000, preemption_count=0, score=18489.389750, test/ctc_loss=0.2880433201789856, test/num_examples=2472, test/wer=0.098633, total_duration=19503.918365, train/ctc_loss=0.18693619966506958, train/wer=0.074042, validation/ctc_loss=0.4975944459438324, validation/num_examples=5348, validation/wer=0.148549
I0502 19:15:02.210922 139621325334272 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=18489.389750
I0502 19:15:02.423995 139794031630144 checkpoints.py:356] Saving checkpoint at step: 20000
I0502 19:15:03.072761 139794031630144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_adafactor/librispeech_conformer_jax/trial_1/checkpoint_20000
I0502 19:15:03.086237 139794031630144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_adafactor/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0502 19:15:04.369122 139794031630144 submission_runner.py:578] Tuning trial 1/1
I0502 19:15:04.369366 139794031630144 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0502 19:15:04.380943 139794031630144 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(30.807848, dtype=float32), 'train/wer': 0.9456302188534661, 'validation/ctc_loss': DeviceArray(29.849506, dtype=float32), 'validation/wer': 0.9052861098515181, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.915693, dtype=float32), 'test/wer': 0.908252594804298, 'test/num_examples': 2472, 'score': 111.28984069824219, 'total_duration': 234.92086386680603, 'accumulated_submission_time': 111.28984069824219, 'accumulated_eval_time': 123.63087224960327, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2597, {'train/ctc_loss': DeviceArray(1.6089301, dtype=float32), 'train/wer': 0.4304185289695372, 'validation/ctc_loss': DeviceArray(2.065058, dtype=float32), 'validation/wer': 0.48169302164034383, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.693238, dtype=float32), 'test/wer': 0.4319866756037617, 'test/num_examples': 2472, 'score': 2511.7295060157776, 'total_duration': 2745.0747108459473, 'accumulated_submission_time': 2511.7295060157776, 'accumulated_eval_time': 233.27012181282043, 'accumulated_logging_time': 0.03702521324157715, 'global_step': 2597, 'preemption_count': 0}), (5221, {'train/ctc_loss': DeviceArray(0.55735064, dtype=float32), 'train/wer': 0.19551447673334577, 'validation/ctc_loss': DeviceArray(0.8859576, dtype=float32), 'validation/wer': 0.2606875126629297, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5994949, dtype=float32), 'test/wer': 0.20106432677269312, 'test/num_examples': 2472, 'score': 4912.0085163116455, 'total_duration': 5256.565467834473, 'accumulated_submission_time': 4912.0085163116455, 'accumulated_eval_time': 344.39952969551086, 'accumulated_logging_time': 0.07772970199584961, 'global_step': 5221, 'preemption_count': 0}), (7847, {'train/ctc_loss': DeviceArray(0.39454228, dtype=float32), 'train/wer': 0.1461486155257207, 'validation/ctc_loss': DeviceArray(0.74232364, dtype=float32), 'validation/wer': 0.22283861879998842, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4764659, dtype=float32), 'test/wer': 0.16100989173928057, 'test/num_examples': 2472, 'score': 7312.852331638336, 'total_duration': 7768.846139669418, 'accumulated_submission_time': 7312.852331638336, 'accumulated_eval_time': 455.7596242427826, 'accumulated_logging_time': 0.11454486846923828, 'global_step': 7847, 'preemption_count': 0}), (10461, {'train/ctc_loss': DeviceArray(0.3153075, dtype=float32), 'train/wer': 0.11664170742179727, 'validation/ctc_loss': DeviceArray(0.62143594, dtype=float32), 'validation/wer': 0.18827967467124623, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38501707, dtype=float32), 'test/wer': 0.1301362906993277, 'test/num_examples': 2472, 'score': 9713.548959493637, 'total_duration': 10281.873970508575, 'accumulated_submission_time': 9713.548959493637, 'accumulated_eval_time': 568.0112118721008, 'accumulated_logging_time': 0.1530444622039795, 'global_step': 10461, 'preemption_count': 0}), (13075, {'train/ctc_loss': DeviceArray(0.26769337, dtype=float32), 'train/wer': 0.10257344147513536, 'validation/ctc_loss': DeviceArray(0.559525, dtype=float32), 'validation/wer': 0.1707590039460101, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.33999276, dtype=float32), 'test/wer': 0.11642597444803282, 'test/num_examples': 2472, 'score': 12113.949732542038, 'total_duration': 12792.794283390045, 'accumulated_submission_time': 12113.949732542038, 'accumulated_eval_time': 678.448846578598, 'accumulated_logging_time': 0.19350099563598633, 'global_step': 13075, 'preemption_count': 0}), (15681, {'train/ctc_loss': DeviceArray(0.2610823, dtype=float32), 'train/wer': 0.09628116147686595, 'validation/ctc_loss': DeviceArray(0.5289547, dtype=float32), 'validation/wer': 0.16140049590444674, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.31872362, dtype=float32), 'test/wer': 0.10822009627688746, 'test/num_examples': 2472, 'score': 14514.81673502922, 'total_duration': 15304.73035645485, 'accumulated_submission_time': 14514.81673502922, 'accumulated_eval_time': 789.4318437576294, 'accumulated_logging_time': 0.23786544799804688, 'global_step': 15681, 'preemption_count': 0}), (18290, {'train/ctc_loss': DeviceArray(0.21924773, dtype=float32), 'train/wer': 0.08407249058770978, 'validation/ctc_loss': DeviceArray(0.5128597, dtype=float32), 'validation/wer': 0.15524510607917105, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.30312216, dtype=float32), 'test/wer': 0.1032437592671582, 'test/num_examples': 2472, 'score': 16915.29617500305, 'total_duration': 17816.92512845993, 'accumulated_submission_time': 16915.29617500305, 'accumulated_eval_time': 900.9867856502533, 'accumulated_logging_time': 0.3573765754699707, 'global_step': 18290, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(0.1869362, dtype=float32), 'train/wer': 0.07404248680412907, 'validation/ctc_loss': DeviceArray(0.49759445, dtype=float32), 'validation/wer': 0.14854943125355768, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.28804332, dtype=float32), 'test/wer': 0.09863303069079682, 'test/num_examples': 2472, 'score': 18489.389750242233, 'total_duration': 19503.91836452484, 'accumulated_submission_time': 18489.389750242233, 'accumulated_eval_time': 1013.8205723762512, 'accumulated_logging_time': 0.3958742618560791, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0502 19:15:04.381160 139794031630144 submission_runner.py:581] Timing: 18489.389750242233
I0502 19:15:04.381226 139794031630144 submission_runner.py:582] ====================
I0502 19:15:04.382050 139794031630144 submission_runner.py:645] Final librispeech_conformer score: 18489.389750242233
