python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_sam --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-02-2023-15-31-08.log
I0502 15:31:28.520254 139871489369920 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_sam/librispeech_deepspeech_jax.
I0502 15:31:28.606425 139871489369920 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 15:31:29.453439 139871489369920 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0502 15:31:29.454099 139871489369920 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 15:31:29.459790 139871489369920 submission_runner.py:538] Using RNG seed 2180677458
I0502 15:31:32.148117 139871489369920 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 15:31:32.148344 139871489369920 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_sam/librispeech_deepspeech_jax/trial_1.
I0502 15:31:32.148571 139871489369920 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_sam/librispeech_deepspeech_jax/trial_1/hparams.json.
I0502 15:31:32.277213 139871489369920 submission_runner.py:241] Initializing dataset.
I0502 15:31:32.277426 139871489369920 submission_runner.py:248] Initializing model.
I0502 15:31:49.300855 139871489369920 submission_runner.py:258] Initializing optimizer.
I0502 15:31:49.958023 139871489369920 submission_runner.py:265] Initializing metrics bundle.
I0502 15:31:49.958229 139871489369920 submission_runner.py:282] Initializing checkpoint and logger.
I0502 15:31:49.959069 139871489369920 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_sam/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0502 15:31:49.959362 139871489369920 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0502 15:31:49.959434 139871489369920 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0502 15:31:50.904020 139871489369920 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_sam/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0502 15:31:50.905097 139871489369920 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_sam/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0502 15:31:50.912554 139871489369920 submission_runner.py:318] Starting training loop.
I0502 15:31:51.111626 139871489369920 input_pipeline.py:20] Loading split = train-clean-100
I0502 15:31:51.148127 139871489369920 input_pipeline.py:20] Loading split = train-clean-360
I0502 15:31:51.467322 139871489369920 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0502 15:33:06.171306 139694104897280 logging_writer.py:48] [0] global_step=0, grad_norm=21.796184539794922, loss=33.894187927246094
I0502 15:33:06.197369 139871489369920 spec.py:298] Evaluating on the training split.
I0502 15:33:06.334045 139871489369920 input_pipeline.py:20] Loading split = train-clean-100
I0502 15:33:06.534510 139871489369920 input_pipeline.py:20] Loading split = train-clean-360
I0502 15:33:06.819329 139871489369920 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0502 15:34:42.232908 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 15:34:42.335142 139871489369920 input_pipeline.py:20] Loading split = dev-clean
I0502 15:34:42.340229 139871489369920 input_pipeline.py:20] Loading split = dev-other
I0502 15:35:38.298254 139871489369920 spec.py:326] Evaluating on the test split.
I0502 15:35:38.397648 139871489369920 input_pipeline.py:20] Loading split = test-clean
I0502 15:36:14.912558 139871489369920 submission_runner.py:415] Time since start: 264.00s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.870005, dtype=float32), 'train/wer': 4.768595256439623, 'validation/ctc_loss': DeviceArray(30.76967, dtype=float32), 'validation/wer': 4.396781445069417, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.750902, dtype=float32), 'test/wer': 4.654581276785896, 'test/num_examples': 2472, 'score': 75.28461813926697, 'total_duration': 263.99857091903687, 'accumulated_submission_time': 75.28461813926697, 'accumulated_eval_time': 188.71378564834595, 'accumulated_logging_time': 0}
I0502 15:36:14.936135 139691286324992 logging_writer.py:48] [1] accumulated_eval_time=188.713786, accumulated_logging_time=0, accumulated_submission_time=75.284618, global_step=1, preemption_count=0, score=75.284618, test/ctc_loss=30.75090217590332, test/num_examples=2472, test/wer=4.654581, total_duration=263.998571, train/ctc_loss=31.870004653930664, train/wer=4.768595, validation/ctc_loss=30.769670486450195, validation/num_examples=5348, validation/wer=4.396781
I0502 15:40:31.018324 139695227995904 logging_writer.py:48] [100] global_step=100, grad_norm=7.584001064300537, loss=9.940764427185059
I0502 15:44:14.091479 139695236388608 logging_writer.py:48] [200] global_step=200, grad_norm=1.5801061391830444, loss=6.277695655822754
I0502 15:47:56.660245 139695227995904 logging_writer.py:48] [300] global_step=300, grad_norm=0.7742932438850403, loss=5.933292865753174
I0502 15:51:39.385914 139695236388608 logging_writer.py:48] [400] global_step=400, grad_norm=0.6561444401741028, loss=5.832819938659668
I0502 15:55:22.847292 139695227995904 logging_writer.py:48] [500] global_step=500, grad_norm=0.4785062372684479, loss=5.820417404174805
I0502 15:59:06.066896 139695236388608 logging_writer.py:48] [600] global_step=600, grad_norm=0.7547652125358582, loss=5.776474952697754
I0502 16:02:50.202630 139695227995904 logging_writer.py:48] [700] global_step=700, grad_norm=0.5594998002052307, loss=5.713106632232666
I0502 16:06:33.646872 139695236388608 logging_writer.py:48] [800] global_step=800, grad_norm=0.6936714053153992, loss=5.633432388305664
I0502 16:10:17.127633 139695227995904 logging_writer.py:48] [900] global_step=900, grad_norm=0.7097064256668091, loss=5.424973011016846
I0502 16:14:01.187007 139695236388608 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8253594636917114, loss=5.109757423400879
I0502 16:16:16.229597 139871489369920 spec.py:298] Evaluating on the training split.
I0502 16:16:44.581778 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 16:17:20.353515 139871489369920 spec.py:326] Evaluating on the test split.
I0502 16:17:38.687544 139871489369920 submission_runner.py:415] Time since start: 2747.77s, 	Step: 1060, 	{'train/ctc_loss': DeviceArray(6.71802, dtype=float32), 'train/wer': 0.9446303886634673, 'validation/ctc_loss': DeviceArray(6.7271876, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.6270437, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2476.5528469085693, 'total_duration': 2747.773008584976, 'accumulated_submission_time': 2476.5528469085693, 'accumulated_eval_time': 271.1700689792633, 'accumulated_logging_time': 0.03457331657409668}
I0502 16:17:38.706363 139695629522688 logging_writer.py:48] [1060] accumulated_eval_time=271.170069, accumulated_logging_time=0.034573, accumulated_submission_time=2476.552847, global_step=1060, preemption_count=0, score=2476.552847, test/ctc_loss=6.627043724060059, test/num_examples=2472, test/wer=0.899580, total_duration=2747.773009, train/ctc_loss=6.718019962310791, train/wer=0.944630, validation/ctc_loss=6.727187633514404, validation/num_examples=5348, validation/wer=0.895995
I0502 16:19:09.892626 139695621129984 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.0015807151794434, loss=4.8238115310668945
I0502 16:22:52.385141 139695629522688 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2495100498199463, loss=4.525188446044922
I0502 16:26:35.059313 139695621129984 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.2871630191802979, loss=4.351213455200195
I0502 16:30:17.778705 139695629522688 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.435804009437561, loss=4.123178482055664
I0502 16:34:00.848453 139695621129984 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.3311058282852173, loss=3.9866673946380615
I0502 16:37:43.375568 139695629522688 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.2091825008392334, loss=3.801086902618408
I0502 16:41:25.795206 139695621129984 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.5799480676651, loss=3.622121810913086
I0502 16:45:08.391631 139695629522688 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.5673456192016602, loss=3.5672032833099365
I0502 16:48:50.767611 139695621129984 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.3964447975158691, loss=3.4656550884246826
I0502 16:52:33.349911 139695629522688 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.3961459398269653, loss=3.381293535232544
I0502 16:56:18.671483 139697370322688 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.3142054080963135, loss=3.2599804401397705
I0502 16:57:40.336724 139871489369920 spec.py:298] Evaluating on the training split.
I0502 16:58:12.299214 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 16:58:47.840935 139871489369920 spec.py:326] Evaluating on the test split.
I0502 16:59:06.516647 139871489369920 submission_runner.py:415] Time since start: 5235.60s, 	Step: 2138, 	{'train/ctc_loss': DeviceArray(5.483969, dtype=float32), 'train/wer': 0.8924486588711611, 'validation/ctc_loss': DeviceArray(5.5327845, dtype=float32), 'validation/wer': 0.8616677440206852, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.366244, dtype=float32), 'test/wer': 0.8577173846810067, 'test/num_examples': 2472, 'score': 4878.157326221466, 'total_duration': 5235.602177143097, 'accumulated_submission_time': 4878.157326221466, 'accumulated_eval_time': 357.34816002845764, 'accumulated_logging_time': 0.06437349319458008}
I0502 16:59:06.537326 139696786642688 logging_writer.py:48] [2138] accumulated_eval_time=357.348160, accumulated_logging_time=0.064373, accumulated_submission_time=4878.157326, global_step=2138, preemption_count=0, score=4878.157326, test/ctc_loss=5.366243839263916, test/num_examples=2472, test/wer=0.857717, total_duration=5235.602177, train/ctc_loss=5.483969211578369, train/wer=0.892449, validation/ctc_loss=5.532784461975098, validation/num_examples=5348, validation/wer=0.861668
I0502 17:01:26.108129 139696778249984 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.3388786315917969, loss=3.1927077770233154
I0502 17:05:08.181697 139696786642688 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.7860980033874512, loss=3.1115801334381104
I0502 17:08:49.679256 139696778249984 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.6958271265029907, loss=3.0691611766815186
I0502 17:12:31.058804 139696786642688 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.7092475891113281, loss=2.9779393672943115
I0502 17:16:12.313565 139696778249984 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.5572640895843506, loss=2.9386885166168213
I0502 17:19:53.781196 139696786642688 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.736294150352478, loss=2.867098093032837
I0502 17:23:35.288244 139696778249984 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.376646637916565, loss=2.769742965698242
I0502 17:27:17.354844 139696786642688 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.308322787284851, loss=2.7607221603393555
I0502 17:30:59.951448 139696778249984 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.2765707969665527, loss=2.7572948932647705
I0502 17:34:44.531035 139696786642688 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.8117103576660156, loss=2.6819920539855957
I0502 17:38:25.615061 139696778249984 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.699265956878662, loss=2.6975326538085938
I0502 17:39:07.380192 139871489369920 spec.py:298] Evaluating on the training split.
I0502 17:39:42.494327 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 17:40:18.830693 139871489369920 spec.py:326] Evaluating on the test split.
I0502 17:40:37.734524 139871489369920 submission_runner.py:415] Time since start: 7726.82s, 	Step: 3220, 	{'train/ctc_loss': DeviceArray(4.1454697, dtype=float32), 'train/wer': 0.782216107658491, 'validation/ctc_loss': DeviceArray(4.3330736, dtype=float32), 'validation/wer': 0.7763413057530705, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(4.018673, dtype=float32), 'test/wer': 0.7450490524648102, 'test/num_examples': 2472, 'score': 7278.974670886993, 'total_duration': 7726.819999933243, 'accumulated_submission_time': 7278.974670886993, 'accumulated_eval_time': 447.70059275627136, 'accumulated_logging_time': 0.0964658260345459}
I0502 17:40:37.753579 139696786642688 logging_writer.py:48] [3220] accumulated_eval_time=447.700593, accumulated_logging_time=0.096466, accumulated_submission_time=7278.974671, global_step=3220, preemption_count=0, score=7278.974671, test/ctc_loss=4.018672943115234, test/num_examples=2472, test/wer=0.745049, total_duration=7726.820000, train/ctc_loss=4.145469665527344, train/wer=0.782216, validation/ctc_loss=4.333073616027832, validation/num_examples=5348, validation/wer=0.776341
I0502 17:43:37.084690 139696778249984 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.4846497774124146, loss=2.6688313484191895
I0502 17:47:18.307987 139696786642688 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.0076498985290527, loss=2.626028060913086
I0502 17:50:59.246932 139696778249984 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.8303639888763428, loss=2.5811469554901123
I0502 17:54:39.710409 139696786642688 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.677771806716919, loss=2.498114824295044
I0502 17:58:20.905386 139696778249984 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.6798145771026611, loss=2.5174057483673096
I0502 18:02:03.003245 139696786642688 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.6179251670837402, loss=2.3825533390045166
I0502 18:05:44.215809 139696778249984 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.7220370769500732, loss=2.4123592376708984
I0502 18:09:24.878127 139696786642688 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.7402441501617432, loss=2.367103099822998
I0502 18:13:05.364466 139696778249984 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.7479515075683594, loss=2.360062599182129
I0502 18:16:49.847523 139696458962688 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.449255108833313, loss=2.3574843406677246
I0502 18:20:30.961791 139696450569984 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.885258436203003, loss=2.3594729900360107
I0502 18:20:39.648113 139871489369920 spec.py:298] Evaluating on the training split.
I0502 18:21:18.080621 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 18:21:55.847442 139871489369920 spec.py:326] Evaluating on the test split.
I0502 18:22:15.611419 139871489369920 submission_runner.py:415] Time since start: 10224.70s, 	Step: 4305, 	{'train/ctc_loss': DeviceArray(1.6632795, dtype=float32), 'train/wer': 0.4426373718438773, 'validation/ctc_loss': DeviceArray(2.0852106, dtype=float32), 'validation/wer': 0.4943704232554101, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.6083852, dtype=float32), 'test/wer': 0.4253854122235086, 'test/num_examples': 2472, 'score': 9680.844708681107, 'total_duration': 10224.696694850922, 'accumulated_submission_time': 9680.844708681107, 'accumulated_eval_time': 543.6617970466614, 'accumulated_logging_time': 0.12613964080810547}
I0502 18:22:15.630663 139697370322688 logging_writer.py:48] [4305] accumulated_eval_time=543.661797, accumulated_logging_time=0.126140, accumulated_submission_time=9680.844709, global_step=4305, preemption_count=0, score=9680.844709, test/ctc_loss=1.6083852052688599, test/num_examples=2472, test/wer=0.425385, total_duration=10224.696695, train/ctc_loss=1.6632795333862305, train/wer=0.442637, validation/ctc_loss=2.0852105617523193, validation/num_examples=5348, validation/wer=0.494370
I0502 18:25:47.907804 139697361929984 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.5006040334701538, loss=2.269364833831787
I0502 18:29:29.746192 139697370322688 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.933719277381897, loss=2.2375283241271973
I0502 18:33:11.779768 139697361929984 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.5202436447143555, loss=2.237433433532715
I0502 18:36:53.770913 139697370322688 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.050950050354004, loss=2.3298323154449463
I0502 18:40:35.335376 139697361929984 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.5677003860473633, loss=2.1894984245300293
I0502 18:44:16.561500 139697370322688 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.438356637954712, loss=2.2360498905181885
I0502 18:47:58.091244 139697361929984 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.542390823364258, loss=2.143343925476074
I0502 18:51:39.685378 139697370322688 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.5247806310653687, loss=2.1781301498413086
I0502 18:55:24.552423 139696499922688 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.4883311986923218, loss=2.055105447769165
I0502 18:59:05.887579 139696491529984 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.1421551704406738, loss=2.145066976547241
I0502 19:02:15.972024 139871489369920 spec.py:298] Evaluating on the training split.
I0502 19:02:54.544647 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 19:03:33.064104 139871489369920 spec.py:326] Evaluating on the test split.
I0502 19:03:52.974417 139871489369920 submission_runner.py:415] Time since start: 12722.06s, 	Step: 5387, 	{'train/ctc_loss': DeviceArray(0.8888197, dtype=float32), 'train/wer': 0.27909391376198656, 'validation/ctc_loss': DeviceArray(1.2589918, dtype=float32), 'validation/wer': 0.3423573792318305, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.89234847, dtype=float32), 'test/wer': 0.2734954197387931, 'test/num_examples': 2472, 'score': 12081.160904169083, 'total_duration': 12722.059762954712, 'accumulated_submission_time': 12081.160904169083, 'accumulated_eval_time': 640.6621906757355, 'accumulated_logging_time': 0.15582513809204102}
I0502 19:03:52.993961 139696208082688 logging_writer.py:48] [5387] accumulated_eval_time=640.662191, accumulated_logging_time=0.155825, accumulated_submission_time=12081.160904, global_step=5387, preemption_count=0, score=12081.160904, test/ctc_loss=0.8923484683036804, test/num_examples=2472, test/wer=0.273495, total_duration=12722.059763, train/ctc_loss=0.888819694519043, train/wer=0.279094, validation/ctc_loss=1.2589918375015259, validation/num_examples=5348, validation/wer=0.342357
I0502 19:04:24.030498 139696199689984 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.6254931688308716, loss=2.152818202972412
I0502 19:08:05.213508 139696208082688 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.713472604751587, loss=2.1128735542297363
I0502 19:11:45.859960 139696199689984 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.2603002786636353, loss=2.11681866645813
I0502 19:15:26.689042 139696208082688 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.7970279455184937, loss=1.9956755638122559
I0502 19:19:07.708315 139696199689984 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.5299756526947021, loss=2.042348861694336
I0502 19:22:48.560694 139696208082688 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.089942693710327, loss=2.132990598678589
I0502 19:26:29.971605 139696199689984 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.5909419059753418, loss=2.0373525619506836
I0502 19:30:11.576105 139696208082688 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.3596327304840088, loss=2.0689144134521484
I0502 19:33:56.834824 139696208082688 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.4114482402801514, loss=2.060906410217285
I0502 19:37:38.207810 139696199689984 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.4874188899993896, loss=1.9974209070205688
I0502 19:41:19.573942 139696208082688 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.3028249740600586, loss=1.953366994857788
I0502 19:43:54.297981 139871489369920 spec.py:298] Evaluating on the training split.
I0502 19:44:33.512980 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 19:45:12.482570 139871489369920 spec.py:326] Evaluating on the test split.
I0502 19:45:32.445069 139871489369920 submission_runner.py:415] Time since start: 15221.53s, 	Step: 6471, 	{'train/ctc_loss': DeviceArray(0.75119567, dtype=float32), 'train/wer': 0.24818219161274147, 'validation/ctc_loss': DeviceArray(1.0957747, dtype=float32), 'validation/wer': 0.31131028760528323, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7431393, dtype=float32), 'test/wer': 0.2396563280726342, 'test/num_examples': 2472, 'score': 14482.440133333206, 'total_duration': 15221.530571699142, 'accumulated_submission_time': 14482.440133333206, 'accumulated_eval_time': 738.8074173927307, 'accumulated_logging_time': 0.18617677688598633}
I0502 19:45:32.463824 139696863442688 logging_writer.py:48] [6471] accumulated_eval_time=738.807417, accumulated_logging_time=0.186177, accumulated_submission_time=14482.440133, global_step=6471, preemption_count=0, score=14482.440133, test/ctc_loss=0.7431393265724182, test/num_examples=2472, test/wer=0.239656, total_duration=15221.530572, train/ctc_loss=0.7511956691741943, train/wer=0.248182, validation/ctc_loss=1.0957746505737305, validation/num_examples=5348, validation/wer=0.311310
I0502 19:46:39.015072 139696855049984 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.9865418672561646, loss=1.973312497138977
I0502 19:50:20.268584 139696863442688 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.1957192420959473, loss=1.958790898323059
I0502 19:54:01.645554 139696855049984 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.4105545282363892, loss=1.8891074657440186
I0502 19:57:42.998137 139696863442688 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.5202523469924927, loss=1.9999563694000244
I0502 20:01:23.580700 139696855049984 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.5616337060928345, loss=2.014575958251953
I0502 20:05:04.588774 139696863442688 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.4685415029525757, loss=1.9560881853103638
I0502 20:08:45.371425 139696855049984 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.102865219116211, loss=1.8887840509414673
I0502 20:12:25.913951 139696863442688 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.0402660369873047, loss=1.9176748991012573
I0502 20:16:10.616682 139696863442688 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.068786144256592, loss=1.8404645919799805
I0502 20:19:51.258815 139696855049984 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.7417114973068237, loss=1.9297934770584106
I0502 20:23:31.956443 139696863442688 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.8671526908874512, loss=1.9227458238601685
I0502 20:25:33.294925 139871489369920 spec.py:298] Evaluating on the training split.
I0502 20:26:12.380856 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 20:26:51.178738 139871489369920 spec.py:326] Evaluating on the test split.
I0502 20:27:11.051606 139871489369920 submission_runner.py:415] Time since start: 17720.14s, 	Step: 7556, 	{'train/ctc_loss': DeviceArray(0.63099647, dtype=float32), 'train/wer': 0.2138504243507115, 'validation/ctc_loss': DeviceArray(0.99086165, dtype=float32), 'validation/wer': 0.285029281517429, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.66075623, dtype=float32), 'test/wer': 0.21579022200556539, 'test/num_examples': 2472, 'score': 16883.246504068375, 'total_duration': 17720.13691663742, 'accumulated_submission_time': 16883.246504068375, 'accumulated_eval_time': 836.5620307922363, 'accumulated_logging_time': 0.2158203125}
I0502 20:27:11.070439 139696571602688 logging_writer.py:48] [7556] accumulated_eval_time=836.562031, accumulated_logging_time=0.215820, accumulated_submission_time=16883.246504, global_step=7556, preemption_count=0, score=16883.246504, test/ctc_loss=0.6607562303543091, test/num_examples=2472, test/wer=0.215790, total_duration=17720.136917, train/ctc_loss=0.6309964656829834, train/wer=0.213850, validation/ctc_loss=0.9908616542816162, validation/num_examples=5348, validation/wer=0.285029
I0502 20:28:50.497268 139696563209984 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.931433916091919, loss=1.9768394231796265
I0502 20:32:31.296875 139696571602688 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.1865794658660889, loss=1.813974380493164
I0502 20:36:12.661265 139696563209984 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.7987276315689087, loss=1.9495574235916138
I0502 20:39:53.284779 139696571602688 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.9042013883590698, loss=1.8879072666168213
I0502 20:43:34.041492 139696563209984 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.6592262983322144, loss=1.990241527557373
I0502 20:47:15.388007 139696571602688 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.7491241693496704, loss=1.9171160459518433
I0502 20:50:56.742366 139696563209984 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.3159656524658203, loss=1.887878656387329
I0502 20:54:41.592058 139696571602688 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.020814895629883, loss=1.8922739028930664
I0502 20:58:22.226901 139696563209984 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.3756346702575684, loss=1.8241373300552368
I0502 21:02:03.475214 139696571602688 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.5828063488006592, loss=1.8007869720458984
I0502 21:05:45.193275 139696563209984 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.462983250617981, loss=1.7725750207901
I0502 21:07:11.447645 139871489369920 spec.py:298] Evaluating on the training split.
I0502 21:07:51.059870 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 21:08:29.687013 139871489369920 spec.py:326] Evaluating on the test split.
I0502 21:08:49.784898 139871489369920 submission_runner.py:415] Time since start: 20218.87s, 	Step: 8640, 	{'train/ctc_loss': DeviceArray(0.52292067, dtype=float32), 'train/wer': 0.18322889310694507, 'validation/ctc_loss': DeviceArray(0.91389155, dtype=float32), 'validation/wer': 0.2622601279317697, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.59213, dtype=float32), 'test/wer': 0.1942802591757561, 'test/num_examples': 2472, 'score': 19283.59943461418, 'total_duration': 20218.869008779526, 'accumulated_submission_time': 19283.59943461418, 'accumulated_eval_time': 934.896053314209, 'accumulated_logging_time': 0.24507927894592285}
I0502 21:08:49.807666 139696571602688 logging_writer.py:48] [8640] accumulated_eval_time=934.896053, accumulated_logging_time=0.245079, accumulated_submission_time=19283.599435, global_step=8640, preemption_count=0, score=19283.599435, test/ctc_loss=0.5921300053596497, test/num_examples=2472, test/wer=0.194280, total_duration=20218.869009, train/ctc_loss=0.5229206681251526, train/wer=0.183229, validation/ctc_loss=0.9138915538787842, validation/num_examples=5348, validation/wer=0.262260
I0502 21:11:04.504984 139696563209984 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.4174559116363525, loss=1.8070404529571533
I0502 21:14:45.195559 139696571602688 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.4201972484588623, loss=1.8617546558380127
I0502 21:18:26.118509 139696563209984 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.3585749864578247, loss=1.7907819747924805
I0502 21:22:07.138158 139696571602688 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.5746554136276245, loss=1.846142292022705
I0502 21:25:48.830891 139696563209984 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.0799388885498047, loss=1.7802226543426514
I0502 21:29:29.900474 139696571602688 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.4643311500549316, loss=1.7808901071548462
I0502 21:33:14.353090 139696571602688 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.5659871101379395, loss=1.8206993341445923
I0502 21:36:54.880047 139696563209984 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.3942341804504395, loss=1.823312759399414
I0502 21:40:35.583111 139696571602688 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.3148618936538696, loss=1.7600682973861694
I0502 21:44:16.756504 139696563209984 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.7893532514572144, loss=1.7990955114364624
I0502 21:47:57.469656 139696571602688 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.1653857231140137, loss=1.7712239027023315
I0502 21:48:50.246121 139871489369920 spec.py:298] Evaluating on the training split.
I0502 21:49:29.375159 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 21:50:07.812738 139871489369920 spec.py:326] Evaluating on the test split.
I0502 21:50:27.841759 139871489369920 submission_runner.py:415] Time since start: 22716.93s, 	Step: 9725, 	{'train/ctc_loss': DeviceArray(0.47180223, dtype=float32), 'train/wer': 0.16240443902984003, 'validation/ctc_loss': DeviceArray(0.85911745, dtype=float32), 'validation/wer': 0.2462445368503314, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5455671, dtype=float32), 'test/wer': 0.1811386671541446, 'test/num_examples': 2472, 'score': 21684.008035182953, 'total_duration': 22716.927237272263, 'accumulated_submission_time': 21684.008035182953, 'accumulated_eval_time': 1032.4898192882538, 'accumulated_logging_time': 0.28319239616394043}
I0502 21:50:27.861140 139696571602688 logging_writer.py:48] [9725] accumulated_eval_time=1032.489819, accumulated_logging_time=0.283192, accumulated_submission_time=21684.008035, global_step=9725, preemption_count=0, score=21684.008035, test/ctc_loss=0.5455670952796936, test/num_examples=2472, test/wer=0.181139, total_duration=22716.927237, train/ctc_loss=0.4718022346496582, train/wer=0.162404, validation/ctc_loss=0.8591174483299255, validation/num_examples=5348, validation/wer=0.246245
I0502 21:53:15.602470 139696563209984 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.0504395961761475, loss=1.7564579248428345
I0502 21:56:56.313574 139696571602688 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.6858772039413452, loss=1.7694236040115356
I0502 22:00:37.798957 139696563209984 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.326478123664856, loss=1.7779514789581299
I0502 22:04:19.751882 139696571602688 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.685618281364441, loss=1.7907135486602783
I0502 22:08:01.084447 139696563209984 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.2490805387496948, loss=1.759214162826538
I0502 22:11:45.532621 139696571602688 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.4772579669952393, loss=1.7077288627624512
I0502 22:15:26.759273 139696563209984 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.5560702085494995, loss=1.7392586469650269
I0502 22:19:07.897303 139696571602688 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.3333412408828735, loss=1.7261799573898315
I0502 22:22:48.894438 139696563209984 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.3885177373886108, loss=1.7341665029525757
I0502 22:26:29.786873 139696571602688 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.917904019355774, loss=1.689117431640625
I0502 22:30:10.578764 139696563209984 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.6873478889465332, loss=1.7366176843643188
I0502 22:30:28.062579 139871489369920 spec.py:298] Evaluating on the training split.
I0502 22:31:07.362223 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 22:31:46.137976 139871489369920 spec.py:326] Evaluating on the test split.
I0502 22:32:06.424780 139871489369920 submission_runner.py:415] Time since start: 25215.51s, 	Step: 10809, 	{'train/ctc_loss': DeviceArray(0.4464667, dtype=float32), 'train/wer': 0.15798505901848583, 'validation/ctc_loss': DeviceArray(0.80295885, dtype=float32), 'validation/wer': 0.23129021987669923, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5021481, dtype=float32), 'test/wer': 0.1657221782137997, 'test/num_examples': 2472, 'score': 24084.183243751526, 'total_duration': 25215.51024222374, 'accumulated_submission_time': 24084.183243751526, 'accumulated_eval_time': 1130.8501091003418, 'accumulated_logging_time': 0.31403565406799316}
I0502 22:32:06.447930 139696648402688 logging_writer.py:48] [10809] accumulated_eval_time=1130.850109, accumulated_logging_time=0.314036, accumulated_submission_time=24084.183244, global_step=10809, preemption_count=0, score=24084.183244, test/ctc_loss=0.5021480917930603, test/num_examples=2472, test/wer=0.165722, total_duration=25215.510242, train/ctc_loss=0.44646671414375305, train/wer=0.157985, validation/ctc_loss=0.8029588460922241, validation/num_examples=5348, validation/wer=0.231290
I0502 22:35:29.606892 139696640009984 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.457409143447876, loss=1.6763076782226562
I0502 22:39:10.459400 139696648402688 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.6840276718139648, loss=1.7025948762893677
I0502 22:42:51.389046 139696640009984 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.55493426322937, loss=1.6563588380813599
I0502 22:46:33.444856 139696648402688 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.6401567459106445, loss=1.709677815437317
I0502 22:50:14.263742 139696640009984 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.5671387910842896, loss=1.7315237522125244
I0502 22:53:59.173585 139696648402688 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.4443765878677368, loss=1.7211331129074097
I0502 22:57:39.800236 139696640009984 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.3363044261932373, loss=1.714034080505371
I0502 23:01:20.521402 139696648402688 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.559892177581787, loss=1.73304283618927
I0502 23:05:01.464765 139696640009984 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.6014550924301147, loss=1.7341982126235962
I0502 23:08:42.304055 139696648402688 logging_writer.py:48] [11800] global_step=11800, grad_norm=4.001204490661621, loss=1.723676323890686
I0502 23:12:07.834573 139871489369920 spec.py:298] Evaluating on the training split.
I0502 23:12:47.123616 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 23:13:25.650277 139871489369920 spec.py:326] Evaluating on the test split.
I0502 23:13:45.590589 139871489369920 submission_runner.py:415] Time since start: 27714.68s, 	Step: 11894, 	{'train/ctc_loss': DeviceArray(0.44423392, dtype=float32), 'train/wer': 0.150088579814379, 'validation/ctc_loss': DeviceArray(0.79011637, dtype=float32), 'validation/wer': 0.2267363891595674, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.48864916, dtype=float32), 'test/wer': 0.15684601791481323, 'test/num_examples': 2472, 'score': 26485.539510965347, 'total_duration': 27714.676038742065, 'accumulated_submission_time': 26485.539510965347, 'accumulated_eval_time': 1228.6042091846466, 'accumulated_logging_time': 0.3533620834350586}
I0502 23:13:45.613113 139697078482688 logging_writer.py:48] [11894] accumulated_eval_time=1228.604209, accumulated_logging_time=0.353362, accumulated_submission_time=26485.539511, global_step=11894, preemption_count=0, score=26485.539511, test/ctc_loss=0.4886491596698761, test/num_examples=2472, test/wer=0.156846, total_duration=27714.676039, train/ctc_loss=0.4442339241504669, train/wer=0.150089, validation/ctc_loss=0.7901163697242737, validation/num_examples=5348, validation/wer=0.226736
I0502 23:14:01.198628 139697070089984 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.399960994720459, loss=1.6294273138046265
I0502 23:17:42.093617 139697078482688 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.3060969114303589, loss=1.790723204612732
I0502 23:21:22.927113 139697070089984 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.4111021757125854, loss=1.7511982917785645
I0502 23:25:04.078081 139697078482688 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.0937039852142334, loss=1.7105869054794312
I0502 23:28:44.659587 139697070089984 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.4413005113601685, loss=1.6984620094299316
I0502 23:32:28.619050 139696423122688 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.532117486000061, loss=1.6871331930160522
I0502 23:36:09.163951 139696414729984 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.1032408475875854, loss=1.6960453987121582
I0502 23:39:50.092085 139696423122688 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.087531566619873, loss=1.739910364151001
I0502 23:43:31.239053 139696414729984 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.1181408166885376, loss=1.658839225769043
I0502 23:47:12.427374 139696423122688 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.5545389652252197, loss=1.66398024559021
I0502 23:50:53.242318 139696414729984 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.616308331489563, loss=1.6906038522720337
I0502 23:53:47.486058 139871489369920 spec.py:298] Evaluating on the training split.
I0502 23:54:27.815124 139871489369920 spec.py:310] Evaluating on the validation split.
I0502 23:55:06.474848 139871489369920 spec.py:326] Evaluating on the test split.
I0502 23:55:26.451274 139871489369920 submission_runner.py:415] Time since start: 30215.54s, 	Step: 12980, 	{'train/ctc_loss': DeviceArray(0.44577932, dtype=float32), 'train/wer': 0.15225683910623367, 'validation/ctc_loss': DeviceArray(0.7783094, dtype=float32), 'validation/wer': 0.22558828353384983, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.47811306, dtype=float32), 'test/wer': 0.1572928726667073, 'test/num_examples': 2472, 'score': 28887.383266448975, 'total_duration': 30215.536543369293, 'accumulated_submission_time': 28887.383266448975, 'accumulated_eval_time': 1327.5673196315765, 'accumulated_logging_time': 0.39139556884765625}
I0502 23:55:26.473618 139696423122688 logging_writer.py:48] [12980] accumulated_eval_time=1327.567320, accumulated_logging_time=0.391396, accumulated_submission_time=28887.383266, global_step=12980, preemption_count=0, score=28887.383266, test/ctc_loss=0.478113055229187, test/num_examples=2472, test/wer=0.157293, total_duration=30215.536543, train/ctc_loss=0.44577932357788086, train/wer=0.152257, validation/ctc_loss=0.7783094048500061, validation/num_examples=5348, validation/wer=0.225588
I0502 23:56:13.026515 139696414729984 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.6554876565933228, loss=1.6882917881011963
I0502 23:59:54.090985 139696423122688 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.2175627946853638, loss=1.7465119361877441
I0503 00:03:35.431730 139696414729984 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.4922486543655396, loss=1.7523835897445679
I0503 00:07:16.649704 139696423122688 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.5643420219421387, loss=1.7030442953109741
I0503 00:11:01.208111 139696423122688 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.2616784572601318, loss=1.6631348133087158
I0503 00:14:41.973399 139696414729984 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.3194547891616821, loss=1.6269152164459229
I0503 00:18:22.767727 139696423122688 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.6045809984207153, loss=1.6205790042877197
I0503 00:22:03.398112 139696414729984 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.7584903240203857, loss=1.7347437143325806
I0503 00:25:44.170258 139696423122688 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.528511881828308, loss=1.666611909866333
I0503 00:29:25.254118 139696414729984 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.347495436668396, loss=1.6298896074295044
I0503 00:33:06.461901 139696423122688 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.8433430194854736, loss=1.6952556371688843
I0503 00:35:27.641591 139871489369920 spec.py:298] Evaluating on the training split.
I0503 00:36:07.923775 139871489369920 spec.py:310] Evaluating on the validation split.
I0503 00:36:46.736896 139871489369920 spec.py:326] Evaluating on the test split.
I0503 00:37:06.984988 139871489369920 submission_runner.py:415] Time since start: 32716.07s, 	Step: 14065, 	{'train/ctc_loss': DeviceArray(0.41207132, dtype=float32), 'train/wer': 0.14324004527214734, 'validation/ctc_loss': DeviceArray(0.74399793, dtype=float32), 'validation/wer': 0.21640343852810928, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45258555, dtype=float32), 'test/wer': 0.14922917555298276, 'test/num_examples': 2472, 'score': 31288.520746707916, 'total_duration': 32716.06949210167, 'accumulated_submission_time': 31288.520746707916, 'accumulated_eval_time': 1426.9078459739685, 'accumulated_logging_time': 0.4303567409515381}
I0503 00:37:07.021659 139696786642688 logging_writer.py:48] [14065] accumulated_eval_time=1426.907846, accumulated_logging_time=0.430357, accumulated_submission_time=31288.520747, global_step=14065, preemption_count=0, score=31288.520747, test/ctc_loss=0.4525855481624603, test/num_examples=2472, test/wer=0.149229, total_duration=32716.069492, train/ctc_loss=0.4120713174343109, train/wer=0.143240, validation/ctc_loss=0.7439979314804077, validation/num_examples=5348, validation/wer=0.216403
I0503 00:38:26.607667 139696778249984 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.553386926651001, loss=1.6535571813583374
I0503 00:42:07.390828 139696786642688 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.8242685794830322, loss=1.6429760456085205
I0503 00:45:48.344671 139696778249984 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.9044867753982544, loss=1.726410150527954
I0503 00:49:29.296688 139696786642688 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.3854236602783203, loss=1.6744483709335327
I0503 00:53:13.877966 139696786642688 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.3117780685424805, loss=1.6701021194458008
I0503 00:56:54.703013 139696778249984 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.7771927118301392, loss=1.658514142036438
I0503 01:00:35.172012 139696786642688 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.4499483108520508, loss=1.5870524644851685
I0503 01:04:16.131877 139696778249984 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.9256954193115234, loss=1.6078941822052002
I0503 01:07:56.837555 139696786642688 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.580755591392517, loss=1.557008981704712
I0503 01:11:37.308650 139696778249984 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.311856746673584, loss=1.645283579826355
I0503 01:15:17.673949 139696786642688 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.6282837390899658, loss=1.6550312042236328
I0503 01:17:07.858269 139871489369920 spec.py:298] Evaluating on the training split.
I0503 01:17:47.616992 139871489369920 spec.py:310] Evaluating on the validation split.
I0503 01:18:26.754640 139871489369920 spec.py:326] Evaluating on the test split.
I0503 01:18:47.182435 139871489369920 submission_runner.py:415] Time since start: 35216.27s, 	Step: 15151, 	{'train/ctc_loss': DeviceArray(0.39249593, dtype=float32), 'train/wer': 0.1338889827287699, 'validation/ctc_loss': DeviceArray(0.7314822, dtype=float32), 'validation/wer': 0.21100058852473252, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44553095, dtype=float32), 'test/wer': 0.14565433753783033, 'test/num_examples': 2472, 'score': 33689.327518224716, 'total_duration': 35216.26794672012, 'accumulated_submission_time': 33689.327518224716, 'accumulated_eval_time': 1526.230177640915, 'accumulated_logging_time': 0.48302149772644043}
I0503 01:18:47.202591 139696786642688 logging_writer.py:48] [15151] accumulated_eval_time=1526.230178, accumulated_logging_time=0.483021, accumulated_submission_time=33689.327518, global_step=15151, preemption_count=0, score=33689.327518, test/ctc_loss=0.4455309510231018, test/num_examples=2472, test/wer=0.145654, total_duration=35216.267947, train/ctc_loss=0.39249593019485474, train/wer=0.133889, validation/ctc_loss=0.731482207775116, validation/num_examples=5348, validation/wer=0.211001
I0503 01:20:37.379804 139696778249984 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.180079460144043, loss=1.666200041770935
I0503 01:24:18.548331 139696786642688 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.5762770175933838, loss=1.6765483617782593
I0503 01:27:59.451199 139696778249984 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.5537389516830444, loss=1.7207502126693726
I0503 01:31:43.761137 139695588562688 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.8665440082550049, loss=1.6677552461624146
I0503 01:35:24.203889 139695580169984 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.711789608001709, loss=1.6319876909255981
I0503 01:39:04.779573 139695588562688 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.5003412961959839, loss=1.6175427436828613
I0503 01:42:45.857713 139695580169984 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.678406000137329, loss=1.6499934196472168
I0503 01:46:26.977976 139695588562688 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.688354730606079, loss=1.7071397304534912
I0503 01:50:05.729984 139871489369920 spec.py:298] Evaluating on the training split.
I0503 01:50:45.912570 139871489369920 spec.py:310] Evaluating on the validation split.
I0503 01:51:25.100699 139871489369920 spec.py:326] Evaluating on the test split.
I0503 01:51:45.569201 139871489369920 submission_runner.py:415] Time since start: 37194.65s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(0.36473885, dtype=float32), 'train/wer': 0.12906961579592582, 'validation/ctc_loss': DeviceArray(0.72222537, dtype=float32), 'validation/wer': 0.20912888691641984, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.435909, dtype=float32), 'test/wer': 0.14079987000589037, 'test/num_examples': 2472, 'score': 35567.833047389984, 'total_duration': 37194.654368162155, 'accumulated_submission_time': 35567.833047389984, 'accumulated_eval_time': 1626.0672061443329, 'accumulated_logging_time': 0.5138900279998779}
I0503 01:51:45.594046 139695588562688 logging_writer.py:48] [16000] accumulated_eval_time=1626.067206, accumulated_logging_time=0.513890, accumulated_submission_time=35567.833047, global_step=16000, preemption_count=0, score=35567.833047, test/ctc_loss=0.4359090030193329, test/num_examples=2472, test/wer=0.140800, total_duration=37194.654368, train/ctc_loss=0.3647388517856598, train/wer=0.129070, validation/ctc_loss=0.7222253680229187, validation/num_examples=5348, validation/wer=0.209129
I0503 01:51:45.617977 139695580169984 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=35567.833047
I0503 01:51:45.744106 139871489369920 checkpoints.py:356] Saving checkpoint at step: 16000
I0503 01:51:46.464365 139871489369920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_sam/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0503 01:51:46.480009 139871489369920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_sam/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0503 01:51:47.830841 139871489369920 submission_runner.py:578] Tuning trial 1/1
I0503 01:51:47.831113 139871489369920 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0503 01:51:47.839893 139871489369920 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.870005, dtype=float32), 'train/wer': 4.768595256439623, 'validation/ctc_loss': DeviceArray(30.76967, dtype=float32), 'validation/wer': 4.396781445069417, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.750902, dtype=float32), 'test/wer': 4.654581276785896, 'test/num_examples': 2472, 'score': 75.28461813926697, 'total_duration': 263.99857091903687, 'accumulated_submission_time': 75.28461813926697, 'accumulated_eval_time': 188.71378564834595, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1060, {'train/ctc_loss': DeviceArray(6.71802, dtype=float32), 'train/wer': 0.9446303886634673, 'validation/ctc_loss': DeviceArray(6.7271876, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.6270437, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2476.5528469085693, 'total_duration': 2747.773008584976, 'accumulated_submission_time': 2476.5528469085693, 'accumulated_eval_time': 271.1700689792633, 'accumulated_logging_time': 0.03457331657409668, 'global_step': 1060, 'preemption_count': 0}), (2138, {'train/ctc_loss': DeviceArray(5.483969, dtype=float32), 'train/wer': 0.8924486588711611, 'validation/ctc_loss': DeviceArray(5.5327845, dtype=float32), 'validation/wer': 0.8616677440206852, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.366244, dtype=float32), 'test/wer': 0.8577173846810067, 'test/num_examples': 2472, 'score': 4878.157326221466, 'total_duration': 5235.602177143097, 'accumulated_submission_time': 4878.157326221466, 'accumulated_eval_time': 357.34816002845764, 'accumulated_logging_time': 0.06437349319458008, 'global_step': 2138, 'preemption_count': 0}), (3220, {'train/ctc_loss': DeviceArray(4.1454697, dtype=float32), 'train/wer': 0.782216107658491, 'validation/ctc_loss': DeviceArray(4.3330736, dtype=float32), 'validation/wer': 0.7763413057530705, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(4.018673, dtype=float32), 'test/wer': 0.7450490524648102, 'test/num_examples': 2472, 'score': 7278.974670886993, 'total_duration': 7726.819999933243, 'accumulated_submission_time': 7278.974670886993, 'accumulated_eval_time': 447.70059275627136, 'accumulated_logging_time': 0.0964658260345459, 'global_step': 3220, 'preemption_count': 0}), (4305, {'train/ctc_loss': DeviceArray(1.6632795, dtype=float32), 'train/wer': 0.4426373718438773, 'validation/ctc_loss': DeviceArray(2.0852106, dtype=float32), 'validation/wer': 0.4943704232554101, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.6083852, dtype=float32), 'test/wer': 0.4253854122235086, 'test/num_examples': 2472, 'score': 9680.844708681107, 'total_duration': 10224.696694850922, 'accumulated_submission_time': 9680.844708681107, 'accumulated_eval_time': 543.6617970466614, 'accumulated_logging_time': 0.12613964080810547, 'global_step': 4305, 'preemption_count': 0}), (5387, {'train/ctc_loss': DeviceArray(0.8888197, dtype=float32), 'train/wer': 0.27909391376198656, 'validation/ctc_loss': DeviceArray(1.2589918, dtype=float32), 'validation/wer': 0.3423573792318305, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.89234847, dtype=float32), 'test/wer': 0.2734954197387931, 'test/num_examples': 2472, 'score': 12081.160904169083, 'total_duration': 12722.059762954712, 'accumulated_submission_time': 12081.160904169083, 'accumulated_eval_time': 640.6621906757355, 'accumulated_logging_time': 0.15582513809204102, 'global_step': 5387, 'preemption_count': 0}), (6471, {'train/ctc_loss': DeviceArray(0.75119567, dtype=float32), 'train/wer': 0.24818219161274147, 'validation/ctc_loss': DeviceArray(1.0957747, dtype=float32), 'validation/wer': 0.31131028760528323, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7431393, dtype=float32), 'test/wer': 0.2396563280726342, 'test/num_examples': 2472, 'score': 14482.440133333206, 'total_duration': 15221.530571699142, 'accumulated_submission_time': 14482.440133333206, 'accumulated_eval_time': 738.8074173927307, 'accumulated_logging_time': 0.18617677688598633, 'global_step': 6471, 'preemption_count': 0}), (7556, {'train/ctc_loss': DeviceArray(0.63099647, dtype=float32), 'train/wer': 0.2138504243507115, 'validation/ctc_loss': DeviceArray(0.99086165, dtype=float32), 'validation/wer': 0.285029281517429, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.66075623, dtype=float32), 'test/wer': 0.21579022200556539, 'test/num_examples': 2472, 'score': 16883.246504068375, 'total_duration': 17720.13691663742, 'accumulated_submission_time': 16883.246504068375, 'accumulated_eval_time': 836.5620307922363, 'accumulated_logging_time': 0.2158203125, 'global_step': 7556, 'preemption_count': 0}), (8640, {'train/ctc_loss': DeviceArray(0.52292067, dtype=float32), 'train/wer': 0.18322889310694507, 'validation/ctc_loss': DeviceArray(0.91389155, dtype=float32), 'validation/wer': 0.2622601279317697, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.59213, dtype=float32), 'test/wer': 0.1942802591757561, 'test/num_examples': 2472, 'score': 19283.59943461418, 'total_duration': 20218.869008779526, 'accumulated_submission_time': 19283.59943461418, 'accumulated_eval_time': 934.896053314209, 'accumulated_logging_time': 0.24507927894592285, 'global_step': 8640, 'preemption_count': 0}), (9725, {'train/ctc_loss': DeviceArray(0.47180223, dtype=float32), 'train/wer': 0.16240443902984003, 'validation/ctc_loss': DeviceArray(0.85911745, dtype=float32), 'validation/wer': 0.2462445368503314, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5455671, dtype=float32), 'test/wer': 0.1811386671541446, 'test/num_examples': 2472, 'score': 21684.008035182953, 'total_duration': 22716.927237272263, 'accumulated_submission_time': 21684.008035182953, 'accumulated_eval_time': 1032.4898192882538, 'accumulated_logging_time': 0.28319239616394043, 'global_step': 9725, 'preemption_count': 0}), (10809, {'train/ctc_loss': DeviceArray(0.4464667, dtype=float32), 'train/wer': 0.15798505901848583, 'validation/ctc_loss': DeviceArray(0.80295885, dtype=float32), 'validation/wer': 0.23129021987669923, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5021481, dtype=float32), 'test/wer': 0.1657221782137997, 'test/num_examples': 2472, 'score': 24084.183243751526, 'total_duration': 25215.51024222374, 'accumulated_submission_time': 24084.183243751526, 'accumulated_eval_time': 1130.8501091003418, 'accumulated_logging_time': 0.31403565406799316, 'global_step': 10809, 'preemption_count': 0}), (11894, {'train/ctc_loss': DeviceArray(0.44423392, dtype=float32), 'train/wer': 0.150088579814379, 'validation/ctc_loss': DeviceArray(0.79011637, dtype=float32), 'validation/wer': 0.2267363891595674, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.48864916, dtype=float32), 'test/wer': 0.15684601791481323, 'test/num_examples': 2472, 'score': 26485.539510965347, 'total_duration': 27714.676038742065, 'accumulated_submission_time': 26485.539510965347, 'accumulated_eval_time': 1228.6042091846466, 'accumulated_logging_time': 0.3533620834350586, 'global_step': 11894, 'preemption_count': 0}), (12980, {'train/ctc_loss': DeviceArray(0.44577932, dtype=float32), 'train/wer': 0.15225683910623367, 'validation/ctc_loss': DeviceArray(0.7783094, dtype=float32), 'validation/wer': 0.22558828353384983, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.47811306, dtype=float32), 'test/wer': 0.1572928726667073, 'test/num_examples': 2472, 'score': 28887.383266448975, 'total_duration': 30215.536543369293, 'accumulated_submission_time': 28887.383266448975, 'accumulated_eval_time': 1327.5673196315765, 'accumulated_logging_time': 0.39139556884765625, 'global_step': 12980, 'preemption_count': 0}), (14065, {'train/ctc_loss': DeviceArray(0.41207132, dtype=float32), 'train/wer': 0.14324004527214734, 'validation/ctc_loss': DeviceArray(0.74399793, dtype=float32), 'validation/wer': 0.21640343852810928, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45258555, dtype=float32), 'test/wer': 0.14922917555298276, 'test/num_examples': 2472, 'score': 31288.520746707916, 'total_duration': 32716.06949210167, 'accumulated_submission_time': 31288.520746707916, 'accumulated_eval_time': 1426.9078459739685, 'accumulated_logging_time': 0.4303567409515381, 'global_step': 14065, 'preemption_count': 0}), (15151, {'train/ctc_loss': DeviceArray(0.39249593, dtype=float32), 'train/wer': 0.1338889827287699, 'validation/ctc_loss': DeviceArray(0.7314822, dtype=float32), 'validation/wer': 0.21100058852473252, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44553095, dtype=float32), 'test/wer': 0.14565433753783033, 'test/num_examples': 2472, 'score': 33689.327518224716, 'total_duration': 35216.26794672012, 'accumulated_submission_time': 33689.327518224716, 'accumulated_eval_time': 1526.230177640915, 'accumulated_logging_time': 0.48302149772644043, 'global_step': 15151, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(0.36473885, dtype=float32), 'train/wer': 0.12906961579592582, 'validation/ctc_loss': DeviceArray(0.72222537, dtype=float32), 'validation/wer': 0.20912888691641984, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.435909, dtype=float32), 'test/wer': 0.14079987000589037, 'test/num_examples': 2472, 'score': 35567.833047389984, 'total_duration': 37194.654368162155, 'accumulated_submission_time': 35567.833047389984, 'accumulated_eval_time': 1626.0672061443329, 'accumulated_logging_time': 0.5138900279998779, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0503 01:51:47.840049 139871489369920 submission_runner.py:581] Timing: 35567.833047389984
I0503 01:51:47.840114 139871489369920 submission_runner.py:582] ====================
I0503 01:51:47.841088 139871489369920 submission_runner.py:645] Final librispeech_deepspeech score: 35567.833047389984
