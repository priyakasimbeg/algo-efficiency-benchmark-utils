python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_sam --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_05-01-2023-19-00-56.log
I0501 19:01:18.680015 139645796558656 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_sam/imagenet_resnet_jax.
I0501 19:01:18.754264 139645796558656 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0501 19:01:19.634053 139645796558656 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0501 19:01:19.634687 139645796558656 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0501 19:01:19.638682 139645796558656 submission_runner.py:538] Using RNG seed 3285196731
I0501 19:01:22.357916 139645796558656 submission_runner.py:547] --- Tuning run 1/1 ---
I0501 19:01:22.358125 139645796558656 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_sam/imagenet_resnet_jax/trial_1.
I0501 19:01:22.358328 139645796558656 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_sam/imagenet_resnet_jax/trial_1/hparams.json.
I0501 19:01:22.480930 139645796558656 submission_runner.py:241] Initializing dataset.
I0501 19:01:22.493058 139645796558656 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:01:22.501865 139645796558656 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 19:01:22.502062 139645796558656 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 19:01:22.772071 139645796558656 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:01:23.881976 139645796558656 submission_runner.py:248] Initializing model.
I0501 19:01:35.479202 139645796558656 submission_runner.py:258] Initializing optimizer.
I0501 19:01:36.594910 139645796558656 submission_runner.py:265] Initializing metrics bundle.
I0501 19:01:36.595149 139645796558656 submission_runner.py:282] Initializing checkpoint and logger.
I0501 19:01:36.596177 139645796558656 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_sam/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0501 19:01:37.450189 139645796558656 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_sam/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0501 19:01:37.451172 139645796558656 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_sam/imagenet_resnet_jax/trial_1/flags_0.json.
I0501 19:01:37.457851 139645796558656 submission_runner.py:318] Starting training loop.
I0501 19:02:37.162512 139469155985152 logging_writer.py:48] [0] global_step=0, grad_norm=0.6045733690261841, loss=6.924281120300293
I0501 19:02:37.179203 139645796558656 spec.py:298] Evaluating on the training split.
I0501 19:02:37.682893 139645796558656 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:02:37.689434 139645796558656 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 19:02:37.689566 139645796558656 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 19:02:37.750879 139645796558656 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:02:49.475162 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 19:02:50.359518 139645796558656 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:02:50.369356 139645796558656 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 19:02:50.369519 139645796558656 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 19:02:50.422090 139645796558656 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:03:08.523754 139645796558656 spec.py:326] Evaluating on the test split.
I0501 19:03:08.944309 139645796558656 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 19:03:08.949715 139645796558656 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0501 19:03:08.982247 139645796558656 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 19:03:18.113970 139645796558656 submission_runner.py:415] Time since start: 100.66s, 	Step: 1, 	{'train/accuracy': 0.0008171236841008067, 'train/loss': 6.910836696624756, 'validation/accuracy': 0.0009399999980814755, 'validation/loss': 6.911313533782959, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.911646366119385, 'test/num_examples': 10000, 'score': 59.72118091583252, 'total_duration': 100.65605092048645, 'accumulated_submission_time': 59.72118091583252, 'accumulated_eval_time': 40.9347198009491, 'accumulated_logging_time': 0}
I0501 19:03:18.131374 139439099590400 logging_writer.py:48] [1] accumulated_eval_time=40.934720, accumulated_logging_time=0, accumulated_submission_time=59.721181, global_step=1, preemption_count=0, score=59.721181, test/accuracy=0.001000, test/loss=6.911646, test/num_examples=10000, total_duration=100.656051, train/accuracy=0.000817, train/loss=6.910837, validation/accuracy=0.000940, validation/loss=6.911314, validation/num_examples=50000
I0501 19:04:21.034868 139439107983104 logging_writer.py:48] [100] global_step=100, grad_norm=0.5862418413162231, loss=6.891930103302002
I0501 19:05:23.835520 139439099590400 logging_writer.py:48] [200] global_step=200, grad_norm=0.6228267550468445, loss=6.812286376953125
I0501 19:06:26.721186 139439107983104 logging_writer.py:48] [300] global_step=300, grad_norm=0.6766424179077148, loss=6.717372894287109
I0501 19:07:29.390893 139439099590400 logging_writer.py:48] [400] global_step=400, grad_norm=0.7103438377380371, loss=6.6831278800964355
I0501 19:08:31.976105 139439107983104 logging_writer.py:48] [500] global_step=500, grad_norm=0.7014662027359009, loss=6.655688285827637
I0501 19:09:34.575816 139439099590400 logging_writer.py:48] [600] global_step=600, grad_norm=0.6803512573242188, loss=6.6562724113464355
I0501 19:10:37.097650 139439107983104 logging_writer.py:48] [700] global_step=700, grad_norm=0.6898244619369507, loss=6.631678581237793
I0501 19:11:39.799128 139439099590400 logging_writer.py:48] [800] global_step=800, grad_norm=0.7182901501655579, loss=6.586031913757324
I0501 19:11:48.143368 139645796558656 spec.py:298] Evaluating on the training split.
I0501 19:11:55.196622 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 19:12:02.830814 139645796558656 spec.py:326] Evaluating on the test split.
I0501 19:12:05.080360 139645796558656 submission_runner.py:415] Time since start: 627.62s, 	Step: 815, 	{'train/accuracy': 0.016561701893806458, 'train/loss': 6.4454474449157715, 'validation/accuracy': 0.015960000455379486, 'validation/loss': 6.466333389282227, 'validation/num_examples': 50000, 'test/accuracy': 0.011900000274181366, 'test/loss': 6.553193092346191, 'test/num_examples': 10000, 'score': 569.7143239974976, 'total_duration': 627.6224095821381, 'accumulated_submission_time': 569.7143239974976, 'accumulated_eval_time': 57.87165379524231, 'accumulated_logging_time': 0.025225400924682617}
I0501 19:12:05.088605 139439435134720 logging_writer.py:48] [815] accumulated_eval_time=57.871654, accumulated_logging_time=0.025225, accumulated_submission_time=569.714324, global_step=815, preemption_count=0, score=569.714324, test/accuracy=0.011900, test/loss=6.553193, test/num_examples=10000, total_duration=627.622410, train/accuracy=0.016562, train/loss=6.445447, validation/accuracy=0.015960, validation/loss=6.466333, validation/num_examples=50000
I0501 19:12:59.010443 139439443527424 logging_writer.py:48] [900] global_step=900, grad_norm=0.6566616892814636, loss=6.578649520874023
I0501 19:14:01.272856 139439435134720 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6133909225463867, loss=6.577888488769531
I0501 19:15:03.868979 139439443527424 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5949797034263611, loss=6.552628517150879
I0501 19:16:06.507738 139439435134720 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5803260803222656, loss=6.5256876945495605
I0501 19:17:09.152694 139439443527424 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5634034276008606, loss=6.561697006225586
I0501 19:18:11.648676 139439435134720 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.588762640953064, loss=6.55251407623291
I0501 19:19:14.279502 139439443527424 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5777499675750732, loss=6.498392105102539
I0501 19:20:16.937329 139439435134720 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5467523336410522, loss=6.548920631408691
I0501 19:20:35.297383 139645796558656 spec.py:298] Evaluating on the training split.
I0501 19:20:42.323149 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 19:20:50.134366 139645796558656 spec.py:326] Evaluating on the test split.
I0501 19:20:52.095585 139645796558656 submission_runner.py:415] Time since start: 1154.64s, 	Step: 1631, 	{'train/accuracy': 0.01911272294819355, 'train/loss': 6.375968933105469, 'validation/accuracy': 0.018059998750686646, 'validation/loss': 6.411831855773926, 'validation/num_examples': 50000, 'test/accuracy': 0.011900000274181366, 'test/loss': 6.503609657287598, 'test/num_examples': 10000, 'score': 1079.903687953949, 'total_duration': 1154.6376321315765, 'accumulated_submission_time': 1079.903687953949, 'accumulated_eval_time': 74.66981720924377, 'accumulated_logging_time': 0.041953086853027344}
I0501 19:20:52.104617 139439443527424 logging_writer.py:48] [1631] accumulated_eval_time=74.669817, accumulated_logging_time=0.041953, accumulated_submission_time=1079.903688, global_step=1631, preemption_count=0, score=1079.903688, test/accuracy=0.011900, test/loss=6.503610, test/num_examples=10000, total_duration=1154.637632, train/accuracy=0.019113, train/loss=6.375969, validation/accuracy=0.018060, validation/loss=6.411832, validation/num_examples=50000
I0501 19:21:35.754587 139439435134720 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5772499442100525, loss=6.539240837097168
I0501 19:22:38.390701 139439443527424 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5400152206420898, loss=6.5485992431640625
I0501 19:23:40.624476 139439435134720 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5268887281417847, loss=6.514819622039795
I0501 19:24:43.169381 139439443527424 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5092256665229797, loss=6.501577854156494
I0501 19:25:45.576395 139439435134720 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.5058109164237976, loss=6.499114036560059
I0501 19:26:48.141844 139439443527424 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.4854423999786377, loss=6.582914352416992
I0501 19:27:50.586605 139439435134720 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.46754035353660583, loss=6.5397233963012695
I0501 19:28:52.911599 139439443527424 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.46693456172943115, loss=6.507481098175049
I0501 19:29:22.679658 139645796558656 spec.py:298] Evaluating on the training split.
I0501 19:29:29.565829 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 19:29:37.447358 139645796558656 spec.py:326] Evaluating on the test split.
I0501 19:29:39.690545 139645796558656 submission_runner.py:415] Time since start: 1682.23s, 	Step: 2449, 	{'train/accuracy': 0.018235810101032257, 'train/loss': 6.346316337585449, 'validation/accuracy': 0.0177800003439188, 'validation/loss': 6.3615031242370605, 'validation/num_examples': 50000, 'test/accuracy': 0.012100000865757465, 'test/loss': 6.44769287109375, 'test/num_examples': 10000, 'score': 1590.4593737125397, 'total_duration': 1682.2326290607452, 'accumulated_submission_time': 1590.4593737125397, 'accumulated_eval_time': 91.68068218231201, 'accumulated_logging_time': 0.059207916259765625}
I0501 19:29:39.699873 139439435134720 logging_writer.py:48] [2449] accumulated_eval_time=91.680682, accumulated_logging_time=0.059208, accumulated_submission_time=1590.459374, global_step=2449, preemption_count=0, score=1590.459374, test/accuracy=0.012100, test/loss=6.447693, test/num_examples=10000, total_duration=1682.232629, train/accuracy=0.018236, train/loss=6.346316, validation/accuracy=0.017780, validation/loss=6.361503, validation/num_examples=50000
I0501 19:30:12.280871 139439443527424 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.4465636909008026, loss=6.486996650695801
I0501 19:31:14.728433 139439435134720 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.4458507299423218, loss=6.526391506195068
I0501 19:32:17.073726 139439443527424 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.423981636762619, loss=6.503296852111816
I0501 19:33:19.787370 139439435134720 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.4178159236907959, loss=6.485016345977783
I0501 19:34:22.187258 139439443527424 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.4039948880672455, loss=6.457578182220459
I0501 19:35:24.815618 139439435134720 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.3896821141242981, loss=6.4831085205078125
I0501 19:36:27.623714 139439443527424 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.37096214294433594, loss=6.484509468078613
I0501 19:37:30.166945 139439435134720 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.3782147765159607, loss=6.4608330726623535
I0501 19:38:09.791681 139645796558656 spec.py:298] Evaluating on the training split.
I0501 19:38:16.662730 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 19:38:24.488966 139645796558656 spec.py:326] Evaluating on the test split.
I0501 19:38:26.753175 139645796558656 submission_runner.py:415] Time since start: 2209.30s, 	Step: 3265, 	{'train/accuracy': 0.022341357544064522, 'train/loss': 6.2720255851745605, 'validation/accuracy': 0.021299999207258224, 'validation/loss': 6.318572998046875, 'validation/num_examples': 50000, 'test/accuracy': 0.016300000250339508, 'test/loss': 6.418375492095947, 'test/num_examples': 10000, 'score': 2100.5307145118713, 'total_duration': 2209.2952592372894, 'accumulated_submission_time': 2100.5307145118713, 'accumulated_eval_time': 108.64215660095215, 'accumulated_logging_time': 0.0779719352722168}
I0501 19:38:26.761429 139439443527424 logging_writer.py:48] [3265] accumulated_eval_time=108.642157, accumulated_logging_time=0.077972, accumulated_submission_time=2100.530715, global_step=3265, preemption_count=0, score=2100.530715, test/accuracy=0.016300, test/loss=6.418375, test/num_examples=10000, total_duration=2209.295259, train/accuracy=0.022341, train/loss=6.272026, validation/accuracy=0.021300, validation/loss=6.318573, validation/num_examples=50000
I0501 19:38:49.461444 139439435134720 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.3902786374092102, loss=6.4457807540893555
I0501 19:39:51.808086 139439443527424 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.39557209610939026, loss=6.448496341705322
I0501 19:40:54.433391 139439435134720 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.37689101696014404, loss=6.429943084716797
I0501 19:41:56.894208 139439443527424 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.3724372684955597, loss=6.435288906097412
I0501 19:42:59.333274 139439435134720 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.36632779240608215, loss=6.463052749633789
I0501 19:44:02.023414 139439443527424 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.3994005024433136, loss=6.438639163970947
I0501 19:45:04.483642 139439435134720 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.356757253408432, loss=6.439745903015137
I0501 19:46:06.927204 139439443527424 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.3702588975429535, loss=6.400087356567383
I0501 19:46:57.284418 139645796558656 spec.py:298] Evaluating on the training split.
I0501 19:47:04.241435 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 19:47:11.954146 139645796558656 spec.py:326] Evaluating on the test split.
I0501 19:47:13.965451 139645796558656 submission_runner.py:415] Time since start: 2736.51s, 	Step: 4082, 	{'train/accuracy': 0.02746332809329033, 'train/loss': 6.220250606536865, 'validation/accuracy': 0.025340000167489052, 'validation/loss': 6.279265403747559, 'validation/num_examples': 50000, 'test/accuracy': 0.019100001081824303, 'test/loss': 6.370682716369629, 'test/num_examples': 10000, 'score': 2611.0350363254547, 'total_duration': 2736.5075266361237, 'accumulated_submission_time': 2611.0350363254547, 'accumulated_eval_time': 125.32316374778748, 'accumulated_logging_time': 0.0938117504119873}
I0501 19:47:13.974184 139439435134720 logging_writer.py:48] [4082] accumulated_eval_time=125.323164, accumulated_logging_time=0.093812, accumulated_submission_time=2611.035036, global_step=4082, preemption_count=0, score=2611.035036, test/accuracy=0.019100, test/loss=6.370683, test/num_examples=10000, total_duration=2736.507527, train/accuracy=0.027463, train/loss=6.220251, validation/accuracy=0.025340, validation/loss=6.279265, validation/num_examples=50000
I0501 19:47:25.809633 139439443527424 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.34777042269706726, loss=6.422058582305908
I0501 19:48:28.189597 139439435134720 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.4147089719772339, loss=6.460116863250732
I0501 19:49:30.678489 139439443527424 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.35359522700309753, loss=6.4376540184021
I0501 19:50:33.061421 139439435134720 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5508638620376587, loss=6.406542778015137
I0501 19:51:35.657780 139439443527424 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.42509573698043823, loss=6.410059452056885
I0501 19:52:38.197724 139439435134720 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.40399494767189026, loss=6.471708297729492
I0501 19:53:40.634457 139439443527424 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.3434353470802307, loss=6.416369915008545
I0501 19:54:43.146101 139439435134720 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.32794347405433655, loss=6.401088237762451
I0501 19:55:44.171549 139645796558656 spec.py:298] Evaluating on the training split.
I0501 19:55:51.181566 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 19:55:59.432336 139645796558656 spec.py:326] Evaluating on the test split.
I0501 19:56:01.674647 139645796558656 submission_runner.py:415] Time since start: 3264.22s, 	Step: 4899, 	{'train/accuracy': 0.026267537847161293, 'train/loss': 6.178737163543701, 'validation/accuracy': 0.024720000103116035, 'validation/loss': 6.217923164367676, 'validation/num_examples': 50000, 'test/accuracy': 0.018900001421570778, 'test/loss': 6.324992656707764, 'test/num_examples': 10000, 'score': 3121.213874101639, 'total_duration': 3264.2167263031006, 'accumulated_submission_time': 3121.213874101639, 'accumulated_eval_time': 142.8262391090393, 'accumulated_logging_time': 0.11025285720825195}
I0501 19:56:01.684018 139439443527424 logging_writer.py:48] [4899] accumulated_eval_time=142.826239, accumulated_logging_time=0.110253, accumulated_submission_time=3121.213874, global_step=4899, preemption_count=0, score=3121.213874, test/accuracy=0.018900, test/loss=6.324993, test/num_examples=10000, total_duration=3264.216726, train/accuracy=0.026268, train/loss=6.178737, validation/accuracy=0.024720, validation/loss=6.217923, validation/num_examples=50000
I0501 19:56:02.951677 139439435134720 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.3046077787876129, loss=6.377431869506836
I0501 19:57:05.442878 139439443527424 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.3135991096496582, loss=6.440720081329346
I0501 19:58:07.857342 139439435134720 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.27903831005096436, loss=6.320814609527588
I0501 19:59:10.446705 139439443527424 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.27821585536003113, loss=6.344354152679443
I0501 20:00:13.071181 139439435134720 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.2732003331184387, loss=6.3544416427612305
I0501 20:01:15.518254 139439443527424 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.30686119198799133, loss=6.315810203552246
I0501 20:02:17.829427 139439435134720 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.28841400146484375, loss=6.320128917694092
I0501 20:03:20.251307 139439443527424 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.27779653668403625, loss=6.280202865600586
I0501 20:04:22.621944 139439435134720 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.31558844447135925, loss=6.286309719085693
I0501 20:04:32.292730 139645796558656 spec.py:298] Evaluating on the training split.
I0501 20:04:39.245639 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 20:04:48.705584 139645796558656 spec.py:326] Evaluating on the test split.
I0501 20:04:50.941748 139645796558656 submission_runner.py:415] Time since start: 3793.48s, 	Step: 5717, 	{'train/accuracy': 0.03435905650258064, 'train/loss': 6.051488399505615, 'validation/accuracy': 0.03175999969244003, 'validation/loss': 6.1136698722839355, 'validation/num_examples': 50000, 'test/accuracy': 0.022200001403689384, 'test/loss': 6.241243839263916, 'test/num_examples': 10000, 'score': 3631.801424741745, 'total_duration': 3793.4838190078735, 'accumulated_submission_time': 3631.801424741745, 'accumulated_eval_time': 161.47521591186523, 'accumulated_logging_time': 0.12981343269348145}
I0501 20:04:50.951351 139439443527424 logging_writer.py:48] [5717] accumulated_eval_time=161.475216, accumulated_logging_time=0.129813, accumulated_submission_time=3631.801425, global_step=5717, preemption_count=0, score=3631.801425, test/accuracy=0.022200, test/loss=6.241244, test/num_examples=10000, total_duration=3793.483819, train/accuracy=0.034359, train/loss=6.051488, validation/accuracy=0.031760, validation/loss=6.113670, validation/num_examples=50000
I0501 20:05:43.393087 139439435134720 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.26081764698028564, loss=6.370706081390381
I0501 20:06:46.192225 139439443527424 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.3521767258644104, loss=6.390142917633057
I0501 20:07:48.725319 139439435134720 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.3843611776828766, loss=6.306338787078857
I0501 20:08:51.274670 139439443527424 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.38627371191978455, loss=6.396640300750732
I0501 20:09:53.866378 139439435134720 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.3161071538925171, loss=6.342049598693848
I0501 20:10:56.327717 139439443527424 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.3734058439731598, loss=6.379837989807129
I0501 20:11:58.990346 139439435134720 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.3627125024795532, loss=6.401784420013428
I0501 20:13:01.580159 139439443527424 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.44728028774261475, loss=6.440912246704102
I0501 20:13:21.156170 139645796558656 spec.py:298] Evaluating on the training split.
I0501 20:13:28.237228 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 20:13:38.073949 139645796558656 spec.py:326] Evaluating on the test split.
I0501 20:13:40.077193 139645796558656 submission_runner.py:415] Time since start: 4322.62s, 	Step: 6533, 	{'train/accuracy': 0.036192599684000015, 'train/loss': 6.022007942199707, 'validation/accuracy': 0.035179998725652695, 'validation/loss': 6.064110279083252, 'validation/num_examples': 50000, 'test/accuracy': 0.027500001713633537, 'test/loss': 6.187184810638428, 'test/num_examples': 10000, 'score': 4141.982597351074, 'total_duration': 4322.619256258011, 'accumulated_submission_time': 4141.982597351074, 'accumulated_eval_time': 180.3962128162384, 'accumulated_logging_time': 0.1519148349761963}
I0501 20:13:40.087913 139439435134720 logging_writer.py:48] [6533] accumulated_eval_time=180.396213, accumulated_logging_time=0.151915, accumulated_submission_time=4141.982597, global_step=6533, preemption_count=0, score=4141.982597, test/accuracy=0.027500, test/loss=6.187185, test/num_examples=10000, total_duration=4322.619256, train/accuracy=0.036193, train/loss=6.022008, validation/accuracy=0.035180, validation/loss=6.064110, validation/num_examples=50000
I0501 20:14:22.625760 139439443527424 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.35757961869239807, loss=6.523782253265381
I0501 20:15:25.247737 139439435134720 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.42898938059806824, loss=6.531131744384766
I0501 20:16:27.880875 139439443527424 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5095381736755371, loss=6.605284690856934
I0501 20:17:30.680837 139439435134720 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5491973161697388, loss=6.631527423858643
I0501 20:18:33.219005 139439443527424 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5096572637557983, loss=6.681550025939941
I0501 20:19:35.908612 139439435134720 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.49817517399787903, loss=6.663681507110596
I0501 20:20:38.545838 139439443527424 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.48221081495285034, loss=6.645129680633545
I0501 20:21:41.241672 139439435134720 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4627910852432251, loss=6.640598297119141
I0501 20:22:10.318115 139645796558656 spec.py:298] Evaluating on the training split.
I0501 20:22:17.404780 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 20:22:27.440942 139645796558656 spec.py:326] Evaluating on the test split.
I0501 20:22:29.612617 139645796558656 submission_runner.py:415] Time since start: 4852.15s, 	Step: 7348, 	{'train/accuracy': 0.026008449494838715, 'train/loss': 6.2319016456604, 'validation/accuracy': 0.024059999734163284, 'validation/loss': 6.267568588256836, 'validation/num_examples': 50000, 'test/accuracy': 0.01940000057220459, 'test/loss': 6.355428218841553, 'test/num_examples': 10000, 'score': 4652.193439245224, 'total_duration': 4852.154700040817, 'accumulated_submission_time': 4652.193439245224, 'accumulated_eval_time': 199.6907036304474, 'accumulated_logging_time': 0.17082643508911133}
I0501 20:22:29.621454 139439443527424 logging_writer.py:48] [7348] accumulated_eval_time=199.690704, accumulated_logging_time=0.170826, accumulated_submission_time=4652.193439, global_step=7348, preemption_count=0, score=4652.193439, test/accuracy=0.019400, test/loss=6.355428, test/num_examples=10000, total_duration=4852.154700, train/accuracy=0.026008, train/loss=6.231902, validation/accuracy=0.024060, validation/loss=6.267569, validation/num_examples=50000
I0501 20:23:02.863205 139439435134720 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4866434633731842, loss=6.681397438049316
I0501 20:24:05.653970 139439443527424 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5331587195396423, loss=6.694049835205078
I0501 20:25:08.113825 139439435134720 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5463935732841492, loss=6.743053436279297
I0501 20:26:10.601073 139439443527424 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5250698924064636, loss=6.627718925476074
I0501 20:27:13.226418 139439435134720 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.526480495929718, loss=6.70673131942749
I0501 20:28:15.987557 139439443527424 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6125946640968323, loss=6.788568496704102
I0501 20:29:18.794618 139439435134720 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4740464985370636, loss=6.55349588394165
I0501 20:30:21.436723 139439443527424 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5032686591148376, loss=6.649117946624756
I0501 20:30:59.925478 139645796558656 spec.py:298] Evaluating on the training split.
I0501 20:31:06.989528 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 20:31:16.850297 139645796558656 spec.py:326] Evaluating on the test split.
I0501 20:31:19.087538 139645796558656 submission_runner.py:415] Time since start: 5381.63s, 	Step: 8163, 	{'train/accuracy': 0.030094068497419357, 'train/loss': 6.146125316619873, 'validation/accuracy': 0.02757999859750271, 'validation/loss': 6.1950531005859375, 'validation/num_examples': 50000, 'test/accuracy': 0.02120000123977661, 'test/loss': 6.300453186035156, 'test/num_examples': 10000, 'score': 5162.478399515152, 'total_duration': 5381.629621744156, 'accumulated_submission_time': 5162.478399515152, 'accumulated_eval_time': 218.8527431488037, 'accumulated_logging_time': 0.18749618530273438}
I0501 20:31:19.097013 139439435134720 logging_writer.py:48] [8163] accumulated_eval_time=218.852743, accumulated_logging_time=0.187496, accumulated_submission_time=5162.478400, global_step=8163, preemption_count=0, score=5162.478400, test/accuracy=0.021200, test/loss=6.300453, test/num_examples=10000, total_duration=5381.629622, train/accuracy=0.030094, train/loss=6.146125, validation/accuracy=0.027580, validation/loss=6.195053, validation/num_examples=50000
I0501 20:31:42.865797 139439443527424 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.539678692817688, loss=6.66556978225708
I0501 20:32:45.529948 139439435134720 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4563741087913513, loss=6.5107903480529785
I0501 20:33:48.129241 139439443527424 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.40185225009918213, loss=6.482528209686279
I0501 20:34:50.578344 139439435134720 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.4015856087207794, loss=6.451199531555176
I0501 20:35:53.023246 139439443527424 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3117404580116272, loss=6.460598945617676
I0501 20:36:55.377367 139439435134720 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3919678032398224, loss=6.463072776794434
I0501 20:37:57.994673 139439443527424 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3508923649787903, loss=6.464564800262451
I0501 20:39:00.556647 139439435134720 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.32487964630126953, loss=6.412096977233887
I0501 20:39:49.596199 139645796558656 spec.py:298] Evaluating on the training split.
I0501 20:39:56.763620 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 20:40:06.775902 139645796558656 spec.py:326] Evaluating on the test split.
I0501 20:40:08.794093 139645796558656 submission_runner.py:415] Time since start: 5911.33s, 	Step: 8980, 	{'train/accuracy': 0.02048788219690323, 'train/loss': 6.259620189666748, 'validation/accuracy': 0.020519999787211418, 'validation/loss': 6.289515495300293, 'validation/num_examples': 50000, 'test/accuracy': 0.01590000092983246, 'test/loss': 6.416529178619385, 'test/num_examples': 10000, 'score': 5672.95721578598, 'total_duration': 5911.334886074066, 'accumulated_submission_time': 5672.95721578598, 'accumulated_eval_time': 238.04933142662048, 'accumulated_logging_time': 0.20601701736450195}
I0501 20:40:08.803822 139439443527424 logging_writer.py:48] [8980] accumulated_eval_time=238.049331, accumulated_logging_time=0.206017, accumulated_submission_time=5672.957216, global_step=8980, preemption_count=0, score=5672.957216, test/accuracy=0.015900, test/loss=6.416529, test/num_examples=10000, total_duration=5911.334886, train/accuracy=0.020488, train/loss=6.259620, validation/accuracy=0.020520, validation/loss=6.289515, validation/num_examples=50000
I0501 20:40:21.915581 139439435134720 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.309876024723053, loss=6.388457298278809
I0501 20:41:24.685989 139439443527424 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.29117199778556824, loss=6.42453670501709
I0501 20:42:27.416217 139439435134720 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.25011542439460754, loss=6.344390869140625
I0501 20:43:30.046592 139439443527424 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.35035088658332825, loss=6.419923305511475
I0501 20:44:32.795760 139439435134720 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.2680016756057739, loss=6.432941913604736
I0501 20:45:35.474959 139439443527424 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.22490864992141724, loss=6.35621452331543
I0501 20:46:38.131762 139439435134720 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.23868989944458008, loss=6.287297248840332
I0501 20:47:40.890846 139439443527424 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.2203323096036911, loss=6.324604034423828
I0501 20:48:39.424585 139645796558656 spec.py:298] Evaluating on the training split.
I0501 20:48:46.716732 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 20:48:56.557008 139645796558656 spec.py:326] Evaluating on the test split.
I0501 20:48:58.557664 139645796558656 submission_runner.py:415] Time since start: 6441.10s, 	Step: 9795, 	{'train/accuracy': 0.03266501799225807, 'train/loss': 6.091101169586182, 'validation/accuracy': 0.029980000108480453, 'validation/loss': 6.145623683929443, 'validation/num_examples': 50000, 'test/accuracy': 0.02200000174343586, 'test/loss': 6.273143291473389, 'test/num_examples': 10000, 'score': 6183.557404994965, 'total_duration': 6441.099747657776, 'accumulated_submission_time': 6183.557404994965, 'accumulated_eval_time': 257.18239569664, 'accumulated_logging_time': 0.2248680591583252}
I0501 20:48:58.567815 139439435134720 logging_writer.py:48] [9795] accumulated_eval_time=257.182396, accumulated_logging_time=0.224868, accumulated_submission_time=6183.557405, global_step=9795, preemption_count=0, score=6183.557405, test/accuracy=0.022000, test/loss=6.273143, test/num_examples=10000, total_duration=6441.099748, train/accuracy=0.032665, train/loss=6.091101, validation/accuracy=0.029980, validation/loss=6.145624, validation/num_examples=50000
I0501 20:49:02.360929 139439443527424 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.262085884809494, loss=6.288637161254883
I0501 20:50:05.206091 139439435134720 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.22389733791351318, loss=6.306799411773682
I0501 20:51:07.884812 139439443527424 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.16344141960144043, loss=6.297479152679443
I0501 20:52:10.555417 139439435134720 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.19829772412776947, loss=6.332691669464111
I0501 20:53:13.288261 139439443527424 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.18891115486621857, loss=6.287068843841553
I0501 20:54:16.042165 139439435134720 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.15285877883434296, loss=6.267702579498291
I0501 20:55:18.814243 139439443527424 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.16512668132781982, loss=6.233226776123047
I0501 20:56:21.342202 139439435134720 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.14715339243412018, loss=6.268401622772217
I0501 20:57:24.086155 139439443527424 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.15329448878765106, loss=6.227968215942383
I0501 20:57:28.842159 139645796558656 spec.py:298] Evaluating on the training split.
I0501 20:57:36.149194 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 20:57:46.006236 139645796558656 spec.py:326] Evaluating on the test split.
I0501 20:57:48.205429 139645796558656 submission_runner.py:415] Time since start: 6970.75s, 	Step: 10609, 	{'train/accuracy': 0.04239078238606453, 'train/loss': 5.946173191070557, 'validation/accuracy': 0.04123999923467636, 'validation/loss': 5.9871721267700195, 'validation/num_examples': 50000, 'test/accuracy': 0.029500002041459084, 'test/loss': 6.130752086639404, 'test/num_examples': 10000, 'score': 6693.812987089157, 'total_duration': 6970.747509241104, 'accumulated_submission_time': 6693.812987089157, 'accumulated_eval_time': 276.54565382003784, 'accumulated_logging_time': 0.24275755882263184}
I0501 20:57:48.215545 139439435134720 logging_writer.py:48] [10609] accumulated_eval_time=276.545654, accumulated_logging_time=0.242758, accumulated_submission_time=6693.812987, global_step=10609, preemption_count=0, score=6693.812987, test/accuracy=0.029500, test/loss=6.130752, test/num_examples=10000, total_duration=6970.747509, train/accuracy=0.042391, train/loss=5.946173, validation/accuracy=0.041240, validation/loss=5.987172, validation/num_examples=50000
I0501 20:58:45.821264 139439443527424 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.15349605679512024, loss=6.265792369842529
I0501 20:59:48.501728 139439435134720 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1920941323041916, loss=6.245453834533691
I0501 21:00:51.155078 139439443527424 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.1678714007139206, loss=6.226899147033691
I0501 21:01:53.855740 139439435134720 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.1856795847415924, loss=6.212226867675781
I0501 21:02:56.675813 139439443527424 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.16416220366954803, loss=6.232240200042725
I0501 21:03:59.534079 139439435134720 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.15765169262886047, loss=6.191000461578369
I0501 21:05:02.333698 139439443527424 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.1810407191514969, loss=6.20047664642334
I0501 21:06:05.173300 139439435134720 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.1576749086380005, loss=6.194393634796143
I0501 21:06:18.639147 139645796558656 spec.py:298] Evaluating on the training split.
I0501 21:06:26.058427 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 21:06:35.810178 139645796558656 spec.py:326] Evaluating on the test split.
I0501 21:06:37.776903 139645796558656 submission_runner.py:415] Time since start: 7500.32s, 	Step: 11423, 	{'train/accuracy': 0.047492824494838715, 'train/loss': 5.873133182525635, 'validation/accuracy': 0.045180000364780426, 'validation/loss': 5.923137187957764, 'validation/num_examples': 50000, 'test/accuracy': 0.03450000286102295, 'test/loss': 6.091293811798096, 'test/num_examples': 10000, 'score': 7204.21644282341, 'total_duration': 7500.318992137909, 'accumulated_submission_time': 7204.21644282341, 'accumulated_eval_time': 295.68339252471924, 'accumulated_logging_time': 0.26197099685668945}
I0501 21:06:37.789805 139439443527424 logging_writer.py:48] [11423] accumulated_eval_time=295.683393, accumulated_logging_time=0.261971, accumulated_submission_time=7204.216443, global_step=11423, preemption_count=0, score=7204.216443, test/accuracy=0.034500, test/loss=6.091294, test/num_examples=10000, total_duration=7500.318992, train/accuracy=0.047493, train/loss=5.873133, validation/accuracy=0.045180, validation/loss=5.923137, validation/num_examples=50000
I0501 21:07:26.783304 139439435134720 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.16594600677490234, loss=6.198909759521484
I0501 21:08:29.597258 139439443527424 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.14467692375183105, loss=6.1854753494262695
I0501 21:09:32.264444 139439435134720 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.21115469932556152, loss=6.234352111816406
I0501 21:10:34.955716 139439443527424 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.18431131541728973, loss=6.177544593811035
I0501 21:11:38.001713 139439435134720 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.1870896816253662, loss=6.207415580749512
I0501 21:12:40.673483 139439443527424 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.21012508869171143, loss=6.208072185516357
I0501 21:13:43.448630 139439435134720 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.18941134214401245, loss=6.229018688201904
I0501 21:14:46.168864 139439443527424 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.18532145023345947, loss=6.174307823181152
I0501 21:15:08.320508 139645796558656 spec.py:298] Evaluating on the training split.
I0501 21:15:15.672293 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 21:15:25.745389 139645796558656 spec.py:326] Evaluating on the test split.
I0501 21:15:27.835646 139645796558656 submission_runner.py:415] Time since start: 8030.38s, 	Step: 12237, 	{'train/accuracy': 0.051399074494838715, 'train/loss': 5.815375328063965, 'validation/accuracy': 0.04939999803900719, 'validation/loss': 5.879857540130615, 'validation/num_examples': 50000, 'test/accuracy': 0.03660000115633011, 'test/loss': 6.050558567047119, 'test/num_examples': 10000, 'score': 7714.727222919464, 'total_duration': 8030.376610279083, 'accumulated_submission_time': 7714.727222919464, 'accumulated_eval_time': 315.1973867416382, 'accumulated_logging_time': 0.2837400436401367}
I0501 21:15:27.845713 139439435134720 logging_writer.py:48] [12237] accumulated_eval_time=315.197387, accumulated_logging_time=0.283740, accumulated_submission_time=7714.727223, global_step=12237, preemption_count=0, score=7714.727223, test/accuracy=0.036600, test/loss=6.050559, test/num_examples=10000, total_duration=8030.376610, train/accuracy=0.051399, train/loss=5.815375, validation/accuracy=0.049400, validation/loss=5.879858, validation/num_examples=50000
I0501 21:16:08.107728 139439443527424 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.21509063243865967, loss=6.197107315063477
I0501 21:17:10.790612 139439435134720 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.17693071067333221, loss=6.172781944274902
I0501 21:18:13.586110 139439443527424 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.20319764316082, loss=6.257561206817627
I0501 21:19:16.491403 139439435134720 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.21906721591949463, loss=6.208327293395996
I0501 21:20:19.354016 139439443527424 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.18074890971183777, loss=6.12525749206543
I0501 21:21:22.405662 139439435134720 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.19737358391284943, loss=6.1667327880859375
I0501 21:22:25.245902 139439443527424 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.20234379172325134, loss=6.176159858703613
I0501 21:23:28.066119 139439435134720 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.21979643404483795, loss=6.214978218078613
I0501 21:23:57.918337 139645796558656 spec.py:298] Evaluating on the training split.
I0501 21:24:05.436062 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 21:24:15.424580 139645796558656 spec.py:326] Evaluating on the test split.
I0501 21:24:17.469314 139645796558656 submission_runner.py:415] Time since start: 8560.01s, 	Step: 13049, 	{'train/accuracy': 0.05066166818141937, 'train/loss': 5.805259704589844, 'validation/accuracy': 0.04848000034689903, 'validation/loss': 5.865444183349609, 'validation/num_examples': 50000, 'test/accuracy': 0.035600002855062485, 'test/loss': 6.052343845367432, 'test/num_examples': 10000, 'score': 8224.779244661331, 'total_duration': 8560.011388540268, 'accumulated_submission_time': 8224.779244661331, 'accumulated_eval_time': 334.7483389377594, 'accumulated_logging_time': 0.3032538890838623}
I0501 21:24:17.479151 139439443527424 logging_writer.py:48] [13049] accumulated_eval_time=334.748339, accumulated_logging_time=0.303254, accumulated_submission_time=8224.779245, global_step=13049, preemption_count=0, score=8224.779245, test/accuracy=0.035600, test/loss=6.052344, test/num_examples=10000, total_duration=8560.011389, train/accuracy=0.050662, train/loss=5.805260, validation/accuracy=0.048480, validation/loss=5.865444, validation/num_examples=50000
I0501 21:24:50.154504 139439435134720 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.1969459354877472, loss=6.217491149902344
I0501 21:25:53.144984 139439443527424 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.20189808309078217, loss=6.153951644897461
I0501 21:26:56.066388 139439435134720 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2096482664346695, loss=6.235769748687744
I0501 21:27:59.023686 139439443527424 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.206624835729599, loss=6.136849403381348
I0501 21:29:01.888714 139439435134720 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2046326994895935, loss=6.204672336578369
I0501 21:30:04.637665 139439443527424 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.22858388721942902, loss=6.129581928253174
I0501 21:31:07.534213 139439435134720 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.23436111211776733, loss=6.215089797973633
I0501 21:32:10.327480 139439443527424 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.19187721610069275, loss=6.160357475280762
I0501 21:32:47.515461 139645796558656 spec.py:298] Evaluating on the training split.
I0501 21:32:55.184555 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 21:33:05.216029 139645796558656 spec.py:326] Evaluating on the test split.
I0501 21:33:07.424507 139645796558656 submission_runner.py:415] Time since start: 9089.97s, 	Step: 13861, 	{'train/accuracy': 0.05159837380051613, 'train/loss': 5.787433624267578, 'validation/accuracy': 0.04843999817967415, 'validation/loss': 5.840727806091309, 'validation/num_examples': 50000, 'test/accuracy': 0.035600002855062485, 'test/loss': 6.047037601470947, 'test/num_examples': 10000, 'score': 8734.797052145004, 'total_duration': 9089.966566324234, 'accumulated_submission_time': 8734.797052145004, 'accumulated_eval_time': 354.6573369503021, 'accumulated_logging_time': 0.32082056999206543}
I0501 21:33:07.435972 139439435134720 logging_writer.py:48] [13861] accumulated_eval_time=354.657337, accumulated_logging_time=0.320821, accumulated_submission_time=8734.797052, global_step=13861, preemption_count=0, score=8734.797052, test/accuracy=0.035600, test/loss=6.047038, test/num_examples=10000, total_duration=9089.966566, train/accuracy=0.051598, train/loss=5.787434, validation/accuracy=0.048440, validation/loss=5.840728, validation/num_examples=50000
I0501 21:33:32.656196 139439443527424 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.2083878070116043, loss=6.203196048736572
I0501 21:34:35.539678 139439435134720 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.1997392326593399, loss=6.133977890014648
I0501 21:35:38.482249 139439443527424 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.2135484218597412, loss=6.150271415710449
I0501 21:36:41.241272 139439435134720 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.20914176106452942, loss=6.12677526473999
I0501 21:37:44.142320 139439443527424 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.2029270976781845, loss=6.082833290100098
I0501 21:38:46.955122 139439435134720 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.24199044704437256, loss=6.169377326965332
I0501 21:39:50.025911 139439443527424 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.24196119606494904, loss=6.1746649742126465
I0501 21:40:53.023793 139439435134720 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.24751505255699158, loss=6.175139427185059
I0501 21:41:38.025964 139645796558656 spec.py:298] Evaluating on the training split.
I0501 21:41:45.855610 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 21:41:55.880608 139645796558656 spec.py:326] Evaluating on the test split.
I0501 21:41:58.093860 139645796558656 submission_runner.py:415] Time since start: 9620.64s, 	Step: 14673, 	{'train/accuracy': 0.06742267310619354, 'train/loss': 5.633881568908691, 'validation/accuracy': 0.05929999798536301, 'validation/loss': 5.739711284637451, 'validation/num_examples': 50000, 'test/accuracy': 0.04200000315904617, 'test/loss': 5.937422275543213, 'test/num_examples': 10000, 'score': 9245.36721611023, 'total_duration': 9620.635943889618, 'accumulated_submission_time': 9245.36721611023, 'accumulated_eval_time': 374.7252116203308, 'accumulated_logging_time': 0.3410913944244385}
I0501 21:41:58.103442 139439443527424 logging_writer.py:48] [14673] accumulated_eval_time=374.725212, accumulated_logging_time=0.341091, accumulated_submission_time=9245.367216, global_step=14673, preemption_count=0, score=9245.367216, test/accuracy=0.042000, test/loss=5.937422, test/num_examples=10000, total_duration=9620.635944, train/accuracy=0.067423, train/loss=5.633882, validation/accuracy=0.059300, validation/loss=5.739711, validation/num_examples=50000
I0501 21:42:15.664916 139439435134720 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.25781330466270447, loss=6.2065749168396
I0501 21:43:18.293972 139439443527424 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.2145572155714035, loss=6.131246089935303
I0501 21:44:21.215187 139439435134720 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.21512040495872498, loss=6.106794834136963
I0501 21:45:24.141436 139439443527424 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.21326786279678345, loss=6.089765548706055
I0501 21:46:26.994496 139439435134720 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.21639452874660492, loss=6.047488212585449
I0501 21:47:29.865580 139439443527424 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.2503983676433563, loss=6.161811828613281
I0501 21:48:32.621771 139439435134720 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.2597264051437378, loss=6.128270149230957
I0501 21:49:35.596344 139439443527424 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.2312650978565216, loss=6.1669602394104
I0501 21:50:28.552852 139645796558656 spec.py:298] Evaluating on the training split.
I0501 21:50:36.424529 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 21:50:46.509412 139645796558656 spec.py:326] Evaluating on the test split.
I0501 21:50:48.758245 139645796558656 submission_runner.py:415] Time since start: 10151.30s, 	Step: 15486, 	{'train/accuracy': 0.06184231489896774, 'train/loss': 5.647058486938477, 'validation/accuracy': 0.0619799979031086, 'validation/loss': 5.708606243133545, 'validation/num_examples': 50000, 'test/accuracy': 0.04480000212788582, 'test/loss': 5.9155473709106445, 'test/num_examples': 10000, 'score': 9755.797398328781, 'total_duration': 10151.300331115723, 'accumulated_submission_time': 9755.797398328781, 'accumulated_eval_time': 394.93059182167053, 'accumulated_logging_time': 0.3590128421783447}
I0501 21:50:48.769450 139439435134720 logging_writer.py:48] [15486] accumulated_eval_time=394.930592, accumulated_logging_time=0.359013, accumulated_submission_time=9755.797398, global_step=15486, preemption_count=0, score=9755.797398, test/accuracy=0.044800, test/loss=5.915547, test/num_examples=10000, total_duration=10151.300331, train/accuracy=0.061842, train/loss=5.647058, validation/accuracy=0.061980, validation/loss=5.708606, validation/num_examples=50000
I0501 21:50:58.210147 139439443527424 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.25991514325141907, loss=6.14260196685791
I0501 21:52:01.115822 139439435134720 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.26687657833099365, loss=6.10685920715332
I0501 21:53:03.915144 139439443527424 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.23719368875026703, loss=6.088284492492676
I0501 21:54:06.792755 139439435134720 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.2735992670059204, loss=6.158463001251221
I0501 21:55:09.703130 139439443527424 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.2384698987007141, loss=6.091331958770752
I0501 21:56:12.483979 139439435134720 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.23421995341777802, loss=6.046689987182617
I0501 21:57:15.304718 139439443527424 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.24208955466747284, loss=6.1108808517456055
I0501 21:58:18.132882 139439435134720 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.2692205607891083, loss=6.108780384063721
I0501 21:59:19.415262 139645796558656 spec.py:298] Evaluating on the training split.
I0501 21:59:27.327970 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 21:59:37.614702 139645796558656 spec.py:326] Evaluating on the test split.
I0501 21:59:39.822988 139645796558656 submission_runner.py:415] Time since start: 10682.37s, 	Step: 16299, 	{'train/accuracy': 0.06399473547935486, 'train/loss': 5.677242279052734, 'validation/accuracy': 0.059579998254776, 'validation/loss': 5.753046035766602, 'validation/num_examples': 50000, 'test/accuracy': 0.044200003147125244, 'test/loss': 5.9595746994018555, 'test/num_examples': 10000, 'score': 10266.423383235931, 'total_duration': 10682.365035533905, 'accumulated_submission_time': 10266.423383235931, 'accumulated_eval_time': 415.3382611274719, 'accumulated_logging_time': 0.3791360855102539}
I0501 21:59:39.835233 139439443527424 logging_writer.py:48] [16299] accumulated_eval_time=415.338261, accumulated_logging_time=0.379136, accumulated_submission_time=10266.423383, global_step=16299, preemption_count=0, score=10266.423383, test/accuracy=0.044200, test/loss=5.959575, test/num_examples=10000, total_duration=10682.365036, train/accuracy=0.063995, train/loss=5.677242, validation/accuracy=0.059580, validation/loss=5.753046, validation/num_examples=50000
I0501 21:59:41.128232 139439435134720 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.2647673785686493, loss=6.122061729431152
I0501 22:00:43.936510 139439443527424 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.22454431653022766, loss=6.119231224060059
I0501 22:01:46.736150 139439435134720 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.275917112827301, loss=6.183158874511719
I0501 22:02:49.510232 139439443527424 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.22792062163352966, loss=6.113910675048828
I0501 22:03:52.426806 139439435134720 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.22211489081382751, loss=6.1072845458984375
I0501 22:04:55.057982 139439443527424 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.2569458782672882, loss=6.070905685424805
I0501 22:05:57.941553 139439435134720 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.19742581248283386, loss=6.08209753036499
I0501 22:07:00.800890 139439443527424 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.268449068069458, loss=6.065630912780762
I0501 22:08:03.501614 139439435134720 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.2831457257270813, loss=6.155825138092041
I0501 22:08:10.114219 139645796558656 spec.py:298] Evaluating on the training split.
I0501 22:08:17.742558 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 22:08:27.840016 139645796558656 spec.py:326] Evaluating on the test split.
I0501 22:08:30.057742 139645796558656 submission_runner.py:415] Time since start: 11212.60s, 	Step: 17112, 	{'train/accuracy': 0.0676618292927742, 'train/loss': 5.64604377746582, 'validation/accuracy': 0.0640999972820282, 'validation/loss': 5.711448669433594, 'validation/num_examples': 50000, 'test/accuracy': 0.046400003135204315, 'test/loss': 5.91102409362793, 'test/num_examples': 10000, 'score': 10776.682438850403, 'total_duration': 11212.599824666977, 'accumulated_submission_time': 10776.682438850403, 'accumulated_eval_time': 435.28177189826965, 'accumulated_logging_time': 0.40015125274658203}
I0501 22:08:30.067337 139439443527424 logging_writer.py:48] [17112] accumulated_eval_time=435.281772, accumulated_logging_time=0.400151, accumulated_submission_time=10776.682439, global_step=17112, preemption_count=0, score=10776.682439, test/accuracy=0.046400, test/loss=5.911024, test/num_examples=10000, total_duration=11212.599825, train/accuracy=0.067662, train/loss=5.646044, validation/accuracy=0.064100, validation/loss=5.711449, validation/num_examples=50000
I0501 22:09:25.876139 139439435134720 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.2482127845287323, loss=6.048864364624023
I0501 22:10:28.615874 139439443527424 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.29247385263442993, loss=6.114599227905273
I0501 22:11:31.333426 139439435134720 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.2482190728187561, loss=6.091248512268066
I0501 22:12:34.199081 139439443527424 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.2900887131690979, loss=6.099644184112549
I0501 22:13:36.827593 139439435134720 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.30172452330589294, loss=6.123155117034912
I0501 22:14:39.645667 139439443527424 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.30070993304252625, loss=6.140641212463379
I0501 22:15:42.402469 139439435134720 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.2948125898838043, loss=6.135680198669434
I0501 22:16:45.055145 139439443527424 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.26206469535827637, loss=6.187815189361572
I0501 22:17:00.381775 139645796558656 spec.py:298] Evaluating on the training split.
I0501 22:17:07.878393 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 22:17:18.279230 139645796558656 spec.py:326] Evaluating on the test split.
I0501 22:17:20.441749 139645796558656 submission_runner.py:415] Time since start: 11742.98s, 	Step: 17926, 	{'train/accuracy': 0.07144849747419357, 'train/loss': 5.579751014709473, 'validation/accuracy': 0.0688600018620491, 'validation/loss': 5.633138656616211, 'validation/num_examples': 50000, 'test/accuracy': 0.04690000042319298, 'test/loss': 5.853484630584717, 'test/num_examples': 10000, 'score': 11286.977898359299, 'total_duration': 11742.983834266663, 'accumulated_submission_time': 11286.977898359299, 'accumulated_eval_time': 455.34172105789185, 'accumulated_logging_time': 0.4177122116088867}
I0501 22:17:20.452471 139439435134720 logging_writer.py:48] [17926] accumulated_eval_time=455.341721, accumulated_logging_time=0.417712, accumulated_submission_time=11286.977898, global_step=17926, preemption_count=0, score=11286.977898, test/accuracy=0.046900, test/loss=5.853485, test/num_examples=10000, total_duration=11742.983834, train/accuracy=0.071448, train/loss=5.579751, validation/accuracy=0.068860, validation/loss=5.633139, validation/num_examples=50000
I0501 22:18:07.548177 139439443527424 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.32079991698265076, loss=6.13924503326416
I0501 22:19:10.500538 139439435134720 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.28048133850097656, loss=6.088634490966797
I0501 22:20:13.063148 139439443527424 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.33397042751312256, loss=6.164444923400879
I0501 22:21:15.732413 139439435134720 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.35622361302375793, loss=6.231682777404785
I0501 22:22:18.473687 139439443527424 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.32575929164886475, loss=6.138611793518066
I0501 22:23:21.312208 139439435134720 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.36146077513694763, loss=6.231308937072754
I0501 22:24:24.068945 139439443527424 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.37781426310539246, loss=6.158431053161621
I0501 22:25:26.778048 139439435134720 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.38345372676849365, loss=6.243772983551025
I0501 22:25:50.911759 139645796558656 spec.py:298] Evaluating on the training split.
I0501 22:25:58.494315 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 22:26:09.180752 139645796558656 spec.py:326] Evaluating on the test split.
I0501 22:26:11.196601 139645796558656 submission_runner.py:415] Time since start: 12273.74s, 	Step: 18740, 	{'train/accuracy': 0.06959502398967743, 'train/loss': 5.549996376037598, 'validation/accuracy': 0.06453999876976013, 'validation/loss': 5.640780925750732, 'validation/num_examples': 50000, 'test/accuracy': 0.05190000310540199, 'test/loss': 5.866814136505127, 'test/num_examples': 10000, 'score': 11797.41777229309, 'total_duration': 12273.737427949905, 'accumulated_submission_time': 11797.41777229309, 'accumulated_eval_time': 475.6252841949463, 'accumulated_logging_time': 0.4367046356201172}
I0501 22:26:11.207456 139439443527424 logging_writer.py:48] [18740] accumulated_eval_time=475.625284, accumulated_logging_time=0.436705, accumulated_submission_time=11797.417772, global_step=18740, preemption_count=0, score=11797.417772, test/accuracy=0.051900, test/loss=5.866814, test/num_examples=10000, total_duration=12273.737428, train/accuracy=0.069595, train/loss=5.549996, validation/accuracy=0.064540, validation/loss=5.640781, validation/num_examples=50000
I0501 22:26:49.510176 139439435134720 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3263765573501587, loss=6.1757588386535645
I0501 22:27:52.385039 139439443527424 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3433460295200348, loss=6.198328018188477
I0501 22:28:55.303787 139439435134720 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3554965555667877, loss=6.149753093719482
I0501 22:29:58.264341 139439443527424 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.3185853958129883, loss=6.154488563537598
I0501 22:31:01.069338 139439435134720 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.35726845264434814, loss=6.224163055419922
I0501 22:32:03.985256 139439443527424 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3146591782569885, loss=6.1730546951293945
I0501 22:33:06.899849 139439435134720 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3318040668964386, loss=6.202796936035156
I0501 22:34:09.770238 139439443527424 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.3530285060405731, loss=6.207674980163574
I0501 22:34:41.406478 139645796558656 spec.py:298] Evaluating on the training split.
I0501 22:34:49.187948 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 22:34:59.367064 139645796558656 spec.py:326] Evaluating on the test split.
I0501 22:35:01.584136 139645796558656 submission_runner.py:415] Time since start: 12804.13s, 	Step: 19552, 	{'train/accuracy': 0.07553412020206451, 'train/loss': 5.532441139221191, 'validation/accuracy': 0.07242000102996826, 'validation/loss': 5.6012749671936035, 'validation/num_examples': 50000, 'test/accuracy': 0.04920000210404396, 'test/loss': 5.822516441345215, 'test/num_examples': 10000, 'score': 12307.595932006836, 'total_duration': 12804.126222610474, 'accumulated_submission_time': 12307.595932006836, 'accumulated_eval_time': 495.8029251098633, 'accumulated_logging_time': 0.4573400020599365}
I0501 22:35:01.594988 139439435134720 logging_writer.py:48] [19552] accumulated_eval_time=495.802925, accumulated_logging_time=0.457340, accumulated_submission_time=12307.595932, global_step=19552, preemption_count=0, score=12307.595932, test/accuracy=0.049200, test/loss=5.822516, test/num_examples=10000, total_duration=12804.126223, train/accuracy=0.075534, train/loss=5.532441, validation/accuracy=0.072420, validation/loss=5.601275, validation/num_examples=50000
I0501 22:35:32.414835 139439443527424 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.35991257429122925, loss=6.213249206542969
I0501 22:36:35.315052 139439435134720 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.30951714515686035, loss=6.074105262756348
I0501 22:37:38.188387 139439443527424 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.33724653720855713, loss=6.222844123840332
I0501 22:38:40.997626 139439435134720 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3179064393043518, loss=6.218382835388184
I0501 22:39:43.817843 139439443527424 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.3209682106971741, loss=6.138331413269043
I0501 22:40:46.685425 139439435134720 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.321875661611557, loss=6.072894096374512
I0501 22:41:49.505806 139439443527424 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.2976047992706299, loss=6.0918378829956055
I0501 22:42:52.317097 139439435134720 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.333432674407959, loss=6.161130428314209
I0501 22:43:32.116798 139645796558656 spec.py:298] Evaluating on the training split.
I0501 22:43:39.125599 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 22:43:48.768231 139645796558656 spec.py:326] Evaluating on the test split.
I0501 22:43:51.006905 139645796558656 submission_runner.py:415] Time since start: 13333.55s, 	Step: 20365, 	{'train/accuracy': 0.07278379797935486, 'train/loss': 5.545177936553955, 'validation/accuracy': 0.07001999765634537, 'validation/loss': 5.614750385284424, 'validation/num_examples': 50000, 'test/accuracy': 0.04840000346302986, 'test/loss': 5.8477373123168945, 'test/num_examples': 10000, 'score': 12818.099341154099, 'total_duration': 13333.548988580704, 'accumulated_submission_time': 12818.099341154099, 'accumulated_eval_time': 514.6930129528046, 'accumulated_logging_time': 0.47579407691955566}
I0501 22:43:51.017744 139439443527424 logging_writer.py:48] [20365] accumulated_eval_time=514.693013, accumulated_logging_time=0.475794, accumulated_submission_time=12818.099341, global_step=20365, preemption_count=0, score=12818.099341, test/accuracy=0.048400, test/loss=5.847737, test/num_examples=10000, total_duration=13333.548989, train/accuracy=0.072784, train/loss=5.545178, validation/accuracy=0.070020, validation/loss=5.614750, validation/num_examples=50000
I0501 22:44:13.659691 139439435134720 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.32218143343925476, loss=6.095476150512695
I0501 22:45:16.387791 139439443527424 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.31104788184165955, loss=6.107998847961426
I0501 22:46:19.256233 139439435134720 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.341026246547699, loss=6.159781455993652
I0501 22:47:22.046399 139439443527424 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.2890776991844177, loss=6.140993118286133
I0501 22:48:24.900239 139439435134720 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.33787965774536133, loss=6.14108419418335
I0501 22:49:27.623687 139439443527424 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.31165504455566406, loss=6.162370681762695
I0501 22:50:30.350301 139439435134720 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.3238559663295746, loss=6.1265459060668945
I0501 22:51:33.223349 139439443527424 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.3079306483268738, loss=6.125378131866455
I0501 22:52:21.131150 139645796558656 spec.py:298] Evaluating on the training split.
I0501 22:52:27.961580 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 22:52:37.633485 139645796558656 spec.py:326] Evaluating on the test split.
I0501 22:52:39.748659 139645796558656 submission_runner.py:415] Time since start: 13862.29s, 	Step: 21178, 	{'train/accuracy': 0.06800063699483871, 'train/loss': 5.6065192222595215, 'validation/accuracy': 0.0634399950504303, 'validation/loss': 5.683626174926758, 'validation/num_examples': 50000, 'test/accuracy': 0.04570000246167183, 'test/loss': 5.894370079040527, 'test/num_examples': 10000, 'score': 13328.193813800812, 'total_duration': 13862.290736436844, 'accumulated_submission_time': 13328.193813800812, 'accumulated_eval_time': 533.3104994297028, 'accumulated_logging_time': 0.4945354461669922}
I0501 22:52:39.759756 139439435134720 logging_writer.py:48] [21178] accumulated_eval_time=533.310499, accumulated_logging_time=0.494535, accumulated_submission_time=13328.193814, global_step=21178, preemption_count=0, score=13328.193814, test/accuracy=0.045700, test/loss=5.894370, test/num_examples=10000, total_duration=13862.290736, train/accuracy=0.068001, train/loss=5.606519, validation/accuracy=0.063440, validation/loss=5.683626, validation/num_examples=50000
I0501 22:52:54.255025 139439443527424 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.33737844228744507, loss=6.078897953033447
I0501 22:53:57.021594 139439435134720 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.33266007900238037, loss=6.182831287384033
I0501 22:54:59.939458 139439443527424 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.3279142379760742, loss=6.15458869934082
I0501 22:56:02.731444 139439435134720 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.347813218832016, loss=6.149070739746094
I0501 22:57:05.475826 139439443527424 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.3359295427799225, loss=6.220546722412109
I0501 22:58:08.295677 139439435134720 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.3548741042613983, loss=6.1642022132873535
I0501 22:59:11.092159 139439443527424 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.2899027466773987, loss=6.086203098297119
I0501 23:00:14.023972 139439435134720 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.3407982289791107, loss=6.069774150848389
I0501 23:01:10.228460 139645796558656 spec.py:298] Evaluating on the training split.
I0501 23:01:16.885918 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 23:01:26.495632 139645796558656 spec.py:326] Evaluating on the test split.
I0501 23:01:28.732751 139645796558656 submission_runner.py:415] Time since start: 14391.27s, 	Step: 21991, 	{'train/accuracy': 0.07838408648967743, 'train/loss': 5.525690078735352, 'validation/accuracy': 0.07457999885082245, 'validation/loss': 5.589944362640381, 'validation/num_examples': 50000, 'test/accuracy': 0.05250000208616257, 'test/loss': 5.810934543609619, 'test/num_examples': 10000, 'score': 13838.644011497498, 'total_duration': 14391.274812221527, 'accumulated_submission_time': 13838.644011497498, 'accumulated_eval_time': 551.8147506713867, 'accumulated_logging_time': 0.5130834579467773}
I0501 23:01:28.745283 139439443527424 logging_writer.py:48] [21991] accumulated_eval_time=551.814751, accumulated_logging_time=0.513083, accumulated_submission_time=13838.644011, global_step=21991, preemption_count=0, score=13838.644011, test/accuracy=0.052500, test/loss=5.810935, test/num_examples=10000, total_duration=14391.274812, train/accuracy=0.078384, train/loss=5.525690, validation/accuracy=0.074580, validation/loss=5.589944, validation/num_examples=50000
I0501 23:01:35.035514 139439435134720 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.3681688606739044, loss=6.112020492553711
I0501 23:02:37.857327 139439443527424 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.33928126096725464, loss=6.0470051765441895
I0501 23:03:40.757603 139439435134720 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.30407455563545227, loss=6.054501533508301
I0501 23:04:43.614780 139439443527424 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3269577920436859, loss=6.100313186645508
I0501 23:05:46.344690 139439435134720 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.3081953227519989, loss=6.0435872077941895
I0501 23:06:49.219667 139439443527424 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.2716273069381714, loss=6.01946496963501
I0501 23:07:52.057836 139439435134720 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.3183956444263458, loss=6.041982650756836
I0501 23:08:54.829770 139439443527424 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.30137455463409424, loss=6.077671051025391
I0501 23:09:57.698766 139439435134720 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.28428328037261963, loss=6.090734481811523
I0501 23:09:59.230256 139645796558656 spec.py:298] Evaluating on the training split.
I0501 23:10:05.838772 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 23:10:15.372368 139645796558656 spec.py:326] Evaluating on the test split.
I0501 23:10:17.592470 139645796558656 submission_runner.py:415] Time since start: 14920.13s, 	Step: 22804, 	{'train/accuracy': 0.07403937727212906, 'train/loss': 5.535967826843262, 'validation/accuracy': 0.06960000097751617, 'validation/loss': 5.6228508949279785, 'validation/num_examples': 50000, 'test/accuracy': 0.05130000412464142, 'test/loss': 5.8489251136779785, 'test/num_examples': 10000, 'score': 14349.10959482193, 'total_duration': 14920.134546995163, 'accumulated_submission_time': 14349.10959482193, 'accumulated_eval_time': 570.1769309043884, 'accumulated_logging_time': 0.5340180397033691}
I0501 23:10:17.603658 139439443527424 logging_writer.py:48] [22804] accumulated_eval_time=570.176931, accumulated_logging_time=0.534018, accumulated_submission_time=14349.109595, global_step=22804, preemption_count=0, score=14349.109595, test/accuracy=0.051300, test/loss=5.848925, test/num_examples=10000, total_duration=14920.134547, train/accuracy=0.074039, train/loss=5.535968, validation/accuracy=0.069600, validation/loss=5.622851, validation/num_examples=50000
I0501 23:11:18.462475 139439435134720 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.3281547427177429, loss=6.028249740600586
I0501 23:12:21.228201 139439443527424 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.295600950717926, loss=6.046097278594971
I0501 23:13:24.090753 139439435134720 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.2827111780643463, loss=6.0213236808776855
I0501 23:14:26.960594 139439443527424 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.2970806956291199, loss=6.0254950523376465
I0501 23:15:29.777544 139439435134720 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.3102739453315735, loss=6.022987365722656
I0501 23:16:32.646189 139439443527424 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.33779171109199524, loss=6.044085502624512
I0501 23:17:35.476926 139439435134720 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.27797067165374756, loss=6.052730560302734
I0501 23:18:38.282809 139439443527424 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.2938911020755768, loss=6.12612771987915
I0501 23:18:47.970084 139645796558656 spec.py:298] Evaluating on the training split.
I0501 23:18:54.405572 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 23:19:04.095050 139645796558656 spec.py:326] Evaluating on the test split.
I0501 23:19:06.003437 139645796558656 submission_runner.py:415] Time since start: 15448.55s, 	Step: 23617, 	{'train/accuracy': 0.0729631707072258, 'train/loss': 5.556201934814453, 'validation/accuracy': 0.06796000152826309, 'validation/loss': 5.625189304351807, 'validation/num_examples': 50000, 'test/accuracy': 0.052000001072883606, 'test/loss': 5.8394551277160645, 'test/num_examples': 10000, 'score': 14859.45750284195, 'total_duration': 15448.5455057621, 'accumulated_submission_time': 14859.45750284195, 'accumulated_eval_time': 588.2102425098419, 'accumulated_logging_time': 0.5527200698852539}
I0501 23:19:06.013989 139439435134720 logging_writer.py:48] [23617] accumulated_eval_time=588.210243, accumulated_logging_time=0.552720, accumulated_submission_time=14859.457503, global_step=23617, preemption_count=0, score=14859.457503, test/accuracy=0.052000, test/loss=5.839455, test/num_examples=10000, total_duration=15448.545506, train/accuracy=0.072963, train/loss=5.556202, validation/accuracy=0.067960, validation/loss=5.625189, validation/num_examples=50000
I0501 23:19:58.904824 139439443527424 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.27063071727752686, loss=5.968646049499512
I0501 23:21:01.626015 139439435134720 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.3080451488494873, loss=5.96681022644043
I0501 23:22:04.389936 139439443527424 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.2773412764072418, loss=5.9848504066467285
I0501 23:23:07.216706 139439435134720 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.28365662693977356, loss=6.0269694328308105
I0501 23:24:09.973675 139439443527424 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.30292463302612305, loss=5.977787971496582
I0501 23:25:12.774619 139439435134720 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.272056519985199, loss=6.023542881011963
I0501 23:26:15.633745 139439443527424 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.2797239422798157, loss=6.006373882293701
I0501 23:27:18.451049 139439435134720 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.27133673429489136, loss=5.998995304107666
I0501 23:27:36.270252 139645796558656 spec.py:298] Evaluating on the training split.
I0501 23:27:42.649492 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 23:27:52.352468 139645796558656 spec.py:326] Evaluating on the test split.
I0501 23:27:54.341650 139645796558656 submission_runner.py:415] Time since start: 15976.88s, 	Step: 24430, 	{'train/accuracy': 0.07667011767625809, 'train/loss': 5.523855686187744, 'validation/accuracy': 0.0708799958229065, 'validation/loss': 5.593801021575928, 'validation/num_examples': 50000, 'test/accuracy': 0.05380000174045563, 'test/loss': 5.814946174621582, 'test/num_examples': 10000, 'score': 15369.69491314888, 'total_duration': 15976.883726358414, 'accumulated_submission_time': 15369.69491314888, 'accumulated_eval_time': 606.2816100120544, 'accumulated_logging_time': 0.5710222721099854}
I0501 23:27:54.352215 139439443527424 logging_writer.py:48] [24430] accumulated_eval_time=606.281610, accumulated_logging_time=0.571022, accumulated_submission_time=15369.694913, global_step=24430, preemption_count=0, score=15369.694913, test/accuracy=0.053800, test/loss=5.814946, test/num_examples=10000, total_duration=15976.883726, train/accuracy=0.076670, train/loss=5.523856, validation/accuracy=0.070880, validation/loss=5.593801, validation/num_examples=50000
I0501 23:28:38.935857 139439435134720 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.2959595024585724, loss=6.035941123962402
I0501 23:29:41.701830 139439443527424 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.2750025689601898, loss=6.024542808532715
I0501 23:30:44.593469 139439435134720 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.27063965797424316, loss=6.016462326049805
I0501 23:31:47.377255 139439443527424 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.2516809403896332, loss=6.005123138427734
I0501 23:32:50.044487 139439435134720 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.2614578902721405, loss=5.980967998504639
I0501 23:33:52.856957 139439443527424 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.2377588152885437, loss=6.021814823150635
I0501 23:34:55.669869 139439435134720 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.2657281458377838, loss=6.009235858917236
I0501 23:35:58.473643 139439443527424 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.2524922788143158, loss=6.0018415451049805
I0501 23:36:24.506489 139645796558656 spec.py:298] Evaluating on the training split.
I0501 23:36:30.765624 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 23:36:40.500716 139645796558656 spec.py:326] Evaluating on the test split.
I0501 23:36:42.489588 139645796558656 submission_runner.py:415] Time since start: 16505.03s, 	Step: 25243, 	{'train/accuracy': 0.08055643737316132, 'train/loss': 5.4853105545043945, 'validation/accuracy': 0.07745999842882156, 'validation/loss': 5.541722297668457, 'validation/num_examples': 50000, 'test/accuracy': 0.055900003761053085, 'test/loss': 5.762324333190918, 'test/num_examples': 10000, 'score': 15879.8308198452, 'total_duration': 16505.03166937828, 'accumulated_submission_time': 15879.8308198452, 'accumulated_eval_time': 624.2646837234497, 'accumulated_logging_time': 0.5888500213623047}
I0501 23:36:42.499843 139439435134720 logging_writer.py:48] [25243] accumulated_eval_time=624.264684, accumulated_logging_time=0.588850, accumulated_submission_time=15879.830820, global_step=25243, preemption_count=0, score=15879.830820, test/accuracy=0.055900, test/loss=5.762324, test/num_examples=10000, total_duration=16505.031669, train/accuracy=0.080556, train/loss=5.485311, validation/accuracy=0.077460, validation/loss=5.541722, validation/num_examples=50000
I0501 23:37:19.002189 139439443527424 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.25471818447113037, loss=6.024561882019043
I0501 23:38:21.717926 139439435134720 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.2943376302719116, loss=5.982561111450195
I0501 23:39:24.557923 139439443527424 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.2565975785255432, loss=6.000454902648926
I0501 23:40:27.488360 139439435134720 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.2918652296066284, loss=6.0587053298950195
I0501 23:41:30.312482 139439443527424 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.27548009157180786, loss=6.017748832702637
I0501 23:42:32.966010 139439435134720 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.2855753004550934, loss=5.971179008483887
I0501 23:43:35.670306 139439443527424 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.3085019588470459, loss=5.993246555328369
I0501 23:44:38.438172 139439435134720 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2595585584640503, loss=6.019965648651123
I0501 23:45:12.746268 139645796558656 spec.py:298] Evaluating on the training split.
I0501 23:45:18.989526 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 23:45:28.633411 139645796558656 spec.py:326] Evaluating on the test split.
I0501 23:45:30.872480 139645796558656 submission_runner.py:415] Time since start: 17033.41s, 	Step: 26056, 	{'train/accuracy': 0.0772879421710968, 'train/loss': 5.502620697021484, 'validation/accuracy': 0.07155999541282654, 'validation/loss': 5.5831298828125, 'validation/num_examples': 50000, 'test/accuracy': 0.05490000173449516, 'test/loss': 5.802459239959717, 'test/num_examples': 10000, 'score': 16390.0584628582, 'total_duration': 17033.414568662643, 'accumulated_submission_time': 16390.0584628582, 'accumulated_eval_time': 642.3908762931824, 'accumulated_logging_time': 0.6069700717926025}
I0501 23:45:30.882877 139439443527424 logging_writer.py:48] [26056] accumulated_eval_time=642.390876, accumulated_logging_time=0.606970, accumulated_submission_time=16390.058463, global_step=26056, preemption_count=0, score=16390.058463, test/accuracy=0.054900, test/loss=5.802459, test/num_examples=10000, total_duration=17033.414569, train/accuracy=0.077288, train/loss=5.502621, validation/accuracy=0.071560, validation/loss=5.583130, validation/num_examples=50000
I0501 23:45:59.071465 139439435134720 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.2711388170719147, loss=6.083579063415527
I0501 23:47:01.928345 139439443527424 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.24584393203258514, loss=6.016395092010498
I0501 23:48:04.652249 139439435134720 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.2696552574634552, loss=6.046350955963135
I0501 23:49:07.368986 139439443527424 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.27881157398223877, loss=6.071850776672363
I0501 23:50:10.172950 139439435134720 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.24697327613830566, loss=5.992060661315918
I0501 23:51:13.049327 139439443527424 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.2572266161441803, loss=5.995229244232178
I0501 23:52:15.886238 139439435134720 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.29216641187667847, loss=5.943236351013184
I0501 23:53:18.673319 139439443527424 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.29259657859802246, loss=6.029158592224121
I0501 23:54:01.143161 139645796558656 spec.py:298] Evaluating on the training split.
I0501 23:54:07.318396 139645796558656 spec.py:310] Evaluating on the validation split.
I0501 23:54:16.985925 139645796558656 spec.py:326] Evaluating on the test split.
I0501 23:54:19.098480 139645796558656 submission_runner.py:415] Time since start: 17561.64s, 	Step: 26869, 	{'train/accuracy': 0.08103475719690323, 'train/loss': 5.472643852233887, 'validation/accuracy': 0.07846000045537949, 'validation/loss': 5.553103446960449, 'validation/num_examples': 50000, 'test/accuracy': 0.056300003081560135, 'test/loss': 5.769197463989258, 'test/num_examples': 10000, 'score': 16900.300141334534, 'total_duration': 17561.640553712845, 'accumulated_submission_time': 16900.300141334534, 'accumulated_eval_time': 660.346165895462, 'accumulated_logging_time': 0.6251132488250732}
I0501 23:54:19.108993 139439435134720 logging_writer.py:48] [26869] accumulated_eval_time=660.346166, accumulated_logging_time=0.625113, accumulated_submission_time=16900.300141, global_step=26869, preemption_count=0, score=16900.300141, test/accuracy=0.056300, test/loss=5.769197, test/num_examples=10000, total_duration=17561.640554, train/accuracy=0.081035, train/loss=5.472644, validation/accuracy=0.078460, validation/loss=5.553103, validation/num_examples=50000
I0501 23:54:39.283069 139439443527424 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.26288896799087524, loss=5.988080978393555
I0501 23:55:42.054034 139439435134720 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.25824540853500366, loss=5.909614086151123
I0501 23:56:45.031709 139439443527424 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.26525381207466125, loss=6.004566669464111
I0501 23:57:47.884800 139439435134720 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.2871192395687103, loss=6.076023101806641
I0501 23:58:50.819908 139439443527424 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.2758484482765198, loss=5.975677490234375
I0501 23:59:53.634480 139439435134720 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.28638842701911926, loss=5.942203044891357
I0502 00:00:56.397583 139439443527424 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.3021317720413208, loss=5.9542741775512695
I0502 00:01:59.255618 139439435134720 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.28564414381980896, loss=5.932551383972168
I0502 00:02:49.638158 139645796558656 spec.py:298] Evaluating on the training split.
I0502 00:02:55.859772 139645796558656 spec.py:310] Evaluating on the validation split.
I0502 00:03:05.708931 139645796558656 spec.py:326] Evaluating on the test split.
I0502 00:03:07.932695 139645796558656 submission_runner.py:415] Time since start: 18090.47s, 	Step: 27682, 	{'train/accuracy': 0.08169244229793549, 'train/loss': 5.484259128570557, 'validation/accuracy': 0.07558000087738037, 'validation/loss': 5.545289993286133, 'validation/num_examples': 50000, 'test/accuracy': 0.05560000240802765, 'test/loss': 5.772364616394043, 'test/num_examples': 10000, 'score': 17410.808962583542, 'total_duration': 18090.4747672081, 'accumulated_submission_time': 17410.808962583542, 'accumulated_eval_time': 678.6406836509705, 'accumulated_logging_time': 0.6450293064117432}
I0502 00:03:07.944878 139439443527424 logging_writer.py:48] [27682] accumulated_eval_time=678.640684, accumulated_logging_time=0.645029, accumulated_submission_time=17410.808963, global_step=27682, preemption_count=0, score=17410.808963, test/accuracy=0.055600, test/loss=5.772365, test/num_examples=10000, total_duration=18090.474767, train/accuracy=0.081692, train/loss=5.484259, validation/accuracy=0.075580, validation/loss=5.545290, validation/num_examples=50000
I0502 00:03:19.878381 139439435134720 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.2655107080936432, loss=5.9726386070251465
I0502 00:04:22.576903 139439443527424 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.29678672552108765, loss=6.0139055252075195
I0502 00:05:25.295431 139439435134720 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.2652446925640106, loss=5.904234886169434
I0502 00:06:27.191592 139645796558656 spec.py:298] Evaluating on the training split.
I0502 00:06:33.326355 139645796558656 spec.py:310] Evaluating on the validation split.
I0502 00:06:43.119367 139645796558656 spec.py:326] Evaluating on the test split.
I0502 00:06:45.245253 139645796558656 submission_runner.py:415] Time since start: 18307.79s, 	Step: 28000, 	{'train/accuracy': 0.0849609375, 'train/loss': 5.440621852874756, 'validation/accuracy': 0.07931999862194061, 'validation/loss': 5.533401012420654, 'validation/num_examples': 50000, 'test/accuracy': 0.05610000342130661, 'test/loss': 5.76111364364624, 'test/num_examples': 10000, 'score': 17610.04269552231, 'total_duration': 18307.78733229637, 'accumulated_submission_time': 17610.04269552231, 'accumulated_eval_time': 696.6943175792694, 'accumulated_logging_time': 0.6657488346099854}
I0502 00:06:45.256198 139439443527424 logging_writer.py:48] [28000] accumulated_eval_time=696.694318, accumulated_logging_time=0.665749, accumulated_submission_time=17610.042696, global_step=28000, preemption_count=0, score=17610.042696, test/accuracy=0.056100, test/loss=5.761114, test/num_examples=10000, total_duration=18307.787332, train/accuracy=0.084961, train/loss=5.440622, validation/accuracy=0.079320, validation/loss=5.533401, validation/num_examples=50000
I0502 00:06:45.274084 139439435134720 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=17610.042696
I0502 00:06:45.563103 139645796558656 checkpoints.py:356] Saving checkpoint at step: 28000
I0502 00:06:46.669202 139645796558656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_sam/imagenet_resnet_jax/trial_1/checkpoint_28000
I0502 00:06:46.692234 139645796558656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_sam/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0502 00:06:47.331731 139645796558656 submission_runner.py:578] Tuning trial 1/1
I0502 00:06:47.332823 139645796558656 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0502 00:06:47.336866 139645796558656 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008171236841008067, 'train/loss': 6.910836696624756, 'validation/accuracy': 0.0009399999980814755, 'validation/loss': 6.911313533782959, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.911646366119385, 'test/num_examples': 10000, 'score': 59.72118091583252, 'total_duration': 100.65605092048645, 'accumulated_submission_time': 59.72118091583252, 'accumulated_eval_time': 40.9347198009491, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (815, {'train/accuracy': 0.016561701893806458, 'train/loss': 6.4454474449157715, 'validation/accuracy': 0.015960000455379486, 'validation/loss': 6.466333389282227, 'validation/num_examples': 50000, 'test/accuracy': 0.011900000274181366, 'test/loss': 6.553193092346191, 'test/num_examples': 10000, 'score': 569.7143239974976, 'total_duration': 627.6224095821381, 'accumulated_submission_time': 569.7143239974976, 'accumulated_eval_time': 57.87165379524231, 'accumulated_logging_time': 0.025225400924682617, 'global_step': 815, 'preemption_count': 0}), (1631, {'train/accuracy': 0.01911272294819355, 'train/loss': 6.375968933105469, 'validation/accuracy': 0.018059998750686646, 'validation/loss': 6.411831855773926, 'validation/num_examples': 50000, 'test/accuracy': 0.011900000274181366, 'test/loss': 6.503609657287598, 'test/num_examples': 10000, 'score': 1079.903687953949, 'total_duration': 1154.6376321315765, 'accumulated_submission_time': 1079.903687953949, 'accumulated_eval_time': 74.66981720924377, 'accumulated_logging_time': 0.041953086853027344, 'global_step': 1631, 'preemption_count': 0}), (2449, {'train/accuracy': 0.018235810101032257, 'train/loss': 6.346316337585449, 'validation/accuracy': 0.0177800003439188, 'validation/loss': 6.3615031242370605, 'validation/num_examples': 50000, 'test/accuracy': 0.012100000865757465, 'test/loss': 6.44769287109375, 'test/num_examples': 10000, 'score': 1590.4593737125397, 'total_duration': 1682.2326290607452, 'accumulated_submission_time': 1590.4593737125397, 'accumulated_eval_time': 91.68068218231201, 'accumulated_logging_time': 0.059207916259765625, 'global_step': 2449, 'preemption_count': 0}), (3265, {'train/accuracy': 0.022341357544064522, 'train/loss': 6.2720255851745605, 'validation/accuracy': 0.021299999207258224, 'validation/loss': 6.318572998046875, 'validation/num_examples': 50000, 'test/accuracy': 0.016300000250339508, 'test/loss': 6.418375492095947, 'test/num_examples': 10000, 'score': 2100.5307145118713, 'total_duration': 2209.2952592372894, 'accumulated_submission_time': 2100.5307145118713, 'accumulated_eval_time': 108.64215660095215, 'accumulated_logging_time': 0.0779719352722168, 'global_step': 3265, 'preemption_count': 0}), (4082, {'train/accuracy': 0.02746332809329033, 'train/loss': 6.220250606536865, 'validation/accuracy': 0.025340000167489052, 'validation/loss': 6.279265403747559, 'validation/num_examples': 50000, 'test/accuracy': 0.019100001081824303, 'test/loss': 6.370682716369629, 'test/num_examples': 10000, 'score': 2611.0350363254547, 'total_duration': 2736.5075266361237, 'accumulated_submission_time': 2611.0350363254547, 'accumulated_eval_time': 125.32316374778748, 'accumulated_logging_time': 0.0938117504119873, 'global_step': 4082, 'preemption_count': 0}), (4899, {'train/accuracy': 0.026267537847161293, 'train/loss': 6.178737163543701, 'validation/accuracy': 0.024720000103116035, 'validation/loss': 6.217923164367676, 'validation/num_examples': 50000, 'test/accuracy': 0.018900001421570778, 'test/loss': 6.324992656707764, 'test/num_examples': 10000, 'score': 3121.213874101639, 'total_duration': 3264.2167263031006, 'accumulated_submission_time': 3121.213874101639, 'accumulated_eval_time': 142.8262391090393, 'accumulated_logging_time': 0.11025285720825195, 'global_step': 4899, 'preemption_count': 0}), (5717, {'train/accuracy': 0.03435905650258064, 'train/loss': 6.051488399505615, 'validation/accuracy': 0.03175999969244003, 'validation/loss': 6.1136698722839355, 'validation/num_examples': 50000, 'test/accuracy': 0.022200001403689384, 'test/loss': 6.241243839263916, 'test/num_examples': 10000, 'score': 3631.801424741745, 'total_duration': 3793.4838190078735, 'accumulated_submission_time': 3631.801424741745, 'accumulated_eval_time': 161.47521591186523, 'accumulated_logging_time': 0.12981343269348145, 'global_step': 5717, 'preemption_count': 0}), (6533, {'train/accuracy': 0.036192599684000015, 'train/loss': 6.022007942199707, 'validation/accuracy': 0.035179998725652695, 'validation/loss': 6.064110279083252, 'validation/num_examples': 50000, 'test/accuracy': 0.027500001713633537, 'test/loss': 6.187184810638428, 'test/num_examples': 10000, 'score': 4141.982597351074, 'total_duration': 4322.619256258011, 'accumulated_submission_time': 4141.982597351074, 'accumulated_eval_time': 180.3962128162384, 'accumulated_logging_time': 0.1519148349761963, 'global_step': 6533, 'preemption_count': 0}), (7348, {'train/accuracy': 0.026008449494838715, 'train/loss': 6.2319016456604, 'validation/accuracy': 0.024059999734163284, 'validation/loss': 6.267568588256836, 'validation/num_examples': 50000, 'test/accuracy': 0.01940000057220459, 'test/loss': 6.355428218841553, 'test/num_examples': 10000, 'score': 4652.193439245224, 'total_duration': 4852.154700040817, 'accumulated_submission_time': 4652.193439245224, 'accumulated_eval_time': 199.6907036304474, 'accumulated_logging_time': 0.17082643508911133, 'global_step': 7348, 'preemption_count': 0}), (8163, {'train/accuracy': 0.030094068497419357, 'train/loss': 6.146125316619873, 'validation/accuracy': 0.02757999859750271, 'validation/loss': 6.1950531005859375, 'validation/num_examples': 50000, 'test/accuracy': 0.02120000123977661, 'test/loss': 6.300453186035156, 'test/num_examples': 10000, 'score': 5162.478399515152, 'total_duration': 5381.629621744156, 'accumulated_submission_time': 5162.478399515152, 'accumulated_eval_time': 218.8527431488037, 'accumulated_logging_time': 0.18749618530273438, 'global_step': 8163, 'preemption_count': 0}), (8980, {'train/accuracy': 0.02048788219690323, 'train/loss': 6.259620189666748, 'validation/accuracy': 0.020519999787211418, 'validation/loss': 6.289515495300293, 'validation/num_examples': 50000, 'test/accuracy': 0.01590000092983246, 'test/loss': 6.416529178619385, 'test/num_examples': 10000, 'score': 5672.95721578598, 'total_duration': 5911.334886074066, 'accumulated_submission_time': 5672.95721578598, 'accumulated_eval_time': 238.04933142662048, 'accumulated_logging_time': 0.20601701736450195, 'global_step': 8980, 'preemption_count': 0}), (9795, {'train/accuracy': 0.03266501799225807, 'train/loss': 6.091101169586182, 'validation/accuracy': 0.029980000108480453, 'validation/loss': 6.145623683929443, 'validation/num_examples': 50000, 'test/accuracy': 0.02200000174343586, 'test/loss': 6.273143291473389, 'test/num_examples': 10000, 'score': 6183.557404994965, 'total_duration': 6441.099747657776, 'accumulated_submission_time': 6183.557404994965, 'accumulated_eval_time': 257.18239569664, 'accumulated_logging_time': 0.2248680591583252, 'global_step': 9795, 'preemption_count': 0}), (10609, {'train/accuracy': 0.04239078238606453, 'train/loss': 5.946173191070557, 'validation/accuracy': 0.04123999923467636, 'validation/loss': 5.9871721267700195, 'validation/num_examples': 50000, 'test/accuracy': 0.029500002041459084, 'test/loss': 6.130752086639404, 'test/num_examples': 10000, 'score': 6693.812987089157, 'total_duration': 6970.747509241104, 'accumulated_submission_time': 6693.812987089157, 'accumulated_eval_time': 276.54565382003784, 'accumulated_logging_time': 0.24275755882263184, 'global_step': 10609, 'preemption_count': 0}), (11423, {'train/accuracy': 0.047492824494838715, 'train/loss': 5.873133182525635, 'validation/accuracy': 0.045180000364780426, 'validation/loss': 5.923137187957764, 'validation/num_examples': 50000, 'test/accuracy': 0.03450000286102295, 'test/loss': 6.091293811798096, 'test/num_examples': 10000, 'score': 7204.21644282341, 'total_duration': 7500.318992137909, 'accumulated_submission_time': 7204.21644282341, 'accumulated_eval_time': 295.68339252471924, 'accumulated_logging_time': 0.26197099685668945, 'global_step': 11423, 'preemption_count': 0}), (12237, {'train/accuracy': 0.051399074494838715, 'train/loss': 5.815375328063965, 'validation/accuracy': 0.04939999803900719, 'validation/loss': 5.879857540130615, 'validation/num_examples': 50000, 'test/accuracy': 0.03660000115633011, 'test/loss': 6.050558567047119, 'test/num_examples': 10000, 'score': 7714.727222919464, 'total_duration': 8030.376610279083, 'accumulated_submission_time': 7714.727222919464, 'accumulated_eval_time': 315.1973867416382, 'accumulated_logging_time': 0.2837400436401367, 'global_step': 12237, 'preemption_count': 0}), (13049, {'train/accuracy': 0.05066166818141937, 'train/loss': 5.805259704589844, 'validation/accuracy': 0.04848000034689903, 'validation/loss': 5.865444183349609, 'validation/num_examples': 50000, 'test/accuracy': 0.035600002855062485, 'test/loss': 6.052343845367432, 'test/num_examples': 10000, 'score': 8224.779244661331, 'total_duration': 8560.011388540268, 'accumulated_submission_time': 8224.779244661331, 'accumulated_eval_time': 334.7483389377594, 'accumulated_logging_time': 0.3032538890838623, 'global_step': 13049, 'preemption_count': 0}), (13861, {'train/accuracy': 0.05159837380051613, 'train/loss': 5.787433624267578, 'validation/accuracy': 0.04843999817967415, 'validation/loss': 5.840727806091309, 'validation/num_examples': 50000, 'test/accuracy': 0.035600002855062485, 'test/loss': 6.047037601470947, 'test/num_examples': 10000, 'score': 8734.797052145004, 'total_duration': 9089.966566324234, 'accumulated_submission_time': 8734.797052145004, 'accumulated_eval_time': 354.6573369503021, 'accumulated_logging_time': 0.32082056999206543, 'global_step': 13861, 'preemption_count': 0}), (14673, {'train/accuracy': 0.06742267310619354, 'train/loss': 5.633881568908691, 'validation/accuracy': 0.05929999798536301, 'validation/loss': 5.739711284637451, 'validation/num_examples': 50000, 'test/accuracy': 0.04200000315904617, 'test/loss': 5.937422275543213, 'test/num_examples': 10000, 'score': 9245.36721611023, 'total_duration': 9620.635943889618, 'accumulated_submission_time': 9245.36721611023, 'accumulated_eval_time': 374.7252116203308, 'accumulated_logging_time': 0.3410913944244385, 'global_step': 14673, 'preemption_count': 0}), (15486, {'train/accuracy': 0.06184231489896774, 'train/loss': 5.647058486938477, 'validation/accuracy': 0.0619799979031086, 'validation/loss': 5.708606243133545, 'validation/num_examples': 50000, 'test/accuracy': 0.04480000212788582, 'test/loss': 5.9155473709106445, 'test/num_examples': 10000, 'score': 9755.797398328781, 'total_duration': 10151.300331115723, 'accumulated_submission_time': 9755.797398328781, 'accumulated_eval_time': 394.93059182167053, 'accumulated_logging_time': 0.3590128421783447, 'global_step': 15486, 'preemption_count': 0}), (16299, {'train/accuracy': 0.06399473547935486, 'train/loss': 5.677242279052734, 'validation/accuracy': 0.059579998254776, 'validation/loss': 5.753046035766602, 'validation/num_examples': 50000, 'test/accuracy': 0.044200003147125244, 'test/loss': 5.9595746994018555, 'test/num_examples': 10000, 'score': 10266.423383235931, 'total_duration': 10682.365035533905, 'accumulated_submission_time': 10266.423383235931, 'accumulated_eval_time': 415.3382611274719, 'accumulated_logging_time': 0.3791360855102539, 'global_step': 16299, 'preemption_count': 0}), (17112, {'train/accuracy': 0.0676618292927742, 'train/loss': 5.64604377746582, 'validation/accuracy': 0.0640999972820282, 'validation/loss': 5.711448669433594, 'validation/num_examples': 50000, 'test/accuracy': 0.046400003135204315, 'test/loss': 5.91102409362793, 'test/num_examples': 10000, 'score': 10776.682438850403, 'total_duration': 11212.599824666977, 'accumulated_submission_time': 10776.682438850403, 'accumulated_eval_time': 435.28177189826965, 'accumulated_logging_time': 0.40015125274658203, 'global_step': 17112, 'preemption_count': 0}), (17926, {'train/accuracy': 0.07144849747419357, 'train/loss': 5.579751014709473, 'validation/accuracy': 0.0688600018620491, 'validation/loss': 5.633138656616211, 'validation/num_examples': 50000, 'test/accuracy': 0.04690000042319298, 'test/loss': 5.853484630584717, 'test/num_examples': 10000, 'score': 11286.977898359299, 'total_duration': 11742.983834266663, 'accumulated_submission_time': 11286.977898359299, 'accumulated_eval_time': 455.34172105789185, 'accumulated_logging_time': 0.4177122116088867, 'global_step': 17926, 'preemption_count': 0}), (18740, {'train/accuracy': 0.06959502398967743, 'train/loss': 5.549996376037598, 'validation/accuracy': 0.06453999876976013, 'validation/loss': 5.640780925750732, 'validation/num_examples': 50000, 'test/accuracy': 0.05190000310540199, 'test/loss': 5.866814136505127, 'test/num_examples': 10000, 'score': 11797.41777229309, 'total_duration': 12273.737427949905, 'accumulated_submission_time': 11797.41777229309, 'accumulated_eval_time': 475.6252841949463, 'accumulated_logging_time': 0.4367046356201172, 'global_step': 18740, 'preemption_count': 0}), (19552, {'train/accuracy': 0.07553412020206451, 'train/loss': 5.532441139221191, 'validation/accuracy': 0.07242000102996826, 'validation/loss': 5.6012749671936035, 'validation/num_examples': 50000, 'test/accuracy': 0.04920000210404396, 'test/loss': 5.822516441345215, 'test/num_examples': 10000, 'score': 12307.595932006836, 'total_duration': 12804.126222610474, 'accumulated_submission_time': 12307.595932006836, 'accumulated_eval_time': 495.8029251098633, 'accumulated_logging_time': 0.4573400020599365, 'global_step': 19552, 'preemption_count': 0}), (20365, {'train/accuracy': 0.07278379797935486, 'train/loss': 5.545177936553955, 'validation/accuracy': 0.07001999765634537, 'validation/loss': 5.614750385284424, 'validation/num_examples': 50000, 'test/accuracy': 0.04840000346302986, 'test/loss': 5.8477373123168945, 'test/num_examples': 10000, 'score': 12818.099341154099, 'total_duration': 13333.548988580704, 'accumulated_submission_time': 12818.099341154099, 'accumulated_eval_time': 514.6930129528046, 'accumulated_logging_time': 0.47579407691955566, 'global_step': 20365, 'preemption_count': 0}), (21178, {'train/accuracy': 0.06800063699483871, 'train/loss': 5.6065192222595215, 'validation/accuracy': 0.0634399950504303, 'validation/loss': 5.683626174926758, 'validation/num_examples': 50000, 'test/accuracy': 0.04570000246167183, 'test/loss': 5.894370079040527, 'test/num_examples': 10000, 'score': 13328.193813800812, 'total_duration': 13862.290736436844, 'accumulated_submission_time': 13328.193813800812, 'accumulated_eval_time': 533.3104994297028, 'accumulated_logging_time': 0.4945354461669922, 'global_step': 21178, 'preemption_count': 0}), (21991, {'train/accuracy': 0.07838408648967743, 'train/loss': 5.525690078735352, 'validation/accuracy': 0.07457999885082245, 'validation/loss': 5.589944362640381, 'validation/num_examples': 50000, 'test/accuracy': 0.05250000208616257, 'test/loss': 5.810934543609619, 'test/num_examples': 10000, 'score': 13838.644011497498, 'total_duration': 14391.274812221527, 'accumulated_submission_time': 13838.644011497498, 'accumulated_eval_time': 551.8147506713867, 'accumulated_logging_time': 0.5130834579467773, 'global_step': 21991, 'preemption_count': 0}), (22804, {'train/accuracy': 0.07403937727212906, 'train/loss': 5.535967826843262, 'validation/accuracy': 0.06960000097751617, 'validation/loss': 5.6228508949279785, 'validation/num_examples': 50000, 'test/accuracy': 0.05130000412464142, 'test/loss': 5.8489251136779785, 'test/num_examples': 10000, 'score': 14349.10959482193, 'total_duration': 14920.134546995163, 'accumulated_submission_time': 14349.10959482193, 'accumulated_eval_time': 570.1769309043884, 'accumulated_logging_time': 0.5340180397033691, 'global_step': 22804, 'preemption_count': 0}), (23617, {'train/accuracy': 0.0729631707072258, 'train/loss': 5.556201934814453, 'validation/accuracy': 0.06796000152826309, 'validation/loss': 5.625189304351807, 'validation/num_examples': 50000, 'test/accuracy': 0.052000001072883606, 'test/loss': 5.8394551277160645, 'test/num_examples': 10000, 'score': 14859.45750284195, 'total_duration': 15448.5455057621, 'accumulated_submission_time': 14859.45750284195, 'accumulated_eval_time': 588.2102425098419, 'accumulated_logging_time': 0.5527200698852539, 'global_step': 23617, 'preemption_count': 0}), (24430, {'train/accuracy': 0.07667011767625809, 'train/loss': 5.523855686187744, 'validation/accuracy': 0.0708799958229065, 'validation/loss': 5.593801021575928, 'validation/num_examples': 50000, 'test/accuracy': 0.05380000174045563, 'test/loss': 5.814946174621582, 'test/num_examples': 10000, 'score': 15369.69491314888, 'total_duration': 15976.883726358414, 'accumulated_submission_time': 15369.69491314888, 'accumulated_eval_time': 606.2816100120544, 'accumulated_logging_time': 0.5710222721099854, 'global_step': 24430, 'preemption_count': 0}), (25243, {'train/accuracy': 0.08055643737316132, 'train/loss': 5.4853105545043945, 'validation/accuracy': 0.07745999842882156, 'validation/loss': 5.541722297668457, 'validation/num_examples': 50000, 'test/accuracy': 0.055900003761053085, 'test/loss': 5.762324333190918, 'test/num_examples': 10000, 'score': 15879.8308198452, 'total_duration': 16505.03166937828, 'accumulated_submission_time': 15879.8308198452, 'accumulated_eval_time': 624.2646837234497, 'accumulated_logging_time': 0.5888500213623047, 'global_step': 25243, 'preemption_count': 0}), (26056, {'train/accuracy': 0.0772879421710968, 'train/loss': 5.502620697021484, 'validation/accuracy': 0.07155999541282654, 'validation/loss': 5.5831298828125, 'validation/num_examples': 50000, 'test/accuracy': 0.05490000173449516, 'test/loss': 5.802459239959717, 'test/num_examples': 10000, 'score': 16390.0584628582, 'total_duration': 17033.414568662643, 'accumulated_submission_time': 16390.0584628582, 'accumulated_eval_time': 642.3908762931824, 'accumulated_logging_time': 0.6069700717926025, 'global_step': 26056, 'preemption_count': 0}), (26869, {'train/accuracy': 0.08103475719690323, 'train/loss': 5.472643852233887, 'validation/accuracy': 0.07846000045537949, 'validation/loss': 5.553103446960449, 'validation/num_examples': 50000, 'test/accuracy': 0.056300003081560135, 'test/loss': 5.769197463989258, 'test/num_examples': 10000, 'score': 16900.300141334534, 'total_duration': 17561.640553712845, 'accumulated_submission_time': 16900.300141334534, 'accumulated_eval_time': 660.346165895462, 'accumulated_logging_time': 0.6251132488250732, 'global_step': 26869, 'preemption_count': 0}), (27682, {'train/accuracy': 0.08169244229793549, 'train/loss': 5.484259128570557, 'validation/accuracy': 0.07558000087738037, 'validation/loss': 5.545289993286133, 'validation/num_examples': 50000, 'test/accuracy': 0.05560000240802765, 'test/loss': 5.772364616394043, 'test/num_examples': 10000, 'score': 17410.808962583542, 'total_duration': 18090.4747672081, 'accumulated_submission_time': 17410.808962583542, 'accumulated_eval_time': 678.6406836509705, 'accumulated_logging_time': 0.6450293064117432, 'global_step': 27682, 'preemption_count': 0}), (28000, {'train/accuracy': 0.0849609375, 'train/loss': 5.440621852874756, 'validation/accuracy': 0.07931999862194061, 'validation/loss': 5.533401012420654, 'validation/num_examples': 50000, 'test/accuracy': 0.05610000342130661, 'test/loss': 5.76111364364624, 'test/num_examples': 10000, 'score': 17610.04269552231, 'total_duration': 18307.78733229637, 'accumulated_submission_time': 17610.04269552231, 'accumulated_eval_time': 696.6943175792694, 'accumulated_logging_time': 0.6657488346099854, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0502 00:06:47.337002 139645796558656 submission_runner.py:581] Timing: 17610.04269552231
I0502 00:06:47.337059 139645796558656 submission_runner.py:582] ====================
I0502 00:06:47.337192 139645796558656 submission_runner.py:645] Final imagenet_resnet score: 17610.04269552231
