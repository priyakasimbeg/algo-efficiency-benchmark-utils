python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_jax_05-02-2023-02-44-17.log
I0502 02:44:37.639891 139790038521664 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_adafactor/ogbg_jax.
I0502 02:44:37.713828 139790038521664 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 02:44:38.541826 139790038521664 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0502 02:44:38.542834 139790038521664 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 02:44:38.547821 139790038521664 submission_runner.py:538] Using RNG seed 2799659108
I0502 02:44:41.166524 139790038521664 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 02:44:41.166753 139790038521664 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_adafactor/ogbg_jax/trial_1.
I0502 02:44:41.166971 139790038521664 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_adafactor/ogbg_jax/trial_1/hparams.json.
I0502 02:44:41.309491 139790038521664 submission_runner.py:241] Initializing dataset.
I0502 02:44:41.580247 139790038521664 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:44:41.585676 139790038521664 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 02:44:41.835464 139790038521664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:44:41.891437 139790038521664 submission_runner.py:248] Initializing model.
I0502 02:44:49.414749 139790038521664 submission_runner.py:258] Initializing optimizer.
I0502 02:44:50.600385 139790038521664 submission_runner.py:265] Initializing metrics bundle.
I0502 02:44:50.600569 139790038521664 submission_runner.py:282] Initializing checkpoint and logger.
I0502 02:44:50.603590 139790038521664 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_adafactor/ogbg_jax/trial_1 with prefix checkpoint_
I0502 02:44:50.603840 139790038521664 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0502 02:44:50.603916 139790038521664 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0502 02:44:51.457329 139790038521664 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_adafactor/ogbg_jax/trial_1/meta_data_0.json.
I0502 02:44:51.458350 139790038521664 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_adafactor/ogbg_jax/trial_1/flags_0.json.
I0502 02:44:51.463772 139790038521664 submission_runner.py:318] Starting training loop.
I0502 02:45:27.972953 139613884643072 logging_writer.py:48] [0] global_step=0, grad_norm=2.699615716934204, loss=0.7465522289276123
I0502 02:45:27.987195 139790038521664 spec.py:298] Evaluating on the training split.
I0502 02:45:27.995712 139790038521664 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:45:27.999754 139790038521664 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 02:45:28.059450 139790038521664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:47:07.778310 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 02:47:07.781669 139790038521664 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:47:07.785751 139790038521664 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 02:47:07.840264 139790038521664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:48:17.310319 139790038521664 spec.py:326] Evaluating on the test split.
I0502 02:48:17.313316 139790038521664 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:48:17.317652 139790038521664 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 02:48:17.371994 139790038521664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 02:49:26.008243 139790038521664 submission_runner.py:415] Time since start: 274.54s, 	Step: 1, 	{'train/accuracy': 0.4566192924976349, 'train/loss': 0.7459662556648254, 'train/mean_average_precision': 0.024366261906756802, 'validation/accuracy': 0.46244072914123535, 'validation/loss': 0.7437726259231567, 'validation/mean_average_precision': 0.02867388806536403, 'validation/num_examples': 43793, 'test/accuracy': 0.4633033871650696, 'test/loss': 0.7438121438026428, 'test/mean_average_precision': 0.029861182216749025, 'test/num_examples': 43793, 'score': 36.52326202392578, 'total_duration': 274.5444288253784, 'accumulated_submission_time': 36.52326202392578, 'accumulated_eval_time': 238.02101707458496, 'accumulated_logging_time': 0}
I0502 02:49:26.024557 139604069521152 logging_writer.py:48] [1] accumulated_eval_time=238.021017, accumulated_logging_time=0, accumulated_submission_time=36.523262, global_step=1, preemption_count=0, score=36.523262, test/accuracy=0.463303, test/loss=0.743812, test/mean_average_precision=0.029861, test/num_examples=43793, total_duration=274.544429, train/accuracy=0.456619, train/loss=0.745966, train/mean_average_precision=0.024366, validation/accuracy=0.462441, validation/loss=0.743773, validation/mean_average_precision=0.028674, validation/num_examples=43793
I0502 02:49:52.483802 139604077913856 logging_writer.py:48] [100] global_step=100, grad_norm=0.47705981135368347, loss=0.4118911027908325
I0502 02:50:19.188831 139604069521152 logging_writer.py:48] [200] global_step=200, grad_norm=0.21315990388393402, loss=0.19338001310825348
I0502 02:50:45.520985 139604077913856 logging_writer.py:48] [300] global_step=300, grad_norm=0.07758500427007675, loss=0.08650543540716171
I0502 02:51:11.838820 139604069521152 logging_writer.py:48] [400] global_step=400, grad_norm=0.044133417308330536, loss=0.05765695497393608
I0502 02:51:38.376763 139604077913856 logging_writer.py:48] [500] global_step=500, grad_norm=0.06861751526594162, loss=0.05806674063205719
I0502 02:52:05.035097 139604069521152 logging_writer.py:48] [600] global_step=600, grad_norm=0.042816467583179474, loss=0.053848255425691605
I0502 02:52:31.673336 139604077913856 logging_writer.py:48] [700] global_step=700, grad_norm=0.10853323340415955, loss=0.04518282413482666
I0502 02:52:57.933570 139604069521152 logging_writer.py:48] [800] global_step=800, grad_norm=0.28806331753730774, loss=0.05477992072701454
I0502 02:53:24.222054 139604077913856 logging_writer.py:48] [900] global_step=900, grad_norm=0.06182476505637169, loss=0.05146941542625427
I0502 02:53:26.075133 139790038521664 spec.py:298] Evaluating on the training split.
I0502 02:54:45.657645 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 02:54:48.261124 139790038521664 spec.py:326] Evaluating on the test split.
I0502 02:54:50.777145 139790038521664 submission_runner.py:415] Time since start: 599.31s, 	Step: 908, 	{'train/accuracy': 0.986880362033844, 'train/loss': 0.050047192722558975, 'train/mean_average_precision': 0.06197843726815895, 'validation/accuracy': 0.9841443300247192, 'validation/loss': 0.06001105532050133, 'validation/mean_average_precision': 0.06055410248735968, 'validation/num_examples': 43793, 'test/accuracy': 0.9831635355949402, 'test/loss': 0.0633639544248581, 'test/mean_average_precision': 0.06216233313845479, 'test/num_examples': 43793, 'score': 276.55537724494934, 'total_duration': 599.313313961029, 'accumulated_submission_time': 276.55537724494934, 'accumulated_eval_time': 322.7229914665222, 'accumulated_logging_time': 0.02710580825805664}
I0502 02:54:50.785523 139604069521152 logging_writer.py:48] [908] accumulated_eval_time=322.722991, accumulated_logging_time=0.027106, accumulated_submission_time=276.555377, global_step=908, preemption_count=0, score=276.555377, test/accuracy=0.983164, test/loss=0.063364, test/mean_average_precision=0.062162, test/num_examples=43793, total_duration=599.313314, train/accuracy=0.986880, train/loss=0.050047, train/mean_average_precision=0.061978, validation/accuracy=0.984144, validation/loss=0.060011, validation/mean_average_precision=0.060554, validation/num_examples=43793
I0502 02:55:15.616219 139604077913856 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.022880736738443375, loss=0.05070185661315918
I0502 02:55:42.137090 139604069521152 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.06045491248369217, loss=0.053601499646902084
I0502 02:56:08.760547 139604077913856 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0687527135014534, loss=0.05259740352630615
I0502 02:56:35.463539 139604069521152 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.04422224313020706, loss=0.05100160464644432
I0502 02:57:02.733543 139604077913856 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.015303835272789001, loss=0.0480898879468441
I0502 02:57:29.755546 139604069521152 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.017334047704935074, loss=0.04982345551252365
I0502 02:57:56.599462 139604077913856 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.08733659982681274, loss=0.04719887673854828
I0502 02:58:23.497950 139604069521152 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.024925753474235535, loss=0.05192619934678078
I0502 02:58:50.150273 139604077913856 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.025568068027496338, loss=0.04855135455727577
I0502 02:58:50.952660 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:00:11.641908 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:00:14.272560 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:00:16.851685 139790038521664 submission_runner.py:415] Time since start: 925.39s, 	Step: 1804, 	{'train/accuracy': 0.9869723320007324, 'train/loss': 0.047540564090013504, 'train/mean_average_precision': 0.10414538719542529, 'validation/accuracy': 0.9843765497207642, 'validation/loss': 0.057258326560258865, 'validation/mean_average_precision': 0.10161576526123806, 'validation/num_examples': 43793, 'test/accuracy': 0.98338383436203, 'test/loss': 0.060542140156030655, 'test/mean_average_precision': 0.10222338116537917, 'test/num_examples': 43793, 'score': 516.704488992691, 'total_duration': 925.3878529071808, 'accumulated_submission_time': 516.704488992691, 'accumulated_eval_time': 408.6219792366028, 'accumulated_logging_time': 0.04600214958190918}
I0502 03:00:16.860139 139604069521152 logging_writer.py:48] [1804] accumulated_eval_time=408.621979, accumulated_logging_time=0.046002, accumulated_submission_time=516.704489, global_step=1804, preemption_count=0, score=516.704489, test/accuracy=0.983384, test/loss=0.060542, test/mean_average_precision=0.102223, test/num_examples=43793, total_duration=925.387853, train/accuracy=0.986972, train/loss=0.047541, train/mean_average_precision=0.104145, validation/accuracy=0.984377, validation/loss=0.057258, validation/mean_average_precision=0.101616, validation/num_examples=43793
I0502 03:00:42.615488 139604077913856 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.06400395929813385, loss=0.048162225633859634
I0502 03:01:09.150163 139604069521152 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.05309944972395897, loss=0.04939659684896469
I0502 03:01:35.756887 139604077913856 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.04650891572237015, loss=0.049590833485126495
I0502 03:02:02.613357 139604069521152 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.03445008024573326, loss=0.04432466998696327
I0502 03:02:29.372455 139604077913856 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.022502636536955833, loss=0.0499817319214344
I0502 03:02:55.940331 139604069521152 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.022360481321811676, loss=0.04811398312449455
I0502 03:03:22.773455 139604077913856 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.010682851076126099, loss=0.04316078498959541
I0502 03:03:49.524653 139604069521152 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.02709190919995308, loss=0.051082294434309006
I0502 03:04:16.338937 139604077913856 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.01661580614745617, loss=0.04973525553941727
I0502 03:04:16.884765 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:05:38.200513 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:05:40.790226 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:05:43.314336 139790038521664 submission_runner.py:415] Time since start: 1251.85s, 	Step: 2703, 	{'train/accuracy': 0.9875162839889526, 'train/loss': 0.044647324830293655, 'train/mean_average_precision': 0.14256530691652586, 'validation/accuracy': 0.9848575592041016, 'validation/loss': 0.05388014391064644, 'validation/mean_average_precision': 0.13680043123030747, 'validation/num_examples': 43793, 'test/accuracy': 0.9838876128196716, 'test/loss': 0.05693531036376953, 'test/mean_average_precision': 0.1346309108155529, 'test/num_examples': 43793, 'score': 756.7114279270172, 'total_duration': 1251.8505067825317, 'accumulated_submission_time': 756.7114279270172, 'accumulated_eval_time': 495.0515127182007, 'accumulated_logging_time': 0.06471538543701172}
I0502 03:05:43.322808 139604069521152 logging_writer.py:48] [2703] accumulated_eval_time=495.051513, accumulated_logging_time=0.064715, accumulated_submission_time=756.711428, global_step=2703, preemption_count=0, score=756.711428, test/accuracy=0.983888, test/loss=0.056935, test/mean_average_precision=0.134631, test/num_examples=43793, total_duration=1251.850507, train/accuracy=0.987516, train/loss=0.044647, train/mean_average_precision=0.142565, validation/accuracy=0.984858, validation/loss=0.053880, validation/mean_average_precision=0.136800, validation/num_examples=43793
I0502 03:06:09.944372 139604077913856 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.023836428299546242, loss=0.04283452779054642
I0502 03:06:36.690274 139604069521152 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.023503534495830536, loss=0.0480295866727829
I0502 03:07:03.465578 139604077913856 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.027027275413274765, loss=0.048852503299713135
I0502 03:07:30.672466 139604069521152 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.010958297178149223, loss=0.04447125643491745
I0502 03:07:57.561265 139604077913856 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.025327524170279503, loss=0.04565437510609627
I0502 03:08:24.405370 139604069521152 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.011711821891367435, loss=0.04370986297726631
I0502 03:08:51.220533 139604077913856 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.01090144645422697, loss=0.051954030990600586
I0502 03:09:17.876776 139604069521152 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.014577312394976616, loss=0.04314393922686577
I0502 03:09:43.449633 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:11:03.695547 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:11:06.302043 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:11:08.854961 139790038521664 submission_runner.py:415] Time since start: 1577.39s, 	Step: 3597, 	{'train/accuracy': 0.9878696799278259, 'train/loss': 0.04201818257570267, 'train/mean_average_precision': 0.17717732002441297, 'validation/accuracy': 0.9850739240646362, 'validation/loss': 0.051490142941474915, 'validation/mean_average_precision': 0.15314752688305638, 'validation/num_examples': 43793, 'test/accuracy': 0.9841015338897705, 'test/loss': 0.05426695570349693, 'test/mean_average_precision': 0.15254572958855372, 'test/num_examples': 43793, 'score': 996.8215925693512, 'total_duration': 1577.391093492508, 'accumulated_submission_time': 996.8215925693512, 'accumulated_eval_time': 580.4567875862122, 'accumulated_logging_time': 0.08233237266540527}
I0502 03:11:08.863663 139604077913856 logging_writer.py:48] [3597] accumulated_eval_time=580.456788, accumulated_logging_time=0.082332, accumulated_submission_time=996.821593, global_step=3597, preemption_count=0, score=996.821593, test/accuracy=0.984102, test/loss=0.054267, test/mean_average_precision=0.152546, test/num_examples=43793, total_duration=1577.391093, train/accuracy=0.987870, train/loss=0.042018, train/mean_average_precision=0.177177, validation/accuracy=0.985074, validation/loss=0.051490, validation/mean_average_precision=0.153148, validation/num_examples=43793
I0502 03:11:09.943186 139604069521152 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.008962828665971756, loss=0.04055815190076828
I0502 03:11:36.700302 139604077913856 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.030463220551609993, loss=0.04300042241811752
I0502 03:12:03.358156 139604069521152 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.010138126090168953, loss=0.04831431061029434
I0502 03:12:30.245241 139604077913856 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.009816747158765793, loss=0.03765147179365158
I0502 03:12:57.151328 139604069521152 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.01229932438582182, loss=0.04091095179319382
I0502 03:13:23.928515 139604077913856 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.01569201610982418, loss=0.04567302390933037
I0502 03:13:50.798228 139604069521152 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.026766125112771988, loss=0.040754783898591995
I0502 03:14:17.634531 139604077913856 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.012355504557490349, loss=0.037883806973695755
I0502 03:14:44.881125 139604069521152 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.013634744100272655, loss=0.04484483599662781
I0502 03:15:08.975118 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:16:28.484231 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:16:31.138163 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:16:33.668666 139790038521664 submission_runner.py:415] Time since start: 1902.20s, 	Step: 4490, 	{'train/accuracy': 0.988457977771759, 'train/loss': 0.03971616551280022, 'train/mean_average_precision': 0.21717630632920235, 'validation/accuracy': 0.9855618476867676, 'validation/loss': 0.04946672171354294, 'validation/mean_average_precision': 0.17869709392573135, 'validation/num_examples': 43793, 'test/accuracy': 0.9846322536468506, 'test/loss': 0.05228358134627342, 'test/mean_average_precision': 0.1784636473479603, 'test/num_examples': 43793, 'score': 1236.9144871234894, 'total_duration': 1902.2048342227936, 'accumulated_submission_time': 1236.9144871234894, 'accumulated_eval_time': 665.1503036022186, 'accumulated_logging_time': 0.10193276405334473}
I0502 03:16:33.677745 139604077913856 logging_writer.py:48] [4490] accumulated_eval_time=665.150304, accumulated_logging_time=0.101933, accumulated_submission_time=1236.914487, global_step=4490, preemption_count=0, score=1236.914487, test/accuracy=0.984632, test/loss=0.052284, test/mean_average_precision=0.178464, test/num_examples=43793, total_duration=1902.204834, train/accuracy=0.988458, train/loss=0.039716, train/mean_average_precision=0.217176, validation/accuracy=0.985562, validation/loss=0.049467, validation/mean_average_precision=0.178697, validation/num_examples=43793
I0502 03:16:36.683177 139604069521152 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.011466518975794315, loss=0.041317444294691086
I0502 03:17:03.752905 139604077913856 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.01047255378216505, loss=0.03890139237046242
I0502 03:17:30.622514 139604069521152 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.010554494336247444, loss=0.04022694006562233
I0502 03:17:57.774042 139604077913856 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.009403746575117111, loss=0.04569266736507416
I0502 03:18:24.829046 139604069521152 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.01054434385150671, loss=0.03872714191675186
I0502 03:18:51.850032 139604077913856 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.010864541865885258, loss=0.04063379392027855
I0502 03:19:18.907832 139604069521152 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0073402151465415955, loss=0.039836086332798004
I0502 03:19:45.875612 139604077913856 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.007430803496390581, loss=0.03989763185381889
I0502 03:20:12.855346 139604069521152 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.007845873013138771, loss=0.04109165817499161
I0502 03:20:33.940589 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:21:54.598536 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:21:57.215768 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:21:59.750261 139790038521664 submission_runner.py:415] Time since start: 2228.29s, 	Step: 5379, 	{'train/accuracy': 0.988875687122345, 'train/loss': 0.03804701939225197, 'train/mean_average_precision': 0.2409850933128835, 'validation/accuracy': 0.9859276413917542, 'validation/loss': 0.04770185425877571, 'validation/mean_average_precision': 0.20543497823037574, 'validation/num_examples': 43793, 'test/accuracy': 0.9850504994392395, 'test/loss': 0.05052958428859711, 'test/mean_average_precision': 0.2013896466472349, 'test/num_examples': 43793, 'score': 1477.158813238144, 'total_duration': 2228.286409854889, 'accumulated_submission_time': 1477.158813238144, 'accumulated_eval_time': 750.9599204063416, 'accumulated_logging_time': 0.12173795700073242}
I0502 03:21:59.759541 139604077913856 logging_writer.py:48] [5379] accumulated_eval_time=750.959920, accumulated_logging_time=0.121738, accumulated_submission_time=1477.158813, global_step=5379, preemption_count=0, score=1477.158813, test/accuracy=0.985050, test/loss=0.050530, test/mean_average_precision=0.201390, test/num_examples=43793, total_duration=2228.286410, train/accuracy=0.988876, train/loss=0.038047, train/mean_average_precision=0.240985, validation/accuracy=0.985928, validation/loss=0.047702, validation/mean_average_precision=0.205435, validation/num_examples=43793
I0502 03:22:05.660937 139604069521152 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.007791560608893633, loss=0.04101628437638283
I0502 03:22:32.731627 139604077913856 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.007304931525141001, loss=0.03963835909962654
I0502 03:22:59.710515 139604069521152 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.006161246448755264, loss=0.03833495080471039
I0502 03:23:26.593559 139604077913856 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.008077355101704597, loss=0.04519295692443848
I0502 03:23:53.270959 139604069521152 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.008861646987497807, loss=0.04017408937215805
I0502 03:24:20.056280 139604077913856 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.008476156741380692, loss=0.03894992917776108
I0502 03:24:47.009919 139604069521152 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.009685813449323177, loss=0.039962559938430786
I0502 03:25:13.847325 139604077913856 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.005986952688544989, loss=0.03747573867440224
I0502 03:25:41.094322 139604069521152 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.00822165235877037, loss=0.04181991517543793
I0502 03:25:59.988671 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:27:21.227156 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:27:23.853863 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:27:26.386473 139790038521664 submission_runner.py:415] Time since start: 2554.92s, 	Step: 6271, 	{'train/accuracy': 0.9893053770065308, 'train/loss': 0.03639327734708786, 'train/mean_average_precision': 0.282608824276557, 'validation/accuracy': 0.9861159920692444, 'validation/loss': 0.04674238711595535, 'validation/mean_average_precision': 0.21425234403451487, 'validation/num_examples': 43793, 'test/accuracy': 0.9852910041809082, 'test/loss': 0.0493013970553875, 'test/mean_average_precision': 0.2170387041190656, 'test/num_examples': 43793, 'score': 1717.3706285953522, 'total_duration': 2554.922627210617, 'accumulated_submission_time': 1717.3706285953522, 'accumulated_eval_time': 837.3576741218567, 'accumulated_logging_time': 0.1407010555267334}
I0502 03:27:26.396490 139604077913856 logging_writer.py:48] [6271] accumulated_eval_time=837.357674, accumulated_logging_time=0.140701, accumulated_submission_time=1717.370629, global_step=6271, preemption_count=0, score=1717.370629, test/accuracy=0.985291, test/loss=0.049301, test/mean_average_precision=0.217039, test/num_examples=43793, total_duration=2554.922627, train/accuracy=0.989305, train/loss=0.036393, train/mean_average_precision=0.282609, validation/accuracy=0.986116, validation/loss=0.046742, validation/mean_average_precision=0.214252, validation/num_examples=43793
I0502 03:27:34.431048 139604069521152 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.005830978974699974, loss=0.03674069792032242
I0502 03:28:01.376085 139604077913856 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.006659392267465591, loss=0.04051995649933815
I0502 03:28:28.675447 139604069521152 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.009277716279029846, loss=0.038726162165403366
I0502 03:28:55.719483 139604077913856 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.008793824352324009, loss=0.03768286481499672
I0502 03:29:22.736643 139604069521152 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.008710063993930817, loss=0.03578447923064232
I0502 03:29:49.762832 139604077913856 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.006732175592333078, loss=0.03663039207458496
I0502 03:30:16.907881 139604069521152 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.006648758426308632, loss=0.034274302423000336
I0502 03:30:44.182449 139604077913856 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.007441381923854351, loss=0.04195930063724518
I0502 03:31:11.298186 139604069521152 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.008335777558386326, loss=0.03983335196971893
I0502 03:31:26.560678 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:32:46.325301 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:32:48.988183 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:32:51.535343 139790038521664 submission_runner.py:415] Time since start: 2880.07s, 	Step: 7157, 	{'train/accuracy': 0.9893465638160706, 'train/loss': 0.03564969822764397, 'train/mean_average_precision': 0.31918585847949055, 'validation/accuracy': 0.9861996173858643, 'validation/loss': 0.046771466732025146, 'validation/mean_average_precision': 0.2277825576520719, 'validation/num_examples': 43793, 'test/accuracy': 0.9853773713111877, 'test/loss': 0.04934900254011154, 'test/mean_average_precision': 0.23443836340480165, 'test/num_examples': 43793, 'score': 1957.5178518295288, 'total_duration': 2880.0714905261993, 'accumulated_submission_time': 1957.5178518295288, 'accumulated_eval_time': 922.332287311554, 'accumulated_logging_time': 0.16008758544921875}
I0502 03:32:51.544503 139604077913856 logging_writer.py:48] [7157] accumulated_eval_time=922.332287, accumulated_logging_time=0.160088, accumulated_submission_time=1957.517852, global_step=7157, preemption_count=0, score=1957.517852, test/accuracy=0.985377, test/loss=0.049349, test/mean_average_precision=0.234438, test/num_examples=43793, total_duration=2880.071491, train/accuracy=0.989347, train/loss=0.035650, train/mean_average_precision=0.319186, validation/accuracy=0.986200, validation/loss=0.046771, validation/mean_average_precision=0.227783, validation/num_examples=43793
I0502 03:33:03.646125 139604069521152 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.008467154577374458, loss=0.04353605583310127
I0502 03:33:30.623358 139604077913856 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.006144041661173105, loss=0.03752708062529564
I0502 03:33:57.511350 139604069521152 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.011741019785404205, loss=0.03535446524620056
I0502 03:34:24.523684 139604077913856 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.006391050759702921, loss=0.034026265144348145
I0502 03:34:51.856056 139604069521152 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.00749241141602397, loss=0.038122884929180145
I0502 03:35:18.565531 139604077913856 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.006434865295886993, loss=0.03925677761435509
I0502 03:35:45.567962 139604069521152 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0077311797067523, loss=0.03879315033555031
I0502 03:36:12.515686 139604077913856 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0059840804897248745, loss=0.036933526396751404
I0502 03:36:39.454086 139604069521152 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.005519597325474024, loss=0.03531346470117569
I0502 03:36:51.812257 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:38:07.340736 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:38:09.930704 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:38:12.442148 139790038521664 submission_runner.py:415] Time since start: 3200.98s, 	Step: 8047, 	{'train/accuracy': 0.9897553324699402, 'train/loss': 0.03428752347826958, 'train/mean_average_precision': 0.3383210872843441, 'validation/accuracy': 0.9864277243614197, 'validation/loss': 0.04615185037255287, 'validation/mean_average_precision': 0.23934673652611074, 'validation/num_examples': 43793, 'test/accuracy': 0.98563551902771, 'test/loss': 0.04875045269727707, 'test/mean_average_precision': 0.24366815238356224, 'test/num_examples': 43793, 'score': 2197.7675795555115, 'total_duration': 3200.9783165454865, 'accumulated_submission_time': 2197.7675795555115, 'accumulated_eval_time': 1002.9621572494507, 'accumulated_logging_time': 0.17969775199890137}
I0502 03:38:12.451443 139604077913856 logging_writer.py:48] [8047] accumulated_eval_time=1002.962157, accumulated_logging_time=0.179698, accumulated_submission_time=2197.767580, global_step=8047, preemption_count=0, score=2197.767580, test/accuracy=0.985636, test/loss=0.048750, test/mean_average_precision=0.243668, test/num_examples=43793, total_duration=3200.978317, train/accuracy=0.989755, train/loss=0.034288, train/mean_average_precision=0.338321, validation/accuracy=0.986428, validation/loss=0.046152, validation/mean_average_precision=0.239347, validation/num_examples=43793
I0502 03:38:27.094471 139604069521152 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.006355144549161196, loss=0.03679904714226723
I0502 03:38:53.754032 139604077913856 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.008037623018026352, loss=0.040364839136600494
I0502 03:39:20.539784 139604069521152 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.005856218747794628, loss=0.037878986448049545
I0502 03:39:47.302543 139604077913856 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0058816466480493546, loss=0.03828800469636917
I0502 03:40:13.954831 139604069521152 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.009014342911541462, loss=0.034219615161418915
I0502 03:40:40.584245 139604077913856 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.009143914096057415, loss=0.03963589295744896
I0502 03:41:07.537825 139604069521152 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.007241642568260431, loss=0.04003264382481575
I0502 03:41:34.427016 139604077913856 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.008719254285097122, loss=0.04023345559835434
I0502 03:42:01.289914 139604069521152 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.006756780203431845, loss=0.036692339926958084
I0502 03:42:12.716497 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:43:30.547639 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:43:33.508551 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:43:36.050496 139790038521664 submission_runner.py:415] Time since start: 3524.59s, 	Step: 8943, 	{'train/accuracy': 0.9901561737060547, 'train/loss': 0.03254234045743942, 'train/mean_average_precision': 0.3749640475887219, 'validation/accuracy': 0.9865556359291077, 'validation/loss': 0.04534602165222168, 'validation/mean_average_precision': 0.24191288895997645, 'validation/num_examples': 43793, 'test/accuracy': 0.9856780767440796, 'test/loss': 0.04806461185216904, 'test/mean_average_precision': 0.24356629879881941, 'test/num_examples': 43793, 'score': 2438.014825105667, 'total_duration': 3524.586663246155, 'accumulated_submission_time': 2438.014825105667, 'accumulated_eval_time': 1086.2961208820343, 'accumulated_logging_time': 0.1993556022644043}
I0502 03:43:36.060859 139604077913856 logging_writer.py:48] [8943] accumulated_eval_time=1086.296121, accumulated_logging_time=0.199356, accumulated_submission_time=2438.014825, global_step=8943, preemption_count=0, score=2438.014825, test/accuracy=0.985678, test/loss=0.048065, test/mean_average_precision=0.243566, test/num_examples=43793, total_duration=3524.586663, train/accuracy=0.990156, train/loss=0.032542, train/mean_average_precision=0.374964, validation/accuracy=0.986556, validation/loss=0.045346, validation/mean_average_precision=0.241913, validation/num_examples=43793
I0502 03:43:51.771927 139604069521152 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.009855324402451515, loss=0.039625126868486404
I0502 03:44:18.384809 139604077913856 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.013192368671298027, loss=0.03524049371480942
I0502 03:44:45.299987 139604069521152 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.007740980479866266, loss=0.034828756004571915
I0502 03:45:12.001263 139604077913856 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.00825083814561367, loss=0.03652181103825569
I0502 03:45:38.761626 139604069521152 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.005779091734439135, loss=0.034065429121255875
I0502 03:46:05.499417 139604077913856 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.006360973697155714, loss=0.03296525403857231
I0502 03:46:32.429648 139604069521152 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.006808061618357897, loss=0.03461194410920143
I0502 03:46:59.218442 139604077913856 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.005775594152510166, loss=0.03560807555913925
I0502 03:47:26.023422 139604069521152 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.008574179373681545, loss=0.03458942472934723
I0502 03:47:36.198391 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:48:57.111667 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:48:59.728360 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:49:02.284768 139790038521664 submission_runner.py:415] Time since start: 3850.82s, 	Step: 9839, 	{'train/accuracy': 0.9907102584838867, 'train/loss': 0.031241560354828835, 'train/mean_average_precision': 0.4032158082876319, 'validation/accuracy': 0.9865884780883789, 'validation/loss': 0.04490460455417633, 'validation/mean_average_precision': 0.25258949111399365, 'validation/num_examples': 43793, 'test/accuracy': 0.9857184886932373, 'test/loss': 0.04753401130437851, 'test/mean_average_precision': 0.24707630478114972, 'test/num_examples': 43793, 'score': 2678.135549545288, 'total_duration': 3850.8209342956543, 'accumulated_submission_time': 2678.135549545288, 'accumulated_eval_time': 1172.382467508316, 'accumulated_logging_time': 0.21894001960754395}
I0502 03:49:02.294480 139604077913856 logging_writer.py:48] [9839] accumulated_eval_time=1172.382468, accumulated_logging_time=0.218940, accumulated_submission_time=2678.135550, global_step=9839, preemption_count=0, score=2678.135550, test/accuracy=0.985718, test/loss=0.047534, test/mean_average_precision=0.247076, test/num_examples=43793, total_duration=3850.820934, train/accuracy=0.990710, train/loss=0.031242, train/mean_average_precision=0.403216, validation/accuracy=0.986588, validation/loss=0.044905, validation/mean_average_precision=0.252589, validation/num_examples=43793
I0502 03:49:18.881384 139604069521152 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.007071338128298521, loss=0.036604706197977066
I0502 03:49:45.396003 139604077913856 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.007630293257534504, loss=0.03282812610268593
I0502 03:50:11.844892 139604069521152 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.006057675462216139, loss=0.034431107342243195
I0502 03:50:38.160567 139604077913856 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.006165707018226385, loss=0.03402389958500862
I0502 03:51:04.493117 139604069521152 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.007977200672030449, loss=0.0340287946164608
I0502 03:51:30.762830 139604077913856 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.01151787955313921, loss=0.03946152329444885
I0502 03:51:57.034799 139604069521152 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.006208439357578754, loss=0.03525157645344734
I0502 03:52:23.443301 139604077913856 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.0067612892016768456, loss=0.034860145300626755
I0502 03:52:49.773559 139604069521152 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.005466603673994541, loss=0.03205399960279465
I0502 03:53:02.491418 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:54:20.919894 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:54:23.523236 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:54:26.116516 139790038521664 submission_runner.py:415] Time since start: 4174.65s, 	Step: 10749, 	{'train/accuracy': 0.9908002018928528, 'train/loss': 0.03048689477145672, 'train/mean_average_precision': 0.4237873400874087, 'validation/accuracy': 0.9867565631866455, 'validation/loss': 0.04454026371240616, 'validation/mean_average_precision': 0.2598142589228313, 'validation/num_examples': 43793, 'test/accuracy': 0.9858015179634094, 'test/loss': 0.04732711613178253, 'test/mean_average_precision': 0.24858280289339155, 'test/num_examples': 43793, 'score': 2918.3144104480743, 'total_duration': 4174.652683258057, 'accumulated_submission_time': 2918.3144104480743, 'accumulated_eval_time': 1256.0075273513794, 'accumulated_logging_time': 0.23923945426940918}
I0502 03:54:26.126171 139604077913856 logging_writer.py:48] [10749] accumulated_eval_time=1256.007527, accumulated_logging_time=0.239239, accumulated_submission_time=2918.314410, global_step=10749, preemption_count=0, score=2918.314410, test/accuracy=0.985802, test/loss=0.047327, test/mean_average_precision=0.248583, test/num_examples=43793, total_duration=4174.652683, train/accuracy=0.990800, train/loss=0.030487, train/mean_average_precision=0.423787, validation/accuracy=0.986757, validation/loss=0.044540, validation/mean_average_precision=0.259814, validation/num_examples=43793
I0502 03:54:39.856967 139604069521152 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.007341496646404266, loss=0.03389011695981026
I0502 03:55:06.427215 139604077913856 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0071752737276256084, loss=0.03337990865111351
I0502 03:55:32.788885 139604069521152 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.005584610626101494, loss=0.03432086110115051
I0502 03:55:59.106612 139604077913856 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.0072469389997422695, loss=0.033680349588394165
I0502 03:56:25.334197 139604069521152 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.006191285792738199, loss=0.03657807037234306
I0502 03:56:51.901627 139604077913856 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.0066577373072505, loss=0.0326535739004612
I0502 03:57:18.322196 139604069521152 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.007266214583069086, loss=0.033314939588308334
I0502 03:57:44.730584 139604077913856 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.006249216850847006, loss=0.03584625944495201
I0502 03:58:11.154725 139604069521152 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.010203850455582142, loss=0.03292974829673767
I0502 03:58:26.253227 139790038521664 spec.py:298] Evaluating on the training split.
I0502 03:59:44.778211 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 03:59:47.372863 139790038521664 spec.py:326] Evaluating on the test split.
I0502 03:59:49.920709 139790038521664 submission_runner.py:415] Time since start: 4498.46s, 	Step: 11658, 	{'train/accuracy': 0.9910860657691956, 'train/loss': 0.029652189463377, 'train/mean_average_precision': 0.43720680089491876, 'validation/accuracy': 0.9866384267807007, 'validation/loss': 0.04459833353757858, 'validation/mean_average_precision': 0.2614042040611645, 'validation/num_examples': 43793, 'test/accuracy': 0.9857597947120667, 'test/loss': 0.047450363636016846, 'test/mean_average_precision': 0.25647120089944575, 'test/num_examples': 43793, 'score': 3158.4242799282074, 'total_duration': 4498.456816911697, 'accumulated_submission_time': 3158.4242799282074, 'accumulated_eval_time': 1339.6749114990234, 'accumulated_logging_time': 0.2582874298095703}
I0502 03:59:49.930557 139604077913856 logging_writer.py:48] [11658] accumulated_eval_time=1339.674911, accumulated_logging_time=0.258287, accumulated_submission_time=3158.424280, global_step=11658, preemption_count=0, score=3158.424280, test/accuracy=0.985760, test/loss=0.047450, test/mean_average_precision=0.256471, test/num_examples=43793, total_duration=4498.456817, train/accuracy=0.991086, train/loss=0.029652, train/mean_average_precision=0.437207, validation/accuracy=0.986638, validation/loss=0.044598, validation/mean_average_precision=0.261404, validation/num_examples=43793
I0502 04:00:01.629023 139604069521152 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.0075286864303052425, loss=0.0342562198638916
I0502 04:00:28.076507 139604077913856 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.007013705559074879, loss=0.03851378709077835
I0502 04:00:54.468915 139604069521152 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.008250239305198193, loss=0.03301965817809105
I0502 04:01:20.777208 139790038521664 spec.py:298] Evaluating on the training split.
I0502 04:02:38.908221 139790038521664 spec.py:310] Evaluating on the validation split.
I0502 04:02:41.574804 139790038521664 spec.py:326] Evaluating on the test split.
I0502 04:02:44.178452 139790038521664 submission_runner.py:415] Time since start: 4672.71s, 	Step: 12000, 	{'train/accuracy': 0.9909914135932922, 'train/loss': 0.03000941313803196, 'train/mean_average_precision': 0.42553899387418226, 'validation/accuracy': 0.98675936460495, 'validation/loss': 0.044503968209028244, 'validation/mean_average_precision': 0.26064276490670363, 'validation/num_examples': 43793, 'test/accuracy': 0.9859421849250793, 'test/loss': 0.04725686088204384, 'test/mean_average_precision': 0.25926397094746273, 'test/num_examples': 43793, 'score': 3249.257448196411, 'total_duration': 4672.7146208286285, 'accumulated_submission_time': 3249.257448196411, 'accumulated_eval_time': 1423.0761346817017, 'accumulated_logging_time': 0.2784581184387207}
I0502 04:02:44.188300 139604077913856 logging_writer.py:48] [12000] accumulated_eval_time=1423.076135, accumulated_logging_time=0.278458, accumulated_submission_time=3249.257448, global_step=12000, preemption_count=0, score=3249.257448, test/accuracy=0.985942, test/loss=0.047257, test/mean_average_precision=0.259264, test/num_examples=43793, total_duration=4672.714621, train/accuracy=0.990991, train/loss=0.030009, train/mean_average_precision=0.425539, validation/accuracy=0.986759, validation/loss=0.044504, validation/mean_average_precision=0.260643, validation/num_examples=43793
I0502 04:02:44.205122 139604069521152 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3249.257448
I0502 04:02:44.234720 139790038521664 checkpoints.py:356] Saving checkpoint at step: 12000
I0502 04:02:44.286746 139790038521664 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_adafactor/ogbg_jax/trial_1/checkpoint_12000
I0502 04:02:44.286948 139790038521664 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_adafactor/ogbg_jax/trial_1/checkpoint_12000.
I0502 04:02:44.443222 139790038521664 submission_runner.py:578] Tuning trial 1/1
I0502 04:02:44.443459 139790038521664 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0502 04:02:44.444979 139790038521664 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4566192924976349, 'train/loss': 0.7459662556648254, 'train/mean_average_precision': 0.024366261906756802, 'validation/accuracy': 0.46244072914123535, 'validation/loss': 0.7437726259231567, 'validation/mean_average_precision': 0.02867388806536403, 'validation/num_examples': 43793, 'test/accuracy': 0.4633033871650696, 'test/loss': 0.7438121438026428, 'test/mean_average_precision': 0.029861182216749025, 'test/num_examples': 43793, 'score': 36.52326202392578, 'total_duration': 274.5444288253784, 'accumulated_submission_time': 36.52326202392578, 'accumulated_eval_time': 238.02101707458496, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (908, {'train/accuracy': 0.986880362033844, 'train/loss': 0.050047192722558975, 'train/mean_average_precision': 0.06197843726815895, 'validation/accuracy': 0.9841443300247192, 'validation/loss': 0.06001105532050133, 'validation/mean_average_precision': 0.06055410248735968, 'validation/num_examples': 43793, 'test/accuracy': 0.9831635355949402, 'test/loss': 0.0633639544248581, 'test/mean_average_precision': 0.06216233313845479, 'test/num_examples': 43793, 'score': 276.55537724494934, 'total_duration': 599.313313961029, 'accumulated_submission_time': 276.55537724494934, 'accumulated_eval_time': 322.7229914665222, 'accumulated_logging_time': 0.02710580825805664, 'global_step': 908, 'preemption_count': 0}), (1804, {'train/accuracy': 0.9869723320007324, 'train/loss': 0.047540564090013504, 'train/mean_average_precision': 0.10414538719542529, 'validation/accuracy': 0.9843765497207642, 'validation/loss': 0.057258326560258865, 'validation/mean_average_precision': 0.10161576526123806, 'validation/num_examples': 43793, 'test/accuracy': 0.98338383436203, 'test/loss': 0.060542140156030655, 'test/mean_average_precision': 0.10222338116537917, 'test/num_examples': 43793, 'score': 516.704488992691, 'total_duration': 925.3878529071808, 'accumulated_submission_time': 516.704488992691, 'accumulated_eval_time': 408.6219792366028, 'accumulated_logging_time': 0.04600214958190918, 'global_step': 1804, 'preemption_count': 0}), (2703, {'train/accuracy': 0.9875162839889526, 'train/loss': 0.044647324830293655, 'train/mean_average_precision': 0.14256530691652586, 'validation/accuracy': 0.9848575592041016, 'validation/loss': 0.05388014391064644, 'validation/mean_average_precision': 0.13680043123030747, 'validation/num_examples': 43793, 'test/accuracy': 0.9838876128196716, 'test/loss': 0.05693531036376953, 'test/mean_average_precision': 0.1346309108155529, 'test/num_examples': 43793, 'score': 756.7114279270172, 'total_duration': 1251.8505067825317, 'accumulated_submission_time': 756.7114279270172, 'accumulated_eval_time': 495.0515127182007, 'accumulated_logging_time': 0.06471538543701172, 'global_step': 2703, 'preemption_count': 0}), (3597, {'train/accuracy': 0.9878696799278259, 'train/loss': 0.04201818257570267, 'train/mean_average_precision': 0.17717732002441297, 'validation/accuracy': 0.9850739240646362, 'validation/loss': 0.051490142941474915, 'validation/mean_average_precision': 0.15314752688305638, 'validation/num_examples': 43793, 'test/accuracy': 0.9841015338897705, 'test/loss': 0.05426695570349693, 'test/mean_average_precision': 0.15254572958855372, 'test/num_examples': 43793, 'score': 996.8215925693512, 'total_duration': 1577.391093492508, 'accumulated_submission_time': 996.8215925693512, 'accumulated_eval_time': 580.4567875862122, 'accumulated_logging_time': 0.08233237266540527, 'global_step': 3597, 'preemption_count': 0}), (4490, {'train/accuracy': 0.988457977771759, 'train/loss': 0.03971616551280022, 'train/mean_average_precision': 0.21717630632920235, 'validation/accuracy': 0.9855618476867676, 'validation/loss': 0.04946672171354294, 'validation/mean_average_precision': 0.17869709392573135, 'validation/num_examples': 43793, 'test/accuracy': 0.9846322536468506, 'test/loss': 0.05228358134627342, 'test/mean_average_precision': 0.1784636473479603, 'test/num_examples': 43793, 'score': 1236.9144871234894, 'total_duration': 1902.2048342227936, 'accumulated_submission_time': 1236.9144871234894, 'accumulated_eval_time': 665.1503036022186, 'accumulated_logging_time': 0.10193276405334473, 'global_step': 4490, 'preemption_count': 0}), (5379, {'train/accuracy': 0.988875687122345, 'train/loss': 0.03804701939225197, 'train/mean_average_precision': 0.2409850933128835, 'validation/accuracy': 0.9859276413917542, 'validation/loss': 0.04770185425877571, 'validation/mean_average_precision': 0.20543497823037574, 'validation/num_examples': 43793, 'test/accuracy': 0.9850504994392395, 'test/loss': 0.05052958428859711, 'test/mean_average_precision': 0.2013896466472349, 'test/num_examples': 43793, 'score': 1477.158813238144, 'total_duration': 2228.286409854889, 'accumulated_submission_time': 1477.158813238144, 'accumulated_eval_time': 750.9599204063416, 'accumulated_logging_time': 0.12173795700073242, 'global_step': 5379, 'preemption_count': 0}), (6271, {'train/accuracy': 0.9893053770065308, 'train/loss': 0.03639327734708786, 'train/mean_average_precision': 0.282608824276557, 'validation/accuracy': 0.9861159920692444, 'validation/loss': 0.04674238711595535, 'validation/mean_average_precision': 0.21425234403451487, 'validation/num_examples': 43793, 'test/accuracy': 0.9852910041809082, 'test/loss': 0.0493013970553875, 'test/mean_average_precision': 0.2170387041190656, 'test/num_examples': 43793, 'score': 1717.3706285953522, 'total_duration': 2554.922627210617, 'accumulated_submission_time': 1717.3706285953522, 'accumulated_eval_time': 837.3576741218567, 'accumulated_logging_time': 0.1407010555267334, 'global_step': 6271, 'preemption_count': 0}), (7157, {'train/accuracy': 0.9893465638160706, 'train/loss': 0.03564969822764397, 'train/mean_average_precision': 0.31918585847949055, 'validation/accuracy': 0.9861996173858643, 'validation/loss': 0.046771466732025146, 'validation/mean_average_precision': 0.2277825576520719, 'validation/num_examples': 43793, 'test/accuracy': 0.9853773713111877, 'test/loss': 0.04934900254011154, 'test/mean_average_precision': 0.23443836340480165, 'test/num_examples': 43793, 'score': 1957.5178518295288, 'total_duration': 2880.0714905261993, 'accumulated_submission_time': 1957.5178518295288, 'accumulated_eval_time': 922.332287311554, 'accumulated_logging_time': 0.16008758544921875, 'global_step': 7157, 'preemption_count': 0}), (8047, {'train/accuracy': 0.9897553324699402, 'train/loss': 0.03428752347826958, 'train/mean_average_precision': 0.3383210872843441, 'validation/accuracy': 0.9864277243614197, 'validation/loss': 0.04615185037255287, 'validation/mean_average_precision': 0.23934673652611074, 'validation/num_examples': 43793, 'test/accuracy': 0.98563551902771, 'test/loss': 0.04875045269727707, 'test/mean_average_precision': 0.24366815238356224, 'test/num_examples': 43793, 'score': 2197.7675795555115, 'total_duration': 3200.9783165454865, 'accumulated_submission_time': 2197.7675795555115, 'accumulated_eval_time': 1002.9621572494507, 'accumulated_logging_time': 0.17969775199890137, 'global_step': 8047, 'preemption_count': 0}), (8943, {'train/accuracy': 0.9901561737060547, 'train/loss': 0.03254234045743942, 'train/mean_average_precision': 0.3749640475887219, 'validation/accuracy': 0.9865556359291077, 'validation/loss': 0.04534602165222168, 'validation/mean_average_precision': 0.24191288895997645, 'validation/num_examples': 43793, 'test/accuracy': 0.9856780767440796, 'test/loss': 0.04806461185216904, 'test/mean_average_precision': 0.24356629879881941, 'test/num_examples': 43793, 'score': 2438.014825105667, 'total_duration': 3524.586663246155, 'accumulated_submission_time': 2438.014825105667, 'accumulated_eval_time': 1086.2961208820343, 'accumulated_logging_time': 0.1993556022644043, 'global_step': 8943, 'preemption_count': 0}), (9839, {'train/accuracy': 0.9907102584838867, 'train/loss': 0.031241560354828835, 'train/mean_average_precision': 0.4032158082876319, 'validation/accuracy': 0.9865884780883789, 'validation/loss': 0.04490460455417633, 'validation/mean_average_precision': 0.25258949111399365, 'validation/num_examples': 43793, 'test/accuracy': 0.9857184886932373, 'test/loss': 0.04753401130437851, 'test/mean_average_precision': 0.24707630478114972, 'test/num_examples': 43793, 'score': 2678.135549545288, 'total_duration': 3850.8209342956543, 'accumulated_submission_time': 2678.135549545288, 'accumulated_eval_time': 1172.382467508316, 'accumulated_logging_time': 0.21894001960754395, 'global_step': 9839, 'preemption_count': 0}), (10749, {'train/accuracy': 0.9908002018928528, 'train/loss': 0.03048689477145672, 'train/mean_average_precision': 0.4237873400874087, 'validation/accuracy': 0.9867565631866455, 'validation/loss': 0.04454026371240616, 'validation/mean_average_precision': 0.2598142589228313, 'validation/num_examples': 43793, 'test/accuracy': 0.9858015179634094, 'test/loss': 0.04732711613178253, 'test/mean_average_precision': 0.24858280289339155, 'test/num_examples': 43793, 'score': 2918.3144104480743, 'total_duration': 4174.652683258057, 'accumulated_submission_time': 2918.3144104480743, 'accumulated_eval_time': 1256.0075273513794, 'accumulated_logging_time': 0.23923945426940918, 'global_step': 10749, 'preemption_count': 0}), (11658, {'train/accuracy': 0.9910860657691956, 'train/loss': 0.029652189463377, 'train/mean_average_precision': 0.43720680089491876, 'validation/accuracy': 0.9866384267807007, 'validation/loss': 0.04459833353757858, 'validation/mean_average_precision': 0.2614042040611645, 'validation/num_examples': 43793, 'test/accuracy': 0.9857597947120667, 'test/loss': 0.047450363636016846, 'test/mean_average_precision': 0.25647120089944575, 'test/num_examples': 43793, 'score': 3158.4242799282074, 'total_duration': 4498.456816911697, 'accumulated_submission_time': 3158.4242799282074, 'accumulated_eval_time': 1339.6749114990234, 'accumulated_logging_time': 0.2582874298095703, 'global_step': 11658, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9909914135932922, 'train/loss': 0.03000941313803196, 'train/mean_average_precision': 0.42553899387418226, 'validation/accuracy': 0.98675936460495, 'validation/loss': 0.044503968209028244, 'validation/mean_average_precision': 0.26064276490670363, 'validation/num_examples': 43793, 'test/accuracy': 0.9859421849250793, 'test/loss': 0.04725686088204384, 'test/mean_average_precision': 0.25926397094746273, 'test/num_examples': 43793, 'score': 3249.257448196411, 'total_duration': 4672.7146208286285, 'accumulated_submission_time': 3249.257448196411, 'accumulated_eval_time': 1423.0761346817017, 'accumulated_logging_time': 0.2784581184387207, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0502 04:02:44.445112 139790038521664 submission_runner.py:581] Timing: 3249.257448196411
I0502 04:02:44.445161 139790038521664 submission_runner.py:582] ====================
I0502 04:02:44.445273 139790038521664 submission_runner.py:645] Final ogbg score: 3249.257448196411
