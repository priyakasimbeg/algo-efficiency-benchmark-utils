python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-02-2023-07-29-01.log
I0502 07:29:22.124942 140022775207744 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_lamb/librispeech_deepspeech_jax.
I0502 07:29:22.209392 140022775207744 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 07:29:23.113818 140022775207744 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0502 07:29:23.114477 140022775207744 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 07:29:23.118365 140022775207744 submission_runner.py:538] Using RNG seed 1158158102
I0502 07:29:25.611394 140022775207744 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 07:29:25.611588 140022775207744 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_lamb/librispeech_deepspeech_jax/trial_1.
I0502 07:29:25.611868 140022775207744 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_lamb/librispeech_deepspeech_jax/trial_1/hparams.json.
I0502 07:29:25.733530 140022775207744 submission_runner.py:241] Initializing dataset.
I0502 07:29:25.733731 140022775207744 submission_runner.py:248] Initializing model.
I0502 07:29:42.174796 140022775207744 submission_runner.py:258] Initializing optimizer.
I0502 07:29:42.821360 140022775207744 submission_runner.py:265] Initializing metrics bundle.
I0502 07:29:42.821564 140022775207744 submission_runner.py:282] Initializing checkpoint and logger.
I0502 07:29:42.822966 140022775207744 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_lamb/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0502 07:29:42.823256 140022775207744 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0502 07:29:42.823330 140022775207744 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0502 07:29:43.803091 140022775207744 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_lamb/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0502 07:29:43.804053 140022775207744 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_lamb/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0502 07:29:43.811520 140022775207744 submission_runner.py:318] Starting training loop.
I0502 07:29:44.004535 140022775207744 input_pipeline.py:20] Loading split = train-clean-100
I0502 07:29:44.039623 140022775207744 input_pipeline.py:20] Loading split = train-clean-360
I0502 07:29:44.361145 140022775207744 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0502 07:30:55.296139 139845712213760 logging_writer.py:48] [0] global_step=0, grad_norm=22.127330780029297, loss=32.640106201171875
I0502 07:30:55.329959 140022775207744 spec.py:298] Evaluating on the training split.
I0502 07:30:55.463677 140022775207744 input_pipeline.py:20] Loading split = train-clean-100
I0502 07:30:55.494971 140022775207744 input_pipeline.py:20] Loading split = train-clean-360
I0502 07:30:55.853926 140022775207744 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0502 07:32:20.030272 140022775207744 spec.py:310] Evaluating on the validation split.
I0502 07:32:20.124476 140022775207744 input_pipeline.py:20] Loading split = dev-clean
I0502 07:32:20.130307 140022775207744 input_pipeline.py:20] Loading split = dev-other
I0502 07:33:11.548878 140022775207744 spec.py:326] Evaluating on the test split.
I0502 07:33:11.643532 140022775207744 input_pipeline.py:20] Loading split = test-clean
I0502 07:33:45.757084 140022775207744 submission_runner.py:415] Time since start: 241.94s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(30.376871, dtype=float32), 'train/wer': 3.928893446890032, 'validation/ctc_loss': DeviceArray(29.11324, dtype=float32), 'validation/wer': 3.748178950110469, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.207697, dtype=float32), 'test/wer': 3.8068571892836105, 'test/num_examples': 2472, 'score': 71.51824259757996, 'total_duration': 241.94423174858093, 'accumulated_submission_time': 71.51824259757996, 'accumulated_eval_time': 170.4258177280426, 'accumulated_logging_time': 0}
I0502 07:33:45.776407 139842541311744 logging_writer.py:48] [1] accumulated_eval_time=170.425818, accumulated_logging_time=0, accumulated_submission_time=71.518243, global_step=1, preemption_count=0, score=71.518243, test/ctc_loss=29.20769691467285, test/num_examples=2472, test/wer=3.806857, total_duration=241.944232, train/ctc_loss=30.37687110900879, train/wer=3.928893, validation/ctc_loss=29.113239288330078, validation/num_examples=5348, validation/wer=3.748179
I0502 07:36:09.322106 139846105343744 logging_writer.py:48] [100] global_step=100, grad_norm=41.066593170166016, loss=30.853717803955078
I0502 07:38:03.845676 139846113736448 logging_writer.py:48] [200] global_step=200, grad_norm=37.78378677368164, loss=22.950132369995117
I0502 07:39:58.341138 139846105343744 logging_writer.py:48] [300] global_step=300, grad_norm=19.555749893188477, loss=12.323311805725098
I0502 07:41:53.068355 139846113736448 logging_writer.py:48] [400] global_step=400, grad_norm=5.100396633148193, loss=7.838764667510986
I0502 07:43:47.945730 139846105343744 logging_writer.py:48] [500] global_step=500, grad_norm=1.3304954767227173, loss=6.578559875488281
I0502 07:45:43.009070 139846113736448 logging_writer.py:48] [600] global_step=600, grad_norm=0.8361093997955322, loss=6.1264142990112305
I0502 07:47:37.853195 139846105343744 logging_writer.py:48] [700] global_step=700, grad_norm=1.772225022315979, loss=5.959591865539551
I0502 07:49:32.663748 139846113736448 logging_writer.py:48] [800] global_step=800, grad_norm=0.44293302297592163, loss=5.852188587188721
I0502 07:51:27.966129 139846105343744 logging_writer.py:48] [900] global_step=900, grad_norm=0.9083024263381958, loss=5.784325122833252
I0502 07:53:23.405339 139846113736448 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5974307656288147, loss=5.751983642578125
I0502 07:55:21.862300 139847255873280 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5670506954193115, loss=5.671577453613281
I0502 07:57:16.973216 139847247480576 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6728686690330505, loss=5.574925422668457
I0502 07:59:12.178927 139847255873280 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7893222570419312, loss=5.493897914886475
I0502 08:01:07.272396 139847247480576 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6982005834579468, loss=5.421950817108154
I0502 08:03:02.487544 139847255873280 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.936050295829773, loss=5.297974586486816
I0502 08:04:57.808802 139847247480576 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7677583694458008, loss=5.165154933929443
I0502 08:06:53.355084 139847255873280 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.1940253973007202, loss=5.0496978759765625
I0502 08:08:48.682266 139847247480576 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.6994538307189941, loss=4.91507625579834
I0502 08:10:43.684692 139847255873280 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.1016383171081543, loss=4.80059289932251
I0502 08:12:38.814089 139847247480576 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.6796756982803345, loss=4.595870494842529
I0502 08:13:46.159156 140022775207744 spec.py:298] Evaluating on the training split.
I0502 08:14:14.766126 140022775207744 spec.py:310] Evaluating on the validation split.
I0502 08:14:48.537708 140022775207744 spec.py:326] Evaluating on the test split.
I0502 08:15:05.892536 140022775207744 submission_runner.py:415] Time since start: 2722.08s, 	Step: 2060, 	{'train/ctc_loss': DeviceArray(6.1734066, dtype=float32), 'train/wer': 0.944517105723878, 'validation/ctc_loss': DeviceArray(6.049012, dtype=float32), 'validation/wer': 0.8959276018099548, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.9912224, dtype=float32), 'test/wer': 0.8994576808238377, 'test/num_examples': 2472, 'score': 2471.8651909828186, 'total_duration': 2722.0778801441193, 'accumulated_submission_time': 2471.8651909828186, 'accumulated_eval_time': 250.15611481666565, 'accumulated_logging_time': 0.030864238739013672}
I0502 08:15:05.916084 139847255873280 logging_writer.py:48] [2060] accumulated_eval_time=250.156115, accumulated_logging_time=0.030864, accumulated_submission_time=2471.865191, global_step=2060, preemption_count=0, score=2471.865191, test/ctc_loss=5.991222381591797, test/num_examples=2472, test/wer=0.899458, total_duration=2722.077880, train/ctc_loss=6.173406600952148, train/wer=0.944517, validation/ctc_loss=6.049012184143066, validation/num_examples=5348, validation/wer=0.895928
I0502 08:15:56.458610 139847911233280 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.325410008430481, loss=4.452831268310547
I0502 08:17:51.019551 139847902840576 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.775400161743164, loss=4.3313093185424805
I0502 08:19:45.280618 139847911233280 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.7629199028015137, loss=4.2891740798950195
I0502 08:21:39.702620 139847902840576 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.4705944061279297, loss=4.105953693389893
I0502 08:23:34.382352 139847911233280 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.2903714179992676, loss=4.042985916137695
I0502 08:25:29.511501 139847902840576 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.4108024835586548, loss=3.8553240299224854
I0502 08:27:24.798045 139847911233280 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.9527713060379028, loss=3.7973361015319824
I0502 08:29:20.021990 139847902840576 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.224886655807495, loss=3.7082746028900146
I0502 08:31:15.072777 139847911233280 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.7898931503295898, loss=3.623539447784424
I0502 08:33:09.407084 139847902840576 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.7847940921783447, loss=3.5769646167755127
I0502 08:35:07.000154 139847255873280 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.378596782684326, loss=3.536508560180664
I0502 08:37:01.917054 139847247480576 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.6139729022979736, loss=3.3996212482452393
I0502 08:38:56.327209 139847255873280 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.2339913845062256, loss=3.381091594696045
I0502 08:40:50.841750 139847247480576 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.826730966567993, loss=3.351186752319336
I0502 08:42:45.336676 139847255873280 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.0404105186462402, loss=3.266741991043091
I0502 08:44:39.470301 139847247480576 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.1366348266601562, loss=3.2249622344970703
I0502 08:46:34.046884 139847255873280 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.1502292156219482, loss=3.2054100036621094
I0502 08:48:29.516073 139847247480576 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.0932116508483887, loss=3.059861898422241
I0502 08:50:23.893408 139847255873280 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.7493233680725098, loss=3.106959104537964
I0502 08:52:18.526021 139847247480576 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.750255823135376, loss=3.078606128692627
I0502 08:54:12.967663 139847255873280 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.9669013023376465, loss=3.0087504386901855
I0502 08:55:06.465997 140022775207744 spec.py:298] Evaluating on the training split.
I0502 08:55:40.431347 140022775207744 spec.py:310] Evaluating on the validation split.
I0502 08:56:16.374727 140022775207744 spec.py:326] Evaluating on the test split.
I0502 08:56:35.046346 140022775207744 submission_runner.py:415] Time since start: 5211.23s, 	Step: 4145, 	{'train/ctc_loss': DeviceArray(4.3015857, dtype=float32), 'train/wer': 0.8239458753962123, 'validation/ctc_loss': DeviceArray(4.5198717, dtype=float32), 'validation/wer': 0.8083531920230779, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(4.2094455, dtype=float32), 'test/wer': 0.7894907886986371, 'test/num_examples': 2472, 'score': 4872.373836278915, 'total_duration': 5211.23140501976, 'accumulated_submission_time': 4872.373836278915, 'accumulated_eval_time': 338.73328971862793, 'accumulated_logging_time': 0.06956648826599121}
I0502 08:56:35.071916 139847327553280 logging_writer.py:48] [4145] accumulated_eval_time=338.733290, accumulated_logging_time=0.069566, accumulated_submission_time=4872.373836, global_step=4145, preemption_count=0, score=4872.373836, test/ctc_loss=4.209445476531982, test/num_examples=2472, test/wer=0.789491, total_duration=5211.231405, train/ctc_loss=4.301585674285889, train/wer=0.823946, validation/ctc_loss=4.519871711730957, validation/num_examples=5348, validation/wer=0.808353
I0502 08:57:38.887725 139847319160576 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.960455894470215, loss=2.965553045272827
I0502 08:59:32.736255 139847327553280 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.4376771450042725, loss=2.9708974361419678
I0502 09:01:26.357137 139847319160576 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.8912103176116943, loss=2.921659231185913
I0502 09:03:20.400544 139847327553280 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.9044854640960693, loss=2.8865163326263428
I0502 09:05:14.697525 139847319160576 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.208400249481201, loss=2.880418539047241
I0502 09:07:09.079535 139847327553280 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.2382006645202637, loss=2.843191385269165
I0502 09:09:02.982634 139847319160576 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.990064859390259, loss=2.7499916553497314
I0502 09:10:56.964952 139847327553280 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.853696584701538, loss=2.783698558807373
I0502 09:12:51.720584 139847319160576 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.581622838973999, loss=2.7507643699645996
I0502 09:14:45.927250 139847327553280 logging_writer.py:48] [5100] global_step=5100, grad_norm=4.152487754821777, loss=2.774503469467163
I0502 09:16:43.564250 139847327553280 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.6214444637298584, loss=2.631711721420288
I0502 09:18:37.560965 139847319160576 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.4712471961975098, loss=2.6728999614715576
I0502 09:20:31.867298 139847327553280 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.9305431842803955, loss=2.5635249614715576
I0502 09:22:26.126289 139847319160576 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.762716293334961, loss=2.626190662384033
I0502 09:24:19.873483 139847327553280 logging_writer.py:48] [5600] global_step=5600, grad_norm=4.240763187408447, loss=2.5773298740386963
I0502 09:26:14.149533 139847319160576 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.0106852054595947, loss=2.5765304565429688
I0502 09:28:08.491464 139847327553280 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.086949348449707, loss=2.539579153060913
I0502 09:30:02.948115 139847319160576 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.2139174938201904, loss=2.5019783973693848
I0502 09:31:57.126323 139847327553280 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.9554173946380615, loss=2.4799044132232666
I0502 09:33:51.040448 139847319160576 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.4239227771759033, loss=2.4751813411712646
I0502 09:35:48.434019 139847327553280 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.4818472862243652, loss=2.481358051300049
I0502 09:36:36.031641 140022775207744 spec.py:298] Evaluating on the training split.
I0502 09:37:15.083598 140022775207744 spec.py:310] Evaluating on the validation split.
I0502 09:37:52.575572 140022775207744 spec.py:326] Evaluating on the test split.
I0502 09:38:11.813737 140022775207744 submission_runner.py:415] Time since start: 7708.00s, 	Step: 6243, 	{'train/ctc_loss': DeviceArray(1.2384573, dtype=float32), 'train/wer': 0.36359515262689035, 'validation/ctc_loss': DeviceArray(1.6889132, dtype=float32), 'validation/wer': 0.42856178062499395, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.2834716, dtype=float32), 'test/wer': 0.35880405419129446, 'test/num_examples': 2472, 'score': 7273.2945992946625, 'total_duration': 7707.999155282974, 'accumulated_submission_time': 7273.2945992946625, 'accumulated_eval_time': 434.5124065876007, 'accumulated_logging_time': 0.10886836051940918}
I0502 09:38:11.835747 139847982913280 logging_writer.py:48] [6243] accumulated_eval_time=434.512407, accumulated_logging_time=0.108868, accumulated_submission_time=7273.294599, global_step=6243, preemption_count=0, score=7273.294599, test/ctc_loss=1.2834715843200684, test/num_examples=2472, test/wer=0.358804, total_duration=7707.999155, train/ctc_loss=1.2384573221206665, train/wer=0.363595, validation/ctc_loss=1.6889132261276245, validation/num_examples=5348, validation/wer=0.428562
I0502 09:39:17.713295 139847974520576 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.8385889530181885, loss=2.407482385635376
I0502 09:41:10.879176 139847982913280 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.9072089195251465, loss=2.453547954559326
I0502 09:43:04.605540 139847974520576 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.7563295364379883, loss=2.410252332687378
I0502 09:44:58.061517 139847982913280 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.352743625640869, loss=2.342432737350464
I0502 09:46:52.144463 139847974520576 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.7446250915527344, loss=2.4149317741394043
I0502 09:48:45.899386 139847982913280 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.849344491958618, loss=2.3881094455718994
I0502 09:50:39.506978 139847974520576 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.2991445064544678, loss=2.297774314880371
I0502 09:52:33.475301 139847982913280 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.0713951587677, loss=2.3138089179992676
I0502 09:54:27.300168 139847974520576 logging_writer.py:48] [7100] global_step=7100, grad_norm=4.438176155090332, loss=2.2551465034484863
I0502 09:56:20.904404 139847982913280 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.9587059020996094, loss=2.249243974685669
I0502 09:58:18.119077 139847982913280 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.2440285682678223, loss=2.3042972087860107
I0502 10:00:11.694432 139847974520576 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.9107038974761963, loss=2.248528480529785
I0502 10:02:05.964438 139847982913280 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.061542272567749, loss=2.18877911567688
I0502 10:03:59.670811 139847974520576 logging_writer.py:48] [7600] global_step=7600, grad_norm=4.359899044036865, loss=2.2126049995422363
I0502 10:05:53.280405 139847982913280 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.611037254333496, loss=2.21313738822937
I0502 10:07:47.529171 139847974520576 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.841125011444092, loss=2.197165012359619
I0502 10:09:41.269205 139847982913280 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.1352274417877197, loss=2.1786110401153564
I0502 10:11:34.806912 139847974520576 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.622227668762207, loss=2.247682809829712
I0502 10:13:28.203473 139847982913280 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.2609646320343018, loss=2.1698176860809326
I0502 10:15:21.920722 139847974520576 logging_writer.py:48] [8200] global_step=8200, grad_norm=4.851594924926758, loss=2.1494696140289307
I0502 10:17:19.204881 139847982913280 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.643493890762329, loss=2.132936954498291
I0502 10:18:12.180968 140022775207744 spec.py:298] Evaluating on the training split.
I0502 10:18:51.615934 140022775207744 spec.py:310] Evaluating on the validation split.
I0502 10:19:29.453240 140022775207744 spec.py:326] Evaluating on the test split.
I0502 10:19:49.223328 140022775207744 submission_runner.py:415] Time since start: 10205.41s, 	Step: 8348, 	{'train/ctc_loss': DeviceArray(0.822764, dtype=float32), 'train/wer': 0.26189927635643895, 'validation/ctc_loss': DeviceArray(1.1843841, dtype=float32), 'validation/wer': 0.32758637324045575, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.8445097, dtype=float32), 'test/wer': 0.26051632035423394, 'test/num_examples': 2472, 'score': 9673.599788665771, 'total_duration': 10205.408656358719, 'accumulated_submission_time': 9673.599788665771, 'accumulated_eval_time': 531.5516774654388, 'accumulated_logging_time': 0.14576196670532227}
I0502 10:19:49.249434 139847982913280 logging_writer.py:48] [8348] accumulated_eval_time=531.551677, accumulated_logging_time=0.145762, accumulated_submission_time=9673.599789, global_step=8348, preemption_count=0, score=9673.599789, test/ctc_loss=0.8445097208023071, test/num_examples=2472, test/wer=0.260516, total_duration=10205.408656, train/ctc_loss=0.822763979434967, train/wer=0.261899, validation/ctc_loss=1.1843841075897217, validation/num_examples=5348, validation/wer=0.327586
I0502 10:20:49.133051 139847974520576 logging_writer.py:48] [8400] global_step=8400, grad_norm=5.046210289001465, loss=2.1464645862579346
I0502 10:22:42.397101 139847982913280 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.401979446411133, loss=2.1723361015319824
I0502 10:24:35.958379 139847974520576 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.7036819458007812, loss=2.051936626434326
I0502 10:26:29.637857 139847982913280 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.1291325092315674, loss=2.1363656520843506
I0502 10:28:23.749075 139847974520576 logging_writer.py:48] [8800] global_step=8800, grad_norm=4.389054775238037, loss=2.098249673843384
I0502 10:30:17.321882 139847982913280 logging_writer.py:48] [8900] global_step=8900, grad_norm=4.354013442993164, loss=2.097432851791382
I0502 10:32:11.090270 139847974520576 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.3210418224334717, loss=2.0206477642059326
I0502 10:34:04.410913 139847982913280 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.001706600189209, loss=2.05549955368042
I0502 10:35:57.914342 139847974520576 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.576734781265259, loss=2.0788490772247314
I0502 10:37:54.695000 139847982913280 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.8792240619659424, loss=2.0204715728759766
I0502 10:39:47.998262 139847974520576 logging_writer.py:48] [9400] global_step=9400, grad_norm=4.297119617462158, loss=2.056523084640503
I0502 10:41:41.653547 139847982913280 logging_writer.py:48] [9500] global_step=9500, grad_norm=4.016784191131592, loss=2.049253225326538
I0502 10:43:35.045250 139847974520576 logging_writer.py:48] [9600] global_step=9600, grad_norm=4.415590286254883, loss=2.0328526496887207
I0502 10:45:29.002112 139847982913280 logging_writer.py:48] [9700] global_step=9700, grad_norm=5.257325172424316, loss=2.000591516494751
I0502 10:47:22.550053 139847974520576 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.546992301940918, loss=1.9745652675628662
I0502 10:49:16.484250 139847982913280 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.3779399394989014, loss=2.039842367172241
I0502 10:51:09.798665 139847974520576 logging_writer.py:48] [10000] global_step=10000, grad_norm=3.4287819862365723, loss=1.9564958810806274
I0502 10:53:03.832687 139847982913280 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.32437801361084, loss=1.916869878768921
I0502 10:54:57.395610 139847974520576 logging_writer.py:48] [10200] global_step=10200, grad_norm=4.077694892883301, loss=1.8615046739578247
I0502 10:56:54.348878 139847982913280 logging_writer.py:48] [10300] global_step=10300, grad_norm=4.580904006958008, loss=1.9851137399673462
I0502 10:58:47.760993 139847974520576 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.5558090209960938, loss=1.9779093265533447
I0502 10:59:50.007374 140022775207744 spec.py:298] Evaluating on the training split.
I0502 11:00:28.719703 140022775207744 spec.py:310] Evaluating on the validation split.
I0502 11:01:07.133064 140022775207744 spec.py:326] Evaluating on the test split.
I0502 11:01:26.807009 140022775207744 submission_runner.py:415] Time since start: 12702.99s, 	Step: 10456, 	{'train/ctc_loss': DeviceArray(0.65371424, dtype=float32), 'train/wer': 0.21425232120724522, 'validation/ctc_loss': DeviceArray(1.0195869, dtype=float32), 'validation/wer': 0.28805873669789384, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.68196183, dtype=float32), 'test/wer': 0.21778075680945708, 'test/num_examples': 2472, 'score': 12074.317021369934, 'total_duration': 12702.99222111702, 'accumulated_submission_time': 12074.317021369934, 'accumulated_eval_time': 628.3481004238129, 'accumulated_logging_time': 0.1865832805633545}
I0502 11:01:26.828353 139848136513280 logging_writer.py:48] [10456] accumulated_eval_time=628.348100, accumulated_logging_time=0.186583, accumulated_submission_time=12074.317021, global_step=10456, preemption_count=0, score=12074.317021, test/ctc_loss=0.6819618344306946, test/num_examples=2472, test/wer=0.217781, total_duration=12702.992221, train/ctc_loss=0.6537142395973206, train/wer=0.214252, validation/ctc_loss=1.0195869207382202, validation/num_examples=5348, validation/wer=0.288059
I0502 11:02:17.919163 139848128120576 logging_writer.py:48] [10500] global_step=10500, grad_norm=5.0534749031066895, loss=1.8998478651046753
I0502 11:04:11.110991 139848136513280 logging_writer.py:48] [10600] global_step=10600, grad_norm=4.328219413757324, loss=1.9553178548812866
I0502 11:06:04.295972 139848128120576 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.270458698272705, loss=1.9823107719421387
I0502 11:07:58.219286 139848136513280 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.8578872680664062, loss=1.9649784564971924
I0502 11:09:52.057760 139848128120576 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.982560396194458, loss=1.910266637802124
I0502 11:11:45.354021 139848136513280 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.703550338745117, loss=1.9580419063568115
I0502 11:13:38.479684 139848128120576 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.030508041381836, loss=1.8921992778778076
I0502 11:15:31.687092 139848136513280 logging_writer.py:48] [11200] global_step=11200, grad_norm=4.532458782196045, loss=1.96553635597229
I0502 11:17:24.921940 139848128120576 logging_writer.py:48] [11300] global_step=11300, grad_norm=4.670754432678223, loss=1.9315458536148071
I0502 11:19:22.546126 139848136513280 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.94331955909729, loss=1.8527731895446777
I0502 11:21:15.630353 139848128120576 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.501697063446045, loss=1.8376657962799072
I0502 11:23:08.880800 139848136513280 logging_writer.py:48] [11600] global_step=11600, grad_norm=4.450279235839844, loss=1.8870484828948975
I0502 11:25:02.232009 139848128120576 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.5842537879943848, loss=1.8988354206085205
I0502 11:26:55.761607 139848136513280 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.135502576828003, loss=1.852602481842041
I0502 11:28:49.277680 139848128120576 logging_writer.py:48] [11900] global_step=11900, grad_norm=4.759000301361084, loss=1.8511196374893188
I0502 11:30:42.732556 139848136513280 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.3578975200653076, loss=1.8774298429489136
I0502 11:32:36.883846 139848128120576 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.831223726272583, loss=1.891463279724121
I0502 11:34:30.842940 139848136513280 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.3084716796875, loss=1.7979309558868408
I0502 11:36:24.393553 139848128120576 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.5608811378479, loss=1.876957893371582
I0502 11:38:21.254560 139848136513280 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.5547635555267334, loss=1.8464465141296387
I0502 11:40:14.445069 139848128120576 logging_writer.py:48] [12500] global_step=12500, grad_norm=4.0605010986328125, loss=1.8392285108566284
I0502 11:41:27.759238 140022775207744 spec.py:298] Evaluating on the training split.
I0502 11:42:06.881644 140022775207744 spec.py:310] Evaluating on the validation split.
I0502 11:42:45.654629 140022775207744 spec.py:326] Evaluating on the test split.
I0502 11:43:05.657835 140022775207744 submission_runner.py:415] Time since start: 15201.84s, 	Step: 12566, 	{'train/ctc_loss': DeviceArray(0.5806394, dtype=float32), 'train/wer': 0.19299977060671855, 'validation/ctc_loss': DeviceArray(0.90938205, dtype=float32), 'validation/wer': 0.26110237435961753, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.59406185, dtype=float32), 'test/wer': 0.19180224646070726, 'test/num_examples': 2472, 'score': 14475.205941438675, 'total_duration': 15201.843183517456, 'accumulated_submission_time': 14475.205941438675, 'accumulated_eval_time': 726.2436175346375, 'accumulated_logging_time': 0.22327566146850586}
I0502 11:43:05.680924 139848566593280 logging_writer.py:48] [12566] accumulated_eval_time=726.243618, accumulated_logging_time=0.223276, accumulated_submission_time=14475.205941, global_step=12566, preemption_count=0, score=14475.205941, test/ctc_loss=0.5940618515014648, test/num_examples=2472, test/wer=0.191802, total_duration=15201.843184, train/ctc_loss=0.5806394219398499, train/wer=0.193000, validation/ctc_loss=0.9093820452690125, validation/num_examples=5348, validation/wer=0.261102
I0502 11:43:45.285789 139848558200576 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.579298257827759, loss=1.8332605361938477
I0502 11:45:38.716644 139848566593280 logging_writer.py:48] [12700] global_step=12700, grad_norm=6.782869338989258, loss=1.8699851036071777
I0502 11:47:32.240664 139848558200576 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.5264153480529785, loss=1.8119659423828125
I0502 11:49:25.391869 139848566593280 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.925806999206543, loss=1.8656750917434692
I0502 11:51:18.607854 139848558200576 logging_writer.py:48] [13000] global_step=13000, grad_norm=5.214451313018799, loss=1.7903717756271362
I0502 11:53:11.826323 139848566593280 logging_writer.py:48] [13100] global_step=13100, grad_norm=4.637562274932861, loss=1.8285878896713257
I0502 11:55:05.157276 139848558200576 logging_writer.py:48] [13200] global_step=13200, grad_norm=3.9619076251983643, loss=1.808610200881958
I0502 11:56:58.609426 139848566593280 logging_writer.py:48] [13300] global_step=13300, grad_norm=8.420633316040039, loss=1.8786859512329102
I0502 11:58:55.221446 139848238913280 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.233234405517578, loss=1.8201498985290527
I0502 12:00:48.310432 139848230520576 logging_writer.py:48] [13500] global_step=13500, grad_norm=5.84939432144165, loss=1.7587398290634155
I0502 12:02:41.618112 139848238913280 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.5992825031280518, loss=1.761160969734192
I0502 12:04:34.639138 139848230520576 logging_writer.py:48] [13700] global_step=13700, grad_norm=9.01094913482666, loss=1.801551342010498
I0502 12:06:27.777934 139848238913280 logging_writer.py:48] [13800] global_step=13800, grad_norm=5.065290451049805, loss=1.8108488321304321
I0502 12:08:21.105365 139848230520576 logging_writer.py:48] [13900] global_step=13900, grad_norm=7.388427734375, loss=1.805940866470337
I0502 12:10:14.277055 139848238913280 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.815518379211426, loss=1.7222239971160889
I0502 12:12:07.850833 139848230520576 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.2113378047943115, loss=1.7758508920669556
I0502 12:14:01.586248 139848238913280 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.1985161304473877, loss=1.7611676454544067
I0502 12:15:54.719226 139848230520576 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.701321840286255, loss=1.7914735078811646
I0502 12:17:47.953352 139848238913280 logging_writer.py:48] [14400] global_step=14400, grad_norm=7.080608367919922, loss=1.8571925163269043
I0502 12:19:44.691352 139848238913280 logging_writer.py:48] [14500] global_step=14500, grad_norm=6.598347187042236, loss=1.710566759109497
I0502 12:21:37.926247 139848230520576 logging_writer.py:48] [14600] global_step=14600, grad_norm=6.403273105621338, loss=1.8028546571731567
I0502 12:23:05.979955 140022775207744 spec.py:298] Evaluating on the training split.
I0502 12:23:45.248619 140022775207744 spec.py:310] Evaluating on the validation split.
I0502 12:24:24.086401 140022775207744 spec.py:326] Evaluating on the test split.
I0502 12:24:44.019719 140022775207744 submission_runner.py:415] Time since start: 17700.21s, 	Step: 14679, 	{'train/ctc_loss': DeviceArray(0.4987128, dtype=float32), 'train/wer': 0.16850658328547424, 'validation/ctc_loss': DeviceArray(0.8443348, dtype=float32), 'validation/wer': 0.24250113363370607, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5378303, dtype=float32), 'test/wer': 0.17396867954420817, 'test/num_examples': 2472, 'score': 16875.46332550049, 'total_duration': 17700.205186128616, 'accumulated_submission_time': 16875.46332550049, 'accumulated_eval_time': 824.2804546356201, 'accumulated_logging_time': 0.2621734142303467}
I0502 12:24:44.043100 139848238913280 logging_writer.py:48] [14679] accumulated_eval_time=824.280455, accumulated_logging_time=0.262173, accumulated_submission_time=16875.463326, global_step=14679, preemption_count=0, score=16875.463326, test/ctc_loss=0.5378302931785583, test/num_examples=2472, test/wer=0.173969, total_duration=17700.205186, train/ctc_loss=0.49871280789375305, train/wer=0.168507, validation/ctc_loss=0.8443347811698914, validation/num_examples=5348, validation/wer=0.242501
I0502 12:25:09.057118 139848230520576 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.9747564792633057, loss=1.7347211837768555
I0502 12:27:02.023303 139848238913280 logging_writer.py:48] [14800] global_step=14800, grad_norm=6.325342178344727, loss=1.7342760562896729
I0502 12:28:55.243636 139848230520576 logging_writer.py:48] [14900] global_step=14900, grad_norm=8.059098243713379, loss=1.741227626800537
I0502 12:30:48.396345 139848238913280 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.676954746246338, loss=1.800731897354126
I0502 12:32:41.563665 139848230520576 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.194153785705566, loss=1.7485898733139038
I0502 12:34:34.937391 139848238913280 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.6777145862579346, loss=1.7713701725006104
I0502 12:36:28.554101 139848230520576 logging_writer.py:48] [15300] global_step=15300, grad_norm=5.659424304962158, loss=1.7356468439102173
I0502 12:38:21.894408 139848238913280 logging_writer.py:48] [15400] global_step=15400, grad_norm=9.269233703613281, loss=1.7865263223648071
I0502 12:40:19.317146 139847368513280 logging_writer.py:48] [15500] global_step=15500, grad_norm=4.731665134429932, loss=1.7100876569747925
I0502 12:42:12.296128 139847360120576 logging_writer.py:48] [15600] global_step=15600, grad_norm=8.869664192199707, loss=1.7818092107772827
I0502 12:44:05.357246 139847368513280 logging_writer.py:48] [15700] global_step=15700, grad_norm=4.960667610168457, loss=1.718519926071167
I0502 12:45:59.096112 139847360120576 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.926891803741455, loss=1.7967238426208496
I0502 12:47:52.447676 139847368513280 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.12530779838562, loss=1.737375020980835
I0502 12:49:44.595392 140022775207744 spec.py:298] Evaluating on the training split.
I0502 12:50:24.226775 140022775207744 spec.py:310] Evaluating on the validation split.
I0502 12:51:02.638205 140022775207744 spec.py:326] Evaluating on the test split.
I0502 12:51:22.467274 140022775207744 submission_runner.py:415] Time since start: 19298.65s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(0.43648478, dtype=float32), 'train/wer': 0.15298889726721965, 'validation/ctc_loss': DeviceArray(0.8231417, dtype=float32), 'validation/wer': 0.23775434398788217, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5157223, dtype=float32), 'test/wer': 0.16675806877500862, 'test/num_examples': 2472, 'score': 18375.984132766724, 'total_duration': 19298.65249657631, 'accumulated_submission_time': 18375.984132766724, 'accumulated_eval_time': 922.1491272449493, 'accumulated_logging_time': 0.30025315284729004}
I0502 12:51:22.491245 139848274753280 logging_writer.py:48] [16000] accumulated_eval_time=922.149127, accumulated_logging_time=0.300253, accumulated_submission_time=18375.984133, global_step=16000, preemption_count=0, score=18375.984133, test/ctc_loss=0.5157222747802734, test/num_examples=2472, test/wer=0.166758, total_duration=19298.652497, train/ctc_loss=0.43648478388786316, train/wer=0.152989, validation/ctc_loss=0.8231416940689087, validation/num_examples=5348, validation/wer=0.237754
I0502 12:51:22.513300 139848266360576 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=18375.984133
I0502 12:51:22.666356 140022775207744 checkpoints.py:356] Saving checkpoint at step: 16000
I0502 12:51:23.359637 140022775207744 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_lamb/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0502 12:51:23.375182 140022775207744 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_lamb/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0502 12:51:24.839901 140022775207744 submission_runner.py:578] Tuning trial 1/1
I0502 12:51:24.840198 140022775207744 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0502 12:51:24.847080 140022775207744 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(30.376871, dtype=float32), 'train/wer': 3.928893446890032, 'validation/ctc_loss': DeviceArray(29.11324, dtype=float32), 'validation/wer': 3.748178950110469, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.207697, dtype=float32), 'test/wer': 3.8068571892836105, 'test/num_examples': 2472, 'score': 71.51824259757996, 'total_duration': 241.94423174858093, 'accumulated_submission_time': 71.51824259757996, 'accumulated_eval_time': 170.4258177280426, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2060, {'train/ctc_loss': DeviceArray(6.1734066, dtype=float32), 'train/wer': 0.944517105723878, 'validation/ctc_loss': DeviceArray(6.049012, dtype=float32), 'validation/wer': 0.8959276018099548, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.9912224, dtype=float32), 'test/wer': 0.8994576808238377, 'test/num_examples': 2472, 'score': 2471.8651909828186, 'total_duration': 2722.0778801441193, 'accumulated_submission_time': 2471.8651909828186, 'accumulated_eval_time': 250.15611481666565, 'accumulated_logging_time': 0.030864238739013672, 'global_step': 2060, 'preemption_count': 0}), (4145, {'train/ctc_loss': DeviceArray(4.3015857, dtype=float32), 'train/wer': 0.8239458753962123, 'validation/ctc_loss': DeviceArray(4.5198717, dtype=float32), 'validation/wer': 0.8083531920230779, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(4.2094455, dtype=float32), 'test/wer': 0.7894907886986371, 'test/num_examples': 2472, 'score': 4872.373836278915, 'total_duration': 5211.23140501976, 'accumulated_submission_time': 4872.373836278915, 'accumulated_eval_time': 338.73328971862793, 'accumulated_logging_time': 0.06956648826599121, 'global_step': 4145, 'preemption_count': 0}), (6243, {'train/ctc_loss': DeviceArray(1.2384573, dtype=float32), 'train/wer': 0.36359515262689035, 'validation/ctc_loss': DeviceArray(1.6889132, dtype=float32), 'validation/wer': 0.42856178062499395, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.2834716, dtype=float32), 'test/wer': 0.35880405419129446, 'test/num_examples': 2472, 'score': 7273.2945992946625, 'total_duration': 7707.999155282974, 'accumulated_submission_time': 7273.2945992946625, 'accumulated_eval_time': 434.5124065876007, 'accumulated_logging_time': 0.10886836051940918, 'global_step': 6243, 'preemption_count': 0}), (8348, {'train/ctc_loss': DeviceArray(0.822764, dtype=float32), 'train/wer': 0.26189927635643895, 'validation/ctc_loss': DeviceArray(1.1843841, dtype=float32), 'validation/wer': 0.32758637324045575, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.8445097, dtype=float32), 'test/wer': 0.26051632035423394, 'test/num_examples': 2472, 'score': 9673.599788665771, 'total_duration': 10205.408656358719, 'accumulated_submission_time': 9673.599788665771, 'accumulated_eval_time': 531.5516774654388, 'accumulated_logging_time': 0.14576196670532227, 'global_step': 8348, 'preemption_count': 0}), (10456, {'train/ctc_loss': DeviceArray(0.65371424, dtype=float32), 'train/wer': 0.21425232120724522, 'validation/ctc_loss': DeviceArray(1.0195869, dtype=float32), 'validation/wer': 0.28805873669789384, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.68196183, dtype=float32), 'test/wer': 0.21778075680945708, 'test/num_examples': 2472, 'score': 12074.317021369934, 'total_duration': 12702.99222111702, 'accumulated_submission_time': 12074.317021369934, 'accumulated_eval_time': 628.3481004238129, 'accumulated_logging_time': 0.1865832805633545, 'global_step': 10456, 'preemption_count': 0}), (12566, {'train/ctc_loss': DeviceArray(0.5806394, dtype=float32), 'train/wer': 0.19299977060671855, 'validation/ctc_loss': DeviceArray(0.90938205, dtype=float32), 'validation/wer': 0.26110237435961753, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.59406185, dtype=float32), 'test/wer': 0.19180224646070726, 'test/num_examples': 2472, 'score': 14475.205941438675, 'total_duration': 15201.843183517456, 'accumulated_submission_time': 14475.205941438675, 'accumulated_eval_time': 726.2436175346375, 'accumulated_logging_time': 0.22327566146850586, 'global_step': 12566, 'preemption_count': 0}), (14679, {'train/ctc_loss': DeviceArray(0.4987128, dtype=float32), 'train/wer': 0.16850658328547424, 'validation/ctc_loss': DeviceArray(0.8443348, dtype=float32), 'validation/wer': 0.24250113363370607, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5378303, dtype=float32), 'test/wer': 0.17396867954420817, 'test/num_examples': 2472, 'score': 16875.46332550049, 'total_duration': 17700.205186128616, 'accumulated_submission_time': 16875.46332550049, 'accumulated_eval_time': 824.2804546356201, 'accumulated_logging_time': 0.2621734142303467, 'global_step': 14679, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(0.43648478, dtype=float32), 'train/wer': 0.15298889726721965, 'validation/ctc_loss': DeviceArray(0.8231417, dtype=float32), 'validation/wer': 0.23775434398788217, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5157223, dtype=float32), 'test/wer': 0.16675806877500862, 'test/num_examples': 2472, 'score': 18375.984132766724, 'total_duration': 19298.65249657631, 'accumulated_submission_time': 18375.984132766724, 'accumulated_eval_time': 922.1491272449493, 'accumulated_logging_time': 0.30025315284729004, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0502 12:51:24.847306 140022775207744 submission_runner.py:581] Timing: 18375.984132766724
I0502 12:51:24.847368 140022775207744 submission_runner.py:582] ====================
I0502 12:51:24.848016 140022775207744 submission_runner.py:645] Final librispeech_deepspeech score: 18375.984132766724
