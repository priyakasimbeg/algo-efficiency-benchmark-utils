python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_sam --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_jax_05-02-2023-07-07-18.log
I0502 07:07:39.995374 140683398973248 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_sam/ogbg_jax.
I0502 07:07:40.067079 140683398973248 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 07:07:40.905015 140683398973248 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0502 07:07:40.905687 140683398973248 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 07:07:40.910405 140683398973248 submission_runner.py:538] Using RNG seed 2126110400
I0502 07:07:43.707971 140683398973248 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 07:07:43.708189 140683398973248 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_sam/ogbg_jax/trial_1.
I0502 07:07:43.708394 140683398973248 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_sam/ogbg_jax/trial_1/hparams.json.
I0502 07:07:43.836257 140683398973248 submission_runner.py:241] Initializing dataset.
I0502 07:07:44.079953 140683398973248 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 07:07:44.085765 140683398973248 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 07:07:44.320559 140683398973248 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 07:07:44.375922 140683398973248 submission_runner.py:248] Initializing model.
I0502 07:07:51.658535 140683398973248 submission_runner.py:258] Initializing optimizer.
I0502 07:07:52.056190 140683398973248 submission_runner.py:265] Initializing metrics bundle.
I0502 07:07:52.056390 140683398973248 submission_runner.py:282] Initializing checkpoint and logger.
I0502 07:07:52.057241 140683398973248 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_sam/ogbg_jax/trial_1 with prefix checkpoint_
I0502 07:07:52.057472 140683398973248 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0502 07:07:52.057531 140683398973248 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0502 07:07:52.925282 140683398973248 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_sam/ogbg_jax/trial_1/meta_data_0.json.
I0502 07:07:52.926250 140683398973248 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_sam/ogbg_jax/trial_1/flags_0.json.
I0502 07:07:52.932211 140683398973248 submission_runner.py:318] Starting training loop.
I0502 07:08:19.886680 140507464328960 logging_writer.py:48] [0] global_step=0, grad_norm=2.546682119369507, loss=0.71556556224823
I0502 07:08:19.899605 140683398973248 spec.py:298] Evaluating on the training split.
I0502 07:08:19.907479 140683398973248 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 07:08:19.911914 140683398973248 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 07:08:19.971091 140683398973248 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 07:09:52.215971 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 07:09:52.218763 140683398973248 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 07:09:52.222607 140683398973248 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 07:09:52.276452 140683398973248 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 07:10:56.404745 140683398973248 spec.py:326] Evaluating on the test split.
I0502 07:10:56.407579 140683398973248 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0502 07:10:56.411727 140683398973248 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0502 07:10:56.466151 140683398973248 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0502 07:12:01.316324 140683398973248 submission_runner.py:415] Time since start: 248.38s, 	Step: 1, 	{'train/accuracy': 0.5173549056053162, 'train/loss': 0.7158744931221008, 'train/mean_average_precision': 0.023710575089221732, 'validation/accuracy': 0.5170056223869324, 'validation/loss': 0.7176371216773987, 'validation/mean_average_precision': 0.027690453479813613, 'validation/num_examples': 43793, 'test/accuracy': 0.516727089881897, 'test/loss': 0.7184858322143555, 'test/mean_average_precision': 0.02788676778431351, 'test/num_examples': 43793, 'score': 26.96721863746643, 'total_duration': 248.38407015800476, 'accumulated_submission_time': 26.96721863746643, 'accumulated_eval_time': 221.4166853427887, 'accumulated_logging_time': 0}
I0502 07:12:01.335841 140497615652608 logging_writer.py:48] [1] accumulated_eval_time=221.416685, accumulated_logging_time=0, accumulated_submission_time=26.967219, global_step=1, preemption_count=0, score=26.967219, test/accuracy=0.516727, test/loss=0.718486, test/mean_average_precision=0.027887, test/num_examples=43793, total_duration=248.384070, train/accuracy=0.517355, train/loss=0.715874, train/mean_average_precision=0.023711, validation/accuracy=0.517006, validation/loss=0.717637, validation/mean_average_precision=0.027690, validation/num_examples=43793
I0502 07:12:25.682157 140497624045312 logging_writer.py:48] [100] global_step=100, grad_norm=0.3876807689666748, loss=0.3462609052658081
I0502 07:12:49.948553 140497615652608 logging_writer.py:48] [200] global_step=200, grad_norm=0.2615106999874115, loss=0.2316107153892517
I0502 07:13:14.431899 140497624045312 logging_writer.py:48] [300] global_step=300, grad_norm=0.15584290027618408, loss=0.14171436429023743
I0502 07:13:38.868416 140497615652608 logging_writer.py:48] [400] global_step=400, grad_norm=0.08512339740991592, loss=0.09392943978309631
I0502 07:14:03.467760 140497624045312 logging_writer.py:48] [500] global_step=500, grad_norm=0.048287127166986465, loss=0.07565245032310486
I0502 07:14:28.148410 140497615652608 logging_writer.py:48] [600] global_step=600, grad_norm=0.03191284462809563, loss=0.06904672086238861
I0502 07:14:52.735465 140497624045312 logging_writer.py:48] [700] global_step=700, grad_norm=0.028559360653162003, loss=0.060043178498744965
I0502 07:15:16.805072 140497615652608 logging_writer.py:48] [800] global_step=800, grad_norm=0.035138294100761414, loss=0.05433199554681778
I0502 07:15:41.274618 140497624045312 logging_writer.py:48] [900] global_step=900, grad_norm=0.017542965710163116, loss=0.05795251950621605
I0502 07:16:01.401805 140683398973248 spec.py:298] Evaluating on the training split.
I0502 07:17:16.421575 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 07:17:19.063323 140683398973248 spec.py:326] Evaluating on the test split.
I0502 07:17:21.618124 140683398973248 submission_runner.py:415] Time since start: 568.69s, 	Step: 984, 	{'train/accuracy': 0.986725926399231, 'train/loss': 0.05291123688220978, 'train/mean_average_precision': 0.050136209216132245, 'validation/accuracy': 0.9841341972351074, 'validation/loss': 0.06234235316514969, 'validation/mean_average_precision': 0.05045361501301569, 'validation/num_examples': 43793, 'test/accuracy': 0.9831517338752747, 'test/loss': 0.06557247042655945, 'test/mean_average_precision': 0.05044444737652363, 'test/num_examples': 43793, 'score': 267.01587867736816, 'total_duration': 568.6858558654785, 'accumulated_submission_time': 267.01587867736816, 'accumulated_eval_time': 301.63297176361084, 'accumulated_logging_time': 0.028897523880004883}
I0502 07:17:21.625899 140497615652608 logging_writer.py:48] [984] accumulated_eval_time=301.632972, accumulated_logging_time=0.028898, accumulated_submission_time=267.015879, global_step=984, preemption_count=0, score=267.015879, test/accuracy=0.983152, test/loss=0.065572, test/mean_average_precision=0.050444, test/num_examples=43793, total_duration=568.685856, train/accuracy=0.986726, train/loss=0.052911, train/mean_average_precision=0.050136, validation/accuracy=0.984134, validation/loss=0.062342, validation/mean_average_precision=0.050454, validation/num_examples=43793
I0502 07:17:25.824162 140497624045312 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.015244076028466225, loss=0.05091994255781174
I0502 07:17:50.306050 140497615652608 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.025234099477529526, loss=0.051668018102645874
I0502 07:18:14.599910 140497624045312 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.017819995060563087, loss=0.05335133895277977
I0502 07:18:39.194419 140497615652608 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.02612335979938507, loss=0.051493946462869644
I0502 07:19:03.620830 140497624045312 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.021884070709347725, loss=0.053844161331653595
I0502 07:19:28.056233 140497615652608 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.01811305060982704, loss=0.04801705852150917
I0502 07:19:52.618608 140497624045312 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.02033812552690506, loss=0.051065873354673386
I0502 07:20:17.016671 140497615652608 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.03166922926902771, loss=0.04949267953634262
I0502 07:20:41.463078 140497624045312 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.02187463268637657, loss=0.04478630796074867
I0502 07:21:05.916862 140497615652608 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.040464967489242554, loss=0.04403429478406906
I0502 07:21:21.671171 140683398973248 spec.py:298] Evaluating on the training split.
I0502 07:22:36.558746 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 07:22:39.214436 140683398973248 spec.py:326] Evaluating on the test split.
I0502 07:22:41.800683 140683398973248 submission_runner.py:415] Time since start: 888.87s, 	Step: 1965, 	{'train/accuracy': 0.987380862236023, 'train/loss': 0.046178754419088364, 'train/mean_average_precision': 0.1144131771546106, 'validation/accuracy': 0.984714686870575, 'validation/loss': 0.05539776012301445, 'validation/mean_average_precision': 0.11248845039475246, 'validation/num_examples': 43793, 'test/accuracy': 0.983755350112915, 'test/loss': 0.05860890820622444, 'test/mean_average_precision': 0.11112497035093982, 'test/num_examples': 43793, 'score': 507.0436429977417, 'total_duration': 888.8684153556824, 'accumulated_submission_time': 507.0436429977417, 'accumulated_eval_time': 381.7624599933624, 'accumulated_logging_time': 0.04616498947143555}
I0502 07:22:41.808845 140497624045312 logging_writer.py:48] [1965] accumulated_eval_time=381.762460, accumulated_logging_time=0.046165, accumulated_submission_time=507.043643, global_step=1965, preemption_count=0, score=507.043643, test/accuracy=0.983755, test/loss=0.058609, test/mean_average_precision=0.111125, test/num_examples=43793, total_duration=888.868415, train/accuracy=0.987381, train/loss=0.046179, train/mean_average_precision=0.114413, validation/accuracy=0.984715, validation/loss=0.055398, validation/mean_average_precision=0.112488, validation/num_examples=43793
I0502 07:22:50.642916 140497615652608 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.038569558411836624, loss=0.050644584000110626
I0502 07:23:15.043585 140497624045312 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.015572012402117252, loss=0.04238598048686981
I0502 07:23:39.350802 140497615652608 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.024691233411431313, loss=0.05024242773652077
I0502 07:24:03.806341 140497624045312 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.051455508917570114, loss=0.05270784720778465
I0502 07:24:28.117174 140497615652608 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.02024172805249691, loss=0.04673608019948006
I0502 07:24:52.384000 140497624045312 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.043506212532520294, loss=0.04648609459400177
I0502 07:25:16.640356 140497615652608 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.023494791239500046, loss=0.04631197825074196
I0502 07:25:41.046452 140497624045312 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.019522778689861298, loss=0.05017773434519768
I0502 07:26:05.390898 140497615652608 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.014317640103399754, loss=0.04179663956165314
I0502 07:26:29.546413 140497624045312 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.029443562030792236, loss=0.04534376040101051
I0502 07:26:42.013470 140683398973248 spec.py:298] Evaluating on the training split.
I0502 07:27:56.306194 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 07:27:58.916551 140683398973248 spec.py:326] Evaluating on the test split.
I0502 07:28:01.452730 140683398973248 submission_runner.py:415] Time since start: 1208.52s, 	Step: 2952, 	{'train/accuracy': 0.9878122806549072, 'train/loss': 0.04370756074786186, 'train/mean_average_precision': 0.14563178902673712, 'validation/accuracy': 0.9849497079849243, 'validation/loss': 0.05291275680065155, 'validation/mean_average_precision': 0.13967857535007894, 'validation/num_examples': 43793, 'test/accuracy': 0.9839696884155273, 'test/loss': 0.055898476392030716, 'test/mean_average_precision': 0.13570694815249312, 'test/num_examples': 43793, 'score': 747.2290349006653, 'total_duration': 1208.52046084404, 'accumulated_submission_time': 747.2290349006653, 'accumulated_eval_time': 461.2016849517822, 'accumulated_logging_time': 0.06530165672302246}
I0502 07:28:01.460721 140497615652608 logging_writer.py:48] [2952] accumulated_eval_time=461.201685, accumulated_logging_time=0.065302, accumulated_submission_time=747.229035, global_step=2952, preemption_count=0, score=747.229035, test/accuracy=0.983970, test/loss=0.055898, test/mean_average_precision=0.135707, test/num_examples=43793, total_duration=1208.520461, train/accuracy=0.987812, train/loss=0.043708, train/mean_average_precision=0.145632, validation/accuracy=0.984950, validation/loss=0.052913, validation/mean_average_precision=0.139679, validation/num_examples=43793
I0502 07:28:13.399529 140497624045312 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.03472370654344559, loss=0.04393460229039192
I0502 07:28:37.668430 140497615652608 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.027729565277695656, loss=0.043239086866378784
I0502 07:29:01.639637 140497624045312 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.022726995870471, loss=0.050838153809309006
I0502 07:29:26.093713 140497615652608 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.015048129484057426, loss=0.04432014748454094
I0502 07:29:50.483062 140497624045312 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.010722100734710693, loss=0.04263656213879585
I0502 07:30:15.332673 140497615652608 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0300755575299263, loss=0.04580329731106758
I0502 07:30:39.435431 140497624045312 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0284686591476202, loss=0.04131760075688362
I0502 07:31:03.588865 140497615652608 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.012615008279681206, loss=0.04429879039525986
I0502 07:31:27.749263 140497624045312 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.013863288797438145, loss=0.04128783941268921
I0502 07:31:52.107349 140497615652608 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.024997899308800697, loss=0.043510887771844864
I0502 07:32:01.611519 140683398973248 spec.py:298] Evaluating on the training split.
I0502 07:33:17.955813 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 07:33:20.618247 140683398973248 spec.py:326] Evaluating on the test split.
I0502 07:33:23.184597 140683398973248 submission_runner.py:415] Time since start: 1530.25s, 	Step: 3940, 	{'train/accuracy': 0.9877329468727112, 'train/loss': 0.04278608039021492, 'train/mean_average_precision': 0.17925923264709687, 'validation/accuracy': 0.9849838018417358, 'validation/loss': 0.05209891498088837, 'validation/mean_average_precision': 0.1580512821861959, 'validation/num_examples': 43793, 'test/accuracy': 0.9840611219406128, 'test/loss': 0.05499963462352753, 'test/mean_average_precision': 0.15534532262188894, 'test/num_examples': 43793, 'score': 987.3621923923492, 'total_duration': 1530.2523148059845, 'accumulated_submission_time': 987.3621923923492, 'accumulated_eval_time': 542.7747147083282, 'accumulated_logging_time': 0.0829007625579834}
I0502 07:33:23.193566 140497624045312 logging_writer.py:48] [3940] accumulated_eval_time=542.774715, accumulated_logging_time=0.082901, accumulated_submission_time=987.362192, global_step=3940, preemption_count=0, score=987.362192, test/accuracy=0.984061, test/loss=0.055000, test/mean_average_precision=0.155345, test/num_examples=43793, total_duration=1530.252315, train/accuracy=0.987733, train/loss=0.042786, train/mean_average_precision=0.179259, validation/accuracy=0.984984, validation/loss=0.052099, validation/mean_average_precision=0.158051, validation/num_examples=43793
I0502 07:33:38.623269 140497615652608 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.016119448468089104, loss=0.0421176441013813
I0502 07:34:03.290371 140497624045312 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.030143938958644867, loss=0.042897775769233704
I0502 07:34:27.855434 140497615652608 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.013679178431630135, loss=0.04430334270000458
I0502 07:34:52.346054 140497624045312 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.02177865244448185, loss=0.039854034781455994
I0502 07:35:17.528419 140497615652608 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.01102469116449356, loss=0.04006088525056839
I0502 07:35:43.029317 140497624045312 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.011515408754348755, loss=0.03943203389644623
I0502 07:36:08.398347 140497615652608 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.02939292974770069, loss=0.04669540748000145
I0502 07:36:33.730642 140497624045312 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.019371213391423225, loss=0.039222102612257004
I0502 07:36:58.827088 140497615652608 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.01106816902756691, loss=0.03807524964213371
I0502 07:37:23.098929 140497624045312 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.017327895388007164, loss=0.04363957419991493
I0502 07:37:23.344914 140683398973248 spec.py:298] Evaluating on the training split.
I0502 07:38:37.426905 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 07:38:40.075663 140683398973248 spec.py:326] Evaluating on the test split.
I0502 07:38:42.640644 140683398973248 submission_runner.py:415] Time since start: 1849.71s, 	Step: 4902, 	{'train/accuracy': 0.988550066947937, 'train/loss': 0.04023940488696098, 'train/mean_average_precision': 0.2137303396869934, 'validation/accuracy': 0.9855801463127136, 'validation/loss': 0.04921437054872513, 'validation/mean_average_precision': 0.1782463028322352, 'validation/num_examples': 43793, 'test/accuracy': 0.9847114682197571, 'test/loss': 0.051728256046772, 'test/mean_average_precision': 0.17701818740163777, 'test/num_examples': 43793, 'score': 1227.494402885437, 'total_duration': 1849.7083630561829, 'accumulated_submission_time': 1227.494402885437, 'accumulated_eval_time': 622.0703899860382, 'accumulated_logging_time': 0.10292553901672363}
I0502 07:38:42.649660 140497615652608 logging_writer.py:48] [4902] accumulated_eval_time=622.070390, accumulated_logging_time=0.102926, accumulated_submission_time=1227.494403, global_step=4902, preemption_count=0, score=1227.494403, test/accuracy=0.984711, test/loss=0.051728, test/mean_average_precision=0.177018, test/num_examples=43793, total_duration=1849.708363, train/accuracy=0.988550, train/loss=0.040239, train/mean_average_precision=0.213730, validation/accuracy=0.985580, validation/loss=0.049214, validation/mean_average_precision=0.178246, validation/num_examples=43793
I0502 07:39:06.965466 140497624045312 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.012635168619453907, loss=0.04425542801618576
I0502 07:39:31.675160 140497615652608 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.012553973123431206, loss=0.04133712872862816
I0502 07:39:56.321609 140497624045312 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.015127732418477535, loss=0.04277748987078667
I0502 07:40:20.908691 140497615652608 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.012756220996379852, loss=0.04333417862653732
I0502 07:40:45.626130 140497624045312 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.013067150488495827, loss=0.044182781130075455
I0502 07:41:10.456413 140497615652608 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.02243683487176895, loss=0.03993159905076027
I0502 07:41:35.729685 140497624045312 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.012312003411352634, loss=0.04015650227665901
I0502 07:42:00.341389 140497615652608 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.015498653054237366, loss=0.044687192887067795
I0502 07:42:25.003575 140497624045312 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.011234061792492867, loss=0.044878873974084854
I0502 07:42:42.875115 140683398973248 spec.py:298] Evaluating on the training split.
I0502 07:43:57.523662 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 07:44:00.364662 140683398973248 spec.py:326] Evaluating on the test split.
I0502 07:44:02.964702 140683398973248 submission_runner.py:415] Time since start: 2170.03s, 	Step: 5874, 	{'train/accuracy': 0.9886568188667297, 'train/loss': 0.03886275738477707, 'train/mean_average_precision': 0.24157015534491993, 'validation/accuracy': 0.9856288433074951, 'validation/loss': 0.04835035651922226, 'validation/mean_average_precision': 0.19072960687598983, 'validation/num_examples': 43793, 'test/accuracy': 0.9847897887229919, 'test/loss': 0.050856299698352814, 'test/mean_average_precision': 0.19265867445013302, 'test/num_examples': 43793, 'score': 1467.6948227882385, 'total_duration': 2170.032426595688, 'accumulated_submission_time': 1467.6948227882385, 'accumulated_eval_time': 702.1599357128143, 'accumulated_logging_time': 0.12876105308532715}
I0502 07:44:02.972988 140497615652608 logging_writer.py:48] [5874] accumulated_eval_time=702.159936, accumulated_logging_time=0.128761, accumulated_submission_time=1467.694823, global_step=5874, preemption_count=0, score=1467.694823, test/accuracy=0.984790, test/loss=0.050856, test/mean_average_precision=0.192659, test/num_examples=43793, total_duration=2170.032427, train/accuracy=0.988657, train/loss=0.038863, train/mean_average_precision=0.241570, validation/accuracy=0.985629, validation/loss=0.048350, validation/mean_average_precision=0.190730, validation/num_examples=43793
I0502 07:44:09.589501 140497624045312 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.021708041429519653, loss=0.0457252636551857
I0502 07:44:33.895464 140497615652608 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.011435563676059246, loss=0.04184722155332565
I0502 07:44:58.102188 140497624045312 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.020227739587426186, loss=0.04531572386622429
I0502 07:45:22.511888 140497615652608 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.01713648997247219, loss=0.041751839220523834
I0502 07:45:46.941245 140497624045312 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.008924274705350399, loss=0.03812490776181221
I0502 07:46:11.167700 140497615652608 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.016759181395173073, loss=0.04252252355217934
I0502 07:46:35.567842 140497624045312 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.015355493873357773, loss=0.03938089311122894
I0502 07:46:59.929453 140497615652608 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.01080313790589571, loss=0.03897519037127495
I0502 07:47:24.384585 140497624045312 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.016697216778993607, loss=0.039369136095047
I0502 07:47:48.719470 140497615652608 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.013019194826483727, loss=0.04324319586157799
I0502 07:48:03.035178 140683398973248 spec.py:298] Evaluating on the training split.
I0502 07:49:18.518923 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 07:49:21.149336 140683398973248 spec.py:326] Evaluating on the test split.
I0502 07:49:23.680272 140683398973248 submission_runner.py:415] Time since start: 2490.75s, 	Step: 6860, 	{'train/accuracy': 0.9889097809791565, 'train/loss': 0.03802445903420448, 'train/mean_average_precision': 0.24066412949784682, 'validation/accuracy': 0.9859138131141663, 'validation/loss': 0.04768490791320801, 'validation/mean_average_precision': 0.20147935980219742, 'validation/num_examples': 43793, 'test/accuracy': 0.9849725961685181, 'test/loss': 0.050206273794174194, 'test/mean_average_precision': 0.20930690162117496, 'test/num_examples': 43793, 'score': 1707.7387595176697, 'total_duration': 2490.7479898929596, 'accumulated_submission_time': 1707.7387595176697, 'accumulated_eval_time': 782.804981470108, 'accumulated_logging_time': 0.14748024940490723}
I0502 07:49:23.688572 140497624045312 logging_writer.py:48] [6860] accumulated_eval_time=782.804981, accumulated_logging_time=0.147480, accumulated_submission_time=1707.738760, global_step=6860, preemption_count=0, score=1707.738760, test/accuracy=0.984973, test/loss=0.050206, test/mean_average_precision=0.209307, test/num_examples=43793, total_duration=2490.747990, train/accuracy=0.988910, train/loss=0.038024, train/mean_average_precision=0.240664, validation/accuracy=0.985914, validation/loss=0.047685, validation/mean_average_precision=0.201479, validation/num_examples=43793
I0502 07:49:33.815846 140497615652608 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.013409300707280636, loss=0.040609922260046005
I0502 07:49:58.184760 140497624045312 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.012637009844183922, loss=0.04277679696679115
I0502 07:50:22.562772 140497615652608 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0101271141320467, loss=0.04028407484292984
I0502 07:50:46.953681 140497624045312 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.012434562668204308, loss=0.04119137302041054
I0502 07:51:11.277875 140497615652608 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.014689715579152107, loss=0.03894089162349701
I0502 07:51:35.596380 140497624045312 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.019571425393223763, loss=0.03822370246052742
I0502 07:52:00.050436 140497615652608 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.013304516673088074, loss=0.03921329230070114
I0502 07:52:24.722551 140497624045312 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.010507318191230297, loss=0.03563985228538513
I0502 07:52:49.139043 140497615652608 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.019415711984038353, loss=0.038565706461668015
I0502 07:53:13.664394 140497624045312 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.012915559113025665, loss=0.04040015861392021
I0502 07:53:23.913384 140683398973248 spec.py:298] Evaluating on the training split.
I0502 07:54:37.926475 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 07:54:40.570815 140683398973248 spec.py:326] Evaluating on the test split.
I0502 07:54:43.147275 140683398973248 submission_runner.py:415] Time since start: 2810.22s, 	Step: 7842, 	{'train/accuracy': 0.989083468914032, 'train/loss': 0.036894842982292175, 'train/mean_average_precision': 0.28643023980085325, 'validation/accuracy': 0.9858862161636353, 'validation/loss': 0.04791201651096344, 'validation/mean_average_precision': 0.22152284390865518, 'validation/num_examples': 43793, 'test/accuracy': 0.9850155711174011, 'test/loss': 0.05063839256763458, 'test/mean_average_precision': 0.22971617833238123, 'test/num_examples': 43793, 'score': 1947.9461119174957, 'total_duration': 2810.215007543564, 'accumulated_submission_time': 1947.9461119174957, 'accumulated_eval_time': 862.0388369560242, 'accumulated_logging_time': 0.16517877578735352}
I0502 07:54:43.155586 140497615652608 logging_writer.py:48] [7842] accumulated_eval_time=862.038837, accumulated_logging_time=0.165179, accumulated_submission_time=1947.946112, global_step=7842, preemption_count=0, score=1947.946112, test/accuracy=0.985016, test/loss=0.050638, test/mean_average_precision=0.229716, test/num_examples=43793, total_duration=2810.215008, train/accuracy=0.989083, train/loss=0.036895, train/mean_average_precision=0.286430, validation/accuracy=0.985886, validation/loss=0.047912, validation/mean_average_precision=0.221523, validation/num_examples=43793
I0502 07:54:57.465679 140497624045312 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.010416095145046711, loss=0.040454570204019547
I0502 07:55:22.027773 140497615652608 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.011923321522772312, loss=0.039365023374557495
I0502 07:55:46.285410 140497624045312 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.01785076968371868, loss=0.034247927367687225
I0502 07:56:10.544857 140497615652608 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.01008726004511118, loss=0.03887609764933586
I0502 07:56:35.091004 140497624045312 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.01373974047601223, loss=0.04053940996527672
I0502 07:56:59.398834 140497615652608 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.015151437371969223, loss=0.04231167584657669
I0502 07:57:23.907293 140497624045312 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.014310385100543499, loss=0.03899557143449783
I0502 07:57:48.330411 140497615652608 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.010909540578722954, loss=0.03879611939191818
I0502 07:58:12.623974 140497624045312 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.013912995345890522, loss=0.03639904782176018
I0502 07:58:37.070837 140497615652608 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.010695458389818668, loss=0.04024745896458626
I0502 07:58:43.208023 140683398973248 spec.py:298] Evaluating on the training split.
I0502 07:59:59.261991 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 08:00:01.934629 140683398973248 spec.py:326] Evaluating on the test split.
I0502 08:00:04.454374 140683398973248 submission_runner.py:415] Time since start: 3131.52s, 	Step: 8826, 	{'train/accuracy': 0.9899046421051025, 'train/loss': 0.034371137619018555, 'train/mean_average_precision': 0.3179118631545889, 'validation/accuracy': 0.9864760637283325, 'validation/loss': 0.045547064393758774, 'validation/mean_average_precision': 0.23274324353300038, 'validation/num_examples': 43793, 'test/accuracy': 0.9854986667633057, 'test/loss': 0.048399895429611206, 'test/mean_average_precision': 0.22456527788453515, 'test/num_examples': 43793, 'score': 2187.9808366298676, 'total_duration': 3131.5221059322357, 'accumulated_submission_time': 2187.9808366298676, 'accumulated_eval_time': 943.28515458107, 'accumulated_logging_time': 0.1830129623413086}
I0502 08:00:04.462775 140497624045312 logging_writer.py:48] [8826] accumulated_eval_time=943.285155, accumulated_logging_time=0.183013, accumulated_submission_time=2187.980837, global_step=8826, preemption_count=0, score=2187.980837, test/accuracy=0.985499, test/loss=0.048400, test/mean_average_precision=0.224565, test/num_examples=43793, total_duration=3131.522106, train/accuracy=0.989905, train/loss=0.034371, train/mean_average_precision=0.317912, validation/accuracy=0.986476, validation/loss=0.045547, validation/mean_average_precision=0.232743, validation/num_examples=43793
I0502 08:00:22.904739 140497615652608 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.013727033510804176, loss=0.037053510546684265
I0502 08:00:47.434626 140497624045312 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.016449816524982452, loss=0.03746563941240311
I0502 08:01:11.846678 140497615652608 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.015602469444274902, loss=0.03906897455453873
I0502 08:01:36.348373 140497624045312 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.016318954527378082, loss=0.04153793677687645
I0502 08:02:01.045714 140497615652608 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.014466235414147377, loss=0.04042195528745651
I0502 08:02:25.678836 140497624045312 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.013830180279910564, loss=0.04033437743782997
I0502 08:02:50.075862 140497615652608 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.012918082065880299, loss=0.039328064769506454
I0502 08:03:14.414379 140497624045312 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.013694639317691326, loss=0.038105662912130356
I0502 08:03:38.904482 140497615652608 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.013849442824721336, loss=0.03683425113558769
I0502 08:04:03.318166 140497624045312 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.017172804102301598, loss=0.038871679455041885
I0502 08:04:04.523478 140683398973248 spec.py:298] Evaluating on the training split.
I0502 08:05:18.403517 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 08:05:21.014566 140683398973248 spec.py:326] Evaluating on the test split.
I0502 08:05:23.530731 140683398973248 submission_runner.py:415] Time since start: 3450.60s, 	Step: 9806, 	{'train/accuracy': 0.9899131655693054, 'train/loss': 0.033645689487457275, 'train/mean_average_precision': 0.33145553704432307, 'validation/accuracy': 0.986379861831665, 'validation/loss': 0.04573700204491615, 'validation/mean_average_precision': 0.2430024575732781, 'validation/num_examples': 43793, 'test/accuracy': 0.9855079054832458, 'test/loss': 0.048607129603624344, 'test/mean_average_precision': 0.23793261541999092, 'test/num_examples': 43793, 'score': 2428.0239713191986, 'total_duration': 3450.598445415497, 'accumulated_submission_time': 2428.0239713191986, 'accumulated_eval_time': 1022.2923521995544, 'accumulated_logging_time': 0.20095372200012207}
I0502 08:05:23.539256 140497615652608 logging_writer.py:48] [9806] accumulated_eval_time=1022.292352, accumulated_logging_time=0.200954, accumulated_submission_time=2428.023971, global_step=9806, preemption_count=0, score=2428.023971, test/accuracy=0.985508, test/loss=0.048607, test/mean_average_precision=0.237933, test/num_examples=43793, total_duration=3450.598445, train/accuracy=0.989913, train/loss=0.033646, train/mean_average_precision=0.331456, validation/accuracy=0.986380, validation/loss=0.045737, validation/mean_average_precision=0.243002, validation/num_examples=43793
I0502 08:05:46.471336 140497624045312 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.01379731297492981, loss=0.03852656111121178
I0502 08:06:10.481252 140497615652608 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.01756809838116169, loss=0.036820653825998306
I0502 08:06:34.440803 140497624045312 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.015153273940086365, loss=0.03958003222942352
I0502 08:06:58.478591 140497615652608 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.01320606004446745, loss=0.038946304470300674
I0502 08:07:22.640158 140497624045312 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.015339108183979988, loss=0.03712040185928345
I0502 08:07:46.752564 140497615652608 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.013988645747303963, loss=0.03829231485724449
I0502 08:08:10.758957 140497624045312 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.01572444848716259, loss=0.036633726209402084
I0502 08:08:35.047533 140497615652608 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.01485372893512249, loss=0.03779148310422897
I0502 08:08:59.260750 140497624045312 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.011823269538581371, loss=0.037761140614748
I0502 08:09:23.672938 140497615652608 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0142923379316926, loss=0.03482481837272644
I0502 08:09:23.678645 140683398973248 spec.py:298] Evaluating on the training split.
I0502 08:10:37.525404 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 08:10:40.130012 140683398973248 spec.py:326] Evaluating on the test split.
I0502 08:10:42.662732 140683398973248 submission_runner.py:415] Time since start: 3769.73s, 	Step: 10801, 	{'train/accuracy': 0.9900858402252197, 'train/loss': 0.033217012882232666, 'train/mean_average_precision': 0.35228701303266985, 'validation/accuracy': 0.9864931106567383, 'validation/loss': 0.04538772255182266, 'validation/mean_average_precision': 0.24848744987157703, 'validation/num_examples': 43793, 'test/accuracy': 0.9855812191963196, 'test/loss': 0.04825616255402565, 'test/mean_average_precision': 0.23906570387847584, 'test/num_examples': 43793, 'score': 2668.144742488861, 'total_duration': 3769.7304468154907, 'accumulated_submission_time': 2668.144742488861, 'accumulated_eval_time': 1101.276385307312, 'accumulated_logging_time': 0.21966195106506348}
I0502 08:10:42.671482 140497624045312 logging_writer.py:48] [10801] accumulated_eval_time=1101.276385, accumulated_logging_time=0.219662, accumulated_submission_time=2668.144742, global_step=10801, preemption_count=0, score=2668.144742, test/accuracy=0.985581, test/loss=0.048256, test/mean_average_precision=0.239066, test/num_examples=43793, total_duration=3769.730447, train/accuracy=0.990086, train/loss=0.033217, train/mean_average_precision=0.352287, validation/accuracy=0.986493, validation/loss=0.045388, validation/mean_average_precision=0.248487, validation/num_examples=43793
I0502 08:11:06.969814 140497615652608 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.014966682530939579, loss=0.03861817717552185
I0502 08:11:31.273157 140497624045312 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.012816371396183968, loss=0.03539837524294853
I0502 08:11:55.498033 140497615652608 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.024721985682845116, loss=0.034100960940122604
I0502 08:12:19.695129 140497624045312 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.017111610621213913, loss=0.03707879036664963
I0502 08:12:44.201962 140497615652608 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.021457990631461143, loss=0.03843054175376892
I0502 08:13:08.429603 140497624045312 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.017707599326968193, loss=0.038987573236227036
I0502 08:13:32.647122 140497615652608 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.015161455608904362, loss=0.03784891590476036
I0502 08:13:57.224873 140497624045312 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.017571207135915756, loss=0.035802211612463
I0502 08:14:21.700093 140497615652608 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.015542853623628616, loss=0.038809362798929214
I0502 08:14:42.792200 140683398973248 spec.py:298] Evaluating on the training split.
I0502 08:15:55.438114 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 08:15:58.062288 140683398973248 spec.py:326] Evaluating on the test split.
I0502 08:16:00.595646 140683398973248 submission_runner.py:415] Time since start: 4087.66s, 	Step: 11787, 	{'train/accuracy': 0.9903041124343872, 'train/loss': 0.032435737550258636, 'train/mean_average_precision': 0.36894511807677277, 'validation/accuracy': 0.9866258502006531, 'validation/loss': 0.044802285730838776, 'validation/mean_average_precision': 0.2560991406460121, 'validation/num_examples': 43793, 'test/accuracy': 0.9857736825942993, 'test/loss': 0.047646913677453995, 'test/mean_average_precision': 0.2442085654075954, 'test/num_examples': 43793, 'score': 2908.2464261054993, 'total_duration': 4087.6633594036102, 'accumulated_submission_time': 2908.2464261054993, 'accumulated_eval_time': 1179.079777956009, 'accumulated_logging_time': 0.2390882968902588}
I0502 08:16:00.604250 140497624045312 logging_writer.py:48] [11787] accumulated_eval_time=1179.079778, accumulated_logging_time=0.239088, accumulated_submission_time=2908.246426, global_step=11787, preemption_count=0, score=2908.246426, test/accuracy=0.985774, test/loss=0.047647, test/mean_average_precision=0.244209, test/num_examples=43793, total_duration=4087.663359, train/accuracy=0.990304, train/loss=0.032436, train/mean_average_precision=0.368945, validation/accuracy=0.986626, validation/loss=0.044802, validation/mean_average_precision=0.256099, validation/num_examples=43793
I0502 08:16:04.059955 140497615652608 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.015203059650957584, loss=0.03576604276895523
I0502 08:16:28.384100 140497624045312 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.016369953751564026, loss=0.03442193940281868
I0502 08:16:52.360901 140683398973248 spec.py:298] Evaluating on the training split.
I0502 08:18:06.074673 140683398973248 spec.py:310] Evaluating on the validation split.
I0502 08:18:08.664427 140683398973248 spec.py:326] Evaluating on the test split.
I0502 08:18:11.463617 140683398973248 submission_runner.py:415] Time since start: 4218.53s, 	Step: 12000, 	{'train/accuracy': 0.9903393387794495, 'train/loss': 0.03262124955654144, 'train/mean_average_precision': 0.36705907030327856, 'validation/accuracy': 0.9866611361503601, 'validation/loss': 0.04491744562983513, 'validation/mean_average_precision': 0.2569022945845512, 'validation/num_examples': 43793, 'test/accuracy': 0.9856759905815125, 'test/loss': 0.04791191220283508, 'test/mean_average_precision': 0.24384239303750707, 'test/num_examples': 43793, 'score': 2959.9918093681335, 'total_duration': 4218.531329631805, 'accumulated_submission_time': 2959.9918093681335, 'accumulated_eval_time': 1258.1824400424957, 'accumulated_logging_time': 0.2569239139556885}
I0502 08:18:11.472552 140497615652608 logging_writer.py:48] [12000] accumulated_eval_time=1258.182440, accumulated_logging_time=0.256924, accumulated_submission_time=2959.991809, global_step=12000, preemption_count=0, score=2959.991809, test/accuracy=0.985676, test/loss=0.047912, test/mean_average_precision=0.243842, test/num_examples=43793, total_duration=4218.531330, train/accuracy=0.990339, train/loss=0.032621, train/mean_average_precision=0.367059, validation/accuracy=0.986661, validation/loss=0.044917, validation/mean_average_precision=0.256902, validation/num_examples=43793
I0502 08:18:11.489359 140497624045312 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2959.991809
I0502 08:18:11.516124 140683398973248 checkpoints.py:356] Saving checkpoint at step: 12000
I0502 08:18:11.618251 140683398973248 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_sam/ogbg_jax/trial_1/checkpoint_12000
I0502 08:18:11.618499 140683398973248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_sam/ogbg_jax/trial_1/checkpoint_12000.
I0502 08:18:11.762319 140683398973248 submission_runner.py:578] Tuning trial 1/1
I0502 08:18:11.762534 140683398973248 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0502 08:18:11.763982 140683398973248 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5173549056053162, 'train/loss': 0.7158744931221008, 'train/mean_average_precision': 0.023710575089221732, 'validation/accuracy': 0.5170056223869324, 'validation/loss': 0.7176371216773987, 'validation/mean_average_precision': 0.027690453479813613, 'validation/num_examples': 43793, 'test/accuracy': 0.516727089881897, 'test/loss': 0.7184858322143555, 'test/mean_average_precision': 0.02788676778431351, 'test/num_examples': 43793, 'score': 26.96721863746643, 'total_duration': 248.38407015800476, 'accumulated_submission_time': 26.96721863746643, 'accumulated_eval_time': 221.4166853427887, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (984, {'train/accuracy': 0.986725926399231, 'train/loss': 0.05291123688220978, 'train/mean_average_precision': 0.050136209216132245, 'validation/accuracy': 0.9841341972351074, 'validation/loss': 0.06234235316514969, 'validation/mean_average_precision': 0.05045361501301569, 'validation/num_examples': 43793, 'test/accuracy': 0.9831517338752747, 'test/loss': 0.06557247042655945, 'test/mean_average_precision': 0.05044444737652363, 'test/num_examples': 43793, 'score': 267.01587867736816, 'total_duration': 568.6858558654785, 'accumulated_submission_time': 267.01587867736816, 'accumulated_eval_time': 301.63297176361084, 'accumulated_logging_time': 0.028897523880004883, 'global_step': 984, 'preemption_count': 0}), (1965, {'train/accuracy': 0.987380862236023, 'train/loss': 0.046178754419088364, 'train/mean_average_precision': 0.1144131771546106, 'validation/accuracy': 0.984714686870575, 'validation/loss': 0.05539776012301445, 'validation/mean_average_precision': 0.11248845039475246, 'validation/num_examples': 43793, 'test/accuracy': 0.983755350112915, 'test/loss': 0.05860890820622444, 'test/mean_average_precision': 0.11112497035093982, 'test/num_examples': 43793, 'score': 507.0436429977417, 'total_duration': 888.8684153556824, 'accumulated_submission_time': 507.0436429977417, 'accumulated_eval_time': 381.7624599933624, 'accumulated_logging_time': 0.04616498947143555, 'global_step': 1965, 'preemption_count': 0}), (2952, {'train/accuracy': 0.9878122806549072, 'train/loss': 0.04370756074786186, 'train/mean_average_precision': 0.14563178902673712, 'validation/accuracy': 0.9849497079849243, 'validation/loss': 0.05291275680065155, 'validation/mean_average_precision': 0.13967857535007894, 'validation/num_examples': 43793, 'test/accuracy': 0.9839696884155273, 'test/loss': 0.055898476392030716, 'test/mean_average_precision': 0.13570694815249312, 'test/num_examples': 43793, 'score': 747.2290349006653, 'total_duration': 1208.52046084404, 'accumulated_submission_time': 747.2290349006653, 'accumulated_eval_time': 461.2016849517822, 'accumulated_logging_time': 0.06530165672302246, 'global_step': 2952, 'preemption_count': 0}), (3940, {'train/accuracy': 0.9877329468727112, 'train/loss': 0.04278608039021492, 'train/mean_average_precision': 0.17925923264709687, 'validation/accuracy': 0.9849838018417358, 'validation/loss': 0.05209891498088837, 'validation/mean_average_precision': 0.1580512821861959, 'validation/num_examples': 43793, 'test/accuracy': 0.9840611219406128, 'test/loss': 0.05499963462352753, 'test/mean_average_precision': 0.15534532262188894, 'test/num_examples': 43793, 'score': 987.3621923923492, 'total_duration': 1530.2523148059845, 'accumulated_submission_time': 987.3621923923492, 'accumulated_eval_time': 542.7747147083282, 'accumulated_logging_time': 0.0829007625579834, 'global_step': 3940, 'preemption_count': 0}), (4902, {'train/accuracy': 0.988550066947937, 'train/loss': 0.04023940488696098, 'train/mean_average_precision': 0.2137303396869934, 'validation/accuracy': 0.9855801463127136, 'validation/loss': 0.04921437054872513, 'validation/mean_average_precision': 0.1782463028322352, 'validation/num_examples': 43793, 'test/accuracy': 0.9847114682197571, 'test/loss': 0.051728256046772, 'test/mean_average_precision': 0.17701818740163777, 'test/num_examples': 43793, 'score': 1227.494402885437, 'total_duration': 1849.7083630561829, 'accumulated_submission_time': 1227.494402885437, 'accumulated_eval_time': 622.0703899860382, 'accumulated_logging_time': 0.10292553901672363, 'global_step': 4902, 'preemption_count': 0}), (5874, {'train/accuracy': 0.9886568188667297, 'train/loss': 0.03886275738477707, 'train/mean_average_precision': 0.24157015534491993, 'validation/accuracy': 0.9856288433074951, 'validation/loss': 0.04835035651922226, 'validation/mean_average_precision': 0.19072960687598983, 'validation/num_examples': 43793, 'test/accuracy': 0.9847897887229919, 'test/loss': 0.050856299698352814, 'test/mean_average_precision': 0.19265867445013302, 'test/num_examples': 43793, 'score': 1467.6948227882385, 'total_duration': 2170.032426595688, 'accumulated_submission_time': 1467.6948227882385, 'accumulated_eval_time': 702.1599357128143, 'accumulated_logging_time': 0.12876105308532715, 'global_step': 5874, 'preemption_count': 0}), (6860, {'train/accuracy': 0.9889097809791565, 'train/loss': 0.03802445903420448, 'train/mean_average_precision': 0.24066412949784682, 'validation/accuracy': 0.9859138131141663, 'validation/loss': 0.04768490791320801, 'validation/mean_average_precision': 0.20147935980219742, 'validation/num_examples': 43793, 'test/accuracy': 0.9849725961685181, 'test/loss': 0.050206273794174194, 'test/mean_average_precision': 0.20930690162117496, 'test/num_examples': 43793, 'score': 1707.7387595176697, 'total_duration': 2490.7479898929596, 'accumulated_submission_time': 1707.7387595176697, 'accumulated_eval_time': 782.804981470108, 'accumulated_logging_time': 0.14748024940490723, 'global_step': 6860, 'preemption_count': 0}), (7842, {'train/accuracy': 0.989083468914032, 'train/loss': 0.036894842982292175, 'train/mean_average_precision': 0.28643023980085325, 'validation/accuracy': 0.9858862161636353, 'validation/loss': 0.04791201651096344, 'validation/mean_average_precision': 0.22152284390865518, 'validation/num_examples': 43793, 'test/accuracy': 0.9850155711174011, 'test/loss': 0.05063839256763458, 'test/mean_average_precision': 0.22971617833238123, 'test/num_examples': 43793, 'score': 1947.9461119174957, 'total_duration': 2810.215007543564, 'accumulated_submission_time': 1947.9461119174957, 'accumulated_eval_time': 862.0388369560242, 'accumulated_logging_time': 0.16517877578735352, 'global_step': 7842, 'preemption_count': 0}), (8826, {'train/accuracy': 0.9899046421051025, 'train/loss': 0.034371137619018555, 'train/mean_average_precision': 0.3179118631545889, 'validation/accuracy': 0.9864760637283325, 'validation/loss': 0.045547064393758774, 'validation/mean_average_precision': 0.23274324353300038, 'validation/num_examples': 43793, 'test/accuracy': 0.9854986667633057, 'test/loss': 0.048399895429611206, 'test/mean_average_precision': 0.22456527788453515, 'test/num_examples': 43793, 'score': 2187.9808366298676, 'total_duration': 3131.5221059322357, 'accumulated_submission_time': 2187.9808366298676, 'accumulated_eval_time': 943.28515458107, 'accumulated_logging_time': 0.1830129623413086, 'global_step': 8826, 'preemption_count': 0}), (9806, {'train/accuracy': 0.9899131655693054, 'train/loss': 0.033645689487457275, 'train/mean_average_precision': 0.33145553704432307, 'validation/accuracy': 0.986379861831665, 'validation/loss': 0.04573700204491615, 'validation/mean_average_precision': 0.2430024575732781, 'validation/num_examples': 43793, 'test/accuracy': 0.9855079054832458, 'test/loss': 0.048607129603624344, 'test/mean_average_precision': 0.23793261541999092, 'test/num_examples': 43793, 'score': 2428.0239713191986, 'total_duration': 3450.598445415497, 'accumulated_submission_time': 2428.0239713191986, 'accumulated_eval_time': 1022.2923521995544, 'accumulated_logging_time': 0.20095372200012207, 'global_step': 9806, 'preemption_count': 0}), (10801, {'train/accuracy': 0.9900858402252197, 'train/loss': 0.033217012882232666, 'train/mean_average_precision': 0.35228701303266985, 'validation/accuracy': 0.9864931106567383, 'validation/loss': 0.04538772255182266, 'validation/mean_average_precision': 0.24848744987157703, 'validation/num_examples': 43793, 'test/accuracy': 0.9855812191963196, 'test/loss': 0.04825616255402565, 'test/mean_average_precision': 0.23906570387847584, 'test/num_examples': 43793, 'score': 2668.144742488861, 'total_duration': 3769.7304468154907, 'accumulated_submission_time': 2668.144742488861, 'accumulated_eval_time': 1101.276385307312, 'accumulated_logging_time': 0.21966195106506348, 'global_step': 10801, 'preemption_count': 0}), (11787, {'train/accuracy': 0.9903041124343872, 'train/loss': 0.032435737550258636, 'train/mean_average_precision': 0.36894511807677277, 'validation/accuracy': 0.9866258502006531, 'validation/loss': 0.044802285730838776, 'validation/mean_average_precision': 0.2560991406460121, 'validation/num_examples': 43793, 'test/accuracy': 0.9857736825942993, 'test/loss': 0.047646913677453995, 'test/mean_average_precision': 0.2442085654075954, 'test/num_examples': 43793, 'score': 2908.2464261054993, 'total_duration': 4087.6633594036102, 'accumulated_submission_time': 2908.2464261054993, 'accumulated_eval_time': 1179.079777956009, 'accumulated_logging_time': 0.2390882968902588, 'global_step': 11787, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9903393387794495, 'train/loss': 0.03262124955654144, 'train/mean_average_precision': 0.36705907030327856, 'validation/accuracy': 0.9866611361503601, 'validation/loss': 0.04491744562983513, 'validation/mean_average_precision': 0.2569022945845512, 'validation/num_examples': 43793, 'test/accuracy': 0.9856759905815125, 'test/loss': 0.04791191220283508, 'test/mean_average_precision': 0.24384239303750707, 'test/num_examples': 43793, 'score': 2959.9918093681335, 'total_duration': 4218.531329631805, 'accumulated_submission_time': 2959.9918093681335, 'accumulated_eval_time': 1258.1824400424957, 'accumulated_logging_time': 0.2569239139556885, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0502 08:18:11.764139 140683398973248 submission_runner.py:581] Timing: 2959.9918093681335
I0502 08:18:11.764190 140683398973248 submission_runner.py:582] ====================
I0502 08:18:11.764318 140683398973248 submission_runner.py:645] Final ogbg score: 2959.9918093681335
