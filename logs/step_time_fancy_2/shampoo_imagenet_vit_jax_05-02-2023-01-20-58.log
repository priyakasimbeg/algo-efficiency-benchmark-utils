python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=baselines/shampoo/jax/submission.py --tuning_search_space=baselines/shampoo/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_shampoo --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_05-02-2023-01-20-58.log
I0502 01:21:19.371486 139815280891712 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_vit_jax.
I0502 01:21:19.471197 139815280891712 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 01:21:20.345855 139815280891712 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0502 01:21:20.346521 139815280891712 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 01:21:20.351606 139815280891712 submission_runner.py:538] Using RNG seed 3367467813
I0502 01:21:22.989055 139815280891712 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 01:21:22.989254 139815280891712 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_vit_jax/trial_1.
I0502 01:21:22.989759 139815280891712 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_vit_jax/trial_1/hparams.json.
I0502 01:21:23.119900 139815280891712 submission_runner.py:241] Initializing dataset.
I0502 01:21:23.137883 139815280891712 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0502 01:21:23.153172 139815280891712 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0502 01:21:23.153284 139815280891712 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0502 01:21:23.410969 139815280891712 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0502 01:21:30.126216 139815280891712 submission_runner.py:248] Initializing model.
I0502 01:21:41.232694 139815280891712 submission_runner.py:258] Initializing optimizer.
I0502 01:21:55.658499 139815280891712 submission_runner.py:265] Initializing metrics bundle.
I0502 01:21:55.658724 139815280891712 submission_runner.py:282] Initializing checkpoint and logger.
I0502 01:21:55.659801 139815280891712 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0502 01:21:56.477400 139815280891712 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_vit_jax/trial_1/meta_data_0.json.
I0502 01:21:56.479443 139815280891712 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_vit_jax/trial_1/flags_0.json.
I0502 01:21:56.484491 139815280891712 submission_runner.py:318] Starting training loop.
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:812: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  matrix = matrix.astype(_MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:813: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  alpha = jnp.asarray(-1.0 / p, _MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:814: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in eye is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  identity = jnp.eye(matrix_size, dtype=_MAT_INV_PTH_ROOT_DTYPE)
I0502 01:26:49.508245 139637137864448 logging_writer.py:48] [0] global_step=0, grad_norm=0.31804782152175903, loss=6.907756805419922
I0502 01:26:49.587987 139815280891712 spec.py:298] Evaluating on the training split.
I0502 01:26:49.605424 139815280891712 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0502 01:26:49.612282 139815280891712 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0502 01:26:49.612391 139815280891712 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0502 01:26:49.673352 139815280891712 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0502 01:27:08.111014 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 01:27:08.117561 139815280891712 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0502 01:27:08.133684 139815280891712 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0502 01:27:08.133979 139815280891712 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0502 01:27:08.185281 139815280891712 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0502 01:27:28.124138 139815280891712 spec.py:326] Evaluating on the test split.
I0502 01:27:28.134937 139815280891712 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0502 01:27:28.139589 139815280891712 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0502 01:27:28.171284 139815280891712 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0502 01:27:39.367333 139815280891712 submission_runner.py:415] Time since start: 342.88s, 	Step: 1, 	{'train/accuracy': 0.0009570312104187906, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 293.10331749916077, 'total_duration': 342.88277888298035, 'accumulated_submission_time': 293.10331749916077, 'accumulated_eval_time': 49.77930545806885, 'accumulated_logging_time': 0}
I0502 01:27:39.384001 139584306386688 logging_writer.py:48] [1] accumulated_eval_time=49.779305, accumulated_logging_time=0, accumulated_submission_time=293.103317, global_step=1, preemption_count=0, score=293.103317, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=342.882779, train/accuracy=0.000957, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0502 01:34:02.604434 139633232697088 logging_writer.py:48] [100] global_step=100, grad_norm=0.4630367159843445, loss=6.85402250289917
I0502 01:34:41.789769 139815280891712 spec.py:298] Evaluating on the training split.
I0502 01:34:48.235587 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 01:34:54.866721 139815280891712 spec.py:326] Evaluating on the test split.
I0502 01:34:56.632541 139815280891712 submission_runner.py:415] Time since start: 780.15s, 	Step: 141, 	{'train/accuracy': 0.005644530989229679, 'train/loss': 6.78612756729126, 'validation/accuracy': 0.0047599999234080315, 'validation/loss': 6.79502534866333, 'validation/num_examples': 50000, 'test/accuracy': 0.004800000227987766, 'test/loss': 6.804804801940918, 'test/num_examples': 10000, 'score': 715.4983599185944, 'total_duration': 780.1479647159576, 'accumulated_submission_time': 715.4983599185944, 'accumulated_eval_time': 64.62202835083008, 'accumulated_logging_time': 0.02530217170715332}
I0502 01:34:56.643423 139584457389824 logging_writer.py:48] [141] accumulated_eval_time=64.622028, accumulated_logging_time=0.025302, accumulated_submission_time=715.498360, global_step=141, preemption_count=0, score=715.498360, test/accuracy=0.004800, test/loss=6.804805, test/num_examples=10000, total_duration=780.147965, train/accuracy=0.005645, train/loss=6.786128, validation/accuracy=0.004760, validation/loss=6.795025, validation/num_examples=50000
I0502 01:35:55.771694 139584465782528 logging_writer.py:48] [200] global_step=200, grad_norm=0.44864702224731445, loss=6.8360772132873535
I0502 01:37:33.555036 139584457389824 logging_writer.py:48] [300] global_step=300, grad_norm=0.8791893720626831, loss=6.700736045837402
I0502 01:39:11.517160 139584465782528 logging_writer.py:48] [400] global_step=400, grad_norm=1.1689122915267944, loss=6.640652179718018
I0502 01:40:50.428868 139584457389824 logging_writer.py:48] [500] global_step=500, grad_norm=0.9546964764595032, loss=6.540663719177246
I0502 01:41:57.209474 139815280891712 spec.py:298] Evaluating on the training split.
I0502 01:42:03.637124 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 01:42:10.396988 139815280891712 spec.py:326] Evaluating on the test split.
I0502 01:42:12.058972 139815280891712 submission_runner.py:415] Time since start: 1215.57s, 	Step: 571, 	{'train/accuracy': 0.026992186903953552, 'train/loss': 6.2548956871032715, 'validation/accuracy': 0.024620000272989273, 'validation/loss': 6.267775058746338, 'validation/num_examples': 50000, 'test/accuracy': 0.019500000402331352, 'test/loss': 6.330530166625977, 'test/num_examples': 10000, 'score': 1136.0484211444855, 'total_duration': 1215.5743610858917, 'accumulated_submission_time': 1136.0484211444855, 'accumulated_eval_time': 79.47144794464111, 'accumulated_logging_time': 0.046309709548950195}
I0502 01:42:12.074061 139584465782528 logging_writer.py:48] [571] accumulated_eval_time=79.471448, accumulated_logging_time=0.046310, accumulated_submission_time=1136.048421, global_step=571, preemption_count=0, score=1136.048421, test/accuracy=0.019500, test/loss=6.330530, test/num_examples=10000, total_duration=1215.574361, train/accuracy=0.026992, train/loss=6.254896, validation/accuracy=0.024620, validation/loss=6.267775, validation/num_examples=50000
I0502 01:42:45.420266 139584457389824 logging_writer.py:48] [600] global_step=600, grad_norm=0.8324894905090332, loss=6.667830944061279
I0502 01:44:24.676921 139584465782528 logging_writer.py:48] [700] global_step=700, grad_norm=1.0981065034866333, loss=6.370298385620117
I0502 01:46:02.995848 139584457389824 logging_writer.py:48] [800] global_step=800, grad_norm=1.5024551153182983, loss=6.28455114364624
I0502 01:47:41.729475 139584465782528 logging_writer.py:48] [900] global_step=900, grad_norm=1.4124547243118286, loss=6.2108235359191895
I0502 01:49:12.263601 139815280891712 spec.py:298] Evaluating on the training split.
I0502 01:49:18.679949 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 01:49:25.499497 139815280891712 spec.py:326] Evaluating on the test split.
I0502 01:49:27.163506 139815280891712 submission_runner.py:415] Time since start: 1650.68s, 	Step: 997, 	{'train/accuracy': 0.05292968451976776, 'train/loss': 5.721033573150635, 'validation/accuracy': 0.049379996955394745, 'validation/loss': 5.746005535125732, 'validation/num_examples': 50000, 'test/accuracy': 0.038700003176927567, 'test/loss': 5.872519016265869, 'test/num_examples': 10000, 'score': 1556.218989610672, 'total_duration': 1650.6789247989655, 'accumulated_submission_time': 1556.218989610672, 'accumulated_eval_time': 94.37130904197693, 'accumulated_logging_time': 0.07445979118347168}
I0502 01:49:27.174036 139584457389824 logging_writer.py:48] [997] accumulated_eval_time=94.371309, accumulated_logging_time=0.074460, accumulated_submission_time=1556.218990, global_step=997, preemption_count=0, score=1556.218990, test/accuracy=0.038700, test/loss=5.872519, test/num_examples=10000, total_duration=1650.678925, train/accuracy=0.052930, train/loss=5.721034, validation/accuracy=0.049380, validation/loss=5.746006, validation/num_examples=50000
I0502 01:49:36.140313 139584465782528 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.4177236557006836, loss=6.130033493041992
I0502 01:51:14.411556 139584457389824 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.207231044769287, loss=6.108361721038818
I0502 01:52:54.007455 139584465782528 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2570040225982666, loss=6.32538366317749
I0502 01:54:33.090655 139584457389824 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.1117802858352661, loss=6.575123310089111
I0502 01:56:12.091173 139584465782528 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.4788833856582642, loss=5.934486389160156
I0502 01:56:31.919492 139815280891712 spec.py:298] Evaluating on the training split.
I0502 01:56:38.360160 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 01:56:44.978178 139815280891712 spec.py:326] Evaluating on the test split.
I0502 01:56:46.647306 139815280891712 submission_runner.py:415] Time since start: 2090.16s, 	Step: 1421, 	{'train/accuracy': 0.07681640237569809, 'train/loss': 5.360898971557617, 'validation/accuracy': 0.0712599977850914, 'validation/loss': 5.396115303039551, 'validation/num_examples': 50000, 'test/accuracy': 0.05470000207424164, 'test/loss': 5.5779571533203125, 'test/num_examples': 10000, 'score': 1980.948757648468, 'total_duration': 2090.1627271175385, 'accumulated_submission_time': 1980.948757648468, 'accumulated_eval_time': 109.09906697273254, 'accumulated_logging_time': 0.09495401382446289}
I0502 01:56:46.657665 139584457389824 logging_writer.py:48] [1421] accumulated_eval_time=109.099067, accumulated_logging_time=0.094954, accumulated_submission_time=1980.948758, global_step=1421, preemption_count=0, score=1980.948758, test/accuracy=0.054700, test/loss=5.577957, test/num_examples=10000, total_duration=2090.162727, train/accuracy=0.076816, train/loss=5.360899, validation/accuracy=0.071260, validation/loss=5.396115, validation/num_examples=50000
I0502 01:58:06.068782 139584465782528 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0825523138046265, loss=6.626089096069336
I0502 01:59:45.735524 139584457389824 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.515891432762146, loss=5.82059383392334
I0502 02:01:26.067134 139584465782528 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.583392858505249, loss=5.7666544914245605
I0502 02:03:04.895070 139584457389824 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.3228052854537964, loss=6.051511764526367
I0502 02:03:46.851254 139815280891712 spec.py:298] Evaluating on the training split.
I0502 02:03:53.308492 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 02:04:00.154744 139815280891712 spec.py:326] Evaluating on the test split.
I0502 02:04:01.817682 139815280891712 submission_runner.py:415] Time since start: 2525.33s, 	Step: 1844, 	{'train/accuracy': 0.1064453125, 'train/loss': 5.026849746704102, 'validation/accuracy': 0.10269999504089355, 'validation/loss': 5.072316646575928, 'validation/num_examples': 50000, 'test/accuracy': 0.07840000092983246, 'test/loss': 5.320460319519043, 'test/num_examples': 10000, 'score': 2401.123947620392, 'total_duration': 2525.3330557346344, 'accumulated_submission_time': 2401.123947620392, 'accumulated_eval_time': 124.06539368629456, 'accumulated_logging_time': 0.11805987358093262}
I0502 02:04:01.832738 139584465782528 logging_writer.py:48] [1844] accumulated_eval_time=124.065394, accumulated_logging_time=0.118060, accumulated_submission_time=2401.123948, global_step=1844, preemption_count=0, score=2401.123948, test/accuracy=0.078400, test/loss=5.320460, test/num_examples=10000, total_duration=2525.333056, train/accuracy=0.106445, train/loss=5.026850, validation/accuracy=0.102700, validation/loss=5.072317, validation/num_examples=50000
I0502 02:04:59.496940 139584457389824 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.2565799951553345, loss=6.220907688140869
I0502 02:06:38.669921 139584465782528 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.3726541996002197, loss=5.689332962036133
I0502 02:08:17.672828 139584457389824 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.2464383840560913, loss=5.614124774932861
I0502 02:09:57.669126 139584465782528 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.1092361211776733, loss=6.161187648773193
I0502 02:11:02.255725 139815280891712 spec.py:298] Evaluating on the training split.
I0502 02:11:08.720024 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 02:11:15.426160 139815280891712 spec.py:326] Evaluating on the test split.
I0502 02:11:17.084989 139815280891712 submission_runner.py:415] Time since start: 2960.60s, 	Step: 2268, 	{'train/accuracy': 0.14701171219348907, 'train/loss': 4.67155647277832, 'validation/accuracy': 0.1361600011587143, 'validation/loss': 4.741461277008057, 'validation/num_examples': 50000, 'test/accuracy': 0.1072000041604042, 'test/loss': 5.021162509918213, 'test/num_examples': 10000, 'score': 2821.522177219391, 'total_duration': 2960.6004118919373, 'accumulated_submission_time': 2821.522177219391, 'accumulated_eval_time': 138.894606590271, 'accumulated_logging_time': 0.15212011337280273}
I0502 02:11:17.096465 139584457389824 logging_writer.py:48] [2268] accumulated_eval_time=138.894607, accumulated_logging_time=0.152120, accumulated_submission_time=2821.522177, global_step=2268, preemption_count=0, score=2821.522177, test/accuracy=0.107200, test/loss=5.021163, test/num_examples=10000, total_duration=2960.600412, train/accuracy=0.147012, train/loss=4.671556, validation/accuracy=0.136160, validation/loss=4.741461, validation/num_examples=50000
I0502 02:11:52.412901 139584465782528 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.6165887117385864, loss=5.51317024230957
I0502 02:13:31.511845 139584457389824 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.7858325242996216, loss=5.602587699890137
I0502 02:15:11.652980 139584465782528 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2958755493164062, loss=5.873764991760254
I0502 02:16:51.067721 139584457389824 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.3701863288879395, loss=5.3684186935424805
I0502 02:18:17.546987 139815280891712 spec.py:298] Evaluating on the training split.
I0502 02:18:24.002798 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 02:18:30.854558 139815280891712 spec.py:326] Evaluating on the test split.
I0502 02:18:32.515690 139815280891712 submission_runner.py:415] Time since start: 3396.03s, 	Step: 2690, 	{'train/accuracy': 0.18089842796325684, 'train/loss': 4.3392181396484375, 'validation/accuracy': 0.17189998924732208, 'validation/loss': 4.4079155921936035, 'validation/num_examples': 50000, 'test/accuracy': 0.12630000710487366, 'test/loss': 4.742824554443359, 'test/num_examples': 10000, 'score': 3241.9575579166412, 'total_duration': 3396.031106710434, 'accumulated_submission_time': 3241.9575579166412, 'accumulated_eval_time': 153.86326241493225, 'accumulated_logging_time': 0.17289447784423828}
I0502 02:18:32.526353 139584465782528 logging_writer.py:48] [2690] accumulated_eval_time=153.863262, accumulated_logging_time=0.172894, accumulated_submission_time=3241.957558, global_step=2690, preemption_count=0, score=3241.957558, test/accuracy=0.126300, test/loss=4.742825, test/num_examples=10000, total_duration=3396.031107, train/accuracy=0.180898, train/loss=4.339218, validation/accuracy=0.171900, validation/loss=4.407916, validation/num_examples=50000
I0502 02:18:49.157389 139584457389824 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.017561435699463, loss=6.122829437255859
I0502 02:20:27.713829 139584465782528 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.0142649412155151, loss=6.324579238891602
I0502 02:22:08.115622 139584457389824 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.26105797290802, loss=5.3156890869140625
I0502 02:23:47.593454 139584465782528 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.5357297658920288, loss=5.057147026062012
I0502 02:25:26.782557 139584457389824 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.238918662071228, loss=5.13363790512085
I0502 02:25:33.032805 139815280891712 spec.py:298] Evaluating on the training split.
I0502 02:25:39.448978 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 02:25:46.198570 139815280891712 spec.py:326] Evaluating on the test split.
I0502 02:25:47.864215 139815280891712 submission_runner.py:415] Time since start: 3831.38s, 	Step: 3110, 	{'train/accuracy': 0.2311132699251175, 'train/loss': 4.003273963928223, 'validation/accuracy': 0.20601999759674072, 'validation/loss': 4.125072479248047, 'validation/num_examples': 50000, 'test/accuracy': 0.16340000927448273, 'test/loss': 4.485968589782715, 'test/num_examples': 10000, 'score': 3662.4443802833557, 'total_duration': 3831.379652738571, 'accumulated_submission_time': 3662.4443802833557, 'accumulated_eval_time': 168.69462847709656, 'accumulated_logging_time': 0.19739818572998047}
I0502 02:25:47.874739 139584465782528 logging_writer.py:48] [3110] accumulated_eval_time=168.694628, accumulated_logging_time=0.197398, accumulated_submission_time=3662.444380, global_step=3110, preemption_count=0, score=3662.444380, test/accuracy=0.163400, test/loss=4.485969, test/num_examples=10000, total_duration=3831.379653, train/accuracy=0.231113, train/loss=4.003274, validation/accuracy=0.206020, validation/loss=4.125072, validation/num_examples=50000
I0502 02:27:20.701114 139584457389824 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.2091524600982666, loss=5.4985246658325195
I0502 02:28:58.816081 139584465782528 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.3649615049362183, loss=4.995649814605713
I0502 02:30:36.675437 139584457389824 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.4365638494491577, loss=5.389094352722168
I0502 02:32:14.415034 139584465782528 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.3023329973220825, loss=4.864479064941406
I0502 02:32:54.081881 139815280891712 spec.py:298] Evaluating on the training split.
I0502 02:33:00.527959 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 02:33:07.416642 139815280891712 spec.py:326] Evaluating on the test split.
I0502 02:33:09.099370 139815280891712 submission_runner.py:415] Time since start: 4272.61s, 	Step: 3541, 	{'train/accuracy': 0.2522656321525574, 'train/loss': 3.8324990272521973, 'validation/accuracy': 0.23201999068260193, 'validation/loss': 3.9364278316497803, 'validation/num_examples': 50000, 'test/accuracy': 0.17760001122951508, 'test/loss': 4.351043224334717, 'test/num_examples': 10000, 'score': 4088.634847640991, 'total_duration': 4272.614780902863, 'accumulated_submission_time': 4088.634847640991, 'accumulated_eval_time': 183.7120566368103, 'accumulated_logging_time': 0.21915936470031738}
I0502 02:33:09.113979 139584457389824 logging_writer.py:48] [3541] accumulated_eval_time=183.712057, accumulated_logging_time=0.219159, accumulated_submission_time=4088.634848, global_step=3541, preemption_count=0, score=4088.634848, test/accuracy=0.177600, test/loss=4.351043, test/num_examples=10000, total_duration=4272.614781, train/accuracy=0.252266, train/loss=3.832499, validation/accuracy=0.232020, validation/loss=3.936428, validation/num_examples=50000
I0502 02:34:08.159554 139584465782528 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.3145103454589844, loss=4.967822551727295
I0502 02:35:45.952698 139584457389824 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.4591730833053589, loss=5.3037238121032715
I0502 02:37:23.812690 139584465782528 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.1596086025238037, loss=5.117587089538574
I0502 02:39:01.612268 139584457389824 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9290202260017395, loss=6.01235294342041
I0502 02:40:09.532034 139815280891712 spec.py:298] Evaluating on the training split.
I0502 02:40:15.979314 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 02:40:22.703554 139815280891712 spec.py:326] Evaluating on the test split.
I0502 02:40:24.396426 139815280891712 submission_runner.py:415] Time since start: 4707.91s, 	Step: 3974, 	{'train/accuracy': 0.2874804735183716, 'train/loss': 3.583763599395752, 'validation/accuracy': 0.2658799886703491, 'validation/loss': 3.685100793838501, 'validation/num_examples': 50000, 'test/accuracy': 0.19780001044273376, 'test/loss': 4.143375873565674, 'test/num_examples': 10000, 'score': 4509.034639835358, 'total_duration': 4707.911815166473, 'accumulated_submission_time': 4509.034639835358, 'accumulated_eval_time': 198.57635688781738, 'accumulated_logging_time': 0.24656295776367188}
I0502 02:40:24.406910 139584465782528 logging_writer.py:48] [3974] accumulated_eval_time=198.576357, accumulated_logging_time=0.246563, accumulated_submission_time=4509.034640, global_step=3974, preemption_count=0, score=4509.034640, test/accuracy=0.197800, test/loss=4.143376, test/num_examples=10000, total_duration=4707.911815, train/accuracy=0.287480, train/loss=3.583764, validation/accuracy=0.265880, validation/loss=3.685101, validation/num_examples=50000
I0502 02:40:55.375616 139584457389824 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.2641897201538086, loss=4.828587532043457
I0502 02:42:34.390709 139584465782528 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.3387136459350586, loss=4.882195949554443
I0502 02:44:13.347976 139584457389824 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8622962236404419, loss=6.223542213439941
I0502 02:45:50.993940 139584465782528 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.256405234336853, loss=5.039762496948242
I0502 02:47:28.727749 139584457389824 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.2249245643615723, loss=4.862178802490234
I0502 02:47:28.779754 139815280891712 spec.py:298] Evaluating on the training split.
I0502 02:47:35.238384 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 02:47:42.540833 139815280891712 spec.py:326] Evaluating on the test split.
I0502 02:47:44.210647 139815280891712 submission_runner.py:415] Time since start: 5147.73s, 	Step: 4401, 	{'train/accuracy': 0.3308202922344208, 'train/loss': 3.318467617034912, 'validation/accuracy': 0.2913399934768677, 'validation/loss': 3.52135968208313, 'validation/num_examples': 50000, 'test/accuracy': 0.22050000727176666, 'test/loss': 3.993187189102173, 'test/num_examples': 10000, 'score': 4933.385240793228, 'total_duration': 5147.726042032242, 'accumulated_submission_time': 4933.385240793228, 'accumulated_eval_time': 214.00714874267578, 'accumulated_logging_time': 0.27379894256591797}
I0502 02:47:44.221383 139584465782528 logging_writer.py:48] [4401] accumulated_eval_time=214.007149, accumulated_logging_time=0.273799, accumulated_submission_time=4933.385241, global_step=4401, preemption_count=0, score=4933.385241, test/accuracy=0.220500, test/loss=3.993187, test/num_examples=10000, total_duration=5147.726042, train/accuracy=0.330820, train/loss=3.318468, validation/accuracy=0.291340, validation/loss=3.521360, validation/num_examples=50000
I0502 02:49:22.486293 139584457389824 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0233339071273804, loss=5.443284511566162
I0502 02:51:00.779404 139584465782528 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.0816434621810913, loss=6.0376877784729
I0502 02:52:38.692381 139584457389824 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.9703301191329956, loss=6.255398273468018
I0502 02:54:16.924074 139584465782528 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.9328944683074951, loss=6.178252696990967
I0502 02:54:44.785845 139815280891712 spec.py:298] Evaluating on the training split.
I0502 02:54:51.241290 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 02:54:58.184346 139815280891712 spec.py:326] Evaluating on the test split.
I0502 02:54:59.862490 139815280891712 submission_runner.py:415] Time since start: 5583.38s, 	Step: 4832, 	{'train/accuracy': 0.3401757776737213, 'train/loss': 3.2193191051483154, 'validation/accuracy': 0.3124399781227112, 'validation/loss': 3.3668107986450195, 'validation/num_examples': 50000, 'test/accuracy': 0.2419000118970871, 'test/loss': 3.8793532848358154, 'test/num_examples': 10000, 'score': 5353.934933662415, 'total_duration': 5583.377871274948, 'accumulated_submission_time': 5353.934933662415, 'accumulated_eval_time': 229.0836923122406, 'accumulated_logging_time': 0.293809175491333}
I0502 02:54:59.878019 139584457389824 logging_writer.py:48] [4832] accumulated_eval_time=229.083692, accumulated_logging_time=0.293809, accumulated_submission_time=5353.934934, global_step=4832, preemption_count=0, score=5353.934934, test/accuracy=0.241900, test/loss=3.879353, test/num_examples=10000, total_duration=5583.377871, train/accuracy=0.340176, train/loss=3.219319, validation/accuracy=0.312440, validation/loss=3.366811, validation/num_examples=50000
I0502 02:56:10.912728 139584465782528 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.257699966430664, loss=4.484123229980469
I0502 02:57:49.353435 139584457389824 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8553541898727417, loss=5.936394214630127
I0502 02:59:27.558888 139584465782528 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.2739336490631104, loss=4.527835845947266
I0502 03:01:05.480429 139584457389824 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.9839794635772705, loss=4.737827777862549
I0502 03:02:04.470809 139815280891712 spec.py:298] Evaluating on the training split.
I0502 03:02:10.921750 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 03:02:17.954408 139815280891712 spec.py:326] Evaluating on the test split.
I0502 03:02:19.629101 139815280891712 submission_runner.py:415] Time since start: 6023.14s, 	Step: 5261, 	{'train/accuracy': 0.36226561665534973, 'train/loss': 3.0730977058410645, 'validation/accuracy': 0.3337399959564209, 'validation/loss': 3.2248952388763428, 'validation/num_examples': 50000, 'test/accuracy': 0.25710001587867737, 'test/loss': 3.747987747192383, 'test/num_examples': 10000, 'score': 5778.510552883148, 'total_duration': 6023.14445233345, 'accumulated_submission_time': 5778.510552883148, 'accumulated_eval_time': 244.24185347557068, 'accumulated_logging_time': 0.32102108001708984}
I0502 03:02:19.641852 139584465782528 logging_writer.py:48] [5261] accumulated_eval_time=244.241853, accumulated_logging_time=0.321021, accumulated_submission_time=5778.510553, global_step=5261, preemption_count=0, score=5778.510553, test/accuracy=0.257100, test/loss=3.747988, test/num_examples=10000, total_duration=6023.144452, train/accuracy=0.362266, train/loss=3.073098, validation/accuracy=0.333740, validation/loss=3.224895, validation/num_examples=50000
I0502 03:02:59.384238 139584457389824 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8818368315696716, loss=6.0990705490112305
I0502 03:04:38.210777 139584465782528 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.9146168231964111, loss=5.857490539550781
I0502 03:06:17.381653 139584457389824 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.01859712600708, loss=5.203676223754883
I0502 03:07:55.605259 139584465782528 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.1864862442016602, loss=4.555569648742676
I0502 03:09:19.970166 139815280891712 spec.py:298] Evaluating on the training split.
I0502 03:09:26.399041 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 03:09:33.260374 139815280891712 spec.py:326] Evaluating on the test split.
I0502 03:09:34.919979 139815280891712 submission_runner.py:415] Time since start: 6458.44s, 	Step: 5689, 	{'train/accuracy': 0.37546873092651367, 'train/loss': 3.0548465251922607, 'validation/accuracy': 0.34417998790740967, 'validation/loss': 3.2012991905212402, 'validation/num_examples': 50000, 'test/accuracy': 0.2587999999523163, 'test/loss': 3.7431299686431885, 'test/num_examples': 10000, 'score': 6198.8203592300415, 'total_duration': 6458.435411453247, 'accumulated_submission_time': 6198.8203592300415, 'accumulated_eval_time': 259.19161653518677, 'accumulated_logging_time': 0.3468759059906006}
I0502 03:09:34.934496 139584457389824 logging_writer.py:48] [5689] accumulated_eval_time=259.191617, accumulated_logging_time=0.346876, accumulated_submission_time=6198.820359, global_step=5689, preemption_count=0, score=6198.820359, test/accuracy=0.258800, test/loss=3.743130, test/num_examples=10000, total_duration=6458.435411, train/accuracy=0.375469, train/loss=3.054847, validation/accuracy=0.344180, validation/loss=3.201299, validation/num_examples=50000
I0502 03:09:49.621078 139584465782528 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.9348919987678528, loss=5.011875629425049
I0502 03:11:28.921713 139584457389824 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.051738977432251, loss=4.807359218597412
I0502 03:13:08.071686 139584465782528 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.1172183752059937, loss=4.450972080230713
I0502 03:14:46.919405 139584457389824 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.0862858295440674, loss=4.446124076843262
I0502 03:16:25.224490 139584465782528 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.0847294330596924, loss=4.406047821044922
I0502 03:16:35.589702 139815280891712 spec.py:298] Evaluating on the training split.
I0502 03:16:42.042636 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 03:16:48.816569 139815280891712 spec.py:326] Evaluating on the test split.
I0502 03:16:50.492505 139815280891712 submission_runner.py:415] Time since start: 6894.01s, 	Step: 6116, 	{'train/accuracy': 0.3955078125, 'train/loss': 2.872520923614502, 'validation/accuracy': 0.3594599962234497, 'validation/loss': 3.0631237030029297, 'validation/num_examples': 50000, 'test/accuracy': 0.2775000035762787, 'test/loss': 3.627045154571533, 'test/num_examples': 10000, 'score': 6619.458873510361, 'total_duration': 6894.007937192917, 'accumulated_submission_time': 6619.458873510361, 'accumulated_eval_time': 274.09437251091003, 'accumulated_logging_time': 0.37261414527893066}
I0502 03:16:50.503540 139584457389824 logging_writer.py:48] [6116] accumulated_eval_time=274.094373, accumulated_logging_time=0.372614, accumulated_submission_time=6619.458874, global_step=6116, preemption_count=0, score=6619.458874, test/accuracy=0.277500, test/loss=3.627045, test/num_examples=10000, total_duration=6894.007937, train/accuracy=0.395508, train/loss=2.872521, validation/accuracy=0.359460, validation/loss=3.063124, validation/num_examples=50000
I0502 03:18:20.325451 139584465782528 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.0345321893692017, loss=4.431341171264648
I0502 03:20:00.068864 139584457389824 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.0244741439819336, loss=4.4154486656188965
I0502 03:21:40.042778 139584465782528 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.8550376296043396, loss=5.378568172454834
I0502 03:23:19.926501 139584457389824 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8069993257522583, loss=6.100441932678223
I0502 03:23:51.218227 139815280891712 spec.py:298] Evaluating on the training split.
I0502 03:23:57.685479 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 03:24:04.835376 139815280891712 spec.py:326] Evaluating on the test split.
I0502 03:24:06.502725 139815280891712 submission_runner.py:415] Time since start: 7330.02s, 	Step: 6537, 	{'train/accuracy': 0.3984375, 'train/loss': 2.920804262161255, 'validation/accuracy': 0.3664399981498718, 'validation/loss': 3.0777952671051025, 'validation/num_examples': 50000, 'test/accuracy': 0.2803000211715698, 'test/loss': 3.640188455581665, 'test/num_examples': 10000, 'score': 7040.157104253769, 'total_duration': 7330.018122434616, 'accumulated_submission_time': 7040.157104253769, 'accumulated_eval_time': 289.3788046836853, 'accumulated_logging_time': 0.3948063850402832}
I0502 03:24:06.516879 139584465782528 logging_writer.py:48] [6537] accumulated_eval_time=289.378805, accumulated_logging_time=0.394806, accumulated_submission_time=7040.157104, global_step=6537, preemption_count=0, score=7040.157104, test/accuracy=0.280300, test/loss=3.640188, test/num_examples=10000, total_duration=7330.018122, train/accuracy=0.398438, train/loss=2.920804, validation/accuracy=0.366440, validation/loss=3.077795, validation/num_examples=50000
I0502 03:25:15.212219 139584457389824 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.0460205078125, loss=4.382228851318359
I0502 03:26:54.952853 139584465782528 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.9785189628601074, loss=4.974443435668945
I0502 03:28:34.690401 139584457389824 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9703454375267029, loss=4.290581703186035
I0502 03:30:14.545484 139584465782528 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9577472805976868, loss=4.199924468994141
I0502 03:31:07.056860 139815280891712 spec.py:298] Evaluating on the training split.
I0502 03:31:13.574492 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 03:31:20.611510 139815280891712 spec.py:326] Evaluating on the test split.
I0502 03:31:22.284279 139815280891712 submission_runner.py:415] Time since start: 7765.78s, 	Step: 6959, 	{'train/accuracy': 0.410468727350235, 'train/loss': 2.7761077880859375, 'validation/accuracy': 0.38580000400543213, 'validation/loss': 2.9107067584991455, 'validation/num_examples': 50000, 'test/accuracy': 0.2939999997615814, 'test/loss': 3.5029091835021973, 'test/num_examples': 10000, 'score': 7460.678845643997, 'total_duration': 7765.777581691742, 'accumulated_submission_time': 7460.678845643997, 'accumulated_eval_time': 304.5840485095978, 'accumulated_logging_time': 0.42180562019348145}
I0502 03:31:22.297454 139584457389824 logging_writer.py:48] [6959] accumulated_eval_time=304.584049, accumulated_logging_time=0.421806, accumulated_submission_time=7460.678846, global_step=6959, preemption_count=0, score=7460.678846, test/accuracy=0.294000, test/loss=3.502909, test/num_examples=10000, total_duration=7765.777582, train/accuracy=0.410469, train/loss=2.776108, validation/accuracy=0.385800, validation/loss=2.910707, validation/num_examples=50000
I0502 03:32:09.625126 139584465782528 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7868559956550598, loss=5.106599807739258
I0502 03:33:50.102612 139584457389824 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.0207751989364624, loss=4.232456684112549
I0502 03:35:30.809288 139584465782528 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8674119114875793, loss=5.085407257080078
I0502 03:37:10.844606 139584457389824 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7962291836738586, loss=5.110138416290283
I0502 03:38:22.703328 139815280891712 spec.py:298] Evaluating on the training split.
I0502 03:38:29.217552 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 03:38:36.621607 139815280891712 spec.py:326] Evaluating on the test split.
I0502 03:38:38.286951 139815280891712 submission_runner.py:415] Time since start: 8201.80s, 	Step: 7377, 	{'train/accuracy': 0.4284375011920929, 'train/loss': 2.748767375946045, 'validation/accuracy': 0.3889999985694885, 'validation/loss': 2.9412271976470947, 'validation/num_examples': 50000, 'test/accuracy': 0.2962000072002411, 'test/loss': 3.5117692947387695, 'test/num_examples': 10000, 'score': 7881.068476438522, 'total_duration': 8201.802328109741, 'accumulated_submission_time': 7881.068476438522, 'accumulated_eval_time': 320.1675887107849, 'accumulated_logging_time': 0.44571495056152344}
I0502 03:38:38.303576 139584465782528 logging_writer.py:48] [7377] accumulated_eval_time=320.167589, accumulated_logging_time=0.445715, accumulated_submission_time=7881.068476, global_step=7377, preemption_count=0, score=7881.068476, test/accuracy=0.296200, test/loss=3.511769, test/num_examples=10000, total_duration=8201.802328, train/accuracy=0.428438, train/loss=2.748767, validation/accuracy=0.389000, validation/loss=2.941227, validation/num_examples=50000
I0502 03:39:07.397680 139584457389824 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.1741434335708618, loss=4.293262481689453
I0502 03:40:48.237595 139584465782528 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6756268739700317, loss=5.888825416564941
I0502 03:42:28.945572 139584457389824 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9468485713005066, loss=4.1457109451293945
I0502 03:44:09.117119 139584465782528 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.9050043821334839, loss=4.201799392700195
I0502 03:45:38.746669 139815280891712 spec.py:298] Evaluating on the training split.
I0502 03:45:45.448250 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 03:45:53.207950 139815280891712 spec.py:326] Evaluating on the test split.
I0502 03:45:54.880350 139815280891712 submission_runner.py:415] Time since start: 8638.40s, 	Step: 7793, 	{'train/accuracy': 0.4378320276737213, 'train/loss': 2.6273064613342285, 'validation/accuracy': 0.40629997849464417, 'validation/loss': 2.794783353805542, 'validation/num_examples': 50000, 'test/accuracy': 0.3125, 'test/loss': 3.3953611850738525, 'test/num_examples': 10000, 'score': 8301.493917942047, 'total_duration': 8638.395778417587, 'accumulated_submission_time': 8301.493917942047, 'accumulated_eval_time': 336.3012204170227, 'accumulated_logging_time': 0.47455286979675293}
I0502 03:45:54.891685 139584457389824 logging_writer.py:48] [7793] accumulated_eval_time=336.301220, accumulated_logging_time=0.474553, accumulated_submission_time=8301.493918, global_step=7793, preemption_count=0, score=8301.493918, test/accuracy=0.312500, test/loss=3.395361, test/num_examples=10000, total_duration=8638.395778, train/accuracy=0.437832, train/loss=2.627306, validation/accuracy=0.406300, validation/loss=2.794783, validation/num_examples=50000
I0502 03:46:06.953006 139584465782528 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.9768394231796265, loss=4.202541828155518
I0502 03:47:47.402327 139584457389824 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.9268291592597961, loss=4.1164045333862305
I0502 03:49:28.255557 139584465782528 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9603682160377502, loss=4.150635242462158
I0502 03:51:08.672657 139584457389824 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.1769238710403442, loss=4.281299114227295
I0502 03:52:49.191509 139584465782528 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.8463274240493774, loss=4.050713062286377
I0502 03:52:55.509994 139815280891712 spec.py:298] Evaluating on the training split.
I0502 03:53:02.386233 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 03:53:10.807963 139815280891712 spec.py:326] Evaluating on the test split.
I0502 03:53:12.465356 139815280891712 submission_runner.py:415] Time since start: 9075.98s, 	Step: 8210, 	{'train/accuracy': 0.4371679723262787, 'train/loss': 2.724919319152832, 'validation/accuracy': 0.40275999903678894, 'validation/loss': 2.8738462924957275, 'validation/num_examples': 50000, 'test/accuracy': 0.3078000247478485, 'test/loss': 3.456850051879883, 'test/num_examples': 10000, 'score': 8722.094465494156, 'total_duration': 9075.980795621872, 'accumulated_submission_time': 8722.094465494156, 'accumulated_eval_time': 353.2565402984619, 'accumulated_logging_time': 0.4982414245605469}
I0502 03:53:12.478798 139584457389824 logging_writer.py:48] [8210] accumulated_eval_time=353.256540, accumulated_logging_time=0.498241, accumulated_submission_time=8722.094465, global_step=8210, preemption_count=0, score=8722.094465, test/accuracy=0.307800, test/loss=3.456850, test/num_examples=10000, total_duration=9075.980796, train/accuracy=0.437168, train/loss=2.724919, validation/accuracy=0.402760, validation/loss=2.873846, validation/num_examples=50000
I0502 03:54:48.455393 139584465782528 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6863228678703308, loss=5.676013946533203
I0502 03:56:30.958024 139584457389824 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.657662034034729, loss=5.610617637634277
I0502 03:58:13.054197 139584465782528 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8687798976898193, loss=4.253262519836426
I0502 03:59:54.946884 139584457389824 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8858663439750671, loss=4.082908630371094
I0502 04:00:15.621954 139815280891712 spec.py:298] Evaluating on the training split.
I0502 04:00:22.598699 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 04:00:31.231034 139815280891712 spec.py:326] Evaluating on the test split.
I0502 04:00:32.900443 139815280891712 submission_runner.py:415] Time since start: 9516.42s, 	Step: 8621, 	{'train/accuracy': 0.4775976538658142, 'train/loss': 2.436032295227051, 'validation/accuracy': 0.41495999693870544, 'validation/loss': 2.736457347869873, 'validation/num_examples': 50000, 'test/accuracy': 0.32120001316070557, 'test/loss': 3.3401036262512207, 'test/num_examples': 10000, 'score': 9145.220792531967, 'total_duration': 9516.41585636139, 'accumulated_submission_time': 9145.220792531967, 'accumulated_eval_time': 370.53495693206787, 'accumulated_logging_time': 0.5231797695159912}
I0502 04:00:32.917092 139584465782528 logging_writer.py:48] [8621] accumulated_eval_time=370.534957, accumulated_logging_time=0.523180, accumulated_submission_time=9145.220793, global_step=8621, preemption_count=0, score=9145.220793, test/accuracy=0.321200, test/loss=3.340104, test/num_examples=10000, total_duration=9516.415856, train/accuracy=0.477598, train/loss=2.436032, validation/accuracy=0.414960, validation/loss=2.736457, validation/num_examples=50000
I0502 04:01:55.260663 139584457389824 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6163651347160339, loss=5.265174865722656
I0502 04:03:37.367856 139584465782528 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.1635072231292725, loss=4.227269172668457
I0502 04:05:20.205765 139584457389824 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.8197727203369141, loss=4.215239524841309
I0502 04:07:02.527052 139584465782528 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8309563398361206, loss=4.100714206695557
I0502 04:07:33.888219 139815280891712 spec.py:298] Evaluating on the training split.
I0502 04:07:41.052022 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 04:07:50.015997 139815280891712 spec.py:326] Evaluating on the test split.
I0502 04:07:51.665951 139815280891712 submission_runner.py:415] Time since start: 9955.18s, 	Step: 9035, 	{'train/accuracy': 0.4689257740974426, 'train/loss': 2.5299060344696045, 'validation/accuracy': 0.4281199872493744, 'validation/loss': 2.715872049331665, 'validation/num_examples': 50000, 'test/accuracy': 0.3286000192165375, 'test/loss': 3.315652847290039, 'test/num_examples': 10000, 'score': 9566.173962831497, 'total_duration': 9955.18007850647, 'accumulated_submission_time': 9566.173962831497, 'accumulated_eval_time': 388.31133365631104, 'accumulated_logging_time': 0.5522511005401611}
I0502 04:07:51.682380 139584457389824 logging_writer.py:48] [9035] accumulated_eval_time=388.311334, accumulated_logging_time=0.552251, accumulated_submission_time=9566.173963, global_step=9035, preemption_count=0, score=9566.173963, test/accuracy=0.328600, test/loss=3.315653, test/num_examples=10000, total_duration=9955.180079, train/accuracy=0.468926, train/loss=2.529906, validation/accuracy=0.428120, validation/loss=2.715872, validation/num_examples=50000
I0502 04:09:03.569059 139584465782528 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7749689221382141, loss=4.001229286193848
I0502 04:10:45.003449 139584457389824 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8475942015647888, loss=4.03637170791626
I0502 04:12:26.267939 139584465782528 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.8282185196876526, loss=4.174345016479492
I0502 04:14:08.149009 139584457389824 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6049498319625854, loss=5.812019348144531
I0502 04:14:51.888152 139815280891712 spec.py:298] Evaluating on the training split.
I0502 04:14:59.099360 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 04:15:08.094081 139815280891712 spec.py:326] Evaluating on the test split.
I0502 04:15:09.751743 139815280891712 submission_runner.py:415] Time since start: 10393.27s, 	Step: 9445, 	{'train/accuracy': 0.4703906178474426, 'train/loss': 2.5148401260375977, 'validation/accuracy': 0.43845999240875244, 'validation/loss': 2.6814186573028564, 'validation/num_examples': 50000, 'test/accuracy': 0.33490002155303955, 'test/loss': 3.27959942817688, 'test/num_examples': 10000, 'score': 9986.361079216003, 'total_duration': 10393.26552438736, 'accumulated_submission_time': 9986.361079216003, 'accumulated_eval_time': 406.1732249259949, 'accumulated_logging_time': 0.5818643569946289}
I0502 04:15:09.767659 139584465782528 logging_writer.py:48] [9445] accumulated_eval_time=406.173225, accumulated_logging_time=0.581864, accumulated_submission_time=9986.361079, global_step=9445, preemption_count=0, score=9986.361079, test/accuracy=0.334900, test/loss=3.279599, test/num_examples=10000, total_duration=10393.265524, train/accuracy=0.470391, train/loss=2.514840, validation/accuracy=0.438460, validation/loss=2.681419, validation/num_examples=50000
I0502 04:16:08.463636 139584457389824 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7494590282440186, loss=4.251907825469971
I0502 04:17:50.546147 139584465782528 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7197885513305664, loss=4.59869909286499
I0502 04:19:32.185731 139584457389824 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7767065167427063, loss=3.889834403991699
I0502 04:21:14.100589 139584465782528 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.8297492861747742, loss=3.8588409423828125
I0502 04:22:15.552632 139815280891712 spec.py:298] Evaluating on the training split.
I0502 04:22:23.065181 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 04:22:31.409992 139815280891712 spec.py:326] Evaluating on the test split.
I0502 04:22:33.088973 139815280891712 submission_runner.py:415] Time since start: 10836.60s, 	Step: 9861, 	{'train/accuracy': 0.48093748092651367, 'train/loss': 2.401608467102051, 'validation/accuracy': 0.45047998428344727, 'validation/loss': 2.550164222717285, 'validation/num_examples': 50000, 'test/accuracy': 0.34610000252723694, 'test/loss': 3.1743485927581787, 'test/num_examples': 10000, 'score': 10412.126942396164, 'total_duration': 10836.60283112526, 'accumulated_submission_time': 10412.126942396164, 'accumulated_eval_time': 423.70794320106506, 'accumulated_logging_time': 0.6112816333770752}
I0502 04:22:33.104617 139584457389824 logging_writer.py:48] [9861] accumulated_eval_time=423.707943, accumulated_logging_time=0.611282, accumulated_submission_time=10412.126942, global_step=9861, preemption_count=0, score=10412.126942, test/accuracy=0.346100, test/loss=3.174349, test/num_examples=10000, total_duration=10836.602831, train/accuracy=0.480937, train/loss=2.401608, validation/accuracy=0.450480, validation/loss=2.550164, validation/num_examples=50000
I0502 04:23:13.750398 139584465782528 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7976889610290527, loss=4.128475666046143
I0502 04:24:56.427869 139584457389824 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7544416189193726, loss=4.8272271156311035
I0502 04:26:38.483689 139584465782528 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6375113725662231, loss=5.860621452331543
I0502 04:28:20.350600 139584457389824 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6922488212585449, loss=4.229833602905273
I0502 04:29:33.613918 139815280891712 spec.py:298] Evaluating on the training split.
I0502 04:29:41.147139 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 04:29:49.453133 139815280891712 spec.py:326] Evaluating on the test split.
I0502 04:29:51.124927 139815280891712 submission_runner.py:415] Time since start: 11274.64s, 	Step: 10277, 	{'train/accuracy': 0.5069335699081421, 'train/loss': 2.2423994541168213, 'validation/accuracy': 0.46685999631881714, 'validation/loss': 2.4583487510681152, 'validation/num_examples': 50000, 'test/accuracy': 0.36510002613067627, 'test/loss': 3.0772602558135986, 'test/num_examples': 10000, 'score': 10832.616870164871, 'total_duration': 11274.638753175735, 'accumulated_submission_time': 10832.616870164871, 'accumulated_eval_time': 441.2173070907593, 'accumulated_logging_time': 0.6407623291015625}
I0502 04:29:51.141227 139584465782528 logging_writer.py:48] [10277] accumulated_eval_time=441.217307, accumulated_logging_time=0.640762, accumulated_submission_time=10832.616870, global_step=10277, preemption_count=0, score=10832.616870, test/accuracy=0.365100, test/loss=3.077260, test/num_examples=10000, total_duration=11274.638753, train/accuracy=0.506934, train/loss=2.242399, validation/accuracy=0.466860, validation/loss=2.458349, validation/num_examples=50000
I0502 04:30:20.325778 139584457389824 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7477790117263794, loss=3.9202799797058105
I0502 04:32:02.397719 139584465782528 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.707245945930481, loss=4.195443153381348
I0502 04:33:44.720280 139584457389824 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6790739297866821, loss=4.1902360916137695
I0502 04:35:27.127850 139584465782528 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5304407477378845, loss=5.785304546356201
I0502 04:36:51.355121 139815280891712 spec.py:298] Evaluating on the training split.
I0502 04:36:58.953425 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 04:37:07.182709 139815280891712 spec.py:326] Evaluating on the test split.
I0502 04:37:08.859799 139815280891712 submission_runner.py:415] Time since start: 11712.37s, 	Step: 10684, 	{'train/accuracy': 0.5169726610183716, 'train/loss': 2.2007522583007812, 'validation/accuracy': 0.48051998019218445, 'validation/loss': 2.3710169792175293, 'validation/num_examples': 50000, 'test/accuracy': 0.3743000030517578, 'test/loss': 3.0191054344177246, 'test/num_examples': 10000, 'score': 11252.811816692352, 'total_duration': 11712.37395477295, 'accumulated_submission_time': 11252.811816692352, 'accumulated_eval_time': 458.72066259384155, 'accumulated_logging_time': 0.6706266403198242}
I0502 04:37:08.871795 139584457389824 logging_writer.py:48] [10684] accumulated_eval_time=458.720663, accumulated_logging_time=0.670627, accumulated_submission_time=11252.811817, global_step=10684, preemption_count=0, score=11252.811817, test/accuracy=0.374300, test/loss=3.019105, test/num_examples=10000, total_duration=11712.373955, train/accuracy=0.516973, train/loss=2.200752, validation/accuracy=0.480520, validation/loss=2.371017, validation/num_examples=50000
I0502 04:37:27.228264 139584465782528 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.532698392868042, loss=5.486570835113525
I0502 04:39:09.141797 139584457389824 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7019807696342468, loss=3.798057794570923
I0502 04:40:51.750575 139584465782528 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.710034966468811, loss=3.7822909355163574
I0502 04:42:34.038601 139584457389824 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7853879928588867, loss=3.794429063796997
I0502 04:44:09.357403 139815280891712 spec.py:298] Evaluating on the training split.
I0502 04:44:17.618979 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 04:44:25.968356 139815280891712 spec.py:326] Evaluating on the test split.
I0502 04:44:27.642594 139815280891712 submission_runner.py:415] Time since start: 12151.16s, 	Step: 11100, 	{'train/accuracy': 0.5300585627555847, 'train/loss': 2.096954107284546, 'validation/accuracy': 0.494519978761673, 'validation/loss': 2.2770297527313232, 'validation/num_examples': 50000, 'test/accuracy': 0.37780001759529114, 'test/loss': 2.933253765106201, 'test/num_examples': 10000, 'score': 11673.278126239777, 'total_duration': 12151.15645289421, 'accumulated_submission_time': 11673.278126239777, 'accumulated_eval_time': 477.00423645973206, 'accumulated_logging_time': 0.696491003036499}
I0502 04:44:27.658184 139584465782528 logging_writer.py:48] [11100] accumulated_eval_time=477.004236, accumulated_logging_time=0.696491, accumulated_submission_time=11673.278126, global_step=11100, preemption_count=0, score=11673.278126, test/accuracy=0.377800, test/loss=2.933254, test/num_examples=10000, total_duration=12151.156453, train/accuracy=0.530059, train/loss=2.096954, validation/accuracy=0.494520, validation/loss=2.277030, validation/num_examples=50000
I0502 04:44:34.026689 139584457389824 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6502479910850525, loss=5.064578533172607
I0502 04:46:16.011343 139584465782528 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6326457262039185, loss=4.401453018188477
I0502 04:47:58.628526 139584457389824 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.7340745329856873, loss=3.679980993270874
I0502 04:49:40.813397 139584465782528 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7407119870185852, loss=4.219967842102051
I0502 04:51:22.672177 139584457389824 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7336588501930237, loss=3.711991786956787
I0502 04:51:27.722599 139815280891712 spec.py:298] Evaluating on the training split.
I0502 04:51:35.123041 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 04:51:43.505096 139815280891712 spec.py:326] Evaluating on the test split.
I0502 04:51:45.193072 139815280891712 submission_runner.py:415] Time since start: 12588.71s, 	Step: 11508, 	{'train/accuracy': 0.5736523270606995, 'train/loss': 1.9004149436950684, 'validation/accuracy': 0.5095999836921692, 'validation/loss': 2.209139585494995, 'validation/num_examples': 50000, 'test/accuracy': 0.3969000279903412, 'test/loss': 2.8658907413482666, 'test/num_examples': 10000, 'score': 12093.323530435562, 'total_duration': 12588.707181692123, 'accumulated_submission_time': 12093.323530435562, 'accumulated_eval_time': 494.47333669662476, 'accumulated_logging_time': 0.7256667613983154}
I0502 04:51:45.205790 139584465782528 logging_writer.py:48] [11508] accumulated_eval_time=494.473337, accumulated_logging_time=0.725667, accumulated_submission_time=12093.323530, global_step=11508, preemption_count=0, score=12093.323530, test/accuracy=0.396900, test/loss=2.865891, test/num_examples=10000, total_duration=12588.707182, train/accuracy=0.573652, train/loss=1.900415, validation/accuracy=0.509600, validation/loss=2.209140, validation/num_examples=50000
I0502 04:53:22.435618 139584457389824 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.748136043548584, loss=3.561829090118408
I0502 04:55:04.324374 139584465782528 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6926160454750061, loss=3.7621209621429443
I0502 04:56:46.155981 139584457389824 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6750530004501343, loss=5.65434455871582
I0502 04:58:28.002148 139584465782528 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5976842045783997, loss=5.094092845916748
I0502 04:58:48.023637 139815280891712 spec.py:298] Evaluating on the training split.
I0502 04:58:56.372629 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 04:59:04.824203 139815280891712 spec.py:326] Evaluating on the test split.
I0502 04:59:06.488608 139815280891712 submission_runner.py:415] Time since start: 13030.00s, 	Step: 11921, 	{'train/accuracy': 0.5526366829872131, 'train/loss': 2.014735698699951, 'validation/accuracy': 0.509880006313324, 'validation/loss': 2.2083921432495117, 'validation/num_examples': 50000, 'test/accuracy': 0.4020000100135803, 'test/loss': 2.8412797451019287, 'test/num_examples': 10000, 'score': 12516.122400522232, 'total_duration': 13030.00281572342, 'accumulated_submission_time': 12516.122400522232, 'accumulated_eval_time': 512.9370329380035, 'accumulated_logging_time': 0.7518754005432129}
I0502 04:59:06.505652 139584457389824 logging_writer.py:48] [11921] accumulated_eval_time=512.937033, accumulated_logging_time=0.751875, accumulated_submission_time=12516.122401, global_step=11921, preemption_count=0, score=12516.122401, test/accuracy=0.402000, test/loss=2.841280, test/num_examples=10000, total_duration=13030.002816, train/accuracy=0.552637, train/loss=2.014736, validation/accuracy=0.509880, validation/loss=2.208392, validation/num_examples=50000
I0502 05:00:28.009718 139584465782528 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7013870477676392, loss=3.824801445007324
I0502 05:02:09.823405 139584457389824 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.736308753490448, loss=3.6497082710266113
I0502 05:03:51.850572 139584465782528 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.763031542301178, loss=3.645892381668091
I0502 05:05:33.592107 139584457389824 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7538356781005859, loss=3.4657206535339355
I0502 05:06:07.095617 139815280891712 spec.py:298] Evaluating on the training split.
I0502 05:06:14.658988 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 05:06:22.963571 139815280891712 spec.py:326] Evaluating on the test split.
I0502 05:06:24.633894 139815280891712 submission_runner.py:415] Time since start: 13468.15s, 	Step: 12339, 	{'train/accuracy': 0.5564648509025574, 'train/loss': 2.0221972465515137, 'validation/accuracy': 0.5171599984169006, 'validation/loss': 2.1966042518615723, 'validation/num_examples': 50000, 'test/accuracy': 0.4090000092983246, 'test/loss': 2.826739549636841, 'test/num_examples': 10000, 'score': 12936.69292807579, 'total_duration': 13468.148062944412, 'accumulated_submission_time': 12936.69292807579, 'accumulated_eval_time': 530.4739997386932, 'accumulated_logging_time': 0.7827839851379395}
I0502 05:06:24.650517 139584465782528 logging_writer.py:48] [12339] accumulated_eval_time=530.474000, accumulated_logging_time=0.782784, accumulated_submission_time=12936.692928, global_step=12339, preemption_count=0, score=12936.692928, test/accuracy=0.409000, test/loss=2.826740, test/num_examples=10000, total_duration=13468.148063, train/accuracy=0.556465, train/loss=2.022197, validation/accuracy=0.517160, validation/loss=2.196604, validation/num_examples=50000
I0502 05:07:33.152733 139584457389824 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.7391275763511658, loss=3.623901605606079
I0502 05:09:15.603087 139584465782528 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.752510130405426, loss=3.5222747325897217
I0502 05:10:57.497333 139584457389824 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6223545074462891, loss=4.031999111175537
I0502 05:12:39.549110 139584465782528 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.707984447479248, loss=3.479538917541504
I0502 05:13:25.022268 139815280891712 spec.py:298] Evaluating on the training split.
I0502 05:13:32.650227 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 05:13:40.943882 139815280891712 spec.py:326] Evaluating on the test split.
I0502 05:13:42.605800 139815280891712 submission_runner.py:415] Time since start: 13906.12s, 	Step: 12747, 	{'train/accuracy': 0.568652331829071, 'train/loss': 1.94989013671875, 'validation/accuracy': 0.5304399728775024, 'validation/loss': 2.1401023864746094, 'validation/num_examples': 50000, 'test/accuracy': 0.41530001163482666, 'test/loss': 2.780726194381714, 'test/num_examples': 10000, 'score': 13357.046221017838, 'total_duration': 13906.120040416718, 'accumulated_submission_time': 13357.046221017838, 'accumulated_eval_time': 548.056293964386, 'accumulated_logging_time': 0.8123905658721924}
I0502 05:13:42.622505 139584457389824 logging_writer.py:48] [12747] accumulated_eval_time=548.056294, accumulated_logging_time=0.812391, accumulated_submission_time=13357.046221, global_step=12747, preemption_count=0, score=13357.046221, test/accuracy=0.415300, test/loss=2.780726, test/num_examples=10000, total_duration=13906.120040, train/accuracy=0.568652, train/loss=1.949890, validation/accuracy=0.530440, validation/loss=2.140102, validation/num_examples=50000
I0502 05:14:39.394651 139584465782528 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6878061890602112, loss=5.617412090301514
I0502 05:16:21.049200 139584457389824 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7190234065055847, loss=3.579991579055786
I0502 05:18:03.042871 139584465782528 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7077941298484802, loss=3.4458675384521484
I0502 05:19:45.420547 139584457389824 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6735906600952148, loss=4.6195478439331055
I0502 05:20:46.794714 139815280891712 spec.py:298] Evaluating on the training split.
I0502 05:20:54.437881 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 05:21:02.915345 139815280891712 spec.py:326] Evaluating on the test split.
I0502 05:21:04.583804 139815280891712 submission_runner.py:415] Time since start: 14348.10s, 	Step: 13161, 	{'train/accuracy': 0.5890234112739563, 'train/loss': 1.8429899215698242, 'validation/accuracy': 0.5428799986839294, 'validation/loss': 2.0718536376953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4237000346183777, 'test/loss': 2.7336270809173584, 'test/num_examples': 10000, 'score': 13781.199891805649, 'total_duration': 14348.09713935852, 'accumulated_submission_time': 13781.199891805649, 'accumulated_eval_time': 565.8432378768921, 'accumulated_logging_time': 0.8421120643615723}
I0502 05:21:04.601600 139584465782528 logging_writer.py:48] [13161] accumulated_eval_time=565.843238, accumulated_logging_time=0.842112, accumulated_submission_time=13781.199892, global_step=13161, preemption_count=0, score=13781.199892, test/accuracy=0.423700, test/loss=2.733627, test/num_examples=10000, total_duration=14348.097139, train/accuracy=0.589023, train/loss=1.842990, validation/accuracy=0.542880, validation/loss=2.071854, validation/num_examples=50000
I0502 05:21:45.607221 139584457389824 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6500528454780579, loss=3.7873454093933105
I0502 05:23:27.667278 139584465782528 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6712836027145386, loss=3.6171560287475586
I0502 05:25:09.711396 139584457389824 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6763687133789062, loss=5.0187249183654785
I0502 05:26:51.853317 139584465782528 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6809431910514832, loss=3.716949939727783
I0502 05:28:05.247399 139815280891712 spec.py:298] Evaluating on the training split.
I0502 05:28:12.792071 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 05:28:21.437656 139815280891712 spec.py:326] Evaluating on the test split.
I0502 05:28:23.082394 139815280891712 submission_runner.py:415] Time since start: 14786.60s, 	Step: 13577, 	{'train/accuracy': 0.5868359208106995, 'train/loss': 1.8818148374557495, 'validation/accuracy': 0.5476199984550476, 'validation/loss': 2.082552433013916, 'validation/num_examples': 50000, 'test/accuracy': 0.43220001459121704, 'test/loss': 2.7186200618743896, 'test/num_examples': 10000, 'score': 14201.827218055725, 'total_duration': 14786.596528053284, 'accumulated_submission_time': 14201.827218055725, 'accumulated_eval_time': 583.6768879890442, 'accumulated_logging_time': 0.8728237152099609}
I0502 05:28:23.099525 139584457389824 logging_writer.py:48] [13577] accumulated_eval_time=583.676888, accumulated_logging_time=0.872824, accumulated_submission_time=14201.827218, global_step=13577, preemption_count=0, score=14201.827218, test/accuracy=0.432200, test/loss=2.718620, test/num_examples=10000, total_duration=14786.596528, train/accuracy=0.586836, train/loss=1.881815, validation/accuracy=0.547620, validation/loss=2.082552, validation/num_examples=50000
I0502 05:28:52.586987 139584465782528 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5864863395690918, loss=5.557250022888184
I0502 05:30:34.587641 139584457389824 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.7459765672683716, loss=3.300015449523926
I0502 05:32:16.613831 139584465782528 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.8456491827964783, loss=3.437440872192383
I0502 05:33:58.853189 139584457389824 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6706694960594177, loss=3.6586496829986572
I0502 05:35:23.709486 139815280891712 spec.py:298] Evaluating on the training split.
I0502 05:35:31.262966 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 05:35:39.808011 139815280891712 spec.py:326] Evaluating on the test split.
I0502 05:35:41.458280 139815280891712 submission_runner.py:415] Time since start: 15224.97s, 	Step: 13985, 	{'train/accuracy': 0.5888085961341858, 'train/loss': 1.8366668224334717, 'validation/accuracy': 0.5507400035858154, 'validation/loss': 2.015629768371582, 'validation/num_examples': 50000, 'test/accuracy': 0.43620002269744873, 'test/loss': 2.6675989627838135, 'test/num_examples': 10000, 'score': 14622.418252706528, 'total_duration': 15224.972486972809, 'accumulated_submission_time': 14622.418252706528, 'accumulated_eval_time': 601.4244141578674, 'accumulated_logging_time': 0.9033582210540771}
I0502 05:35:41.476508 139584465782528 logging_writer.py:48] [13985] accumulated_eval_time=601.424414, accumulated_logging_time=0.903358, accumulated_submission_time=14622.418253, global_step=13985, preemption_count=0, score=14622.418253, test/accuracy=0.436200, test/loss=2.667599, test/num_examples=10000, total_duration=15224.972487, train/accuracy=0.588809, train/loss=1.836667, validation/accuracy=0.550740, validation/loss=2.015630, validation/num_examples=50000
I0502 05:35:59.315272 139584457389824 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6997835636138916, loss=3.43884015083313
I0502 05:37:42.303526 139584465782528 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.659611701965332, loss=3.9940237998962402
I0502 05:39:25.016395 139584457389824 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.7141552567481995, loss=4.2693681716918945
I0502 05:41:07.611102 139584465782528 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6444678902626038, loss=3.431441068649292
I0502 05:42:41.650658 139815280891712 spec.py:298] Evaluating on the training split.
I0502 05:42:49.160710 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 05:42:57.719858 139815280891712 spec.py:326] Evaluating on the test split.
I0502 05:42:59.399676 139815280891712 submission_runner.py:415] Time since start: 15662.91s, 	Step: 14397, 	{'train/accuracy': 0.6306445002555847, 'train/loss': 1.6395663022994995, 'validation/accuracy': 0.5621199607849121, 'validation/loss': 1.958893060684204, 'validation/num_examples': 50000, 'test/accuracy': 0.4439000189304352, 'test/loss': 2.6122660636901855, 'test/num_examples': 10000, 'score': 15042.57299733162, 'total_duration': 15662.913816690445, 'accumulated_submission_time': 15042.57299733162, 'accumulated_eval_time': 619.17209815979, 'accumulated_logging_time': 0.9354605674743652}
I0502 05:42:59.413816 139584457389824 logging_writer.py:48] [14397] accumulated_eval_time=619.172098, accumulated_logging_time=0.935461, accumulated_submission_time=15042.572997, global_step=14397, preemption_count=0, score=15042.572997, test/accuracy=0.443900, test/loss=2.612266, test/num_examples=10000, total_duration=15662.913817, train/accuracy=0.630645, train/loss=1.639566, validation/accuracy=0.562120, validation/loss=1.958893, validation/num_examples=50000
I0502 05:43:08.177952 139584465782528 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6974765062332153, loss=3.6294589042663574
I0502 05:44:50.174506 139584457389824 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7005326151847839, loss=4.713616847991943
I0502 05:46:33.090893 139584465782528 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.7694122195243835, loss=3.5042316913604736
I0502 05:48:15.833451 139584457389824 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7101219892501831, loss=3.3702456951141357
I0502 05:49:58.105266 139584465782528 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6538029313087463, loss=3.6603333950042725
I0502 05:49:59.560580 139815280891712 spec.py:298] Evaluating on the training split.
I0502 05:50:07.094771 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 05:50:15.686039 139815280891712 spec.py:326] Evaluating on the test split.
I0502 05:50:17.354927 139815280891712 submission_runner.py:415] Time since start: 16100.87s, 	Step: 14803, 	{'train/accuracy': 0.615917980670929, 'train/loss': 1.6823394298553467, 'validation/accuracy': 0.5661799907684326, 'validation/loss': 1.9142708778381348, 'validation/num_examples': 50000, 'test/accuracy': 0.44840002059936523, 'test/loss': 2.569552183151245, 'test/num_examples': 10000, 'score': 15462.704107284546, 'total_duration': 16100.86913561821, 'accumulated_submission_time': 15462.704107284546, 'accumulated_eval_time': 636.9651741981506, 'accumulated_logging_time': 0.9598383903503418}
I0502 05:50:17.373018 139584457389824 logging_writer.py:48] [14803] accumulated_eval_time=636.965174, accumulated_logging_time=0.959838, accumulated_submission_time=15462.704107, global_step=14803, preemption_count=0, score=15462.704107, test/accuracy=0.448400, test/loss=2.569552, test/num_examples=10000, total_duration=16100.869136, train/accuracy=0.615918, train/loss=1.682339, validation/accuracy=0.566180, validation/loss=1.914271, validation/num_examples=50000
I0502 05:51:58.698213 139584465782528 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.8195034265518188, loss=3.374598979949951
I0502 05:53:41.227984 139584457389824 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7733738422393799, loss=3.450087547302246
I0502 05:55:23.617237 139584465782528 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.7440317869186401, loss=3.264533519744873
I0502 05:57:06.088401 139584457389824 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.8779276013374329, loss=5.420017242431641
I0502 05:57:17.714708 139815280891712 spec.py:298] Evaluating on the training split.
I0502 05:57:25.212142 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 05:57:33.655091 139815280891712 spec.py:326] Evaluating on the test split.
I0502 05:57:35.314879 139815280891712 submission_runner.py:415] Time since start: 16538.83s, 	Step: 15216, 	{'train/accuracy': 0.6160351634025574, 'train/loss': 1.7533750534057617, 'validation/accuracy': 0.5708999633789062, 'validation/loss': 1.956499695777893, 'validation/num_examples': 50000, 'test/accuracy': 0.45010003447532654, 'test/loss': 2.604702949523926, 'test/num_examples': 10000, 'score': 15883.030010938644, 'total_duration': 16538.82902598381, 'accumulated_submission_time': 15883.030010938644, 'accumulated_eval_time': 654.5640182495117, 'accumulated_logging_time': 0.9881417751312256}
I0502 05:57:35.326801 139584465782528 logging_writer.py:48] [15216] accumulated_eval_time=654.564018, accumulated_logging_time=0.988142, accumulated_submission_time=15883.030011, global_step=15216, preemption_count=0, score=15883.030011, test/accuracy=0.450100, test/loss=2.604703, test/num_examples=10000, total_duration=16538.829026, train/accuracy=0.616035, train/loss=1.753375, validation/accuracy=0.570900, validation/loss=1.956500, validation/num_examples=50000
I0502 05:59:07.006927 139584457389824 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.915557861328125, loss=3.258894205093384
I0502 06:00:49.605236 139584465782528 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6640783548355103, loss=4.394015312194824
I0502 06:02:32.303917 139584457389824 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7037465572357178, loss=3.366549491882324
I0502 06:04:14.731044 139584465782528 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7394750714302063, loss=3.4763236045837402
I0502 06:04:35.459013 139815280891712 spec.py:298] Evaluating on the training split.
I0502 06:04:42.983773 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 06:04:51.475871 139815280891712 spec.py:326] Evaluating on the test split.
I0502 06:04:53.127624 139815280891712 submission_runner.py:415] Time since start: 16976.64s, 	Step: 15621, 	{'train/accuracy': 0.6290624737739563, 'train/loss': 1.6345751285552979, 'validation/accuracy': 0.5811200141906738, 'validation/loss': 1.8429571390151978, 'validation/num_examples': 50000, 'test/accuracy': 0.4609000086784363, 'test/loss': 2.5042402744293213, 'test/num_examples': 10000, 'score': 16303.143368244171, 'total_duration': 16976.641666173935, 'accumulated_submission_time': 16303.143368244171, 'accumulated_eval_time': 672.2311902046204, 'accumulated_logging_time': 1.0134801864624023}
I0502 06:04:53.145597 139584457389824 logging_writer.py:48] [15621] accumulated_eval_time=672.231190, accumulated_logging_time=1.013480, accumulated_submission_time=16303.143368, global_step=15621, preemption_count=0, score=16303.143368, test/accuracy=0.460900, test/loss=2.504240, test/num_examples=10000, total_duration=16976.641666, train/accuracy=0.629062, train/loss=1.634575, validation/accuracy=0.581120, validation/loss=1.842957, validation/num_examples=50000
I0502 06:06:15.627932 139584465782528 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.7111824154853821, loss=3.5280489921569824
I0502 06:07:57.945288 139584457389824 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7018168568611145, loss=3.3813846111297607
I0502 06:09:40.619361 139584465782528 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.8395910263061523, loss=3.324035882949829
I0502 06:11:23.028733 139584457389824 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7051435708999634, loss=3.331326484680176
I0502 06:11:53.647886 139815280891712 spec.py:298] Evaluating on the training split.
I0502 06:12:01.150759 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 06:12:09.811968 139815280891712 spec.py:326] Evaluating on the test split.
I0502 06:12:11.481017 139815280891712 submission_runner.py:415] Time since start: 17414.99s, 	Step: 16034, 	{'train/accuracy': 0.6342382431030273, 'train/loss': 1.6185213327407837, 'validation/accuracy': 0.5816799998283386, 'validation/loss': 1.8678675889968872, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.5194969177246094, 'test/num_examples': 10000, 'score': 16723.62685751915, 'total_duration': 17414.99485874176, 'accumulated_submission_time': 16723.62685751915, 'accumulated_eval_time': 690.062686920166, 'accumulated_logging_time': 1.0447022914886475}
I0502 06:12:11.493059 139584465782528 logging_writer.py:48] [16034] accumulated_eval_time=690.062687, accumulated_logging_time=1.044702, accumulated_submission_time=16723.626858, global_step=16034, preemption_count=0, score=16723.626858, test/accuracy=0.461100, test/loss=2.519497, test/num_examples=10000, total_duration=17414.994859, train/accuracy=0.634238, train/loss=1.618521, validation/accuracy=0.581680, validation/loss=1.867868, validation/num_examples=50000
I0502 06:13:23.861399 139584457389824 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7730730175971985, loss=5.3486738204956055
I0502 06:15:06.291700 139584465782528 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.9006462693214417, loss=3.2514004707336426
I0502 06:16:49.869112 139584457389824 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.7326768040657043, loss=3.2801826000213623
I0502 06:18:33.009023 139584465782528 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.6865026354789734, loss=3.654573678970337
I0502 06:19:13.697557 139815280891712 spec.py:298] Evaluating on the training split.
I0502 06:19:21.160051 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 06:19:29.825845 139815280891712 spec.py:326] Evaluating on the test split.
I0502 06:19:31.489431 139815280891712 submission_runner.py:415] Time since start: 17855.00s, 	Step: 16441, 	{'train/accuracy': 0.6317773461341858, 'train/loss': 1.6280465126037598, 'validation/accuracy': 0.5896799564361572, 'validation/loss': 1.845659852027893, 'validation/num_examples': 50000, 'test/accuracy': 0.4644000232219696, 'test/loss': 2.4914140701293945, 'test/num_examples': 10000, 'score': 17145.812705755234, 'total_duration': 17855.003532648087, 'accumulated_submission_time': 17145.812705755234, 'accumulated_eval_time': 707.8531801700592, 'accumulated_logging_time': 1.0698328018188477}
I0502 06:19:31.507524 139584457389824 logging_writer.py:48] [16441] accumulated_eval_time=707.853180, accumulated_logging_time=1.069833, accumulated_submission_time=17145.812706, global_step=16441, preemption_count=0, score=17145.812706, test/accuracy=0.464400, test/loss=2.491414, test/num_examples=10000, total_duration=17855.003533, train/accuracy=0.631777, train/loss=1.628047, validation/accuracy=0.589680, validation/loss=1.845660, validation/num_examples=50000
I0502 06:20:33.952131 139584465782528 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.663947582244873, loss=3.495126724243164
I0502 06:22:16.344453 139584457389824 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.8013575077056885, loss=3.2743468284606934
I0502 06:23:58.581362 139584465782528 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.7575538754463196, loss=3.684926748275757
I0502 06:25:41.239972 139584457389824 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.8634414672851562, loss=3.293539524078369
I0502 06:26:31.689174 139815280891712 spec.py:298] Evaluating on the training split.
I0502 06:26:39.150843 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 06:26:47.857016 139815280891712 spec.py:326] Evaluating on the test split.
I0502 06:26:49.517495 139815280891712 submission_runner.py:415] Time since start: 18293.03s, 	Step: 16854, 	{'train/accuracy': 0.6373828053474426, 'train/loss': 1.656376600265503, 'validation/accuracy': 0.5918599963188171, 'validation/loss': 1.863510251045227, 'validation/num_examples': 50000, 'test/accuracy': 0.4700000286102295, 'test/loss': 2.4905102252960205, 'test/num_examples': 10000, 'score': 17565.975972175598, 'total_duration': 18293.03175520897, 'accumulated_submission_time': 17565.975972175598, 'accumulated_eval_time': 725.6802845001221, 'accumulated_logging_time': 1.100714921951294}
I0502 06:26:49.530158 139584465782528 logging_writer.py:48] [16854] accumulated_eval_time=725.680285, accumulated_logging_time=1.100715, accumulated_submission_time=17565.975972, global_step=16854, preemption_count=0, score=17565.975972, test/accuracy=0.470000, test/loss=2.490510, test/num_examples=10000, total_duration=18293.031755, train/accuracy=0.637383, train/loss=1.656377, validation/accuracy=0.591860, validation/loss=1.863510, validation/num_examples=50000
I0502 06:27:41.719542 139584457389824 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.8020741939544678, loss=5.3644185066223145
I0502 06:29:24.645397 139584465782528 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.9133309721946716, loss=4.567396640777588
I0502 06:31:07.481089 139584457389824 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.7637293934822083, loss=3.230456829071045
I0502 06:32:50.184488 139584465782528 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.8149164319038391, loss=3.1819589138031006
I0502 06:33:51.971981 139815280891712 spec.py:298] Evaluating on the training split.
I0502 06:33:59.455233 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 06:34:08.060950 139815280891712 spec.py:326] Evaluating on the test split.
I0502 06:34:09.721478 139815280891712 submission_runner.py:415] Time since start: 18733.24s, 	Step: 17261, 	{'train/accuracy': 0.6604687571525574, 'train/loss': 1.5321271419525146, 'validation/accuracy': 0.588979959487915, 'validation/loss': 1.8548794984817505, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.4987239837646484, 'test/num_examples': 10000, 'score': 17988.39752984047, 'total_duration': 18733.235650777817, 'accumulated_submission_time': 17988.39752984047, 'accumulated_eval_time': 743.4284825325012, 'accumulated_logging_time': 1.128204107284546}
I0502 06:34:09.740067 139584457389824 logging_writer.py:48] [17261] accumulated_eval_time=743.428483, accumulated_logging_time=1.128204, accumulated_submission_time=17988.397530, global_step=17261, preemption_count=0, score=17988.397530, test/accuracy=0.467400, test/loss=2.498724, test/num_examples=10000, total_duration=18733.235651, train/accuracy=0.660469, train/loss=1.532127, validation/accuracy=0.588980, validation/loss=1.854879, validation/num_examples=50000
I0502 06:34:51.004730 139584465782528 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.7208813428878784, loss=4.1718010902404785
I0502 06:36:33.455738 139584457389824 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7130771279335022, loss=3.199415683746338
I0502 06:38:16.102474 139584465782528 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7128339409828186, loss=4.051407814025879
I0502 06:39:58.807899 139584457389824 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.8802277445793152, loss=3.4584994316101074
I0502 06:41:10.259498 139815280891712 spec.py:298] Evaluating on the training split.
I0502 06:41:17.643732 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 06:41:26.279985 139815280891712 spec.py:326] Evaluating on the test split.
I0502 06:41:27.940756 139815280891712 submission_runner.py:415] Time since start: 19171.45s, 	Step: 17674, 	{'train/accuracy': 0.6518163681030273, 'train/loss': 1.5523284673690796, 'validation/accuracy': 0.6021199822425842, 'validation/loss': 1.7871452569961548, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.4381721019744873, 'test/num_examples': 10000, 'score': 18408.898463249207, 'total_duration': 19171.45481991768, 'accumulated_submission_time': 18408.898463249207, 'accumulated_eval_time': 761.108330488205, 'accumulated_logging_time': 1.1597340106964111}
I0502 06:41:27.956075 139584465782528 logging_writer.py:48] [17674] accumulated_eval_time=761.108330, accumulated_logging_time=1.159734, accumulated_submission_time=18408.898463, global_step=17674, preemption_count=0, score=18408.898463, test/accuracy=0.475400, test/loss=2.438172, test/num_examples=10000, total_duration=19171.454820, train/accuracy=0.651816, train/loss=1.552328, validation/accuracy=0.602120, validation/loss=1.787145, validation/num_examples=50000
I0502 06:41:59.658311 139584457389824 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.8549611568450928, loss=3.120222330093384
I0502 06:43:42.423844 139584465782528 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6738051176071167, loss=3.825450897216797
I0502 06:45:24.916166 139584457389824 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.9101815223693848, loss=3.3509633541107178
I0502 06:47:08.320618 139584465782528 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6421070098876953, loss=4.407497406005859
I0502 06:48:30.267477 139815280891712 spec.py:298] Evaluating on the training split.
I0502 06:48:37.700340 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 06:48:46.311219 139815280891712 spec.py:326] Evaluating on the test split.
I0502 06:48:47.959273 139815280891712 submission_runner.py:415] Time since start: 19611.47s, 	Step: 18081, 	{'train/accuracy': 0.6507031321525574, 'train/loss': 1.5641251802444458, 'validation/accuracy': 0.6025199890136719, 'validation/loss': 1.7798306941986084, 'validation/num_examples': 50000, 'test/accuracy': 0.4796000123023987, 'test/loss': 2.4260525703430176, 'test/num_examples': 10000, 'score': 18831.19221496582, 'total_duration': 19611.473540067673, 'accumulated_submission_time': 18831.19221496582, 'accumulated_eval_time': 778.7989149093628, 'accumulated_logging_time': 1.1871819496154785}
I0502 06:48:47.972296 139584457389824 logging_writer.py:48] [18081] accumulated_eval_time=778.798915, accumulated_logging_time=1.187182, accumulated_submission_time=18831.192215, global_step=18081, preemption_count=0, score=18831.192215, test/accuracy=0.479600, test/loss=2.426053, test/num_examples=10000, total_duration=19611.473540, train/accuracy=0.650703, train/loss=1.564125, validation/accuracy=0.602520, validation/loss=1.779831, validation/num_examples=50000
I0502 06:49:08.919053 139584465782528 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7239283323287964, loss=4.194246292114258
I0502 06:50:51.333456 139584457389824 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.7065974473953247, loss=3.7241384983062744
I0502 06:52:34.086298 139584465782528 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.8917222023010254, loss=3.305788993835449
I0502 06:54:16.807762 139584457389824 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.7741443514823914, loss=3.1657509803771973
I0502 06:55:48.240392 139815280891712 spec.py:298] Evaluating on the training split.
I0502 06:55:55.766047 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 06:56:04.460031 139815280891712 spec.py:326] Evaluating on the test split.
I0502 06:56:06.141467 139815280891712 submission_runner.py:415] Time since start: 20049.66s, 	Step: 18493, 	{'train/accuracy': 0.6647265553474426, 'train/loss': 1.5321500301361084, 'validation/accuracy': 0.6112799644470215, 'validation/loss': 1.7746847867965698, 'validation/num_examples': 50000, 'test/accuracy': 0.4871000349521637, 'test/loss': 2.4198668003082275, 'test/num_examples': 10000, 'score': 19251.440666913986, 'total_duration': 20049.65583729744, 'accumulated_submission_time': 19251.440666913986, 'accumulated_eval_time': 796.6988821029663, 'accumulated_logging_time': 1.2143619060516357}
I0502 06:56:06.161062 139584465782528 logging_writer.py:48] [18493] accumulated_eval_time=796.698882, accumulated_logging_time=1.214362, accumulated_submission_time=19251.440667, global_step=18493, preemption_count=0, score=19251.440667, test/accuracy=0.487100, test/loss=2.419867, test/num_examples=10000, total_duration=20049.655837, train/accuracy=0.664727, train/loss=1.532150, validation/accuracy=0.611280, validation/loss=1.774685, validation/num_examples=50000
I0502 06:56:17.886893 139584457389824 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7890406250953674, loss=4.801243782043457
I0502 06:58:00.806562 139584465782528 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.7147644758224487, loss=4.21621036529541
I0502 06:59:43.518970 139584457389824 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.9409672021865845, loss=3.262040138244629
I0502 07:01:26.707661 139584465782528 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7921083569526672, loss=3.2901580333709717
I0502 07:03:09.880100 139584457389824 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.7875834703445435, loss=3.1748745441436768
I0502 07:03:09.929335 139815280891712 spec.py:298] Evaluating on the training split.
I0502 07:03:17.282188 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 07:03:26.084240 139815280891712 spec.py:326] Evaluating on the test split.
I0502 07:03:27.759094 139815280891712 submission_runner.py:415] Time since start: 20491.27s, 	Step: 18901, 	{'train/accuracy': 0.6704491972923279, 'train/loss': 1.4149999618530273, 'validation/accuracy': 0.6163399815559387, 'validation/loss': 1.6806557178497314, 'validation/num_examples': 50000, 'test/accuracy': 0.48680001497268677, 'test/loss': 2.3434183597564697, 'test/num_examples': 10000, 'score': 19675.189138889313, 'total_duration': 20491.2732026577, 'accumulated_submission_time': 19675.189138889313, 'accumulated_eval_time': 814.527245759964, 'accumulated_logging_time': 1.2482469081878662}
I0502 07:03:27.777055 139584465782528 logging_writer.py:48] [18901] accumulated_eval_time=814.527246, accumulated_logging_time=1.248247, accumulated_submission_time=19675.189139, global_step=18901, preemption_count=0, score=19675.189139, test/accuracy=0.486800, test/loss=2.343418, test/num_examples=10000, total_duration=20491.273203, train/accuracy=0.670449, train/loss=1.415000, validation/accuracy=0.616340, validation/loss=1.680656, validation/num_examples=50000
I0502 07:05:10.795354 139584457389824 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8974425196647644, loss=3.926560163497925
I0502 07:06:53.617122 139584465782528 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.8492798209190369, loss=3.0779457092285156
I0502 07:08:36.293263 139584457389824 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.7351490259170532, loss=3.24137282371521
I0502 07:10:19.243759 139584465782528 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.8427808880805969, loss=3.1427559852600098
I0502 07:10:28.510687 139815280891712 spec.py:298] Evaluating on the training split.
I0502 07:10:35.807203 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 07:10:44.626754 139815280891712 spec.py:326] Evaluating on the test split.
I0502 07:10:46.291096 139815280891712 submission_runner.py:415] Time since start: 20929.81s, 	Step: 19313, 	{'train/accuracy': 0.66796875, 'train/loss': 1.4613032341003418, 'validation/accuracy': 0.6147400140762329, 'validation/loss': 1.7056126594543457, 'validation/num_examples': 50000, 'test/accuracy': 0.49000000953674316, 'test/loss': 2.3593437671661377, 'test/num_examples': 10000, 'score': 20095.90381550789, 'total_duration': 20929.80507349968, 'accumulated_submission_time': 20095.90381550789, 'accumulated_eval_time': 832.3061635494232, 'accumulated_logging_time': 1.279527187347412}
I0502 07:10:46.310134 139584457389824 logging_writer.py:48] [19313] accumulated_eval_time=832.306164, accumulated_logging_time=1.279527, accumulated_submission_time=20095.903816, global_step=19313, preemption_count=0, score=20095.903816, test/accuracy=0.490000, test/loss=2.359344, test/num_examples=10000, total_duration=20929.805073, train/accuracy=0.667969, train/loss=1.461303, validation/accuracy=0.614740, validation/loss=1.705613, validation/num_examples=50000
I0502 07:12:20.172163 139584465782528 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.7927046418190002, loss=3.099269390106201
I0502 07:14:02.842556 139584457389824 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7964765429496765, loss=3.200645923614502
I0502 07:15:45.349493 139584465782528 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7474857568740845, loss=3.2180063724517822
I0502 07:17:28.137878 139584457389824 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.7334674596786499, loss=3.017292022705078
I0502 07:17:48.656826 139815280891712 spec.py:298] Evaluating on the training split.
I0502 07:17:55.930624 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 07:18:04.459943 139815280891712 spec.py:326] Evaluating on the test split.
I0502 07:18:06.127632 139815280891712 submission_runner.py:415] Time since start: 21369.64s, 	Step: 19721, 	{'train/accuracy': 0.6633984446525574, 'train/loss': 1.4948433637619019, 'validation/accuracy': 0.6168599724769592, 'validation/loss': 1.7143555879592896, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.3389101028442383, 'test/num_examples': 10000, 'score': 20518.231073141098, 'total_duration': 21369.641753196716, 'accumulated_submission_time': 20518.231073141098, 'accumulated_eval_time': 849.7756156921387, 'accumulated_logging_time': 1.312471866607666}
I0502 07:18:06.146721 139584465782528 logging_writer.py:48] [19721] accumulated_eval_time=849.775616, accumulated_logging_time=1.312472, accumulated_submission_time=20518.231073, global_step=19721, preemption_count=0, score=20518.231073, test/accuracy=0.501800, test/loss=2.338910, test/num_examples=10000, total_duration=21369.641753, train/accuracy=0.663398, train/loss=1.494843, validation/accuracy=0.616860, validation/loss=1.714356, validation/num_examples=50000
I0502 07:19:28.277188 139584457389824 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.0253914594650269, loss=5.221541404724121
I0502 07:21:11.404646 139584465782528 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.7048869729042053, loss=4.765870094299316
I0502 07:22:54.238520 139584457389824 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7092509865760803, loss=3.291273832321167
I0502 07:24:36.995845 139584465782528 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.7683332562446594, loss=3.126099109649658
I0502 07:25:06.575962 139815280891712 spec.py:298] Evaluating on the training split.
I0502 07:25:14.211152 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 07:25:22.781656 139815280891712 spec.py:326] Evaluating on the test split.
I0502 07:25:24.427371 139815280891712 submission_runner.py:415] Time since start: 21807.94s, 	Step: 20133, 	{'train/accuracy': 0.6953515410423279, 'train/loss': 1.3708524703979492, 'validation/accuracy': 0.622219979763031, 'validation/loss': 1.6914513111114502, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.324892282485962, 'test/num_examples': 10000, 'score': 20938.643839120865, 'total_duration': 21807.941600084305, 'accumulated_submission_time': 20938.643839120865, 'accumulated_eval_time': 867.625776052475, 'accumulated_logging_time': 1.3424482345581055}
I0502 07:25:24.449685 139584457389824 logging_writer.py:48] [20133] accumulated_eval_time=867.625776, accumulated_logging_time=1.342448, accumulated_submission_time=20938.643839, global_step=20133, preemption_count=0, score=20938.643839, test/accuracy=0.504500, test/loss=2.324892, test/num_examples=10000, total_duration=21807.941600, train/accuracy=0.695352, train/loss=1.370852, validation/accuracy=0.622220, validation/loss=1.691451, validation/num_examples=50000
I0502 07:26:38.049767 139584465782528 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7249169945716858, loss=3.5576179027557373
I0502 07:28:20.932088 139584457389824 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.8286164999008179, loss=3.5278449058532715
I0502 07:30:04.027235 139584465782528 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.2684996128082275, loss=4.989191055297852
I0502 07:31:47.014662 139584457389824 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.1933887004852295, loss=4.059263229370117
I0502 07:32:28.554028 139815280891712 spec.py:298] Evaluating on the training split.
I0502 07:32:35.934578 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 07:32:44.694844 139815280891712 spec.py:326] Evaluating on the test split.
I0502 07:32:46.337008 139815280891712 submission_runner.py:415] Time since start: 22249.85s, 	Step: 20541, 	{'train/accuracy': 0.6773632764816284, 'train/loss': 1.4184391498565674, 'validation/accuracy': 0.6247999668121338, 'validation/loss': 1.6651924848556519, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.3144419193267822, 'test/num_examples': 10000, 'score': 21362.729795455933, 'total_duration': 22249.851257801056, 'accumulated_submission_time': 21362.729795455933, 'accumulated_eval_time': 885.4075343608856, 'accumulated_logging_time': 1.3776235580444336}
I0502 07:32:46.349715 139584465782528 logging_writer.py:48] [20541] accumulated_eval_time=885.407534, accumulated_logging_time=1.377624, accumulated_submission_time=21362.729795, global_step=20541, preemption_count=0, score=21362.729795, test/accuracy=0.500400, test/loss=2.314442, test/num_examples=10000, total_duration=22249.851258, train/accuracy=0.677363, train/loss=1.418439, validation/accuracy=0.624800, validation/loss=1.665192, validation/num_examples=50000
I0502 07:33:47.948908 139584457389824 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.8331976532936096, loss=3.4309322834014893
I0502 07:35:31.037125 139584465782528 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.1708582639694214, loss=5.103827476501465
I0502 07:37:13.932110 139584457389824 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.7078495025634766, loss=3.1474547386169434
I0502 07:38:56.692157 139584465782528 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.8785122036933899, loss=5.071335315704346
I0502 07:39:46.626482 139815280891712 spec.py:298] Evaluating on the training split.
I0502 07:39:53.886343 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 07:40:02.719979 139815280891712 spec.py:326] Evaluating on the test split.
I0502 07:40:04.369369 139815280891712 submission_runner.py:415] Time since start: 22687.88s, 	Step: 20953, 	{'train/accuracy': 0.6841210722923279, 'train/loss': 1.3917090892791748, 'validation/accuracy': 0.6310799717903137, 'validation/loss': 1.6344183683395386, 'validation/num_examples': 50000, 'test/accuracy': 0.5089000463485718, 'test/loss': 2.2670302391052246, 'test/num_examples': 10000, 'score': 21782.987602710724, 'total_duration': 22687.883763313293, 'accumulated_submission_time': 21782.987602710724, 'accumulated_eval_time': 903.1493408679962, 'accumulated_logging_time': 1.4037363529205322}
I0502 07:40:04.382746 139584457389824 logging_writer.py:48] [20953] accumulated_eval_time=903.149341, accumulated_logging_time=1.403736, accumulated_submission_time=21782.987603, global_step=20953, preemption_count=0, score=21782.987603, test/accuracy=0.508900, test/loss=2.267030, test/num_examples=10000, total_duration=22687.883763, train/accuracy=0.684121, train/loss=1.391709, validation/accuracy=0.631080, validation/loss=1.634418, validation/num_examples=50000
I0502 07:40:57.482833 139584465782528 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7989360094070435, loss=2.9642562866210938
I0502 07:42:40.389714 139584457389824 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.9515398740768433, loss=3.0188567638397217
I0502 07:44:23.510269 139584465782528 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.8818743228912354, loss=3.0609025955200195
I0502 07:46:06.559933 139584457389824 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7808859944343567, loss=4.166447639465332
I0502 07:47:08.565371 139815280891712 spec.py:298] Evaluating on the training split.
I0502 07:47:15.796997 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 07:47:24.492982 139815280891712 spec.py:326] Evaluating on the test split.
I0502 07:47:26.153311 139815280891712 submission_runner.py:415] Time since start: 23129.67s, 	Step: 21361, 	{'train/accuracy': 0.6929882764816284, 'train/loss': 1.399707317352295, 'validation/accuracy': 0.6350399851799011, 'validation/loss': 1.6503700017929077, 'validation/num_examples': 50000, 'test/accuracy': 0.516700029373169, 'test/loss': 2.295945405960083, 'test/num_examples': 10000, 'score': 22207.15165424347, 'total_duration': 23129.66749238968, 'accumulated_submission_time': 22207.15165424347, 'accumulated_eval_time': 920.7359850406647, 'accumulated_logging_time': 1.4301187992095947}
I0502 07:47:26.171106 139584465782528 logging_writer.py:48] [21361] accumulated_eval_time=920.735985, accumulated_logging_time=1.430119, accumulated_submission_time=22207.151654, global_step=21361, preemption_count=0, score=22207.151654, test/accuracy=0.516700, test/loss=2.295945, test/num_examples=10000, total_duration=23129.667492, train/accuracy=0.692988, train/loss=1.399707, validation/accuracy=0.635040, validation/loss=1.650370, validation/num_examples=50000
I0502 07:48:07.219146 139584457389824 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6934413909912109, loss=3.263829231262207
I0502 07:49:50.156933 139584465782528 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8024862408638, loss=3.0295538902282715
I0502 07:51:32.789142 139584457389824 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.8111540675163269, loss=3.4680891036987305
I0502 07:53:15.733842 139584465782528 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.9694266319274902, loss=3.0724494457244873
I0502 07:54:26.866363 139815280891712 spec.py:298] Evaluating on the training split.
I0502 07:54:34.086747 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 07:54:42.818822 139815280891712 spec.py:326] Evaluating on the test split.
I0502 07:54:44.474915 139815280891712 submission_runner.py:415] Time since start: 23567.99s, 	Step: 21774, 	{'train/accuracy': 0.6981640458106995, 'train/loss': 1.324546217918396, 'validation/accuracy': 0.6363399624824524, 'validation/loss': 1.5979310274124146, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.2487776279449463, 'test/num_examples': 10000, 'score': 22627.827165603638, 'total_duration': 23567.989166259766, 'accumulated_submission_time': 22627.827165603638, 'accumulated_eval_time': 938.3433132171631, 'accumulated_logging_time': 1.4621117115020752}
I0502 07:54:44.494522 139584457389824 logging_writer.py:48] [21774] accumulated_eval_time=938.343313, accumulated_logging_time=1.462112, accumulated_submission_time=22627.827166, global_step=21774, preemption_count=0, score=22627.827166, test/accuracy=0.515800, test/loss=2.248778, test/num_examples=10000, total_duration=23567.989166, train/accuracy=0.698164, train/loss=1.324546, validation/accuracy=0.636340, validation/loss=1.597931, validation/num_examples=50000
I0502 07:55:16.294337 139584465782528 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.7340620160102844, loss=3.549163818359375
I0502 07:56:59.804925 139584457389824 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.822270929813385, loss=3.684180498123169
I0502 07:58:42.563955 139584465782528 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.029478907585144, loss=2.993738889694214
I0502 08:00:25.552307 139584457389824 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.9044255614280701, loss=3.0287158489227295
I0502 08:01:47.792500 139815280891712 spec.py:298] Evaluating on the training split.
I0502 08:01:55.101018 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 08:02:03.959125 139815280891712 spec.py:326] Evaluating on the test split.
I0502 08:02:05.621920 139815280891712 submission_runner.py:415] Time since start: 24009.14s, 	Step: 22181, 	{'train/accuracy': 0.6920703053474426, 'train/loss': 1.3648252487182617, 'validation/accuracy': 0.6392799615859985, 'validation/loss': 1.604385256767273, 'validation/num_examples': 50000, 'test/accuracy': 0.5136000514030457, 'test/loss': 2.247358798980713, 'test/num_examples': 10000, 'score': 23051.107225179672, 'total_duration': 24009.136248350143, 'accumulated_submission_time': 23051.107225179672, 'accumulated_eval_time': 956.1715850830078, 'accumulated_logging_time': 1.4941492080688477}
I0502 08:02:05.634925 139584465782528 logging_writer.py:48] [22181] accumulated_eval_time=956.171585, accumulated_logging_time=1.494149, accumulated_submission_time=23051.107225, global_step=22181, preemption_count=0, score=23051.107225, test/accuracy=0.513600, test/loss=2.247359, test/num_examples=10000, total_duration=24009.136248, train/accuracy=0.692070, train/loss=1.364825, validation/accuracy=0.639280, validation/loss=1.604385, validation/num_examples=50000
I0502 08:02:26.593562 139584457389824 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.984902560710907, loss=3.2028212547302246
I0502 08:04:09.480700 139584465782528 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.7768800258636475, loss=3.0475380420684814
I0502 08:05:52.416778 139584457389824 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.8352579474449158, loss=3.0856142044067383
I0502 08:07:35.332281 139584465782528 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8266675472259521, loss=4.494440078735352
I0502 08:09:06.325011 139815280891712 spec.py:298] Evaluating on the training split.
I0502 08:09:13.581960 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 08:09:22.314507 139815280891712 spec.py:326] Evaluating on the test split.
I0502 08:09:23.972729 139815280891712 submission_runner.py:415] Time since start: 24447.49s, 	Step: 22593, 	{'train/accuracy': 0.6877734065055847, 'train/loss': 1.3861896991729736, 'validation/accuracy': 0.6380999684333801, 'validation/loss': 1.6107978820800781, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.25054931640625, 'test/num_examples': 10000, 'score': 23471.77864933014, 'total_duration': 24447.48698449135, 'accumulated_submission_time': 23471.77864933014, 'accumulated_eval_time': 973.8181018829346, 'accumulated_logging_time': 1.5201361179351807}
I0502 08:09:23.986607 139584457389824 logging_writer.py:48] [22593] accumulated_eval_time=973.818102, accumulated_logging_time=1.520136, accumulated_submission_time=23471.778649, global_step=22593, preemption_count=0, score=23471.778649, test/accuracy=0.515800, test/loss=2.250549, test/num_examples=10000, total_duration=24447.486984, train/accuracy=0.687773, train/loss=1.386190, validation/accuracy=0.638100, validation/loss=1.610798, validation/num_examples=50000
I0502 08:09:36.135584 139584465782528 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.8383976221084595, loss=2.9910430908203125
I0502 08:11:19.103200 139584457389824 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.8983879089355469, loss=3.1182267665863037
I0502 08:13:02.066207 139584465782528 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.7644919157028198, loss=2.9769763946533203
I0502 08:14:45.104596 139584457389824 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.8635838031768799, loss=2.813171863555908
I0502 08:16:28.038521 139584465782528 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.780929684638977, loss=3.4702954292297363
I0502 08:16:28.083635 139815280891712 spec.py:298] Evaluating on the training split.
I0502 08:16:35.322843 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 08:16:44.145128 139815280891712 spec.py:326] Evaluating on the test split.
I0502 08:16:45.809249 139815280891712 submission_runner.py:415] Time since start: 24889.32s, 	Step: 23001, 	{'train/accuracy': 0.7114648222923279, 'train/loss': 1.2889801263809204, 'validation/accuracy': 0.6399399638175964, 'validation/loss': 1.6103993654251099, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.2331113815307617, 'test/num_examples': 10000, 'score': 23895.85806131363, 'total_duration': 24889.32353758812, 'accumulated_submission_time': 23895.85806131363, 'accumulated_eval_time': 991.5424997806549, 'accumulated_logging_time': 1.546048641204834}
I0502 08:16:45.823580 139584457389824 logging_writer.py:48] [23001] accumulated_eval_time=991.542500, accumulated_logging_time=1.546049, accumulated_submission_time=23895.858061, global_step=23001, preemption_count=0, score=23895.858061, test/accuracy=0.518600, test/loss=2.233111, test/num_examples=10000, total_duration=24889.323538, train/accuracy=0.711465, train/loss=1.288980, validation/accuracy=0.639940, validation/loss=1.610399, validation/num_examples=50000
I0502 08:18:29.091112 139584465782528 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.8333413600921631, loss=3.024035930633545
I0502 08:20:12.355502 139584457389824 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.199378490447998, loss=4.676509857177734
I0502 08:21:55.069163 139584465782528 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.8321526646614075, loss=3.044343948364258
I0502 08:23:37.697921 139584457389824 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.7426967024803162, loss=2.9182517528533936
I0502 08:23:46.445401 139815280891712 spec.py:298] Evaluating on the training split.
I0502 08:23:53.719869 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 08:24:02.505604 139815280891712 spec.py:326] Evaluating on the test split.
I0502 08:24:04.180250 139815280891712 submission_runner.py:415] Time since start: 25327.69s, 	Step: 23413, 	{'train/accuracy': 0.6994726657867432, 'train/loss': 1.3357393741607666, 'validation/accuracy': 0.6406399607658386, 'validation/loss': 1.6038819551467896, 'validation/num_examples': 50000, 'test/accuracy': 0.5163000226020813, 'test/loss': 2.2348439693450928, 'test/num_examples': 10000, 'score': 24316.46390581131, 'total_duration': 25327.694259166718, 'accumulated_submission_time': 24316.46390581131, 'accumulated_eval_time': 1009.2758927345276, 'accumulated_logging_time': 1.570772647857666}
I0502 08:24:04.199574 139584465782528 logging_writer.py:48] [23413] accumulated_eval_time=1009.275893, accumulated_logging_time=1.570773, accumulated_submission_time=24316.463906, global_step=23413, preemption_count=0, score=24316.463906, test/accuracy=0.516300, test/loss=2.234844, test/num_examples=10000, total_duration=25327.694259, train/accuracy=0.699473, train/loss=1.335739, validation/accuracy=0.640640, validation/loss=1.603882, validation/num_examples=50000
I0502 08:25:38.947049 139584457389824 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8550107479095459, loss=3.059636354446411
I0502 08:27:21.755264 139584465782528 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.7421811819076538, loss=3.910224676132202
I0502 08:29:04.877514 139584457389824 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.7420070767402649, loss=3.238701343536377
I0502 08:30:47.461743 139584465782528 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.7872559428215027, loss=2.9494521617889404
I0502 08:31:08.062726 139815280891712 spec.py:298] Evaluating on the training split.
I0502 08:31:15.300272 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 08:31:24.061380 139815280891712 spec.py:326] Evaluating on the test split.
I0502 08:31:25.719570 139815280891712 submission_runner.py:415] Time since start: 25769.23s, 	Step: 23821, 	{'train/accuracy': 0.6929296851158142, 'train/loss': 1.345149040222168, 'validation/accuracy': 0.642039954662323, 'validation/loss': 1.574186086654663, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.2225840091705322, 'test/num_examples': 10000, 'score': 24740.308277845383, 'total_duration': 25769.23399066925, 'accumulated_submission_time': 24740.308277845383, 'accumulated_eval_time': 1026.9316737651825, 'accumulated_logging_time': 1.6033740043640137}
I0502 08:31:25.739609 139584457389824 logging_writer.py:48] [23821] accumulated_eval_time=1026.931674, accumulated_logging_time=1.603374, accumulated_submission_time=24740.308278, global_step=23821, preemption_count=0, score=24740.308278, test/accuracy=0.517700, test/loss=2.222584, test/num_examples=10000, total_duration=25769.233991, train/accuracy=0.692930, train/loss=1.345149, validation/accuracy=0.642040, validation/loss=1.574186, validation/num_examples=50000
I0502 08:32:47.943048 139584465782528 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.7931427955627441, loss=3.101142406463623
I0502 08:34:30.985710 139584457389824 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.9206404089927673, loss=3.025239944458008
I0502 08:36:14.034001 139584465782528 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.9413594007492065, loss=2.9554972648620605
I0502 08:37:57.251186 139584457389824 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7501768469810486, loss=2.9577431678771973
I0502 08:38:25.993239 139815280891712 spec.py:298] Evaluating on the training split.
I0502 08:38:33.242902 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 08:38:42.108879 139815280891712 spec.py:326] Evaluating on the test split.
I0502 08:38:43.766280 139815280891712 submission_runner.py:415] Time since start: 26207.28s, 	Step: 24232, 	{'train/accuracy': 0.7093945145606995, 'train/loss': 1.2782460451126099, 'validation/accuracy': 0.6481800079345703, 'validation/loss': 1.5577459335327148, 'validation/num_examples': 50000, 'test/accuracy': 0.5297999978065491, 'test/loss': 2.206908702850342, 'test/num_examples': 10000, 'score': 25160.541655302048, 'total_duration': 26207.28022122383, 'accumulated_submission_time': 25160.541655302048, 'accumulated_eval_time': 1044.703180551529, 'accumulated_logging_time': 1.6380655765533447}
I0502 08:38:43.786411 139584465782528 logging_writer.py:48] [24232] accumulated_eval_time=1044.703181, accumulated_logging_time=1.638066, accumulated_submission_time=25160.541655, global_step=24232, preemption_count=0, score=25160.541655, test/accuracy=0.529800, test/loss=2.206909, test/num_examples=10000, total_duration=26207.280221, train/accuracy=0.709395, train/loss=1.278246, validation/accuracy=0.648180, validation/loss=1.557746, validation/num_examples=50000
I0502 08:39:58.336477 139584457389824 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.7471250295639038, loss=3.484330415725708
I0502 08:41:41.105092 139584465782528 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8997893333435059, loss=2.912952423095703
I0502 08:43:24.044749 139584457389824 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8794414401054382, loss=4.473483085632324
I0502 08:45:07.072593 139584465782528 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.1246094703674316, loss=2.9279377460479736
I0502 08:45:48.552137 139815280891712 spec.py:298] Evaluating on the training split.
I0502 08:45:55.775489 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 08:46:04.611804 139815280891712 spec.py:326] Evaluating on the test split.
I0502 08:46:06.270399 139815280891712 submission_runner.py:415] Time since start: 26649.78s, 	Step: 24641, 	{'train/accuracy': 0.7135351300239563, 'train/loss': 1.257032036781311, 'validation/accuracy': 0.6475200057029724, 'validation/loss': 1.549261212348938, 'validation/num_examples': 50000, 'test/accuracy': 0.5218000411987305, 'test/loss': 2.1899499893188477, 'test/num_examples': 10000, 'score': 25585.291342020035, 'total_duration': 26649.78462100029, 'accumulated_submission_time': 25585.291342020035, 'accumulated_eval_time': 1062.4201850891113, 'accumulated_logging_time': 1.6686794757843018}
I0502 08:46:06.290917 139584457389824 logging_writer.py:48] [24641] accumulated_eval_time=1062.420185, accumulated_logging_time=1.668679, accumulated_submission_time=25585.291342, global_step=24641, preemption_count=0, score=25585.291342, test/accuracy=0.521800, test/loss=2.189950, test/num_examples=10000, total_duration=26649.784621, train/accuracy=0.713535, train/loss=1.257032, validation/accuracy=0.647520, validation/loss=1.549261, validation/num_examples=50000
I0502 08:47:07.857376 139584465782528 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.746034562587738, loss=2.8637149333953857
I0502 08:48:50.922317 139584457389824 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.8341241478919983, loss=4.440075874328613
I0502 08:50:34.141825 139584465782528 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.8967626094818115, loss=3.081176996231079
I0502 08:52:17.136864 139584457389824 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7570456862449646, loss=3.856254816055298
I0502 08:53:06.798183 139815280891712 spec.py:298] Evaluating on the training split.
I0502 08:53:14.006945 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 08:53:22.718039 139815280891712 spec.py:326] Evaluating on the test split.
I0502 08:53:24.380338 139815280891712 submission_runner.py:415] Time since start: 27087.89s, 	Step: 25052, 	{'train/accuracy': 0.7065820097923279, 'train/loss': 1.2981622219085693, 'validation/accuracy': 0.6503999829292297, 'validation/loss': 1.5620273351669312, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.201713800430298, 'test/num_examples': 10000, 'score': 26005.78003501892, 'total_duration': 27087.894649267197, 'accumulated_submission_time': 26005.78003501892, 'accumulated_eval_time': 1080.0011739730835, 'accumulated_logging_time': 1.7021591663360596}
I0502 08:53:24.393997 139584465782528 logging_writer.py:48] [25052] accumulated_eval_time=1080.001174, accumulated_logging_time=1.702159, accumulated_submission_time=26005.780035, global_step=25052, preemption_count=0, score=26005.780035, test/accuracy=0.528100, test/loss=2.201714, test/num_examples=10000, total_duration=27087.894649, train/accuracy=0.706582, train/loss=1.298162, validation/accuracy=0.650400, validation/loss=1.562027, validation/num_examples=50000
I0502 08:54:18.342740 139584457389824 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.13271164894104, loss=5.031415939331055
I0502 08:56:01.446202 139584465782528 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.1486634016036987, loss=2.8418757915496826
I0502 08:57:44.440640 139584457389824 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.786622941493988, loss=2.925311326980591
I0502 08:59:27.615301 139584465782528 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.8014100790023804, loss=3.977015495300293
I0502 09:00:29.725379 139815280891712 spec.py:298] Evaluating on the training split.
I0502 09:00:36.951087 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 09:00:45.714470 139815280891712 spec.py:326] Evaluating on the test split.
I0502 09:00:47.376295 139815280891712 submission_runner.py:415] Time since start: 27530.89s, 	Step: 25461, 	{'train/accuracy': 0.7049804329872131, 'train/loss': 1.3283151388168335, 'validation/accuracy': 0.6520000100135803, 'validation/loss': 1.5657927989959717, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.212352752685547, 'test/num_examples': 10000, 'score': 26431.09242415428, 'total_duration': 27530.89063000679, 'accumulated_submission_time': 26431.09242415428, 'accumulated_eval_time': 1097.6509501934052, 'accumulated_logging_time': 1.7293190956115723}
I0502 09:00:47.396938 139584457389824 logging_writer.py:48] [25461] accumulated_eval_time=1097.650950, accumulated_logging_time=1.729319, accumulated_submission_time=26431.092424, global_step=25461, preemption_count=0, score=26431.092424, test/accuracy=0.529300, test/loss=2.212353, test/num_examples=10000, total_duration=27530.890630, train/accuracy=0.704980, train/loss=1.328315, validation/accuracy=0.652000, validation/loss=1.565793, validation/num_examples=50000
I0502 09:01:28.603740 139584465782528 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8581173419952393, loss=2.904358386993408
I0502 09:03:11.756845 139584457389824 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.8211710453033447, loss=3.995255947113037
I0502 09:04:54.945274 139584465782528 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8825501799583435, loss=2.947479486465454
I0502 09:06:38.280987 139584457389824 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.8362801671028137, loss=3.048741340637207
I0502 09:07:48.097233 139815280891712 spec.py:298] Evaluating on the training split.
I0502 09:07:55.296405 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 09:08:04.044035 139815280891712 spec.py:326] Evaluating on the test split.
I0502 09:08:05.691110 139815280891712 submission_runner.py:415] Time since start: 27969.21s, 	Step: 25871, 	{'train/accuracy': 0.729296863079071, 'train/loss': 1.2073310613632202, 'validation/accuracy': 0.655239999294281, 'validation/loss': 1.530431866645813, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.176771879196167, 'test/num_examples': 10000, 'score': 26851.774047851562, 'total_duration': 27969.20543575287, 'accumulated_submission_time': 26851.774047851562, 'accumulated_eval_time': 1115.243684053421, 'accumulated_logging_time': 1.7630674839019775}
I0502 09:08:05.705572 139584465782528 logging_writer.py:48] [25871] accumulated_eval_time=1115.243684, accumulated_logging_time=1.763067, accumulated_submission_time=26851.774048, global_step=25871, preemption_count=0, score=26851.774048, test/accuracy=0.531300, test/loss=2.176772, test/num_examples=10000, total_duration=27969.205436, train/accuracy=0.729297, train/loss=1.207331, validation/accuracy=0.655240, validation/loss=1.530432, validation/num_examples=50000
I0502 09:08:39.794312 139584457389824 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.005881667137146, loss=3.048100709915161
I0502 09:10:22.828477 139584465782528 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.9702991843223572, loss=2.93135404586792
I0502 09:12:05.652979 139584457389824 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.7881814241409302, loss=2.986572742462158
I0502 09:13:48.751196 139584465782528 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.816933512687683, loss=4.58731746673584
I0502 09:15:11.587265 139815280891712 spec.py:298] Evaluating on the training split.
I0502 09:15:18.898610 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 09:15:27.603206 139815280891712 spec.py:326] Evaluating on the test split.
I0502 09:15:29.272627 139815280891712 submission_runner.py:415] Time since start: 28412.79s, 	Step: 26281, 	{'train/accuracy': 0.718066394329071, 'train/loss': 1.2039657831192017, 'validation/accuracy': 0.6617199778556824, 'validation/loss': 1.4720354080200195, 'validation/num_examples': 50000, 'test/accuracy': 0.5378000140190125, 'test/loss': 2.123887300491333, 'test/num_examples': 10000, 'score': 27277.636660337448, 'total_duration': 28412.786891937256, 'accumulated_submission_time': 27277.636660337448, 'accumulated_eval_time': 1132.9278500080109, 'accumulated_logging_time': 1.7910280227661133}
I0502 09:15:29.287380 139584457389824 logging_writer.py:48] [26281] accumulated_eval_time=1132.927850, accumulated_logging_time=1.791028, accumulated_submission_time=27277.636660, global_step=26281, preemption_count=0, score=27277.636660, test/accuracy=0.537800, test/loss=2.123887, test/num_examples=10000, total_duration=28412.786892, train/accuracy=0.718066, train/loss=1.203966, validation/accuracy=0.661720, validation/loss=1.472035, validation/num_examples=50000
I0502 09:15:50.222243 139584465782528 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.8862524628639221, loss=2.9587888717651367
I0502 09:17:34.001729 139584457389824 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.8126964569091797, loss=2.901453733444214
I0502 09:19:17.161232 139584465782528 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.9976884126663208, loss=2.9173123836517334
I0502 09:21:00.886413 139584457389824 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.8112763166427612, loss=3.0235517024993896
I0502 09:22:29.514548 139815280891712 spec.py:298] Evaluating on the training split.
I0502 09:22:36.721316 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 09:22:45.463004 139815280891712 spec.py:326] Evaluating on the test split.
I0502 09:22:47.127677 139815280891712 submission_runner.py:415] Time since start: 28850.64s, 	Step: 26689, 	{'train/accuracy': 0.7130468487739563, 'train/loss': 1.2613356113433838, 'validation/accuracy': 0.6563199758529663, 'validation/loss': 1.517512321472168, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.1700754165649414, 'test/num_examples': 10000, 'score': 27697.84457063675, 'total_duration': 28850.641550779343, 'accumulated_submission_time': 27697.84457063675, 'accumulated_eval_time': 1150.5393764972687, 'accumulated_logging_time': 1.8195197582244873}
I0502 09:22:47.148476 139584465782528 logging_writer.py:48] [26689] accumulated_eval_time=1150.539376, accumulated_logging_time=1.819520, accumulated_submission_time=27697.844571, global_step=26689, preemption_count=0, score=27697.844571, test/accuracy=0.529400, test/loss=2.170075, test/num_examples=10000, total_duration=28850.641551, train/accuracy=0.713047, train/loss=1.261336, validation/accuracy=0.656320, validation/loss=1.517512, validation/num_examples=50000
I0502 09:23:01.873896 139584457389824 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.8980454802513123, loss=2.974458694458008
I0502 09:24:44.956122 139584465782528 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.1033644676208496, loss=2.874340057373047
I0502 09:26:28.181657 139584457389824 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.9796326160430908, loss=2.995595932006836
I0502 09:28:11.289862 139584465782528 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.8981359004974365, loss=3.876802921295166
I0502 09:29:47.597868 139815280891712 spec.py:298] Evaluating on the training split.
I0502 09:29:54.827422 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 09:30:03.533145 139815280891712 spec.py:326] Evaluating on the test split.
I0502 09:30:05.205023 139815280891712 submission_runner.py:415] Time since start: 29288.72s, 	Step: 27099, 	{'train/accuracy': 0.7370312213897705, 'train/loss': 1.184836983680725, 'validation/accuracy': 0.6611599922180176, 'validation/loss': 1.5080310106277466, 'validation/num_examples': 50000, 'test/accuracy': 0.5338000059127808, 'test/loss': 2.154926300048828, 'test/num_examples': 10000, 'score': 28118.275394439697, 'total_duration': 29288.719147205353, 'accumulated_submission_time': 28118.275394439697, 'accumulated_eval_time': 1168.1451878547668, 'accumulated_logging_time': 1.8533272743225098}
I0502 09:30:05.223451 139584457389824 logging_writer.py:48] [27099] accumulated_eval_time=1168.145188, accumulated_logging_time=1.853327, accumulated_submission_time=28118.275394, global_step=27099, preemption_count=0, score=28118.275394, test/accuracy=0.533800, test/loss=2.154926, test/num_examples=10000, total_duration=29288.719147, train/accuracy=0.737031, train/loss=1.184837, validation/accuracy=0.661160, validation/loss=1.508031, validation/num_examples=50000
I0502 09:30:12.402307 139584465782528 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.8232388496398926, loss=3.509830951690674
I0502 09:31:55.248180 139584457389824 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.8789188861846924, loss=3.86433482170105
I0502 09:33:38.373577 139584465782528 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.8669666647911072, loss=3.8553342819213867
I0502 09:35:21.727624 139584457389824 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.9020694494247437, loss=4.14703369140625
I0502 09:37:05.617650 139584465782528 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8906252384185791, loss=2.8388543128967285
I0502 09:37:05.666049 139815280891712 spec.py:298] Evaluating on the training split.
I0502 09:37:12.885146 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 09:37:21.671197 139815280891712 spec.py:326] Evaluating on the test split.
I0502 09:37:23.316445 139815280891712 submission_runner.py:415] Time since start: 29726.83s, 	Step: 27501, 	{'train/accuracy': 0.7279101610183716, 'train/loss': 1.194553256034851, 'validation/accuracy': 0.660319983959198, 'validation/loss': 1.4935650825500488, 'validation/num_examples': 50000, 'test/accuracy': 0.5389000177383423, 'test/loss': 2.1363677978515625, 'test/num_examples': 10000, 'score': 28538.69926929474, 'total_duration': 29726.830756902695, 'accumulated_submission_time': 28538.69926929474, 'accumulated_eval_time': 1185.7943935394287, 'accumulated_logging_time': 1.8851313591003418}
I0502 09:37:23.335960 139584457389824 logging_writer.py:48] [27501] accumulated_eval_time=1185.794394, accumulated_logging_time=1.885131, accumulated_submission_time=28538.699269, global_step=27501, preemption_count=0, score=28538.699269, test/accuracy=0.538900, test/loss=2.136368, test/num_examples=10000, total_duration=29726.830757, train/accuracy=0.727910, train/loss=1.194553, validation/accuracy=0.660320, validation/loss=1.493565, validation/num_examples=50000
I0502 09:39:06.410660 139584465782528 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8425915241241455, loss=2.8893330097198486
I0502 09:40:49.945743 139584457389824 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.8923304677009583, loss=4.558169364929199
I0502 09:42:33.450491 139584465782528 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.216485857963562, loss=4.56251335144043
I0502 09:44:16.754498 139584457389824 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.8192514181137085, loss=4.820633411407471
I0502 09:44:24.013906 139815280891712 spec.py:298] Evaluating on the training split.
I0502 09:44:31.294857 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 09:44:40.155877 139815280891712 spec.py:326] Evaluating on the test split.
I0502 09:44:41.820882 139815280891712 submission_runner.py:415] Time since start: 30165.34s, 	Step: 27911, 	{'train/accuracy': 0.7194726467132568, 'train/loss': 1.2184903621673584, 'validation/accuracy': 0.6620599627494812, 'validation/loss': 1.4811300039291382, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.136683225631714, 'test/num_examples': 10000, 'score': 28959.358723640442, 'total_duration': 30165.33520245552, 'accumulated_submission_time': 28959.358723640442, 'accumulated_eval_time': 1203.6002326011658, 'accumulated_logging_time': 1.9175615310668945}
I0502 09:44:41.838294 139584465782528 logging_writer.py:48] [27911] accumulated_eval_time=1203.600233, accumulated_logging_time=1.917562, accumulated_submission_time=28959.358724, global_step=27911, preemption_count=0, score=28959.358724, test/accuracy=0.538600, test/loss=2.136683, test/num_examples=10000, total_duration=30165.335202, train/accuracy=0.719473, train/loss=1.218490, validation/accuracy=0.662060, validation/loss=1.481130, validation/num_examples=50000
I0502 09:46:11.970367 139815280891712 spec.py:298] Evaluating on the training split.
I0502 09:46:19.274220 139815280891712 spec.py:310] Evaluating on the validation split.
I0502 09:46:27.987328 139815280891712 spec.py:326] Evaluating on the test split.
I0502 09:46:29.647624 139815280891712 submission_runner.py:415] Time since start: 30273.16s, 	Step: 28000, 	{'train/accuracy': 0.7194140553474426, 'train/loss': 1.2568016052246094, 'validation/accuracy': 0.662339985370636, 'validation/loss': 1.512110710144043, 'validation/num_examples': 50000, 'test/accuracy': 0.5423000454902649, 'test/loss': 2.1370725631713867, 'test/num_examples': 10000, 'score': 29049.47698688507, 'total_duration': 30273.163052797318, 'accumulated_submission_time': 29049.47698688507, 'accumulated_eval_time': 1221.277441740036, 'accumulated_logging_time': 1.947253704071045}
I0502 09:46:29.664003 139584457389824 logging_writer.py:48] [28000] accumulated_eval_time=1221.277442, accumulated_logging_time=1.947254, accumulated_submission_time=29049.476987, global_step=28000, preemption_count=0, score=29049.476987, test/accuracy=0.542300, test/loss=2.137073, test/num_examples=10000, total_duration=30273.163053, train/accuracy=0.719414, train/loss=1.256802, validation/accuracy=0.662340, validation/loss=1.512111, validation/num_examples=50000
I0502 09:46:29.688396 139584465782528 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=29049.476987
I0502 09:46:31.537599 139815280891712 checkpoints.py:356] Saving checkpoint at step: 28000
I0502 09:46:36.361704 139815280891712 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_vit_jax/trial_1/checkpoint_28000
I0502 09:46:36.479400 139815280891712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_vit_jax/trial_1/checkpoint_28000.
I0502 09:46:37.947588 139815280891712 submission_runner.py:578] Tuning trial 1/1
I0502 09:46:37.947832 139815280891712 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.07758862577375368, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0502 09:46:37.977352 139815280891712 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009570312104187906, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 293.10331749916077, 'total_duration': 342.88277888298035, 'accumulated_submission_time': 293.10331749916077, 'accumulated_eval_time': 49.77930545806885, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (141, {'train/accuracy': 0.005644530989229679, 'train/loss': 6.78612756729126, 'validation/accuracy': 0.0047599999234080315, 'validation/loss': 6.79502534866333, 'validation/num_examples': 50000, 'test/accuracy': 0.004800000227987766, 'test/loss': 6.804804801940918, 'test/num_examples': 10000, 'score': 715.4983599185944, 'total_duration': 780.1479647159576, 'accumulated_submission_time': 715.4983599185944, 'accumulated_eval_time': 64.62202835083008, 'accumulated_logging_time': 0.02530217170715332, 'global_step': 141, 'preemption_count': 0}), (571, {'train/accuracy': 0.026992186903953552, 'train/loss': 6.2548956871032715, 'validation/accuracy': 0.024620000272989273, 'validation/loss': 6.267775058746338, 'validation/num_examples': 50000, 'test/accuracy': 0.019500000402331352, 'test/loss': 6.330530166625977, 'test/num_examples': 10000, 'score': 1136.0484211444855, 'total_duration': 1215.5743610858917, 'accumulated_submission_time': 1136.0484211444855, 'accumulated_eval_time': 79.47144794464111, 'accumulated_logging_time': 0.046309709548950195, 'global_step': 571, 'preemption_count': 0}), (997, {'train/accuracy': 0.05292968451976776, 'train/loss': 5.721033573150635, 'validation/accuracy': 0.049379996955394745, 'validation/loss': 5.746005535125732, 'validation/num_examples': 50000, 'test/accuracy': 0.038700003176927567, 'test/loss': 5.872519016265869, 'test/num_examples': 10000, 'score': 1556.218989610672, 'total_duration': 1650.6789247989655, 'accumulated_submission_time': 1556.218989610672, 'accumulated_eval_time': 94.37130904197693, 'accumulated_logging_time': 0.07445979118347168, 'global_step': 997, 'preemption_count': 0}), (1421, {'train/accuracy': 0.07681640237569809, 'train/loss': 5.360898971557617, 'validation/accuracy': 0.0712599977850914, 'validation/loss': 5.396115303039551, 'validation/num_examples': 50000, 'test/accuracy': 0.05470000207424164, 'test/loss': 5.5779571533203125, 'test/num_examples': 10000, 'score': 1980.948757648468, 'total_duration': 2090.1627271175385, 'accumulated_submission_time': 1980.948757648468, 'accumulated_eval_time': 109.09906697273254, 'accumulated_logging_time': 0.09495401382446289, 'global_step': 1421, 'preemption_count': 0}), (1844, {'train/accuracy': 0.1064453125, 'train/loss': 5.026849746704102, 'validation/accuracy': 0.10269999504089355, 'validation/loss': 5.072316646575928, 'validation/num_examples': 50000, 'test/accuracy': 0.07840000092983246, 'test/loss': 5.320460319519043, 'test/num_examples': 10000, 'score': 2401.123947620392, 'total_duration': 2525.3330557346344, 'accumulated_submission_time': 2401.123947620392, 'accumulated_eval_time': 124.06539368629456, 'accumulated_logging_time': 0.11805987358093262, 'global_step': 1844, 'preemption_count': 0}), (2268, {'train/accuracy': 0.14701171219348907, 'train/loss': 4.67155647277832, 'validation/accuracy': 0.1361600011587143, 'validation/loss': 4.741461277008057, 'validation/num_examples': 50000, 'test/accuracy': 0.1072000041604042, 'test/loss': 5.021162509918213, 'test/num_examples': 10000, 'score': 2821.522177219391, 'total_duration': 2960.6004118919373, 'accumulated_submission_time': 2821.522177219391, 'accumulated_eval_time': 138.894606590271, 'accumulated_logging_time': 0.15212011337280273, 'global_step': 2268, 'preemption_count': 0}), (2690, {'train/accuracy': 0.18089842796325684, 'train/loss': 4.3392181396484375, 'validation/accuracy': 0.17189998924732208, 'validation/loss': 4.4079155921936035, 'validation/num_examples': 50000, 'test/accuracy': 0.12630000710487366, 'test/loss': 4.742824554443359, 'test/num_examples': 10000, 'score': 3241.9575579166412, 'total_duration': 3396.031106710434, 'accumulated_submission_time': 3241.9575579166412, 'accumulated_eval_time': 153.86326241493225, 'accumulated_logging_time': 0.17289447784423828, 'global_step': 2690, 'preemption_count': 0}), (3110, {'train/accuracy': 0.2311132699251175, 'train/loss': 4.003273963928223, 'validation/accuracy': 0.20601999759674072, 'validation/loss': 4.125072479248047, 'validation/num_examples': 50000, 'test/accuracy': 0.16340000927448273, 'test/loss': 4.485968589782715, 'test/num_examples': 10000, 'score': 3662.4443802833557, 'total_duration': 3831.379652738571, 'accumulated_submission_time': 3662.4443802833557, 'accumulated_eval_time': 168.69462847709656, 'accumulated_logging_time': 0.19739818572998047, 'global_step': 3110, 'preemption_count': 0}), (3541, {'train/accuracy': 0.2522656321525574, 'train/loss': 3.8324990272521973, 'validation/accuracy': 0.23201999068260193, 'validation/loss': 3.9364278316497803, 'validation/num_examples': 50000, 'test/accuracy': 0.17760001122951508, 'test/loss': 4.351043224334717, 'test/num_examples': 10000, 'score': 4088.634847640991, 'total_duration': 4272.614780902863, 'accumulated_submission_time': 4088.634847640991, 'accumulated_eval_time': 183.7120566368103, 'accumulated_logging_time': 0.21915936470031738, 'global_step': 3541, 'preemption_count': 0}), (3974, {'train/accuracy': 0.2874804735183716, 'train/loss': 3.583763599395752, 'validation/accuracy': 0.2658799886703491, 'validation/loss': 3.685100793838501, 'validation/num_examples': 50000, 'test/accuracy': 0.19780001044273376, 'test/loss': 4.143375873565674, 'test/num_examples': 10000, 'score': 4509.034639835358, 'total_duration': 4707.911815166473, 'accumulated_submission_time': 4509.034639835358, 'accumulated_eval_time': 198.57635688781738, 'accumulated_logging_time': 0.24656295776367188, 'global_step': 3974, 'preemption_count': 0}), (4401, {'train/accuracy': 0.3308202922344208, 'train/loss': 3.318467617034912, 'validation/accuracy': 0.2913399934768677, 'validation/loss': 3.52135968208313, 'validation/num_examples': 50000, 'test/accuracy': 0.22050000727176666, 'test/loss': 3.993187189102173, 'test/num_examples': 10000, 'score': 4933.385240793228, 'total_duration': 5147.726042032242, 'accumulated_submission_time': 4933.385240793228, 'accumulated_eval_time': 214.00714874267578, 'accumulated_logging_time': 0.27379894256591797, 'global_step': 4401, 'preemption_count': 0}), (4832, {'train/accuracy': 0.3401757776737213, 'train/loss': 3.2193191051483154, 'validation/accuracy': 0.3124399781227112, 'validation/loss': 3.3668107986450195, 'validation/num_examples': 50000, 'test/accuracy': 0.2419000118970871, 'test/loss': 3.8793532848358154, 'test/num_examples': 10000, 'score': 5353.934933662415, 'total_duration': 5583.377871274948, 'accumulated_submission_time': 5353.934933662415, 'accumulated_eval_time': 229.0836923122406, 'accumulated_logging_time': 0.293809175491333, 'global_step': 4832, 'preemption_count': 0}), (5261, {'train/accuracy': 0.36226561665534973, 'train/loss': 3.0730977058410645, 'validation/accuracy': 0.3337399959564209, 'validation/loss': 3.2248952388763428, 'validation/num_examples': 50000, 'test/accuracy': 0.25710001587867737, 'test/loss': 3.747987747192383, 'test/num_examples': 10000, 'score': 5778.510552883148, 'total_duration': 6023.14445233345, 'accumulated_submission_time': 5778.510552883148, 'accumulated_eval_time': 244.24185347557068, 'accumulated_logging_time': 0.32102108001708984, 'global_step': 5261, 'preemption_count': 0}), (5689, {'train/accuracy': 0.37546873092651367, 'train/loss': 3.0548465251922607, 'validation/accuracy': 0.34417998790740967, 'validation/loss': 3.2012991905212402, 'validation/num_examples': 50000, 'test/accuracy': 0.2587999999523163, 'test/loss': 3.7431299686431885, 'test/num_examples': 10000, 'score': 6198.8203592300415, 'total_duration': 6458.435411453247, 'accumulated_submission_time': 6198.8203592300415, 'accumulated_eval_time': 259.19161653518677, 'accumulated_logging_time': 0.3468759059906006, 'global_step': 5689, 'preemption_count': 0}), (6116, {'train/accuracy': 0.3955078125, 'train/loss': 2.872520923614502, 'validation/accuracy': 0.3594599962234497, 'validation/loss': 3.0631237030029297, 'validation/num_examples': 50000, 'test/accuracy': 0.2775000035762787, 'test/loss': 3.627045154571533, 'test/num_examples': 10000, 'score': 6619.458873510361, 'total_duration': 6894.007937192917, 'accumulated_submission_time': 6619.458873510361, 'accumulated_eval_time': 274.09437251091003, 'accumulated_logging_time': 0.37261414527893066, 'global_step': 6116, 'preemption_count': 0}), (6537, {'train/accuracy': 0.3984375, 'train/loss': 2.920804262161255, 'validation/accuracy': 0.3664399981498718, 'validation/loss': 3.0777952671051025, 'validation/num_examples': 50000, 'test/accuracy': 0.2803000211715698, 'test/loss': 3.640188455581665, 'test/num_examples': 10000, 'score': 7040.157104253769, 'total_duration': 7330.018122434616, 'accumulated_submission_time': 7040.157104253769, 'accumulated_eval_time': 289.3788046836853, 'accumulated_logging_time': 0.3948063850402832, 'global_step': 6537, 'preemption_count': 0}), (6959, {'train/accuracy': 0.410468727350235, 'train/loss': 2.7761077880859375, 'validation/accuracy': 0.38580000400543213, 'validation/loss': 2.9107067584991455, 'validation/num_examples': 50000, 'test/accuracy': 0.2939999997615814, 'test/loss': 3.5029091835021973, 'test/num_examples': 10000, 'score': 7460.678845643997, 'total_duration': 7765.777581691742, 'accumulated_submission_time': 7460.678845643997, 'accumulated_eval_time': 304.5840485095978, 'accumulated_logging_time': 0.42180562019348145, 'global_step': 6959, 'preemption_count': 0}), (7377, {'train/accuracy': 0.4284375011920929, 'train/loss': 2.748767375946045, 'validation/accuracy': 0.3889999985694885, 'validation/loss': 2.9412271976470947, 'validation/num_examples': 50000, 'test/accuracy': 0.2962000072002411, 'test/loss': 3.5117692947387695, 'test/num_examples': 10000, 'score': 7881.068476438522, 'total_duration': 8201.802328109741, 'accumulated_submission_time': 7881.068476438522, 'accumulated_eval_time': 320.1675887107849, 'accumulated_logging_time': 0.44571495056152344, 'global_step': 7377, 'preemption_count': 0}), (7793, {'train/accuracy': 0.4378320276737213, 'train/loss': 2.6273064613342285, 'validation/accuracy': 0.40629997849464417, 'validation/loss': 2.794783353805542, 'validation/num_examples': 50000, 'test/accuracy': 0.3125, 'test/loss': 3.3953611850738525, 'test/num_examples': 10000, 'score': 8301.493917942047, 'total_duration': 8638.395778417587, 'accumulated_submission_time': 8301.493917942047, 'accumulated_eval_time': 336.3012204170227, 'accumulated_logging_time': 0.47455286979675293, 'global_step': 7793, 'preemption_count': 0}), (8210, {'train/accuracy': 0.4371679723262787, 'train/loss': 2.724919319152832, 'validation/accuracy': 0.40275999903678894, 'validation/loss': 2.8738462924957275, 'validation/num_examples': 50000, 'test/accuracy': 0.3078000247478485, 'test/loss': 3.456850051879883, 'test/num_examples': 10000, 'score': 8722.094465494156, 'total_duration': 9075.980795621872, 'accumulated_submission_time': 8722.094465494156, 'accumulated_eval_time': 353.2565402984619, 'accumulated_logging_time': 0.4982414245605469, 'global_step': 8210, 'preemption_count': 0}), (8621, {'train/accuracy': 0.4775976538658142, 'train/loss': 2.436032295227051, 'validation/accuracy': 0.41495999693870544, 'validation/loss': 2.736457347869873, 'validation/num_examples': 50000, 'test/accuracy': 0.32120001316070557, 'test/loss': 3.3401036262512207, 'test/num_examples': 10000, 'score': 9145.220792531967, 'total_duration': 9516.41585636139, 'accumulated_submission_time': 9145.220792531967, 'accumulated_eval_time': 370.53495693206787, 'accumulated_logging_time': 0.5231797695159912, 'global_step': 8621, 'preemption_count': 0}), (9035, {'train/accuracy': 0.4689257740974426, 'train/loss': 2.5299060344696045, 'validation/accuracy': 0.4281199872493744, 'validation/loss': 2.715872049331665, 'validation/num_examples': 50000, 'test/accuracy': 0.3286000192165375, 'test/loss': 3.315652847290039, 'test/num_examples': 10000, 'score': 9566.173962831497, 'total_duration': 9955.18007850647, 'accumulated_submission_time': 9566.173962831497, 'accumulated_eval_time': 388.31133365631104, 'accumulated_logging_time': 0.5522511005401611, 'global_step': 9035, 'preemption_count': 0}), (9445, {'train/accuracy': 0.4703906178474426, 'train/loss': 2.5148401260375977, 'validation/accuracy': 0.43845999240875244, 'validation/loss': 2.6814186573028564, 'validation/num_examples': 50000, 'test/accuracy': 0.33490002155303955, 'test/loss': 3.27959942817688, 'test/num_examples': 10000, 'score': 9986.361079216003, 'total_duration': 10393.26552438736, 'accumulated_submission_time': 9986.361079216003, 'accumulated_eval_time': 406.1732249259949, 'accumulated_logging_time': 0.5818643569946289, 'global_step': 9445, 'preemption_count': 0}), (9861, {'train/accuracy': 0.48093748092651367, 'train/loss': 2.401608467102051, 'validation/accuracy': 0.45047998428344727, 'validation/loss': 2.550164222717285, 'validation/num_examples': 50000, 'test/accuracy': 0.34610000252723694, 'test/loss': 3.1743485927581787, 'test/num_examples': 10000, 'score': 10412.126942396164, 'total_duration': 10836.60283112526, 'accumulated_submission_time': 10412.126942396164, 'accumulated_eval_time': 423.70794320106506, 'accumulated_logging_time': 0.6112816333770752, 'global_step': 9861, 'preemption_count': 0}), (10277, {'train/accuracy': 0.5069335699081421, 'train/loss': 2.2423994541168213, 'validation/accuracy': 0.46685999631881714, 'validation/loss': 2.4583487510681152, 'validation/num_examples': 50000, 'test/accuracy': 0.36510002613067627, 'test/loss': 3.0772602558135986, 'test/num_examples': 10000, 'score': 10832.616870164871, 'total_duration': 11274.638753175735, 'accumulated_submission_time': 10832.616870164871, 'accumulated_eval_time': 441.2173070907593, 'accumulated_logging_time': 0.6407623291015625, 'global_step': 10277, 'preemption_count': 0}), (10684, {'train/accuracy': 0.5169726610183716, 'train/loss': 2.2007522583007812, 'validation/accuracy': 0.48051998019218445, 'validation/loss': 2.3710169792175293, 'validation/num_examples': 50000, 'test/accuracy': 0.3743000030517578, 'test/loss': 3.0191054344177246, 'test/num_examples': 10000, 'score': 11252.811816692352, 'total_duration': 11712.37395477295, 'accumulated_submission_time': 11252.811816692352, 'accumulated_eval_time': 458.72066259384155, 'accumulated_logging_time': 0.6706266403198242, 'global_step': 10684, 'preemption_count': 0}), (11100, {'train/accuracy': 0.5300585627555847, 'train/loss': 2.096954107284546, 'validation/accuracy': 0.494519978761673, 'validation/loss': 2.2770297527313232, 'validation/num_examples': 50000, 'test/accuracy': 0.37780001759529114, 'test/loss': 2.933253765106201, 'test/num_examples': 10000, 'score': 11673.278126239777, 'total_duration': 12151.15645289421, 'accumulated_submission_time': 11673.278126239777, 'accumulated_eval_time': 477.00423645973206, 'accumulated_logging_time': 0.696491003036499, 'global_step': 11100, 'preemption_count': 0}), (11508, {'train/accuracy': 0.5736523270606995, 'train/loss': 1.9004149436950684, 'validation/accuracy': 0.5095999836921692, 'validation/loss': 2.209139585494995, 'validation/num_examples': 50000, 'test/accuracy': 0.3969000279903412, 'test/loss': 2.8658907413482666, 'test/num_examples': 10000, 'score': 12093.323530435562, 'total_duration': 12588.707181692123, 'accumulated_submission_time': 12093.323530435562, 'accumulated_eval_time': 494.47333669662476, 'accumulated_logging_time': 0.7256667613983154, 'global_step': 11508, 'preemption_count': 0}), (11921, {'train/accuracy': 0.5526366829872131, 'train/loss': 2.014735698699951, 'validation/accuracy': 0.509880006313324, 'validation/loss': 2.2083921432495117, 'validation/num_examples': 50000, 'test/accuracy': 0.4020000100135803, 'test/loss': 2.8412797451019287, 'test/num_examples': 10000, 'score': 12516.122400522232, 'total_duration': 13030.00281572342, 'accumulated_submission_time': 12516.122400522232, 'accumulated_eval_time': 512.9370329380035, 'accumulated_logging_time': 0.7518754005432129, 'global_step': 11921, 'preemption_count': 0}), (12339, {'train/accuracy': 0.5564648509025574, 'train/loss': 2.0221972465515137, 'validation/accuracy': 0.5171599984169006, 'validation/loss': 2.1966042518615723, 'validation/num_examples': 50000, 'test/accuracy': 0.4090000092983246, 'test/loss': 2.826739549636841, 'test/num_examples': 10000, 'score': 12936.69292807579, 'total_duration': 13468.148062944412, 'accumulated_submission_time': 12936.69292807579, 'accumulated_eval_time': 530.4739997386932, 'accumulated_logging_time': 0.7827839851379395, 'global_step': 12339, 'preemption_count': 0}), (12747, {'train/accuracy': 0.568652331829071, 'train/loss': 1.94989013671875, 'validation/accuracy': 0.5304399728775024, 'validation/loss': 2.1401023864746094, 'validation/num_examples': 50000, 'test/accuracy': 0.41530001163482666, 'test/loss': 2.780726194381714, 'test/num_examples': 10000, 'score': 13357.046221017838, 'total_duration': 13906.120040416718, 'accumulated_submission_time': 13357.046221017838, 'accumulated_eval_time': 548.056293964386, 'accumulated_logging_time': 0.8123905658721924, 'global_step': 12747, 'preemption_count': 0}), (13161, {'train/accuracy': 0.5890234112739563, 'train/loss': 1.8429899215698242, 'validation/accuracy': 0.5428799986839294, 'validation/loss': 2.0718536376953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4237000346183777, 'test/loss': 2.7336270809173584, 'test/num_examples': 10000, 'score': 13781.199891805649, 'total_duration': 14348.09713935852, 'accumulated_submission_time': 13781.199891805649, 'accumulated_eval_time': 565.8432378768921, 'accumulated_logging_time': 0.8421120643615723, 'global_step': 13161, 'preemption_count': 0}), (13577, {'train/accuracy': 0.5868359208106995, 'train/loss': 1.8818148374557495, 'validation/accuracy': 0.5476199984550476, 'validation/loss': 2.082552433013916, 'validation/num_examples': 50000, 'test/accuracy': 0.43220001459121704, 'test/loss': 2.7186200618743896, 'test/num_examples': 10000, 'score': 14201.827218055725, 'total_duration': 14786.596528053284, 'accumulated_submission_time': 14201.827218055725, 'accumulated_eval_time': 583.6768879890442, 'accumulated_logging_time': 0.8728237152099609, 'global_step': 13577, 'preemption_count': 0}), (13985, {'train/accuracy': 0.5888085961341858, 'train/loss': 1.8366668224334717, 'validation/accuracy': 0.5507400035858154, 'validation/loss': 2.015629768371582, 'validation/num_examples': 50000, 'test/accuracy': 0.43620002269744873, 'test/loss': 2.6675989627838135, 'test/num_examples': 10000, 'score': 14622.418252706528, 'total_duration': 15224.972486972809, 'accumulated_submission_time': 14622.418252706528, 'accumulated_eval_time': 601.4244141578674, 'accumulated_logging_time': 0.9033582210540771, 'global_step': 13985, 'preemption_count': 0}), (14397, {'train/accuracy': 0.6306445002555847, 'train/loss': 1.6395663022994995, 'validation/accuracy': 0.5621199607849121, 'validation/loss': 1.958893060684204, 'validation/num_examples': 50000, 'test/accuracy': 0.4439000189304352, 'test/loss': 2.6122660636901855, 'test/num_examples': 10000, 'score': 15042.57299733162, 'total_duration': 15662.913816690445, 'accumulated_submission_time': 15042.57299733162, 'accumulated_eval_time': 619.17209815979, 'accumulated_logging_time': 0.9354605674743652, 'global_step': 14397, 'preemption_count': 0}), (14803, {'train/accuracy': 0.615917980670929, 'train/loss': 1.6823394298553467, 'validation/accuracy': 0.5661799907684326, 'validation/loss': 1.9142708778381348, 'validation/num_examples': 50000, 'test/accuracy': 0.44840002059936523, 'test/loss': 2.569552183151245, 'test/num_examples': 10000, 'score': 15462.704107284546, 'total_duration': 16100.86913561821, 'accumulated_submission_time': 15462.704107284546, 'accumulated_eval_time': 636.9651741981506, 'accumulated_logging_time': 0.9598383903503418, 'global_step': 14803, 'preemption_count': 0}), (15216, {'train/accuracy': 0.6160351634025574, 'train/loss': 1.7533750534057617, 'validation/accuracy': 0.5708999633789062, 'validation/loss': 1.956499695777893, 'validation/num_examples': 50000, 'test/accuracy': 0.45010003447532654, 'test/loss': 2.604702949523926, 'test/num_examples': 10000, 'score': 15883.030010938644, 'total_duration': 16538.82902598381, 'accumulated_submission_time': 15883.030010938644, 'accumulated_eval_time': 654.5640182495117, 'accumulated_logging_time': 0.9881417751312256, 'global_step': 15216, 'preemption_count': 0}), (15621, {'train/accuracy': 0.6290624737739563, 'train/loss': 1.6345751285552979, 'validation/accuracy': 0.5811200141906738, 'validation/loss': 1.8429571390151978, 'validation/num_examples': 50000, 'test/accuracy': 0.4609000086784363, 'test/loss': 2.5042402744293213, 'test/num_examples': 10000, 'score': 16303.143368244171, 'total_duration': 16976.641666173935, 'accumulated_submission_time': 16303.143368244171, 'accumulated_eval_time': 672.2311902046204, 'accumulated_logging_time': 1.0134801864624023, 'global_step': 15621, 'preemption_count': 0}), (16034, {'train/accuracy': 0.6342382431030273, 'train/loss': 1.6185213327407837, 'validation/accuracy': 0.5816799998283386, 'validation/loss': 1.8678675889968872, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.5194969177246094, 'test/num_examples': 10000, 'score': 16723.62685751915, 'total_duration': 17414.99485874176, 'accumulated_submission_time': 16723.62685751915, 'accumulated_eval_time': 690.062686920166, 'accumulated_logging_time': 1.0447022914886475, 'global_step': 16034, 'preemption_count': 0}), (16441, {'train/accuracy': 0.6317773461341858, 'train/loss': 1.6280465126037598, 'validation/accuracy': 0.5896799564361572, 'validation/loss': 1.845659852027893, 'validation/num_examples': 50000, 'test/accuracy': 0.4644000232219696, 'test/loss': 2.4914140701293945, 'test/num_examples': 10000, 'score': 17145.812705755234, 'total_duration': 17855.003532648087, 'accumulated_submission_time': 17145.812705755234, 'accumulated_eval_time': 707.8531801700592, 'accumulated_logging_time': 1.0698328018188477, 'global_step': 16441, 'preemption_count': 0}), (16854, {'train/accuracy': 0.6373828053474426, 'train/loss': 1.656376600265503, 'validation/accuracy': 0.5918599963188171, 'validation/loss': 1.863510251045227, 'validation/num_examples': 50000, 'test/accuracy': 0.4700000286102295, 'test/loss': 2.4905102252960205, 'test/num_examples': 10000, 'score': 17565.975972175598, 'total_duration': 18293.03175520897, 'accumulated_submission_time': 17565.975972175598, 'accumulated_eval_time': 725.6802845001221, 'accumulated_logging_time': 1.100714921951294, 'global_step': 16854, 'preemption_count': 0}), (17261, {'train/accuracy': 0.6604687571525574, 'train/loss': 1.5321271419525146, 'validation/accuracy': 0.588979959487915, 'validation/loss': 1.8548794984817505, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.4987239837646484, 'test/num_examples': 10000, 'score': 17988.39752984047, 'total_duration': 18733.235650777817, 'accumulated_submission_time': 17988.39752984047, 'accumulated_eval_time': 743.4284825325012, 'accumulated_logging_time': 1.128204107284546, 'global_step': 17261, 'preemption_count': 0}), (17674, {'train/accuracy': 0.6518163681030273, 'train/loss': 1.5523284673690796, 'validation/accuracy': 0.6021199822425842, 'validation/loss': 1.7871452569961548, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.4381721019744873, 'test/num_examples': 10000, 'score': 18408.898463249207, 'total_duration': 19171.45481991768, 'accumulated_submission_time': 18408.898463249207, 'accumulated_eval_time': 761.108330488205, 'accumulated_logging_time': 1.1597340106964111, 'global_step': 17674, 'preemption_count': 0}), (18081, {'train/accuracy': 0.6507031321525574, 'train/loss': 1.5641251802444458, 'validation/accuracy': 0.6025199890136719, 'validation/loss': 1.7798306941986084, 'validation/num_examples': 50000, 'test/accuracy': 0.4796000123023987, 'test/loss': 2.4260525703430176, 'test/num_examples': 10000, 'score': 18831.19221496582, 'total_duration': 19611.473540067673, 'accumulated_submission_time': 18831.19221496582, 'accumulated_eval_time': 778.7989149093628, 'accumulated_logging_time': 1.1871819496154785, 'global_step': 18081, 'preemption_count': 0}), (18493, {'train/accuracy': 0.6647265553474426, 'train/loss': 1.5321500301361084, 'validation/accuracy': 0.6112799644470215, 'validation/loss': 1.7746847867965698, 'validation/num_examples': 50000, 'test/accuracy': 0.4871000349521637, 'test/loss': 2.4198668003082275, 'test/num_examples': 10000, 'score': 19251.440666913986, 'total_duration': 20049.65583729744, 'accumulated_submission_time': 19251.440666913986, 'accumulated_eval_time': 796.6988821029663, 'accumulated_logging_time': 1.2143619060516357, 'global_step': 18493, 'preemption_count': 0}), (18901, {'train/accuracy': 0.6704491972923279, 'train/loss': 1.4149999618530273, 'validation/accuracy': 0.6163399815559387, 'validation/loss': 1.6806557178497314, 'validation/num_examples': 50000, 'test/accuracy': 0.48680001497268677, 'test/loss': 2.3434183597564697, 'test/num_examples': 10000, 'score': 19675.189138889313, 'total_duration': 20491.2732026577, 'accumulated_submission_time': 19675.189138889313, 'accumulated_eval_time': 814.527245759964, 'accumulated_logging_time': 1.2482469081878662, 'global_step': 18901, 'preemption_count': 0}), (19313, {'train/accuracy': 0.66796875, 'train/loss': 1.4613032341003418, 'validation/accuracy': 0.6147400140762329, 'validation/loss': 1.7056126594543457, 'validation/num_examples': 50000, 'test/accuracy': 0.49000000953674316, 'test/loss': 2.3593437671661377, 'test/num_examples': 10000, 'score': 20095.90381550789, 'total_duration': 20929.80507349968, 'accumulated_submission_time': 20095.90381550789, 'accumulated_eval_time': 832.3061635494232, 'accumulated_logging_time': 1.279527187347412, 'global_step': 19313, 'preemption_count': 0}), (19721, {'train/accuracy': 0.6633984446525574, 'train/loss': 1.4948433637619019, 'validation/accuracy': 0.6168599724769592, 'validation/loss': 1.7143555879592896, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.3389101028442383, 'test/num_examples': 10000, 'score': 20518.231073141098, 'total_duration': 21369.641753196716, 'accumulated_submission_time': 20518.231073141098, 'accumulated_eval_time': 849.7756156921387, 'accumulated_logging_time': 1.312471866607666, 'global_step': 19721, 'preemption_count': 0}), (20133, {'train/accuracy': 0.6953515410423279, 'train/loss': 1.3708524703979492, 'validation/accuracy': 0.622219979763031, 'validation/loss': 1.6914513111114502, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.324892282485962, 'test/num_examples': 10000, 'score': 20938.643839120865, 'total_duration': 21807.941600084305, 'accumulated_submission_time': 20938.643839120865, 'accumulated_eval_time': 867.625776052475, 'accumulated_logging_time': 1.3424482345581055, 'global_step': 20133, 'preemption_count': 0}), (20541, {'train/accuracy': 0.6773632764816284, 'train/loss': 1.4184391498565674, 'validation/accuracy': 0.6247999668121338, 'validation/loss': 1.6651924848556519, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.3144419193267822, 'test/num_examples': 10000, 'score': 21362.729795455933, 'total_duration': 22249.851257801056, 'accumulated_submission_time': 21362.729795455933, 'accumulated_eval_time': 885.4075343608856, 'accumulated_logging_time': 1.3776235580444336, 'global_step': 20541, 'preemption_count': 0}), (20953, {'train/accuracy': 0.6841210722923279, 'train/loss': 1.3917090892791748, 'validation/accuracy': 0.6310799717903137, 'validation/loss': 1.6344183683395386, 'validation/num_examples': 50000, 'test/accuracy': 0.5089000463485718, 'test/loss': 2.2670302391052246, 'test/num_examples': 10000, 'score': 21782.987602710724, 'total_duration': 22687.883763313293, 'accumulated_submission_time': 21782.987602710724, 'accumulated_eval_time': 903.1493408679962, 'accumulated_logging_time': 1.4037363529205322, 'global_step': 20953, 'preemption_count': 0}), (21361, {'train/accuracy': 0.6929882764816284, 'train/loss': 1.399707317352295, 'validation/accuracy': 0.6350399851799011, 'validation/loss': 1.6503700017929077, 'validation/num_examples': 50000, 'test/accuracy': 0.516700029373169, 'test/loss': 2.295945405960083, 'test/num_examples': 10000, 'score': 22207.15165424347, 'total_duration': 23129.66749238968, 'accumulated_submission_time': 22207.15165424347, 'accumulated_eval_time': 920.7359850406647, 'accumulated_logging_time': 1.4301187992095947, 'global_step': 21361, 'preemption_count': 0}), (21774, {'train/accuracy': 0.6981640458106995, 'train/loss': 1.324546217918396, 'validation/accuracy': 0.6363399624824524, 'validation/loss': 1.5979310274124146, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.2487776279449463, 'test/num_examples': 10000, 'score': 22627.827165603638, 'total_duration': 23567.989166259766, 'accumulated_submission_time': 22627.827165603638, 'accumulated_eval_time': 938.3433132171631, 'accumulated_logging_time': 1.4621117115020752, 'global_step': 21774, 'preemption_count': 0}), (22181, {'train/accuracy': 0.6920703053474426, 'train/loss': 1.3648252487182617, 'validation/accuracy': 0.6392799615859985, 'validation/loss': 1.604385256767273, 'validation/num_examples': 50000, 'test/accuracy': 0.5136000514030457, 'test/loss': 2.247358798980713, 'test/num_examples': 10000, 'score': 23051.107225179672, 'total_duration': 24009.136248350143, 'accumulated_submission_time': 23051.107225179672, 'accumulated_eval_time': 956.1715850830078, 'accumulated_logging_time': 1.4941492080688477, 'global_step': 22181, 'preemption_count': 0}), (22593, {'train/accuracy': 0.6877734065055847, 'train/loss': 1.3861896991729736, 'validation/accuracy': 0.6380999684333801, 'validation/loss': 1.6107978820800781, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.25054931640625, 'test/num_examples': 10000, 'score': 23471.77864933014, 'total_duration': 24447.48698449135, 'accumulated_submission_time': 23471.77864933014, 'accumulated_eval_time': 973.8181018829346, 'accumulated_logging_time': 1.5201361179351807, 'global_step': 22593, 'preemption_count': 0}), (23001, {'train/accuracy': 0.7114648222923279, 'train/loss': 1.2889801263809204, 'validation/accuracy': 0.6399399638175964, 'validation/loss': 1.6103993654251099, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.2331113815307617, 'test/num_examples': 10000, 'score': 23895.85806131363, 'total_duration': 24889.32353758812, 'accumulated_submission_time': 23895.85806131363, 'accumulated_eval_time': 991.5424997806549, 'accumulated_logging_time': 1.546048641204834, 'global_step': 23001, 'preemption_count': 0}), (23413, {'train/accuracy': 0.6994726657867432, 'train/loss': 1.3357393741607666, 'validation/accuracy': 0.6406399607658386, 'validation/loss': 1.6038819551467896, 'validation/num_examples': 50000, 'test/accuracy': 0.5163000226020813, 'test/loss': 2.2348439693450928, 'test/num_examples': 10000, 'score': 24316.46390581131, 'total_duration': 25327.694259166718, 'accumulated_submission_time': 24316.46390581131, 'accumulated_eval_time': 1009.2758927345276, 'accumulated_logging_time': 1.570772647857666, 'global_step': 23413, 'preemption_count': 0}), (23821, {'train/accuracy': 0.6929296851158142, 'train/loss': 1.345149040222168, 'validation/accuracy': 0.642039954662323, 'validation/loss': 1.574186086654663, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.2225840091705322, 'test/num_examples': 10000, 'score': 24740.308277845383, 'total_duration': 25769.23399066925, 'accumulated_submission_time': 24740.308277845383, 'accumulated_eval_time': 1026.9316737651825, 'accumulated_logging_time': 1.6033740043640137, 'global_step': 23821, 'preemption_count': 0}), (24232, {'train/accuracy': 0.7093945145606995, 'train/loss': 1.2782460451126099, 'validation/accuracy': 0.6481800079345703, 'validation/loss': 1.5577459335327148, 'validation/num_examples': 50000, 'test/accuracy': 0.5297999978065491, 'test/loss': 2.206908702850342, 'test/num_examples': 10000, 'score': 25160.541655302048, 'total_duration': 26207.28022122383, 'accumulated_submission_time': 25160.541655302048, 'accumulated_eval_time': 1044.703180551529, 'accumulated_logging_time': 1.6380655765533447, 'global_step': 24232, 'preemption_count': 0}), (24641, {'train/accuracy': 0.7135351300239563, 'train/loss': 1.257032036781311, 'validation/accuracy': 0.6475200057029724, 'validation/loss': 1.549261212348938, 'validation/num_examples': 50000, 'test/accuracy': 0.5218000411987305, 'test/loss': 2.1899499893188477, 'test/num_examples': 10000, 'score': 25585.291342020035, 'total_duration': 26649.78462100029, 'accumulated_submission_time': 25585.291342020035, 'accumulated_eval_time': 1062.4201850891113, 'accumulated_logging_time': 1.6686794757843018, 'global_step': 24641, 'preemption_count': 0}), (25052, {'train/accuracy': 0.7065820097923279, 'train/loss': 1.2981622219085693, 'validation/accuracy': 0.6503999829292297, 'validation/loss': 1.5620273351669312, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.201713800430298, 'test/num_examples': 10000, 'score': 26005.78003501892, 'total_duration': 27087.894649267197, 'accumulated_submission_time': 26005.78003501892, 'accumulated_eval_time': 1080.0011739730835, 'accumulated_logging_time': 1.7021591663360596, 'global_step': 25052, 'preemption_count': 0}), (25461, {'train/accuracy': 0.7049804329872131, 'train/loss': 1.3283151388168335, 'validation/accuracy': 0.6520000100135803, 'validation/loss': 1.5657927989959717, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.212352752685547, 'test/num_examples': 10000, 'score': 26431.09242415428, 'total_duration': 27530.89063000679, 'accumulated_submission_time': 26431.09242415428, 'accumulated_eval_time': 1097.6509501934052, 'accumulated_logging_time': 1.7293190956115723, 'global_step': 25461, 'preemption_count': 0}), (25871, {'train/accuracy': 0.729296863079071, 'train/loss': 1.2073310613632202, 'validation/accuracy': 0.655239999294281, 'validation/loss': 1.530431866645813, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.176771879196167, 'test/num_examples': 10000, 'score': 26851.774047851562, 'total_duration': 27969.20543575287, 'accumulated_submission_time': 26851.774047851562, 'accumulated_eval_time': 1115.243684053421, 'accumulated_logging_time': 1.7630674839019775, 'global_step': 25871, 'preemption_count': 0}), (26281, {'train/accuracy': 0.718066394329071, 'train/loss': 1.2039657831192017, 'validation/accuracy': 0.6617199778556824, 'validation/loss': 1.4720354080200195, 'validation/num_examples': 50000, 'test/accuracy': 0.5378000140190125, 'test/loss': 2.123887300491333, 'test/num_examples': 10000, 'score': 27277.636660337448, 'total_duration': 28412.786891937256, 'accumulated_submission_time': 27277.636660337448, 'accumulated_eval_time': 1132.9278500080109, 'accumulated_logging_time': 1.7910280227661133, 'global_step': 26281, 'preemption_count': 0}), (26689, {'train/accuracy': 0.7130468487739563, 'train/loss': 1.2613356113433838, 'validation/accuracy': 0.6563199758529663, 'validation/loss': 1.517512321472168, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.1700754165649414, 'test/num_examples': 10000, 'score': 27697.84457063675, 'total_duration': 28850.641550779343, 'accumulated_submission_time': 27697.84457063675, 'accumulated_eval_time': 1150.5393764972687, 'accumulated_logging_time': 1.8195197582244873, 'global_step': 26689, 'preemption_count': 0}), (27099, {'train/accuracy': 0.7370312213897705, 'train/loss': 1.184836983680725, 'validation/accuracy': 0.6611599922180176, 'validation/loss': 1.5080310106277466, 'validation/num_examples': 50000, 'test/accuracy': 0.5338000059127808, 'test/loss': 2.154926300048828, 'test/num_examples': 10000, 'score': 28118.275394439697, 'total_duration': 29288.719147205353, 'accumulated_submission_time': 28118.275394439697, 'accumulated_eval_time': 1168.1451878547668, 'accumulated_logging_time': 1.8533272743225098, 'global_step': 27099, 'preemption_count': 0}), (27501, {'train/accuracy': 0.7279101610183716, 'train/loss': 1.194553256034851, 'validation/accuracy': 0.660319983959198, 'validation/loss': 1.4935650825500488, 'validation/num_examples': 50000, 'test/accuracy': 0.5389000177383423, 'test/loss': 2.1363677978515625, 'test/num_examples': 10000, 'score': 28538.69926929474, 'total_duration': 29726.830756902695, 'accumulated_submission_time': 28538.69926929474, 'accumulated_eval_time': 1185.7943935394287, 'accumulated_logging_time': 1.8851313591003418, 'global_step': 27501, 'preemption_count': 0}), (27911, {'train/accuracy': 0.7194726467132568, 'train/loss': 1.2184903621673584, 'validation/accuracy': 0.6620599627494812, 'validation/loss': 1.4811300039291382, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.136683225631714, 'test/num_examples': 10000, 'score': 28959.358723640442, 'total_duration': 30165.33520245552, 'accumulated_submission_time': 28959.358723640442, 'accumulated_eval_time': 1203.6002326011658, 'accumulated_logging_time': 1.9175615310668945, 'global_step': 27911, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7194140553474426, 'train/loss': 1.2568016052246094, 'validation/accuracy': 0.662339985370636, 'validation/loss': 1.512110710144043, 'validation/num_examples': 50000, 'test/accuracy': 0.5423000454902649, 'test/loss': 2.1370725631713867, 'test/num_examples': 10000, 'score': 29049.47698688507, 'total_duration': 30273.163052797318, 'accumulated_submission_time': 29049.47698688507, 'accumulated_eval_time': 1221.277441740036, 'accumulated_logging_time': 1.947253704071045, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0502 09:46:37.977542 139815280891712 submission_runner.py:581] Timing: 29049.47698688507
I0502 09:46:37.977592 139815280891712 submission_runner.py:582] ====================
I0502 09:46:37.977773 139815280891712 submission_runner.py:645] Final imagenet_vit score: 29049.47698688507
