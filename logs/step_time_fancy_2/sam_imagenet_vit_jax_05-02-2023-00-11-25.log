python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_sam --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_05-02-2023-00-11-25.log
I0502 00:11:46.267040 139653621614400 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_sam/imagenet_vit_jax.
I0502 00:11:46.334876 139653621614400 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 00:11:47.213433 139653621614400 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0502 00:11:47.214602 139653621614400 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 00:11:47.218845 139653621614400 submission_runner.py:538] Using RNG seed 2134777071
I0502 00:11:49.788907 139653621614400 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 00:11:49.789128 139653621614400 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_sam/imagenet_vit_jax/trial_1.
I0502 00:11:49.789341 139653621614400 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_sam/imagenet_vit_jax/trial_1/hparams.json.
I0502 00:11:49.911439 139653621614400 submission_runner.py:241] Initializing dataset.
I0502 00:11:49.923761 139653621614400 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0502 00:11:49.931075 139653621614400 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0502 00:11:49.931215 139653621614400 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0502 00:11:50.191438 139653621614400 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0502 00:11:56.863089 139653621614400 submission_runner.py:248] Initializing model.
I0502 00:12:08.121875 139653621614400 submission_runner.py:258] Initializing optimizer.
I0502 00:12:08.785119 139653621614400 submission_runner.py:265] Initializing metrics bundle.
I0502 00:12:08.785318 139653621614400 submission_runner.py:282] Initializing checkpoint and logger.
I0502 00:12:08.786220 139653621614400 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_sam/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0502 00:12:09.689344 139653621614400 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_sam/imagenet_vit_jax/trial_1/meta_data_0.json.
I0502 00:12:09.690349 139653621614400 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_sam/imagenet_vit_jax/trial_1/flags_0.json.
I0502 00:12:09.695307 139653621614400 submission_runner.py:318] Starting training loop.
I0502 00:13:14.549225 139475455825664 logging_writer.py:48] [0] global_step=0, grad_norm=0.3305286169052124, loss=6.907756805419922
I0502 00:13:14.568341 139653621614400 spec.py:298] Evaluating on the training split.
I0502 00:13:14.575141 139653621614400 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0502 00:13:14.582079 139653621614400 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0502 00:13:14.582200 139653621614400 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0502 00:13:14.646422 139653621614400 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0502 00:13:34.045295 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 00:13:34.051952 139653621614400 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0502 00:13:34.064145 139653621614400 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0502 00:13:34.064316 139653621614400 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0502 00:13:34.120444 139653621614400 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0502 00:13:53.705263 139653621614400 spec.py:326] Evaluating on the test split.
I0502 00:13:53.711702 139653621614400 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0502 00:13:53.717077 139653621614400 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0502 00:13:53.750267 139653621614400 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0502 00:14:04.664506 139653621614400 submission_runner.py:415] Time since start: 114.97s, 	Step: 1, 	{'train/accuracy': 0.0009374999790452421, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 64.87282133102417, 'total_duration': 114.96912932395935, 'accumulated_submission_time': 64.87282133102417, 'accumulated_eval_time': 50.09613847732544, 'accumulated_logging_time': 0}
I0502 00:14:04.683393 139412994242304 logging_writer.py:48] [1] accumulated_eval_time=50.096138, accumulated_logging_time=0, accumulated_submission_time=64.872821, global_step=1, preemption_count=0, score=64.872821, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=114.969129, train/accuracy=0.000937, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0502 00:15:52.628585 139472494593792 logging_writer.py:48] [100] global_step=100, grad_norm=0.08551342040300369, loss=6.907346248626709
I0502 00:17:10.256764 139472502986496 logging_writer.py:48] [200] global_step=200, grad_norm=0.44120901823043823, loss=6.870083332061768
I0502 00:18:27.874599 139472494593792 logging_writer.py:48] [300] global_step=300, grad_norm=0.4674947261810303, loss=6.805568695068359
I0502 00:19:45.469644 139472502986496 logging_writer.py:48] [400] global_step=400, grad_norm=0.5397949814796448, loss=6.774572849273682
I0502 00:21:03.094169 139472494593792 logging_writer.py:48] [500] global_step=500, grad_norm=0.699401319026947, loss=6.6685991287231445
I0502 00:21:05.135916 139653621614400 spec.py:298] Evaluating on the training split.
I0502 00:21:11.911842 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 00:21:18.396531 139653621614400 spec.py:326] Evaluating on the test split.
I0502 00:21:20.306048 139653621614400 submission_runner.py:415] Time since start: 550.61s, 	Step: 504, 	{'train/accuracy': 0.011660155840218067, 'train/loss': 6.5327653884887695, 'validation/accuracy': 0.011260000057518482, 'validation/loss': 6.544586658477783, 'validation/num_examples': 50000, 'test/accuracy': 0.010100000537931919, 'test/loss': 6.571342468261719, 'test/num_examples': 10000, 'score': 485.30918073654175, 'total_duration': 550.6106333732605, 'accumulated_submission_time': 485.30918073654175, 'accumulated_eval_time': 65.26620078086853, 'accumulated_logging_time': 0.027448177337646484}
I0502 00:21:20.322092 139413162030848 logging_writer.py:48] [504] accumulated_eval_time=65.266201, accumulated_logging_time=0.027448, accumulated_submission_time=485.309181, global_step=504, preemption_count=0, score=485.309181, test/accuracy=0.010100, test/loss=6.571342, test/num_examples=10000, total_duration=550.610633, train/accuracy=0.011660, train/loss=6.532765, validation/accuracy=0.011260, validation/loss=6.544587, validation/num_examples=50000
I0502 00:22:35.820173 139413313009408 logging_writer.py:48] [600] global_step=600, grad_norm=0.837641179561615, loss=6.595310688018799
I0502 00:23:53.465529 139413162030848 logging_writer.py:48] [700] global_step=700, grad_norm=0.560818612575531, loss=6.70686149597168
I0502 00:25:11.133090 139413313009408 logging_writer.py:48] [800] global_step=800, grad_norm=0.8336759209632874, loss=6.541672706604004
I0502 00:26:28.793334 139413162030848 logging_writer.py:48] [900] global_step=900, grad_norm=0.8624686002731323, loss=6.588916778564453
I0502 00:27:46.434463 139413313009408 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8089656829833984, loss=6.451130390167236
I0502 00:28:21.060890 139653621614400 spec.py:298] Evaluating on the training split.
I0502 00:28:27.811230 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 00:28:34.323774 139653621614400 spec.py:326] Evaluating on the test split.
I0502 00:28:36.072190 139653621614400 submission_runner.py:415] Time since start: 986.38s, 	Step: 1046, 	{'train/accuracy': 0.027480468153953552, 'train/loss': 6.1062774658203125, 'validation/accuracy': 0.025539999827742577, 'validation/loss': 6.123623371124268, 'validation/num_examples': 50000, 'test/accuracy': 0.022200001403689384, 'test/loss': 6.208202838897705, 'test/num_examples': 10000, 'score': 906.0267350673676, 'total_duration': 986.3767714500427, 'accumulated_submission_time': 906.0267350673676, 'accumulated_eval_time': 80.27742528915405, 'accumulated_logging_time': 0.05632376670837402}
I0502 00:28:36.086453 139413162030848 logging_writer.py:48] [1046] accumulated_eval_time=80.277425, accumulated_logging_time=0.056324, accumulated_submission_time=906.026735, global_step=1046, preemption_count=0, score=906.026735, test/accuracy=0.022200, test/loss=6.208203, test/num_examples=10000, total_duration=986.376771, train/accuracy=0.027480, train/loss=6.106277, validation/accuracy=0.025540, validation/loss=6.123623, validation/num_examples=50000
I0502 00:29:18.968581 139413313009408 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7375926971435547, loss=6.675769805908203
I0502 00:30:36.688821 139413162030848 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7199302315711975, loss=6.487710952758789
I0502 00:31:54.376777 139413313009408 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.9569238424301147, loss=6.351008415222168
I0502 00:33:12.203487 139413162030848 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8345819115638733, loss=6.531259059906006
I0502 00:34:30.032589 139413313009408 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0705734491348267, loss=6.264810562133789
I0502 00:35:36.531548 139653621614400 spec.py:298] Evaluating on the training split.
I0502 00:35:43.355434 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 00:35:49.794673 139653621614400 spec.py:326] Evaluating on the test split.
I0502 00:35:51.532365 139653621614400 submission_runner.py:415] Time since start: 1421.84s, 	Step: 1587, 	{'train/accuracy': 0.04564452916383743, 'train/loss': 5.783807277679443, 'validation/accuracy': 0.04343999922275543, 'validation/loss': 5.8202223777771, 'validation/num_examples': 50000, 'test/accuracy': 0.03460000082850456, 'test/loss': 5.942713737487793, 'test/num_examples': 10000, 'score': 1326.438285112381, 'total_duration': 1421.836966753006, 'accumulated_submission_time': 1326.438285112381, 'accumulated_eval_time': 95.27819895744324, 'accumulated_logging_time': 0.09591507911682129}
I0502 00:35:51.543619 139413162030848 logging_writer.py:48] [1587] accumulated_eval_time=95.278199, accumulated_logging_time=0.095915, accumulated_submission_time=1326.438285, global_step=1587, preemption_count=0, score=1326.438285, test/accuracy=0.034600, test/loss=5.942714, test/num_examples=10000, total_duration=1421.836967, train/accuracy=0.045645, train/loss=5.783807, validation/accuracy=0.043440, validation/loss=5.820222, validation/num_examples=50000
I0502 00:36:02.545541 139413313009408 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0251331329345703, loss=6.202824592590332
I0502 00:37:20.324738 139413162030848 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.0646543502807617, loss=6.684657096862793
I0502 00:38:38.001747 139413313009408 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8279712796211243, loss=6.197076797485352
I0502 00:39:55.723641 139413162030848 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9798451066017151, loss=6.200510501861572
I0502 00:41:13.465156 139413313009408 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7922029495239258, loss=6.744053840637207
I0502 00:42:31.210201 139413162030848 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8970321416854858, loss=6.316606044769287
I0502 00:42:51.899545 139653621614400 spec.py:298] Evaluating on the training split.
I0502 00:42:58.683188 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 00:43:05.183755 139653621614400 spec.py:326] Evaluating on the test split.
I0502 00:43:06.930338 139653621614400 submission_runner.py:415] Time since start: 1857.23s, 	Step: 2128, 	{'train/accuracy': 0.05667968466877937, 'train/loss': 5.542654991149902, 'validation/accuracy': 0.05494000017642975, 'validation/loss': 5.574263572692871, 'validation/num_examples': 50000, 'test/accuracy': 0.045100003480911255, 'test/loss': 5.740973472595215, 'test/num_examples': 10000, 'score': 1746.7724061012268, 'total_duration': 1857.2349350452423, 'accumulated_submission_time': 1746.7724061012268, 'accumulated_eval_time': 110.30893468856812, 'accumulated_logging_time': 0.1208643913269043}
I0502 00:43:06.940418 139413313009408 logging_writer.py:48] [2128] accumulated_eval_time=110.308935, accumulated_logging_time=0.120864, accumulated_submission_time=1746.772406, global_step=2128, preemption_count=0, score=1746.772406, test/accuracy=0.045100, test/loss=5.740973, test/num_examples=10000, total_duration=1857.234935, train/accuracy=0.056680, train/loss=5.542655, validation/accuracy=0.054940, validation/loss=5.574264, validation/num_examples=50000
I0502 00:44:03.770524 139413162030848 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7497793436050415, loss=6.470210075378418
I0502 00:45:21.554857 139413313009408 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8812960982322693, loss=6.664348125457764
I0502 00:46:39.341687 139413162030848 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9273253083229065, loss=5.996110916137695
I0502 00:47:57.120515 139413313009408 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2301262617111206, loss=6.084290981292725
I0502 00:49:14.937835 139413162030848 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.896740734577179, loss=5.919322967529297
I0502 00:50:07.528819 139653621614400 spec.py:298] Evaluating on the training split.
I0502 00:50:14.318609 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 00:50:20.919375 139653621614400 spec.py:326] Evaluating on the test split.
I0502 00:50:22.670954 139653621614400 submission_runner.py:415] Time since start: 2292.98s, 	Step: 2669, 	{'train/accuracy': 0.07832030951976776, 'train/loss': 5.328441143035889, 'validation/accuracy': 0.07174000144004822, 'validation/loss': 5.37614107131958, 'validation/num_examples': 50000, 'test/accuracy': 0.055900003761053085, 'test/loss': 5.57596492767334, 'test/num_examples': 10000, 'score': 2167.3390300273895, 'total_duration': 2292.9755618572235, 'accumulated_submission_time': 2167.3390300273895, 'accumulated_eval_time': 125.45103335380554, 'accumulated_logging_time': 0.14440584182739258}
I0502 00:50:22.681591 139413313009408 logging_writer.py:48] [2669] accumulated_eval_time=125.451033, accumulated_logging_time=0.144406, accumulated_submission_time=2167.339030, global_step=2669, preemption_count=0, score=2167.339030, test/accuracy=0.055900, test/loss=5.575965, test/num_examples=10000, total_duration=2292.975562, train/accuracy=0.078320, train/loss=5.328441, validation/accuracy=0.071740, validation/loss=5.376141, validation/num_examples=50000
I0502 00:50:47.678954 139413162030848 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8612386584281921, loss=6.041934490203857
I0502 00:52:05.486019 139413313009408 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8644761443138123, loss=6.080977439880371
I0502 00:53:23.206852 139413162030848 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8833288550376892, loss=5.831037998199463
I0502 00:54:41.013376 139413313009408 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0379050970077515, loss=5.814491271972656
I0502 00:55:58.832910 139413162030848 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9381886720657349, loss=6.532912254333496
I0502 00:57:16.636631 139413313009408 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0597667694091797, loss=5.8180341720581055
I0502 00:57:23.372724 139653621614400 spec.py:298] Evaluating on the training split.
I0502 00:57:30.162527 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 00:57:36.615850 139653621614400 spec.py:326] Evaluating on the test split.
I0502 00:57:38.360712 139653621614400 submission_runner.py:415] Time since start: 2728.67s, 	Step: 3210, 	{'train/accuracy': 0.10332030802965164, 'train/loss': 5.04941987991333, 'validation/accuracy': 0.09637999534606934, 'validation/loss': 5.102444171905518, 'validation/num_examples': 50000, 'test/accuracy': 0.07440000027418137, 'test/loss': 5.3394083976745605, 'test/num_examples': 10000, 'score': 2588.0098960399628, 'total_duration': 2728.6653208732605, 'accumulated_submission_time': 2588.0098960399628, 'accumulated_eval_time': 140.43897581100464, 'accumulated_logging_time': 0.16705918312072754}
I0502 00:57:38.371240 139413162030848 logging_writer.py:48] [3210] accumulated_eval_time=140.438976, accumulated_logging_time=0.167059, accumulated_submission_time=2588.009896, global_step=3210, preemption_count=0, score=2588.009896, test/accuracy=0.074400, test/loss=5.339408, test/num_examples=10000, total_duration=2728.665321, train/accuracy=0.103320, train/loss=5.049420, validation/accuracy=0.096380, validation/loss=5.102444, validation/num_examples=50000
I0502 00:58:49.282907 139413313009408 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.027462363243103, loss=5.719717979431152
I0502 01:00:07.115609 139413162030848 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.061991572380066, loss=5.7624406814575195
I0502 01:01:24.894287 139413313009408 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8124607801437378, loss=6.2066874504089355
I0502 01:02:42.722832 139413162030848 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9212446808815002, loss=5.671993255615234
I0502 01:04:00.567247 139413313009408 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.018884539604187, loss=6.167836666107178
I0502 01:04:38.393533 139653621614400 spec.py:298] Evaluating on the training split.
I0502 01:04:45.169810 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 01:04:51.660734 139653621614400 spec.py:326] Evaluating on the test split.
I0502 01:04:53.400466 139653621614400 submission_runner.py:415] Time since start: 3163.71s, 	Step: 3750, 	{'train/accuracy': 0.1263476461172104, 'train/loss': 4.845317840576172, 'validation/accuracy': 0.11713999509811401, 'validation/loss': 4.910996437072754, 'validation/num_examples': 50000, 'test/accuracy': 0.08580000698566437, 'test/loss': 5.191555500030518, 'test/num_examples': 10000, 'score': 3008.0100514888763, 'total_duration': 3163.705046892166, 'accumulated_submission_time': 3008.0100514888763, 'accumulated_eval_time': 155.44584369659424, 'accumulated_logging_time': 0.19133615493774414}
I0502 01:04:53.415577 139413162030848 logging_writer.py:48] [3750] accumulated_eval_time=155.445844, accumulated_logging_time=0.191336, accumulated_submission_time=3008.010051, global_step=3750, preemption_count=0, score=3008.010051, test/accuracy=0.085800, test/loss=5.191556, test/num_examples=10000, total_duration=3163.705047, train/accuracy=0.126348, train/loss=4.845318, validation/accuracy=0.117140, validation/loss=4.910996, validation/num_examples=50000
I0502 01:05:33.182488 139413313009408 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8259239196777344, loss=6.576797008514404
I0502 01:06:50.987291 139413162030848 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9537835121154785, loss=5.503060817718506
I0502 01:08:08.779083 139413313009408 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8693126440048218, loss=5.519550800323486
I0502 01:09:26.631972 139413162030848 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8596093654632568, loss=5.838214874267578
I0502 01:10:44.459017 139413313009408 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.0926529169082642, loss=5.594581604003906
I0502 01:11:53.432415 139653621614400 spec.py:298] Evaluating on the training split.
I0502 01:12:00.220754 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 01:12:06.678331 139653621614400 spec.py:326] Evaluating on the test split.
I0502 01:12:08.423478 139653621614400 submission_runner.py:415] Time since start: 3598.73s, 	Step: 4290, 	{'train/accuracy': 0.15830077230930328, 'train/loss': 4.5480499267578125, 'validation/accuracy': 0.14611999690532684, 'validation/loss': 4.636673450469971, 'validation/num_examples': 50000, 'test/accuracy': 0.1120000034570694, 'test/loss': 4.947347164154053, 'test/num_examples': 10000, 'score': 3428.006046295166, 'total_duration': 3598.7280633449554, 'accumulated_submission_time': 3428.006046295166, 'accumulated_eval_time': 170.43685603141785, 'accumulated_logging_time': 0.21928739547729492}
I0502 01:12:08.439650 139413162030848 logging_writer.py:48] [4290] accumulated_eval_time=170.436856, accumulated_logging_time=0.219287, accumulated_submission_time=3428.006046, global_step=4290, preemption_count=0, score=3428.006046, test/accuracy=0.112000, test/loss=4.947347, test/num_examples=10000, total_duration=3598.728063, train/accuracy=0.158301, train/loss=4.548050, validation/accuracy=0.146120, validation/loss=4.636673, validation/num_examples=50000
I0502 01:12:17.108175 139413313009408 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.1007179021835327, loss=5.402416706085205
I0502 01:13:34.921666 139413162030848 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.9364669919013977, loss=5.409395694732666
I0502 01:14:52.749978 139413313009408 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.008906364440918, loss=5.357527256011963
I0502 01:16:10.568241 139413162030848 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.0069477558135986, loss=5.44582462310791
I0502 01:17:28.377016 139413313009408 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.0393221378326416, loss=5.388348579406738
I0502 01:18:46.249758 139413162030848 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7270910739898682, loss=6.35060453414917
I0502 01:19:08.527899 139653621614400 spec.py:298] Evaluating on the training split.
I0502 01:19:15.316395 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 01:19:21.966946 139653621614400 spec.py:326] Evaluating on the test split.
I0502 01:19:23.703942 139653621614400 submission_runner.py:415] Time since start: 4034.01s, 	Step: 4830, 	{'train/accuracy': 0.18183593451976776, 'train/loss': 4.369006156921387, 'validation/accuracy': 0.16885998845100403, 'validation/loss': 4.4567108154296875, 'validation/num_examples': 50000, 'test/accuracy': 0.1282000094652176, 'test/loss': 4.785923004150391, 'test/num_examples': 10000, 'score': 3848.07204079628, 'total_duration': 4034.0085546970367, 'accumulated_submission_time': 3848.07204079628, 'accumulated_eval_time': 185.6128535270691, 'accumulated_logging_time': 0.24971699714660645}
I0502 01:19:23.716433 139413313009408 logging_writer.py:48] [4830] accumulated_eval_time=185.612854, accumulated_logging_time=0.249717, accumulated_submission_time=3848.072041, global_step=4830, preemption_count=0, score=3848.072041, test/accuracy=0.128200, test/loss=4.785923, test/num_examples=10000, total_duration=4034.008555, train/accuracy=0.181836, train/loss=4.369006, validation/accuracy=0.168860, validation/loss=4.456711, validation/num_examples=50000
I0502 01:20:19.052486 139413162030848 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.1298803091049194, loss=5.340322971343994
I0502 01:21:36.913731 139413313009408 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8799542188644409, loss=6.282027244567871
I0502 01:22:54.819964 139413162030848 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.9385135173797607, loss=5.195303916931152
I0502 01:24:12.668585 139413313009408 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.2032253742218018, loss=5.222592353820801
I0502 01:25:30.552247 139413162030848 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9976384043693542, loss=5.234945774078369
I0502 01:26:23.981604 139653621614400 spec.py:298] Evaluating on the training split.
I0502 01:26:30.772797 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 01:26:37.382846 139653621614400 spec.py:326] Evaluating on the test split.
I0502 01:26:39.130028 139653621614400 submission_runner.py:415] Time since start: 4469.43s, 	Step: 5370, 	{'train/accuracy': 0.21193358302116394, 'train/loss': 4.104508399963379, 'validation/accuracy': 0.1959799975156784, 'validation/loss': 4.183114051818848, 'validation/num_examples': 50000, 'test/accuracy': 0.14750000834465027, 'test/loss': 4.566035270690918, 'test/num_examples': 10000, 'score': 4268.315876483917, 'total_duration': 4469.434617996216, 'accumulated_submission_time': 4268.315876483917, 'accumulated_eval_time': 200.76121759414673, 'accumulated_logging_time': 0.27559852600097656}
I0502 01:26:39.141668 139413313009408 logging_writer.py:48] [5370] accumulated_eval_time=200.761218, accumulated_logging_time=0.275599, accumulated_submission_time=4268.315876, global_step=5370, preemption_count=0, score=4268.315876, test/accuracy=0.147500, test/loss=4.566035, test/num_examples=10000, total_duration=4469.434618, train/accuracy=0.211934, train/loss=4.104508, validation/accuracy=0.195980, validation/loss=4.183114, validation/num_examples=50000
I0502 01:27:03.378142 139413162030848 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.0313419103622437, loss=6.395435810089111
I0502 01:28:21.231106 139413313009408 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7865210175514221, loss=6.498126029968262
I0502 01:29:39.107484 139413162030848 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9549009203910828, loss=5.0954484939575195
I0502 01:30:56.956348 139413313009408 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.9198868870735168, loss=5.138808250427246
I0502 01:32:14.832761 139413162030848 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7323630452156067, loss=5.92808723449707
I0502 01:33:32.668385 139413313009408 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6415819525718689, loss=6.161728382110596
I0502 01:33:39.390368 139653621614400 spec.py:298] Evaluating on the training split.
I0502 01:33:46.192153 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 01:33:52.823235 139653621614400 spec.py:326] Evaluating on the test split.
I0502 01:33:54.568017 139653621614400 submission_runner.py:415] Time since start: 4904.87s, 	Step: 5910, 	{'train/accuracy': 0.2245507836341858, 'train/loss': 4.051093101501465, 'validation/accuracy': 0.2065799981355667, 'validation/loss': 4.145547866821289, 'validation/num_examples': 50000, 'test/accuracy': 0.16140000522136688, 'test/loss': 4.552667617797852, 'test/num_examples': 10000, 'score': 4688.544184207916, 'total_duration': 4904.872576951981, 'accumulated_submission_time': 4688.544184207916, 'accumulated_eval_time': 215.93876934051514, 'accumulated_logging_time': 0.29979920387268066}
I0502 01:33:54.583777 139413162030848 logging_writer.py:48] [5910] accumulated_eval_time=215.938769, accumulated_logging_time=0.299799, accumulated_submission_time=4688.544184, global_step=5910, preemption_count=0, score=4688.544184, test/accuracy=0.161400, test/loss=4.552668, test/num_examples=10000, total_duration=4904.872577, train/accuracy=0.224551, train/loss=4.051093, validation/accuracy=0.206580, validation/loss=4.145548, validation/num_examples=50000
I0502 01:35:05.547254 139413313009408 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9634692072868347, loss=5.320242881774902
I0502 01:36:23.413842 139413162030848 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.928441047668457, loss=4.946505546569824
I0502 01:37:41.260085 139413313009408 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7125758528709412, loss=6.382269859313965
I0502 01:38:59.092680 139413162030848 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7431474924087524, loss=5.88502836227417
I0502 01:40:16.946933 139413313009408 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.9287846088409424, loss=4.884108543395996
I0502 01:40:54.797101 139653621614400 spec.py:298] Evaluating on the training split.
I0502 01:41:01.587686 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 01:41:08.216445 139653621614400 spec.py:326] Evaluating on the test split.
I0502 01:41:09.971181 139653621614400 submission_runner.py:415] Time since start: 5340.28s, 	Step: 6450, 	{'train/accuracy': 0.26005858182907104, 'train/loss': 3.7926025390625, 'validation/accuracy': 0.24053999781608582, 'validation/loss': 3.9104056358337402, 'validation/num_examples': 50000, 'test/accuracy': 0.18390001356601715, 'test/loss': 4.3344316482543945, 'test/num_examples': 10000, 'score': 5108.739726305008, 'total_duration': 5340.275751113892, 'accumulated_submission_time': 5108.739726305008, 'accumulated_eval_time': 231.11277103424072, 'accumulated_logging_time': 0.32534122467041016}
I0502 01:41:09.986424 139413162030848 logging_writer.py:48] [6450] accumulated_eval_time=231.112771, accumulated_logging_time=0.325341, accumulated_submission_time=5108.739726, global_step=6450, preemption_count=0, score=5108.739726, test/accuracy=0.183900, test/loss=4.334432, test/num_examples=10000, total_duration=5340.275751, train/accuracy=0.260059, train/loss=3.792603, validation/accuracy=0.240540, validation/loss=3.910406, validation/num_examples=50000
I0502 01:41:49.794584 139413313009408 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.9730007648468018, loss=4.937283515930176
I0502 01:43:07.614927 139413162030848 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7959468364715576, loss=5.024372100830078
I0502 01:44:25.463456 139413313009408 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8142940998077393, loss=4.950514316558838
I0502 01:45:43.311579 139413162030848 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9446049332618713, loss=4.848287105560303
I0502 01:47:01.174557 139413313009408 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9118505716323853, loss=4.806370735168457
I0502 01:48:10.088542 139653621614400 spec.py:298] Evaluating on the training split.
I0502 01:48:16.885019 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 01:48:23.541204 139653621614400 spec.py:326] Evaluating on the test split.
I0502 01:48:25.286034 139653621614400 submission_runner.py:415] Time since start: 5775.59s, 	Step: 6990, 	{'train/accuracy': 0.2891796827316284, 'train/loss': 3.6054911613464355, 'validation/accuracy': 0.25165998935699463, 'validation/loss': 3.809664487838745, 'validation/num_examples': 50000, 'test/accuracy': 0.1972000151872635, 'test/loss': 4.233774185180664, 'test/num_examples': 10000, 'score': 5528.8206968307495, 'total_duration': 5775.590601205826, 'accumulated_submission_time': 5528.8206968307495, 'accumulated_eval_time': 246.31018090248108, 'accumulated_logging_time': 0.35364270210266113}
I0502 01:48:25.301720 139413162030848 logging_writer.py:48] [6990] accumulated_eval_time=246.310181, accumulated_logging_time=0.353643, accumulated_submission_time=5528.820697, global_step=6990, preemption_count=0, score=5528.820697, test/accuracy=0.197200, test/loss=4.233774, test/num_examples=10000, total_duration=5775.590601, train/accuracy=0.289180, train/loss=3.605491, validation/accuracy=0.251660, validation/loss=3.809664, validation/num_examples=50000
I0502 01:48:33.985260 139413313009408 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6406728625297546, loss=5.9447245597839355
I0502 01:49:51.865117 139413162030848 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.8707071542739868, loss=4.905494689941406
I0502 01:51:09.683929 139413313009408 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8101069927215576, loss=4.869235992431641
I0502 01:52:27.535116 139413162030848 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6570941209793091, loss=5.8797078132629395
I0502 01:53:45.361371 139413313009408 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.9231098890304565, loss=4.834907531738281
I0502 01:55:03.177695 139413162030848 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.5034106969833374, loss=4.852417945861816
I0502 01:55:25.452184 139653621614400 spec.py:298] Evaluating on the training split.
I0502 01:55:32.250610 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 01:55:38.908502 139653621614400 spec.py:326] Evaluating on the test split.
I0502 01:55:40.653855 139653621614400 submission_runner.py:415] Time since start: 6210.96s, 	Step: 7530, 	{'train/accuracy': 0.30537107586860657, 'train/loss': 3.45223069190979, 'validation/accuracy': 0.27715998888015747, 'validation/loss': 3.597196578979492, 'validation/num_examples': 50000, 'test/accuracy': 0.21390001475811005, 'test/loss': 4.073435306549072, 'test/num_examples': 10000, 'score': 5948.952146053314, 'total_duration': 6210.95846414566, 'accumulated_submission_time': 5948.952146053314, 'accumulated_eval_time': 261.5118029117584, 'accumulated_logging_time': 0.38050031661987305}
I0502 01:55:40.665338 139413313009408 logging_writer.py:48] [7530] accumulated_eval_time=261.511803, accumulated_logging_time=0.380500, accumulated_submission_time=5948.952146, global_step=7530, preemption_count=0, score=5948.952146, test/accuracy=0.213900, test/loss=4.073435, test/num_examples=10000, total_duration=6210.958464, train/accuracy=0.305371, train/loss=3.452231, validation/accuracy=0.277160, validation/loss=3.597197, validation/num_examples=50000
I0502 01:56:35.940607 139413162030848 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7951479554176331, loss=4.710866928100586
I0502 01:57:53.789143 139413313009408 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.867981493473053, loss=4.915706634521484
I0502 01:59:11.660191 139413162030848 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.854326605796814, loss=4.786742687225342
I0502 02:00:29.496867 139413313009408 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8591720461845398, loss=4.560177803039551
I0502 02:01:47.342354 139413162030848 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8298007249832153, loss=4.681521415710449
I0502 02:02:40.723848 139653621614400 spec.py:298] Evaluating on the training split.
I0502 02:02:47.533617 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 02:02:54.255134 139653621614400 spec.py:326] Evaluating on the test split.
I0502 02:02:56.006777 139653621614400 submission_runner.py:415] Time since start: 6646.31s, 	Step: 8070, 	{'train/accuracy': 0.3279492259025574, 'train/loss': 3.2899789810180664, 'validation/accuracy': 0.3025200068950653, 'validation/loss': 3.419285774230957, 'validation/num_examples': 50000, 'test/accuracy': 0.2321000099182129, 'test/loss': 3.912594795227051, 'test/num_examples': 10000, 'score': 6368.98800611496, 'total_duration': 6646.3113803863525, 'accumulated_submission_time': 6368.98800611496, 'accumulated_eval_time': 276.79468607902527, 'accumulated_logging_time': 0.40654850006103516}
I0502 02:02:56.017642 139413313009408 logging_writer.py:48] [8070] accumulated_eval_time=276.794686, accumulated_logging_time=0.406549, accumulated_submission_time=6368.988006, global_step=8070, preemption_count=0, score=6368.988006, test/accuracy=0.232100, test/loss=3.912595, test/num_examples=10000, total_duration=6646.311380, train/accuracy=0.327949, train/loss=3.289979, validation/accuracy=0.302520, validation/loss=3.419286, validation/num_examples=50000
I0502 02:03:20.244533 139413162030848 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.715149998664856, loss=4.989997863769531
I0502 02:04:38.131094 139413313009408 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.8849000930786133, loss=4.644027233123779
I0502 02:05:55.961041 139413162030848 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6037359237670898, loss=5.8876953125
I0502 02:07:13.788734 139413313009408 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6724019646644592, loss=5.682131767272949
I0502 02:08:31.663347 139413162030848 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.0653735399246216, loss=4.558475971221924
I0502 02:09:49.530674 139413313009408 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8305873870849609, loss=4.528692722320557
I0502 02:09:56.273958 139653621614400 spec.py:298] Evaluating on the training split.
I0502 02:10:03.095602 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 02:10:09.805634 139653621614400 spec.py:326] Evaluating on the test split.
I0502 02:10:11.546859 139653621614400 submission_runner.py:415] Time since start: 7081.85s, 	Step: 8610, 	{'train/accuracy': 0.34619140625, 'train/loss': 3.1740331649780273, 'validation/accuracy': 0.3144199848175049, 'validation/loss': 3.350982189178467, 'validation/num_examples': 50000, 'test/accuracy': 0.24070000648498535, 'test/loss': 3.8787009716033936, 'test/num_examples': 10000, 'score': 6789.22288942337, 'total_duration': 7081.8514585494995, 'accumulated_submission_time': 6789.22288942337, 'accumulated_eval_time': 292.06752943992615, 'accumulated_logging_time': 0.4307565689086914}
I0502 02:10:11.559066 139413162030848 logging_writer.py:48] [8610] accumulated_eval_time=292.067529, accumulated_logging_time=0.430757, accumulated_submission_time=6789.222889, global_step=8610, preemption_count=0, score=6789.222889, test/accuracy=0.240700, test/loss=3.878701, test/num_examples=10000, total_duration=7081.851459, train/accuracy=0.346191, train/loss=3.174033, validation/accuracy=0.314420, validation/loss=3.350982, validation/num_examples=50000
I0502 02:11:22.478988 139413313009408 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6370913982391357, loss=5.694935321807861
I0502 02:12:40.303100 139413162030848 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5763388872146606, loss=6.111059665679932
I0502 02:13:58.188978 139413313009408 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.787924587726593, loss=4.516522407531738
I0502 02:15:15.988992 139413162030848 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5932343602180481, loss=6.085263252258301
I0502 02:16:33.834407 139413313009408 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7702380418777466, loss=4.756016731262207
I0502 02:17:11.661717 139653621614400 spec.py:298] Evaluating on the training split.
I0502 02:17:18.486482 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 02:17:25.289108 139653621614400 spec.py:326] Evaluating on the test split.
I0502 02:17:27.039517 139653621614400 submission_runner.py:415] Time since start: 7517.34s, 	Step: 9150, 	{'train/accuracy': 0.34925779700279236, 'train/loss': 3.171983480453491, 'validation/accuracy': 0.3237600028514862, 'validation/loss': 3.312438726425171, 'validation/num_examples': 50000, 'test/accuracy': 0.25370001792907715, 'test/loss': 3.825394630432129, 'test/num_examples': 10000, 'score': 7209.303078889847, 'total_duration': 7517.344086885452, 'accumulated_submission_time': 7209.303078889847, 'accumulated_eval_time': 307.44526982307434, 'accumulated_logging_time': 0.45702695846557617}
I0502 02:17:27.054257 139413162030848 logging_writer.py:48] [9150] accumulated_eval_time=307.445270, accumulated_logging_time=0.457027, accumulated_submission_time=7209.303079, global_step=9150, preemption_count=0, score=7209.303079, test/accuracy=0.253700, test/loss=3.825395, test/num_examples=10000, total_duration=7517.344087, train/accuracy=0.349258, train/loss=3.171983, validation/accuracy=0.323760, validation/loss=3.312439, validation/num_examples=50000
I0502 02:18:06.819053 139413313009408 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8309321403503418, loss=4.629321098327637
I0502 02:19:24.635397 139413162030848 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6942456364631653, loss=5.073070049285889
I0502 02:20:42.458622 139413313009408 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7286489605903625, loss=4.67377233505249
I0502 02:22:00.325184 139413162030848 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8346673250198364, loss=4.542148590087891
I0502 02:23:18.179196 139413313009408 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.9275507926940918, loss=4.843402862548828
I0502 02:24:27.159479 139653621614400 spec.py:298] Evaluating on the training split.
I0502 02:24:34.070579 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 02:24:41.877868 139653621614400 spec.py:326] Evaluating on the test split.
I0502 02:24:43.611344 139653621614400 submission_runner.py:415] Time since start: 7953.92s, 	Step: 9690, 	{'train/accuracy': 0.38755857944488525, 'train/loss': 2.901573419570923, 'validation/accuracy': 0.34887999296188354, 'validation/loss': 3.1177330017089844, 'validation/num_examples': 50000, 'test/accuracy': 0.262800008058548, 'test/loss': 3.6641533374786377, 'test/num_examples': 10000, 'score': 7629.388253211975, 'total_duration': 7953.9159553050995, 'accumulated_submission_time': 7629.388253211975, 'accumulated_eval_time': 323.89712047576904, 'accumulated_logging_time': 0.48378610610961914}
I0502 02:24:43.623377 139413162030848 logging_writer.py:48] [9690] accumulated_eval_time=323.897120, accumulated_logging_time=0.483786, accumulated_submission_time=7629.388253, global_step=9690, preemption_count=0, score=7629.388253, test/accuracy=0.262800, test/loss=3.664153, test/num_examples=10000, total_duration=7953.915955, train/accuracy=0.387559, train/loss=2.901573, validation/accuracy=0.348880, validation/loss=3.117733, validation/num_examples=50000
I0502 02:24:52.227543 139413313009408 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.642681896686554, loss=5.049079895019531
I0502 02:26:10.105219 139413162030848 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.48856934905052185, loss=5.996888637542725
I0502 02:27:27.936430 139413313009408 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.864770770072937, loss=4.40500545501709
I0502 02:28:45.776687 139413162030848 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8338721394538879, loss=4.741176605224609
I0502 02:30:03.599905 139413313009408 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.42277243733406067, loss=5.875286102294922
I0502 02:31:21.461007 139413162030848 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5534414649009705, loss=6.07672119140625
I0502 02:31:43.751749 139653621614400 spec.py:298] Evaluating on the training split.
I0502 02:31:50.875479 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 02:31:59.726106 139653621614400 spec.py:326] Evaluating on the test split.
I0502 02:32:01.468079 139653621614400 submission_runner.py:415] Time since start: 8391.77s, 	Step: 10230, 	{'train/accuracy': 0.38972654938697815, 'train/loss': 2.951045513153076, 'validation/accuracy': 0.3606799840927124, 'validation/loss': 3.100334882736206, 'validation/num_examples': 50000, 'test/accuracy': 0.2802000045776367, 'test/loss': 3.627120018005371, 'test/num_examples': 10000, 'score': 8049.498726129532, 'total_duration': 8391.772688388824, 'accumulated_submission_time': 8049.498726129532, 'accumulated_eval_time': 341.6133997440338, 'accumulated_logging_time': 0.5057449340820312}
I0502 02:32:01.480100 139413313009408 logging_writer.py:48] [10230] accumulated_eval_time=341.613400, accumulated_logging_time=0.505745, accumulated_submission_time=8049.498726, global_step=10230, preemption_count=0, score=8049.498726, test/accuracy=0.280200, test/loss=3.627120, test/num_examples=10000, total_duration=8391.772688, train/accuracy=0.389727, train/loss=2.951046, validation/accuracy=0.360680, validation/loss=3.100335, validation/num_examples=50000
I0502 02:32:56.741548 139413162030848 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.752554178237915, loss=4.415177822113037
I0502 02:34:14.638183 139413313009408 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7751511931419373, loss=4.338714599609375
I0502 02:35:32.576048 139413162030848 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5550938844680786, loss=5.584689140319824
I0502 02:36:50.472639 139413313009408 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5133605003356934, loss=5.782480716705322
I0502 02:38:08.308219 139413162030848 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.8633483648300171, loss=4.349845886230469
I0502 02:39:01.738378 139653621614400 spec.py:298] Evaluating on the training split.
I0502 02:39:09.122140 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 02:39:18.444041 139653621614400 spec.py:326] Evaluating on the test split.
I0502 02:39:20.175950 139653621614400 submission_runner.py:415] Time since start: 8830.48s, 	Step: 10770, 	{'train/accuracy': 0.3985937535762787, 'train/loss': 2.8954787254333496, 'validation/accuracy': 0.3694799840450287, 'validation/loss': 3.0494508743286133, 'validation/num_examples': 50000, 'test/accuracy': 0.2851000130176544, 'test/loss': 3.5948896408081055, 'test/num_examples': 10000, 'score': 8469.736020088196, 'total_duration': 8830.48053431511, 'accumulated_submission_time': 8469.736020088196, 'accumulated_eval_time': 360.0509202480316, 'accumulated_logging_time': 0.5309243202209473}
I0502 02:39:20.193219 139413313009408 logging_writer.py:48] [10770] accumulated_eval_time=360.050920, accumulated_logging_time=0.530924, accumulated_submission_time=8469.736020, global_step=10770, preemption_count=0, score=8469.736020, test/accuracy=0.285100, test/loss=3.594890, test/num_examples=10000, total_duration=8830.480534, train/accuracy=0.398594, train/loss=2.895479, validation/accuracy=0.369480, validation/loss=3.049451, validation/num_examples=50000
I0502 02:39:44.330775 139413162030848 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.8986846208572388, loss=4.26576042175293
I0502 02:41:02.134739 139413313009408 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7776450514793396, loss=4.212967872619629
I0502 02:42:19.968430 139413162030848 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8081851601600647, loss=4.113213539123535
I0502 02:43:37.791822 139413313009408 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5053207278251648, loss=6.031558036804199
I0502 02:44:55.580078 139413162030848 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7811546325683594, loss=4.149083614349365
I0502 02:46:13.387577 139413313009408 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9793396592140198, loss=4.246953964233398
I0502 02:46:20.885328 139653621614400 spec.py:298] Evaluating on the training split.
I0502 02:46:28.388412 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 02:46:37.773254 139653621614400 spec.py:326] Evaluating on the test split.
I0502 02:46:39.520182 139653621614400 submission_runner.py:415] Time since start: 9269.82s, 	Step: 11311, 	{'train/accuracy': 0.4358789026737213, 'train/loss': 2.6131887435913086, 'validation/accuracy': 0.398639976978302, 'validation/loss': 2.821199893951416, 'validation/num_examples': 50000, 'test/accuracy': 0.3027999997138977, 'test/loss': 3.40556263923645, 'test/num_examples': 10000, 'score': 8890.406812906265, 'total_duration': 9269.823364019394, 'accumulated_submission_time': 8890.406812906265, 'accumulated_eval_time': 378.6842978000641, 'accumulated_logging_time': 0.5615575313568115}
I0502 02:46:39.537467 139413162030848 logging_writer.py:48] [11311] accumulated_eval_time=378.684298, accumulated_logging_time=0.561558, accumulated_submission_time=8890.406813, global_step=11311, preemption_count=0, score=8890.406813, test/accuracy=0.302800, test/loss=3.405563, test/num_examples=10000, total_duration=9269.823364, train/accuracy=0.435879, train/loss=2.613189, validation/accuracy=0.398640, validation/loss=2.821200, validation/num_examples=50000
I0502 02:47:49.601355 139413313009408 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7315613627433777, loss=4.689698696136475
I0502 02:49:07.440130 139413162030848 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.533601701259613, loss=5.569758415222168
I0502 02:50:25.266415 139413313009408 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.7785879373550415, loss=4.241835594177246
I0502 02:51:43.081975 139413162030848 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7585048079490662, loss=4.3585686683654785
I0502 02:53:00.942351 139413313009408 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.7911739349365234, loss=4.038669109344482
I0502 02:53:39.584750 139653621614400 spec.py:298] Evaluating on the training split.
I0502 02:53:47.059578 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 02:53:56.830347 139653621614400 spec.py:326] Evaluating on the test split.
I0502 02:53:58.563796 139653621614400 submission_runner.py:415] Time since start: 9708.87s, 	Step: 11851, 	{'train/accuracy': 0.45068359375, 'train/loss': 2.561367988586426, 'validation/accuracy': 0.41395998001098633, 'validation/loss': 2.7378463745117188, 'validation/num_examples': 50000, 'test/accuracy': 0.32510000467300415, 'test/loss': 3.3062238693237305, 'test/num_examples': 10000, 'score': 9310.431515932083, 'total_duration': 9708.868353128433, 'accumulated_submission_time': 9310.431515932083, 'accumulated_eval_time': 397.6632468700409, 'accumulated_logging_time': 0.5934393405914307}
I0502 02:53:58.579471 139413162030848 logging_writer.py:48] [11851] accumulated_eval_time=397.663247, accumulated_logging_time=0.593439, accumulated_submission_time=9310.431516, global_step=11851, preemption_count=0, score=9310.431516, test/accuracy=0.325100, test/loss=3.306224, test/num_examples=10000, total_duration=9708.868353, train/accuracy=0.450684, train/loss=2.561368, validation/accuracy=0.413960, validation/loss=2.737846, validation/num_examples=50000
I0502 02:54:37.543098 139413313009408 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.699554979801178, loss=4.583843231201172
I0502 02:55:55.390237 139413162030848 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6730430722236633, loss=4.278128147125244
I0502 02:57:13.277508 139413313009408 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.7069823741912842, loss=4.216843128204346
I0502 02:58:31.171997 139413162030848 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.7812106609344482, loss=4.073715686798096
I0502 02:59:48.991685 139413313009408 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5884915590286255, loss=5.781854152679443
I0502 03:00:58.721096 139653621614400 spec.py:298] Evaluating on the training split.
I0502 03:01:06.423266 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 03:01:15.571345 139653621614400 spec.py:326] Evaluating on the test split.
I0502 03:01:17.314048 139653621614400 submission_runner.py:415] Time since start: 10147.62s, 	Step: 12391, 	{'train/accuracy': 0.4526757597923279, 'train/loss': 2.592681884765625, 'validation/accuracy': 0.41609999537467957, 'validation/loss': 2.766192674636841, 'validation/num_examples': 50000, 'test/accuracy': 0.3246000111103058, 'test/loss': 3.3422627449035645, 'test/num_examples': 10000, 'score': 9730.551590442657, 'total_duration': 10147.618627548218, 'accumulated_submission_time': 9730.551590442657, 'accumulated_eval_time': 416.25614857673645, 'accumulated_logging_time': 0.6226837635040283}
I0502 03:01:17.335316 139413162030848 logging_writer.py:48] [12391] accumulated_eval_time=416.256149, accumulated_logging_time=0.622684, accumulated_submission_time=9730.551590, global_step=12391, preemption_count=0, score=9730.551590, test/accuracy=0.324600, test/loss=3.342263, test/num_examples=10000, total_duration=10147.618628, train/accuracy=0.452676, train/loss=2.592682, validation/accuracy=0.416100, validation/loss=2.766193, validation/num_examples=50000
I0502 03:01:25.181102 139413313009408 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.7961485385894775, loss=4.081780910491943
I0502 03:02:42.965869 139413162030848 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6326970458030701, loss=5.201889514923096
I0502 03:04:00.808748 139413313009408 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6368535757064819, loss=5.904397964477539
I0502 03:05:18.611674 139413162030848 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.0085793733596802, loss=4.142276763916016
I0502 03:06:36.434461 139413313009408 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5756364464759827, loss=5.052231311798096
I0502 03:07:54.320785 139413162030848 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7393010854721069, loss=4.058539390563965
I0502 03:08:17.355159 139653621614400 spec.py:298] Evaluating on the training split.
I0502 03:08:25.456395 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 03:08:34.021830 139653621614400 spec.py:326] Evaluating on the test split.
I0502 03:08:35.694188 139653621614400 submission_runner.py:415] Time since start: 10586.00s, 	Step: 12931, 	{'train/accuracy': 0.46384763717651367, 'train/loss': 2.4832041263580322, 'validation/accuracy': 0.42447999119758606, 'validation/loss': 2.670438051223755, 'validation/num_examples': 50000, 'test/accuracy': 0.33320000767707825, 'test/loss': 3.2545461654663086, 'test/num_examples': 10000, 'score': 10150.551031827927, 'total_duration': 10585.99878501892, 'accumulated_submission_time': 10150.551031827927, 'accumulated_eval_time': 434.5951066017151, 'accumulated_logging_time': 0.6565165519714355}
I0502 03:08:35.709105 139413313009408 logging_writer.py:48] [12931] accumulated_eval_time=434.595107, accumulated_logging_time=0.656517, accumulated_submission_time=10150.551032, global_step=12931, preemption_count=0, score=10150.551032, test/accuracy=0.333200, test/loss=3.254546, test/num_examples=10000, total_duration=10585.998785, train/accuracy=0.463848, train/loss=2.483204, validation/accuracy=0.424480, validation/loss=2.670438, validation/num_examples=50000
I0502 03:09:30.224801 139413162030848 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6268724203109741, loss=4.701578140258789
I0502 03:10:48.050800 139413313009408 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.8333040475845337, loss=3.90629243850708
I0502 03:12:05.969395 139413162030848 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.8437271118164062, loss=4.435504913330078
I0502 03:13:23.826304 139413313009408 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6206080317497253, loss=4.970839023590088
I0502 03:14:41.671783 139413162030848 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.8287357091903687, loss=4.1699676513671875
I0502 03:15:35.856019 139653621614400 spec.py:298] Evaluating on the training split.
I0502 03:15:43.965806 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 03:15:52.613530 139653621614400 spec.py:326] Evaluating on the test split.
I0502 03:15:54.277030 139653621614400 submission_runner.py:415] Time since start: 11024.58s, 	Step: 13471, 	{'train/accuracy': 0.47746092081069946, 'train/loss': 2.392457962036133, 'validation/accuracy': 0.44849997758865356, 'validation/loss': 2.55674409866333, 'validation/num_examples': 50000, 'test/accuracy': 0.34880000352859497, 'test/loss': 3.1676065921783447, 'test/num_examples': 10000, 'score': 10570.678523778915, 'total_duration': 11024.581651210785, 'accumulated_submission_time': 10570.678523778915, 'accumulated_eval_time': 453.0160663127899, 'accumulated_logging_time': 0.6837038993835449}
I0502 03:15:54.289163 139413313009408 logging_writer.py:48] [13471] accumulated_eval_time=453.016066, accumulated_logging_time=0.683704, accumulated_submission_time=10570.678524, global_step=13471, preemption_count=0, score=10570.678524, test/accuracy=0.348800, test/loss=3.167607, test/num_examples=10000, total_duration=11024.581651, train/accuracy=0.477461, train/loss=2.392458, validation/accuracy=0.448500, validation/loss=2.556744, validation/num_examples=50000
I0502 03:16:17.662363 139413162030848 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7572734355926514, loss=4.039731502532959
I0502 03:17:35.526405 139413313009408 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.9569379687309265, loss=4.071871757507324
I0502 03:18:53.369950 139413162030848 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.8017851114273071, loss=3.9909753799438477
I0502 03:20:11.229276 139413313009408 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7263701558113098, loss=4.061622142791748
I0502 03:21:29.057864 139413162030848 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.604895830154419, loss=5.267091751098633
I0502 03:22:46.888697 139413313009408 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7393409013748169, loss=4.13134241104126
I0502 03:22:54.401843 139653621614400 spec.py:298] Evaluating on the training split.
I0502 03:23:02.431722 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 03:23:10.968936 139653621614400 spec.py:326] Evaluating on the test split.
I0502 03:23:12.627581 139653621614400 submission_runner.py:415] Time since start: 11462.93s, 	Step: 14011, 	{'train/accuracy': 0.4989062249660492, 'train/loss': 2.3269894123077393, 'validation/accuracy': 0.4503999948501587, 'validation/loss': 2.5686025619506836, 'validation/num_examples': 50000, 'test/accuracy': 0.35340002179145813, 'test/loss': 3.1562888622283936, 'test/num_examples': 10000, 'score': 10990.765407323837, 'total_duration': 11462.932204246521, 'accumulated_submission_time': 10990.765407323837, 'accumulated_eval_time': 471.2417540550232, 'accumulated_logging_time': 0.7145755290985107}
I0502 03:23:12.640434 139413162030848 logging_writer.py:48] [14011] accumulated_eval_time=471.241754, accumulated_logging_time=0.714576, accumulated_submission_time=10990.765407, global_step=14011, preemption_count=0, score=10990.765407, test/accuracy=0.353400, test/loss=3.156289, test/num_examples=10000, total_duration=11462.932204, train/accuracy=0.498906, train/loss=2.326989, validation/accuracy=0.450400, validation/loss=2.568603, validation/num_examples=50000
I0502 03:24:22.687963 139413313009408 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5675445199012756, loss=5.500142574310303
I0502 03:25:40.560731 139413162030848 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.8370397686958313, loss=4.053123950958252
I0502 03:26:58.438738 139413313009408 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.7772296071052551, loss=3.9347116947174072
I0502 03:28:16.267183 139413162030848 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.753933846950531, loss=4.167515754699707
I0502 03:29:34.074373 139413313009408 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7200665473937988, loss=3.9801435470581055
I0502 03:30:12.681122 139653621614400 spec.py:298] Evaluating on the training split.
I0502 03:30:20.722476 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 03:30:29.344966 139653621614400 spec.py:326] Evaluating on the test split.
I0502 03:30:31.006416 139653621614400 submission_runner.py:415] Time since start: 11901.31s, 	Step: 14551, 	{'train/accuracy': 0.4942968785762787, 'train/loss': 2.3598196506500244, 'validation/accuracy': 0.4604399800300598, 'validation/loss': 2.5231099128723145, 'validation/num_examples': 50000, 'test/accuracy': 0.3604000210762024, 'test/loss': 3.1187779903411865, 'test/num_examples': 10000, 'score': 11410.786851406097, 'total_duration': 11901.31103682518, 'accumulated_submission_time': 11410.786851406097, 'accumulated_eval_time': 489.567001581192, 'accumulated_logging_time': 0.739464521408081}
I0502 03:30:31.016992 139413162030848 logging_writer.py:48] [14551] accumulated_eval_time=489.567002, accumulated_logging_time=0.739465, accumulated_submission_time=11410.786851, global_step=14551, preemption_count=0, score=11410.786851, test/accuracy=0.360400, test/loss=3.118778, test/num_examples=10000, total_duration=11901.311037, train/accuracy=0.494297, train/loss=2.359820, validation/accuracy=0.460440, validation/loss=2.523110, validation/num_examples=50000
I0502 03:31:09.987168 139413313009408 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.7768320441246033, loss=4.048539161682129
I0502 03:32:27.801043 139413162030848 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7930410504341125, loss=3.752843141555786
I0502 03:33:45.658325 139413313009408 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.7564343810081482, loss=3.888068675994873
I0502 03:35:03.493636 139413162030848 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6788375973701477, loss=5.944506645202637
I0502 03:36:21.329873 139413313009408 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7254104018211365, loss=4.444469928741455
I0502 03:37:31.102611 139653621614400 spec.py:298] Evaluating on the training split.
I0502 03:37:39.115819 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 03:37:47.812898 139653621614400 spec.py:326] Evaluating on the test split.
I0502 03:37:49.474307 139653621614400 submission_runner.py:415] Time since start: 12339.78s, 	Step: 15091, 	{'train/accuracy': 0.4972851574420929, 'train/loss': 2.3453261852264404, 'validation/accuracy': 0.46045997738838196, 'validation/loss': 2.531139612197876, 'validation/num_examples': 50000, 'test/accuracy': 0.35350000858306885, 'test/loss': 3.138578414916992, 'test/num_examples': 10000, 'score': 11830.854567289352, 'total_duration': 12339.778903484344, 'accumulated_submission_time': 11830.854567289352, 'accumulated_eval_time': 507.938622713089, 'accumulated_logging_time': 0.7608397006988525}
I0502 03:37:49.492006 139413162030848 logging_writer.py:48] [15091] accumulated_eval_time=507.938623, accumulated_logging_time=0.760840, accumulated_submission_time=11830.854567, global_step=15091, preemption_count=0, score=11830.854567, test/accuracy=0.353500, test/loss=3.138578, test/num_examples=10000, total_duration=12339.778903, train/accuracy=0.497285, train/loss=2.345326, validation/accuracy=0.460460, validation/loss=2.531140, validation/num_examples=50000
I0502 03:37:57.307312 139413313009408 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6089826226234436, loss=5.504627227783203
I0502 03:39:15.148768 139413162030848 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.8022615909576416, loss=3.9187896251678467
I0502 03:40:33.025845 139413313009408 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.7637863755226135, loss=3.8156516551971436
I0502 03:41:50.845390 139413162030848 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5447099208831787, loss=5.568778038024902
I0502 03:43:08.695742 139413313009408 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7385697364807129, loss=3.9531774520874023
I0502 03:44:26.528628 139413162030848 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.8590579628944397, loss=3.8273439407348633
I0502 03:44:49.621224 139653621614400 spec.py:298] Evaluating on the training split.
I0502 03:44:57.620584 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 03:45:06.210567 139653621614400 spec.py:326] Evaluating on the test split.
I0502 03:45:07.871140 139653621614400 submission_runner.py:415] Time since start: 12778.18s, 	Step: 15631, 	{'train/accuracy': 0.5125585794448853, 'train/loss': 2.2513651847839355, 'validation/accuracy': 0.47151997685432434, 'validation/loss': 2.453138589859009, 'validation/num_examples': 50000, 'test/accuracy': 0.3628000319004059, 'test/loss': 3.058053493499756, 'test/num_examples': 10000, 'score': 12250.961234331131, 'total_duration': 12778.17575454712, 'accumulated_submission_time': 12250.961234331131, 'accumulated_eval_time': 526.1884808540344, 'accumulated_logging_time': 0.7940304279327393}
I0502 03:45:07.883880 139413313009408 logging_writer.py:48] [15631] accumulated_eval_time=526.188481, accumulated_logging_time=0.794030, accumulated_submission_time=12250.961234, global_step=15631, preemption_count=0, score=12250.961234, test/accuracy=0.362800, test/loss=3.058053, test/num_examples=10000, total_duration=12778.175755, train/accuracy=0.512559, train/loss=2.251365, validation/accuracy=0.471520, validation/loss=2.453139, validation/num_examples=50000
I0502 03:46:02.351382 139413162030848 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6749265789985657, loss=4.536729335784912
I0502 03:47:20.221788 139413313009408 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.5779075026512146, loss=5.032252311706543
I0502 03:48:38.085289 139413162030848 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6462460160255432, loss=5.995163440704346
I0502 03:49:55.921246 139413313009408 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9210960865020752, loss=3.717648506164551
I0502 03:51:13.783196 139413162030848 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.804574728012085, loss=4.33997106552124
I0502 03:52:07.985207 139653621614400 spec.py:298] Evaluating on the training split.
I0502 03:52:15.975224 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 03:52:24.556696 139653621614400 spec.py:326] Evaluating on the test split.
I0502 03:52:26.212808 139653621614400 submission_runner.py:415] Time since start: 13216.52s, 	Step: 16171, 	{'train/accuracy': 0.5072460770606995, 'train/loss': 2.2633824348449707, 'validation/accuracy': 0.4778999984264374, 'validation/loss': 2.4165658950805664, 'validation/num_examples': 50000, 'test/accuracy': 0.36730000376701355, 'test/loss': 3.025869369506836, 'test/num_examples': 10000, 'score': 12671.038233280182, 'total_duration': 13216.517429351807, 'accumulated_submission_time': 12671.038233280182, 'accumulated_eval_time': 544.4160325527191, 'accumulated_logging_time': 0.823927640914917}
I0502 03:52:26.226152 139413313009408 logging_writer.py:48] [16171] accumulated_eval_time=544.416033, accumulated_logging_time=0.823928, accumulated_submission_time=12671.038233, global_step=16171, preemption_count=0, score=12671.038233, test/accuracy=0.367300, test/loss=3.025869, test/num_examples=10000, total_duration=13216.517429, train/accuracy=0.507246, train/loss=2.263382, validation/accuracy=0.477900, validation/loss=2.416566, validation/num_examples=50000
I0502 03:52:49.583651 139413162030848 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7535415887832642, loss=4.284891605377197
I0502 03:54:07.491291 139413313009408 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6139737367630005, loss=5.0726704597473145
I0502 03:55:25.340015 139413162030848 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5694334506988525, loss=5.632933139801025
I0502 03:56:43.217920 139413313009408 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6124328374862671, loss=5.47068977355957
I0502 03:58:01.085156 139413162030848 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.672026515007019, loss=4.231726169586182
I0502 03:59:18.925551 139413313009408 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.7932653427124023, loss=3.9313695430755615
I0502 03:59:26.408799 139653621614400 spec.py:298] Evaluating on the training split.
I0502 03:59:34.396853 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 03:59:43.076361 139653621614400 spec.py:326] Evaluating on the test split.
I0502 03:59:44.736675 139653621614400 submission_runner.py:415] Time since start: 13655.04s, 	Step: 16711, 	{'train/accuracy': 0.5418164134025574, 'train/loss': 2.1171469688415527, 'validation/accuracy': 0.48625999689102173, 'validation/loss': 2.3979251384735107, 'validation/num_examples': 50000, 'test/accuracy': 0.38100001215934753, 'test/loss': 3.002438545227051, 'test/num_examples': 10000, 'score': 13091.20087981224, 'total_duration': 13655.041302204132, 'accumulated_submission_time': 13091.20087981224, 'accumulated_eval_time': 562.7438662052155, 'accumulated_logging_time': 0.8501362800598145}
I0502 03:59:44.749134 139413162030848 logging_writer.py:48] [16711] accumulated_eval_time=562.743866, accumulated_logging_time=0.850136, accumulated_submission_time=13091.200880, global_step=16711, preemption_count=0, score=13091.200880, test/accuracy=0.381000, test/loss=3.002439, test/num_examples=10000, total_duration=13655.041302, train/accuracy=0.541816, train/loss=2.117147, validation/accuracy=0.486260, validation/loss=2.397925, validation/num_examples=50000
I0502 04:00:54.825277 139413313009408 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6617922186851501, loss=4.543708324432373
I0502 04:02:12.700344 139413162030848 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.9385943412780762, loss=3.854243278503418
I0502 04:03:30.651735 139413313009408 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.8840256929397583, loss=3.741062641143799
I0502 04:04:48.488772 139413162030848 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.8693248629570007, loss=3.8109307289123535
I0502 04:06:06.345189 139413313009408 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.6470017433166504, loss=4.642200469970703
I0502 04:06:44.955877 139653621614400 spec.py:298] Evaluating on the training split.
I0502 04:06:52.907579 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 04:07:01.751585 139653621614400 spec.py:326] Evaluating on the test split.
I0502 04:07:03.409343 139653621614400 submission_runner.py:415] Time since start: 14093.71s, 	Step: 17251, 	{'train/accuracy': 0.5215234160423279, 'train/loss': 2.279108762741089, 'validation/accuracy': 0.48429998755455017, 'validation/loss': 2.4717276096343994, 'validation/num_examples': 50000, 'test/accuracy': 0.37060001492500305, 'test/loss': 3.0945940017700195, 'test/num_examples': 10000, 'score': 13511.388444900513, 'total_duration': 14093.713958740234, 'accumulated_submission_time': 13511.388444900513, 'accumulated_eval_time': 581.1972789764404, 'accumulated_logging_time': 0.8745758533477783}
I0502 04:07:03.421656 139413162030848 logging_writer.py:48] [17251] accumulated_eval_time=581.197279, accumulated_logging_time=0.874576, accumulated_submission_time=13511.388445, global_step=17251, preemption_count=0, score=13511.388445, test/accuracy=0.370600, test/loss=3.094594, test/num_examples=10000, total_duration=14093.713959, train/accuracy=0.521523, train/loss=2.279109, validation/accuracy=0.484300, validation/loss=2.471728, validation/num_examples=50000
I0502 04:07:42.363850 139413313009408 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5837040543556213, loss=5.244073390960693
I0502 04:09:00.199038 139413162030848 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.8110043406486511, loss=3.743407964706421
I0502 04:10:18.009083 139413313009408 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6866885423660278, loss=5.644618988037109
I0502 04:11:35.859165 139413162030848 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.7853154540061951, loss=3.8609554767608643
I0502 04:12:53.733657 139413313009408 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.8227059245109558, loss=3.7541983127593994
I0502 04:14:03.548527 139653621614400 spec.py:298] Evaluating on the training split.
I0502 04:14:11.486543 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 04:14:20.165894 139653621614400 spec.py:326] Evaluating on the test split.
I0502 04:14:21.823029 139653621614400 submission_runner.py:415] Time since start: 14532.13s, 	Step: 17791, 	{'train/accuracy': 0.5377734303474426, 'train/loss': 2.099475860595703, 'validation/accuracy': 0.4939599931240082, 'validation/loss': 2.2994842529296875, 'validation/num_examples': 50000, 'test/accuracy': 0.38680002093315125, 'test/loss': 2.917004108428955, 'test/num_examples': 10000, 'score': 13931.49606513977, 'total_duration': 14532.127606868744, 'accumulated_submission_time': 13931.49606513977, 'accumulated_eval_time': 599.4717071056366, 'accumulated_logging_time': 0.8989613056182861}
I0502 04:14:21.838909 139413162030848 logging_writer.py:48] [17791] accumulated_eval_time=599.471707, accumulated_logging_time=0.898961, accumulated_submission_time=13931.496065, global_step=17791, preemption_count=0, score=13931.496065, test/accuracy=0.386800, test/loss=2.917004, test/num_examples=10000, total_duration=14532.127607, train/accuracy=0.537773, train/loss=2.099476, validation/accuracy=0.493960, validation/loss=2.299484, validation/num_examples=50000
I0502 04:14:29.674566 139413313009408 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.5797370672225952, loss=5.434814929962158
I0502 04:15:47.543530 139413162030848 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.8402735590934753, loss=3.7119579315185547
I0502 04:17:05.423604 139413313009408 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7541820406913757, loss=3.6391706466674805
I0502 04:18:23.286222 139413162030848 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.6902551054954529, loss=5.481085300445557
I0502 04:19:41.100317 139413313009408 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.8238753080368042, loss=3.661942481994629
I0502 04:20:58.956452 139413162030848 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5868141651153564, loss=5.13661003112793
I0502 04:21:22.041064 139653621614400 spec.py:298] Evaluating on the training split.
I0502 04:21:29.989449 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 04:21:38.684575 139653621614400 spec.py:326] Evaluating on the test split.
I0502 04:21:40.342828 139653621614400 submission_runner.py:415] Time since start: 14970.65s, 	Step: 18331, 	{'train/accuracy': 0.5472460985183716, 'train/loss': 2.078303813934326, 'validation/accuracy': 0.5027399659156799, 'validation/loss': 2.2991981506347656, 'validation/num_examples': 50000, 'test/accuracy': 0.3930000066757202, 'test/loss': 2.931837558746338, 'test/num_examples': 10000, 'score': 14351.677814722061, 'total_duration': 14970.647427082062, 'accumulated_submission_time': 14351.677814722061, 'accumulated_eval_time': 617.7734076976776, 'accumulated_logging_time': 0.9278745651245117}
I0502 04:21:40.360999 139413313009408 logging_writer.py:48] [18331] accumulated_eval_time=617.773408, accumulated_logging_time=0.927875, accumulated_submission_time=14351.677815, global_step=18331, preemption_count=0, score=14351.677815, test/accuracy=0.393000, test/loss=2.931838, test/num_examples=10000, total_duration=14970.647427, train/accuracy=0.547246, train/loss=2.078304, validation/accuracy=0.502740, validation/loss=2.299198, validation/num_examples=50000
I0502 04:22:34.888616 139413162030848 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.808453381061554, loss=3.5517289638519287
I0502 04:23:52.705310 139413313009408 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.8393206000328064, loss=3.5842337608337402
I0502 04:25:10.518457 139413162030848 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.6095978617668152, loss=5.716908931732178
I0502 04:26:28.348483 139413313009408 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.7498963475227356, loss=4.527806282043457
I0502 04:27:46.209865 139413162030848 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7975924015045166, loss=3.650022506713867
I0502 04:28:40.433562 139653621614400 spec.py:298] Evaluating on the training split.
I0502 04:28:48.339120 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 04:28:57.024986 139653621614400 spec.py:326] Evaluating on the test split.
I0502 04:28:58.688895 139653621614400 submission_runner.py:415] Time since start: 15408.99s, 	Step: 18871, 	{'train/accuracy': 0.5433007478713989, 'train/loss': 2.0737359523773193, 'validation/accuracy': 0.5039600133895874, 'validation/loss': 2.265446901321411, 'validation/num_examples': 50000, 'test/accuracy': 0.39330002665519714, 'test/loss': 2.8816025257110596, 'test/num_examples': 10000, 'score': 14771.731041908264, 'total_duration': 15408.993498325348, 'accumulated_submission_time': 14771.731041908264, 'accumulated_eval_time': 636.0286748409271, 'accumulated_logging_time': 0.9581594467163086}
I0502 04:28:58.706402 139413313009408 logging_writer.py:48] [18871] accumulated_eval_time=636.028675, accumulated_logging_time=0.958159, accumulated_submission_time=14771.731042, global_step=18871, preemption_count=0, score=14771.731042, test/accuracy=0.393300, test/loss=2.881603, test/num_examples=10000, total_duration=15408.993498, train/accuracy=0.543301, train/loss=2.073736, validation/accuracy=0.503960, validation/loss=2.265447, validation/num_examples=50000
I0502 04:29:22.074394 139413162030848 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.8848559260368347, loss=3.7606048583984375
I0502 04:30:39.910486 139413313009408 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6407970786094666, loss=5.723195552825928
I0502 04:31:57.734717 139413162030848 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.7472827434539795, loss=3.610544443130493
I0502 04:33:15.556762 139413313009408 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.7984732389450073, loss=3.6584763526916504
I0502 04:34:33.431883 139413162030848 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.7889699935913086, loss=3.600266933441162
I0502 04:35:51.291692 139413313009408 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.7416915893554688, loss=3.99232816696167
I0502 04:35:58.812338 139653621614400 spec.py:298] Evaluating on the training split.
I0502 04:36:06.713037 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 04:36:15.446592 139653621614400 spec.py:326] Evaluating on the test split.
I0502 04:36:17.112034 139653621614400 submission_runner.py:415] Time since start: 15847.42s, 	Step: 19411, 	{'train/accuracy': 0.5798632502555847, 'train/loss': 1.8920432329177856, 'validation/accuracy': 0.5148000121116638, 'validation/loss': 2.2058675289154053, 'validation/num_examples': 50000, 'test/accuracy': 0.40220001339912415, 'test/loss': 2.8267576694488525, 'test/num_examples': 10000, 'score': 15191.817367315292, 'total_duration': 15847.416659832, 'accumulated_submission_time': 15191.817367315292, 'accumulated_eval_time': 654.3283517360687, 'accumulated_logging_time': 0.9880571365356445}
I0502 04:36:17.123986 139413162030848 logging_writer.py:48] [19411] accumulated_eval_time=654.328352, accumulated_logging_time=0.988057, accumulated_submission_time=15191.817367, global_step=19411, preemption_count=0, score=15191.817367, test/accuracy=0.402200, test/loss=2.826758, test/num_examples=10000, total_duration=15847.416660, train/accuracy=0.579863, train/loss=1.892043, validation/accuracy=0.514800, validation/loss=2.205868, validation/num_examples=50000
I0502 04:37:27.158674 139413313009408 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.8154764175415039, loss=3.6498665809631348
I0502 04:38:44.963607 139413162030848 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.6972387433052063, loss=5.387880325317383
I0502 04:40:02.798317 139413313009408 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.8468793630599976, loss=3.57338809967041
I0502 04:41:20.615391 139413162030848 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.6360844373703003, loss=4.899780750274658
I0502 04:42:38.415199 139413313009408 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.8417269587516785, loss=3.6818745136260986
I0502 04:43:17.781766 139653621614400 spec.py:298] Evaluating on the training split.
I0502 04:43:25.677644 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 04:43:34.467213 139653621614400 spec.py:326] Evaluating on the test split.
I0502 04:43:36.119173 139653621614400 submission_runner.py:415] Time since start: 16286.42s, 	Step: 19952, 	{'train/accuracy': 0.5670703053474426, 'train/loss': 1.969274878501892, 'validation/accuracy': 0.5212000012397766, 'validation/loss': 2.1840291023254395, 'validation/num_examples': 50000, 'test/accuracy': 0.406000018119812, 'test/loss': 2.8097076416015625, 'test/num_examples': 10000, 'score': 15612.458054304123, 'total_duration': 16286.423792123795, 'accumulated_submission_time': 15612.458054304123, 'accumulated_eval_time': 672.6657116413116, 'accumulated_logging_time': 1.0098767280578613}
I0502 04:43:36.131685 139413162030848 logging_writer.py:48] [19952] accumulated_eval_time=672.665712, accumulated_logging_time=1.009877, accumulated_submission_time=15612.458054, global_step=19952, preemption_count=0, score=15612.458054, test/accuracy=0.406000, test/loss=2.809708, test/num_examples=10000, total_duration=16286.423792, train/accuracy=0.567070, train/loss=1.969275, validation/accuracy=0.521200, validation/loss=2.184029, validation/num_examples=50000
I0502 04:44:14.343160 139413313009408 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.772833526134491, loss=3.477144479751587
I0502 04:45:32.222121 139413162030848 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6528654098510742, loss=4.880125999450684
I0502 04:46:50.078272 139413313009408 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.8061468005180359, loss=3.504424810409546
I0502 04:48:07.948592 139413162030848 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7493957281112671, loss=4.149580478668213
I0502 04:49:25.835972 139413313009408 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.8478599190711975, loss=3.4961366653442383
I0502 04:50:36.453358 139653621614400 spec.py:298] Evaluating on the training split.
I0502 04:50:44.424186 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 04:50:53.266730 139653621614400 spec.py:326] Evaluating on the test split.
I0502 04:50:54.924058 139653621614400 submission_runner.py:415] Time since start: 16725.23s, 	Step: 20492, 	{'train/accuracy': 0.5654882788658142, 'train/loss': 1.952566385269165, 'validation/accuracy': 0.5236600041389465, 'validation/loss': 2.1569571495056152, 'validation/num_examples': 50000, 'test/accuracy': 0.4068000316619873, 'test/loss': 2.8063700199127197, 'test/num_examples': 10000, 'score': 16032.759641885757, 'total_duration': 16725.228646993637, 'accumulated_submission_time': 16032.759641885757, 'accumulated_eval_time': 691.1363332271576, 'accumulated_logging_time': 1.0351815223693848}
I0502 04:50:54.941887 139413162030848 logging_writer.py:48] [20492] accumulated_eval_time=691.136333, accumulated_logging_time=1.035182, accumulated_submission_time=16032.759642, global_step=20492, preemption_count=0, score=16032.759642, test/accuracy=0.406800, test/loss=2.806370, test/num_examples=10000, total_duration=16725.228647, train/accuracy=0.565488, train/loss=1.952566, validation/accuracy=0.523660, validation/loss=2.156957, validation/num_examples=50000
I0502 04:51:01.974167 139413313009408 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6456459760665894, loss=5.605362892150879
I0502 04:52:19.820772 139413162030848 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6414355635643005, loss=5.646364212036133
I0502 04:53:37.627486 139413313009408 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.9997214674949646, loss=3.6149439811706543
I0502 04:54:55.472159 139413162030848 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.8051759004592896, loss=3.5488812923431396
I0502 04:56:13.341871 139413313009408 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.8573084473609924, loss=3.6003074645996094
I0502 04:57:31.183682 139413162030848 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.9478644728660583, loss=3.5018422603607178
I0502 04:57:55.014407 139653621614400 spec.py:298] Evaluating on the training split.
I0502 04:58:02.916224 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 04:58:11.658827 139653621614400 spec.py:326] Evaluating on the test split.
I0502 04:58:13.323509 139653621614400 submission_runner.py:415] Time since start: 17163.63s, 	Step: 21032, 	{'train/accuracy': 0.5785155892372131, 'train/loss': 1.9298574924468994, 'validation/accuracy': 0.526960015296936, 'validation/loss': 2.1626498699188232, 'validation/num_examples': 50000, 'test/accuracy': 0.41850000619888306, 'test/loss': 2.795966148376465, 'test/num_examples': 10000, 'score': 16452.812147140503, 'total_duration': 17163.628099679947, 'accumulated_submission_time': 16452.812147140503, 'accumulated_eval_time': 709.4453580379486, 'accumulated_logging_time': 1.065666675567627}
I0502 04:58:13.340397 139413313009408 logging_writer.py:48] [21032] accumulated_eval_time=709.445358, accumulated_logging_time=1.065667, accumulated_submission_time=16452.812147, global_step=21032, preemption_count=0, score=16452.812147, test/accuracy=0.418500, test/loss=2.795966, test/num_examples=10000, total_duration=17163.628100, train/accuracy=0.578516, train/loss=1.929857, validation/accuracy=0.526960, validation/loss=2.162650, validation/num_examples=50000
I0502 04:59:07.090282 139413162030848 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.7847880721092224, loss=3.524773359298706
I0502 05:00:24.916122 139413313009408 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.7158342599868774, loss=5.702966213226318
I0502 05:01:42.776309 139413162030848 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7443466186523438, loss=3.9443535804748535
I0502 05:03:00.638437 139413313009408 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6467900276184082, loss=5.707688808441162
I0502 05:04:18.515093 139413162030848 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7786980867385864, loss=3.5672357082366943
I0502 05:05:13.514256 139653621614400 spec.py:298] Evaluating on the training split.
I0502 05:05:21.411956 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 05:05:30.234844 139653621614400 spec.py:326] Evaluating on the test split.
I0502 05:05:31.906636 139653621614400 submission_runner.py:415] Time since start: 17602.21s, 	Step: 21572, 	{'train/accuracy': 0.5745507478713989, 'train/loss': 1.883386492729187, 'validation/accuracy': 0.5284199714660645, 'validation/loss': 2.106630802154541, 'validation/num_examples': 50000, 'test/accuracy': 0.41350001096725464, 'test/loss': 2.749572277069092, 'test/num_examples': 10000, 'score': 16872.966319322586, 'total_duration': 17602.21125483513, 'accumulated_submission_time': 16872.966319322586, 'accumulated_eval_time': 727.8376910686493, 'accumulated_logging_time': 1.0948185920715332}
I0502 05:05:31.919293 139413313009408 logging_writer.py:48] [21572] accumulated_eval_time=727.837691, accumulated_logging_time=1.094819, accumulated_submission_time=16872.966319, global_step=21572, preemption_count=0, score=16872.966319, test/accuracy=0.413500, test/loss=2.749572, test/num_examples=10000, total_duration=17602.211255, train/accuracy=0.574551, train/loss=1.883386, validation/accuracy=0.528420, validation/loss=2.106631, validation/num_examples=50000
I0502 05:05:54.545796 139413162030848 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.8015176653862, loss=4.133081912994385
I0502 05:07:12.425124 139413313009408 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.7983471155166626, loss=3.4811079502105713
I0502 05:08:30.299940 139413162030848 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.7521172761917114, loss=4.524839401245117
I0502 05:09:48.217197 139413313009408 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.8833795785903931, loss=3.5689501762390137
I0502 05:11:06.109333 139413162030848 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.8784663081169128, loss=3.5621109008789062
I0502 05:12:23.967673 139413313009408 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.8420805931091309, loss=3.558755397796631
I0502 05:12:32.232259 139653621614400 spec.py:298] Evaluating on the training split.
I0502 05:12:40.113297 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 05:12:49.033920 139653621614400 spec.py:326] Evaluating on the test split.
I0502 05:12:50.698942 139653621614400 submission_runner.py:415] Time since start: 18041.00s, 	Step: 22112, 	{'train/accuracy': 0.598339855670929, 'train/loss': 1.811564564704895, 'validation/accuracy': 0.5315799713134766, 'validation/loss': 2.1263439655303955, 'validation/num_examples': 50000, 'test/accuracy': 0.42170003056526184, 'test/loss': 2.7670469284057617, 'test/num_examples': 10000, 'score': 17293.259194374084, 'total_duration': 18041.003535747528, 'accumulated_submission_time': 17293.259194374084, 'accumulated_eval_time': 746.304297208786, 'accumulated_logging_time': 1.1203477382659912}
I0502 05:12:50.717540 139413162030848 logging_writer.py:48] [22112] accumulated_eval_time=746.304297, accumulated_logging_time=1.120348, accumulated_submission_time=17293.259194, global_step=22112, preemption_count=0, score=17293.259194, test/accuracy=0.421700, test/loss=2.767047, test/num_examples=10000, total_duration=18041.003536, train/accuracy=0.598340, train/loss=1.811565, validation/accuracy=0.531580, validation/loss=2.126344, validation/num_examples=50000
I0502 05:13:59.998816 139413313009408 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7817342281341553, loss=5.307621955871582
I0502 05:15:17.867957 139413162030848 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.7831346988677979, loss=4.676685333251953
I0502 05:16:35.766177 139413313009408 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7646929621696472, loss=5.616963863372803
I0502 05:17:53.615511 139413162030848 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8140526413917542, loss=5.360024929046631
I0502 05:19:11.474602 139413313009408 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6902410984039307, loss=5.546383380889893
I0502 05:19:50.921296 139653621614400 spec.py:298] Evaluating on the training split.
I0502 05:19:58.796090 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 05:20:07.533155 139653621614400 spec.py:326] Evaluating on the test split.
I0502 05:20:09.182874 139653621614400 submission_runner.py:415] Time since start: 18479.49s, 	Step: 22652, 	{'train/accuracy': 0.5834375023841858, 'train/loss': 1.857932448387146, 'validation/accuracy': 0.5338199734687805, 'validation/loss': 2.08170485496521, 'validation/num_examples': 50000, 'test/accuracy': 0.4166000187397003, 'test/loss': 2.7438559532165527, 'test/num_examples': 10000, 'score': 17713.444076299667, 'total_duration': 18479.487469911575, 'accumulated_submission_time': 17713.444076299667, 'accumulated_eval_time': 764.5658068656921, 'accumulated_logging_time': 1.150357961654663}
I0502 05:20:09.202948 139413162030848 logging_writer.py:48] [22652] accumulated_eval_time=764.565807, accumulated_logging_time=1.150358, accumulated_submission_time=17713.444076, global_step=22652, preemption_count=0, score=17713.444076, test/accuracy=0.416600, test/loss=2.743856, test/num_examples=10000, total_duration=18479.487470, train/accuracy=0.583438, train/loss=1.857932, validation/accuracy=0.533820, validation/loss=2.081705, validation/num_examples=50000
I0502 05:20:47.396104 139413313009408 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.8120576739311218, loss=3.6118385791778564
I0502 05:22:05.274953 139413162030848 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.8211426138877869, loss=3.4444642066955566
I0502 05:23:23.179343 139413313009408 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.9201448559761047, loss=3.6042280197143555
I0502 05:24:41.064703 139413162030848 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.770061731338501, loss=3.8368115425109863
I0502 05:25:58.917878 139413313009408 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.6449355483055115, loss=5.090422630310059
I0502 05:27:09.474793 139653621614400 spec.py:298] Evaluating on the training split.
I0502 05:27:17.337146 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 05:27:26.085553 139653621614400 spec.py:326] Evaluating on the test split.
I0502 05:27:27.750713 139653621614400 submission_runner.py:415] Time since start: 18918.06s, 	Step: 23192, 	{'train/accuracy': 0.5718554854393005, 'train/loss': 1.972922682762146, 'validation/accuracy': 0.5333600044250488, 'validation/loss': 2.1591458320617676, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.7841849327087402, 'test/num_examples': 10000, 'score': 18133.69573187828, 'total_duration': 18918.055296182632, 'accumulated_submission_time': 18133.69573187828, 'accumulated_eval_time': 782.8416438102722, 'accumulated_logging_time': 1.1833250522613525}
I0502 05:27:27.768257 139413162030848 logging_writer.py:48] [23192] accumulated_eval_time=782.841644, accumulated_logging_time=1.183325, accumulated_submission_time=18133.695732, global_step=23192, preemption_count=0, score=18133.695732, test/accuracy=0.415800, test/loss=2.784185, test/num_examples=10000, total_duration=18918.055296, train/accuracy=0.571855, train/loss=1.972923, validation/accuracy=0.533360, validation/loss=2.159146, validation/num_examples=50000
I0502 05:27:34.812458 139413313009408 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.9148337841033936, loss=3.467874765396118
I0502 05:28:52.667618 139413162030848 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.8317595720291138, loss=3.4934966564178467
I0502 05:30:10.575949 139413313009408 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.8863319754600525, loss=3.4585297107696533
I0502 05:31:28.420444 139413162030848 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8743610978126526, loss=3.471517562866211
I0502 05:32:46.307108 139413313009408 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.8266951441764832, loss=3.465388774871826
I0502 05:34:04.172012 139413162030848 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.7613218426704407, loss=5.212262153625488
I0502 05:34:28.030167 139653621614400 spec.py:298] Evaluating on the training split.
I0502 05:34:35.896892 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 05:34:44.884941 139653621614400 spec.py:326] Evaluating on the test split.
I0502 05:34:46.560554 139653621614400 submission_runner.py:415] Time since start: 19356.87s, 	Step: 23732, 	{'train/accuracy': 0.5986718535423279, 'train/loss': 1.8310506343841553, 'validation/accuracy': 0.5467999577522278, 'validation/loss': 2.0759449005126953, 'validation/num_examples': 50000, 'test/accuracy': 0.42660000920295715, 'test/loss': 2.725771427154541, 'test/num_examples': 10000, 'score': 18553.937541007996, 'total_duration': 19356.86517238617, 'accumulated_submission_time': 18553.937541007996, 'accumulated_eval_time': 801.3719823360443, 'accumulated_logging_time': 1.2136051654815674}
I0502 05:34:46.573617 139413313009408 logging_writer.py:48] [23732] accumulated_eval_time=801.371982, accumulated_logging_time=1.213605, accumulated_submission_time=18553.937541, global_step=23732, preemption_count=0, score=18553.937541, test/accuracy=0.426600, test/loss=2.725771, test/num_examples=10000, total_duration=19356.865172, train/accuracy=0.598672, train/loss=1.831051, validation/accuracy=0.546800, validation/loss=2.075945, validation/num_examples=50000
I0502 05:35:40.329323 139413162030848 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.7093812227249146, loss=5.178690433502197
I0502 05:36:58.194313 139413313009408 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.806948184967041, loss=5.357364177703857
I0502 05:38:16.047153 139413162030848 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.0122945308685303, loss=3.551436424255371
I0502 05:39:33.922852 139413313009408 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.7885160446166992, loss=3.983499050140381
I0502 05:40:51.797684 139413162030848 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7878096699714661, loss=3.948662519454956
I0502 05:41:46.772832 139653621614400 spec.py:298] Evaluating on the training split.
I0502 05:41:54.645043 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 05:42:03.495461 139653621614400 spec.py:326] Evaluating on the test split.
I0502 05:42:05.162539 139653621614400 submission_runner.py:415] Time since start: 19795.47s, 	Step: 24272, 	{'train/accuracy': 0.5940819978713989, 'train/loss': 1.795839786529541, 'validation/accuracy': 0.5488399863243103, 'validation/loss': 2.013808012008667, 'validation/num_examples': 50000, 'test/accuracy': 0.43290001153945923, 'test/loss': 2.6545422077178955, 'test/num_examples': 10000, 'score': 18974.116726398468, 'total_duration': 19795.467153072357, 'accumulated_submission_time': 18974.116726398468, 'accumulated_eval_time': 819.7616374492645, 'accumulated_logging_time': 1.2393014430999756}
I0502 05:42:05.175218 139413313009408 logging_writer.py:48] [24272] accumulated_eval_time=819.761637, accumulated_logging_time=1.239301, accumulated_submission_time=18974.116726, global_step=24272, preemption_count=0, score=18974.116726, test/accuracy=0.432900, test/loss=2.654542, test/num_examples=10000, total_duration=19795.467153, train/accuracy=0.594082, train/loss=1.795840, validation/accuracy=0.548840, validation/loss=2.013808, validation/num_examples=50000
I0502 05:42:27.808521 139413162030848 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.9101124405860901, loss=3.637032985687256
I0502 05:43:45.705205 139413313009408 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8065052032470703, loss=4.149209976196289
I0502 05:45:03.598399 139413162030848 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.703562319278717, loss=4.7951340675354
I0502 05:46:21.487536 139413313009408 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.8128905892372131, loss=3.4047718048095703
I0502 05:47:39.385793 139413162030848 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.9186122417449951, loss=3.3803040981292725
I0502 05:48:57.298254 139413313009408 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6787630319595337, loss=5.667707443237305
I0502 05:49:05.572619 139653621614400 spec.py:298] Evaluating on the training split.
I0502 05:49:13.453667 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 05:49:22.224511 139653621614400 spec.py:326] Evaluating on the test split.
I0502 05:49:23.890439 139653621614400 submission_runner.py:415] Time since start: 20234.20s, 	Step: 24812, 	{'train/accuracy': 0.5988085865974426, 'train/loss': 1.8303464651107788, 'validation/accuracy': 0.5506399869918823, 'validation/loss': 2.0595686435699463, 'validation/num_examples': 50000, 'test/accuracy': 0.4376000165939331, 'test/loss': 2.687897205352783, 'test/num_examples': 10000, 'score': 19394.49334168434, 'total_duration': 20234.195056200027, 'accumulated_submission_time': 19394.49334168434, 'accumulated_eval_time': 838.0794067382812, 'accumulated_logging_time': 1.265521764755249}
I0502 05:49:23.906810 139413162030848 logging_writer.py:48] [24812] accumulated_eval_time=838.079407, accumulated_logging_time=1.265522, accumulated_submission_time=19394.493342, global_step=24812, preemption_count=0, score=19394.493342, test/accuracy=0.437600, test/loss=2.687897, test/num_examples=10000, total_duration=20234.195056, train/accuracy=0.598809, train/loss=1.830346, validation/accuracy=0.550640, validation/loss=2.059569, validation/num_examples=50000
I0502 05:50:33.197628 139413313009408 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.960934042930603, loss=3.4708733558654785
I0502 05:51:51.059494 139413162030848 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7408602833747864, loss=4.333308696746826
I0502 05:53:08.989585 139413313009408 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.9094078540802002, loss=3.3327739238739014
I0502 05:54:26.846267 139413162030848 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.8641536831855774, loss=3.8222365379333496
I0502 05:55:44.757444 139413313009408 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.8505123853683472, loss=3.5141148567199707
I0502 05:56:24.155827 139653621614400 spec.py:298] Evaluating on the training split.
I0502 05:56:32.060767 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 05:56:40.847607 139653621614400 spec.py:326] Evaluating on the test split.
I0502 05:56:42.503888 139653621614400 submission_runner.py:415] Time since start: 20672.81s, 	Step: 25352, 	{'train/accuracy': 0.6066796779632568, 'train/loss': 1.765734076499939, 'validation/accuracy': 0.5554999709129333, 'validation/loss': 2.0010578632354736, 'validation/num_examples': 50000, 'test/accuracy': 0.43630000948905945, 'test/loss': 2.635207414627075, 'test/num_examples': 10000, 'score': 19814.72307920456, 'total_duration': 20672.808485269547, 'accumulated_submission_time': 19814.72307920456, 'accumulated_eval_time': 856.4274027347565, 'accumulated_logging_time': 1.2939541339874268}
I0502 05:56:42.523424 139413162030848 logging_writer.py:48] [25352] accumulated_eval_time=856.427403, accumulated_logging_time=1.293954, accumulated_submission_time=19814.723079, global_step=25352, preemption_count=0, score=19814.723079, test/accuracy=0.436300, test/loss=2.635207, test/num_examples=10000, total_duration=20672.808485, train/accuracy=0.606680, train/loss=1.765734, validation/accuracy=0.555500, validation/loss=2.001058, validation/num_examples=50000
I0502 05:57:20.719729 139413313009408 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.8013958930969238, loss=3.446279764175415
I0502 05:58:38.625593 139413162030848 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8668990135192871, loss=3.372041940689087
I0502 05:59:56.488373 139413313009408 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.716405987739563, loss=4.615029335021973
I0502 06:01:14.357807 139413162030848 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8074628710746765, loss=3.4044384956359863
I0502 06:02:32.257284 139413313009408 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.1815803050994873, loss=3.4977753162384033
I0502 06:03:42.811836 139653621614400 spec.py:298] Evaluating on the training split.
I0502 06:03:50.690443 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 06:03:59.586556 139653621614400 spec.py:326] Evaluating on the test split.
I0502 06:04:01.257778 139653621614400 submission_runner.py:415] Time since start: 21111.56s, 	Step: 25892, 	{'train/accuracy': 0.5953124761581421, 'train/loss': 1.874042272567749, 'validation/accuracy': 0.5527799725532532, 'validation/loss': 2.0759801864624023, 'validation/num_examples': 50000, 'test/accuracy': 0.4417000114917755, 'test/loss': 2.700547933578491, 'test/num_examples': 10000, 'score': 20234.990947008133, 'total_duration': 21111.562367916107, 'accumulated_submission_time': 20234.990947008133, 'accumulated_eval_time': 874.8732771873474, 'accumulated_logging_time': 1.3265931606292725}
I0502 06:04:01.277467 139413162030848 logging_writer.py:48] [25892] accumulated_eval_time=874.873277, accumulated_logging_time=1.326593, accumulated_submission_time=20234.990947, global_step=25892, preemption_count=0, score=20234.990947, test/accuracy=0.441700, test/loss=2.700548, test/num_examples=10000, total_duration=21111.562368, train/accuracy=0.595312, train/loss=1.874042, validation/accuracy=0.552780, validation/loss=2.075980, validation/num_examples=50000
I0502 06:04:08.341198 139413313009408 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.8675178289413452, loss=3.377511501312256
I0502 06:05:26.233968 139413162030848 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8949059844017029, loss=3.4561655521392822
I0502 06:06:44.129944 139413313009408 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.7893351316452026, loss=3.345289468765259
I0502 06:08:02.049873 139413162030848 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.8700103759765625, loss=3.6021080017089844
I0502 06:09:19.954570 139413313009408 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.8533639907836914, loss=3.495924472808838
I0502 06:10:37.873430 139413162030848 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.8222323656082153, loss=3.385843276977539
I0502 06:11:01.714621 139653621614400 spec.py:298] Evaluating on the training split.
I0502 06:11:09.539729 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 06:11:18.378385 139653621614400 spec.py:326] Evaluating on the test split.
I0502 06:11:20.048204 139653621614400 submission_runner.py:415] Time since start: 21550.35s, 	Step: 26432, 	{'train/accuracy': 0.6222070455551147, 'train/loss': 1.6842291355133057, 'validation/accuracy': 0.5636599659919739, 'validation/loss': 1.9642481803894043, 'validation/num_examples': 50000, 'test/accuracy': 0.43880000710487366, 'test/loss': 2.6279914379119873, 'test/num_examples': 10000, 'score': 20655.408534526825, 'total_duration': 21550.352798223495, 'accumulated_submission_time': 20655.408534526825, 'accumulated_eval_time': 893.2067959308624, 'accumulated_logging_time': 1.358436107635498}
I0502 06:11:20.068140 139413313009408 logging_writer.py:48] [26432] accumulated_eval_time=893.206796, accumulated_logging_time=1.358436, accumulated_submission_time=20655.408535, global_step=26432, preemption_count=0, score=20655.408535, test/accuracy=0.438800, test/loss=2.627991, test/num_examples=10000, total_duration=21550.352798, train/accuracy=0.622207, train/loss=1.684229, validation/accuracy=0.563660, validation/loss=1.964248, validation/num_examples=50000
I0502 06:12:13.818321 139413162030848 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6787267923355103, loss=5.132561206817627
I0502 06:13:31.710478 139413313009408 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.879930853843689, loss=3.3247008323669434
I0502 06:14:49.591386 139413162030848 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.8613751530647278, loss=3.3786895275115967
I0502 06:16:07.531101 139413313009408 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.8746808767318726, loss=3.5117945671081543
I0502 06:17:25.405379 139413162030848 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.7257724404335022, loss=4.390375137329102
I0502 06:18:20.421340 139653621614400 spec.py:298] Evaluating on the training split.
I0502 06:18:28.283891 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 06:18:37.145639 139653621614400 spec.py:326] Evaluating on the test split.
I0502 06:18:38.823286 139653621614400 submission_runner.py:415] Time since start: 21989.13s, 	Step: 26972, 	{'train/accuracy': 0.6056445240974426, 'train/loss': 1.7544585466384888, 'validation/accuracy': 0.5628399848937988, 'validation/loss': 1.9719257354736328, 'validation/num_examples': 50000, 'test/accuracy': 0.44450002908706665, 'test/loss': 2.6022119522094727, 'test/num_examples': 10000, 'score': 21075.74286007881, 'total_duration': 21989.127904891968, 'accumulated_submission_time': 21075.74286007881, 'accumulated_eval_time': 911.608695268631, 'accumulated_logging_time': 1.389988660812378}
I0502 06:18:38.836800 139413313009408 logging_writer.py:48] [26972] accumulated_eval_time=911.608695, accumulated_logging_time=1.389989, accumulated_submission_time=21075.742860, global_step=26972, preemption_count=0, score=21075.742860, test/accuracy=0.444500, test/loss=2.602212, test/num_examples=10000, total_duration=21989.127905, train/accuracy=0.605645, train/loss=1.754459, validation/accuracy=0.562840, validation/loss=1.971926, validation/num_examples=50000
I0502 06:19:01.442003 139413162030848 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.880574643611908, loss=3.3920044898986816
I0502 06:20:19.307062 139413313009408 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.8355194926261902, loss=3.296076774597168
I0502 06:21:37.226006 139413162030848 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.7721424698829651, loss=3.9556243419647217
I0502 06:22:55.171528 139413313009408 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.8946025967597961, loss=3.3221089839935303
I0502 06:24:13.044411 139413162030848 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.8414329886436462, loss=3.201188564300537
I0502 06:25:30.950090 139413313009408 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.9840294718742371, loss=3.372446298599243
I0502 06:25:39.221784 139653621614400 spec.py:298] Evaluating on the training split.
I0502 06:25:47.096124 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 06:25:55.922292 139653621614400 spec.py:326] Evaluating on the test split.
I0502 06:25:57.595109 139653621614400 submission_runner.py:415] Time since start: 22427.90s, 	Step: 27512, 	{'train/accuracy': 0.6108788847923279, 'train/loss': 1.8101662397384644, 'validation/accuracy': 0.5648599863052368, 'validation/loss': 2.0145692825317383, 'validation/num_examples': 50000, 'test/accuracy': 0.4488000273704529, 'test/loss': 2.6260673999786377, 'test/num_examples': 10000, 'score': 21496.107757806778, 'total_duration': 22427.899676799774, 'accumulated_submission_time': 21496.107757806778, 'accumulated_eval_time': 929.9819240570068, 'accumulated_logging_time': 1.416158676147461}
I0502 06:25:57.614671 139413162030848 logging_writer.py:48] [27512] accumulated_eval_time=929.981924, accumulated_logging_time=1.416159, accumulated_submission_time=21496.107758, global_step=27512, preemption_count=0, score=21496.107758, test/accuracy=0.448800, test/loss=2.626067, test/num_examples=10000, total_duration=22427.899677, train/accuracy=0.610879, train/loss=1.810166, validation/accuracy=0.564860, validation/loss=2.014569, validation/num_examples=50000
I0502 06:27:06.933044 139413313009408 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8948984742164612, loss=3.5084750652313232
I0502 06:28:24.839736 139413162030848 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.9121428728103638, loss=3.425652027130127
I0502 06:29:42.748735 139413313009408 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.7661792039871216, loss=5.55532169342041
I0502 06:31:00.649824 139413162030848 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.852914035320282, loss=3.3456125259399414
I0502 06:32:17.415152 139653621614400 spec.py:298] Evaluating on the training split.
I0502 06:32:25.253845 139653621614400 spec.py:310] Evaluating on the validation split.
I0502 06:32:34.170842 139653621614400 spec.py:326] Evaluating on the test split.
I0502 06:32:35.822814 139653621614400 submission_runner.py:415] Time since start: 22826.13s, 	Step: 28000, 	{'train/accuracy': 0.6216210722923279, 'train/loss': 1.6432124376296997, 'validation/accuracy': 0.5716800093650818, 'validation/loss': 1.8852722644805908, 'validation/num_examples': 50000, 'test/accuracy': 0.45280003547668457, 'test/loss': 2.5376338958740234, 'test/num_examples': 10000, 'score': 21875.88953113556, 'total_duration': 22826.12738919258, 'accumulated_submission_time': 21875.88953113556, 'accumulated_eval_time': 948.389502286911, 'accumulated_logging_time': 1.4476940631866455}
I0502 06:32:35.843887 139413313009408 logging_writer.py:48] [28000] accumulated_eval_time=948.389502, accumulated_logging_time=1.447694, accumulated_submission_time=21875.889531, global_step=28000, preemption_count=0, score=21875.889531, test/accuracy=0.452800, test/loss=2.537634, test/num_examples=10000, total_duration=22826.127389, train/accuracy=0.621621, train/loss=1.643212, validation/accuracy=0.571680, validation/loss=1.885272, validation/num_examples=50000
I0502 06:32:35.874349 139413162030848 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=21875.889531
I0502 06:32:36.293569 139653621614400 checkpoints.py:356] Saving checkpoint at step: 28000
I0502 06:32:37.240298 139653621614400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_sam/imagenet_vit_jax/trial_1/checkpoint_28000
I0502 06:32:37.262589 139653621614400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_sam/imagenet_vit_jax/trial_1/checkpoint_28000.
I0502 06:32:38.581260 139653621614400 submission_runner.py:578] Tuning trial 1/1
I0502 06:32:38.581534 139653621614400 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0502 06:32:38.593724 139653621614400 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009374999790452421, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 64.87282133102417, 'total_duration': 114.96912932395935, 'accumulated_submission_time': 64.87282133102417, 'accumulated_eval_time': 50.09613847732544, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (504, {'train/accuracy': 0.011660155840218067, 'train/loss': 6.5327653884887695, 'validation/accuracy': 0.011260000057518482, 'validation/loss': 6.544586658477783, 'validation/num_examples': 50000, 'test/accuracy': 0.010100000537931919, 'test/loss': 6.571342468261719, 'test/num_examples': 10000, 'score': 485.30918073654175, 'total_duration': 550.6106333732605, 'accumulated_submission_time': 485.30918073654175, 'accumulated_eval_time': 65.26620078086853, 'accumulated_logging_time': 0.027448177337646484, 'global_step': 504, 'preemption_count': 0}), (1046, {'train/accuracy': 0.027480468153953552, 'train/loss': 6.1062774658203125, 'validation/accuracy': 0.025539999827742577, 'validation/loss': 6.123623371124268, 'validation/num_examples': 50000, 'test/accuracy': 0.022200001403689384, 'test/loss': 6.208202838897705, 'test/num_examples': 10000, 'score': 906.0267350673676, 'total_duration': 986.3767714500427, 'accumulated_submission_time': 906.0267350673676, 'accumulated_eval_time': 80.27742528915405, 'accumulated_logging_time': 0.05632376670837402, 'global_step': 1046, 'preemption_count': 0}), (1587, {'train/accuracy': 0.04564452916383743, 'train/loss': 5.783807277679443, 'validation/accuracy': 0.04343999922275543, 'validation/loss': 5.8202223777771, 'validation/num_examples': 50000, 'test/accuracy': 0.03460000082850456, 'test/loss': 5.942713737487793, 'test/num_examples': 10000, 'score': 1326.438285112381, 'total_duration': 1421.836966753006, 'accumulated_submission_time': 1326.438285112381, 'accumulated_eval_time': 95.27819895744324, 'accumulated_logging_time': 0.09591507911682129, 'global_step': 1587, 'preemption_count': 0}), (2128, {'train/accuracy': 0.05667968466877937, 'train/loss': 5.542654991149902, 'validation/accuracy': 0.05494000017642975, 'validation/loss': 5.574263572692871, 'validation/num_examples': 50000, 'test/accuracy': 0.045100003480911255, 'test/loss': 5.740973472595215, 'test/num_examples': 10000, 'score': 1746.7724061012268, 'total_duration': 1857.2349350452423, 'accumulated_submission_time': 1746.7724061012268, 'accumulated_eval_time': 110.30893468856812, 'accumulated_logging_time': 0.1208643913269043, 'global_step': 2128, 'preemption_count': 0}), (2669, {'train/accuracy': 0.07832030951976776, 'train/loss': 5.328441143035889, 'validation/accuracy': 0.07174000144004822, 'validation/loss': 5.37614107131958, 'validation/num_examples': 50000, 'test/accuracy': 0.055900003761053085, 'test/loss': 5.57596492767334, 'test/num_examples': 10000, 'score': 2167.3390300273895, 'total_duration': 2292.9755618572235, 'accumulated_submission_time': 2167.3390300273895, 'accumulated_eval_time': 125.45103335380554, 'accumulated_logging_time': 0.14440584182739258, 'global_step': 2669, 'preemption_count': 0}), (3210, {'train/accuracy': 0.10332030802965164, 'train/loss': 5.04941987991333, 'validation/accuracy': 0.09637999534606934, 'validation/loss': 5.102444171905518, 'validation/num_examples': 50000, 'test/accuracy': 0.07440000027418137, 'test/loss': 5.3394083976745605, 'test/num_examples': 10000, 'score': 2588.0098960399628, 'total_duration': 2728.6653208732605, 'accumulated_submission_time': 2588.0098960399628, 'accumulated_eval_time': 140.43897581100464, 'accumulated_logging_time': 0.16705918312072754, 'global_step': 3210, 'preemption_count': 0}), (3750, {'train/accuracy': 0.1263476461172104, 'train/loss': 4.845317840576172, 'validation/accuracy': 0.11713999509811401, 'validation/loss': 4.910996437072754, 'validation/num_examples': 50000, 'test/accuracy': 0.08580000698566437, 'test/loss': 5.191555500030518, 'test/num_examples': 10000, 'score': 3008.0100514888763, 'total_duration': 3163.705046892166, 'accumulated_submission_time': 3008.0100514888763, 'accumulated_eval_time': 155.44584369659424, 'accumulated_logging_time': 0.19133615493774414, 'global_step': 3750, 'preemption_count': 0}), (4290, {'train/accuracy': 0.15830077230930328, 'train/loss': 4.5480499267578125, 'validation/accuracy': 0.14611999690532684, 'validation/loss': 4.636673450469971, 'validation/num_examples': 50000, 'test/accuracy': 0.1120000034570694, 'test/loss': 4.947347164154053, 'test/num_examples': 10000, 'score': 3428.006046295166, 'total_duration': 3598.7280633449554, 'accumulated_submission_time': 3428.006046295166, 'accumulated_eval_time': 170.43685603141785, 'accumulated_logging_time': 0.21928739547729492, 'global_step': 4290, 'preemption_count': 0}), (4830, {'train/accuracy': 0.18183593451976776, 'train/loss': 4.369006156921387, 'validation/accuracy': 0.16885998845100403, 'validation/loss': 4.4567108154296875, 'validation/num_examples': 50000, 'test/accuracy': 0.1282000094652176, 'test/loss': 4.785923004150391, 'test/num_examples': 10000, 'score': 3848.07204079628, 'total_duration': 4034.0085546970367, 'accumulated_submission_time': 3848.07204079628, 'accumulated_eval_time': 185.6128535270691, 'accumulated_logging_time': 0.24971699714660645, 'global_step': 4830, 'preemption_count': 0}), (5370, {'train/accuracy': 0.21193358302116394, 'train/loss': 4.104508399963379, 'validation/accuracy': 0.1959799975156784, 'validation/loss': 4.183114051818848, 'validation/num_examples': 50000, 'test/accuracy': 0.14750000834465027, 'test/loss': 4.566035270690918, 'test/num_examples': 10000, 'score': 4268.315876483917, 'total_duration': 4469.434617996216, 'accumulated_submission_time': 4268.315876483917, 'accumulated_eval_time': 200.76121759414673, 'accumulated_logging_time': 0.27559852600097656, 'global_step': 5370, 'preemption_count': 0}), (5910, {'train/accuracy': 0.2245507836341858, 'train/loss': 4.051093101501465, 'validation/accuracy': 0.2065799981355667, 'validation/loss': 4.145547866821289, 'validation/num_examples': 50000, 'test/accuracy': 0.16140000522136688, 'test/loss': 4.552667617797852, 'test/num_examples': 10000, 'score': 4688.544184207916, 'total_duration': 4904.872576951981, 'accumulated_submission_time': 4688.544184207916, 'accumulated_eval_time': 215.93876934051514, 'accumulated_logging_time': 0.29979920387268066, 'global_step': 5910, 'preemption_count': 0}), (6450, {'train/accuracy': 0.26005858182907104, 'train/loss': 3.7926025390625, 'validation/accuracy': 0.24053999781608582, 'validation/loss': 3.9104056358337402, 'validation/num_examples': 50000, 'test/accuracy': 0.18390001356601715, 'test/loss': 4.3344316482543945, 'test/num_examples': 10000, 'score': 5108.739726305008, 'total_duration': 5340.275751113892, 'accumulated_submission_time': 5108.739726305008, 'accumulated_eval_time': 231.11277103424072, 'accumulated_logging_time': 0.32534122467041016, 'global_step': 6450, 'preemption_count': 0}), (6990, {'train/accuracy': 0.2891796827316284, 'train/loss': 3.6054911613464355, 'validation/accuracy': 0.25165998935699463, 'validation/loss': 3.809664487838745, 'validation/num_examples': 50000, 'test/accuracy': 0.1972000151872635, 'test/loss': 4.233774185180664, 'test/num_examples': 10000, 'score': 5528.8206968307495, 'total_duration': 5775.590601205826, 'accumulated_submission_time': 5528.8206968307495, 'accumulated_eval_time': 246.31018090248108, 'accumulated_logging_time': 0.35364270210266113, 'global_step': 6990, 'preemption_count': 0}), (7530, {'train/accuracy': 0.30537107586860657, 'train/loss': 3.45223069190979, 'validation/accuracy': 0.27715998888015747, 'validation/loss': 3.597196578979492, 'validation/num_examples': 50000, 'test/accuracy': 0.21390001475811005, 'test/loss': 4.073435306549072, 'test/num_examples': 10000, 'score': 5948.952146053314, 'total_duration': 6210.95846414566, 'accumulated_submission_time': 5948.952146053314, 'accumulated_eval_time': 261.5118029117584, 'accumulated_logging_time': 0.38050031661987305, 'global_step': 7530, 'preemption_count': 0}), (8070, {'train/accuracy': 0.3279492259025574, 'train/loss': 3.2899789810180664, 'validation/accuracy': 0.3025200068950653, 'validation/loss': 3.419285774230957, 'validation/num_examples': 50000, 'test/accuracy': 0.2321000099182129, 'test/loss': 3.912594795227051, 'test/num_examples': 10000, 'score': 6368.98800611496, 'total_duration': 6646.3113803863525, 'accumulated_submission_time': 6368.98800611496, 'accumulated_eval_time': 276.79468607902527, 'accumulated_logging_time': 0.40654850006103516, 'global_step': 8070, 'preemption_count': 0}), (8610, {'train/accuracy': 0.34619140625, 'train/loss': 3.1740331649780273, 'validation/accuracy': 0.3144199848175049, 'validation/loss': 3.350982189178467, 'validation/num_examples': 50000, 'test/accuracy': 0.24070000648498535, 'test/loss': 3.8787009716033936, 'test/num_examples': 10000, 'score': 6789.22288942337, 'total_duration': 7081.8514585494995, 'accumulated_submission_time': 6789.22288942337, 'accumulated_eval_time': 292.06752943992615, 'accumulated_logging_time': 0.4307565689086914, 'global_step': 8610, 'preemption_count': 0}), (9150, {'train/accuracy': 0.34925779700279236, 'train/loss': 3.171983480453491, 'validation/accuracy': 0.3237600028514862, 'validation/loss': 3.312438726425171, 'validation/num_examples': 50000, 'test/accuracy': 0.25370001792907715, 'test/loss': 3.825394630432129, 'test/num_examples': 10000, 'score': 7209.303078889847, 'total_duration': 7517.344086885452, 'accumulated_submission_time': 7209.303078889847, 'accumulated_eval_time': 307.44526982307434, 'accumulated_logging_time': 0.45702695846557617, 'global_step': 9150, 'preemption_count': 0}), (9690, {'train/accuracy': 0.38755857944488525, 'train/loss': 2.901573419570923, 'validation/accuracy': 0.34887999296188354, 'validation/loss': 3.1177330017089844, 'validation/num_examples': 50000, 'test/accuracy': 0.262800008058548, 'test/loss': 3.6641533374786377, 'test/num_examples': 10000, 'score': 7629.388253211975, 'total_duration': 7953.9159553050995, 'accumulated_submission_time': 7629.388253211975, 'accumulated_eval_time': 323.89712047576904, 'accumulated_logging_time': 0.48378610610961914, 'global_step': 9690, 'preemption_count': 0}), (10230, {'train/accuracy': 0.38972654938697815, 'train/loss': 2.951045513153076, 'validation/accuracy': 0.3606799840927124, 'validation/loss': 3.100334882736206, 'validation/num_examples': 50000, 'test/accuracy': 0.2802000045776367, 'test/loss': 3.627120018005371, 'test/num_examples': 10000, 'score': 8049.498726129532, 'total_duration': 8391.772688388824, 'accumulated_submission_time': 8049.498726129532, 'accumulated_eval_time': 341.6133997440338, 'accumulated_logging_time': 0.5057449340820312, 'global_step': 10230, 'preemption_count': 0}), (10770, {'train/accuracy': 0.3985937535762787, 'train/loss': 2.8954787254333496, 'validation/accuracy': 0.3694799840450287, 'validation/loss': 3.0494508743286133, 'validation/num_examples': 50000, 'test/accuracy': 0.2851000130176544, 'test/loss': 3.5948896408081055, 'test/num_examples': 10000, 'score': 8469.736020088196, 'total_duration': 8830.48053431511, 'accumulated_submission_time': 8469.736020088196, 'accumulated_eval_time': 360.0509202480316, 'accumulated_logging_time': 0.5309243202209473, 'global_step': 10770, 'preemption_count': 0}), (11311, {'train/accuracy': 0.4358789026737213, 'train/loss': 2.6131887435913086, 'validation/accuracy': 0.398639976978302, 'validation/loss': 2.821199893951416, 'validation/num_examples': 50000, 'test/accuracy': 0.3027999997138977, 'test/loss': 3.40556263923645, 'test/num_examples': 10000, 'score': 8890.406812906265, 'total_duration': 9269.823364019394, 'accumulated_submission_time': 8890.406812906265, 'accumulated_eval_time': 378.6842978000641, 'accumulated_logging_time': 0.5615575313568115, 'global_step': 11311, 'preemption_count': 0}), (11851, {'train/accuracy': 0.45068359375, 'train/loss': 2.561367988586426, 'validation/accuracy': 0.41395998001098633, 'validation/loss': 2.7378463745117188, 'validation/num_examples': 50000, 'test/accuracy': 0.32510000467300415, 'test/loss': 3.3062238693237305, 'test/num_examples': 10000, 'score': 9310.431515932083, 'total_duration': 9708.868353128433, 'accumulated_submission_time': 9310.431515932083, 'accumulated_eval_time': 397.6632468700409, 'accumulated_logging_time': 0.5934393405914307, 'global_step': 11851, 'preemption_count': 0}), (12391, {'train/accuracy': 0.4526757597923279, 'train/loss': 2.592681884765625, 'validation/accuracy': 0.41609999537467957, 'validation/loss': 2.766192674636841, 'validation/num_examples': 50000, 'test/accuracy': 0.3246000111103058, 'test/loss': 3.3422627449035645, 'test/num_examples': 10000, 'score': 9730.551590442657, 'total_duration': 10147.618627548218, 'accumulated_submission_time': 9730.551590442657, 'accumulated_eval_time': 416.25614857673645, 'accumulated_logging_time': 0.6226837635040283, 'global_step': 12391, 'preemption_count': 0}), (12931, {'train/accuracy': 0.46384763717651367, 'train/loss': 2.4832041263580322, 'validation/accuracy': 0.42447999119758606, 'validation/loss': 2.670438051223755, 'validation/num_examples': 50000, 'test/accuracy': 0.33320000767707825, 'test/loss': 3.2545461654663086, 'test/num_examples': 10000, 'score': 10150.551031827927, 'total_duration': 10585.99878501892, 'accumulated_submission_time': 10150.551031827927, 'accumulated_eval_time': 434.5951066017151, 'accumulated_logging_time': 0.6565165519714355, 'global_step': 12931, 'preemption_count': 0}), (13471, {'train/accuracy': 0.47746092081069946, 'train/loss': 2.392457962036133, 'validation/accuracy': 0.44849997758865356, 'validation/loss': 2.55674409866333, 'validation/num_examples': 50000, 'test/accuracy': 0.34880000352859497, 'test/loss': 3.1676065921783447, 'test/num_examples': 10000, 'score': 10570.678523778915, 'total_duration': 11024.581651210785, 'accumulated_submission_time': 10570.678523778915, 'accumulated_eval_time': 453.0160663127899, 'accumulated_logging_time': 0.6837038993835449, 'global_step': 13471, 'preemption_count': 0}), (14011, {'train/accuracy': 0.4989062249660492, 'train/loss': 2.3269894123077393, 'validation/accuracy': 0.4503999948501587, 'validation/loss': 2.5686025619506836, 'validation/num_examples': 50000, 'test/accuracy': 0.35340002179145813, 'test/loss': 3.1562888622283936, 'test/num_examples': 10000, 'score': 10990.765407323837, 'total_duration': 11462.932204246521, 'accumulated_submission_time': 10990.765407323837, 'accumulated_eval_time': 471.2417540550232, 'accumulated_logging_time': 0.7145755290985107, 'global_step': 14011, 'preemption_count': 0}), (14551, {'train/accuracy': 0.4942968785762787, 'train/loss': 2.3598196506500244, 'validation/accuracy': 0.4604399800300598, 'validation/loss': 2.5231099128723145, 'validation/num_examples': 50000, 'test/accuracy': 0.3604000210762024, 'test/loss': 3.1187779903411865, 'test/num_examples': 10000, 'score': 11410.786851406097, 'total_duration': 11901.31103682518, 'accumulated_submission_time': 11410.786851406097, 'accumulated_eval_time': 489.567001581192, 'accumulated_logging_time': 0.739464521408081, 'global_step': 14551, 'preemption_count': 0}), (15091, {'train/accuracy': 0.4972851574420929, 'train/loss': 2.3453261852264404, 'validation/accuracy': 0.46045997738838196, 'validation/loss': 2.531139612197876, 'validation/num_examples': 50000, 'test/accuracy': 0.35350000858306885, 'test/loss': 3.138578414916992, 'test/num_examples': 10000, 'score': 11830.854567289352, 'total_duration': 12339.778903484344, 'accumulated_submission_time': 11830.854567289352, 'accumulated_eval_time': 507.938622713089, 'accumulated_logging_time': 0.7608397006988525, 'global_step': 15091, 'preemption_count': 0}), (15631, {'train/accuracy': 0.5125585794448853, 'train/loss': 2.2513651847839355, 'validation/accuracy': 0.47151997685432434, 'validation/loss': 2.453138589859009, 'validation/num_examples': 50000, 'test/accuracy': 0.3628000319004059, 'test/loss': 3.058053493499756, 'test/num_examples': 10000, 'score': 12250.961234331131, 'total_duration': 12778.17575454712, 'accumulated_submission_time': 12250.961234331131, 'accumulated_eval_time': 526.1884808540344, 'accumulated_logging_time': 0.7940304279327393, 'global_step': 15631, 'preemption_count': 0}), (16171, {'train/accuracy': 0.5072460770606995, 'train/loss': 2.2633824348449707, 'validation/accuracy': 0.4778999984264374, 'validation/loss': 2.4165658950805664, 'validation/num_examples': 50000, 'test/accuracy': 0.36730000376701355, 'test/loss': 3.025869369506836, 'test/num_examples': 10000, 'score': 12671.038233280182, 'total_duration': 13216.517429351807, 'accumulated_submission_time': 12671.038233280182, 'accumulated_eval_time': 544.4160325527191, 'accumulated_logging_time': 0.823927640914917, 'global_step': 16171, 'preemption_count': 0}), (16711, {'train/accuracy': 0.5418164134025574, 'train/loss': 2.1171469688415527, 'validation/accuracy': 0.48625999689102173, 'validation/loss': 2.3979251384735107, 'validation/num_examples': 50000, 'test/accuracy': 0.38100001215934753, 'test/loss': 3.002438545227051, 'test/num_examples': 10000, 'score': 13091.20087981224, 'total_duration': 13655.041302204132, 'accumulated_submission_time': 13091.20087981224, 'accumulated_eval_time': 562.7438662052155, 'accumulated_logging_time': 0.8501362800598145, 'global_step': 16711, 'preemption_count': 0}), (17251, {'train/accuracy': 0.5215234160423279, 'train/loss': 2.279108762741089, 'validation/accuracy': 0.48429998755455017, 'validation/loss': 2.4717276096343994, 'validation/num_examples': 50000, 'test/accuracy': 0.37060001492500305, 'test/loss': 3.0945940017700195, 'test/num_examples': 10000, 'score': 13511.388444900513, 'total_duration': 14093.713958740234, 'accumulated_submission_time': 13511.388444900513, 'accumulated_eval_time': 581.1972789764404, 'accumulated_logging_time': 0.8745758533477783, 'global_step': 17251, 'preemption_count': 0}), (17791, {'train/accuracy': 0.5377734303474426, 'train/loss': 2.099475860595703, 'validation/accuracy': 0.4939599931240082, 'validation/loss': 2.2994842529296875, 'validation/num_examples': 50000, 'test/accuracy': 0.38680002093315125, 'test/loss': 2.917004108428955, 'test/num_examples': 10000, 'score': 13931.49606513977, 'total_duration': 14532.127606868744, 'accumulated_submission_time': 13931.49606513977, 'accumulated_eval_time': 599.4717071056366, 'accumulated_logging_time': 0.8989613056182861, 'global_step': 17791, 'preemption_count': 0}), (18331, {'train/accuracy': 0.5472460985183716, 'train/loss': 2.078303813934326, 'validation/accuracy': 0.5027399659156799, 'validation/loss': 2.2991981506347656, 'validation/num_examples': 50000, 'test/accuracy': 0.3930000066757202, 'test/loss': 2.931837558746338, 'test/num_examples': 10000, 'score': 14351.677814722061, 'total_duration': 14970.647427082062, 'accumulated_submission_time': 14351.677814722061, 'accumulated_eval_time': 617.7734076976776, 'accumulated_logging_time': 0.9278745651245117, 'global_step': 18331, 'preemption_count': 0}), (18871, {'train/accuracy': 0.5433007478713989, 'train/loss': 2.0737359523773193, 'validation/accuracy': 0.5039600133895874, 'validation/loss': 2.265446901321411, 'validation/num_examples': 50000, 'test/accuracy': 0.39330002665519714, 'test/loss': 2.8816025257110596, 'test/num_examples': 10000, 'score': 14771.731041908264, 'total_duration': 15408.993498325348, 'accumulated_submission_time': 14771.731041908264, 'accumulated_eval_time': 636.0286748409271, 'accumulated_logging_time': 0.9581594467163086, 'global_step': 18871, 'preemption_count': 0}), (19411, {'train/accuracy': 0.5798632502555847, 'train/loss': 1.8920432329177856, 'validation/accuracy': 0.5148000121116638, 'validation/loss': 2.2058675289154053, 'validation/num_examples': 50000, 'test/accuracy': 0.40220001339912415, 'test/loss': 2.8267576694488525, 'test/num_examples': 10000, 'score': 15191.817367315292, 'total_duration': 15847.416659832, 'accumulated_submission_time': 15191.817367315292, 'accumulated_eval_time': 654.3283517360687, 'accumulated_logging_time': 0.9880571365356445, 'global_step': 19411, 'preemption_count': 0}), (19952, {'train/accuracy': 0.5670703053474426, 'train/loss': 1.969274878501892, 'validation/accuracy': 0.5212000012397766, 'validation/loss': 2.1840291023254395, 'validation/num_examples': 50000, 'test/accuracy': 0.406000018119812, 'test/loss': 2.8097076416015625, 'test/num_examples': 10000, 'score': 15612.458054304123, 'total_duration': 16286.423792123795, 'accumulated_submission_time': 15612.458054304123, 'accumulated_eval_time': 672.6657116413116, 'accumulated_logging_time': 1.0098767280578613, 'global_step': 19952, 'preemption_count': 0}), (20492, {'train/accuracy': 0.5654882788658142, 'train/loss': 1.952566385269165, 'validation/accuracy': 0.5236600041389465, 'validation/loss': 2.1569571495056152, 'validation/num_examples': 50000, 'test/accuracy': 0.4068000316619873, 'test/loss': 2.8063700199127197, 'test/num_examples': 10000, 'score': 16032.759641885757, 'total_duration': 16725.228646993637, 'accumulated_submission_time': 16032.759641885757, 'accumulated_eval_time': 691.1363332271576, 'accumulated_logging_time': 1.0351815223693848, 'global_step': 20492, 'preemption_count': 0}), (21032, {'train/accuracy': 0.5785155892372131, 'train/loss': 1.9298574924468994, 'validation/accuracy': 0.526960015296936, 'validation/loss': 2.1626498699188232, 'validation/num_examples': 50000, 'test/accuracy': 0.41850000619888306, 'test/loss': 2.795966148376465, 'test/num_examples': 10000, 'score': 16452.812147140503, 'total_duration': 17163.628099679947, 'accumulated_submission_time': 16452.812147140503, 'accumulated_eval_time': 709.4453580379486, 'accumulated_logging_time': 1.065666675567627, 'global_step': 21032, 'preemption_count': 0}), (21572, {'train/accuracy': 0.5745507478713989, 'train/loss': 1.883386492729187, 'validation/accuracy': 0.5284199714660645, 'validation/loss': 2.106630802154541, 'validation/num_examples': 50000, 'test/accuracy': 0.41350001096725464, 'test/loss': 2.749572277069092, 'test/num_examples': 10000, 'score': 16872.966319322586, 'total_duration': 17602.21125483513, 'accumulated_submission_time': 16872.966319322586, 'accumulated_eval_time': 727.8376910686493, 'accumulated_logging_time': 1.0948185920715332, 'global_step': 21572, 'preemption_count': 0}), (22112, {'train/accuracy': 0.598339855670929, 'train/loss': 1.811564564704895, 'validation/accuracy': 0.5315799713134766, 'validation/loss': 2.1263439655303955, 'validation/num_examples': 50000, 'test/accuracy': 0.42170003056526184, 'test/loss': 2.7670469284057617, 'test/num_examples': 10000, 'score': 17293.259194374084, 'total_duration': 18041.003535747528, 'accumulated_submission_time': 17293.259194374084, 'accumulated_eval_time': 746.304297208786, 'accumulated_logging_time': 1.1203477382659912, 'global_step': 22112, 'preemption_count': 0}), (22652, {'train/accuracy': 0.5834375023841858, 'train/loss': 1.857932448387146, 'validation/accuracy': 0.5338199734687805, 'validation/loss': 2.08170485496521, 'validation/num_examples': 50000, 'test/accuracy': 0.4166000187397003, 'test/loss': 2.7438559532165527, 'test/num_examples': 10000, 'score': 17713.444076299667, 'total_duration': 18479.487469911575, 'accumulated_submission_time': 17713.444076299667, 'accumulated_eval_time': 764.5658068656921, 'accumulated_logging_time': 1.150357961654663, 'global_step': 22652, 'preemption_count': 0}), (23192, {'train/accuracy': 0.5718554854393005, 'train/loss': 1.972922682762146, 'validation/accuracy': 0.5333600044250488, 'validation/loss': 2.1591458320617676, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.7841849327087402, 'test/num_examples': 10000, 'score': 18133.69573187828, 'total_duration': 18918.055296182632, 'accumulated_submission_time': 18133.69573187828, 'accumulated_eval_time': 782.8416438102722, 'accumulated_logging_time': 1.1833250522613525, 'global_step': 23192, 'preemption_count': 0}), (23732, {'train/accuracy': 0.5986718535423279, 'train/loss': 1.8310506343841553, 'validation/accuracy': 0.5467999577522278, 'validation/loss': 2.0759449005126953, 'validation/num_examples': 50000, 'test/accuracy': 0.42660000920295715, 'test/loss': 2.725771427154541, 'test/num_examples': 10000, 'score': 18553.937541007996, 'total_duration': 19356.86517238617, 'accumulated_submission_time': 18553.937541007996, 'accumulated_eval_time': 801.3719823360443, 'accumulated_logging_time': 1.2136051654815674, 'global_step': 23732, 'preemption_count': 0}), (24272, {'train/accuracy': 0.5940819978713989, 'train/loss': 1.795839786529541, 'validation/accuracy': 0.5488399863243103, 'validation/loss': 2.013808012008667, 'validation/num_examples': 50000, 'test/accuracy': 0.43290001153945923, 'test/loss': 2.6545422077178955, 'test/num_examples': 10000, 'score': 18974.116726398468, 'total_duration': 19795.467153072357, 'accumulated_submission_time': 18974.116726398468, 'accumulated_eval_time': 819.7616374492645, 'accumulated_logging_time': 1.2393014430999756, 'global_step': 24272, 'preemption_count': 0}), (24812, {'train/accuracy': 0.5988085865974426, 'train/loss': 1.8303464651107788, 'validation/accuracy': 0.5506399869918823, 'validation/loss': 2.0595686435699463, 'validation/num_examples': 50000, 'test/accuracy': 0.4376000165939331, 'test/loss': 2.687897205352783, 'test/num_examples': 10000, 'score': 19394.49334168434, 'total_duration': 20234.195056200027, 'accumulated_submission_time': 19394.49334168434, 'accumulated_eval_time': 838.0794067382812, 'accumulated_logging_time': 1.265521764755249, 'global_step': 24812, 'preemption_count': 0}), (25352, {'train/accuracy': 0.6066796779632568, 'train/loss': 1.765734076499939, 'validation/accuracy': 0.5554999709129333, 'validation/loss': 2.0010578632354736, 'validation/num_examples': 50000, 'test/accuracy': 0.43630000948905945, 'test/loss': 2.635207414627075, 'test/num_examples': 10000, 'score': 19814.72307920456, 'total_duration': 20672.808485269547, 'accumulated_submission_time': 19814.72307920456, 'accumulated_eval_time': 856.4274027347565, 'accumulated_logging_time': 1.2939541339874268, 'global_step': 25352, 'preemption_count': 0}), (25892, {'train/accuracy': 0.5953124761581421, 'train/loss': 1.874042272567749, 'validation/accuracy': 0.5527799725532532, 'validation/loss': 2.0759801864624023, 'validation/num_examples': 50000, 'test/accuracy': 0.4417000114917755, 'test/loss': 2.700547933578491, 'test/num_examples': 10000, 'score': 20234.990947008133, 'total_duration': 21111.562367916107, 'accumulated_submission_time': 20234.990947008133, 'accumulated_eval_time': 874.8732771873474, 'accumulated_logging_time': 1.3265931606292725, 'global_step': 25892, 'preemption_count': 0}), (26432, {'train/accuracy': 0.6222070455551147, 'train/loss': 1.6842291355133057, 'validation/accuracy': 0.5636599659919739, 'validation/loss': 1.9642481803894043, 'validation/num_examples': 50000, 'test/accuracy': 0.43880000710487366, 'test/loss': 2.6279914379119873, 'test/num_examples': 10000, 'score': 20655.408534526825, 'total_duration': 21550.352798223495, 'accumulated_submission_time': 20655.408534526825, 'accumulated_eval_time': 893.2067959308624, 'accumulated_logging_time': 1.358436107635498, 'global_step': 26432, 'preemption_count': 0}), (26972, {'train/accuracy': 0.6056445240974426, 'train/loss': 1.7544585466384888, 'validation/accuracy': 0.5628399848937988, 'validation/loss': 1.9719257354736328, 'validation/num_examples': 50000, 'test/accuracy': 0.44450002908706665, 'test/loss': 2.6022119522094727, 'test/num_examples': 10000, 'score': 21075.74286007881, 'total_duration': 21989.127904891968, 'accumulated_submission_time': 21075.74286007881, 'accumulated_eval_time': 911.608695268631, 'accumulated_logging_time': 1.389988660812378, 'global_step': 26972, 'preemption_count': 0}), (27512, {'train/accuracy': 0.6108788847923279, 'train/loss': 1.8101662397384644, 'validation/accuracy': 0.5648599863052368, 'validation/loss': 2.0145692825317383, 'validation/num_examples': 50000, 'test/accuracy': 0.4488000273704529, 'test/loss': 2.6260673999786377, 'test/num_examples': 10000, 'score': 21496.107757806778, 'total_duration': 22427.899676799774, 'accumulated_submission_time': 21496.107757806778, 'accumulated_eval_time': 929.9819240570068, 'accumulated_logging_time': 1.416158676147461, 'global_step': 27512, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6216210722923279, 'train/loss': 1.6432124376296997, 'validation/accuracy': 0.5716800093650818, 'validation/loss': 1.8852722644805908, 'validation/num_examples': 50000, 'test/accuracy': 0.45280003547668457, 'test/loss': 2.5376338958740234, 'test/num_examples': 10000, 'score': 21875.88953113556, 'total_duration': 22826.12738919258, 'accumulated_submission_time': 21875.88953113556, 'accumulated_eval_time': 948.389502286911, 'accumulated_logging_time': 1.4476940631866455, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0502 06:32:38.593913 139653621614400 submission_runner.py:581] Timing: 21875.88953113556
I0502 06:32:38.593972 139653621614400 submission_runner.py:582] ====================
I0502 06:32:38.594145 139653621614400 submission_runner.py:645] Final imagenet_vit score: 21875.88953113556
