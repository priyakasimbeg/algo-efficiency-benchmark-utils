torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_b_pytorch/timing_momentum --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_pytorch_05-20-2023-07-45-08.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0520 07:45:32.362238 140568223172416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0520 07:45:32.362269 139706509236032 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0520 07:45:32.362295 140348155938624 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0520 07:45:32.363104 139948377577280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0520 07:45:32.363387 139811717031744 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0520 07:45:32.363437 140038126991168 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0520 07:45:32.363688 139849944258368 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0520 07:45:32.363923 139811717031744 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:45:32.363931 140336859240256 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0520 07:45:32.364069 139849944258368 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:45:32.364310 140336859240256 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:45:32.372948 140568223172416 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:45:32.372999 139706509236032 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:45:32.373076 140348155938624 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:45:32.373713 139948377577280 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:45:32.374082 140038126991168 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:45:33.583622 139849944258368 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_b_pytorch/timing_momentum/ogbg_pytorch.
W0520 07:45:33.703815 139811717031744 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:45:33.706034 139706509236032 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:45:33.706242 140336859240256 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:45:33.706393 140038126991168 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:45:33.706396 140568223172416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:45:33.706557 140348155938624 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:45:33.706667 139849944258368 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:45:33.709052 139948377577280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0520 07:45:33.712172 139849944258368 submission_runner.py:544] Using RNG seed 3100203796
I0520 07:45:33.713426 139849944258368 submission_runner.py:553] --- Tuning run 1/1 ---
I0520 07:45:33.713535 139849944258368 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_b_pytorch/timing_momentum/ogbg_pytorch/trial_1.
I0520 07:45:33.713738 139849944258368 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_b_pytorch/timing_momentum/ogbg_pytorch/trial_1/hparams.json.
I0520 07:45:33.714750 139849944258368 submission_runner.py:241] Initializing dataset.
I0520 07:45:33.714865 139849944258368 submission_runner.py:248] Initializing model.
I0520 07:45:37.746463 139849944258368 submission_runner.py:258] Initializing optimizer.
I0520 07:45:38.294354 139849944258368 submission_runner.py:265] Initializing metrics bundle.
I0520 07:45:38.294554 139849944258368 submission_runner.py:283] Initializing checkpoint and logger.
I0520 07:45:38.299019 139849944258368 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0520 07:45:38.299140 139849944258368 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0520 07:45:38.782294 139849944258368 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_b_pytorch/timing_momentum/ogbg_pytorch/trial_1/meta_data_0.json.
I0520 07:45:38.783332 139849944258368 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_b_pytorch/timing_momentum/ogbg_pytorch/trial_1/flags_0.json.
I0520 07:45:38.844835 139849944258368 submission_runner.py:319] Starting training loop.
I0520 07:45:39.092215 139849944258368 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0520 07:45:39.098227 139849944258368 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0520 07:45:39.245381 139849944258368 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0520 07:45:43.873937 139811419584256 logging_writer.py:48] [0] global_step=0, grad_norm=2.548619, loss=0.763996
I0520 07:45:43.884018 139849944258368 submission.py:139] 0) loss = 0.764, grad_norm = 2.549
I0520 07:45:43.884808 139849944258368 spec.py:298] Evaluating on the training split.
I0520 07:45:43.890458 139849944258368 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0520 07:45:43.894611 139849944258368 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0520 07:45:43.949176 139849944258368 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0520 07:46:41.962135 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 07:46:41.965437 139849944258368 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0520 07:46:41.969816 139849944258368 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0520 07:46:42.026127 139849944258368 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0520 07:47:27.849168 139849944258368 spec.py:326] Evaluating on the test split.
I0520 07:47:27.852582 139849944258368 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0520 07:47:27.856933 139849944258368 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0520 07:47:27.913016 139849944258368 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0520 07:48:14.768779 139849944258368 submission_runner.py:421] Time since start: 155.92s, 	Step: 1, 	{'train/accuracy': 0.5016493984187683, 'train/loss': 0.7639511287898042, 'train/mean_average_precision': 0.024553067411332084, 'validation/accuracy': 0.49915219285105955, 'validation/loss': 0.762280568755394, 'validation/mean_average_precision': 0.02670647641971499, 'validation/num_examples': 43793, 'test/accuracy': 0.49681682116789705, 'test/loss': 0.7624870008790333, 'test/mean_average_precision': 0.028973853583578506, 'test/num_examples': 43793, 'score': 5.0393102169036865, 'total_duration': 155.92417740821838, 'accumulated_submission_time': 5.0393102169036865, 'accumulated_eval_time': 150.88354659080505, 'accumulated_logging_time': 0}
I0520 07:48:14.786526 139798005745408 logging_writer.py:48] [1] accumulated_eval_time=150.883547, accumulated_logging_time=0, accumulated_submission_time=5.039310, global_step=1, preemption_count=0, score=5.039310, test/accuracy=0.496817, test/loss=0.762487, test/mean_average_precision=0.028974, test/num_examples=43793, total_duration=155.924177, train/accuracy=0.501649, train/loss=0.763951, train/mean_average_precision=0.024553, validation/accuracy=0.499152, validation/loss=0.762281, validation/mean_average_precision=0.026706, validation/num_examples=43793
I0520 07:48:15.072832 139849944258368 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:48:15.072802 140348155938624 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:48:15.072803 140568223172416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:48:15.072888 139706509236032 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:48:15.072886 140038126991168 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:48:15.072886 139948377577280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:48:15.072890 139811717031744 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:48:15.072888 140336859240256 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:48:15.105525 139798014138112 logging_writer.py:48] [1] global_step=1, grad_norm=2.538919, loss=0.763691
I0520 07:48:15.109637 139849944258368 submission.py:139] 1) loss = 0.764, grad_norm = 2.539
I0520 07:48:15.413937 139798005745408 logging_writer.py:48] [2] global_step=2, grad_norm=2.523166, loss=0.763305
I0520 07:48:15.417833 139849944258368 submission.py:139] 2) loss = 0.763, grad_norm = 2.523
I0520 07:48:15.706799 139798014138112 logging_writer.py:48] [3] global_step=3, grad_norm=2.510407, loss=0.759509
I0520 07:48:15.710931 139849944258368 submission.py:139] 3) loss = 0.760, grad_norm = 2.510
I0520 07:48:16.010071 139798005745408 logging_writer.py:48] [4] global_step=4, grad_norm=2.454273, loss=0.750953
I0520 07:48:16.014161 139849944258368 submission.py:139] 4) loss = 0.751, grad_norm = 2.454
I0520 07:48:16.315205 139798014138112 logging_writer.py:48] [5] global_step=5, grad_norm=2.333718, loss=0.739467
I0520 07:48:16.319261 139849944258368 submission.py:139] 5) loss = 0.739, grad_norm = 2.334
I0520 07:48:16.619689 139798005745408 logging_writer.py:48] [6] global_step=6, grad_norm=2.153310, loss=0.725221
I0520 07:48:16.623504 139849944258368 submission.py:139] 6) loss = 0.725, grad_norm = 2.153
I0520 07:48:16.924764 139798014138112 logging_writer.py:48] [7] global_step=7, grad_norm=1.983006, loss=0.707736
I0520 07:48:16.929144 139849944258368 submission.py:139] 7) loss = 0.708, grad_norm = 1.983
I0520 07:48:17.223756 139798005745408 logging_writer.py:48] [8] global_step=8, grad_norm=1.905373, loss=0.690546
I0520 07:48:17.227690 139849944258368 submission.py:139] 8) loss = 0.691, grad_norm = 1.905
I0520 07:48:17.529295 139798014138112 logging_writer.py:48] [9] global_step=9, grad_norm=1.787051, loss=0.669510
I0520 07:48:17.533148 139849944258368 submission.py:139] 9) loss = 0.670, grad_norm = 1.787
I0520 07:48:17.833409 139798005745408 logging_writer.py:48] [10] global_step=10, grad_norm=1.692213, loss=0.649905
I0520 07:48:17.837224 139849944258368 submission.py:139] 10) loss = 0.650, grad_norm = 1.692
I0520 07:48:18.137848 139798014138112 logging_writer.py:48] [11] global_step=11, grad_norm=1.458499, loss=0.630586
I0520 07:48:18.141815 139849944258368 submission.py:139] 11) loss = 0.631, grad_norm = 1.458
I0520 07:48:18.445538 139798005745408 logging_writer.py:48] [12] global_step=12, grad_norm=1.395308, loss=0.614313
I0520 07:48:18.449603 139849944258368 submission.py:139] 12) loss = 0.614, grad_norm = 1.395
I0520 07:48:18.755909 139798014138112 logging_writer.py:48] [13] global_step=13, grad_norm=1.332155, loss=0.597798
I0520 07:48:18.760243 139849944258368 submission.py:139] 13) loss = 0.598, grad_norm = 1.332
I0520 07:48:19.062742 139798005745408 logging_writer.py:48] [14] global_step=14, grad_norm=1.247645, loss=0.582783
I0520 07:48:19.066703 139849944258368 submission.py:139] 14) loss = 0.583, grad_norm = 1.248
I0520 07:48:19.365472 139798014138112 logging_writer.py:48] [15] global_step=15, grad_norm=1.113943, loss=0.568793
I0520 07:48:19.369508 139849944258368 submission.py:139] 15) loss = 0.569, grad_norm = 1.114
I0520 07:48:19.665211 139798005745408 logging_writer.py:48] [16] global_step=16, grad_norm=1.067772, loss=0.553903
I0520 07:48:19.669198 139849944258368 submission.py:139] 16) loss = 0.554, grad_norm = 1.068
I0520 07:48:19.966083 139798014138112 logging_writer.py:48] [17] global_step=17, grad_norm=0.980851, loss=0.542427
I0520 07:48:19.970244 139849944258368 submission.py:139] 17) loss = 0.542, grad_norm = 0.981
I0520 07:48:20.269652 139798005745408 logging_writer.py:48] [18] global_step=18, grad_norm=0.932557, loss=0.530470
I0520 07:48:20.273936 139849944258368 submission.py:139] 18) loss = 0.530, grad_norm = 0.933
I0520 07:48:20.580650 139798014138112 logging_writer.py:48] [19] global_step=19, grad_norm=0.873253, loss=0.519980
I0520 07:48:20.584957 139849944258368 submission.py:139] 19) loss = 0.520, grad_norm = 0.873
I0520 07:48:20.883846 139798005745408 logging_writer.py:48] [20] global_step=20, grad_norm=0.829086, loss=0.506900
I0520 07:48:20.887846 139849944258368 submission.py:139] 20) loss = 0.507, grad_norm = 0.829
I0520 07:48:21.190244 139798014138112 logging_writer.py:48] [21] global_step=21, grad_norm=0.805798, loss=0.492420
I0520 07:48:21.194515 139849944258368 submission.py:139] 21) loss = 0.492, grad_norm = 0.806
I0520 07:48:21.496403 139798005745408 logging_writer.py:48] [22] global_step=22, grad_norm=0.762295, loss=0.483419
I0520 07:48:21.500564 139849944258368 submission.py:139] 22) loss = 0.483, grad_norm = 0.762
I0520 07:48:21.801726 139798014138112 logging_writer.py:48] [23] global_step=23, grad_norm=0.710982, loss=0.470977
I0520 07:48:21.805692 139849944258368 submission.py:139] 23) loss = 0.471, grad_norm = 0.711
I0520 07:48:22.113228 139798005745408 logging_writer.py:48] [24] global_step=24, grad_norm=0.643877, loss=0.462025
I0520 07:48:22.117270 139849944258368 submission.py:139] 24) loss = 0.462, grad_norm = 0.644
I0520 07:48:22.420206 139798014138112 logging_writer.py:48] [25] global_step=25, grad_norm=0.616556, loss=0.451009
I0520 07:48:22.424537 139849944258368 submission.py:139] 25) loss = 0.451, grad_norm = 0.617
I0520 07:48:22.723438 139798005745408 logging_writer.py:48] [26] global_step=26, grad_norm=0.597171, loss=0.440649
I0520 07:48:22.727647 139849944258368 submission.py:139] 26) loss = 0.441, grad_norm = 0.597
I0520 07:48:23.032712 139798014138112 logging_writer.py:48] [27] global_step=27, grad_norm=0.565572, loss=0.431659
I0520 07:48:23.036763 139849944258368 submission.py:139] 27) loss = 0.432, grad_norm = 0.566
I0520 07:48:23.342432 139798005745408 logging_writer.py:48] [28] global_step=28, grad_norm=0.542814, loss=0.422804
I0520 07:48:23.346999 139849944258368 submission.py:139] 28) loss = 0.423, grad_norm = 0.543
I0520 07:48:23.653534 139798014138112 logging_writer.py:48] [29] global_step=29, grad_norm=0.525413, loss=0.414703
I0520 07:48:23.657436 139849944258368 submission.py:139] 29) loss = 0.415, grad_norm = 0.525
I0520 07:48:23.960761 139798005745408 logging_writer.py:48] [30] global_step=30, grad_norm=0.519047, loss=0.405995
I0520 07:48:23.964792 139849944258368 submission.py:139] 30) loss = 0.406, grad_norm = 0.519
I0520 07:48:24.270719 139798014138112 logging_writer.py:48] [31] global_step=31, grad_norm=0.510880, loss=0.396324
I0520 07:48:24.274859 139849944258368 submission.py:139] 31) loss = 0.396, grad_norm = 0.511
I0520 07:48:24.575144 139798005745408 logging_writer.py:48] [32] global_step=32, grad_norm=0.496410, loss=0.388039
I0520 07:48:24.579115 139849944258368 submission.py:139] 32) loss = 0.388, grad_norm = 0.496
I0520 07:48:24.874509 139798014138112 logging_writer.py:48] [33] global_step=33, grad_norm=0.469729, loss=0.378468
I0520 07:48:24.878459 139849944258368 submission.py:139] 33) loss = 0.378, grad_norm = 0.470
I0520 07:48:25.182029 139798005745408 logging_writer.py:48] [34] global_step=34, grad_norm=0.448012, loss=0.368130
I0520 07:48:25.185976 139849944258368 submission.py:139] 34) loss = 0.368, grad_norm = 0.448
I0520 07:48:25.490460 139798014138112 logging_writer.py:48] [35] global_step=35, grad_norm=0.421031, loss=0.361490
I0520 07:48:25.494518 139849944258368 submission.py:139] 35) loss = 0.361, grad_norm = 0.421
I0520 07:48:25.798144 139798005745408 logging_writer.py:48] [36] global_step=36, grad_norm=0.409456, loss=0.355721
I0520 07:48:25.802110 139849944258368 submission.py:139] 36) loss = 0.356, grad_norm = 0.409
I0520 07:48:26.109652 139798014138112 logging_writer.py:48] [37] global_step=37, grad_norm=0.395844, loss=0.343677
I0520 07:48:26.113671 139849944258368 submission.py:139] 37) loss = 0.344, grad_norm = 0.396
I0520 07:48:26.426774 139798005745408 logging_writer.py:48] [38] global_step=38, grad_norm=0.388161, loss=0.337337
I0520 07:48:26.430896 139849944258368 submission.py:139] 38) loss = 0.337, grad_norm = 0.388
I0520 07:48:26.733247 139798014138112 logging_writer.py:48] [39] global_step=39, grad_norm=0.379677, loss=0.327568
I0520 07:48:26.737338 139849944258368 submission.py:139] 39) loss = 0.328, grad_norm = 0.380
I0520 07:48:27.041060 139798005745408 logging_writer.py:48] [40] global_step=40, grad_norm=0.364650, loss=0.321525
I0520 07:48:27.045139 139849944258368 submission.py:139] 40) loss = 0.322, grad_norm = 0.365
I0520 07:48:27.348215 139798014138112 logging_writer.py:48] [41] global_step=41, grad_norm=0.355445, loss=0.312778
I0520 07:48:27.352319 139849944258368 submission.py:139] 41) loss = 0.313, grad_norm = 0.355
I0520 07:48:27.657630 139798005745408 logging_writer.py:48] [42] global_step=42, grad_norm=0.348308, loss=0.306419
I0520 07:48:27.661613 139849944258368 submission.py:139] 42) loss = 0.306, grad_norm = 0.348
I0520 07:48:27.963094 139798014138112 logging_writer.py:48] [43] global_step=43, grad_norm=0.338987, loss=0.298979
I0520 07:48:27.967295 139849944258368 submission.py:139] 43) loss = 0.299, grad_norm = 0.339
I0520 07:48:28.270670 139798005745408 logging_writer.py:48] [44] global_step=44, grad_norm=0.332940, loss=0.288875
I0520 07:48:28.274743 139849944258368 submission.py:139] 44) loss = 0.289, grad_norm = 0.333
I0520 07:48:28.575227 139798014138112 logging_writer.py:48] [45] global_step=45, grad_norm=0.322471, loss=0.281816
I0520 07:48:28.579225 139849944258368 submission.py:139] 45) loss = 0.282, grad_norm = 0.322
I0520 07:48:28.881307 139798005745408 logging_writer.py:48] [46] global_step=46, grad_norm=0.318326, loss=0.274722
I0520 07:48:28.885388 139849944258368 submission.py:139] 46) loss = 0.275, grad_norm = 0.318
I0520 07:48:29.187715 139798014138112 logging_writer.py:48] [47] global_step=47, grad_norm=0.306430, loss=0.267541
I0520 07:48:29.191936 139849944258368 submission.py:139] 47) loss = 0.268, grad_norm = 0.306
I0520 07:48:29.486863 139798005745408 logging_writer.py:48] [48] global_step=48, grad_norm=0.301250, loss=0.260841
I0520 07:48:29.491092 139849944258368 submission.py:139] 48) loss = 0.261, grad_norm = 0.301
I0520 07:48:29.783597 139798014138112 logging_writer.py:48] [49] global_step=49, grad_norm=0.291047, loss=0.255963
I0520 07:48:29.787689 139849944258368 submission.py:139] 49) loss = 0.256, grad_norm = 0.291
I0520 07:48:30.082649 139798005745408 logging_writer.py:48] [50] global_step=50, grad_norm=0.281185, loss=0.249132
I0520 07:48:30.086605 139849944258368 submission.py:139] 50) loss = 0.249, grad_norm = 0.281
I0520 07:48:30.387926 139798014138112 logging_writer.py:48] [51] global_step=51, grad_norm=0.272243, loss=0.241254
I0520 07:48:30.391974 139849944258368 submission.py:139] 51) loss = 0.241, grad_norm = 0.272
I0520 07:48:30.699170 139798005745408 logging_writer.py:48] [52] global_step=52, grad_norm=0.263633, loss=0.234777
I0520 07:48:30.703234 139849944258368 submission.py:139] 52) loss = 0.235, grad_norm = 0.264
I0520 07:48:31.001403 139798014138112 logging_writer.py:48] [53] global_step=53, grad_norm=0.258124, loss=0.228615
I0520 07:48:31.005484 139849944258368 submission.py:139] 53) loss = 0.229, grad_norm = 0.258
I0520 07:48:31.306110 139798005745408 logging_writer.py:48] [54] global_step=54, grad_norm=0.250492, loss=0.221670
I0520 07:48:31.310328 139849944258368 submission.py:139] 54) loss = 0.222, grad_norm = 0.250
I0520 07:48:31.609583 139798014138112 logging_writer.py:48] [55] global_step=55, grad_norm=0.239489, loss=0.215109
I0520 07:48:31.613505 139849944258368 submission.py:139] 55) loss = 0.215, grad_norm = 0.239
I0520 07:48:31.914002 139798005745408 logging_writer.py:48] [56] global_step=56, grad_norm=0.232702, loss=0.211660
I0520 07:48:31.917921 139849944258368 submission.py:139] 56) loss = 0.212, grad_norm = 0.233
I0520 07:48:32.224128 139798014138112 logging_writer.py:48] [57] global_step=57, grad_norm=0.226141, loss=0.203573
I0520 07:48:32.228352 139849944258368 submission.py:139] 57) loss = 0.204, grad_norm = 0.226
I0520 07:48:32.534628 139798005745408 logging_writer.py:48] [58] global_step=58, grad_norm=0.218028, loss=0.201488
I0520 07:48:32.538569 139849944258368 submission.py:139] 58) loss = 0.201, grad_norm = 0.218
I0520 07:48:32.840718 139798014138112 logging_writer.py:48] [59] global_step=59, grad_norm=0.212165, loss=0.193064
I0520 07:48:32.844738 139849944258368 submission.py:139] 59) loss = 0.193, grad_norm = 0.212
I0520 07:48:33.152586 139798005745408 logging_writer.py:48] [60] global_step=60, grad_norm=0.206770, loss=0.186335
I0520 07:48:33.156682 139849944258368 submission.py:139] 60) loss = 0.186, grad_norm = 0.207
I0520 07:48:33.460901 139798014138112 logging_writer.py:48] [61] global_step=61, grad_norm=0.199379, loss=0.185070
I0520 07:48:33.464926 139849944258368 submission.py:139] 61) loss = 0.185, grad_norm = 0.199
I0520 07:48:33.769693 139798005745408 logging_writer.py:48] [62] global_step=62, grad_norm=0.191928, loss=0.181804
I0520 07:48:33.773632 139849944258368 submission.py:139] 62) loss = 0.182, grad_norm = 0.192
I0520 07:48:34.074599 139798014138112 logging_writer.py:48] [63] global_step=63, grad_norm=0.187445, loss=0.175032
I0520 07:48:34.078518 139849944258368 submission.py:139] 63) loss = 0.175, grad_norm = 0.187
I0520 07:48:34.394949 139798005745408 logging_writer.py:48] [64] global_step=64, grad_norm=0.178852, loss=0.170006
I0520 07:48:34.399102 139849944258368 submission.py:139] 64) loss = 0.170, grad_norm = 0.179
I0520 07:48:34.707056 139798014138112 logging_writer.py:48] [65] global_step=65, grad_norm=0.171782, loss=0.166929
I0520 07:48:34.711204 139849944258368 submission.py:139] 65) loss = 0.167, grad_norm = 0.172
I0520 07:48:35.014266 139798005745408 logging_writer.py:48] [66] global_step=66, grad_norm=0.169963, loss=0.162624
I0520 07:48:35.018254 139849944258368 submission.py:139] 66) loss = 0.163, grad_norm = 0.170
I0520 07:48:35.326811 139798014138112 logging_writer.py:48] [67] global_step=67, grad_norm=0.164240, loss=0.160597
I0520 07:48:35.330899 139849944258368 submission.py:139] 67) loss = 0.161, grad_norm = 0.164
I0520 07:48:35.635883 139798005745408 logging_writer.py:48] [68] global_step=68, grad_norm=0.158545, loss=0.151360
I0520 07:48:35.640136 139849944258368 submission.py:139] 68) loss = 0.151, grad_norm = 0.159
I0520 07:48:35.941865 139798014138112 logging_writer.py:48] [69] global_step=69, grad_norm=0.153108, loss=0.150806
I0520 07:48:35.945980 139849944258368 submission.py:139] 69) loss = 0.151, grad_norm = 0.153
I0520 07:48:36.247989 139798005745408 logging_writer.py:48] [70] global_step=70, grad_norm=0.151099, loss=0.148725
I0520 07:48:36.252321 139849944258368 submission.py:139] 70) loss = 0.149, grad_norm = 0.151
I0520 07:48:36.559626 139798014138112 logging_writer.py:48] [71] global_step=71, grad_norm=0.144623, loss=0.144219
I0520 07:48:36.563408 139849944258368 submission.py:139] 71) loss = 0.144, grad_norm = 0.145
I0520 07:48:36.869178 139798005745408 logging_writer.py:48] [72] global_step=72, grad_norm=0.141958, loss=0.141493
I0520 07:48:36.873146 139849944258368 submission.py:139] 72) loss = 0.141, grad_norm = 0.142
I0520 07:48:37.182090 139798014138112 logging_writer.py:48] [73] global_step=73, grad_norm=0.138392, loss=0.141032
I0520 07:48:37.186412 139849944258368 submission.py:139] 73) loss = 0.141, grad_norm = 0.138
I0520 07:48:37.490328 139798005745408 logging_writer.py:48] [74] global_step=74, grad_norm=0.134353, loss=0.135419
I0520 07:48:37.494522 139849944258368 submission.py:139] 74) loss = 0.135, grad_norm = 0.134
I0520 07:48:37.797035 139798014138112 logging_writer.py:48] [75] global_step=75, grad_norm=0.128661, loss=0.133716
I0520 07:48:37.801318 139849944258368 submission.py:139] 75) loss = 0.134, grad_norm = 0.129
I0520 07:48:38.106825 139798005745408 logging_writer.py:48] [76] global_step=76, grad_norm=0.124508, loss=0.134530
I0520 07:48:38.111445 139849944258368 submission.py:139] 76) loss = 0.135, grad_norm = 0.125
I0520 07:48:38.414623 139798014138112 logging_writer.py:48] [77] global_step=77, grad_norm=0.122247, loss=0.128401
I0520 07:48:38.418483 139849944258368 submission.py:139] 77) loss = 0.128, grad_norm = 0.122
I0520 07:48:38.722953 139798005745408 logging_writer.py:48] [78] global_step=78, grad_norm=0.119818, loss=0.126231
I0520 07:48:38.726974 139849944258368 submission.py:139] 78) loss = 0.126, grad_norm = 0.120
I0520 07:48:39.031071 139798014138112 logging_writer.py:48] [79] global_step=79, grad_norm=0.116786, loss=0.119526
I0520 07:48:39.035051 139849944258368 submission.py:139] 79) loss = 0.120, grad_norm = 0.117
I0520 07:48:39.338960 139798005745408 logging_writer.py:48] [80] global_step=80, grad_norm=0.111141, loss=0.122104
I0520 07:48:39.343271 139849944258368 submission.py:139] 80) loss = 0.122, grad_norm = 0.111
I0520 07:48:39.646499 139798014138112 logging_writer.py:48] [81] global_step=81, grad_norm=0.108768, loss=0.120353
I0520 07:48:39.650581 139849944258368 submission.py:139] 81) loss = 0.120, grad_norm = 0.109
I0520 07:48:39.951532 139798005745408 logging_writer.py:48] [82] global_step=82, grad_norm=0.109258, loss=0.116149
I0520 07:48:39.955685 139849944258368 submission.py:139] 82) loss = 0.116, grad_norm = 0.109
I0520 07:48:40.258082 139798014138112 logging_writer.py:48] [83] global_step=83, grad_norm=0.102767, loss=0.113248
I0520 07:48:40.262382 139849944258368 submission.py:139] 83) loss = 0.113, grad_norm = 0.103
I0520 07:48:40.597398 139798005745408 logging_writer.py:48] [84] global_step=84, grad_norm=0.098864, loss=0.115056
I0520 07:48:40.601449 139849944258368 submission.py:139] 84) loss = 0.115, grad_norm = 0.099
I0520 07:48:40.921925 139798014138112 logging_writer.py:48] [85] global_step=85, grad_norm=0.097610, loss=0.112678
I0520 07:48:40.925966 139849944258368 submission.py:139] 85) loss = 0.113, grad_norm = 0.098
I0520 07:48:41.229300 139798005745408 logging_writer.py:48] [86] global_step=86, grad_norm=0.097009, loss=0.106773
I0520 07:48:41.233274 139849944258368 submission.py:139] 86) loss = 0.107, grad_norm = 0.097
I0520 07:48:41.534761 139798014138112 logging_writer.py:48] [87] global_step=87, grad_norm=0.091946, loss=0.107717
I0520 07:48:41.538854 139849944258368 submission.py:139] 87) loss = 0.108, grad_norm = 0.092
I0520 07:48:41.842290 139798005745408 logging_writer.py:48] [88] global_step=88, grad_norm=0.090766, loss=0.103417
I0520 07:48:41.846477 139849944258368 submission.py:139] 88) loss = 0.103, grad_norm = 0.091
I0520 07:48:42.158786 139798014138112 logging_writer.py:48] [89] global_step=89, grad_norm=0.088179, loss=0.103737
I0520 07:48:42.163481 139849944258368 submission.py:139] 89) loss = 0.104, grad_norm = 0.088
I0520 07:48:42.470756 139798005745408 logging_writer.py:48] [90] global_step=90, grad_norm=0.087262, loss=0.100856
I0520 07:48:42.474976 139849944258368 submission.py:139] 90) loss = 0.101, grad_norm = 0.087
I0520 07:48:42.781773 139798014138112 logging_writer.py:48] [91] global_step=91, grad_norm=0.084109, loss=0.099480
I0520 07:48:42.785753 139849944258368 submission.py:139] 91) loss = 0.099, grad_norm = 0.084
I0520 07:48:43.092965 139798005745408 logging_writer.py:48] [92] global_step=92, grad_norm=0.083354, loss=0.098043
I0520 07:48:43.097067 139849944258368 submission.py:139] 92) loss = 0.098, grad_norm = 0.083
I0520 07:48:43.403429 139798014138112 logging_writer.py:48] [93] global_step=93, grad_norm=0.079598, loss=0.097172
I0520 07:48:43.407640 139849944258368 submission.py:139] 93) loss = 0.097, grad_norm = 0.080
I0520 07:48:43.713364 139798005745408 logging_writer.py:48] [94] global_step=94, grad_norm=0.078632, loss=0.096899
I0520 07:48:43.717350 139849944258368 submission.py:139] 94) loss = 0.097, grad_norm = 0.079
I0520 07:48:44.023995 139798014138112 logging_writer.py:48] [95] global_step=95, grad_norm=0.075742, loss=0.099788
I0520 07:48:44.028089 139849944258368 submission.py:139] 95) loss = 0.100, grad_norm = 0.076
I0520 07:48:44.330593 139798005745408 logging_writer.py:48] [96] global_step=96, grad_norm=0.076796, loss=0.091964
I0520 07:48:44.335066 139849944258368 submission.py:139] 96) loss = 0.092, grad_norm = 0.077
I0520 07:48:44.640768 139798014138112 logging_writer.py:48] [97] global_step=97, grad_norm=0.073905, loss=0.094031
I0520 07:48:44.644907 139849944258368 submission.py:139] 97) loss = 0.094, grad_norm = 0.074
I0520 07:48:44.941570 139798005745408 logging_writer.py:48] [98] global_step=98, grad_norm=0.071392, loss=0.094071
I0520 07:48:44.945556 139849944258368 submission.py:139] 98) loss = 0.094, grad_norm = 0.071
I0520 07:48:45.246919 139798014138112 logging_writer.py:48] [99] global_step=99, grad_norm=0.072599, loss=0.091189
I0520 07:48:45.251012 139849944258368 submission.py:139] 99) loss = 0.091, grad_norm = 0.073
I0520 07:48:45.552663 139798005745408 logging_writer.py:48] [100] global_step=100, grad_norm=0.068091, loss=0.089380
I0520 07:48:45.556710 139849944258368 submission.py:139] 100) loss = 0.089, grad_norm = 0.068
I0520 07:50:45.631976 139798014138112 logging_writer.py:48] [500] global_step=500, grad_norm=0.009510, loss=0.052582
I0520 07:50:45.636712 139849944258368 submission.py:139] 500) loss = 0.053, grad_norm = 0.010
I0520 07:52:14.993757 139849944258368 spec.py:298] Evaluating on the training split.
I0520 07:53:13.012865 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 07:53:16.276151 139849944258368 spec.py:326] Evaluating on the test split.
I0520 07:53:19.479967 139849944258368 submission_runner.py:421] Time since start: 460.64s, 	Step: 803, 	{'train/accuracy': 0.9866398117266003, 'train/loss': 0.05596716676378214, 'train/mean_average_precision': 0.0290695139174885, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06556733231807564, 'validation/mean_average_precision': 0.03145445288585219, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06893498607319774, 'test/mean_average_precision': 0.03345847371232002, 'test/num_examples': 43793, 'score': 245.04941534996033, 'total_duration': 460.63546419143677, 'accumulated_submission_time': 245.04941534996033, 'accumulated_eval_time': 215.369455575943, 'accumulated_logging_time': 0.02752375602722168}
I0520 07:53:19.490540 139798005745408 logging_writer.py:48] [803] accumulated_eval_time=215.369456, accumulated_logging_time=0.027524, accumulated_submission_time=245.049415, global_step=803, preemption_count=0, score=245.049415, test/accuracy=0.983142, test/loss=0.068935, test/mean_average_precision=0.033458, test/num_examples=43793, total_duration=460.635464, train/accuracy=0.986640, train/loss=0.055967, train/mean_average_precision=0.029070, validation/accuracy=0.984118, validation/loss=0.065567, validation/mean_average_precision=0.031454, validation/num_examples=43793
I0520 07:54:19.280826 139798014138112 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.021940, loss=0.055247
I0520 07:54:19.285525 139849944258368 submission.py:139] 1000) loss = 0.055, grad_norm = 0.022
I0520 07:56:46.477052 139798005745408 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.030990, loss=0.054130
I0520 07:56:46.481961 139849944258368 submission.py:139] 1500) loss = 0.054, grad_norm = 0.031
I0520 07:57:19.678767 139849944258368 spec.py:298] Evaluating on the training split.
I0520 07:58:17.971251 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 07:58:21.257761 139849944258368 spec.py:326] Evaluating on the test split.
I0520 07:58:24.599360 139849944258368 submission_runner.py:421] Time since start: 765.75s, 	Step: 1613, 	{'train/accuracy': 0.9867374908926215, 'train/loss': 0.05357215382116062, 'train/mean_average_precision': 0.044770238696210884, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06247776206110706, 'validation/mean_average_precision': 0.04526330316127139, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06561670246470494, 'test/mean_average_precision': 0.046299369224323164, 'test/num_examples': 43793, 'score': 485.040091753006, 'total_duration': 765.754908323288, 'accumulated_submission_time': 485.040091753006, 'accumulated_eval_time': 280.289799451828, 'accumulated_logging_time': 0.04873943328857422}
I0520 07:58:24.609593 139798014138112 logging_writer.py:48] [1613] accumulated_eval_time=280.289799, accumulated_logging_time=0.048739, accumulated_submission_time=485.040092, global_step=1613, preemption_count=0, score=485.040092, test/accuracy=0.983142, test/loss=0.065617, test/mean_average_precision=0.046299, test/num_examples=43793, total_duration=765.754908, train/accuracy=0.986737, train/loss=0.053572, train/mean_average_precision=0.044770, validation/accuracy=0.984118, validation/loss=0.062478, validation/mean_average_precision=0.045263, validation/num_examples=43793
I0520 08:00:19.356241 139798005745408 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.024054, loss=0.051525
I0520 08:00:19.361349 139849944258368 submission.py:139] 2000) loss = 0.052, grad_norm = 0.024
I0520 08:02:24.802112 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:03:23.552774 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:03:26.833289 139849944258368 spec.py:326] Evaluating on the test split.
I0520 08:03:30.100476 139849944258368 submission_runner.py:421] Time since start: 1071.26s, 	Step: 2423, 	{'train/accuracy': 0.9867799234973151, 'train/loss': 0.051113070960432674, 'train/mean_average_precision': 0.06327770557869247, 'validation/accuracy': 0.9841228473979607, 'validation/loss': 0.061015221152433165, 'validation/mean_average_precision': 0.056583511432248434, 'validation/num_examples': 43793, 'test/accuracy': 0.9831446310945291, 'test/loss': 0.06434823518373986, 'test/mean_average_precision': 0.05820814338436526, 'test/num_examples': 43793, 'score': 725.0320479869843, 'total_duration': 1071.256052494049, 'accumulated_submission_time': 725.0320479869843, 'accumulated_eval_time': 345.5879611968994, 'accumulated_logging_time': 0.06976556777954102}
I0520 08:03:30.110907 139798014138112 logging_writer.py:48] [2423] accumulated_eval_time=345.587961, accumulated_logging_time=0.069766, accumulated_submission_time=725.032048, global_step=2423, preemption_count=0, score=725.032048, test/accuracy=0.983145, test/loss=0.064348, test/mean_average_precision=0.058208, test/num_examples=43793, total_duration=1071.256052, train/accuracy=0.986780, train/loss=0.051113, train/mean_average_precision=0.063278, validation/accuracy=0.984123, validation/loss=0.061015, validation/mean_average_precision=0.056584, validation/num_examples=43793
I0520 08:03:53.411518 139798005745408 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.038584, loss=0.052922
I0520 08:03:53.416455 139849944258368 submission.py:139] 2500) loss = 0.053, grad_norm = 0.039
I0520 08:06:21.537991 139798014138112 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.058298, loss=0.057007
I0520 08:06:21.543103 139849944258368 submission.py:139] 3000) loss = 0.057, grad_norm = 0.058
I0520 08:07:30.361160 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:08:32.841156 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:08:36.180570 139849944258368 spec.py:326] Evaluating on the test split.
I0520 08:08:39.428884 139849944258368 submission_runner.py:421] Time since start: 1380.58s, 	Step: 3233, 	{'train/accuracy': 0.9871528581238852, 'train/loss': 0.04837277260930451, 'train/mean_average_precision': 0.07949972672700964, 'validation/accuracy': 0.9843355603240056, 'validation/loss': 0.05806792042466268, 'validation/mean_average_precision': 0.08636371773533155, 'validation/num_examples': 43793, 'test/accuracy': 0.983348489322083, 'test/loss': 0.061189031542848764, 'test/mean_average_precision': 0.08816472096565536, 'test/num_examples': 43793, 'score': 965.0769324302673, 'total_duration': 1380.584406375885, 'accumulated_submission_time': 965.0769324302673, 'accumulated_eval_time': 414.6553897857666, 'accumulated_logging_time': 0.09103178977966309}
I0520 08:08:39.439075 139798005745408 logging_writer.py:48] [3233] accumulated_eval_time=414.655390, accumulated_logging_time=0.091032, accumulated_submission_time=965.076932, global_step=3233, preemption_count=0, score=965.076932, test/accuracy=0.983348, test/loss=0.061189, test/mean_average_precision=0.088165, test/num_examples=43793, total_duration=1380.584406, train/accuracy=0.987153, train/loss=0.048373, train/mean_average_precision=0.079500, validation/accuracy=0.984336, validation/loss=0.058068, validation/mean_average_precision=0.086364, validation/num_examples=43793
I0520 08:10:01.480304 139798014138112 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.029692, loss=0.051036
I0520 08:10:01.485428 139849944258368 submission.py:139] 3500) loss = 0.051, grad_norm = 0.030
I0520 08:12:30.541804 139798005745408 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.062379, loss=0.047516
I0520 08:12:30.546966 139849944258368 submission.py:139] 4000) loss = 0.048, grad_norm = 0.062
I0520 08:12:39.508686 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:13:39.854591 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:13:43.145205 139849944258368 spec.py:326] Evaluating on the test split.
I0520 08:13:46.399039 139849944258368 submission_runner.py:421] Time since start: 1687.55s, 	Step: 4031, 	{'train/accuracy': 0.9870554138062612, 'train/loss': 0.04833993292393682, 'train/mean_average_precision': 0.1012354233481301, 'validation/accuracy': 0.9844536890672863, 'validation/loss': 0.058382162925111246, 'validation/mean_average_precision': 0.10024439140702306, 'validation/num_examples': 43793, 'test/accuracy': 0.9834908531256226, 'test/loss': 0.061589745636317764, 'test/mean_average_precision': 0.1047559606125709, 'test/num_examples': 43793, 'score': 1204.9457309246063, 'total_duration': 1687.5545389652252, 'accumulated_submission_time': 1204.9457309246063, 'accumulated_eval_time': 481.5454411506653, 'accumulated_logging_time': 0.11387324333190918}
I0520 08:13:46.408925 139798014138112 logging_writer.py:48] [4031] accumulated_eval_time=481.545441, accumulated_logging_time=0.113873, accumulated_submission_time=1204.945731, global_step=4031, preemption_count=0, score=1204.945731, test/accuracy=0.983491, test/loss=0.061590, test/mean_average_precision=0.104756, test/num_examples=43793, total_duration=1687.554539, train/accuracy=0.987055, train/loss=0.048340, train/mean_average_precision=0.101235, validation/accuracy=0.984454, validation/loss=0.058382, validation/mean_average_precision=0.100244, validation/num_examples=43793
I0520 08:16:06.473529 139798005745408 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.048083, loss=0.047040
I0520 08:16:06.485952 139849944258368 submission.py:139] 4500) loss = 0.047, grad_norm = 0.048
I0520 08:17:46.458245 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:18:48.157312 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:18:51.458560 139849944258368 spec.py:326] Evaluating on the test split.
I0520 08:18:54.753874 139849944258368 submission_runner.py:421] Time since start: 1995.91s, 	Step: 4836, 	{'train/accuracy': 0.9871295219041413, 'train/loss': 0.04607655105168166, 'train/mean_average_precision': 0.11945332969014197, 'validation/accuracy': 0.9845584217675145, 'validation/loss': 0.05551171184583671, 'validation/mean_average_precision': 0.11766012349754748, 'validation/num_examples': 43793, 'test/accuracy': 0.9835923610447145, 'test/loss': 0.05867434132522169, 'test/mean_average_precision': 0.12251901956145561, 'test/num_examples': 43793, 'score': 1444.7897601127625, 'total_duration': 1995.9093787670135, 'accumulated_submission_time': 1444.7897601127625, 'accumulated_eval_time': 549.8408114910126, 'accumulated_logging_time': 0.14016127586364746}
I0520 08:18:54.764609 139798014138112 logging_writer.py:48] [4836] accumulated_eval_time=549.840811, accumulated_logging_time=0.140161, accumulated_submission_time=1444.789760, global_step=4836, preemption_count=0, score=1444.789760, test/accuracy=0.983592, test/loss=0.058674, test/mean_average_precision=0.122519, test/num_examples=43793, total_duration=1995.909379, train/accuracy=0.987130, train/loss=0.046077, train/mean_average_precision=0.119453, validation/accuracy=0.984558, validation/loss=0.055512, validation/mean_average_precision=0.117660, validation/num_examples=43793
I0520 08:19:43.625583 139798005745408 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.072144, loss=0.051702
I0520 08:19:43.630474 139849944258368 submission.py:139] 5000) loss = 0.052, grad_norm = 0.072
I0520 08:22:13.305452 139798014138112 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.073654, loss=0.049262
I0520 08:22:13.315250 139849944258368 submission.py:139] 5500) loss = 0.049, grad_norm = 0.074
I0520 08:22:54.901389 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:23:55.388846 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:23:58.687367 139849944258368 spec.py:326] Evaluating on the test split.
I0520 08:24:02.010334 139849944258368 submission_runner.py:421] Time since start: 2303.17s, 	Step: 5640, 	{'train/accuracy': 0.9877498816137718, 'train/loss': 0.04456622847675336, 'train/mean_average_precision': 0.13566135028547086, 'validation/accuracy': 0.9848864218519502, 'validation/loss': 0.05459988023734541, 'validation/mean_average_precision': 0.13242300590188072, 'validation/num_examples': 43793, 'test/accuracy': 0.9838783522358489, 'test/loss': 0.05756127513953127, 'test/mean_average_precision': 0.13405449816701237, 'test/num_examples': 43793, 'score': 1684.7261793613434, 'total_duration': 2303.165634393692, 'accumulated_submission_time': 1684.7261793613434, 'accumulated_eval_time': 616.949300289154, 'accumulated_logging_time': 0.16211199760437012}
I0520 08:24:02.022513 139798005745408 logging_writer.py:48] [5640] accumulated_eval_time=616.949300, accumulated_logging_time=0.162112, accumulated_submission_time=1684.726179, global_step=5640, preemption_count=0, score=1684.726179, test/accuracy=0.983878, test/loss=0.057561, test/mean_average_precision=0.134054, test/num_examples=43793, total_duration=2303.165634, train/accuracy=0.987750, train/loss=0.044566, train/mean_average_precision=0.135661, validation/accuracy=0.984886, validation/loss=0.054600, validation/mean_average_precision=0.132423, validation/num_examples=43793
I0520 08:25:50.825390 139798014138112 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.068642, loss=0.047091
I0520 08:25:50.832104 139849944258368 submission.py:139] 6000) loss = 0.047, grad_norm = 0.069
I0520 08:28:02.220465 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:29:03.704981 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:29:07.013986 139849944258368 spec.py:326] Evaluating on the test split.
I0520 08:29:10.263669 139849944258368 submission_runner.py:421] Time since start: 2611.42s, 	Step: 6443, 	{'train/accuracy': 0.9876134370127251, 'train/loss': 0.04344250691571633, 'train/mean_average_precision': 0.1601512783465037, 'validation/accuracy': 0.9848701842240078, 'validation/loss': 0.05371612196528882, 'validation/mean_average_precision': 0.14603840768171308, 'validation/num_examples': 43793, 'test/accuracy': 0.9838901456870296, 'test/loss': 0.0567919630473267, 'test/mean_average_precision': 0.1523565242912596, 'test/num_examples': 43793, 'score': 1924.7258088588715, 'total_duration': 2611.419226884842, 'accumulated_submission_time': 1924.7258088588715, 'accumulated_eval_time': 684.9922595024109, 'accumulated_logging_time': 0.18715476989746094}
I0520 08:29:10.274368 139798005745408 logging_writer.py:48] [6443] accumulated_eval_time=684.992260, accumulated_logging_time=0.187155, accumulated_submission_time=1924.725809, global_step=6443, preemption_count=0, score=1924.725809, test/accuracy=0.983890, test/loss=0.056792, test/mean_average_precision=0.152357, test/num_examples=43793, total_duration=2611.419227, train/accuracy=0.987613, train/loss=0.043443, train/mean_average_precision=0.160151, validation/accuracy=0.984870, validation/loss=0.053716, validation/mean_average_precision=0.146038, validation/num_examples=43793
I0520 08:29:28.111108 139798014138112 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.042310, loss=0.044721
I0520 08:29:28.120564 139849944258368 submission.py:139] 6500) loss = 0.045, grad_norm = 0.042
I0520 08:31:57.880062 139798005745408 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.038786, loss=0.044762
I0520 08:31:57.885589 139849944258368 submission.py:139] 7000) loss = 0.045, grad_norm = 0.039
I0520 08:33:10.400115 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:34:12.222810 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:34:15.541843 139849944258368 spec.py:326] Evaluating on the test split.
I0520 08:34:18.792563 139849944258368 submission_runner.py:421] Time since start: 2919.95s, 	Step: 7243, 	{'train/accuracy': 0.9877880955628014, 'train/loss': 0.04277129189472356, 'train/mean_average_precision': 0.16657537372840803, 'validation/accuracy': 0.98493716443927, 'validation/loss': 0.05311233909018135, 'validation/mean_average_precision': 0.15711157542148083, 'validation/num_examples': 43793, 'test/accuracy': 0.983970172677185, 'test/loss': 0.056157953166520586, 'test/mean_average_precision': 0.15907595536019123, 'test/num_examples': 43793, 'score': 2164.6550483703613, 'total_duration': 2919.9481287002563, 'accumulated_submission_time': 2164.6550483703613, 'accumulated_eval_time': 753.3844847679138, 'accumulated_logging_time': 0.210465669631958}
I0520 08:34:18.803125 139798014138112 logging_writer.py:48] [7243] accumulated_eval_time=753.384485, accumulated_logging_time=0.210466, accumulated_submission_time=2164.655048, global_step=7243, preemption_count=0, score=2164.655048, test/accuracy=0.983970, test/loss=0.056158, test/mean_average_precision=0.159076, test/num_examples=43793, total_duration=2919.948129, train/accuracy=0.987788, train/loss=0.042771, train/mean_average_precision=0.166575, validation/accuracy=0.984937, validation/loss=0.053112, validation/mean_average_precision=0.157112, validation/num_examples=43793
I0520 08:35:35.985743 139798005745408 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.087729, loss=0.048029
I0520 08:35:35.995360 139849944258368 submission.py:139] 7500) loss = 0.048, grad_norm = 0.088
I0520 08:38:05.286446 139798014138112 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.057680, loss=0.045654
I0520 08:38:05.297017 139849944258368 submission.py:139] 8000) loss = 0.046, grad_norm = 0.058
I0520 08:38:18.970557 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:39:20.627979 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:39:23.905580 139849944258368 spec.py:326] Evaluating on the test split.
I0520 08:39:27.191847 139849944258368 submission_runner.py:421] Time since start: 3228.35s, 	Step: 8047, 	{'train/accuracy': 0.9883998617812304, 'train/loss': 0.04062917166692427, 'train/mean_average_precision': 0.18857954845249636, 'validation/accuracy': 0.9854661051694924, 'validation/loss': 0.0502607198536259, 'validation/mean_average_precision': 0.1646216370712833, 'validation/num_examples': 43793, 'test/accuracy': 0.9844903481131952, 'test/loss': 0.053008515977388586, 'test/mean_average_precision': 0.1676594517464928, 'test/num_examples': 43793, 'score': 2404.6248967647552, 'total_duration': 3228.347370147705, 'accumulated_submission_time': 2404.6248967647552, 'accumulated_eval_time': 821.6055314540863, 'accumulated_logging_time': 0.23203134536743164}
I0520 08:39:27.202348 139798005745408 logging_writer.py:48] [8047] accumulated_eval_time=821.605531, accumulated_logging_time=0.232031, accumulated_submission_time=2404.624897, global_step=8047, preemption_count=0, score=2404.624897, test/accuracy=0.984490, test/loss=0.053009, test/mean_average_precision=0.167659, test/num_examples=43793, total_duration=3228.347370, train/accuracy=0.988400, train/loss=0.040629, train/mean_average_precision=0.188580, validation/accuracy=0.985466, validation/loss=0.050261, validation/mean_average_precision=0.164622, validation/num_examples=43793
I0520 08:41:43.314326 139798014138112 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.058047, loss=0.041807
I0520 08:41:43.320365 139849944258368 submission.py:139] 8500) loss = 0.042, grad_norm = 0.058
I0520 08:43:27.309102 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:44:28.971954 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:44:32.335535 139849944258368 spec.py:326] Evaluating on the test split.
I0520 08:44:35.873124 139849944258368 submission_runner.py:421] Time since start: 3537.03s, 	Step: 8849, 	{'train/accuracy': 0.9883685878579848, 'train/loss': 0.040508148500088464, 'train/mean_average_precision': 0.2009646593357563, 'validation/accuracy': 0.9856970854269725, 'validation/loss': 0.04933918689672138, 'validation/mean_average_precision': 0.1774807735032276, 'validation/num_examples': 43793, 'test/accuracy': 0.9846988394822843, 'test/loss': 0.05193473606782751, 'test/mean_average_precision': 0.1770704844167362, 'test/num_examples': 43793, 'score': 2644.5327122211456, 'total_duration': 3537.028579235077, 'accumulated_submission_time': 2644.5327122211456, 'accumulated_eval_time': 890.1692190170288, 'accumulated_logging_time': 0.25582122802734375}
I0520 08:44:35.884322 139798005745408 logging_writer.py:48] [8849] accumulated_eval_time=890.169219, accumulated_logging_time=0.255821, accumulated_submission_time=2644.532712, global_step=8849, preemption_count=0, score=2644.532712, test/accuracy=0.984699, test/loss=0.051935, test/mean_average_precision=0.177070, test/num_examples=43793, total_duration=3537.028579, train/accuracy=0.988369, train/loss=0.040508, train/mean_average_precision=0.200965, validation/accuracy=0.985697, validation/loss=0.049339, validation/mean_average_precision=0.177481, validation/num_examples=43793
I0520 08:45:21.473139 139798014138112 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.054587, loss=0.045079
I0520 08:45:21.478399 139849944258368 submission.py:139] 9000) loss = 0.045, grad_norm = 0.055
I0520 08:47:51.965871 139798005745408 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.070121, loss=0.043641
I0520 08:47:51.972800 139849944258368 submission.py:139] 9500) loss = 0.044, grad_norm = 0.070
I0520 08:48:36.014394 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:49:38.415815 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:49:41.778414 139849944258368 spec.py:326] Evaluating on the test split.
I0520 08:49:45.023347 139849944258368 submission_runner.py:421] Time since start: 3846.18s, 	Step: 9647, 	{'train/accuracy': 0.9886989675301179, 'train/loss': 0.03941249831030528, 'train/mean_average_precision': 0.20909181530548612, 'validation/accuracy': 0.9857283428607615, 'validation/loss': 0.04913453569923691, 'validation/mean_average_precision': 0.1893224815200842, 'validation/num_examples': 43793, 'test/accuracy': 0.9847510676232278, 'test/loss': 0.0517991015074979, 'test/mean_average_precision': 0.18495280777343762, 'test/num_examples': 43793, 'score': 2884.466842651367, 'total_duration': 3846.178879737854, 'accumulated_submission_time': 2884.466842651367, 'accumulated_eval_time': 959.1778910160065, 'accumulated_logging_time': 0.27802062034606934}
I0520 08:49:45.034295 139798014138112 logging_writer.py:48] [9647] accumulated_eval_time=959.177891, accumulated_logging_time=0.278021, accumulated_submission_time=2884.466843, global_step=9647, preemption_count=0, score=2884.466843, test/accuracy=0.984751, test/loss=0.051799, test/mean_average_precision=0.184953, test/num_examples=43793, total_duration=3846.178880, train/accuracy=0.988699, train/loss=0.039412, train/mean_average_precision=0.209092, validation/accuracy=0.985728, validation/loss=0.049135, validation/mean_average_precision=0.189322, validation/num_examples=43793
I0520 08:51:32.264698 139798005745408 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.066053, loss=0.045648
I0520 08:51:32.270259 139849944258368 submission.py:139] 10000) loss = 0.046, grad_norm = 0.066
I0520 08:53:45.085017 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:54:47.621083 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:54:50.985856 139849944258368 spec.py:326] Evaluating on the test split.
I0520 08:54:54.261013 139849944258368 submission_runner.py:421] Time since start: 4155.42s, 	Step: 10442, 	{'train/accuracy': 0.9883711831055478, 'train/loss': 0.03960082063329029, 'train/mean_average_precision': 0.22399204875657466, 'validation/accuracy': 0.9853901942588619, 'validation/loss': 0.05014300973669063, 'validation/mean_average_precision': 0.1958352323040932, 'validation/num_examples': 43793, 'test/accuracy': 0.9844069515655596, 'test/loss': 0.053212555187033606, 'test/mean_average_precision': 0.18906725427118803, 'test/num_examples': 43793, 'score': 3124.319163799286, 'total_duration': 4155.416544437408, 'accumulated_submission_time': 3124.319163799286, 'accumulated_eval_time': 1028.3536145687103, 'accumulated_logging_time': 0.30367136001586914}
I0520 08:54:54.272017 139798014138112 logging_writer.py:48] [10442] accumulated_eval_time=1028.353615, accumulated_logging_time=0.303671, accumulated_submission_time=3124.319164, global_step=10442, preemption_count=0, score=3124.319164, test/accuracy=0.984407, test/loss=0.053213, test/mean_average_precision=0.189067, test/num_examples=43793, total_duration=4155.416544, train/accuracy=0.988371, train/loss=0.039601, train/mean_average_precision=0.223992, validation/accuracy=0.985390, validation/loss=0.050143, validation/mean_average_precision=0.195835, validation/num_examples=43793
I0520 08:55:12.100454 139798005745408 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.045372, loss=0.043451
I0520 08:55:12.105542 139849944258368 submission.py:139] 10500) loss = 0.043, grad_norm = 0.045
I0520 08:57:42.192911 139798014138112 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.068182, loss=0.043136
I0520 08:57:42.199163 139849944258368 submission.py:139] 11000) loss = 0.043, grad_norm = 0.068
I0520 08:58:54.563219 139849944258368 spec.py:298] Evaluating on the training split.
I0520 08:59:55.870265 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 08:59:59.246411 139849944258368 spec.py:326] Evaluating on the test split.
I0520 09:00:02.585451 139849944258368 submission_runner.py:421] Time since start: 4463.74s, 	Step: 11243, 	{'train/accuracy': 0.9883202107739992, 'train/loss': 0.03949809075072501, 'train/mean_average_precision': 0.22537887073427826, 'validation/accuracy': 0.9854165804042683, 'validation/loss': 0.050326567874908565, 'validation/mean_average_precision': 0.19987003196149905, 'validation/num_examples': 43793, 'test/accuracy': 0.984413269485835, 'test/loss': 0.05356143930015976, 'test/mean_average_precision': 0.19058500502735784, 'test/num_examples': 43793, 'score': 3364.4134697914124, 'total_duration': 4463.740974187851, 'accumulated_submission_time': 3364.4134697914124, 'accumulated_eval_time': 1096.3755612373352, 'accumulated_logging_time': 0.3262357711791992}
I0520 09:00:02.596295 139798005745408 logging_writer.py:48] [11243] accumulated_eval_time=1096.375561, accumulated_logging_time=0.326236, accumulated_submission_time=3364.413470, global_step=11243, preemption_count=0, score=3364.413470, test/accuracy=0.984413, test/loss=0.053561, test/mean_average_precision=0.190585, test/num_examples=43793, total_duration=4463.740974, train/accuracy=0.988320, train/loss=0.039498, train/mean_average_precision=0.225379, validation/accuracy=0.985417, validation/loss=0.050327, validation/mean_average_precision=0.199870, validation/num_examples=43793
I0520 09:01:20.046129 139798014138112 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.035046, loss=0.043129
I0520 09:01:20.051718 139849944258368 submission.py:139] 11500) loss = 0.043, grad_norm = 0.035
I0520 09:03:49.074090 139849944258368 spec.py:298] Evaluating on the training split.
I0520 09:04:52.473813 139849944258368 spec.py:310] Evaluating on the validation split.
I0520 09:04:55.815048 139849944258368 spec.py:326] Evaluating on the test split.
I0520 09:04:59.080563 139849944258368 submission_runner.py:421] Time since start: 4760.24s, 	Step: 12000, 	{'train/accuracy': 0.9887854165572645, 'train/loss': 0.03846492903068591, 'train/mean_average_precision': 0.24007719803159444, 'validation/accuracy': 0.98573159038635, 'validation/loss': 0.04913660028825849, 'validation/mean_average_precision': 0.20331883537423287, 'validation/num_examples': 43793, 'test/accuracy': 0.9847481192604327, 'test/loss': 0.05188155036709223, 'test/mean_average_precision': 0.19729998325224962, 'test/num_examples': 43793, 'score': 3590.705176115036, 'total_duration': 4760.236100435257, 'accumulated_submission_time': 3590.705176115036, 'accumulated_eval_time': 1166.381784439087, 'accumulated_logging_time': 0.34844136238098145}
I0520 09:04:59.091450 139798005745408 logging_writer.py:48] [12000] accumulated_eval_time=1166.381784, accumulated_logging_time=0.348441, accumulated_submission_time=3590.705176, global_step=12000, preemption_count=0, score=3590.705176, test/accuracy=0.984748, test/loss=0.051882, test/mean_average_precision=0.197300, test/num_examples=43793, total_duration=4760.236100, train/accuracy=0.988785, train/loss=0.038465, train/mean_average_precision=0.240077, validation/accuracy=0.985732, validation/loss=0.049137, validation/mean_average_precision=0.203319, validation/num_examples=43793
I0520 09:04:59.113055 139798014138112 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3590.705176
I0520 09:04:59.176357 139849944258368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b_pytorch/timing_momentum/ogbg_pytorch/trial_1/checkpoint_12000.
I0520 09:04:59.351883 139849944258368 submission_runner.py:584] Tuning trial 1/1
I0520 09:04:59.352126 139849944258368 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0520 09:04:59.353905 139849944258368 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5016493984187683, 'train/loss': 0.7639511287898042, 'train/mean_average_precision': 0.024553067411332084, 'validation/accuracy': 0.49915219285105955, 'validation/loss': 0.762280568755394, 'validation/mean_average_precision': 0.02670647641971499, 'validation/num_examples': 43793, 'test/accuracy': 0.49681682116789705, 'test/loss': 0.7624870008790333, 'test/mean_average_precision': 0.028973853583578506, 'test/num_examples': 43793, 'score': 5.0393102169036865, 'total_duration': 155.92417740821838, 'accumulated_submission_time': 5.0393102169036865, 'accumulated_eval_time': 150.88354659080505, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (803, {'train/accuracy': 0.9866398117266003, 'train/loss': 0.05596716676378214, 'train/mean_average_precision': 0.0290695139174885, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06556733231807564, 'validation/mean_average_precision': 0.03145445288585219, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06893498607319774, 'test/mean_average_precision': 0.03345847371232002, 'test/num_examples': 43793, 'score': 245.04941534996033, 'total_duration': 460.63546419143677, 'accumulated_submission_time': 245.04941534996033, 'accumulated_eval_time': 215.369455575943, 'accumulated_logging_time': 0.02752375602722168, 'global_step': 803, 'preemption_count': 0}), (1613, {'train/accuracy': 0.9867374908926215, 'train/loss': 0.05357215382116062, 'train/mean_average_precision': 0.044770238696210884, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06247776206110706, 'validation/mean_average_precision': 0.04526330316127139, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06561670246470494, 'test/mean_average_precision': 0.046299369224323164, 'test/num_examples': 43793, 'score': 485.040091753006, 'total_duration': 765.754908323288, 'accumulated_submission_time': 485.040091753006, 'accumulated_eval_time': 280.289799451828, 'accumulated_logging_time': 0.04873943328857422, 'global_step': 1613, 'preemption_count': 0}), (2423, {'train/accuracy': 0.9867799234973151, 'train/loss': 0.051113070960432674, 'train/mean_average_precision': 0.06327770557869247, 'validation/accuracy': 0.9841228473979607, 'validation/loss': 0.061015221152433165, 'validation/mean_average_precision': 0.056583511432248434, 'validation/num_examples': 43793, 'test/accuracy': 0.9831446310945291, 'test/loss': 0.06434823518373986, 'test/mean_average_precision': 0.05820814338436526, 'test/num_examples': 43793, 'score': 725.0320479869843, 'total_duration': 1071.256052494049, 'accumulated_submission_time': 725.0320479869843, 'accumulated_eval_time': 345.5879611968994, 'accumulated_logging_time': 0.06976556777954102, 'global_step': 2423, 'preemption_count': 0}), (3233, {'train/accuracy': 0.9871528581238852, 'train/loss': 0.04837277260930451, 'train/mean_average_precision': 0.07949972672700964, 'validation/accuracy': 0.9843355603240056, 'validation/loss': 0.05806792042466268, 'validation/mean_average_precision': 0.08636371773533155, 'validation/num_examples': 43793, 'test/accuracy': 0.983348489322083, 'test/loss': 0.061189031542848764, 'test/mean_average_precision': 0.08816472096565536, 'test/num_examples': 43793, 'score': 965.0769324302673, 'total_duration': 1380.584406375885, 'accumulated_submission_time': 965.0769324302673, 'accumulated_eval_time': 414.6553897857666, 'accumulated_logging_time': 0.09103178977966309, 'global_step': 3233, 'preemption_count': 0}), (4031, {'train/accuracy': 0.9870554138062612, 'train/loss': 0.04833993292393682, 'train/mean_average_precision': 0.1012354233481301, 'validation/accuracy': 0.9844536890672863, 'validation/loss': 0.058382162925111246, 'validation/mean_average_precision': 0.10024439140702306, 'validation/num_examples': 43793, 'test/accuracy': 0.9834908531256226, 'test/loss': 0.061589745636317764, 'test/mean_average_precision': 0.1047559606125709, 'test/num_examples': 43793, 'score': 1204.9457309246063, 'total_duration': 1687.5545389652252, 'accumulated_submission_time': 1204.9457309246063, 'accumulated_eval_time': 481.5454411506653, 'accumulated_logging_time': 0.11387324333190918, 'global_step': 4031, 'preemption_count': 0}), (4836, {'train/accuracy': 0.9871295219041413, 'train/loss': 0.04607655105168166, 'train/mean_average_precision': 0.11945332969014197, 'validation/accuracy': 0.9845584217675145, 'validation/loss': 0.05551171184583671, 'validation/mean_average_precision': 0.11766012349754748, 'validation/num_examples': 43793, 'test/accuracy': 0.9835923610447145, 'test/loss': 0.05867434132522169, 'test/mean_average_precision': 0.12251901956145561, 'test/num_examples': 43793, 'score': 1444.7897601127625, 'total_duration': 1995.9093787670135, 'accumulated_submission_time': 1444.7897601127625, 'accumulated_eval_time': 549.8408114910126, 'accumulated_logging_time': 0.14016127586364746, 'global_step': 4836, 'preemption_count': 0}), (5640, {'train/accuracy': 0.9877498816137718, 'train/loss': 0.04456622847675336, 'train/mean_average_precision': 0.13566135028547086, 'validation/accuracy': 0.9848864218519502, 'validation/loss': 0.05459988023734541, 'validation/mean_average_precision': 0.13242300590188072, 'validation/num_examples': 43793, 'test/accuracy': 0.9838783522358489, 'test/loss': 0.05756127513953127, 'test/mean_average_precision': 0.13405449816701237, 'test/num_examples': 43793, 'score': 1684.7261793613434, 'total_duration': 2303.165634393692, 'accumulated_submission_time': 1684.7261793613434, 'accumulated_eval_time': 616.949300289154, 'accumulated_logging_time': 0.16211199760437012, 'global_step': 5640, 'preemption_count': 0}), (6443, {'train/accuracy': 0.9876134370127251, 'train/loss': 0.04344250691571633, 'train/mean_average_precision': 0.1601512783465037, 'validation/accuracy': 0.9848701842240078, 'validation/loss': 0.05371612196528882, 'validation/mean_average_precision': 0.14603840768171308, 'validation/num_examples': 43793, 'test/accuracy': 0.9838901456870296, 'test/loss': 0.0567919630473267, 'test/mean_average_precision': 0.1523565242912596, 'test/num_examples': 43793, 'score': 1924.7258088588715, 'total_duration': 2611.419226884842, 'accumulated_submission_time': 1924.7258088588715, 'accumulated_eval_time': 684.9922595024109, 'accumulated_logging_time': 0.18715476989746094, 'global_step': 6443, 'preemption_count': 0}), (7243, {'train/accuracy': 0.9877880955628014, 'train/loss': 0.04277129189472356, 'train/mean_average_precision': 0.16657537372840803, 'validation/accuracy': 0.98493716443927, 'validation/loss': 0.05311233909018135, 'validation/mean_average_precision': 0.15711157542148083, 'validation/num_examples': 43793, 'test/accuracy': 0.983970172677185, 'test/loss': 0.056157953166520586, 'test/mean_average_precision': 0.15907595536019123, 'test/num_examples': 43793, 'score': 2164.6550483703613, 'total_duration': 2919.9481287002563, 'accumulated_submission_time': 2164.6550483703613, 'accumulated_eval_time': 753.3844847679138, 'accumulated_logging_time': 0.210465669631958, 'global_step': 7243, 'preemption_count': 0}), (8047, {'train/accuracy': 0.9883998617812304, 'train/loss': 0.04062917166692427, 'train/mean_average_precision': 0.18857954845249636, 'validation/accuracy': 0.9854661051694924, 'validation/loss': 0.0502607198536259, 'validation/mean_average_precision': 0.1646216370712833, 'validation/num_examples': 43793, 'test/accuracy': 0.9844903481131952, 'test/loss': 0.053008515977388586, 'test/mean_average_precision': 0.1676594517464928, 'test/num_examples': 43793, 'score': 2404.6248967647552, 'total_duration': 3228.347370147705, 'accumulated_submission_time': 2404.6248967647552, 'accumulated_eval_time': 821.6055314540863, 'accumulated_logging_time': 0.23203134536743164, 'global_step': 8047, 'preemption_count': 0}), (8849, {'train/accuracy': 0.9883685878579848, 'train/loss': 0.040508148500088464, 'train/mean_average_precision': 0.2009646593357563, 'validation/accuracy': 0.9856970854269725, 'validation/loss': 0.04933918689672138, 'validation/mean_average_precision': 0.1774807735032276, 'validation/num_examples': 43793, 'test/accuracy': 0.9846988394822843, 'test/loss': 0.05193473606782751, 'test/mean_average_precision': 0.1770704844167362, 'test/num_examples': 43793, 'score': 2644.5327122211456, 'total_duration': 3537.028579235077, 'accumulated_submission_time': 2644.5327122211456, 'accumulated_eval_time': 890.1692190170288, 'accumulated_logging_time': 0.25582122802734375, 'global_step': 8849, 'preemption_count': 0}), (9647, {'train/accuracy': 0.9886989675301179, 'train/loss': 0.03941249831030528, 'train/mean_average_precision': 0.20909181530548612, 'validation/accuracy': 0.9857283428607615, 'validation/loss': 0.04913453569923691, 'validation/mean_average_precision': 0.1893224815200842, 'validation/num_examples': 43793, 'test/accuracy': 0.9847510676232278, 'test/loss': 0.0517991015074979, 'test/mean_average_precision': 0.18495280777343762, 'test/num_examples': 43793, 'score': 2884.466842651367, 'total_duration': 3846.178879737854, 'accumulated_submission_time': 2884.466842651367, 'accumulated_eval_time': 959.1778910160065, 'accumulated_logging_time': 0.27802062034606934, 'global_step': 9647, 'preemption_count': 0}), (10442, {'train/accuracy': 0.9883711831055478, 'train/loss': 0.03960082063329029, 'train/mean_average_precision': 0.22399204875657466, 'validation/accuracy': 0.9853901942588619, 'validation/loss': 0.05014300973669063, 'validation/mean_average_precision': 0.1958352323040932, 'validation/num_examples': 43793, 'test/accuracy': 0.9844069515655596, 'test/loss': 0.053212555187033606, 'test/mean_average_precision': 0.18906725427118803, 'test/num_examples': 43793, 'score': 3124.319163799286, 'total_duration': 4155.416544437408, 'accumulated_submission_time': 3124.319163799286, 'accumulated_eval_time': 1028.3536145687103, 'accumulated_logging_time': 0.30367136001586914, 'global_step': 10442, 'preemption_count': 0}), (11243, {'train/accuracy': 0.9883202107739992, 'train/loss': 0.03949809075072501, 'train/mean_average_precision': 0.22537887073427826, 'validation/accuracy': 0.9854165804042683, 'validation/loss': 0.050326567874908565, 'validation/mean_average_precision': 0.19987003196149905, 'validation/num_examples': 43793, 'test/accuracy': 0.984413269485835, 'test/loss': 0.05356143930015976, 'test/mean_average_precision': 0.19058500502735784, 'test/num_examples': 43793, 'score': 3364.4134697914124, 'total_duration': 4463.740974187851, 'accumulated_submission_time': 3364.4134697914124, 'accumulated_eval_time': 1096.3755612373352, 'accumulated_logging_time': 0.3262357711791992, 'global_step': 11243, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9887854165572645, 'train/loss': 0.03846492903068591, 'train/mean_average_precision': 0.24007719803159444, 'validation/accuracy': 0.98573159038635, 'validation/loss': 0.04913660028825849, 'validation/mean_average_precision': 0.20331883537423287, 'validation/num_examples': 43793, 'test/accuracy': 0.9847481192604327, 'test/loss': 0.05188155036709223, 'test/mean_average_precision': 0.19729998325224962, 'test/num_examples': 43793, 'score': 3590.705176115036, 'total_duration': 4760.236100435257, 'accumulated_submission_time': 3590.705176115036, 'accumulated_eval_time': 1166.381784439087, 'accumulated_logging_time': 0.34844136238098145, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0520 09:04:59.354099 139849944258368 submission_runner.py:587] Timing: 3590.705176115036
I0520 09:04:59.354159 139849944258368 submission_runner.py:588] ====================
I0520 09:04:59.354298 139849944258368 submission_runner.py:651] Final ogbg score: 3590.705176115036
