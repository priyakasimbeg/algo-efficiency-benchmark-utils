torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_b_pytorch/timing_nadamw --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_pytorch_05-20-2023-08-10-47.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0520 08:11:10.362437 139670868195136 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0520 08:11:10.362488 140547236538176 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0520 08:11:10.362536 140292183320384 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0520 08:11:10.363051 139788210001728 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0520 08:11:10.363155 140459024148288 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0520 08:11:10.363450 140278673221440 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0520 08:11:10.363527 140717070968640 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0520 08:11:10.363598 139923615557440 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0520 08:11:10.363771 140278673221440 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 08:11:10.363867 140717070968640 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 08:11:10.363954 139923615557440 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 08:11:10.373153 139670868195136 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 08:11:10.373179 140547236538176 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 08:11:10.373200 140292183320384 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 08:11:10.373757 139788210001728 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 08:11:10.373820 140459024148288 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 08:11:11.554089 139923615557440 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_b_pytorch/timing_nadamw/ogbg_pytorch.
W0520 08:11:11.596230 140459024148288 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 08:11:11.596850 140292183320384 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 08:11:11.597111 140547236538176 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 08:11:11.597133 139923615557440 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 08:11:11.597272 139670868195136 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 08:11:11.598173 140717070968640 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 08:11:11.599037 139788210001728 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 08:11:11.599872 140278673221440 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0520 08:11:11.602124 139923615557440 submission_runner.py:544] Using RNG seed 397668984
I0520 08:11:11.603401 139923615557440 submission_runner.py:553] --- Tuning run 1/1 ---
I0520 08:11:11.603507 139923615557440 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_b_pytorch/timing_nadamw/ogbg_pytorch/trial_1.
I0520 08:11:11.603815 139923615557440 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/ogbg_pytorch/trial_1/hparams.json.
I0520 08:11:11.604792 139923615557440 submission_runner.py:241] Initializing dataset.
I0520 08:11:11.604907 139923615557440 submission_runner.py:248] Initializing model.
I0520 08:11:15.670914 139923615557440 submission_runner.py:258] Initializing optimizer.
I0520 08:11:15.671808 139923615557440 submission_runner.py:265] Initializing metrics bundle.
I0520 08:11:15.671909 139923615557440 submission_runner.py:283] Initializing checkpoint and logger.
I0520 08:11:15.674890 139923615557440 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0520 08:11:15.675018 139923615557440 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0520 08:11:16.151345 139923615557440 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0520 08:11:16.152219 139923615557440 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/ogbg_pytorch/trial_1/flags_0.json.
I0520 08:11:16.204257 139923615557440 submission_runner.py:319] Starting training loop.
I0520 08:11:16.833752 139923615557440 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0520 08:11:16.839639 139923615557440 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0520 08:11:16.979934 139923615557440 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0520 08:11:21.615058 139885096724224 logging_writer.py:48] [0] global_step=0, grad_norm=2.549451, loss=0.762775
I0520 08:11:21.624034 139923615557440 submission.py:296] 0) loss = 0.763, grad_norm = 2.549
I0520 08:11:21.635943 139923615557440 spec.py:298] Evaluating on the training split.
I0520 08:11:21.642035 139923615557440 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0520 08:11:21.646512 139923615557440 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0520 08:11:21.701229 139923615557440 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0520 08:12:19.612030 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 08:12:19.615390 139923615557440 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0520 08:12:19.620748 139923615557440 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0520 08:12:19.676505 139923615557440 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0520 08:13:05.914635 139923615557440 spec.py:326] Evaluating on the test split.
I0520 08:13:05.917923 139923615557440 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0520 08:13:05.922366 139923615557440 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0520 08:13:05.978105 139923615557440 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0520 08:13:52.664731 139923615557440 submission_runner.py:421] Time since start: 156.46s, 	Step: 1, 	{'train/accuracy': 0.5056398465104689, 'train/loss': 0.7614446830209161, 'train/mean_average_precision': 0.020061397106703308, 'validation/accuracy': 0.5151878652958861, 'validation/loss': 0.7520013383864832, 'validation/mean_average_precision': 0.023434862810344396, 'validation/num_examples': 43793, 'test/accuracy': 0.5163408795977086, 'test/loss': 0.750518069462585, 'test/mean_average_precision': 0.025403965380798324, 'test/num_examples': 43793, 'score': 5.4310407638549805, 'total_duration': 156.46069288253784, 'accumulated_submission_time': 5.4310407638549805, 'accumulated_eval_time': 151.02845454216003, 'accumulated_logging_time': 0}
I0520 08:13:52.681320 139871674509056 logging_writer.py:48] [1] accumulated_eval_time=151.028455, accumulated_logging_time=0, accumulated_submission_time=5.431041, global_step=1, preemption_count=0, score=5.431041, test/accuracy=0.516341, test/loss=0.750518, test/mean_average_precision=0.025404, test/num_examples=43793, total_duration=156.460693, train/accuracy=0.505640, train/loss=0.761445, train/mean_average_precision=0.020061, validation/accuracy=0.515188, validation/loss=0.752001, validation/mean_average_precision=0.023435, validation/num_examples=43793
I0520 08:13:52.960213 139670868195136 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 08:13:52.960221 140717070968640 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 08:13:52.960217 140547236538176 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 08:13:52.960226 139788210001728 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 08:13:52.960235 140292183320384 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 08:13:52.960235 140459024148288 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 08:13:52.960253 139923615557440 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 08:13:52.960247 140278673221440 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 08:13:52.997514 139871682901760 logging_writer.py:48] [1] global_step=1, grad_norm=2.560025, loss=0.760616
I0520 08:13:53.001578 139923615557440 submission.py:296] 1) loss = 0.761, grad_norm = 2.560
I0520 08:13:53.303826 139871674509056 logging_writer.py:48] [2] global_step=2, grad_norm=2.565780, loss=0.760718
I0520 08:13:53.308053 139923615557440 submission.py:296] 2) loss = 0.761, grad_norm = 2.566
I0520 08:13:53.612153 139871682901760 logging_writer.py:48] [3] global_step=3, grad_norm=2.541453, loss=0.759038
I0520 08:13:53.616074 139923615557440 submission.py:296] 3) loss = 0.759, grad_norm = 2.541
I0520 08:13:53.919725 139871674509056 logging_writer.py:48] [4] global_step=4, grad_norm=2.530882, loss=0.755587
I0520 08:13:53.923565 139923615557440 submission.py:296] 4) loss = 0.756, grad_norm = 2.531
I0520 08:13:54.230257 139871682901760 logging_writer.py:48] [5] global_step=5, grad_norm=2.525660, loss=0.753333
I0520 08:13:54.234330 139923615557440 submission.py:296] 5) loss = 0.753, grad_norm = 2.526
I0520 08:13:54.539683 139871674509056 logging_writer.py:48] [6] global_step=6, grad_norm=2.499644, loss=0.747537
I0520 08:13:54.544402 139923615557440 submission.py:296] 6) loss = 0.748, grad_norm = 2.500
I0520 08:13:54.849975 139871682901760 logging_writer.py:48] [7] global_step=7, grad_norm=2.471758, loss=0.741978
I0520 08:13:54.854330 139923615557440 submission.py:296] 7) loss = 0.742, grad_norm = 2.472
I0520 08:13:55.158478 139871674509056 logging_writer.py:48] [8] global_step=8, grad_norm=2.424117, loss=0.736879
I0520 08:13:55.163411 139923615557440 submission.py:296] 8) loss = 0.737, grad_norm = 2.424
I0520 08:13:55.468767 139871682901760 logging_writer.py:48] [9] global_step=9, grad_norm=2.341275, loss=0.729260
I0520 08:13:55.472696 139923615557440 submission.py:296] 9) loss = 0.729, grad_norm = 2.341
I0520 08:13:55.816691 139871674509056 logging_writer.py:48] [10] global_step=10, grad_norm=2.298843, loss=0.722129
I0520 08:13:55.821137 139923615557440 submission.py:296] 10) loss = 0.722, grad_norm = 2.299
I0520 08:13:56.157248 139871682901760 logging_writer.py:48] [11] global_step=11, grad_norm=2.319973, loss=0.717461
I0520 08:13:56.161459 139923615557440 submission.py:296] 11) loss = 0.717, grad_norm = 2.320
I0520 08:13:56.497676 139871674509056 logging_writer.py:48] [12] global_step=12, grad_norm=2.305191, loss=0.709301
I0520 08:13:56.501606 139923615557440 submission.py:296] 12) loss = 0.709, grad_norm = 2.305
I0520 08:13:56.833430 139871682901760 logging_writer.py:48] [13] global_step=13, grad_norm=2.271252, loss=0.701882
I0520 08:13:56.837287 139923615557440 submission.py:296] 13) loss = 0.702, grad_norm = 2.271
I0520 08:13:57.174592 139871674509056 logging_writer.py:48] [14] global_step=14, grad_norm=2.135166, loss=0.692419
I0520 08:13:57.178584 139923615557440 submission.py:296] 14) loss = 0.692, grad_norm = 2.135
I0520 08:13:57.507792 139871682901760 logging_writer.py:48] [15] global_step=15, grad_norm=2.016601, loss=0.683655
I0520 08:13:57.511736 139923615557440 submission.py:296] 15) loss = 0.684, grad_norm = 2.017
I0520 08:13:57.859628 139871674509056 logging_writer.py:48] [16] global_step=16, grad_norm=1.951375, loss=0.678491
I0520 08:13:57.863397 139923615557440 submission.py:296] 16) loss = 0.678, grad_norm = 1.951
I0520 08:13:58.198861 139871682901760 logging_writer.py:48] [17] global_step=17, grad_norm=1.905868, loss=0.670264
I0520 08:13:58.203305 139923615557440 submission.py:296] 17) loss = 0.670, grad_norm = 1.906
I0520 08:13:58.539519 139871674509056 logging_writer.py:48] [18] global_step=18, grad_norm=1.828096, loss=0.659972
I0520 08:13:58.543364 139923615557440 submission.py:296] 18) loss = 0.660, grad_norm = 1.828
I0520 08:13:58.888503 139871682901760 logging_writer.py:48] [19] global_step=19, grad_norm=1.808556, loss=0.656529
I0520 08:13:58.892430 139923615557440 submission.py:296] 19) loss = 0.657, grad_norm = 1.809
I0520 08:13:59.226144 139871674509056 logging_writer.py:48] [20] global_step=20, grad_norm=1.760845, loss=0.649001
I0520 08:13:59.230169 139923615557440 submission.py:296] 20) loss = 0.649, grad_norm = 1.761
I0520 08:13:59.544437 139871682901760 logging_writer.py:48] [21] global_step=21, grad_norm=1.706014, loss=0.639837
I0520 08:13:59.548561 139923615557440 submission.py:296] 21) loss = 0.640, grad_norm = 1.706
I0520 08:13:59.856452 139871674509056 logging_writer.py:48] [22] global_step=22, grad_norm=1.637491, loss=0.631492
I0520 08:13:59.860633 139923615557440 submission.py:296] 22) loss = 0.631, grad_norm = 1.637
I0520 08:14:00.161534 139871682901760 logging_writer.py:48] [23] global_step=23, grad_norm=1.604506, loss=0.625182
I0520 08:14:00.165705 139923615557440 submission.py:296] 23) loss = 0.625, grad_norm = 1.605
I0520 08:14:00.467648 139871674509056 logging_writer.py:48] [24] global_step=24, grad_norm=1.520801, loss=0.619893
I0520 08:14:00.471682 139923615557440 submission.py:296] 24) loss = 0.620, grad_norm = 1.521
I0520 08:14:00.781993 139871682901760 logging_writer.py:48] [25] global_step=25, grad_norm=1.411791, loss=0.614417
I0520 08:14:00.786022 139923615557440 submission.py:296] 25) loss = 0.614, grad_norm = 1.412
I0520 08:14:01.093320 139871674509056 logging_writer.py:48] [26] global_step=26, grad_norm=1.357754, loss=0.605764
I0520 08:14:01.097118 139923615557440 submission.py:296] 26) loss = 0.606, grad_norm = 1.358
I0520 08:14:01.455384 139871682901760 logging_writer.py:48] [27] global_step=27, grad_norm=1.322128, loss=0.598476
I0520 08:14:01.459678 139923615557440 submission.py:296] 27) loss = 0.598, grad_norm = 1.322
I0520 08:14:01.782287 139871674509056 logging_writer.py:48] [28] global_step=28, grad_norm=1.302692, loss=0.593086
I0520 08:14:01.786884 139923615557440 submission.py:296] 28) loss = 0.593, grad_norm = 1.303
I0520 08:14:02.088132 139871682901760 logging_writer.py:48] [29] global_step=29, grad_norm=1.289971, loss=0.588844
I0520 08:14:02.092472 139923615557440 submission.py:296] 29) loss = 0.589, grad_norm = 1.290
I0520 08:14:02.396234 139871674509056 logging_writer.py:48] [30] global_step=30, grad_norm=1.253290, loss=0.583302
I0520 08:14:02.400529 139923615557440 submission.py:296] 30) loss = 0.583, grad_norm = 1.253
I0520 08:14:02.705115 139871682901760 logging_writer.py:48] [31] global_step=31, grad_norm=1.215232, loss=0.576136
I0520 08:14:02.709013 139923615557440 submission.py:296] 31) loss = 0.576, grad_norm = 1.215
I0520 08:14:03.013985 139871674509056 logging_writer.py:48] [32] global_step=32, grad_norm=1.171378, loss=0.572323
I0520 08:14:03.018038 139923615557440 submission.py:296] 32) loss = 0.572, grad_norm = 1.171
I0520 08:14:03.320726 139871682901760 logging_writer.py:48] [33] global_step=33, grad_norm=1.135878, loss=0.566410
I0520 08:14:03.324626 139923615557440 submission.py:296] 33) loss = 0.566, grad_norm = 1.136
I0520 08:14:03.627663 139871674509056 logging_writer.py:48] [34] global_step=34, grad_norm=1.108859, loss=0.562223
I0520 08:14:03.631630 139923615557440 submission.py:296] 34) loss = 0.562, grad_norm = 1.109
I0520 08:14:03.937742 139871682901760 logging_writer.py:48] [35] global_step=35, grad_norm=1.066273, loss=0.554168
I0520 08:14:03.941604 139923615557440 submission.py:296] 35) loss = 0.554, grad_norm = 1.066
I0520 08:14:04.249378 139871674509056 logging_writer.py:48] [36] global_step=36, grad_norm=1.016673, loss=0.549099
I0520 08:14:04.253206 139923615557440 submission.py:296] 36) loss = 0.549, grad_norm = 1.017
I0520 08:14:04.563193 139871682901760 logging_writer.py:48] [37] global_step=37, grad_norm=0.996605, loss=0.545343
I0520 08:14:04.567201 139923615557440 submission.py:296] 37) loss = 0.545, grad_norm = 0.997
I0520 08:14:04.874399 139871674509056 logging_writer.py:48] [38] global_step=38, grad_norm=0.978436, loss=0.539824
I0520 08:14:04.878404 139923615557440 submission.py:296] 38) loss = 0.540, grad_norm = 0.978
I0520 08:14:05.182976 139871682901760 logging_writer.py:48] [39] global_step=39, grad_norm=0.954392, loss=0.535680
I0520 08:14:05.186892 139923615557440 submission.py:296] 39) loss = 0.536, grad_norm = 0.954
I0520 08:14:05.493062 139871674509056 logging_writer.py:48] [40] global_step=40, grad_norm=0.927456, loss=0.532391
I0520 08:14:05.496903 139923615557440 submission.py:296] 40) loss = 0.532, grad_norm = 0.927
I0520 08:14:05.805455 139871682901760 logging_writer.py:48] [41] global_step=41, grad_norm=0.902912, loss=0.528539
I0520 08:14:05.809583 139923615557440 submission.py:296] 41) loss = 0.529, grad_norm = 0.903
I0520 08:14:06.111839 139871674509056 logging_writer.py:48] [42] global_step=42, grad_norm=0.877963, loss=0.523599
I0520 08:14:06.116063 139923615557440 submission.py:296] 42) loss = 0.524, grad_norm = 0.878
I0520 08:14:06.422128 139871682901760 logging_writer.py:48] [43] global_step=43, grad_norm=0.862483, loss=0.518366
I0520 08:14:06.426128 139923615557440 submission.py:296] 43) loss = 0.518, grad_norm = 0.862
I0520 08:14:06.735087 139871674509056 logging_writer.py:48] [44] global_step=44, grad_norm=0.848156, loss=0.513312
I0520 08:14:06.739312 139923615557440 submission.py:296] 44) loss = 0.513, grad_norm = 0.848
I0520 08:14:07.042559 139871682901760 logging_writer.py:48] [45] global_step=45, grad_norm=0.839472, loss=0.511485
I0520 08:14:07.046523 139923615557440 submission.py:296] 45) loss = 0.511, grad_norm = 0.839
I0520 08:14:07.350407 139871674509056 logging_writer.py:48] [46] global_step=46, grad_norm=0.826123, loss=0.508972
I0520 08:14:07.354207 139923615557440 submission.py:296] 46) loss = 0.509, grad_norm = 0.826
I0520 08:14:07.662331 139871682901760 logging_writer.py:48] [47] global_step=47, grad_norm=0.818621, loss=0.500533
I0520 08:14:07.666302 139923615557440 submission.py:296] 47) loss = 0.501, grad_norm = 0.819
I0520 08:14:07.979197 139871674509056 logging_writer.py:48] [48] global_step=48, grad_norm=0.794908, loss=0.498642
I0520 08:14:07.983497 139923615557440 submission.py:296] 48) loss = 0.499, grad_norm = 0.795
I0520 08:14:08.291397 139871682901760 logging_writer.py:48] [49] global_step=49, grad_norm=0.782341, loss=0.494816
I0520 08:14:08.295384 139923615557440 submission.py:296] 49) loss = 0.495, grad_norm = 0.782
I0520 08:14:08.597557 139871674509056 logging_writer.py:48] [50] global_step=50, grad_norm=0.769661, loss=0.489251
I0520 08:14:08.601582 139923615557440 submission.py:296] 50) loss = 0.489, grad_norm = 0.770
I0520 08:14:08.917424 139871682901760 logging_writer.py:48] [51] global_step=51, grad_norm=0.746433, loss=0.486039
I0520 08:14:08.921653 139923615557440 submission.py:296] 51) loss = 0.486, grad_norm = 0.746
I0520 08:14:09.225308 139871674509056 logging_writer.py:48] [52] global_step=52, grad_norm=0.731871, loss=0.481831
I0520 08:14:09.229168 139923615557440 submission.py:296] 52) loss = 0.482, grad_norm = 0.732
I0520 08:14:09.533671 139871682901760 logging_writer.py:48] [53] global_step=53, grad_norm=0.711399, loss=0.478900
I0520 08:14:09.537641 139923615557440 submission.py:296] 53) loss = 0.479, grad_norm = 0.711
I0520 08:14:09.842067 139871674509056 logging_writer.py:48] [54] global_step=54, grad_norm=0.689232, loss=0.474673
I0520 08:14:09.846023 139923615557440 submission.py:296] 54) loss = 0.475, grad_norm = 0.689
I0520 08:14:10.157330 139871682901760 logging_writer.py:48] [55] global_step=55, grad_norm=0.666410, loss=0.471004
I0520 08:14:10.161364 139923615557440 submission.py:296] 55) loss = 0.471, grad_norm = 0.666
I0520 08:14:10.471654 139871674509056 logging_writer.py:48] [56] global_step=56, grad_norm=0.661754, loss=0.468212
I0520 08:14:10.475524 139923615557440 submission.py:296] 56) loss = 0.468, grad_norm = 0.662
I0520 08:14:10.779847 139871682901760 logging_writer.py:48] [57] global_step=57, grad_norm=0.648265, loss=0.463267
I0520 08:14:10.783711 139923615557440 submission.py:296] 57) loss = 0.463, grad_norm = 0.648
I0520 08:14:11.084125 139871674509056 logging_writer.py:48] [58] global_step=58, grad_norm=0.631093, loss=0.461299
I0520 08:14:11.088013 139923615557440 submission.py:296] 58) loss = 0.461, grad_norm = 0.631
I0520 08:14:11.392807 139871682901760 logging_writer.py:48] [59] global_step=59, grad_norm=0.624124, loss=0.458442
I0520 08:14:11.396750 139923615557440 submission.py:296] 59) loss = 0.458, grad_norm = 0.624
I0520 08:14:11.704553 139871674509056 logging_writer.py:48] [60] global_step=60, grad_norm=0.614030, loss=0.454138
I0520 08:14:11.708589 139923615557440 submission.py:296] 60) loss = 0.454, grad_norm = 0.614
I0520 08:14:12.015189 139871682901760 logging_writer.py:48] [61] global_step=61, grad_norm=0.600954, loss=0.452069
I0520 08:14:12.019194 139923615557440 submission.py:296] 61) loss = 0.452, grad_norm = 0.601
I0520 08:14:12.319906 139871674509056 logging_writer.py:48] [62] global_step=62, grad_norm=0.588325, loss=0.447177
I0520 08:14:12.323755 139923615557440 submission.py:296] 62) loss = 0.447, grad_norm = 0.588
I0520 08:14:12.627580 139871682901760 logging_writer.py:48] [63] global_step=63, grad_norm=0.571589, loss=0.444444
I0520 08:14:12.631607 139923615557440 submission.py:296] 63) loss = 0.444, grad_norm = 0.572
I0520 08:14:12.938237 139871674509056 logging_writer.py:48] [64] global_step=64, grad_norm=0.552550, loss=0.441717
I0520 08:14:12.942249 139923615557440 submission.py:296] 64) loss = 0.442, grad_norm = 0.553
I0520 08:14:13.249404 139871682901760 logging_writer.py:48] [65] global_step=65, grad_norm=0.536766, loss=0.438357
I0520 08:14:13.253331 139923615557440 submission.py:296] 65) loss = 0.438, grad_norm = 0.537
I0520 08:14:13.562021 139871674509056 logging_writer.py:48] [66] global_step=66, grad_norm=0.525664, loss=0.435491
I0520 08:14:13.566046 139923615557440 submission.py:296] 66) loss = 0.435, grad_norm = 0.526
I0520 08:14:13.886632 139871682901760 logging_writer.py:48] [67] global_step=67, grad_norm=0.519826, loss=0.434128
I0520 08:14:13.890605 139923615557440 submission.py:296] 67) loss = 0.434, grad_norm = 0.520
I0520 08:14:14.200303 139871674509056 logging_writer.py:48] [68] global_step=68, grad_norm=0.517951, loss=0.431408
I0520 08:14:14.204311 139923615557440 submission.py:296] 68) loss = 0.431, grad_norm = 0.518
I0520 08:14:14.517895 139871682901760 logging_writer.py:48] [69] global_step=69, grad_norm=0.527176, loss=0.428453
I0520 08:14:14.521984 139923615557440 submission.py:296] 69) loss = 0.428, grad_norm = 0.527
I0520 08:14:14.836177 139871674509056 logging_writer.py:48] [70] global_step=70, grad_norm=0.517120, loss=0.425313
I0520 08:14:14.840059 139923615557440 submission.py:296] 70) loss = 0.425, grad_norm = 0.517
I0520 08:14:15.149049 139871682901760 logging_writer.py:48] [71] global_step=71, grad_norm=0.510821, loss=0.420575
I0520 08:14:15.152945 139923615557440 submission.py:296] 71) loss = 0.421, grad_norm = 0.511
I0520 08:14:15.459757 139871674509056 logging_writer.py:48] [72] global_step=72, grad_norm=0.492437, loss=0.420068
I0520 08:14:15.463647 139923615557440 submission.py:296] 72) loss = 0.420, grad_norm = 0.492
I0520 08:14:15.769419 139871682901760 logging_writer.py:48] [73] global_step=73, grad_norm=0.482284, loss=0.418602
I0520 08:14:15.773555 139923615557440 submission.py:296] 73) loss = 0.419, grad_norm = 0.482
I0520 08:14:16.080369 139871674509056 logging_writer.py:48] [74] global_step=74, grad_norm=0.482103, loss=0.415645
I0520 08:14:16.084215 139923615557440 submission.py:296] 74) loss = 0.416, grad_norm = 0.482
I0520 08:14:16.394689 139871682901760 logging_writer.py:48] [75] global_step=75, grad_norm=0.473044, loss=0.413167
I0520 08:14:16.398754 139923615557440 submission.py:296] 75) loss = 0.413, grad_norm = 0.473
I0520 08:14:16.708251 139871674509056 logging_writer.py:48] [76] global_step=76, grad_norm=0.472664, loss=0.410887
I0520 08:14:16.712314 139923615557440 submission.py:296] 76) loss = 0.411, grad_norm = 0.473
I0520 08:14:17.015332 139871682901760 logging_writer.py:48] [77] global_step=77, grad_norm=0.470184, loss=0.407915
I0520 08:14:17.019278 139923615557440 submission.py:296] 77) loss = 0.408, grad_norm = 0.470
I0520 08:14:17.324952 139871674509056 logging_writer.py:48] [78] global_step=78, grad_norm=0.465684, loss=0.407334
I0520 08:14:17.328809 139923615557440 submission.py:296] 78) loss = 0.407, grad_norm = 0.466
I0520 08:14:17.631669 139871682901760 logging_writer.py:48] [79] global_step=79, grad_norm=0.463579, loss=0.405504
I0520 08:14:17.635402 139923615557440 submission.py:296] 79) loss = 0.406, grad_norm = 0.464
I0520 08:14:17.953859 139871674509056 logging_writer.py:48] [80] global_step=80, grad_norm=0.467818, loss=0.401569
I0520 08:14:17.957962 139923615557440 submission.py:296] 80) loss = 0.402, grad_norm = 0.468
I0520 08:14:18.284162 139871682901760 logging_writer.py:48] [81] global_step=81, grad_norm=0.458149, loss=0.401999
I0520 08:14:18.288041 139923615557440 submission.py:296] 81) loss = 0.402, grad_norm = 0.458
I0520 08:14:18.594067 139871674509056 logging_writer.py:48] [82] global_step=82, grad_norm=0.457915, loss=0.396322
I0520 08:14:18.598099 139923615557440 submission.py:296] 82) loss = 0.396, grad_norm = 0.458
I0520 08:14:18.902772 139871682901760 logging_writer.py:48] [83] global_step=83, grad_norm=0.453201, loss=0.395156
I0520 08:14:18.906756 139923615557440 submission.py:296] 83) loss = 0.395, grad_norm = 0.453
I0520 08:14:19.214910 139871674509056 logging_writer.py:48] [84] global_step=84, grad_norm=0.448492, loss=0.392947
I0520 08:14:19.218888 139923615557440 submission.py:296] 84) loss = 0.393, grad_norm = 0.448
I0520 08:14:19.524473 139871682901760 logging_writer.py:48] [85] global_step=85, grad_norm=0.445627, loss=0.390871
I0520 08:14:19.528496 139923615557440 submission.py:296] 85) loss = 0.391, grad_norm = 0.446
I0520 08:14:19.832987 139871674509056 logging_writer.py:48] [86] global_step=86, grad_norm=0.442312, loss=0.388523
I0520 08:14:19.836995 139923615557440 submission.py:296] 86) loss = 0.389, grad_norm = 0.442
I0520 08:14:20.146074 139871682901760 logging_writer.py:48] [87] global_step=87, grad_norm=0.440616, loss=0.387287
I0520 08:14:20.149927 139923615557440 submission.py:296] 87) loss = 0.387, grad_norm = 0.441
I0520 08:14:20.458148 139871674509056 logging_writer.py:48] [88] global_step=88, grad_norm=0.446086, loss=0.386034
I0520 08:14:20.462137 139923615557440 submission.py:296] 88) loss = 0.386, grad_norm = 0.446
I0520 08:14:20.773151 139871682901760 logging_writer.py:48] [89] global_step=89, grad_norm=0.434536, loss=0.384661
I0520 08:14:20.777126 139923615557440 submission.py:296] 89) loss = 0.385, grad_norm = 0.435
I0520 08:14:21.087566 139871674509056 logging_writer.py:48] [90] global_step=90, grad_norm=0.432028, loss=0.381820
I0520 08:14:21.091549 139923615557440 submission.py:296] 90) loss = 0.382, grad_norm = 0.432
I0520 08:14:21.396166 139871682901760 logging_writer.py:48] [91] global_step=91, grad_norm=0.432227, loss=0.378169
I0520 08:14:21.400011 139923615557440 submission.py:296] 91) loss = 0.378, grad_norm = 0.432
I0520 08:14:21.704527 139871674509056 logging_writer.py:48] [92] global_step=92, grad_norm=0.426453, loss=0.377849
I0520 08:14:21.708456 139923615557440 submission.py:296] 92) loss = 0.378, grad_norm = 0.426
I0520 08:14:22.021897 139871682901760 logging_writer.py:48] [93] global_step=93, grad_norm=0.423357, loss=0.375988
I0520 08:14:22.025742 139923615557440 submission.py:296] 93) loss = 0.376, grad_norm = 0.423
I0520 08:14:22.334450 139871674509056 logging_writer.py:48] [94] global_step=94, grad_norm=0.422064, loss=0.373964
I0520 08:14:22.338431 139923615557440 submission.py:296] 94) loss = 0.374, grad_norm = 0.422
I0520 08:14:22.644151 139871682901760 logging_writer.py:48] [95] global_step=95, grad_norm=0.421876, loss=0.371384
I0520 08:14:22.648143 139923615557440 submission.py:296] 95) loss = 0.371, grad_norm = 0.422
I0520 08:14:22.954854 139871674509056 logging_writer.py:48] [96] global_step=96, grad_norm=0.416582, loss=0.370428
I0520 08:14:22.958836 139923615557440 submission.py:296] 96) loss = 0.370, grad_norm = 0.417
I0520 08:14:23.276990 139871682901760 logging_writer.py:48] [97] global_step=97, grad_norm=0.418101, loss=0.367849
I0520 08:14:23.280834 139923615557440 submission.py:296] 97) loss = 0.368, grad_norm = 0.418
I0520 08:14:23.602894 139871674509056 logging_writer.py:48] [98] global_step=98, grad_norm=0.414816, loss=0.365870
I0520 08:14:23.606785 139923615557440 submission.py:296] 98) loss = 0.366, grad_norm = 0.415
I0520 08:14:23.918203 139871682901760 logging_writer.py:48] [99] global_step=99, grad_norm=0.409210, loss=0.365128
I0520 08:14:23.922184 139923615557440 submission.py:296] 99) loss = 0.365, grad_norm = 0.409
I0520 08:14:24.229745 139871674509056 logging_writer.py:48] [100] global_step=100, grad_norm=0.409039, loss=0.361884
I0520 08:14:24.233636 139923615557440 submission.py:296] 100) loss = 0.362, grad_norm = 0.409
I0520 08:16:23.942327 139871682901760 logging_writer.py:48] [500] global_step=500, grad_norm=0.075894, loss=0.069842
I0520 08:16:23.946821 139923615557440 submission.py:296] 500) loss = 0.070, grad_norm = 0.076
I0520 08:17:52.977758 139923615557440 spec.py:298] Evaluating on the training split.
I0520 08:18:50.075848 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 08:18:53.307804 139923615557440 spec.py:326] Evaluating on the test split.
I0520 08:18:56.488568 139923615557440 submission_runner.py:421] Time since start: 460.28s, 	Step: 798, 	{'train/accuracy': 0.9867183538577873, 'train/loss': 0.053905786017857775, 'train/mean_average_precision': 0.05292351836555613, 'validation/accuracy': 0.9841212236351665, 'validation/loss': 0.06348822457370137, 'validation/mean_average_precision': 0.053573367125372494, 'validation/num_examples': 43793, 'test/accuracy': 0.9831492642360644, 'test/loss': 0.0668356464643444, 'test/mean_average_precision': 0.0544183036269973, 'test/num_examples': 43793, 'score': 245.5354359149933, 'total_duration': 460.28461813926697, 'accumulated_submission_time': 245.5354359149933, 'accumulated_eval_time': 214.5390067100525, 'accumulated_logging_time': 0.02620100975036621}
I0520 08:18:56.499260 139871674509056 logging_writer.py:48] [798] accumulated_eval_time=214.539007, accumulated_logging_time=0.026201, accumulated_submission_time=245.535436, global_step=798, preemption_count=0, score=245.535436, test/accuracy=0.983149, test/loss=0.066836, test/mean_average_precision=0.054418, test/num_examples=43793, total_duration=460.284618, train/accuracy=0.986718, train/loss=0.053906, train/mean_average_precision=0.052924, validation/accuracy=0.984121, validation/loss=0.063488, validation/mean_average_precision=0.053573, validation/num_examples=43793
I0520 08:19:58.122462 139871682901760 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.085435, loss=0.051273
I0520 08:19:58.127087 139923615557440 submission.py:296] 1000) loss = 0.051, grad_norm = 0.085
I0520 08:22:28.580477 139871674509056 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.069648, loss=0.049148
I0520 08:22:28.585656 139923615557440 submission.py:296] 1500) loss = 0.049, grad_norm = 0.070
I0520 08:22:56.522333 139923615557440 spec.py:298] Evaluating on the training split.
I0520 08:23:54.657292 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 08:23:57.893651 139923615557440 spec.py:326] Evaluating on the test split.
I0520 08:24:01.231367 139923615557440 submission_runner.py:421] Time since start: 765.03s, 	Step: 1594, 	{'train/accuracy': 0.9876167533570744, 'train/loss': 0.04467179831929132, 'train/mean_average_precision': 0.13129945043142555, 'validation/accuracy': 0.9848628772914337, 'validation/loss': 0.053962597740371696, 'validation/mean_average_precision': 0.1254587618814877, 'validation/num_examples': 43793, 'test/accuracy': 0.9838636104218728, 'test/loss': 0.05713847464976609, 'test/mean_average_precision': 0.12171744178845405, 'test/num_examples': 43793, 'score': 485.3691461086273, 'total_duration': 765.0270464420319, 'accumulated_submission_time': 485.3691461086273, 'accumulated_eval_time': 279.2474012374878, 'accumulated_logging_time': 0.04951167106628418}
I0520 08:24:01.249259 139871682901760 logging_writer.py:48] [1594] accumulated_eval_time=279.247401, accumulated_logging_time=0.049512, accumulated_submission_time=485.369146, global_step=1594, preemption_count=0, score=485.369146, test/accuracy=0.983864, test/loss=0.057138, test/mean_average_precision=0.121717, test/num_examples=43793, total_duration=765.027046, train/accuracy=0.987617, train/loss=0.044672, train/mean_average_precision=0.131299, validation/accuracy=0.984863, validation/loss=0.053963, validation/mean_average_precision=0.125459, validation/num_examples=43793
I0520 08:26:03.875275 139871674509056 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.065374, loss=0.043134
I0520 08:26:03.880509 139923615557440 submission.py:296] 2000) loss = 0.043, grad_norm = 0.065
I0520 08:28:01.336918 139923615557440 spec.py:298] Evaluating on the training split.
I0520 08:29:00.280559 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 08:29:03.624701 139923615557440 spec.py:326] Evaluating on the test split.
I0520 08:29:06.805558 139923615557440 submission_runner.py:421] Time since start: 1070.60s, 	Step: 2392, 	{'train/accuracy': 0.9881636178985944, 'train/loss': 0.042234313305993795, 'train/mean_average_precision': 0.17436598246943455, 'validation/accuracy': 0.9851758575700228, 'validation/loss': 0.05158150817321003, 'validation/mean_average_precision': 0.1555115334408861, 'validation/num_examples': 43793, 'test/accuracy': 0.9842481611693038, 'test/loss': 0.05408809718561924, 'test/mean_average_precision': 0.15620595120994107, 'test/num_examples': 43793, 'score': 725.2667095661163, 'total_duration': 1070.601625919342, 'accumulated_submission_time': 725.2667095661163, 'accumulated_eval_time': 344.7158131599426, 'accumulated_logging_time': 0.0808875560760498}
I0520 08:29:06.815742 139871682901760 logging_writer.py:48] [2392] accumulated_eval_time=344.715813, accumulated_logging_time=0.080888, accumulated_submission_time=725.266710, global_step=2392, preemption_count=0, score=725.266710, test/accuracy=0.984248, test/loss=0.054088, test/mean_average_precision=0.156206, test/num_examples=43793, total_duration=1070.601626, train/accuracy=0.988164, train/loss=0.042234, train/mean_average_precision=0.174366, validation/accuracy=0.985176, validation/loss=0.051582, validation/mean_average_precision=0.155512, validation/num_examples=43793
I0520 08:29:39.908101 139871674509056 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.073249, loss=0.048712
I0520 08:29:39.912980 139923615557440 submission.py:296] 2500) loss = 0.049, grad_norm = 0.073
I0520 08:32:11.136403 139871682901760 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.033565, loss=0.040757
I0520 08:32:11.141680 139923615557440 submission.py:296] 3000) loss = 0.041, grad_norm = 0.034
I0520 08:33:06.928636 139923615557440 spec.py:298] Evaluating on the training split.
I0520 08:34:06.589492 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 08:34:09.808489 139923615557440 spec.py:326] Evaluating on the test split.
I0520 08:34:12.965922 139923615557440 submission_runner.py:421] Time since start: 1376.76s, 	Step: 3186, 	{'train/accuracy': 0.9881962568787462, 'train/loss': 0.04076877575635545, 'train/mean_average_precision': 0.19080390786512927, 'validation/accuracy': 0.9854348477357033, 'validation/loss': 0.04982229755635878, 'validation/mean_average_precision': 0.1783507154657079, 'validation/num_examples': 43793, 'test/accuracy': 0.9845274132454778, 'test/loss': 0.052613942112687266, 'test/mean_average_precision': 0.17981449856166187, 'test/num_examples': 43793, 'score': 965.193056344986, 'total_duration': 1376.7619318962097, 'accumulated_submission_time': 965.193056344986, 'accumulated_eval_time': 410.7527940273285, 'accumulated_logging_time': 0.10181355476379395}
I0520 08:34:12.977149 139871674509056 logging_writer.py:48] [3186] accumulated_eval_time=410.752794, accumulated_logging_time=0.101814, accumulated_submission_time=965.193056, global_step=3186, preemption_count=0, score=965.193056, test/accuracy=0.984527, test/loss=0.052614, test/mean_average_precision=0.179814, test/num_examples=43793, total_duration=1376.761932, train/accuracy=0.988196, train/loss=0.040769, train/mean_average_precision=0.190804, validation/accuracy=0.985435, validation/loss=0.049822, validation/mean_average_precision=0.178351, validation/num_examples=43793
I0520 08:35:49.495020 139871682901760 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.021711, loss=0.041107
I0520 08:35:49.500497 139923615557440 submission.py:296] 3500) loss = 0.041, grad_norm = 0.022
I0520 08:38:13.103743 139923615557440 spec.py:298] Evaluating on the training split.
I0520 08:39:14.021679 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 08:39:17.269638 139923615557440 spec.py:326] Evaluating on the test split.
I0520 08:39:20.467034 139923615557440 submission_runner.py:421] Time since start: 1684.26s, 	Step: 3970, 	{'train/accuracy': 0.9886336725578807, 'train/loss': 0.039247502410507805, 'train/mean_average_precision': 0.22713203510404706, 'validation/accuracy': 0.9856739468071546, 'validation/loss': 0.0486817691068168, 'validation/mean_average_precision': 0.1867612746196072, 'validation/num_examples': 43793, 'test/accuracy': 0.9847649670478338, 'test/loss': 0.05148620650585734, 'test/mean_average_precision': 0.1920014621008941, 'test/num_examples': 43793, 'score': 1205.1316895484924, 'total_duration': 1684.2630410194397, 'accumulated_submission_time': 1205.1316895484924, 'accumulated_eval_time': 478.11580657958984, 'accumulated_logging_time': 0.12392473220825195}
I0520 08:39:20.477219 139871674509056 logging_writer.py:48] [3970] accumulated_eval_time=478.115807, accumulated_logging_time=0.123925, accumulated_submission_time=1205.131690, global_step=3970, preemption_count=0, score=1205.131690, test/accuracy=0.984765, test/loss=0.051486, test/mean_average_precision=0.192001, test/num_examples=43793, total_duration=1684.263041, train/accuracy=0.988634, train/loss=0.039248, train/mean_average_precision=0.227132, validation/accuracy=0.985674, validation/loss=0.048682, validation/mean_average_precision=0.186761, validation/num_examples=43793
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0520 08:39:30.009582 139871682901760 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.023117, loss=0.041665
I0520 08:39:30.014920 139923615557440 submission.py:296] 4000) loss = 0.042, grad_norm = 0.023
I0520 08:42:02.674791 139871674509056 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.015247, loss=0.042186
I0520 08:42:02.680103 139923615557440 submission.py:296] 4500) loss = 0.042, grad_norm = 0.015
I0520 08:43:20.726522 139923615557440 spec.py:298] Evaluating on the training split.
I0520 08:44:21.671167 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 08:44:24.934845 139923615557440 spec.py:326] Evaluating on the test split.
I0520 08:44:28.147391 139923615557440 submission_runner.py:421] Time since start: 1991.94s, 	Step: 4757, 	{'train/accuracy': 0.9888880104897938, 'train/loss': 0.03769673538485742, 'train/mean_average_precision': 0.2400557250809326, 'validation/accuracy': 0.986030768681188, 'validation/loss': 0.04720989473348775, 'validation/mean_average_precision': 0.20927988778717083, 'validation/num_examples': 43793, 'test/accuracy': 0.9851263520875883, 'test/loss': 0.049926104683095225, 'test/mean_average_precision': 0.2062239633173526, 'test/num_examples': 43793, 'score': 1445.1914274692535, 'total_duration': 1991.943438053131, 'accumulated_submission_time': 1445.1914274692535, 'accumulated_eval_time': 545.5364081859589, 'accumulated_logging_time': 0.14581990242004395}
I0520 08:44:28.157792 139871682901760 logging_writer.py:48] [4757] accumulated_eval_time=545.536408, accumulated_logging_time=0.145820, accumulated_submission_time=1445.191427, global_step=4757, preemption_count=0, score=1445.191427, test/accuracy=0.985126, test/loss=0.049926, test/mean_average_precision=0.206224, test/num_examples=43793, total_duration=1991.943438, train/accuracy=0.988888, train/loss=0.037697, train/mean_average_precision=0.240056, validation/accuracy=0.986031, validation/loss=0.047210, validation/mean_average_precision=0.209280, validation/num_examples=43793
I0520 08:45:42.293836 139871674509056 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.029562, loss=0.039170
I0520 08:45:42.299502 139923615557440 submission.py:296] 5000) loss = 0.039, grad_norm = 0.030
I0520 08:48:10.660127 139871682901760 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.012974, loss=0.037349
I0520 08:48:10.672461 139923615557440 submission.py:296] 5500) loss = 0.037, grad_norm = 0.013
I0520 08:48:28.374541 139923615557440 spec.py:298] Evaluating on the training split.
I0520 08:49:27.876791 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 08:49:31.099462 139923615557440 spec.py:326] Evaluating on the test split.
I0520 08:49:34.275417 139923615557440 submission_runner.py:421] Time since start: 2298.07s, 	Step: 5561, 	{'train/accuracy': 0.9892034154292465, 'train/loss': 0.03662500703415036, 'train/mean_average_precision': 0.2720163445721131, 'validation/accuracy': 0.9861610756454254, 'validation/loss': 0.04664325226088672, 'validation/mean_average_precision': 0.21488246926308774, 'validation/num_examples': 43793, 'test/accuracy': 0.9853087293862056, 'test/loss': 0.04923485181212695, 'test/mean_average_precision': 0.21450469193760677, 'test/num_examples': 43793, 'score': 1685.2184610366821, 'total_duration': 2298.0714888572693, 'accumulated_submission_time': 1685.2184610366821, 'accumulated_eval_time': 611.4370291233063, 'accumulated_logging_time': 0.1669173240661621}
I0520 08:49:34.286008 139871674509056 logging_writer.py:48] [5561] accumulated_eval_time=611.437029, accumulated_logging_time=0.166917, accumulated_submission_time=1685.218461, global_step=5561, preemption_count=0, score=1685.218461, test/accuracy=0.985309, test/loss=0.049235, test/mean_average_precision=0.214505, test/num_examples=43793, total_duration=2298.071489, train/accuracy=0.989203, train/loss=0.036625, train/mean_average_precision=0.272016, validation/accuracy=0.986161, validation/loss=0.046643, validation/mean_average_precision=0.214882, validation/num_examples=43793
I0520 08:51:44.269350 139871682901760 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.014929, loss=0.040144
I0520 08:51:44.274464 139923615557440 submission.py:296] 6000) loss = 0.040, grad_norm = 0.015
I0520 08:53:34.558111 139923615557440 spec.py:298] Evaluating on the training split.
I0520 08:54:33.522677 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 08:54:36.744523 139923615557440 spec.py:326] Evaluating on the test split.
I0520 08:54:39.921793 139923615557440 submission_runner.py:421] Time since start: 2603.72s, 	Step: 6377, 	{'train/accuracy': 0.9894478300442232, 'train/loss': 0.0357380471714819, 'train/mean_average_precision': 0.2908440979582141, 'validation/accuracy': 0.9862714915154335, 'validation/loss': 0.046615791006911544, 'validation/mean_average_precision': 0.22848088835180294, 'validation/num_examples': 43793, 'test/accuracy': 0.9854312970395489, 'test/loss': 0.04923103802587736, 'test/mean_average_precision': 0.22436566969424937, 'test/num_examples': 43793, 'score': 1925.2947688102722, 'total_duration': 2603.7178659439087, 'accumulated_submission_time': 1925.2947688102722, 'accumulated_eval_time': 676.8005027770996, 'accumulated_logging_time': 0.19359993934631348}
I0520 08:54:39.933130 139871674509056 logging_writer.py:48] [6377] accumulated_eval_time=676.800503, accumulated_logging_time=0.193600, accumulated_submission_time=1925.294769, global_step=6377, preemption_count=0, score=1925.294769, test/accuracy=0.985431, test/loss=0.049231, test/mean_average_precision=0.224366, test/num_examples=43793, total_duration=2603.717866, train/accuracy=0.989448, train/loss=0.035738, train/mean_average_precision=0.290844, validation/accuracy=0.986271, validation/loss=0.046616, validation/mean_average_precision=0.228481, validation/num_examples=43793
I0520 08:55:16.639228 139871682901760 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.013314, loss=0.040169
I0520 08:55:16.643976 139923615557440 submission.py:296] 6500) loss = 0.040, grad_norm = 0.013
I0520 08:57:43.777495 139871674509056 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.015482, loss=0.040146
I0520 08:57:43.783790 139923615557440 submission.py:296] 7000) loss = 0.040, grad_norm = 0.015
I0520 08:58:39.995064 139923615557440 spec.py:298] Evaluating on the training split.
I0520 08:59:39.585013 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 08:59:42.839280 139923615557440 spec.py:326] Evaluating on the test split.
I0520 08:59:46.076303 139923615557440 submission_runner.py:421] Time since start: 2909.87s, 	Step: 7191, 	{'train/accuracy': 0.9894466177951716, 'train/loss': 0.035782570773564, 'train/mean_average_precision': 0.2907682412963136, 'validation/accuracy': 0.9862292736827833, 'validation/loss': 0.04660637825696371, 'validation/mean_average_precision': 0.22815404763915836, 'validation/num_examples': 43793, 'test/accuracy': 0.985374014562385, 'test/loss': 0.049215588736453854, 'test/mean_average_precision': 0.23251257676807974, 'test/num_examples': 43793, 'score': 2165.166445016861, 'total_duration': 2909.8723697662354, 'accumulated_submission_time': 2165.166445016861, 'accumulated_eval_time': 742.8815128803253, 'accumulated_logging_time': 0.21563053131103516}
I0520 08:59:46.087012 139871682901760 logging_writer.py:48] [7191] accumulated_eval_time=742.881513, accumulated_logging_time=0.215631, accumulated_submission_time=2165.166445, global_step=7191, preemption_count=0, score=2165.166445, test/accuracy=0.985374, test/loss=0.049216, test/mean_average_precision=0.232513, test/num_examples=43793, total_duration=2909.872370, train/accuracy=0.989447, train/loss=0.035783, train/mean_average_precision=0.290768, validation/accuracy=0.986229, validation/loss=0.046606, validation/mean_average_precision=0.228154, validation/num_examples=43793
I0520 09:01:19.068836 139871674509056 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.011596, loss=0.035856
I0520 09:01:19.074227 139923615557440 submission.py:296] 7500) loss = 0.036, grad_norm = 0.012
I0520 09:03:46.313912 139923615557440 spec.py:298] Evaluating on the training split.
I0520 09:04:46.603861 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 09:04:49.815584 139923615557440 spec.py:326] Evaluating on the test split.
I0520 09:04:53.017720 139923615557440 submission_runner.py:421] Time since start: 3216.81s, 	Step: 8000, 	{'train/accuracy': 0.9895963711086109, 'train/loss': 0.03462295638091727, 'train/mean_average_precision': 0.3295150473854438, 'validation/accuracy': 0.9864545707704836, 'validation/loss': 0.04601538251386084, 'validation/mean_average_precision': 0.2390377085108702, 'validation/num_examples': 43793, 'test/accuracy': 0.9854692045612015, 'test/loss': 0.04889621128536403, 'test/mean_average_precision': 0.24063529349276105, 'test/num_examples': 43793, 'score': 2405.2018201351166, 'total_duration': 3216.8137714862823, 'accumulated_submission_time': 2405.2018201351166, 'accumulated_eval_time': 809.5850596427917, 'accumulated_logging_time': 0.23758506774902344}
I0520 09:04:53.028207 139871682901760 logging_writer.py:48] [8000] accumulated_eval_time=809.585060, accumulated_logging_time=0.237585, accumulated_submission_time=2405.201820, global_step=8000, preemption_count=0, score=2405.201820, test/accuracy=0.985469, test/loss=0.048896, test/mean_average_precision=0.240635, test/num_examples=43793, total_duration=3216.813771, train/accuracy=0.989596, train/loss=0.034623, train/mean_average_precision=0.329515, validation/accuracy=0.986455, validation/loss=0.046015, validation/mean_average_precision=0.239038, validation/num_examples=43793
I0520 09:04:53.344414 139871674509056 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.014723, loss=0.038101
I0520 09:04:53.349019 139923615557440 submission.py:296] 8000) loss = 0.038, grad_norm = 0.015
I0520 09:07:23.404713 139871682901760 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.011056, loss=0.039936
I0520 09:07:23.412719 139923615557440 submission.py:296] 8500) loss = 0.040, grad_norm = 0.011
I0520 09:08:53.078742 139923615557440 spec.py:298] Evaluating on the training split.
I0520 09:09:53.201796 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 09:09:56.463607 139923615557440 spec.py:326] Evaluating on the test split.
I0520 09:09:59.731317 139923615557440 submission_runner.py:421] Time since start: 3523.53s, 	Step: 8802, 	{'train/accuracy': 0.9900331597445273, 'train/loss': 0.03337267354184221, 'train/mean_average_precision': 0.35350068826388387, 'validation/accuracy': 0.9865966500149792, 'validation/loss': 0.04514889892137497, 'validation/mean_average_precision': 0.24455027687394215, 'validation/num_examples': 43793, 'test/accuracy': 0.9857644620354065, 'test/loss': 0.047751101397776684, 'test/mean_average_precision': 0.24562697094327232, 'test/num_examples': 43793, 'score': 2645.0635714530945, 'total_duration': 3523.5273826122284, 'accumulated_submission_time': 2645.0635714530945, 'accumulated_eval_time': 876.2374141216278, 'accumulated_logging_time': 0.2599971294403076}
I0520 09:09:59.742027 139871674509056 logging_writer.py:48] [8802] accumulated_eval_time=876.237414, accumulated_logging_time=0.259997, accumulated_submission_time=2645.063571, global_step=8802, preemption_count=0, score=2645.063571, test/accuracy=0.985764, test/loss=0.047751, test/mean_average_precision=0.245627, test/num_examples=43793, total_duration=3523.527383, train/accuracy=0.990033, train/loss=0.033373, train/mean_average_precision=0.353501, validation/accuracy=0.986597, validation/loss=0.045149, validation/mean_average_precision=0.244550, validation/num_examples=43793
I0520 09:10:59.053871 139871682901760 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.013347, loss=0.034377
I0520 09:10:59.059299 139923615557440 submission.py:296] 9000) loss = 0.034, grad_norm = 0.013
I0520 09:13:27.476256 139871674509056 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.014618, loss=0.036685
I0520 09:13:27.482712 139923615557440 submission.py:296] 9500) loss = 0.037, grad_norm = 0.015
I0520 09:13:59.795153 139923615557440 spec.py:298] Evaluating on the training split.
I0520 09:15:00.737312 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 09:15:04.061415 139923615557440 spec.py:326] Evaluating on the test split.
I0520 09:15:07.273238 139923615557440 submission_runner.py:421] Time since start: 3831.07s, 	Step: 9610, 	{'train/accuracy': 0.9902096809671445, 'train/loss': 0.03250648366883259, 'train/mean_average_precision': 0.36816196564899195, 'validation/accuracy': 0.9865487490125493, 'validation/loss': 0.045181358320201155, 'validation/mean_average_precision': 0.24303316611155562, 'validation/num_examples': 43793, 'test/accuracy': 0.9857290816818641, 'test/loss': 0.04797662811752511, 'test/mean_average_precision': 0.24797430404614781, 'test/num_examples': 43793, 'score': 2884.927171945572, 'total_duration': 3831.069319009781, 'accumulated_submission_time': 2884.927171945572, 'accumulated_eval_time': 943.7152621746063, 'accumulated_logging_time': 0.2818603515625}
I0520 09:15:07.283875 139871682901760 logging_writer.py:48] [9610] accumulated_eval_time=943.715262, accumulated_logging_time=0.281860, accumulated_submission_time=2884.927172, global_step=9610, preemption_count=0, score=2884.927172, test/accuracy=0.985729, test/loss=0.047977, test/mean_average_precision=0.247974, test/num_examples=43793, total_duration=3831.069319, train/accuracy=0.990210, train/loss=0.032506, train/mean_average_precision=0.368162, validation/accuracy=0.986549, validation/loss=0.045181, validation/mean_average_precision=0.243033, validation/num_examples=43793
I0520 09:17:03.772180 139871674509056 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.012900, loss=0.033336
I0520 09:17:03.778082 139923615557440 submission.py:296] 10000) loss = 0.033, grad_norm = 0.013
I0520 09:19:07.371088 139923615557440 spec.py:298] Evaluating on the training split.
I0520 09:20:08.186463 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 09:20:11.477121 139923615557440 spec.py:326] Evaluating on the test split.
I0520 09:20:14.751839 139923615557440 submission_runner.py:421] Time since start: 4138.55s, 	Step: 10416, 	{'train/accuracy': 0.9906282250414882, 'train/loss': 0.03117940031722155, 'train/mean_average_precision': 0.3946850612178842, 'validation/accuracy': 0.9867732342188523, 'validation/loss': 0.04445995729909792, 'validation/mean_average_precision': 0.2557663162868069, 'validation/num_examples': 43793, 'test/accuracy': 0.9859422061924885, 'test/loss': 0.0470593681016208, 'test/mean_average_precision': 0.2573346544482099, 'test/num_examples': 43793, 'score': 3124.8261091709137, 'total_duration': 4138.547799348831, 'accumulated_submission_time': 3124.8261091709137, 'accumulated_eval_time': 1011.0956690311432, 'accumulated_logging_time': 0.30385828018188477}
I0520 09:20:14.762978 139871682901760 logging_writer.py:48] [10416] accumulated_eval_time=1011.095669, accumulated_logging_time=0.303858, accumulated_submission_time=3124.826109, global_step=10416, preemption_count=0, score=3124.826109, test/accuracy=0.985942, test/loss=0.047059, test/mean_average_precision=0.257335, test/num_examples=43793, total_duration=4138.547799, train/accuracy=0.990628, train/loss=0.031179, train/mean_average_precision=0.394685, validation/accuracy=0.986773, validation/loss=0.044460, validation/mean_average_precision=0.255766, validation/num_examples=43793
I0520 09:20:40.135545 139871674509056 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.009490, loss=0.034324
I0520 09:20:40.141389 139923615557440 submission.py:296] 10500) loss = 0.034, grad_norm = 0.009
I0520 09:23:09.257915 139871682901760 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.010764, loss=0.038545
I0520 09:23:09.264255 139923615557440 submission.py:296] 11000) loss = 0.039, grad_norm = 0.011
I0520 09:24:15.007757 139923615557440 spec.py:298] Evaluating on the training split.
I0520 09:25:16.383625 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 09:25:19.704981 139923615557440 spec.py:326] Evaluating on the test split.
I0520 09:25:23.002825 139923615557440 submission_runner.py:421] Time since start: 4446.80s, 	Step: 11219, 	{'train/accuracy': 0.9906666186051889, 'train/loss': 0.03096438875944462, 'train/mean_average_precision': 0.4052065781834726, 'validation/accuracy': 0.9868268183910621, 'validation/loss': 0.04466161468393051, 'validation/mean_average_precision': 0.2590052667489329, 'validation/num_examples': 43793, 'test/accuracy': 0.9859240948210323, 'test/loss': 0.04740519197211354, 'test/mean_average_precision': 0.2552654214263252, 'test/num_examples': 43793, 'score': 3364.880858182907, 'total_duration': 4446.798855781555, 'accumulated_submission_time': 3364.880858182907, 'accumulated_eval_time': 1079.0904786586761, 'accumulated_logging_time': 0.32578396797180176}
I0520 09:25:23.013466 139871674509056 logging_writer.py:48] [11219] accumulated_eval_time=1079.090479, accumulated_logging_time=0.325784, accumulated_submission_time=3364.880858, global_step=11219, preemption_count=0, score=3364.880858, test/accuracy=0.985924, test/loss=0.047405, test/mean_average_precision=0.255265, test/num_examples=43793, total_duration=4446.798856, train/accuracy=0.990667, train/loss=0.030964, train/mean_average_precision=0.405207, validation/accuracy=0.986827, validation/loss=0.044662, validation/mean_average_precision=0.259005, validation/num_examples=43793
I0520 09:26:47.342608 139871682901760 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.010046, loss=0.035677
I0520 09:26:47.348223 139923615557440 submission.py:296] 11500) loss = 0.036, grad_norm = 0.010
I0520 09:29:16.888951 139923615557440 spec.py:298] Evaluating on the training split.
I0520 09:30:17.974288 139923615557440 spec.py:310] Evaluating on the validation split.
I0520 09:30:21.275742 139923615557440 spec.py:326] Evaluating on the test split.
I0520 09:30:24.514611 139923615557440 submission_runner.py:421] Time since start: 4748.31s, 	Step: 12000, 	{'train/accuracy': 0.9909063006093297, 'train/loss': 0.029930730083182176, 'train/mean_average_precision': 0.4265837156152665, 'validation/accuracy': 0.9867561847095129, 'validation/loss': 0.04491170269694822, 'validation/mean_average_precision': 0.25943362779772877, 'validation/num_examples': 43793, 'test/accuracy': 0.9858718666800887, 'test/loss': 0.047839433820627504, 'test/mean_average_precision': 0.25487176663016586, 'test/num_examples': 43793, 'score': 3598.5672850608826, 'total_duration': 4748.310679197311, 'accumulated_submission_time': 3598.5672850608826, 'accumulated_eval_time': 1146.7158689498901, 'accumulated_logging_time': 0.34818029403686523}
I0520 09:30:24.525141 139871674509056 logging_writer.py:48] [12000] accumulated_eval_time=1146.715869, accumulated_logging_time=0.348180, accumulated_submission_time=3598.567285, global_step=12000, preemption_count=0, score=3598.567285, test/accuracy=0.985872, test/loss=0.047839, test/mean_average_precision=0.254872, test/num_examples=43793, total_duration=4748.310679, train/accuracy=0.990906, train/loss=0.029931, train/mean_average_precision=0.426584, validation/accuracy=0.986756, validation/loss=0.044912, validation/mean_average_precision=0.259434, validation/num_examples=43793
I0520 09:30:24.545746 139871682901760 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3598.567285
I0520 09:30:24.643493 139923615557440 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_12000.
I0520 09:30:24.816757 139923615557440 submission_runner.py:584] Tuning trial 1/1
I0520 09:30:24.816983 139923615557440 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0520 09:30:24.818489 139923615557440 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5056398465104689, 'train/loss': 0.7614446830209161, 'train/mean_average_precision': 0.020061397106703308, 'validation/accuracy': 0.5151878652958861, 'validation/loss': 0.7520013383864832, 'validation/mean_average_precision': 0.023434862810344396, 'validation/num_examples': 43793, 'test/accuracy': 0.5163408795977086, 'test/loss': 0.750518069462585, 'test/mean_average_precision': 0.025403965380798324, 'test/num_examples': 43793, 'score': 5.4310407638549805, 'total_duration': 156.46069288253784, 'accumulated_submission_time': 5.4310407638549805, 'accumulated_eval_time': 151.02845454216003, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (798, {'train/accuracy': 0.9867183538577873, 'train/loss': 0.053905786017857775, 'train/mean_average_precision': 0.05292351836555613, 'validation/accuracy': 0.9841212236351665, 'validation/loss': 0.06348822457370137, 'validation/mean_average_precision': 0.053573367125372494, 'validation/num_examples': 43793, 'test/accuracy': 0.9831492642360644, 'test/loss': 0.0668356464643444, 'test/mean_average_precision': 0.0544183036269973, 'test/num_examples': 43793, 'score': 245.5354359149933, 'total_duration': 460.28461813926697, 'accumulated_submission_time': 245.5354359149933, 'accumulated_eval_time': 214.5390067100525, 'accumulated_logging_time': 0.02620100975036621, 'global_step': 798, 'preemption_count': 0}), (1594, {'train/accuracy': 0.9876167533570744, 'train/loss': 0.04467179831929132, 'train/mean_average_precision': 0.13129945043142555, 'validation/accuracy': 0.9848628772914337, 'validation/loss': 0.053962597740371696, 'validation/mean_average_precision': 0.1254587618814877, 'validation/num_examples': 43793, 'test/accuracy': 0.9838636104218728, 'test/loss': 0.05713847464976609, 'test/mean_average_precision': 0.12171744178845405, 'test/num_examples': 43793, 'score': 485.3691461086273, 'total_duration': 765.0270464420319, 'accumulated_submission_time': 485.3691461086273, 'accumulated_eval_time': 279.2474012374878, 'accumulated_logging_time': 0.04951167106628418, 'global_step': 1594, 'preemption_count': 0}), (2392, {'train/accuracy': 0.9881636178985944, 'train/loss': 0.042234313305993795, 'train/mean_average_precision': 0.17436598246943455, 'validation/accuracy': 0.9851758575700228, 'validation/loss': 0.05158150817321003, 'validation/mean_average_precision': 0.1555115334408861, 'validation/num_examples': 43793, 'test/accuracy': 0.9842481611693038, 'test/loss': 0.05408809718561924, 'test/mean_average_precision': 0.15620595120994107, 'test/num_examples': 43793, 'score': 725.2667095661163, 'total_duration': 1070.601625919342, 'accumulated_submission_time': 725.2667095661163, 'accumulated_eval_time': 344.7158131599426, 'accumulated_logging_time': 0.0808875560760498, 'global_step': 2392, 'preemption_count': 0}), (3186, {'train/accuracy': 0.9881962568787462, 'train/loss': 0.04076877575635545, 'train/mean_average_precision': 0.19080390786512927, 'validation/accuracy': 0.9854348477357033, 'validation/loss': 0.04982229755635878, 'validation/mean_average_precision': 0.1783507154657079, 'validation/num_examples': 43793, 'test/accuracy': 0.9845274132454778, 'test/loss': 0.052613942112687266, 'test/mean_average_precision': 0.17981449856166187, 'test/num_examples': 43793, 'score': 965.193056344986, 'total_duration': 1376.7619318962097, 'accumulated_submission_time': 965.193056344986, 'accumulated_eval_time': 410.7527940273285, 'accumulated_logging_time': 0.10181355476379395, 'global_step': 3186, 'preemption_count': 0}), (3970, {'train/accuracy': 0.9886336725578807, 'train/loss': 0.039247502410507805, 'train/mean_average_precision': 0.22713203510404706, 'validation/accuracy': 0.9856739468071546, 'validation/loss': 0.0486817691068168, 'validation/mean_average_precision': 0.1867612746196072, 'validation/num_examples': 43793, 'test/accuracy': 0.9847649670478338, 'test/loss': 0.05148620650585734, 'test/mean_average_precision': 0.1920014621008941, 'test/num_examples': 43793, 'score': 1205.1316895484924, 'total_duration': 1684.2630410194397, 'accumulated_submission_time': 1205.1316895484924, 'accumulated_eval_time': 478.11580657958984, 'accumulated_logging_time': 0.12392473220825195, 'global_step': 3970, 'preemption_count': 0}), (4757, {'train/accuracy': 0.9888880104897938, 'train/loss': 0.03769673538485742, 'train/mean_average_precision': 0.2400557250809326, 'validation/accuracy': 0.986030768681188, 'validation/loss': 0.04720989473348775, 'validation/mean_average_precision': 0.20927988778717083, 'validation/num_examples': 43793, 'test/accuracy': 0.9851263520875883, 'test/loss': 0.049926104683095225, 'test/mean_average_precision': 0.2062239633173526, 'test/num_examples': 43793, 'score': 1445.1914274692535, 'total_duration': 1991.943438053131, 'accumulated_submission_time': 1445.1914274692535, 'accumulated_eval_time': 545.5364081859589, 'accumulated_logging_time': 0.14581990242004395, 'global_step': 4757, 'preemption_count': 0}), (5561, {'train/accuracy': 0.9892034154292465, 'train/loss': 0.03662500703415036, 'train/mean_average_precision': 0.2720163445721131, 'validation/accuracy': 0.9861610756454254, 'validation/loss': 0.04664325226088672, 'validation/mean_average_precision': 0.21488246926308774, 'validation/num_examples': 43793, 'test/accuracy': 0.9853087293862056, 'test/loss': 0.04923485181212695, 'test/mean_average_precision': 0.21450469193760677, 'test/num_examples': 43793, 'score': 1685.2184610366821, 'total_duration': 2298.0714888572693, 'accumulated_submission_time': 1685.2184610366821, 'accumulated_eval_time': 611.4370291233063, 'accumulated_logging_time': 0.1669173240661621, 'global_step': 5561, 'preemption_count': 0}), (6377, {'train/accuracy': 0.9894478300442232, 'train/loss': 0.0357380471714819, 'train/mean_average_precision': 0.2908440979582141, 'validation/accuracy': 0.9862714915154335, 'validation/loss': 0.046615791006911544, 'validation/mean_average_precision': 0.22848088835180294, 'validation/num_examples': 43793, 'test/accuracy': 0.9854312970395489, 'test/loss': 0.04923103802587736, 'test/mean_average_precision': 0.22436566969424937, 'test/num_examples': 43793, 'score': 1925.2947688102722, 'total_duration': 2603.7178659439087, 'accumulated_submission_time': 1925.2947688102722, 'accumulated_eval_time': 676.8005027770996, 'accumulated_logging_time': 0.19359993934631348, 'global_step': 6377, 'preemption_count': 0}), (7191, {'train/accuracy': 0.9894466177951716, 'train/loss': 0.035782570773564, 'train/mean_average_precision': 0.2907682412963136, 'validation/accuracy': 0.9862292736827833, 'validation/loss': 0.04660637825696371, 'validation/mean_average_precision': 0.22815404763915836, 'validation/num_examples': 43793, 'test/accuracy': 0.985374014562385, 'test/loss': 0.049215588736453854, 'test/mean_average_precision': 0.23251257676807974, 'test/num_examples': 43793, 'score': 2165.166445016861, 'total_duration': 2909.8723697662354, 'accumulated_submission_time': 2165.166445016861, 'accumulated_eval_time': 742.8815128803253, 'accumulated_logging_time': 0.21563053131103516, 'global_step': 7191, 'preemption_count': 0}), (8000, {'train/accuracy': 0.9895963711086109, 'train/loss': 0.03462295638091727, 'train/mean_average_precision': 0.3295150473854438, 'validation/accuracy': 0.9864545707704836, 'validation/loss': 0.04601538251386084, 'validation/mean_average_precision': 0.2390377085108702, 'validation/num_examples': 43793, 'test/accuracy': 0.9854692045612015, 'test/loss': 0.04889621128536403, 'test/mean_average_precision': 0.24063529349276105, 'test/num_examples': 43793, 'score': 2405.2018201351166, 'total_duration': 3216.8137714862823, 'accumulated_submission_time': 2405.2018201351166, 'accumulated_eval_time': 809.5850596427917, 'accumulated_logging_time': 0.23758506774902344, 'global_step': 8000, 'preemption_count': 0}), (8802, {'train/accuracy': 0.9900331597445273, 'train/loss': 0.03337267354184221, 'train/mean_average_precision': 0.35350068826388387, 'validation/accuracy': 0.9865966500149792, 'validation/loss': 0.04514889892137497, 'validation/mean_average_precision': 0.24455027687394215, 'validation/num_examples': 43793, 'test/accuracy': 0.9857644620354065, 'test/loss': 0.047751101397776684, 'test/mean_average_precision': 0.24562697094327232, 'test/num_examples': 43793, 'score': 2645.0635714530945, 'total_duration': 3523.5273826122284, 'accumulated_submission_time': 2645.0635714530945, 'accumulated_eval_time': 876.2374141216278, 'accumulated_logging_time': 0.2599971294403076, 'global_step': 8802, 'preemption_count': 0}), (9610, {'train/accuracy': 0.9902096809671445, 'train/loss': 0.03250648366883259, 'train/mean_average_precision': 0.36816196564899195, 'validation/accuracy': 0.9865487490125493, 'validation/loss': 0.045181358320201155, 'validation/mean_average_precision': 0.24303316611155562, 'validation/num_examples': 43793, 'test/accuracy': 0.9857290816818641, 'test/loss': 0.04797662811752511, 'test/mean_average_precision': 0.24797430404614781, 'test/num_examples': 43793, 'score': 2884.927171945572, 'total_duration': 3831.069319009781, 'accumulated_submission_time': 2884.927171945572, 'accumulated_eval_time': 943.7152621746063, 'accumulated_logging_time': 0.2818603515625, 'global_step': 9610, 'preemption_count': 0}), (10416, {'train/accuracy': 0.9906282250414882, 'train/loss': 0.03117940031722155, 'train/mean_average_precision': 0.3946850612178842, 'validation/accuracy': 0.9867732342188523, 'validation/loss': 0.04445995729909792, 'validation/mean_average_precision': 0.2557663162868069, 'validation/num_examples': 43793, 'test/accuracy': 0.9859422061924885, 'test/loss': 0.0470593681016208, 'test/mean_average_precision': 0.2573346544482099, 'test/num_examples': 43793, 'score': 3124.8261091709137, 'total_duration': 4138.547799348831, 'accumulated_submission_time': 3124.8261091709137, 'accumulated_eval_time': 1011.0956690311432, 'accumulated_logging_time': 0.30385828018188477, 'global_step': 10416, 'preemption_count': 0}), (11219, {'train/accuracy': 0.9906666186051889, 'train/loss': 0.03096438875944462, 'train/mean_average_precision': 0.4052065781834726, 'validation/accuracy': 0.9868268183910621, 'validation/loss': 0.04466161468393051, 'validation/mean_average_precision': 0.2590052667489329, 'validation/num_examples': 43793, 'test/accuracy': 0.9859240948210323, 'test/loss': 0.04740519197211354, 'test/mean_average_precision': 0.2552654214263252, 'test/num_examples': 43793, 'score': 3364.880858182907, 'total_duration': 4446.798855781555, 'accumulated_submission_time': 3364.880858182907, 'accumulated_eval_time': 1079.0904786586761, 'accumulated_logging_time': 0.32578396797180176, 'global_step': 11219, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9909063006093297, 'train/loss': 0.029930730083182176, 'train/mean_average_precision': 0.4265837156152665, 'validation/accuracy': 0.9867561847095129, 'validation/loss': 0.04491170269694822, 'validation/mean_average_precision': 0.25943362779772877, 'validation/num_examples': 43793, 'test/accuracy': 0.9858718666800887, 'test/loss': 0.047839433820627504, 'test/mean_average_precision': 0.25487176663016586, 'test/num_examples': 43793, 'score': 3598.5672850608826, 'total_duration': 4748.310679197311, 'accumulated_submission_time': 3598.5672850608826, 'accumulated_eval_time': 1146.7158689498901, 'accumulated_logging_time': 0.34818029403686523, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0520 09:30:24.818630 139923615557440 submission_runner.py:587] Timing: 3598.5672850608826
I0520 09:30:24.818687 139923615557440 submission_runner.py:588] ====================
I0520 09:30:24.818839 139923615557440 submission_runner.py:651] Final ogbg score: 3598.5672850608826
