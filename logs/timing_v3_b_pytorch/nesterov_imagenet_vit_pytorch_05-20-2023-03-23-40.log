torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_b_pytorch/timing_nesterov --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_vit_pytorch_05-20-2023-03-23-40.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0520 03:24:04.263876 139741616723776 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0520 03:24:04.263899 140453048964928 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0520 03:24:04.263928 140335360665408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0520 03:24:04.264687 140683238561600 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0520 03:24:04.265060 139634097223488 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0520 03:24:04.265222 139752972392256 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0520 03:24:05.252528 140147440625472 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0520 03:24:05.261839 140384716638016 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0520 03:24:05.262206 140384716638016 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:24:05.263105 140147440625472 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:24:05.271315 139741616723776 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:24:05.271338 140453048964928 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:24:05.271364 140335360665408 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:24:05.271422 139634097223488 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:24:05.271440 140683238561600 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:24:05.271425 139752972392256 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:24:07.546423 140384716638016 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_b_pytorch/timing_nesterov/imagenet_vit_pytorch.
W0520 03:24:07.668164 139752972392256 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:24:07.669241 140384716638016 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:24:07.669382 140683238561600 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:24:07.670099 139741616723776 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:24:07.670859 140335360665408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:24:07.671221 139634097223488 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:24:07.671396 140147440625472 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:24:07.671432 140453048964928 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0520 03:24:07.674710 140384716638016 submission_runner.py:544] Using RNG seed 806618763
I0520 03:24:07.676217 140384716638016 submission_runner.py:553] --- Tuning run 1/1 ---
I0520 03:24:07.676339 140384716638016 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_b_pytorch/timing_nesterov/imagenet_vit_pytorch/trial_1.
I0520 03:24:07.676640 140384716638016 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_b_pytorch/timing_nesterov/imagenet_vit_pytorch/trial_1/hparams.json.
I0520 03:24:07.677658 140384716638016 submission_runner.py:241] Initializing dataset.
I0520 03:24:14.157709 140384716638016 submission_runner.py:248] Initializing model.
I0520 03:24:18.437011 140384716638016 submission_runner.py:258] Initializing optimizer.
I0520 03:24:18.924745 140384716638016 submission_runner.py:265] Initializing metrics bundle.
I0520 03:24:18.924969 140384716638016 submission_runner.py:283] Initializing checkpoint and logger.
I0520 03:24:19.482835 140384716638016 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_b_pytorch/timing_nesterov/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0520 03:24:19.483854 140384716638016 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_b_pytorch/timing_nesterov/imagenet_vit_pytorch/trial_1/flags_0.json.
I0520 03:24:19.541745 140384716638016 submission_runner.py:319] Starting training loop.
I0520 03:24:26.246741 140355798296320 logging_writer.py:48] [0] global_step=0, grad_norm=0.305298, loss=6.907755
I0520 03:24:26.280458 140384716638016 submission.py:139] 0) loss = 6.908, grad_norm = 0.305
I0520 03:24:26.281624 140384716638016 spec.py:298] Evaluating on the training split.
I0520 03:25:27.007066 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 03:26:21.518941 140384716638016 spec.py:326] Evaluating on the test split.
I0520 03:26:21.539488 140384716638016 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0520 03:26:21.545630 140384716638016 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0520 03:26:21.626889 140384716638016 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0520 03:26:33.902235 140384716638016 submission_runner.py:421] Time since start: 134.36s, 	Step: 1, 	{'train/accuracy': 0.0010546875, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.739134311676025, 'total_duration': 134.3608717918396, 'accumulated_submission_time': 6.739134311676025, 'accumulated_eval_time': 127.62046074867249, 'accumulated_logging_time': 0}
I0520 03:26:33.919493 140350773524224 logging_writer.py:48] [1] accumulated_eval_time=127.620461, accumulated_logging_time=0, accumulated_submission_time=6.739134, global_step=1, preemption_count=0, score=6.739134, test/accuracy=0.001000, test/loss=6.907755, test/num_examples=10000, total_duration=134.360872, train/accuracy=0.001055, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0520 03:26:33.939463 140384716638016 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:26:33.939468 140147440625472 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:26:33.939937 139741616723776 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:26:33.939929 140335360665408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:26:33.939948 139752972392256 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:26:33.939979 139634097223488 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:26:33.939978 140453048964928 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:26:33.940164 140683238561600 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:26:34.489336 140350765131520 logging_writer.py:48] [1] global_step=1, grad_norm=0.307207, loss=6.907754
I0520 03:26:34.492791 140384716638016 submission.py:139] 1) loss = 6.908, grad_norm = 0.307
I0520 03:26:34.915554 140350773524224 logging_writer.py:48] [2] global_step=2, grad_norm=0.304378, loss=6.907754
I0520 03:26:34.919425 140384716638016 submission.py:139] 2) loss = 6.908, grad_norm = 0.304
I0520 03:26:35.312157 140350765131520 logging_writer.py:48] [3] global_step=3, grad_norm=0.293853, loss=6.907754
I0520 03:26:35.315891 140384716638016 submission.py:139] 3) loss = 6.908, grad_norm = 0.294
I0520 03:26:35.714230 140350773524224 logging_writer.py:48] [4] global_step=4, grad_norm=0.291708, loss=6.907754
I0520 03:26:35.717910 140384716638016 submission.py:139] 4) loss = 6.908, grad_norm = 0.292
I0520 03:26:36.112278 140350765131520 logging_writer.py:48] [5] global_step=5, grad_norm=0.299385, loss=6.907754
I0520 03:26:36.116998 140384716638016 submission.py:139] 5) loss = 6.908, grad_norm = 0.299
I0520 03:26:36.511917 140350773524224 logging_writer.py:48] [6] global_step=6, grad_norm=0.310384, loss=6.907748
I0520 03:26:36.517271 140384716638016 submission.py:139] 6) loss = 6.908, grad_norm = 0.310
I0520 03:26:36.917995 140350765131520 logging_writer.py:48] [7] global_step=7, grad_norm=0.310676, loss=6.907742
I0520 03:26:36.922524 140384716638016 submission.py:139] 7) loss = 6.908, grad_norm = 0.311
I0520 03:26:37.323297 140350773524224 logging_writer.py:48] [8] global_step=8, grad_norm=0.303599, loss=6.907739
I0520 03:26:37.327256 140384716638016 submission.py:139] 8) loss = 6.908, grad_norm = 0.304
I0520 03:26:37.724729 140350765131520 logging_writer.py:48] [9] global_step=9, grad_norm=0.304939, loss=6.907743
I0520 03:26:37.728780 140384716638016 submission.py:139] 9) loss = 6.908, grad_norm = 0.305
I0520 03:26:38.123855 140350773524224 logging_writer.py:48] [10] global_step=10, grad_norm=0.299295, loss=6.907734
I0520 03:26:38.128934 140384716638016 submission.py:139] 10) loss = 6.908, grad_norm = 0.299
I0520 03:26:38.525438 140350765131520 logging_writer.py:48] [11] global_step=11, grad_norm=0.302852, loss=6.907720
I0520 03:26:38.530244 140384716638016 submission.py:139] 11) loss = 6.908, grad_norm = 0.303
I0520 03:26:38.928605 140350773524224 logging_writer.py:48] [12] global_step=12, grad_norm=0.301537, loss=6.907736
I0520 03:26:38.933314 140384716638016 submission.py:139] 12) loss = 6.908, grad_norm = 0.302
I0520 03:26:39.332652 140350765131520 logging_writer.py:48] [13] global_step=13, grad_norm=0.301133, loss=6.907722
I0520 03:26:39.337095 140384716638016 submission.py:139] 13) loss = 6.908, grad_norm = 0.301
I0520 03:26:39.734043 140350773524224 logging_writer.py:48] [14] global_step=14, grad_norm=0.308574, loss=6.907726
I0520 03:26:39.739154 140384716638016 submission.py:139] 14) loss = 6.908, grad_norm = 0.309
I0520 03:26:40.134493 140350765131520 logging_writer.py:48] [15] global_step=15, grad_norm=0.298693, loss=6.907743
I0520 03:26:40.139163 140384716638016 submission.py:139] 15) loss = 6.908, grad_norm = 0.299
I0520 03:26:40.534312 140350773524224 logging_writer.py:48] [16] global_step=16, grad_norm=0.295735, loss=6.907703
I0520 03:26:40.539511 140384716638016 submission.py:139] 16) loss = 6.908, grad_norm = 0.296
I0520 03:26:40.936310 140350765131520 logging_writer.py:48] [17] global_step=17, grad_norm=0.300053, loss=6.907675
I0520 03:26:40.940352 140384716638016 submission.py:139] 17) loss = 6.908, grad_norm = 0.300
I0520 03:26:41.337479 140350773524224 logging_writer.py:48] [18] global_step=18, grad_norm=0.310002, loss=6.907649
I0520 03:26:41.342203 140384716638016 submission.py:139] 18) loss = 6.908, grad_norm = 0.310
I0520 03:26:41.740820 140350765131520 logging_writer.py:48] [19] global_step=19, grad_norm=0.301031, loss=6.907670
I0520 03:26:41.746314 140384716638016 submission.py:139] 19) loss = 6.908, grad_norm = 0.301
I0520 03:26:42.144740 140350773524224 logging_writer.py:48] [20] global_step=20, grad_norm=0.304252, loss=6.907642
I0520 03:26:42.150126 140384716638016 submission.py:139] 20) loss = 6.908, grad_norm = 0.304
I0520 03:26:42.550098 140350765131520 logging_writer.py:48] [21] global_step=21, grad_norm=0.300775, loss=6.907609
I0520 03:26:42.555062 140384716638016 submission.py:139] 21) loss = 6.908, grad_norm = 0.301
I0520 03:26:42.959271 140350773524224 logging_writer.py:48] [22] global_step=22, grad_norm=0.301375, loss=6.907664
I0520 03:26:42.965327 140384716638016 submission.py:139] 22) loss = 6.908, grad_norm = 0.301
I0520 03:26:43.366389 140350765131520 logging_writer.py:48] [23] global_step=23, grad_norm=0.295302, loss=6.907605
I0520 03:26:43.374776 140384716638016 submission.py:139] 23) loss = 6.908, grad_norm = 0.295
I0520 03:26:43.779232 140350773524224 logging_writer.py:48] [24] global_step=24, grad_norm=0.304935, loss=6.907536
I0520 03:26:43.784261 140384716638016 submission.py:139] 24) loss = 6.908, grad_norm = 0.305
I0520 03:26:44.189633 140350765131520 logging_writer.py:48] [25] global_step=25, grad_norm=0.303177, loss=6.907551
I0520 03:26:44.194365 140384716638016 submission.py:139] 25) loss = 6.908, grad_norm = 0.303
I0520 03:26:44.591569 140350773524224 logging_writer.py:48] [26] global_step=26, grad_norm=0.306215, loss=6.907542
I0520 03:26:44.596343 140384716638016 submission.py:139] 26) loss = 6.908, grad_norm = 0.306
I0520 03:26:45.002344 140350765131520 logging_writer.py:48] [27] global_step=27, grad_norm=0.298999, loss=6.907581
I0520 03:26:45.006745 140384716638016 submission.py:139] 27) loss = 6.908, grad_norm = 0.299
I0520 03:26:45.411753 140350773524224 logging_writer.py:48] [28] global_step=28, grad_norm=0.308156, loss=6.907445
I0520 03:26:45.416997 140384716638016 submission.py:139] 28) loss = 6.907, grad_norm = 0.308
I0520 03:26:45.817286 140350765131520 logging_writer.py:48] [29] global_step=29, grad_norm=0.299213, loss=6.907598
I0520 03:26:45.821827 140384716638016 submission.py:139] 29) loss = 6.908, grad_norm = 0.299
I0520 03:26:46.217142 140350773524224 logging_writer.py:48] [30] global_step=30, grad_norm=0.297473, loss=6.907372
I0520 03:26:46.221743 140384716638016 submission.py:139] 30) loss = 6.907, grad_norm = 0.297
I0520 03:26:46.617698 140350765131520 logging_writer.py:48] [31] global_step=31, grad_norm=0.299690, loss=6.907488
I0520 03:26:46.622256 140384716638016 submission.py:139] 31) loss = 6.907, grad_norm = 0.300
I0520 03:26:47.021089 140350773524224 logging_writer.py:48] [32] global_step=32, grad_norm=0.299006, loss=6.907525
I0520 03:26:47.029892 140384716638016 submission.py:139] 32) loss = 6.908, grad_norm = 0.299
I0520 03:26:47.438337 140350765131520 logging_writer.py:48] [33] global_step=33, grad_norm=0.302514, loss=6.907397
I0520 03:26:47.443582 140384716638016 submission.py:139] 33) loss = 6.907, grad_norm = 0.303
I0520 03:26:47.852462 140350773524224 logging_writer.py:48] [34] global_step=34, grad_norm=0.302706, loss=6.907317
I0520 03:26:47.862006 140384716638016 submission.py:139] 34) loss = 6.907, grad_norm = 0.303
I0520 03:26:48.269217 140350765131520 logging_writer.py:48] [35] global_step=35, grad_norm=0.301003, loss=6.907346
I0520 03:26:48.273925 140384716638016 submission.py:139] 35) loss = 6.907, grad_norm = 0.301
I0520 03:26:48.680838 140350773524224 logging_writer.py:48] [36] global_step=36, grad_norm=0.304536, loss=6.907242
I0520 03:26:48.685146 140384716638016 submission.py:139] 36) loss = 6.907, grad_norm = 0.305
I0520 03:26:49.092658 140350765131520 logging_writer.py:48] [37] global_step=37, grad_norm=0.298562, loss=6.907382
I0520 03:26:49.096772 140384716638016 submission.py:139] 37) loss = 6.907, grad_norm = 0.299
I0520 03:26:49.493134 140350773524224 logging_writer.py:48] [38] global_step=38, grad_norm=0.294246, loss=6.907210
I0520 03:26:49.498583 140384716638016 submission.py:139] 38) loss = 6.907, grad_norm = 0.294
I0520 03:26:49.901803 140350765131520 logging_writer.py:48] [39] global_step=39, grad_norm=0.301400, loss=6.907241
I0520 03:26:49.905910 140384716638016 submission.py:139] 39) loss = 6.907, grad_norm = 0.301
I0520 03:26:50.313389 140350773524224 logging_writer.py:48] [40] global_step=40, grad_norm=0.303152, loss=6.907194
I0520 03:26:50.321551 140384716638016 submission.py:139] 40) loss = 6.907, grad_norm = 0.303
I0520 03:26:50.731605 140350765131520 logging_writer.py:48] [41] global_step=41, grad_norm=0.303814, loss=6.907115
I0520 03:26:50.736025 140384716638016 submission.py:139] 41) loss = 6.907, grad_norm = 0.304
I0520 03:26:51.144324 140350773524224 logging_writer.py:48] [42] global_step=42, grad_norm=0.291346, loss=6.907341
I0520 03:26:51.149140 140384716638016 submission.py:139] 42) loss = 6.907, grad_norm = 0.291
I0520 03:26:51.554383 140350765131520 logging_writer.py:48] [43] global_step=43, grad_norm=0.300371, loss=6.907150
I0520 03:26:51.559941 140384716638016 submission.py:139] 43) loss = 6.907, grad_norm = 0.300
I0520 03:26:51.971131 140350773524224 logging_writer.py:48] [44] global_step=44, grad_norm=0.303288, loss=6.906975
I0520 03:26:51.976236 140384716638016 submission.py:139] 44) loss = 6.907, grad_norm = 0.303
I0520 03:26:52.373545 140350765131520 logging_writer.py:48] [45] global_step=45, grad_norm=0.293011, loss=6.907116
I0520 03:26:52.378388 140384716638016 submission.py:139] 45) loss = 6.907, grad_norm = 0.293
I0520 03:26:52.774373 140350773524224 logging_writer.py:48] [46] global_step=46, grad_norm=0.307948, loss=6.906685
I0520 03:26:52.778885 140384716638016 submission.py:139] 46) loss = 6.907, grad_norm = 0.308
I0520 03:26:53.182199 140350765131520 logging_writer.py:48] [47] global_step=47, grad_norm=0.294330, loss=6.907012
I0520 03:26:53.188068 140384716638016 submission.py:139] 47) loss = 6.907, grad_norm = 0.294
I0520 03:26:53.611916 140350773524224 logging_writer.py:48] [48] global_step=48, grad_norm=0.295340, loss=6.906780
I0520 03:26:53.615728 140384716638016 submission.py:139] 48) loss = 6.907, grad_norm = 0.295
I0520 03:26:54.015068 140350765131520 logging_writer.py:48] [49] global_step=49, grad_norm=0.301212, loss=6.907139
I0520 03:26:54.021250 140384716638016 submission.py:139] 49) loss = 6.907, grad_norm = 0.301
I0520 03:26:54.424110 140350773524224 logging_writer.py:48] [50] global_step=50, grad_norm=0.298877, loss=6.906674
I0520 03:26:54.428716 140384716638016 submission.py:139] 50) loss = 6.907, grad_norm = 0.299
I0520 03:26:54.838463 140350765131520 logging_writer.py:48] [51] global_step=51, grad_norm=0.300803, loss=6.906855
I0520 03:26:54.844284 140384716638016 submission.py:139] 51) loss = 6.907, grad_norm = 0.301
I0520 03:26:55.252304 140350773524224 logging_writer.py:48] [52] global_step=52, grad_norm=0.297304, loss=6.907071
I0520 03:26:55.257791 140384716638016 submission.py:139] 52) loss = 6.907, grad_norm = 0.297
I0520 03:26:55.663800 140350765131520 logging_writer.py:48] [53] global_step=53, grad_norm=0.294359, loss=6.906573
I0520 03:26:55.668440 140384716638016 submission.py:139] 53) loss = 6.907, grad_norm = 0.294
I0520 03:26:56.067971 140350773524224 logging_writer.py:48] [54] global_step=54, grad_norm=0.302334, loss=6.906989
I0520 03:26:56.073273 140384716638016 submission.py:139] 54) loss = 6.907, grad_norm = 0.302
I0520 03:26:56.474523 140350765131520 logging_writer.py:48] [55] global_step=55, grad_norm=0.304693, loss=6.906533
I0520 03:26:56.479641 140384716638016 submission.py:139] 55) loss = 6.907, grad_norm = 0.305
I0520 03:26:56.877719 140350773524224 logging_writer.py:48] [56] global_step=56, grad_norm=0.298040, loss=6.906535
I0520 03:26:56.882047 140384716638016 submission.py:139] 56) loss = 6.907, grad_norm = 0.298
I0520 03:26:57.279531 140350765131520 logging_writer.py:48] [57] global_step=57, grad_norm=0.299166, loss=6.907184
I0520 03:26:57.285265 140384716638016 submission.py:139] 57) loss = 6.907, grad_norm = 0.299
I0520 03:26:57.681065 140350773524224 logging_writer.py:48] [58] global_step=58, grad_norm=0.311861, loss=6.906169
I0520 03:26:57.686287 140384716638016 submission.py:139] 58) loss = 6.906, grad_norm = 0.312
I0520 03:26:58.086345 140350765131520 logging_writer.py:48] [59] global_step=59, grad_norm=0.306065, loss=6.906611
I0520 03:26:58.091985 140384716638016 submission.py:139] 59) loss = 6.907, grad_norm = 0.306
I0520 03:26:58.492168 140350773524224 logging_writer.py:48] [60] global_step=60, grad_norm=0.295375, loss=6.905964
I0520 03:26:58.500284 140384716638016 submission.py:139] 60) loss = 6.906, grad_norm = 0.295
I0520 03:26:58.901993 140350765131520 logging_writer.py:48] [61] global_step=61, grad_norm=0.294496, loss=6.905821
I0520 03:26:58.907130 140384716638016 submission.py:139] 61) loss = 6.906, grad_norm = 0.294
I0520 03:26:59.306228 140350773524224 logging_writer.py:48] [62] global_step=62, grad_norm=0.294465, loss=6.906852
I0520 03:26:59.311914 140384716638016 submission.py:139] 62) loss = 6.907, grad_norm = 0.294
I0520 03:26:59.728175 140350765131520 logging_writer.py:48] [63] global_step=63, grad_norm=0.301873, loss=6.905802
I0520 03:26:59.733670 140384716638016 submission.py:139] 63) loss = 6.906, grad_norm = 0.302
I0520 03:27:00.131867 140350773524224 logging_writer.py:48] [64] global_step=64, grad_norm=0.302521, loss=6.905939
I0520 03:27:00.136970 140384716638016 submission.py:139] 64) loss = 6.906, grad_norm = 0.303
I0520 03:27:00.535444 140350765131520 logging_writer.py:48] [65] global_step=65, grad_norm=0.302073, loss=6.905726
I0520 03:27:00.540289 140384716638016 submission.py:139] 65) loss = 6.906, grad_norm = 0.302
I0520 03:27:00.940091 140350773524224 logging_writer.py:48] [66] global_step=66, grad_norm=0.295866, loss=6.905776
I0520 03:27:00.944278 140384716638016 submission.py:139] 66) loss = 6.906, grad_norm = 0.296
I0520 03:27:01.341528 140350765131520 logging_writer.py:48] [67] global_step=67, grad_norm=0.301292, loss=6.907127
I0520 03:27:01.347563 140384716638016 submission.py:139] 67) loss = 6.907, grad_norm = 0.301
I0520 03:27:01.761363 140350773524224 logging_writer.py:48] [68] global_step=68, grad_norm=0.301703, loss=6.906177
I0520 03:27:01.766801 140384716638016 submission.py:139] 68) loss = 6.906, grad_norm = 0.302
I0520 03:27:02.171952 140350765131520 logging_writer.py:48] [69] global_step=69, grad_norm=0.291654, loss=6.906237
I0520 03:27:02.177397 140384716638016 submission.py:139] 69) loss = 6.906, grad_norm = 0.292
I0520 03:27:02.579430 140350773524224 logging_writer.py:48] [70] global_step=70, grad_norm=0.301533, loss=6.905056
I0520 03:27:02.584564 140384716638016 submission.py:139] 70) loss = 6.905, grad_norm = 0.302
I0520 03:27:02.982573 140350765131520 logging_writer.py:48] [71] global_step=71, grad_norm=0.302326, loss=6.905775
I0520 03:27:02.987924 140384716638016 submission.py:139] 71) loss = 6.906, grad_norm = 0.302
I0520 03:27:03.404538 140350773524224 logging_writer.py:48] [72] global_step=72, grad_norm=0.306319, loss=6.904836
I0520 03:27:03.409470 140384716638016 submission.py:139] 72) loss = 6.905, grad_norm = 0.306
I0520 03:27:03.808508 140350765131520 logging_writer.py:48] [73] global_step=73, grad_norm=0.307094, loss=6.905849
I0520 03:27:03.814179 140384716638016 submission.py:139] 73) loss = 6.906, grad_norm = 0.307
I0520 03:27:04.219091 140350773524224 logging_writer.py:48] [74] global_step=74, grad_norm=0.293527, loss=6.905534
I0520 03:27:04.224797 140384716638016 submission.py:139] 74) loss = 6.906, grad_norm = 0.294
I0520 03:27:04.624277 140350765131520 logging_writer.py:48] [75] global_step=75, grad_norm=0.294284, loss=6.904934
I0520 03:27:04.628909 140384716638016 submission.py:139] 75) loss = 6.905, grad_norm = 0.294
I0520 03:27:05.028618 140350773524224 logging_writer.py:48] [76] global_step=76, grad_norm=0.302891, loss=6.904824
I0520 03:27:05.033031 140384716638016 submission.py:139] 76) loss = 6.905, grad_norm = 0.303
I0520 03:27:05.430359 140350765131520 logging_writer.py:48] [77] global_step=77, grad_norm=0.301582, loss=6.905440
I0520 03:27:05.434786 140384716638016 submission.py:139] 77) loss = 6.905, grad_norm = 0.302
I0520 03:27:05.834788 140350773524224 logging_writer.py:48] [78] global_step=78, grad_norm=0.295537, loss=6.905332
I0520 03:27:05.839702 140384716638016 submission.py:139] 78) loss = 6.905, grad_norm = 0.296
I0520 03:27:06.241027 140350765131520 logging_writer.py:48] [79] global_step=79, grad_norm=0.310455, loss=6.905446
I0520 03:27:06.246738 140384716638016 submission.py:139] 79) loss = 6.905, grad_norm = 0.310
I0520 03:27:06.650032 140350773524224 logging_writer.py:48] [80] global_step=80, grad_norm=0.296414, loss=6.905786
I0520 03:27:06.655378 140384716638016 submission.py:139] 80) loss = 6.906, grad_norm = 0.296
I0520 03:27:07.054786 140350765131520 logging_writer.py:48] [81] global_step=81, grad_norm=0.298003, loss=6.905914
I0520 03:27:07.060148 140384716638016 submission.py:139] 81) loss = 6.906, grad_norm = 0.298
I0520 03:27:07.458739 140350773524224 logging_writer.py:48] [82] global_step=82, grad_norm=0.295345, loss=6.904680
I0520 03:27:07.464133 140384716638016 submission.py:139] 82) loss = 6.905, grad_norm = 0.295
I0520 03:27:07.870970 140350765131520 logging_writer.py:48] [83] global_step=83, grad_norm=0.302352, loss=6.905343
I0520 03:27:07.876554 140384716638016 submission.py:139] 83) loss = 6.905, grad_norm = 0.302
I0520 03:27:08.276269 140350773524224 logging_writer.py:48] [84] global_step=84, grad_norm=0.297682, loss=6.905871
I0520 03:27:08.281824 140384716638016 submission.py:139] 84) loss = 6.906, grad_norm = 0.298
I0520 03:27:08.678668 140350765131520 logging_writer.py:48] [85] global_step=85, grad_norm=0.302984, loss=6.904355
I0520 03:27:08.684280 140384716638016 submission.py:139] 85) loss = 6.904, grad_norm = 0.303
I0520 03:27:09.088927 140350773524224 logging_writer.py:48] [86] global_step=86, grad_norm=0.304258, loss=6.904295
I0520 03:27:09.093768 140384716638016 submission.py:139] 86) loss = 6.904, grad_norm = 0.304
I0520 03:27:09.491717 140350765131520 logging_writer.py:48] [87] global_step=87, grad_norm=0.296638, loss=6.904617
I0520 03:27:09.496815 140384716638016 submission.py:139] 87) loss = 6.905, grad_norm = 0.297
I0520 03:27:09.894645 140350773524224 logging_writer.py:48] [88] global_step=88, grad_norm=0.302506, loss=6.903711
I0520 03:27:09.899486 140384716638016 submission.py:139] 88) loss = 6.904, grad_norm = 0.303
I0520 03:27:10.295478 140350765131520 logging_writer.py:48] [89] global_step=89, grad_norm=0.307842, loss=6.903204
I0520 03:27:10.300274 140384716638016 submission.py:139] 89) loss = 6.903, grad_norm = 0.308
I0520 03:27:10.708514 140350773524224 logging_writer.py:48] [90] global_step=90, grad_norm=0.306097, loss=6.904912
I0520 03:27:10.714228 140384716638016 submission.py:139] 90) loss = 6.905, grad_norm = 0.306
I0520 03:27:11.123513 140350765131520 logging_writer.py:48] [91] global_step=91, grad_norm=0.296776, loss=6.903346
I0520 03:27:11.129320 140384716638016 submission.py:139] 91) loss = 6.903, grad_norm = 0.297
I0520 03:27:11.540418 140350773524224 logging_writer.py:48] [92] global_step=92, grad_norm=0.306204, loss=6.903467
I0520 03:27:11.545731 140384716638016 submission.py:139] 92) loss = 6.903, grad_norm = 0.306
I0520 03:27:11.952900 140350765131520 logging_writer.py:48] [93] global_step=93, grad_norm=0.299329, loss=6.904300
I0520 03:27:11.958737 140384716638016 submission.py:139] 93) loss = 6.904, grad_norm = 0.299
I0520 03:27:12.359802 140350773524224 logging_writer.py:48] [94] global_step=94, grad_norm=0.297726, loss=6.903899
I0520 03:27:12.365310 140384716638016 submission.py:139] 94) loss = 6.904, grad_norm = 0.298
I0520 03:27:12.778097 140350765131520 logging_writer.py:48] [95] global_step=95, grad_norm=0.298843, loss=6.904208
I0520 03:27:12.784227 140384716638016 submission.py:139] 95) loss = 6.904, grad_norm = 0.299
I0520 03:27:13.195405 140350773524224 logging_writer.py:48] [96] global_step=96, grad_norm=0.297279, loss=6.904188
I0520 03:27:13.201438 140384716638016 submission.py:139] 96) loss = 6.904, grad_norm = 0.297
I0520 03:27:13.599007 140350765131520 logging_writer.py:48] [97] global_step=97, grad_norm=0.301546, loss=6.902124
I0520 03:27:13.606340 140384716638016 submission.py:139] 97) loss = 6.902, grad_norm = 0.302
I0520 03:27:14.010150 140350773524224 logging_writer.py:48] [98] global_step=98, grad_norm=0.302373, loss=6.903414
I0520 03:27:14.014626 140384716638016 submission.py:139] 98) loss = 6.903, grad_norm = 0.302
I0520 03:27:14.413759 140350765131520 logging_writer.py:48] [99] global_step=99, grad_norm=0.296799, loss=6.903347
I0520 03:27:14.419892 140384716638016 submission.py:139] 99) loss = 6.903, grad_norm = 0.297
I0520 03:27:14.820106 140350773524224 logging_writer.py:48] [100] global_step=100, grad_norm=0.297049, loss=6.903171
I0520 03:27:14.825073 140384716638016 submission.py:139] 100) loss = 6.903, grad_norm = 0.297
I0520 03:29:56.818864 140350765131520 logging_writer.py:48] [500] global_step=500, grad_norm=1.005002, loss=6.753343
I0520 03:29:56.826217 140384716638016 submission.py:139] 500) loss = 6.753, grad_norm = 1.005
I0520 03:33:19.293736 140350773524224 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.403291, loss=6.514362
I0520 03:33:19.300088 140384716638016 submission.py:139] 1000) loss = 6.514, grad_norm = 1.403
I0520 03:33:34.269892 140384716638016 spec.py:298] Evaluating on the training split.
I0520 03:34:17.963652 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 03:35:01.770904 140384716638016 spec.py:326] Evaluating on the test split.
I0520 03:35:03.238198 140384716638016 submission_runner.py:421] Time since start: 643.70s, 	Step: 1039, 	{'train/accuracy': 0.04443359375, 'train/loss': 5.947308349609375, 'validation/accuracy': 0.04176, 'validation/loss': 5.98111375, 'validation/num_examples': 50000, 'test/accuracy': 0.0298, 'test/loss': 6.101627734375, 'test/num_examples': 10000, 'score': 426.5390110015869, 'total_duration': 643.6969928741455, 'accumulated_submission_time': 426.5390110015869, 'accumulated_eval_time': 216.58877515792847, 'accumulated_logging_time': 0.025425195693969727}
I0520 03:35:03.248048 140339599898368 logging_writer.py:48] [1039] accumulated_eval_time=216.588775, accumulated_logging_time=0.025425, accumulated_submission_time=426.539011, global_step=1039, preemption_count=0, score=426.539011, test/accuracy=0.029800, test/loss=6.101628, test/num_examples=10000, total_duration=643.696993, train/accuracy=0.044434, train/loss=5.947308, validation/accuracy=0.041760, validation/loss=5.981114, validation/num_examples=50000
I0520 03:38:07.198790 140340220630784 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.922759, loss=6.293175
I0520 03:38:07.205974 140384716638016 submission.py:139] 1500) loss = 6.293, grad_norm = 0.923
I0520 03:41:21.116342 140339599898368 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.013613, loss=6.357317
I0520 03:41:21.122027 140384716638016 submission.py:139] 2000) loss = 6.357, grad_norm = 1.014
I0520 03:42:03.340274 140384716638016 spec.py:298] Evaluating on the training split.
I0520 03:42:48.001185 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 03:43:34.511272 140384716638016 spec.py:326] Evaluating on the test split.
I0520 03:43:35.932976 140384716638016 submission_runner.py:421] Time since start: 1156.39s, 	Step: 2110, 	{'train/accuracy': 0.0899609375, 'train/loss': 5.3186572265625, 'validation/accuracy': 0.08218, 'validation/loss': 5.3742275, 'validation/num_examples': 50000, 'test/accuracy': 0.0622, 'test/loss': 5.58468203125, 'test/num_examples': 10000, 'score': 846.0730986595154, 'total_duration': 1156.3916800022125, 'accumulated_submission_time': 846.0730986595154, 'accumulated_eval_time': 309.1814980506897, 'accumulated_logging_time': 0.04387950897216797}
I0520 03:43:35.943070 140340220630784 logging_writer.py:48] [2110] accumulated_eval_time=309.181498, accumulated_logging_time=0.043880, accumulated_submission_time=846.073099, global_step=2110, preemption_count=0, score=846.073099, test/accuracy=0.062200, test/loss=5.584682, test/num_examples=10000, total_duration=1156.391680, train/accuracy=0.089961, train/loss=5.318657, validation/accuracy=0.082180, validation/loss=5.374227, validation/num_examples=50000
I0520 03:46:12.516568 140339599898368 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.787754, loss=6.209475
I0520 03:46:12.520996 140384716638016 submission.py:139] 2500) loss = 6.209, grad_norm = 0.788
I0520 03:49:28.819714 140340220630784 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.864477, loss=6.099035
I0520 03:49:28.829340 140384716638016 submission.py:139] 3000) loss = 6.099, grad_norm = 0.864
I0520 03:50:35.995374 140384716638016 spec.py:298] Evaluating on the training split.
I0520 03:51:20.783831 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 03:52:15.514133 140384716638016 spec.py:326] Evaluating on the test split.
I0520 03:52:16.934766 140384716638016 submission_runner.py:421] Time since start: 1677.39s, 	Step: 3174, 	{'train/accuracy': 0.12431640625, 'train/loss': 4.96303955078125, 'validation/accuracy': 0.11644, 'validation/loss': 5.023806875, 'validation/num_examples': 50000, 'test/accuracy': 0.0853, 'test/loss': 5.303829296875, 'test/num_examples': 10000, 'score': 1265.5601253509521, 'total_duration': 1677.3935618400574, 'accumulated_submission_time': 1265.5601253509521, 'accumulated_eval_time': 410.1208860874176, 'accumulated_logging_time': 0.06256437301635742}
I0520 03:52:16.945958 140339599898368 logging_writer.py:48] [3174] accumulated_eval_time=410.120886, accumulated_logging_time=0.062564, accumulated_submission_time=1265.560125, global_step=3174, preemption_count=0, score=1265.560125, test/accuracy=0.085300, test/loss=5.303829, test/num_examples=10000, total_duration=1677.393562, train/accuracy=0.124316, train/loss=4.963040, validation/accuracy=0.116440, validation/loss=5.023807, validation/num_examples=50000
I0520 03:54:23.671863 140340220630784 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.760291, loss=6.114944
I0520 03:54:23.676610 140384716638016 submission.py:139] 3500) loss = 6.115, grad_norm = 0.760
I0520 03:57:39.864029 140339599898368 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.670353, loss=6.225854
I0520 03:57:39.870348 140384716638016 submission.py:139] 4000) loss = 6.226, grad_norm = 0.670
I0520 03:59:17.202422 140384716638016 spec.py:298] Evaluating on the training split.
I0520 04:00:02.222843 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 04:00:48.190534 140384716638016 spec.py:326] Evaluating on the test split.
I0520 04:00:49.616443 140384716638016 submission_runner.py:421] Time since start: 2190.08s, 	Step: 4252, 	{'train/accuracy': 0.155234375, 'train/loss': 4.661991577148438, 'validation/accuracy': 0.14478, 'validation/loss': 4.751675, 'validation/num_examples': 50000, 'test/accuracy': 0.1104, 'test/loss': 5.04901953125, 'test/num_examples': 10000, 'score': 1685.2435584068298, 'total_duration': 2190.0752398967743, 'accumulated_submission_time': 1685.2435584068298, 'accumulated_eval_time': 502.5351781845093, 'accumulated_logging_time': 0.08181405067443848}
I0520 04:00:49.626450 140340220630784 logging_writer.py:48] [4252] accumulated_eval_time=502.535178, accumulated_logging_time=0.081814, accumulated_submission_time=1685.243558, global_step=4252, preemption_count=0, score=1685.243558, test/accuracy=0.110400, test/loss=5.049020, test/num_examples=10000, total_duration=2190.075240, train/accuracy=0.155234, train/loss=4.661992, validation/accuracy=0.144780, validation/loss=4.751675, validation/num_examples=50000
I0520 04:02:26.407618 140339599898368 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.787017, loss=5.861567
I0520 04:02:26.412738 140384716638016 submission.py:139] 4500) loss = 5.862, grad_norm = 0.787
I0520 04:05:44.422767 140340220630784 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.644911, loss=5.785326
I0520 04:05:44.427452 140384716638016 submission.py:139] 5000) loss = 5.785, grad_norm = 0.645
I0520 04:07:49.959855 140384716638016 spec.py:298] Evaluating on the training split.
I0520 04:08:34.819895 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 04:09:20.377363 140384716638016 spec.py:326] Evaluating on the test split.
I0520 04:09:21.802448 140384716638016 submission_runner.py:421] Time since start: 2702.26s, 	Step: 5319, 	{'train/accuracy': 0.19314453125, 'train/loss': 4.325930480957031, 'validation/accuracy': 0.17946, 'validation/loss': 4.4237665625, 'validation/num_examples': 50000, 'test/accuracy': 0.1393, 'test/loss': 4.77616796875, 'test/num_examples': 10000, 'score': 2105.020482301712, 'total_duration': 2702.2597370147705, 'accumulated_submission_time': 2105.020482301712, 'accumulated_eval_time': 594.3765861988068, 'accumulated_logging_time': 0.10081338882446289}
I0520 04:09:21.812108 140339599898368 logging_writer.py:48] [5319] accumulated_eval_time=594.376586, accumulated_logging_time=0.100813, accumulated_submission_time=2105.020482, global_step=5319, preemption_count=0, score=2105.020482, test/accuracy=0.139300, test/loss=4.776168, test/num_examples=10000, total_duration=2702.259737, train/accuracy=0.193145, train/loss=4.325930, validation/accuracy=0.179460, validation/loss=4.423767, validation/num_examples=50000
I0520 04:10:32.449392 140340220630784 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.944486, loss=5.856449
I0520 04:10:32.453653 140384716638016 submission.py:139] 5500) loss = 5.856, grad_norm = 0.944
I0520 04:13:46.704704 140339599898368 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.823085, loss=5.657452
I0520 04:13:46.709949 140384716638016 submission.py:139] 6000) loss = 5.657, grad_norm = 0.823
I0520 04:16:22.023425 140384716638016 spec.py:298] Evaluating on the training split.
I0520 04:17:07.370409 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 04:17:52.962671 140384716638016 spec.py:326] Evaluating on the test split.
I0520 04:17:54.385621 140384716638016 submission_runner.py:421] Time since start: 3214.84s, 	Step: 6388, 	{'train/accuracy': 0.23458984375, 'train/loss': 4.034852294921875, 'validation/accuracy': 0.2176, 'validation/loss': 4.1460875, 'validation/num_examples': 50000, 'test/accuracy': 0.1633, 'test/loss': 4.517291015625, 'test/num_examples': 10000, 'score': 2524.6669387817383, 'total_duration': 3214.8443591594696, 'accumulated_submission_time': 2524.6669387817383, 'accumulated_eval_time': 686.7388386726379, 'accumulated_logging_time': 0.1181178092956543}
I0520 04:17:54.397203 140340220630784 logging_writer.py:48] [6388] accumulated_eval_time=686.738839, accumulated_logging_time=0.118118, accumulated_submission_time=2524.666939, global_step=6388, preemption_count=0, score=2524.666939, test/accuracy=0.163300, test/loss=4.517291, test/num_examples=10000, total_duration=3214.844359, train/accuracy=0.234590, train/loss=4.034852, validation/accuracy=0.217600, validation/loss=4.146088, validation/num_examples=50000
I0520 04:18:38.264065 140339599898368 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.858382, loss=5.533947
I0520 04:18:38.269270 140384716638016 submission.py:139] 6500) loss = 5.534, grad_norm = 0.858
I0520 04:21:52.150360 140340220630784 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.801610, loss=5.566567
I0520 04:21:52.156663 140384716638016 submission.py:139] 7000) loss = 5.567, grad_norm = 0.802
I0520 04:24:54.611790 140384716638016 spec.py:298] Evaluating on the training split.
I0520 04:25:40.980294 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 04:26:33.890588 140384716638016 spec.py:326] Evaluating on the test split.
I0520 04:26:35.315100 140384716638016 submission_runner.py:421] Time since start: 3735.77s, 	Step: 7462, 	{'train/accuracy': 0.28169921875, 'train/loss': 3.706548156738281, 'validation/accuracy': 0.26448, 'validation/loss': 3.8317990625, 'validation/num_examples': 50000, 'test/accuracy': 0.2026, 'test/loss': 4.2466328125, 'test/num_examples': 10000, 'score': 2944.3297176361084, 'total_duration': 3735.773869752884, 'accumulated_submission_time': 2944.3297176361084, 'accumulated_eval_time': 787.4422793388367, 'accumulated_logging_time': 0.13861989974975586}
I0520 04:26:35.330174 140339599898368 logging_writer.py:48] [7462] accumulated_eval_time=787.442279, accumulated_logging_time=0.138620, accumulated_submission_time=2944.329718, global_step=7462, preemption_count=0, score=2944.329718, test/accuracy=0.202600, test/loss=4.246633, test/num_examples=10000, total_duration=3735.773870, train/accuracy=0.281699, train/loss=3.706548, validation/accuracy=0.264480, validation/loss=3.831799, validation/num_examples=50000
I0520 04:26:50.411403 140340220630784 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.579565, loss=5.509976
I0520 04:26:50.420470 140384716638016 submission.py:139] 7500) loss = 5.510, grad_norm = 0.580
I0520 04:30:06.971056 140339599898368 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.767351, loss=5.603114
I0520 04:30:06.976508 140384716638016 submission.py:139] 8000) loss = 5.603, grad_norm = 0.767
I0520 04:33:21.810639 140340220630784 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.696453, loss=5.119315
I0520 04:33:21.815175 140384716638016 submission.py:139] 8500) loss = 5.119, grad_norm = 0.696
I0520 04:33:35.682532 140384716638016 spec.py:298] Evaluating on the training split.
I0520 04:34:20.807529 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 04:35:06.436801 140384716638016 spec.py:326] Evaluating on the test split.
I0520 04:35:07.859386 140384716638016 submission_runner.py:421] Time since start: 4248.32s, 	Step: 8535, 	{'train/accuracy': 0.3198828125, 'train/loss': 3.4558209228515624, 'validation/accuracy': 0.2973, 'validation/loss': 3.59031, 'validation/num_examples': 50000, 'test/accuracy': 0.2331, 'test/loss': 4.040454296875, 'test/num_examples': 10000, 'score': 3364.1154477596283, 'total_duration': 4248.318153858185, 'accumulated_submission_time': 3364.1154477596283, 'accumulated_eval_time': 879.6191956996918, 'accumulated_logging_time': 0.16360235214233398}
I0520 04:35:07.873501 140339599898368 logging_writer.py:48] [8535] accumulated_eval_time=879.619196, accumulated_logging_time=0.163602, accumulated_submission_time=3364.115448, global_step=8535, preemption_count=0, score=3364.115448, test/accuracy=0.233100, test/loss=4.040454, test/num_examples=10000, total_duration=4248.318154, train/accuracy=0.319883, train/loss=3.455821, validation/accuracy=0.297300, validation/loss=3.590310, validation/num_examples=50000
I0520 04:38:13.255284 140340220630784 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.601739, loss=5.331738
I0520 04:38:13.259878 140384716638016 submission.py:139] 9000) loss = 5.332, grad_norm = 0.602
I0520 04:41:27.826824 140339599898368 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.566897, loss=5.476256
I0520 04:41:27.831583 140384716638016 submission.py:139] 9500) loss = 5.476, grad_norm = 0.567
I0520 04:42:08.145265 140384716638016 spec.py:298] Evaluating on the training split.
I0520 04:42:52.999299 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 04:43:39.083665 140384716638016 spec.py:326] Evaluating on the test split.
I0520 04:43:40.502122 140384716638016 submission_runner.py:421] Time since start: 4760.96s, 	Step: 9605, 	{'train/accuracy': 0.34962890625, 'train/loss': 3.2826504516601562, 'validation/accuracy': 0.32144, 'validation/loss': 3.426675, 'validation/num_examples': 50000, 'test/accuracy': 0.2537, 'test/loss': 3.9026375, 'test/num_examples': 10000, 'score': 3783.821884393692, 'total_duration': 4760.960906744003, 'accumulated_submission_time': 3783.821884393692, 'accumulated_eval_time': 971.9761302471161, 'accumulated_logging_time': 0.18607544898986816}
I0520 04:43:40.512100 140340220630784 logging_writer.py:48] [9605] accumulated_eval_time=971.976130, accumulated_logging_time=0.186075, accumulated_submission_time=3783.821884, global_step=9605, preemption_count=0, score=3783.821884, test/accuracy=0.253700, test/loss=3.902637, test/num_examples=10000, total_duration=4760.960907, train/accuracy=0.349629, train/loss=3.282650, validation/accuracy=0.321440, validation/loss=3.426675, validation/num_examples=50000
I0520 04:46:18.132430 140339599898368 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.663572, loss=5.145089
I0520 04:46:18.142903 140384716638016 submission.py:139] 10000) loss = 5.145, grad_norm = 0.664
I0520 04:49:34.607906 140340220630784 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.630184, loss=4.926202
I0520 04:49:34.613583 140384716638016 submission.py:139] 10500) loss = 4.926, grad_norm = 0.630
I0520 04:50:40.613652 140384716638016 spec.py:298] Evaluating on the training split.
I0520 04:51:26.238428 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 04:52:12.473566 140384716638016 spec.py:326] Evaluating on the test split.
I0520 04:52:13.894531 140384716638016 submission_runner.py:421] Time since start: 5274.35s, 	Step: 10671, 	{'train/accuracy': 0.39193359375, 'train/loss': 3.0403787231445314, 'validation/accuracy': 0.3564, 'validation/loss': 3.2020596875, 'validation/num_examples': 50000, 'test/accuracy': 0.2757, 'test/loss': 3.712549609375, 'test/num_examples': 10000, 'score': 4203.362256765366, 'total_duration': 5274.353318452835, 'accumulated_submission_time': 4203.362256765366, 'accumulated_eval_time': 1065.2571041584015, 'accumulated_logging_time': 0.2040863037109375}
I0520 04:52:13.906091 140339599898368 logging_writer.py:48] [10671] accumulated_eval_time=1065.257104, accumulated_logging_time=0.204086, accumulated_submission_time=4203.362257, global_step=10671, preemption_count=0, score=4203.362257, test/accuracy=0.275700, test/loss=3.712550, test/num_examples=10000, total_duration=5274.353318, train/accuracy=0.391934, train/loss=3.040379, validation/accuracy=0.356400, validation/loss=3.202060, validation/num_examples=50000
I0520 04:54:22.464063 140340220630784 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.568615, loss=5.066749
I0520 04:54:22.469629 140384716638016 submission.py:139] 11000) loss = 5.067, grad_norm = 0.569
I0520 04:57:47.175039 140339599898368 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.586864, loss=4.933639
I0520 04:57:47.189007 140384716638016 submission.py:139] 11500) loss = 4.934, grad_norm = 0.587
I0520 04:59:14.310976 140384716638016 spec.py:298] Evaluating on the training split.
I0520 04:59:59.673200 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 05:00:45.582968 140384716638016 spec.py:326] Evaluating on the test split.
I0520 05:00:47.007057 140384716638016 submission_runner.py:421] Time since start: 5787.47s, 	Step: 11714, 	{'train/accuracy': 0.411328125, 'train/loss': 2.912144775390625, 'validation/accuracy': 0.38102, 'validation/loss': 3.08264625, 'validation/num_examples': 50000, 'test/accuracy': 0.2925, 'test/loss': 3.62970859375, 'test/num_examples': 10000, 'score': 4623.218049764633, 'total_duration': 5787.465730428696, 'accumulated_submission_time': 4623.218049764633, 'accumulated_eval_time': 1157.9530777931213, 'accumulated_logging_time': 0.22621417045593262}
I0520 05:00:47.018376 140340220630784 logging_writer.py:48] [11714] accumulated_eval_time=1157.953078, accumulated_logging_time=0.226214, accumulated_submission_time=4623.218050, global_step=11714, preemption_count=0, score=4623.218050, test/accuracy=0.292500, test/loss=3.629709, test/num_examples=10000, total_duration=5787.465730, train/accuracy=0.411328, train/loss=2.912145, validation/accuracy=0.381020, validation/loss=3.082646, validation/num_examples=50000
I0520 05:02:44.453344 140339599898368 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.598445, loss=4.909667
I0520 05:02:44.459643 140384716638016 submission.py:139] 12000) loss = 4.910, grad_norm = 0.598
I0520 05:06:08.752884 140340220630784 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.603683, loss=5.177705
I0520 05:06:08.759510 140384716638016 submission.py:139] 12500) loss = 5.178, grad_norm = 0.604
I0520 05:07:47.087600 140384716638016 spec.py:298] Evaluating on the training split.
I0520 05:08:32.303797 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 05:09:18.065381 140384716638016 spec.py:326] Evaluating on the test split.
I0520 05:09:19.488259 140384716638016 submission_runner.py:421] Time since start: 6299.95s, 	Step: 12747, 	{'train/accuracy': 0.43623046875, 'train/loss': 2.7709356689453126, 'validation/accuracy': 0.40068, 'validation/loss': 2.9484196875, 'validation/num_examples': 50000, 'test/accuracy': 0.3097, 'test/loss': 3.491196484375, 'test/num_examples': 10000, 'score': 5042.7560024261475, 'total_duration': 6299.947057247162, 'accumulated_submission_time': 5042.7560024261475, 'accumulated_eval_time': 1250.3538222312927, 'accumulated_logging_time': 0.24575257301330566}
I0520 05:09:19.499826 140339599898368 logging_writer.py:48] [12747] accumulated_eval_time=1250.353822, accumulated_logging_time=0.245753, accumulated_submission_time=5042.756002, global_step=12747, preemption_count=0, score=5042.756002, test/accuracy=0.309700, test/loss=3.491196, test/num_examples=10000, total_duration=6299.947057, train/accuracy=0.436230, train/loss=2.770936, validation/accuracy=0.400680, validation/loss=2.948420, validation/num_examples=50000
I0520 05:10:58.304617 140340220630784 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.679310, loss=5.041498
I0520 05:10:58.311171 140384716638016 submission.py:139] 13000) loss = 5.041, grad_norm = 0.679
I0520 05:14:12.677159 140339599898368 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.567447, loss=5.094106
I0520 05:14:12.682640 140384716638016 submission.py:139] 13500) loss = 5.094, grad_norm = 0.567
I0520 05:16:19.582211 140384716638016 spec.py:298] Evaluating on the training split.
I0520 05:17:04.810644 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 05:17:50.218586 140384716638016 spec.py:326] Evaluating on the test split.
I0520 05:17:51.640908 140384716638016 submission_runner.py:421] Time since start: 6812.10s, 	Step: 13810, 	{'train/accuracy': 0.46568359375, 'train/loss': 2.6221136474609374, 'validation/accuracy': 0.42584, 'validation/loss': 2.81882625, 'validation/num_examples': 50000, 'test/accuracy': 0.3343, 'test/loss': 3.35210859375, 'test/num_examples': 10000, 'score': 5462.277799367905, 'total_duration': 6812.0996651649475, 'accumulated_submission_time': 5462.277799367905, 'accumulated_eval_time': 1342.412626504898, 'accumulated_logging_time': 0.2652163505554199}
I0520 05:17:51.651343 140340220630784 logging_writer.py:48] [13810] accumulated_eval_time=1342.412627, accumulated_logging_time=0.265216, accumulated_submission_time=5462.277799, global_step=13810, preemption_count=0, score=5462.277799, test/accuracy=0.334300, test/loss=3.352109, test/num_examples=10000, total_duration=6812.099665, train/accuracy=0.465684, train/loss=2.622114, validation/accuracy=0.425840, validation/loss=2.818826, validation/num_examples=50000
I0520 05:19:06.003895 140339599898368 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.600722, loss=4.897893
I0520 05:19:06.009382 140384716638016 submission.py:139] 14000) loss = 4.898, grad_norm = 0.601
I0520 05:22:20.686561 140340220630784 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.555464, loss=5.014330
I0520 05:22:20.692852 140384716638016 submission.py:139] 14500) loss = 5.014, grad_norm = 0.555
I0520 05:24:51.700086 140384716638016 spec.py:298] Evaluating on the training split.
I0520 05:25:36.539935 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 05:26:22.530035 140384716638016 spec.py:326] Evaluating on the test split.
I0520 05:26:23.950927 140384716638016 submission_runner.py:421] Time since start: 7324.41s, 	Step: 14880, 	{'train/accuracy': 0.49255859375, 'train/loss': 2.4854055786132814, 'validation/accuracy': 0.4492, 'validation/loss': 2.6889246875, 'validation/num_examples': 50000, 'test/accuracy': 0.3544, 'test/loss': 3.252184765625, 'test/num_examples': 10000, 'score': 5881.772726774216, 'total_duration': 7324.409709692001, 'accumulated_submission_time': 5881.772726774216, 'accumulated_eval_time': 1434.663516998291, 'accumulated_logging_time': 0.2861301898956299}
I0520 05:26:23.961275 140339599898368 logging_writer.py:48] [14880] accumulated_eval_time=1434.663517, accumulated_logging_time=0.286130, accumulated_submission_time=5881.772727, global_step=14880, preemption_count=0, score=5881.772727, test/accuracy=0.354400, test/loss=3.252185, test/num_examples=10000, total_duration=7324.409710, train/accuracy=0.492559, train/loss=2.485406, validation/accuracy=0.449200, validation/loss=2.688925, validation/num_examples=50000
I0520 05:27:12.316063 140340220630784 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.587734, loss=4.942146
I0520 05:27:12.322372 140384716638016 submission.py:139] 15000) loss = 4.942, grad_norm = 0.588
I0520 05:30:28.821554 140339599898368 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.675512, loss=4.430774
I0520 05:30:28.826693 140384716638016 submission.py:139] 15500) loss = 4.431, grad_norm = 0.676
I0520 05:33:24.343151 140384716638016 spec.py:298] Evaluating on the training split.
I0520 05:34:09.652579 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 05:34:55.171973 140384716638016 spec.py:326] Evaluating on the test split.
I0520 05:34:56.592875 140384716638016 submission_runner.py:421] Time since start: 7837.05s, 	Step: 15953, 	{'train/accuracy': 0.504375, 'train/loss': 2.421617279052734, 'validation/accuracy': 0.46236, 'validation/loss': 2.626495625, 'validation/num_examples': 50000, 'test/accuracy': 0.3558, 'test/loss': 3.191871875, 'test/num_examples': 10000, 'score': 6301.590001583099, 'total_duration': 7837.051640033722, 'accumulated_submission_time': 6301.590001583099, 'accumulated_eval_time': 1526.913357257843, 'accumulated_logging_time': 0.3049025535583496}
I0520 05:34:56.603615 140340220630784 logging_writer.py:48] [15953] accumulated_eval_time=1526.913357, accumulated_logging_time=0.304903, accumulated_submission_time=6301.590002, global_step=15953, preemption_count=0, score=6301.590002, test/accuracy=0.355800, test/loss=3.191872, test/num_examples=10000, total_duration=7837.051640, train/accuracy=0.504375, train/loss=2.421617, validation/accuracy=0.462360, validation/loss=2.626496, validation/num_examples=50000
I0520 05:35:15.286933 140339599898368 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.570785, loss=4.827092
I0520 05:35:15.291066 140384716638016 submission.py:139] 16000) loss = 4.827, grad_norm = 0.571
I0520 05:38:35.742516 140340220630784 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.585747, loss=4.863723
I0520 05:38:35.747735 140384716638016 submission.py:139] 16500) loss = 4.864, grad_norm = 0.586
I0520 05:41:49.957116 140339599898368 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.688423, loss=4.892071
I0520 05:41:49.964756 140384716638016 submission.py:139] 17000) loss = 4.892, grad_norm = 0.688
I0520 05:41:56.942503 140384716638016 spec.py:298] Evaluating on the training split.
I0520 05:42:42.522403 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 05:43:28.429278 140384716638016 spec.py:326] Evaluating on the test split.
I0520 05:43:29.851843 140384716638016 submission_runner.py:421] Time since start: 8350.31s, 	Step: 17019, 	{'train/accuracy': 0.52095703125, 'train/loss': 2.3394671630859376, 'validation/accuracy': 0.47906, 'validation/loss': 2.54531234375, 'validation/num_examples': 50000, 'test/accuracy': 0.3709, 'test/loss': 3.1380255859375, 'test/num_examples': 10000, 'score': 6721.366400718689, 'total_duration': 8350.310612916946, 'accumulated_submission_time': 6721.366400718689, 'accumulated_eval_time': 1619.8226809501648, 'accumulated_logging_time': 0.32584643363952637}
I0520 05:43:29.865010 140340220630784 logging_writer.py:48] [17019] accumulated_eval_time=1619.822681, accumulated_logging_time=0.325846, accumulated_submission_time=6721.366401, global_step=17019, preemption_count=0, score=6721.366401, test/accuracy=0.370900, test/loss=3.138026, test/num_examples=10000, total_duration=8350.310613, train/accuracy=0.520957, train/loss=2.339467, validation/accuracy=0.479060, validation/loss=2.545312, validation/num_examples=50000
I0520 05:46:41.478650 140339599898368 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.576924, loss=4.742628
I0520 05:46:41.484038 140384716638016 submission.py:139] 17500) loss = 4.743, grad_norm = 0.577
I0520 05:49:58.268526 140340220630784 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.684999, loss=4.768442
I0520 05:49:58.275517 140384716638016 submission.py:139] 18000) loss = 4.768, grad_norm = 0.685
I0520 05:50:30.102557 140384716638016 spec.py:298] Evaluating on the training split.
I0520 05:51:15.454338 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 05:52:01.519331 140384716638016 spec.py:326] Evaluating on the test split.
I0520 05:52:02.946723 140384716638016 submission_runner.py:421] Time since start: 8863.41s, 	Step: 18083, 	{'train/accuracy': 0.53625, 'train/loss': 2.2731942749023437, 'validation/accuracy': 0.49074, 'validation/loss': 2.49163375, 'validation/num_examples': 50000, 'test/accuracy': 0.3859, 'test/loss': 3.075822265625, 'test/num_examples': 10000, 'score': 7141.044600009918, 'total_duration': 8863.405514240265, 'accumulated_submission_time': 7141.044600009918, 'accumulated_eval_time': 1712.6671342849731, 'accumulated_logging_time': 0.3473656177520752}
I0520 05:52:02.957081 140339599898368 logging_writer.py:48] [18083] accumulated_eval_time=1712.667134, accumulated_logging_time=0.347366, accumulated_submission_time=7141.044600, global_step=18083, preemption_count=0, score=7141.044600, test/accuracy=0.385900, test/loss=3.075822, test/num_examples=10000, total_duration=8863.405514, train/accuracy=0.536250, train/loss=2.273194, validation/accuracy=0.490740, validation/loss=2.491634, validation/num_examples=50000
I0520 05:54:46.098054 140340220630784 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.625820, loss=4.559845
I0520 05:54:46.103992 140384716638016 submission.py:139] 18500) loss = 4.560, grad_norm = 0.626
I0520 05:58:07.363029 140339599898368 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.693306, loss=4.699518
I0520 05:58:07.369077 140384716638016 submission.py:139] 19000) loss = 4.700, grad_norm = 0.693
I0520 05:59:03.336077 140384716638016 spec.py:298] Evaluating on the training split.
I0520 05:59:48.912220 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 06:00:34.878738 140384716638016 spec.py:326] Evaluating on the test split.
I0520 06:00:36.302189 140384716638016 submission_runner.py:421] Time since start: 9376.76s, 	Step: 19145, 	{'train/accuracy': 0.5488671875, 'train/loss': 2.2381442260742186, 'validation/accuracy': 0.50174, 'validation/loss': 2.46621328125, 'validation/num_examples': 50000, 'test/accuracy': 0.3973, 'test/loss': 3.0374328125, 'test/num_examples': 10000, 'score': 7560.860235214233, 'total_duration': 9376.760954618454, 'accumulated_submission_time': 7560.860235214233, 'accumulated_eval_time': 1805.6333637237549, 'accumulated_logging_time': 0.36609339714050293}
I0520 06:00:36.313100 140340220630784 logging_writer.py:48] [19145] accumulated_eval_time=1805.633364, accumulated_logging_time=0.366093, accumulated_submission_time=7560.860235, global_step=19145, preemption_count=0, score=7560.860235, test/accuracy=0.397300, test/loss=3.037433, test/num_examples=10000, total_duration=9376.760955, train/accuracy=0.548867, train/loss=2.238144, validation/accuracy=0.501740, validation/loss=2.466213, validation/num_examples=50000
I0520 06:02:54.955805 140339599898368 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.621265, loss=4.436132
I0520 06:02:54.961166 140384716638016 submission.py:139] 19500) loss = 4.436, grad_norm = 0.621
I0520 06:06:13.540528 140340220630784 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.696906, loss=4.669382
I0520 06:06:13.546975 140384716638016 submission.py:139] 20000) loss = 4.669, grad_norm = 0.697
I0520 06:07:36.488851 140384716638016 spec.py:298] Evaluating on the training split.
I0520 06:08:21.867635 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 06:09:07.665898 140384716638016 spec.py:326] Evaluating on the test split.
I0520 06:09:09.087315 140384716638016 submission_runner.py:421] Time since start: 9889.55s, 	Step: 20209, 	{'train/accuracy': 0.55865234375, 'train/loss': 2.1180075073242186, 'validation/accuracy': 0.51044, 'validation/loss': 2.35282984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4054, 'test/loss': 2.9408439453125, 'test/num_examples': 10000, 'score': 7980.481246232986, 'total_duration': 9889.546065568924, 'accumulated_submission_time': 7980.481246232986, 'accumulated_eval_time': 1898.2321481704712, 'accumulated_logging_time': 0.38515138626098633}
I0520 06:09:09.098447 140339599898368 logging_writer.py:48] [20209] accumulated_eval_time=1898.232148, accumulated_logging_time=0.385151, accumulated_submission_time=7980.481246, global_step=20209, preemption_count=0, score=7980.481246, test/accuracy=0.405400, test/loss=2.940844, test/num_examples=10000, total_duration=9889.546066, train/accuracy=0.558652, train/loss=2.118008, validation/accuracy=0.510440, validation/loss=2.352830, validation/num_examples=50000
I0520 06:11:02.505098 140340220630784 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.620360, loss=4.418557
I0520 06:11:02.510694 140384716638016 submission.py:139] 20500) loss = 4.419, grad_norm = 0.620
I0520 06:14:16.726565 140339599898368 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.630823, loss=4.504497
I0520 06:14:16.732221 140384716638016 submission.py:139] 21000) loss = 4.504, grad_norm = 0.631
I0520 06:16:09.397153 140384716638016 spec.py:298] Evaluating on the training split.
I0520 06:16:57.434083 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 06:17:43.819164 140384716638016 spec.py:326] Evaluating on the test split.
I0520 06:17:45.246263 140384716638016 submission_runner.py:421] Time since start: 10405.71s, 	Step: 21275, 	{'train/accuracy': 0.57646484375, 'train/loss': 2.047589874267578, 'validation/accuracy': 0.52882, 'validation/loss': 2.28533953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4209, 'test/loss': 2.8756140625, 'test/num_examples': 10000, 'score': 8400.216166973114, 'total_duration': 10405.705049753189, 'accumulated_submission_time': 8400.216166973114, 'accumulated_eval_time': 1994.0816884040833, 'accumulated_logging_time': 0.40603137016296387}
I0520 06:17:45.258221 140340220630784 logging_writer.py:48] [21275] accumulated_eval_time=1994.081688, accumulated_logging_time=0.406031, accumulated_submission_time=8400.216167, global_step=21275, preemption_count=0, score=8400.216167, test/accuracy=0.420900, test/loss=2.875614, test/num_examples=10000, total_duration=10405.705050, train/accuracy=0.576465, train/loss=2.047590, validation/accuracy=0.528820, validation/loss=2.285340, validation/num_examples=50000
I0520 06:19:13.301188 140339599898368 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.610598, loss=4.665814
I0520 06:19:13.306047 140384716638016 submission.py:139] 21500) loss = 4.666, grad_norm = 0.611
I0520 06:22:27.512089 140340220630784 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.608832, loss=4.825382
I0520 06:22:27.518291 140384716638016 submission.py:139] 22000) loss = 4.825, grad_norm = 0.609
I0520 06:24:45.532432 140384716638016 spec.py:298] Evaluating on the training split.
I0520 06:25:30.833728 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 06:26:16.686793 140384716638016 spec.py:326] Evaluating on the test split.
I0520 06:26:18.109525 140384716638016 submission_runner.py:421] Time since start: 10918.57s, 	Step: 22353, 	{'train/accuracy': 0.58939453125, 'train/loss': 1.9871305847167968, 'validation/accuracy': 0.536, 'validation/loss': 2.235166875, 'validation/num_examples': 50000, 'test/accuracy': 0.4206, 'test/loss': 2.8382515625, 'test/num_examples': 10000, 'score': 8819.929946422577, 'total_duration': 10918.568313121796, 'accumulated_submission_time': 8819.929946422577, 'accumulated_eval_time': 2086.6588463783264, 'accumulated_logging_time': 0.42879509925842285}
I0520 06:26:18.122728 140339599898368 logging_writer.py:48] [22353] accumulated_eval_time=2086.658846, accumulated_logging_time=0.428795, accumulated_submission_time=8819.929946, global_step=22353, preemption_count=0, score=8819.929946, test/accuracy=0.420600, test/loss=2.838252, test/num_examples=10000, total_duration=10918.568313, train/accuracy=0.589395, train/loss=1.987131, validation/accuracy=0.536000, validation/loss=2.235167, validation/num_examples=50000
I0520 06:27:17.607344 140340220630784 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.649456, loss=4.265515
I0520 06:27:17.613157 140384716638016 submission.py:139] 22500) loss = 4.266, grad_norm = 0.649
I0520 06:30:34.318739 140339599898368 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.573281, loss=4.840242
I0520 06:30:34.324684 140384716638016 submission.py:139] 23000) loss = 4.840, grad_norm = 0.573
I0520 06:33:18.183529 140384716638016 spec.py:298] Evaluating on the training split.
I0520 06:34:03.359556 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 06:34:49.254019 140384716638016 spec.py:326] Evaluating on the test split.
I0520 06:34:50.677161 140384716638016 submission_runner.py:421] Time since start: 11431.14s, 	Step: 23423, 	{'train/accuracy': 0.60080078125, 'train/loss': 1.9434852600097656, 'validation/accuracy': 0.54488, 'validation/loss': 2.190995, 'validation/num_examples': 50000, 'test/accuracy': 0.4249, 'test/loss': 2.7983130859375, 'test/num_examples': 10000, 'score': 9239.430367469788, 'total_duration': 11431.135954856873, 'accumulated_submission_time': 9239.430367469788, 'accumulated_eval_time': 2179.1526477336884, 'accumulated_logging_time': 0.45078301429748535}
I0520 06:34:50.688745 140340220630784 logging_writer.py:48] [23423] accumulated_eval_time=2179.152648, accumulated_logging_time=0.450783, accumulated_submission_time=9239.430367, global_step=23423, preemption_count=0, score=9239.430367, test/accuracy=0.424900, test/loss=2.798313, test/num_examples=10000, total_duration=11431.135955, train/accuracy=0.600801, train/loss=1.943485, validation/accuracy=0.544880, validation/loss=2.190995, validation/num_examples=50000
I0520 06:35:21.343122 140339599898368 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.635348, loss=4.347307
I0520 06:35:21.349037 140384716638016 submission.py:139] 23500) loss = 4.347, grad_norm = 0.635
I0520 06:38:42.674602 140340220630784 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.659587, loss=4.416590
I0520 06:38:42.681761 140384716638016 submission.py:139] 24000) loss = 4.417, grad_norm = 0.660
I0520 06:41:50.937933 140384716638016 spec.py:298] Evaluating on the training split.
I0520 06:42:35.826888 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 06:43:21.650547 140384716638016 spec.py:326] Evaluating on the test split.
I0520 06:43:23.073218 140384716638016 submission_runner.py:421] Time since start: 11943.53s, 	Step: 24485, 	{'train/accuracy': 0.6076171875, 'train/loss': 1.9141189575195312, 'validation/accuracy': 0.55382, 'validation/loss': 2.16075765625, 'validation/num_examples': 50000, 'test/accuracy': 0.4404, 'test/loss': 2.774184765625, 'test/num_examples': 10000, 'score': 9659.115862369537, 'total_duration': 11943.532006978989, 'accumulated_submission_time': 9659.115862369537, 'accumulated_eval_time': 2271.287932872772, 'accumulated_logging_time': 0.47638821601867676}
I0520 06:43:23.087270 140339599898368 logging_writer.py:48] [24485] accumulated_eval_time=2271.287933, accumulated_logging_time=0.476388, accumulated_submission_time=9659.115862, global_step=24485, preemption_count=0, score=9659.115862, test/accuracy=0.440400, test/loss=2.774185, test/num_examples=10000, total_duration=11943.532007, train/accuracy=0.607617, train/loss=1.914119, validation/accuracy=0.553820, validation/loss=2.160758, validation/num_examples=50000
I0520 06:43:29.290238 140340220630784 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.702471, loss=4.230798
I0520 06:43:29.295474 140384716638016 submission.py:139] 24500) loss = 4.231, grad_norm = 0.702
I0520 06:46:48.604471 140339599898368 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.630392, loss=4.466346
I0520 06:46:48.611637 140384716638016 submission.py:139] 25000) loss = 4.466, grad_norm = 0.630
I0520 06:50:05.835110 140340220630784 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.636298, loss=4.661563
I0520 06:50:05.843278 140384716638016 submission.py:139] 25500) loss = 4.662, grad_norm = 0.636
I0520 06:50:23.312762 140384716638016 spec.py:298] Evaluating on the training split.
I0520 06:51:08.848012 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 06:51:54.429807 140384716638016 spec.py:326] Evaluating on the test split.
I0520 06:51:55.853488 140384716638016 submission_runner.py:421] Time since start: 12456.31s, 	Step: 25546, 	{'train/accuracy': 0.6197265625, 'train/loss': 1.8329373168945313, 'validation/accuracy': 0.5616, 'validation/loss': 2.0934059375, 'validation/num_examples': 50000, 'test/accuracy': 0.4454, 'test/loss': 2.701642578125, 'test/num_examples': 10000, 'score': 10078.788527965546, 'total_duration': 12456.312270641327, 'accumulated_submission_time': 10078.788527965546, 'accumulated_eval_time': 2363.82874083519, 'accumulated_logging_time': 0.5005838871002197}
I0520 06:51:55.867261 140339599898368 logging_writer.py:48] [25546] accumulated_eval_time=2363.828741, accumulated_logging_time=0.500584, accumulated_submission_time=10078.788528, global_step=25546, preemption_count=0, score=10078.788528, test/accuracy=0.445400, test/loss=2.701643, test/num_examples=10000, total_duration=12456.312271, train/accuracy=0.619727, train/loss=1.832937, validation/accuracy=0.561600, validation/loss=2.093406, validation/num_examples=50000
I0520 06:54:53.062443 140340220630784 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.668164, loss=4.412601
I0520 06:54:53.067920 140384716638016 submission.py:139] 26000) loss = 4.413, grad_norm = 0.668
I0520 06:58:13.212303 140339599898368 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.618002, loss=4.556192
I0520 06:58:13.217821 140384716638016 submission.py:139] 26500) loss = 4.556, grad_norm = 0.618
I0520 06:58:55.945504 140384716638016 spec.py:298] Evaluating on the training split.
I0520 06:59:41.251908 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 07:00:27.600040 140384716638016 spec.py:326] Evaluating on the test split.
I0520 07:00:29.021571 140384716638016 submission_runner.py:421] Time since start: 12969.48s, 	Step: 26611, 	{'train/accuracy': 0.62318359375, 'train/loss': 1.8171615600585938, 'validation/accuracy': 0.5673, 'validation/loss': 2.085154375, 'validation/num_examples': 50000, 'test/accuracy': 0.4563, 'test/loss': 2.67572890625, 'test/num_examples': 10000, 'score': 10498.310272455215, 'total_duration': 12969.480365514755, 'accumulated_submission_time': 10498.310272455215, 'accumulated_eval_time': 2456.9049117565155, 'accumulated_logging_time': 0.522892951965332}
I0520 07:00:29.033656 140340220630784 logging_writer.py:48] [26611] accumulated_eval_time=2456.904912, accumulated_logging_time=0.522893, accumulated_submission_time=10498.310272, global_step=26611, preemption_count=0, score=10498.310272, test/accuracy=0.456300, test/loss=2.675729, test/num_examples=10000, total_duration=12969.480366, train/accuracy=0.623184, train/loss=1.817162, validation/accuracy=0.567300, validation/loss=2.085154, validation/num_examples=50000
I0520 07:03:00.542116 140339599898368 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.585928, loss=4.421138
I0520 07:03:00.547122 140384716638016 submission.py:139] 27000) loss = 4.421, grad_norm = 0.586
I0520 07:06:19.665454 140340220630784 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.670635, loss=4.498806
I0520 07:06:19.675249 140384716638016 submission.py:139] 27500) loss = 4.499, grad_norm = 0.671
I0520 07:07:29.177000 140384716638016 spec.py:298] Evaluating on the training split.
I0520 07:08:15.018579 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 07:09:01.118824 140384716638016 spec.py:326] Evaluating on the test split.
I0520 07:09:02.544426 140384716638016 submission_runner.py:421] Time since start: 13483.00s, 	Step: 27674, 	{'train/accuracy': 0.63560546875, 'train/loss': 1.7649761962890624, 'validation/accuracy': 0.57756, 'validation/loss': 2.0276146875, 'validation/num_examples': 50000, 'test/accuracy': 0.4557, 'test/loss': 2.6427314453125, 'test/num_examples': 10000, 'score': 10917.897287845612, 'total_duration': 13483.003182649612, 'accumulated_submission_time': 10917.897287845612, 'accumulated_eval_time': 2550.272453069687, 'accumulated_logging_time': 0.5447568893432617}
I0520 07:09:02.556071 140339599898368 logging_writer.py:48] [27674] accumulated_eval_time=2550.272453, accumulated_logging_time=0.544757, accumulated_submission_time=10917.897288, global_step=27674, preemption_count=0, score=10917.897288, test/accuracy=0.455700, test/loss=2.642731, test/num_examples=10000, total_duration=13483.003183, train/accuracy=0.635605, train/loss=1.764976, validation/accuracy=0.577560, validation/loss=2.027615, validation/num_examples=50000
I0520 07:11:09.381149 140384716638016 spec.py:298] Evaluating on the training split.
I0520 07:11:54.377832 140384716638016 spec.py:310] Evaluating on the validation split.
I0520 07:12:40.402132 140384716638016 spec.py:326] Evaluating on the test split.
I0520 07:12:41.826373 140384716638016 submission_runner.py:421] Time since start: 13702.29s, 	Step: 28000, 	{'train/accuracy': 0.64216796875, 'train/loss': 1.736297607421875, 'validation/accuracy': 0.58066, 'validation/loss': 2.0111953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4591, 'test/loss': 2.6366369140625, 'test/num_examples': 10000, 'score': 11044.543913602829, 'total_duration': 13702.285175561905, 'accumulated_submission_time': 11044.543913602829, 'accumulated_eval_time': 2642.717852830887, 'accumulated_logging_time': 0.5644726753234863}
I0520 07:12:41.837234 140340220630784 logging_writer.py:48] [28000] accumulated_eval_time=2642.717853, accumulated_logging_time=0.564473, accumulated_submission_time=11044.543914, global_step=28000, preemption_count=0, score=11044.543914, test/accuracy=0.459100, test/loss=2.636637, test/num_examples=10000, total_duration=13702.285176, train/accuracy=0.642168, train/loss=1.736298, validation/accuracy=0.580660, validation/loss=2.011195, validation/num_examples=50000
I0520 07:12:41.854866 140339599898368 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=11044.543914
I0520 07:12:42.363904 140384716638016 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b_pytorch/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_28000.
I0520 07:12:42.636640 140384716638016 submission_runner.py:584] Tuning trial 1/1
I0520 07:12:42.636850 140384716638016 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0520 07:12:42.637919 140384716638016 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010546875, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.739134311676025, 'total_duration': 134.3608717918396, 'accumulated_submission_time': 6.739134311676025, 'accumulated_eval_time': 127.62046074867249, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1039, {'train/accuracy': 0.04443359375, 'train/loss': 5.947308349609375, 'validation/accuracy': 0.04176, 'validation/loss': 5.98111375, 'validation/num_examples': 50000, 'test/accuracy': 0.0298, 'test/loss': 6.101627734375, 'test/num_examples': 10000, 'score': 426.5390110015869, 'total_duration': 643.6969928741455, 'accumulated_submission_time': 426.5390110015869, 'accumulated_eval_time': 216.58877515792847, 'accumulated_logging_time': 0.025425195693969727, 'global_step': 1039, 'preemption_count': 0}), (2110, {'train/accuracy': 0.0899609375, 'train/loss': 5.3186572265625, 'validation/accuracy': 0.08218, 'validation/loss': 5.3742275, 'validation/num_examples': 50000, 'test/accuracy': 0.0622, 'test/loss': 5.58468203125, 'test/num_examples': 10000, 'score': 846.0730986595154, 'total_duration': 1156.3916800022125, 'accumulated_submission_time': 846.0730986595154, 'accumulated_eval_time': 309.1814980506897, 'accumulated_logging_time': 0.04387950897216797, 'global_step': 2110, 'preemption_count': 0}), (3174, {'train/accuracy': 0.12431640625, 'train/loss': 4.96303955078125, 'validation/accuracy': 0.11644, 'validation/loss': 5.023806875, 'validation/num_examples': 50000, 'test/accuracy': 0.0853, 'test/loss': 5.303829296875, 'test/num_examples': 10000, 'score': 1265.5601253509521, 'total_duration': 1677.3935618400574, 'accumulated_submission_time': 1265.5601253509521, 'accumulated_eval_time': 410.1208860874176, 'accumulated_logging_time': 0.06256437301635742, 'global_step': 3174, 'preemption_count': 0}), (4252, {'train/accuracy': 0.155234375, 'train/loss': 4.661991577148438, 'validation/accuracy': 0.14478, 'validation/loss': 4.751675, 'validation/num_examples': 50000, 'test/accuracy': 0.1104, 'test/loss': 5.04901953125, 'test/num_examples': 10000, 'score': 1685.2435584068298, 'total_duration': 2190.0752398967743, 'accumulated_submission_time': 1685.2435584068298, 'accumulated_eval_time': 502.5351781845093, 'accumulated_logging_time': 0.08181405067443848, 'global_step': 4252, 'preemption_count': 0}), (5319, {'train/accuracy': 0.19314453125, 'train/loss': 4.325930480957031, 'validation/accuracy': 0.17946, 'validation/loss': 4.4237665625, 'validation/num_examples': 50000, 'test/accuracy': 0.1393, 'test/loss': 4.77616796875, 'test/num_examples': 10000, 'score': 2105.020482301712, 'total_duration': 2702.2597370147705, 'accumulated_submission_time': 2105.020482301712, 'accumulated_eval_time': 594.3765861988068, 'accumulated_logging_time': 0.10081338882446289, 'global_step': 5319, 'preemption_count': 0}), (6388, {'train/accuracy': 0.23458984375, 'train/loss': 4.034852294921875, 'validation/accuracy': 0.2176, 'validation/loss': 4.1460875, 'validation/num_examples': 50000, 'test/accuracy': 0.1633, 'test/loss': 4.517291015625, 'test/num_examples': 10000, 'score': 2524.6669387817383, 'total_duration': 3214.8443591594696, 'accumulated_submission_time': 2524.6669387817383, 'accumulated_eval_time': 686.7388386726379, 'accumulated_logging_time': 0.1181178092956543, 'global_step': 6388, 'preemption_count': 0}), (7462, {'train/accuracy': 0.28169921875, 'train/loss': 3.706548156738281, 'validation/accuracy': 0.26448, 'validation/loss': 3.8317990625, 'validation/num_examples': 50000, 'test/accuracy': 0.2026, 'test/loss': 4.2466328125, 'test/num_examples': 10000, 'score': 2944.3297176361084, 'total_duration': 3735.773869752884, 'accumulated_submission_time': 2944.3297176361084, 'accumulated_eval_time': 787.4422793388367, 'accumulated_logging_time': 0.13861989974975586, 'global_step': 7462, 'preemption_count': 0}), (8535, {'train/accuracy': 0.3198828125, 'train/loss': 3.4558209228515624, 'validation/accuracy': 0.2973, 'validation/loss': 3.59031, 'validation/num_examples': 50000, 'test/accuracy': 0.2331, 'test/loss': 4.040454296875, 'test/num_examples': 10000, 'score': 3364.1154477596283, 'total_duration': 4248.318153858185, 'accumulated_submission_time': 3364.1154477596283, 'accumulated_eval_time': 879.6191956996918, 'accumulated_logging_time': 0.16360235214233398, 'global_step': 8535, 'preemption_count': 0}), (9605, {'train/accuracy': 0.34962890625, 'train/loss': 3.2826504516601562, 'validation/accuracy': 0.32144, 'validation/loss': 3.426675, 'validation/num_examples': 50000, 'test/accuracy': 0.2537, 'test/loss': 3.9026375, 'test/num_examples': 10000, 'score': 3783.821884393692, 'total_duration': 4760.960906744003, 'accumulated_submission_time': 3783.821884393692, 'accumulated_eval_time': 971.9761302471161, 'accumulated_logging_time': 0.18607544898986816, 'global_step': 9605, 'preemption_count': 0}), (10671, {'train/accuracy': 0.39193359375, 'train/loss': 3.0403787231445314, 'validation/accuracy': 0.3564, 'validation/loss': 3.2020596875, 'validation/num_examples': 50000, 'test/accuracy': 0.2757, 'test/loss': 3.712549609375, 'test/num_examples': 10000, 'score': 4203.362256765366, 'total_duration': 5274.353318452835, 'accumulated_submission_time': 4203.362256765366, 'accumulated_eval_time': 1065.2571041584015, 'accumulated_logging_time': 0.2040863037109375, 'global_step': 10671, 'preemption_count': 0}), (11714, {'train/accuracy': 0.411328125, 'train/loss': 2.912144775390625, 'validation/accuracy': 0.38102, 'validation/loss': 3.08264625, 'validation/num_examples': 50000, 'test/accuracy': 0.2925, 'test/loss': 3.62970859375, 'test/num_examples': 10000, 'score': 4623.218049764633, 'total_duration': 5787.465730428696, 'accumulated_submission_time': 4623.218049764633, 'accumulated_eval_time': 1157.9530777931213, 'accumulated_logging_time': 0.22621417045593262, 'global_step': 11714, 'preemption_count': 0}), (12747, {'train/accuracy': 0.43623046875, 'train/loss': 2.7709356689453126, 'validation/accuracy': 0.40068, 'validation/loss': 2.9484196875, 'validation/num_examples': 50000, 'test/accuracy': 0.3097, 'test/loss': 3.491196484375, 'test/num_examples': 10000, 'score': 5042.7560024261475, 'total_duration': 6299.947057247162, 'accumulated_submission_time': 5042.7560024261475, 'accumulated_eval_time': 1250.3538222312927, 'accumulated_logging_time': 0.24575257301330566, 'global_step': 12747, 'preemption_count': 0}), (13810, {'train/accuracy': 0.46568359375, 'train/loss': 2.6221136474609374, 'validation/accuracy': 0.42584, 'validation/loss': 2.81882625, 'validation/num_examples': 50000, 'test/accuracy': 0.3343, 'test/loss': 3.35210859375, 'test/num_examples': 10000, 'score': 5462.277799367905, 'total_duration': 6812.0996651649475, 'accumulated_submission_time': 5462.277799367905, 'accumulated_eval_time': 1342.412626504898, 'accumulated_logging_time': 0.2652163505554199, 'global_step': 13810, 'preemption_count': 0}), (14880, {'train/accuracy': 0.49255859375, 'train/loss': 2.4854055786132814, 'validation/accuracy': 0.4492, 'validation/loss': 2.6889246875, 'validation/num_examples': 50000, 'test/accuracy': 0.3544, 'test/loss': 3.252184765625, 'test/num_examples': 10000, 'score': 5881.772726774216, 'total_duration': 7324.409709692001, 'accumulated_submission_time': 5881.772726774216, 'accumulated_eval_time': 1434.663516998291, 'accumulated_logging_time': 0.2861301898956299, 'global_step': 14880, 'preemption_count': 0}), (15953, {'train/accuracy': 0.504375, 'train/loss': 2.421617279052734, 'validation/accuracy': 0.46236, 'validation/loss': 2.626495625, 'validation/num_examples': 50000, 'test/accuracy': 0.3558, 'test/loss': 3.191871875, 'test/num_examples': 10000, 'score': 6301.590001583099, 'total_duration': 7837.051640033722, 'accumulated_submission_time': 6301.590001583099, 'accumulated_eval_time': 1526.913357257843, 'accumulated_logging_time': 0.3049025535583496, 'global_step': 15953, 'preemption_count': 0}), (17019, {'train/accuracy': 0.52095703125, 'train/loss': 2.3394671630859376, 'validation/accuracy': 0.47906, 'validation/loss': 2.54531234375, 'validation/num_examples': 50000, 'test/accuracy': 0.3709, 'test/loss': 3.1380255859375, 'test/num_examples': 10000, 'score': 6721.366400718689, 'total_duration': 8350.310612916946, 'accumulated_submission_time': 6721.366400718689, 'accumulated_eval_time': 1619.8226809501648, 'accumulated_logging_time': 0.32584643363952637, 'global_step': 17019, 'preemption_count': 0}), (18083, {'train/accuracy': 0.53625, 'train/loss': 2.2731942749023437, 'validation/accuracy': 0.49074, 'validation/loss': 2.49163375, 'validation/num_examples': 50000, 'test/accuracy': 0.3859, 'test/loss': 3.075822265625, 'test/num_examples': 10000, 'score': 7141.044600009918, 'total_duration': 8863.405514240265, 'accumulated_submission_time': 7141.044600009918, 'accumulated_eval_time': 1712.6671342849731, 'accumulated_logging_time': 0.3473656177520752, 'global_step': 18083, 'preemption_count': 0}), (19145, {'train/accuracy': 0.5488671875, 'train/loss': 2.2381442260742186, 'validation/accuracy': 0.50174, 'validation/loss': 2.46621328125, 'validation/num_examples': 50000, 'test/accuracy': 0.3973, 'test/loss': 3.0374328125, 'test/num_examples': 10000, 'score': 7560.860235214233, 'total_duration': 9376.760954618454, 'accumulated_submission_time': 7560.860235214233, 'accumulated_eval_time': 1805.6333637237549, 'accumulated_logging_time': 0.36609339714050293, 'global_step': 19145, 'preemption_count': 0}), (20209, {'train/accuracy': 0.55865234375, 'train/loss': 2.1180075073242186, 'validation/accuracy': 0.51044, 'validation/loss': 2.35282984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4054, 'test/loss': 2.9408439453125, 'test/num_examples': 10000, 'score': 7980.481246232986, 'total_duration': 9889.546065568924, 'accumulated_submission_time': 7980.481246232986, 'accumulated_eval_time': 1898.2321481704712, 'accumulated_logging_time': 0.38515138626098633, 'global_step': 20209, 'preemption_count': 0}), (21275, {'train/accuracy': 0.57646484375, 'train/loss': 2.047589874267578, 'validation/accuracy': 0.52882, 'validation/loss': 2.28533953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4209, 'test/loss': 2.8756140625, 'test/num_examples': 10000, 'score': 8400.216166973114, 'total_duration': 10405.705049753189, 'accumulated_submission_time': 8400.216166973114, 'accumulated_eval_time': 1994.0816884040833, 'accumulated_logging_time': 0.40603137016296387, 'global_step': 21275, 'preemption_count': 0}), (22353, {'train/accuracy': 0.58939453125, 'train/loss': 1.9871305847167968, 'validation/accuracy': 0.536, 'validation/loss': 2.235166875, 'validation/num_examples': 50000, 'test/accuracy': 0.4206, 'test/loss': 2.8382515625, 'test/num_examples': 10000, 'score': 8819.929946422577, 'total_duration': 10918.568313121796, 'accumulated_submission_time': 8819.929946422577, 'accumulated_eval_time': 2086.6588463783264, 'accumulated_logging_time': 0.42879509925842285, 'global_step': 22353, 'preemption_count': 0}), (23423, {'train/accuracy': 0.60080078125, 'train/loss': 1.9434852600097656, 'validation/accuracy': 0.54488, 'validation/loss': 2.190995, 'validation/num_examples': 50000, 'test/accuracy': 0.4249, 'test/loss': 2.7983130859375, 'test/num_examples': 10000, 'score': 9239.430367469788, 'total_duration': 11431.135954856873, 'accumulated_submission_time': 9239.430367469788, 'accumulated_eval_time': 2179.1526477336884, 'accumulated_logging_time': 0.45078301429748535, 'global_step': 23423, 'preemption_count': 0}), (24485, {'train/accuracy': 0.6076171875, 'train/loss': 1.9141189575195312, 'validation/accuracy': 0.55382, 'validation/loss': 2.16075765625, 'validation/num_examples': 50000, 'test/accuracy': 0.4404, 'test/loss': 2.774184765625, 'test/num_examples': 10000, 'score': 9659.115862369537, 'total_duration': 11943.532006978989, 'accumulated_submission_time': 9659.115862369537, 'accumulated_eval_time': 2271.287932872772, 'accumulated_logging_time': 0.47638821601867676, 'global_step': 24485, 'preemption_count': 0}), (25546, {'train/accuracy': 0.6197265625, 'train/loss': 1.8329373168945313, 'validation/accuracy': 0.5616, 'validation/loss': 2.0934059375, 'validation/num_examples': 50000, 'test/accuracy': 0.4454, 'test/loss': 2.701642578125, 'test/num_examples': 10000, 'score': 10078.788527965546, 'total_duration': 12456.312270641327, 'accumulated_submission_time': 10078.788527965546, 'accumulated_eval_time': 2363.82874083519, 'accumulated_logging_time': 0.5005838871002197, 'global_step': 25546, 'preemption_count': 0}), (26611, {'train/accuracy': 0.62318359375, 'train/loss': 1.8171615600585938, 'validation/accuracy': 0.5673, 'validation/loss': 2.085154375, 'validation/num_examples': 50000, 'test/accuracy': 0.4563, 'test/loss': 2.67572890625, 'test/num_examples': 10000, 'score': 10498.310272455215, 'total_duration': 12969.480365514755, 'accumulated_submission_time': 10498.310272455215, 'accumulated_eval_time': 2456.9049117565155, 'accumulated_logging_time': 0.522892951965332, 'global_step': 26611, 'preemption_count': 0}), (27674, {'train/accuracy': 0.63560546875, 'train/loss': 1.7649761962890624, 'validation/accuracy': 0.57756, 'validation/loss': 2.0276146875, 'validation/num_examples': 50000, 'test/accuracy': 0.4557, 'test/loss': 2.6427314453125, 'test/num_examples': 10000, 'score': 10917.897287845612, 'total_duration': 13483.003182649612, 'accumulated_submission_time': 10917.897287845612, 'accumulated_eval_time': 2550.272453069687, 'accumulated_logging_time': 0.5447568893432617, 'global_step': 27674, 'preemption_count': 0}), (28000, {'train/accuracy': 0.64216796875, 'train/loss': 1.736297607421875, 'validation/accuracy': 0.58066, 'validation/loss': 2.0111953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4591, 'test/loss': 2.6366369140625, 'test/num_examples': 10000, 'score': 11044.543913602829, 'total_duration': 13702.285175561905, 'accumulated_submission_time': 11044.543913602829, 'accumulated_eval_time': 2642.717852830887, 'accumulated_logging_time': 0.5644726753234863, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0520 07:12:42.638054 140384716638016 submission_runner.py:587] Timing: 11044.543913602829
I0520 07:12:42.638112 140384716638016 submission_runner.py:588] ====================
I0520 07:12:42.638261 140384716638016 submission_runner.py:651] Final imagenet_vit score: 11044.543913602829
