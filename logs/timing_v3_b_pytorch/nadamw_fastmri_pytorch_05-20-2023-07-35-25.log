torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_b_pytorch/timing_nadamw --overwrite=True --save_checkpoints=False --max_global_steps=5428 2>&1 | tee -a /logs/fastmri_pytorch_05-20-2023-07-35-25.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0520 07:35:48.738292 140040208774976 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0520 07:35:48.738329 140698294208320 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0520 07:35:48.738353 139638190348096 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0520 07:35:48.738383 139809175062336 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0520 07:35:48.739091 140015760492352 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0520 07:35:48.739250 139776086329152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0520 07:35:49.725860 139885904201536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0520 07:35:49.726417 139885904201536 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:35:49.726470 139757714609984 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0520 07:35:49.726881 139757714609984 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:35:49.731044 140698294208320 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:35:49.731081 139638190348096 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:35:49.731091 140015760492352 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:35:49.731036 140040208774976 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:35:49.731157 139776086329152 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:35:49.731172 139809175062336 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 07:35:50.321706 139757714609984 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_b_pytorch/timing_nadamw/fastmri_pytorch.
W0520 07:35:50.366298 140040208774976 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:35:50.366621 140015760492352 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:35:50.366849 139638190348096 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:35:50.368671 139885904201536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:35:50.369110 139776086329152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:35:50.370202 139809175062336 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:35:50.370476 140698294208320 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 07:35:50.370718 139757714609984 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0520 07:35:50.375671 139757714609984 submission_runner.py:544] Using RNG seed 561000070
I0520 07:35:50.376934 139757714609984 submission_runner.py:553] --- Tuning run 1/1 ---
I0520 07:35:50.377046 139757714609984 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_b_pytorch/timing_nadamw/fastmri_pytorch/trial_1.
I0520 07:35:50.377381 139757714609984 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/fastmri_pytorch/trial_1/hparams.json.
I0520 07:35:50.378419 139757714609984 submission_runner.py:241] Initializing dataset.
I0520 07:35:50.378544 139757714609984 submission_runner.py:248] Initializing model.
I0520 07:35:54.512611 139757714609984 submission_runner.py:258] Initializing optimizer.
I0520 07:35:54.513427 139757714609984 submission_runner.py:265] Initializing metrics bundle.
I0520 07:35:54.513562 139757714609984 submission_runner.py:283] Initializing checkpoint and logger.
I0520 07:35:54.516930 139757714609984 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0520 07:35:54.517037 139757714609984 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0520 07:35:54.994231 139757714609984 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/fastmri_pytorch/trial_1/meta_data_0.json.
I0520 07:35:54.995141 139757714609984 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/fastmri_pytorch/trial_1/flags_0.json.
I0520 07:35:55.049326 139757714609984 submission_runner.py:319] Starting training loop.
I0520 07:36:42.238393 139715504232192 logging_writer.py:48] [0] global_step=0, grad_norm=4.288869, loss=0.942247
I0520 07:36:42.246455 139757714609984 submission.py:296] 0) loss = 0.942, grad_norm = 4.289
I0520 07:36:42.247224 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:38:21.309710 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:39:25.260323 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:40:26.280308 139757714609984 submission_runner.py:421] Time since start: 271.23s, 	Step: 1, 	{'train/ssim': 0.20803960732051305, 'train/loss': 1.0006442070007324, 'validation/ssim': 0.20153477142282464, 'validation/loss': 1.016404546373804, 'validation/num_examples': 3554, 'test/ssim': 0.2236656395625349, 'test/loss': 1.0144664058136694, 'test/num_examples': 3581, 'score': 47.19718599319458, 'total_duration': 271.2316882610321, 'accumulated_submission_time': 47.19718599319458, 'accumulated_eval_time': 224.0330147743225, 'accumulated_logging_time': 0}
I0520 07:40:26.297072 139691772856064 logging_writer.py:48] [1] accumulated_eval_time=224.033015, accumulated_logging_time=0, accumulated_submission_time=47.197186, global_step=1, preemption_count=0, score=47.197186, test/loss=1.014466, test/num_examples=3581, test/ssim=0.223666, total_duration=271.231688, train/loss=1.000644, train/ssim=0.208040, validation/loss=1.016405, validation/num_examples=3554, validation/ssim=0.201535
I0520 07:40:26.322259 139757714609984 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:40:26.322237 139809175062336 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:40:26.322235 139885904201536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:40:26.322237 140015760492352 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:40:26.322238 140040208774976 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:40:26.322244 139776086329152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:40:26.322306 140698294208320 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:40:26.322347 139638190348096 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 07:40:26.383149 139691764463360 logging_writer.py:48] [1] global_step=1, grad_norm=4.287898, loss=1.020757
I0520 07:40:26.389204 139757714609984 submission.py:296] 1) loss = 1.021, grad_norm = 4.288
I0520 07:40:26.462213 139691772856064 logging_writer.py:48] [2] global_step=2, grad_norm=3.601022, loss=0.999093
I0520 07:40:26.469119 139757714609984 submission.py:296] 2) loss = 0.999, grad_norm = 3.601
I0520 07:40:26.547815 139691764463360 logging_writer.py:48] [3] global_step=3, grad_norm=4.507820, loss=0.956680
I0520 07:40:26.551098 139757714609984 submission.py:296] 3) loss = 0.957, grad_norm = 4.508
I0520 07:40:26.628718 139691772856064 logging_writer.py:48] [4] global_step=4, grad_norm=4.580523, loss=0.968920
I0520 07:40:26.633328 139757714609984 submission.py:296] 4) loss = 0.969, grad_norm = 4.581
I0520 07:40:26.713025 139691764463360 logging_writer.py:48] [5] global_step=5, grad_norm=3.674592, loss=0.980551
I0520 07:40:26.718801 139757714609984 submission.py:296] 5) loss = 0.981, grad_norm = 3.675
I0520 07:40:26.800534 139691772856064 logging_writer.py:48] [6] global_step=6, grad_norm=4.309069, loss=0.952376
I0520 07:40:26.806523 139757714609984 submission.py:296] 6) loss = 0.952, grad_norm = 4.309
I0520 07:40:26.880764 139691764463360 logging_writer.py:48] [7] global_step=7, grad_norm=4.204575, loss=0.973238
I0520 07:40:26.886149 139757714609984 submission.py:296] 7) loss = 0.973, grad_norm = 4.205
I0520 07:40:26.967128 139691772856064 logging_writer.py:48] [8] global_step=8, grad_norm=4.138628, loss=0.943422
I0520 07:40:26.973246 139757714609984 submission.py:296] 8) loss = 0.943, grad_norm = 4.139
I0520 07:40:27.056000 139691764463360 logging_writer.py:48] [9] global_step=9, grad_norm=4.237453, loss=0.980953
I0520 07:40:27.061590 139757714609984 submission.py:296] 9) loss = 0.981, grad_norm = 4.237
I0520 07:40:27.134208 139691772856064 logging_writer.py:48] [10] global_step=10, grad_norm=3.871797, loss=0.908368
I0520 07:40:27.137778 139757714609984 submission.py:296] 10) loss = 0.908, grad_norm = 3.872
I0520 07:40:27.208335 139691764463360 logging_writer.py:48] [11] global_step=11, grad_norm=4.090175, loss=0.865725
I0520 07:40:27.214118 139757714609984 submission.py:296] 11) loss = 0.866, grad_norm = 4.090
I0520 07:40:27.291191 139691772856064 logging_writer.py:48] [12] global_step=12, grad_norm=3.950390, loss=0.880837
I0520 07:40:27.296890 139757714609984 submission.py:296] 12) loss = 0.881, grad_norm = 3.950
I0520 07:40:27.376531 139691764463360 logging_writer.py:48] [13] global_step=13, grad_norm=3.647883, loss=0.933231
I0520 07:40:27.380102 139757714609984 submission.py:296] 13) loss = 0.933, grad_norm = 3.648
I0520 07:40:27.449530 139691772856064 logging_writer.py:48] [14] global_step=14, grad_norm=3.685163, loss=0.920246
I0520 07:40:27.452985 139757714609984 submission.py:296] 14) loss = 0.920, grad_norm = 3.685
I0520 07:40:27.663677 139691764463360 logging_writer.py:48] [15] global_step=15, grad_norm=3.754159, loss=0.855102
I0520 07:40:27.669997 139757714609984 submission.py:296] 15) loss = 0.855, grad_norm = 3.754
I0520 07:40:27.930284 139691772856064 logging_writer.py:48] [16] global_step=16, grad_norm=3.464761, loss=0.835122
I0520 07:40:27.933686 139757714609984 submission.py:296] 16) loss = 0.835, grad_norm = 3.465
I0520 07:40:28.207185 139691764463360 logging_writer.py:48] [17] global_step=17, grad_norm=3.478910, loss=0.840624
I0520 07:40:28.211900 139757714609984 submission.py:296] 17) loss = 0.841, grad_norm = 3.479
I0520 07:40:28.485816 139691772856064 logging_writer.py:48] [18] global_step=18, grad_norm=3.214260, loss=0.840473
I0520 07:40:28.493431 139757714609984 submission.py:296] 18) loss = 0.840, grad_norm = 3.214
I0520 07:40:28.735816 139691764463360 logging_writer.py:48] [19] global_step=19, grad_norm=2.799331, loss=0.805151
I0520 07:40:28.741707 139757714609984 submission.py:296] 19) loss = 0.805, grad_norm = 2.799
I0520 07:40:29.031420 139691772856064 logging_writer.py:48] [20] global_step=20, grad_norm=2.988440, loss=0.811530
I0520 07:40:29.035673 139757714609984 submission.py:296] 20) loss = 0.812, grad_norm = 2.988
I0520 07:40:29.330228 139691764463360 logging_writer.py:48] [21] global_step=21, grad_norm=2.748231, loss=0.804811
I0520 07:40:29.335773 139757714609984 submission.py:296] 21) loss = 0.805, grad_norm = 2.748
I0520 07:40:29.563535 139691772856064 logging_writer.py:48] [22] global_step=22, grad_norm=2.612674, loss=0.747603
I0520 07:40:29.569400 139757714609984 submission.py:296] 22) loss = 0.748, grad_norm = 2.613
I0520 07:40:29.865174 139691764463360 logging_writer.py:48] [23] global_step=23, grad_norm=2.215149, loss=0.777233
I0520 07:40:29.870175 139757714609984 submission.py:296] 23) loss = 0.777, grad_norm = 2.215
I0520 07:40:30.198818 139691772856064 logging_writer.py:48] [24] global_step=24, grad_norm=2.196575, loss=0.767202
I0520 07:40:30.204350 139757714609984 submission.py:296] 24) loss = 0.767, grad_norm = 2.197
I0520 07:40:30.495352 139691764463360 logging_writer.py:48] [25] global_step=25, grad_norm=2.129455, loss=0.769956
I0520 07:40:30.501089 139757714609984 submission.py:296] 25) loss = 0.770, grad_norm = 2.129
I0520 07:40:30.685267 139691772856064 logging_writer.py:48] [26] global_step=26, grad_norm=1.885012, loss=0.760909
I0520 07:40:30.691714 139757714609984 submission.py:296] 26) loss = 0.761, grad_norm = 1.885
I0520 07:40:30.965684 139691764463360 logging_writer.py:48] [27] global_step=27, grad_norm=1.892405, loss=0.646587
I0520 07:40:30.969091 139757714609984 submission.py:296] 27) loss = 0.647, grad_norm = 1.892
I0520 07:40:31.249670 139691772856064 logging_writer.py:48] [28] global_step=28, grad_norm=1.801569, loss=0.757039
I0520 07:40:31.253720 139757714609984 submission.py:296] 28) loss = 0.757, grad_norm = 1.802
I0520 07:40:31.529570 139691764463360 logging_writer.py:48] [29] global_step=29, grad_norm=1.667918, loss=0.814173
I0520 07:40:31.533028 139757714609984 submission.py:296] 29) loss = 0.814, grad_norm = 1.668
I0520 07:40:31.802757 139691772856064 logging_writer.py:48] [30] global_step=30, grad_norm=1.715485, loss=0.645616
I0520 07:40:31.806200 139757714609984 submission.py:296] 30) loss = 0.646, grad_norm = 1.715
I0520 07:40:32.094146 139691764463360 logging_writer.py:48] [31] global_step=31, grad_norm=1.535537, loss=0.769475
I0520 07:40:32.097497 139757714609984 submission.py:296] 31) loss = 0.769, grad_norm = 1.536
I0520 07:40:32.344609 139691772856064 logging_writer.py:48] [32] global_step=32, grad_norm=1.526264, loss=0.699917
I0520 07:40:32.348069 139757714609984 submission.py:296] 32) loss = 0.700, grad_norm = 1.526
I0520 07:40:32.597331 139691764463360 logging_writer.py:48] [33] global_step=33, grad_norm=1.495619, loss=0.703468
I0520 07:40:32.600774 139757714609984 submission.py:296] 33) loss = 0.703, grad_norm = 1.496
I0520 07:40:32.855467 139691772856064 logging_writer.py:48] [34] global_step=34, grad_norm=1.493061, loss=0.675290
I0520 07:40:32.860953 139757714609984 submission.py:296] 34) loss = 0.675, grad_norm = 1.493
I0520 07:40:33.165201 139691764463360 logging_writer.py:48] [35] global_step=35, grad_norm=1.424935, loss=0.730684
I0520 07:40:33.170800 139757714609984 submission.py:296] 35) loss = 0.731, grad_norm = 1.425
I0520 07:40:33.414329 139691772856064 logging_writer.py:48] [36] global_step=36, grad_norm=1.428870, loss=0.642306
I0520 07:40:33.421928 139757714609984 submission.py:296] 36) loss = 0.642, grad_norm = 1.429
I0520 07:40:33.685854 139691764463360 logging_writer.py:48] [37] global_step=37, grad_norm=1.518593, loss=0.581360
I0520 07:40:33.690630 139757714609984 submission.py:296] 37) loss = 0.581, grad_norm = 1.519
I0520 07:40:33.937613 139691772856064 logging_writer.py:48] [38] global_step=38, grad_norm=1.421136, loss=0.592336
I0520 07:40:33.944526 139757714609984 submission.py:296] 38) loss = 0.592, grad_norm = 1.421
I0520 07:40:34.189913 139691764463360 logging_writer.py:48] [39] global_step=39, grad_norm=1.415731, loss=0.624623
I0520 07:40:34.194445 139757714609984 submission.py:296] 39) loss = 0.625, grad_norm = 1.416
I0520 07:40:34.488291 139691772856064 logging_writer.py:48] [40] global_step=40, grad_norm=1.407162, loss=0.650635
I0520 07:40:34.493975 139757714609984 submission.py:296] 40) loss = 0.651, grad_norm = 1.407
I0520 07:40:34.831469 139691764463360 logging_writer.py:48] [41] global_step=41, grad_norm=1.357783, loss=0.630663
I0520 07:40:34.837688 139757714609984 submission.py:296] 41) loss = 0.631, grad_norm = 1.358
I0520 07:40:35.084741 139691772856064 logging_writer.py:48] [42] global_step=42, grad_norm=1.367138, loss=0.666908
I0520 07:40:35.090486 139757714609984 submission.py:296] 42) loss = 0.667, grad_norm = 1.367
I0520 07:40:35.378361 139691764463360 logging_writer.py:48] [43] global_step=43, grad_norm=1.383336, loss=0.588790
I0520 07:40:35.384565 139757714609984 submission.py:296] 43) loss = 0.589, grad_norm = 1.383
I0520 07:40:35.696132 139691772856064 logging_writer.py:48] [44] global_step=44, grad_norm=1.340297, loss=0.640949
I0520 07:40:35.700224 139757714609984 submission.py:296] 44) loss = 0.641, grad_norm = 1.340
I0520 07:40:35.918391 139691764463360 logging_writer.py:48] [45] global_step=45, grad_norm=1.257702, loss=0.688627
I0520 07:40:35.922094 139757714609984 submission.py:296] 45) loss = 0.689, grad_norm = 1.258
I0520 07:40:36.167853 139691772856064 logging_writer.py:48] [46] global_step=46, grad_norm=1.289098, loss=0.576887
I0520 07:40:36.171753 139757714609984 submission.py:296] 46) loss = 0.577, grad_norm = 1.289
I0520 07:40:36.483500 139691764463360 logging_writer.py:48] [47] global_step=47, grad_norm=1.238045, loss=0.598493
I0520 07:40:36.486988 139757714609984 submission.py:296] 47) loss = 0.598, grad_norm = 1.238
I0520 07:40:36.742403 139691772856064 logging_writer.py:48] [48] global_step=48, grad_norm=1.235310, loss=0.608338
I0520 07:40:36.746706 139757714609984 submission.py:296] 48) loss = 0.608, grad_norm = 1.235
I0520 07:40:36.992820 139691764463360 logging_writer.py:48] [49] global_step=49, grad_norm=1.210642, loss=0.591135
I0520 07:40:36.999160 139757714609984 submission.py:296] 49) loss = 0.591, grad_norm = 1.211
I0520 07:40:37.276032 139691772856064 logging_writer.py:48] [50] global_step=50, grad_norm=1.189144, loss=0.585643
I0520 07:40:37.279358 139757714609984 submission.py:296] 50) loss = 0.586, grad_norm = 1.189
I0520 07:40:37.588052 139691764463360 logging_writer.py:48] [51] global_step=51, grad_norm=1.147062, loss=0.604833
I0520 07:40:37.592148 139757714609984 submission.py:296] 51) loss = 0.605, grad_norm = 1.147
I0520 07:40:37.823351 139691772856064 logging_writer.py:48] [52] global_step=52, grad_norm=1.148087, loss=0.544333
I0520 07:40:37.830073 139757714609984 submission.py:296] 52) loss = 0.544, grad_norm = 1.148
I0520 07:40:38.123821 139691764463360 logging_writer.py:48] [53] global_step=53, grad_norm=1.132556, loss=0.593191
I0520 07:40:38.128899 139757714609984 submission.py:296] 53) loss = 0.593, grad_norm = 1.133
I0520 07:40:38.367879 139691772856064 logging_writer.py:48] [54] global_step=54, grad_norm=1.153714, loss=0.512277
I0520 07:40:38.373732 139757714609984 submission.py:296] 54) loss = 0.512, grad_norm = 1.154
I0520 07:40:38.648725 139691764463360 logging_writer.py:48] [55] global_step=55, grad_norm=1.138335, loss=0.598253
I0520 07:40:38.654931 139757714609984 submission.py:296] 55) loss = 0.598, grad_norm = 1.138
I0520 07:40:38.914165 139691772856064 logging_writer.py:48] [56] global_step=56, grad_norm=1.128105, loss=0.678066
I0520 07:40:38.919883 139757714609984 submission.py:296] 56) loss = 0.678, grad_norm = 1.128
I0520 07:40:39.156963 139691764463360 logging_writer.py:48] [57] global_step=57, grad_norm=1.149911, loss=0.512657
I0520 07:40:39.163431 139757714609984 submission.py:296] 57) loss = 0.513, grad_norm = 1.150
I0520 07:40:39.438976 139691772856064 logging_writer.py:48] [58] global_step=58, grad_norm=1.062930, loss=0.610510
I0520 07:40:39.444502 139757714609984 submission.py:296] 58) loss = 0.611, grad_norm = 1.063
I0520 07:40:39.752582 139691764463360 logging_writer.py:48] [59] global_step=59, grad_norm=1.125719, loss=0.524481
I0520 07:40:39.758733 139757714609984 submission.py:296] 59) loss = 0.524, grad_norm = 1.126
I0520 07:40:39.971166 139691772856064 logging_writer.py:48] [60] global_step=60, grad_norm=1.131489, loss=0.577091
I0520 07:40:39.976581 139757714609984 submission.py:296] 60) loss = 0.577, grad_norm = 1.131
I0520 07:40:40.255058 139691764463360 logging_writer.py:48] [61] global_step=61, grad_norm=1.137752, loss=0.479606
I0520 07:40:40.259747 139757714609984 submission.py:296] 61) loss = 0.480, grad_norm = 1.138
I0520 07:40:40.565510 139691772856064 logging_writer.py:48] [62] global_step=62, grad_norm=1.127248, loss=0.614310
I0520 07:40:40.568996 139757714609984 submission.py:296] 62) loss = 0.614, grad_norm = 1.127
I0520 07:40:40.844139 139691764463360 logging_writer.py:48] [63] global_step=63, grad_norm=1.119479, loss=0.495419
I0520 07:40:40.848221 139757714609984 submission.py:296] 63) loss = 0.495, grad_norm = 1.119
I0520 07:40:41.102190 139691772856064 logging_writer.py:48] [64] global_step=64, grad_norm=1.099477, loss=0.448376
I0520 07:40:41.105801 139757714609984 submission.py:296] 64) loss = 0.448, grad_norm = 1.099
I0520 07:40:41.379032 139691764463360 logging_writer.py:48] [65] global_step=65, grad_norm=1.055296, loss=0.448830
I0520 07:40:41.384220 139757714609984 submission.py:296] 65) loss = 0.449, grad_norm = 1.055
I0520 07:40:41.652406 139691772856064 logging_writer.py:48] [66] global_step=66, grad_norm=1.043843, loss=0.536071
I0520 07:40:41.658864 139757714609984 submission.py:296] 66) loss = 0.536, grad_norm = 1.044
I0520 07:40:41.969755 139691764463360 logging_writer.py:48] [67] global_step=67, grad_norm=1.040240, loss=0.422173
I0520 07:40:41.975679 139757714609984 submission.py:296] 67) loss = 0.422, grad_norm = 1.040
I0520 07:40:42.209919 139691772856064 logging_writer.py:48] [68] global_step=68, grad_norm=1.026677, loss=0.421797
I0520 07:40:42.215248 139757714609984 submission.py:296] 68) loss = 0.422, grad_norm = 1.027
I0520 07:40:42.476467 139691764463360 logging_writer.py:48] [69] global_step=69, grad_norm=0.987622, loss=0.510993
I0520 07:40:42.482393 139757714609984 submission.py:296] 69) loss = 0.511, grad_norm = 0.988
I0520 07:40:42.742278 139691772856064 logging_writer.py:48] [70] global_step=70, grad_norm=1.017580, loss=0.507500
I0520 07:40:42.745749 139757714609984 submission.py:296] 70) loss = 0.508, grad_norm = 1.018
I0520 07:40:43.028766 139691764463360 logging_writer.py:48] [71] global_step=71, grad_norm=1.007727, loss=0.458980
I0520 07:40:43.032406 139757714609984 submission.py:296] 71) loss = 0.459, grad_norm = 1.008
I0520 07:40:43.306273 139691772856064 logging_writer.py:48] [72] global_step=72, grad_norm=0.971874, loss=0.517668
I0520 07:40:43.309911 139757714609984 submission.py:296] 72) loss = 0.518, grad_norm = 0.972
I0520 07:40:43.544847 139691764463360 logging_writer.py:48] [73] global_step=73, grad_norm=0.996082, loss=0.378290
I0520 07:40:43.550365 139757714609984 submission.py:296] 73) loss = 0.378, grad_norm = 0.996
I0520 07:40:43.834210 139691772856064 logging_writer.py:48] [74] global_step=74, grad_norm=0.927227, loss=0.467995
I0520 07:40:43.839675 139757714609984 submission.py:296] 74) loss = 0.468, grad_norm = 0.927
I0520 07:40:44.076188 139691764463360 logging_writer.py:48] [75] global_step=75, grad_norm=0.913959, loss=0.385855
I0520 07:40:44.081860 139757714609984 submission.py:296] 75) loss = 0.386, grad_norm = 0.914
I0520 07:40:44.421818 139691772856064 logging_writer.py:48] [76] global_step=76, grad_norm=0.877281, loss=0.450529
I0520 07:40:44.426277 139757714609984 submission.py:296] 76) loss = 0.451, grad_norm = 0.877
I0520 07:40:44.775192 139691764463360 logging_writer.py:48] [77] global_step=77, grad_norm=0.850459, loss=0.377025
I0520 07:40:44.781243 139757714609984 submission.py:296] 77) loss = 0.377, grad_norm = 0.850
I0520 07:40:44.982586 139691772856064 logging_writer.py:48] [78] global_step=78, grad_norm=0.814071, loss=0.374674
I0520 07:40:44.987671 139757714609984 submission.py:296] 78) loss = 0.375, grad_norm = 0.814
I0520 07:40:45.330625 139691764463360 logging_writer.py:48] [79] global_step=79, grad_norm=0.793625, loss=0.414311
I0520 07:40:45.335149 139757714609984 submission.py:296] 79) loss = 0.414, grad_norm = 0.794
I0520 07:40:45.581994 139691772856064 logging_writer.py:48] [80] global_step=80, grad_norm=0.844392, loss=0.334853
I0520 07:40:45.585853 139757714609984 submission.py:296] 80) loss = 0.335, grad_norm = 0.844
I0520 07:40:45.881548 139691764463360 logging_writer.py:48] [81] global_step=81, grad_norm=0.826527, loss=0.370278
I0520 07:40:45.885329 139757714609984 submission.py:296] 81) loss = 0.370, grad_norm = 0.827
I0520 07:40:46.162775 139691772856064 logging_writer.py:48] [82] global_step=82, grad_norm=0.859706, loss=0.351565
I0520 07:40:46.166444 139757714609984 submission.py:296] 82) loss = 0.352, grad_norm = 0.860
I0520 07:40:46.491335 139691764463360 logging_writer.py:48] [83] global_step=83, grad_norm=0.761340, loss=0.428864
I0520 07:40:46.494997 139757714609984 submission.py:296] 83) loss = 0.429, grad_norm = 0.761
I0520 07:40:46.780827 139691772856064 logging_writer.py:48] [84] global_step=84, grad_norm=0.775081, loss=0.397217
I0520 07:40:46.785653 139757714609984 submission.py:296] 84) loss = 0.397, grad_norm = 0.775
I0520 07:40:47.056945 139691764463360 logging_writer.py:48] [85] global_step=85, grad_norm=0.730620, loss=0.344292
I0520 07:40:47.063367 139757714609984 submission.py:296] 85) loss = 0.344, grad_norm = 0.731
I0520 07:40:47.325752 139691772856064 logging_writer.py:48] [86] global_step=86, grad_norm=0.658222, loss=0.398113
I0520 07:40:47.331955 139757714609984 submission.py:296] 86) loss = 0.398, grad_norm = 0.658
I0520 07:40:47.530038 139691764463360 logging_writer.py:48] [87] global_step=87, grad_norm=0.653383, loss=0.324074
I0520 07:40:47.533362 139757714609984 submission.py:296] 87) loss = 0.324, grad_norm = 0.653
I0520 07:40:47.837522 139691772856064 logging_writer.py:48] [88] global_step=88, grad_norm=0.631734, loss=0.367805
I0520 07:40:47.841401 139757714609984 submission.py:296] 88) loss = 0.368, grad_norm = 0.632
I0520 07:40:48.144795 139691764463360 logging_writer.py:48] [89] global_step=89, grad_norm=0.598332, loss=0.338411
I0520 07:40:48.148714 139757714609984 submission.py:296] 89) loss = 0.338, grad_norm = 0.598
I0520 07:40:48.446191 139691772856064 logging_writer.py:48] [90] global_step=90, grad_norm=0.545993, loss=0.330936
I0520 07:40:48.449634 139757714609984 submission.py:296] 90) loss = 0.331, grad_norm = 0.546
I0520 07:40:48.793965 139691764463360 logging_writer.py:48] [91] global_step=91, grad_norm=0.582493, loss=0.405168
I0520 07:40:48.801193 139757714609984 submission.py:296] 91) loss = 0.405, grad_norm = 0.582
I0520 07:40:49.053427 139691772856064 logging_writer.py:48] [92] global_step=92, grad_norm=0.535047, loss=0.343116
I0520 07:40:49.059006 139757714609984 submission.py:296] 92) loss = 0.343, grad_norm = 0.535
I0520 07:40:49.273171 139691764463360 logging_writer.py:48] [93] global_step=93, grad_norm=0.478583, loss=0.293264
I0520 07:40:49.276655 139757714609984 submission.py:296] 93) loss = 0.293, grad_norm = 0.479
I0520 07:40:49.622215 139691772856064 logging_writer.py:48] [94] global_step=94, grad_norm=0.465488, loss=0.281374
I0520 07:40:49.625898 139757714609984 submission.py:296] 94) loss = 0.281, grad_norm = 0.465
I0520 07:40:49.849197 139691764463360 logging_writer.py:48] [95] global_step=95, grad_norm=0.401840, loss=0.280310
I0520 07:40:49.852794 139757714609984 submission.py:296] 95) loss = 0.280, grad_norm = 0.402
I0520 07:40:50.138800 139691772856064 logging_writer.py:48] [96] global_step=96, grad_norm=0.405345, loss=0.293807
I0520 07:40:50.142328 139757714609984 submission.py:296] 96) loss = 0.294, grad_norm = 0.405
I0520 07:40:50.451803 139691764463360 logging_writer.py:48] [97] global_step=97, grad_norm=0.380406, loss=0.301307
I0520 07:40:50.457233 139757714609984 submission.py:296] 97) loss = 0.301, grad_norm = 0.380
I0520 07:40:50.770521 139691772856064 logging_writer.py:48] [98] global_step=98, grad_norm=0.415791, loss=0.320490
I0520 07:40:50.773941 139757714609984 submission.py:296] 98) loss = 0.320, grad_norm = 0.416
I0520 07:40:51.056134 139691764463360 logging_writer.py:48] [99] global_step=99, grad_norm=0.374129, loss=0.295825
I0520 07:40:51.060553 139757714609984 submission.py:296] 99) loss = 0.296, grad_norm = 0.374
I0520 07:40:51.274949 139691772856064 logging_writer.py:48] [100] global_step=100, grad_norm=0.320420, loss=0.327447
I0520 07:40:51.280189 139757714609984 submission.py:296] 100) loss = 0.327, grad_norm = 0.320
I0520 07:41:46.364866 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:41:48.518966 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:41:50.747274 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:41:52.924221 139757714609984 submission_runner.py:421] Time since start: 357.88s, 	Step: 303, 	{'train/ssim': 0.6950137274605888, 'train/loss': 0.3096295084272112, 'validation/ssim': 0.6748840984629995, 'validation/loss': 0.33195280076146594, 'validation/num_examples': 3554, 'test/ssim': 0.693282471725775, 'test/loss': 0.3333463096245811, 'test/num_examples': 3581, 'score': 124.25231504440308, 'total_duration': 357.87564063072205, 'accumulated_submission_time': 124.25231504440308, 'accumulated_eval_time': 230.59247970581055, 'accumulated_logging_time': 0.025751113891601562}
I0520 07:41:52.936665 139691764463360 logging_writer.py:48] [303] accumulated_eval_time=230.592480, accumulated_logging_time=0.025751, accumulated_submission_time=124.252315, global_step=303, preemption_count=0, score=124.252315, test/loss=0.333346, test/num_examples=3581, test/ssim=0.693282, total_duration=357.875641, train/loss=0.309630, train/ssim=0.695014, validation/loss=0.331953, validation/num_examples=3554, validation/ssim=0.674884
I0520 07:42:55.886038 139691772856064 logging_writer.py:48] [500] global_step=500, grad_norm=0.257218, loss=0.290363
I0520 07:42:55.889997 139757714609984 submission.py:296] 500) loss = 0.290, grad_norm = 0.257
I0520 07:43:13.215496 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:43:15.378833 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:43:17.589844 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:43:19.738798 139757714609984 submission_runner.py:421] Time since start: 444.69s, 	Step: 547, 	{'train/ssim': 0.715353148324149, 'train/loss': 0.2916774409157889, 'validation/ssim': 0.6953204685741418, 'validation/loss': 0.31276845851505347, 'validation/num_examples': 3554, 'test/ssim': 0.7128607636222424, 'test/loss': 0.31496765426469564, 'test/num_examples': 3581, 'score': 200.8668553829193, 'total_duration': 444.6901783943176, 'accumulated_submission_time': 200.8668553829193, 'accumulated_eval_time': 237.11579155921936, 'accumulated_logging_time': 0.05092024803161621}
I0520 07:43:19.754183 139691764463360 logging_writer.py:48] [547] accumulated_eval_time=237.115792, accumulated_logging_time=0.050920, accumulated_submission_time=200.866855, global_step=547, preemption_count=0, score=200.866855, test/loss=0.314968, test/num_examples=3581, test/ssim=0.712861, total_duration=444.690178, train/loss=0.291677, train/ssim=0.715353, validation/loss=0.312768, validation/num_examples=3554, validation/ssim=0.695320
I0520 07:44:39.822068 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:44:41.931592 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:44:44.148426 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:44:46.316836 139757714609984 submission_runner.py:421] Time since start: 531.27s, 	Step: 776, 	{'train/ssim': 0.7226311138698033, 'train/loss': 0.2850535937717983, 'validation/ssim': 0.7030010062385692, 'validation/loss': 0.30560106954751687, 'validation/num_examples': 3554, 'test/ssim': 0.7203456747635087, 'test/loss': 0.3078970186073199, 'test/num_examples': 3581, 'score': 277.47726678848267, 'total_duration': 531.2682504653931, 'accumulated_submission_time': 277.47726678848267, 'accumulated_eval_time': 243.61060309410095, 'accumulated_logging_time': 0.07763409614562988}
I0520 07:44:46.328733 139691772856064 logging_writer.py:48] [776] accumulated_eval_time=243.610603, accumulated_logging_time=0.077634, accumulated_submission_time=277.477267, global_step=776, preemption_count=0, score=277.477267, test/loss=0.307897, test/num_examples=3581, test/ssim=0.720346, total_duration=531.268250, train/loss=0.285054, train/ssim=0.722631, validation/loss=0.305601, validation/num_examples=3554, validation/ssim=0.703001
I0520 07:46:02.742763 139691764463360 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.249476, loss=0.205113
I0520 07:46:02.747040 139757714609984 submission.py:296] 1000) loss = 0.205, grad_norm = 0.249
I0520 07:46:06.537346 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:46:08.616602 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:46:10.729114 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:46:12.796393 139757714609984 submission_runner.py:421] Time since start: 617.75s, 	Step: 1015, 	{'train/ssim': 0.7279258455548968, 'train/loss': 0.28092520577566965, 'validation/ssim': 0.7082511287897439, 'validation/loss': 0.301424471930835, 'validation/num_examples': 3554, 'test/ssim': 0.725484490627618, 'test/loss': 0.30348407965870217, 'test/num_examples': 3581, 'score': 354.07655811309814, 'total_duration': 617.7478160858154, 'accumulated_submission_time': 354.07655811309814, 'accumulated_eval_time': 249.86967539787292, 'accumulated_logging_time': 0.10522580146789551}
I0520 07:46:12.806467 139691772856064 logging_writer.py:48] [1015] accumulated_eval_time=249.869675, accumulated_logging_time=0.105226, accumulated_submission_time=354.076558, global_step=1015, preemption_count=0, score=354.076558, test/loss=0.303484, test/num_examples=3581, test/ssim=0.725484, total_duration=617.747816, train/loss=0.280925, train/ssim=0.727926, validation/loss=0.301424, validation/num_examples=3554, validation/ssim=0.708251
I0520 07:47:33.082225 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:47:35.144620 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:47:37.270679 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:47:39.333050 139757714609984 submission_runner.py:421] Time since start: 704.28s, 	Step: 1323, 	{'train/ssim': 0.7270787102835519, 'train/loss': 0.2780165672302246, 'validation/ssim': 0.7087465542786298, 'validation/loss': 0.2982167086711276, 'validation/num_examples': 3554, 'test/ssim': 0.7251933081061506, 'test/loss': 0.3003026498359397, 'test/num_examples': 3581, 'score': 428.2650763988495, 'total_duration': 704.2844815254211, 'accumulated_submission_time': 428.2650763988495, 'accumulated_eval_time': 256.12060928344727, 'accumulated_logging_time': 0.12419795989990234}
I0520 07:47:39.344490 139691764463360 logging_writer.py:48] [1323] accumulated_eval_time=256.120609, accumulated_logging_time=0.124198, accumulated_submission_time=428.265076, global_step=1323, preemption_count=0, score=428.265076, test/loss=0.300303, test/num_examples=3581, test/ssim=0.725193, total_duration=704.284482, train/loss=0.278017, train/ssim=0.727079, validation/loss=0.298217, validation/num_examples=3554, validation/ssim=0.708747
I0520 07:48:24.786657 139691772856064 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.272266, loss=0.250672
I0520 07:48:24.790232 139757714609984 submission.py:296] 1500) loss = 0.251, grad_norm = 0.272
I0520 07:48:59.609473 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:49:01.685418 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:49:03.821730 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:49:05.883087 139757714609984 submission_runner.py:421] Time since start: 790.83s, 	Step: 1630, 	{'train/ssim': 0.7331886972699847, 'train/loss': 0.27462029457092285, 'validation/ssim': 0.7127682112144766, 'validation/loss': 0.29520036292733537, 'validation/num_examples': 3554, 'test/ssim': 0.7299733102005376, 'test/loss': 0.29701772784094876, 'test/num_examples': 3581, 'score': 502.4608108997345, 'total_duration': 790.8344902992249, 'accumulated_submission_time': 502.4608108997345, 'accumulated_eval_time': 262.3941957950592, 'accumulated_logging_time': 0.1435995101928711}
I0520 07:49:05.893287 139691764463360 logging_writer.py:48] [1630] accumulated_eval_time=262.394196, accumulated_logging_time=0.143600, accumulated_submission_time=502.460811, global_step=1630, preemption_count=0, score=502.460811, test/loss=0.297018, test/num_examples=3581, test/ssim=0.729973, total_duration=790.834490, train/loss=0.274620, train/ssim=0.733189, validation/loss=0.295200, validation/num_examples=3554, validation/ssim=0.712768
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0520 07:50:26.017029 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:50:28.104645 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:50:30.222389 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:50:32.283978 139757714609984 submission_runner.py:421] Time since start: 877.24s, 	Step: 1938, 	{'train/ssim': 0.7363000597272601, 'train/loss': 0.27263220718928743, 'validation/ssim': 0.7162287019247678, 'validation/loss': 0.2931461881638647, 'validation/num_examples': 3554, 'test/ssim': 0.7333624402227031, 'test/loss': 0.29494236208679486, 'test/num_examples': 3581, 'score': 576.44158244133, 'total_duration': 877.2353978157043, 'accumulated_submission_time': 576.44158244133, 'accumulated_eval_time': 268.66123700141907, 'accumulated_logging_time': 0.1618952751159668}
I0520 07:50:32.294991 139691772856064 logging_writer.py:48] [1938] accumulated_eval_time=268.661237, accumulated_logging_time=0.161895, accumulated_submission_time=576.441582, global_step=1938, preemption_count=0, score=576.441582, test/loss=0.294942, test/num_examples=3581, test/ssim=0.733362, total_duration=877.235398, train/loss=0.272632, train/ssim=0.736300, validation/loss=0.293146, validation/num_examples=3554, validation/ssim=0.716229
I0520 07:50:46.885906 139691764463360 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.105941, loss=0.234538
I0520 07:50:46.889718 139757714609984 submission.py:296] 2000) loss = 0.235, grad_norm = 0.106
I0520 07:51:52.479874 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:51:54.570029 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:51:56.702461 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:51:58.765481 139757714609984 submission_runner.py:421] Time since start: 963.72s, 	Step: 2243, 	{'train/ssim': 0.7389851978846959, 'train/loss': 0.2712343760899135, 'validation/ssim': 0.7181986571029122, 'validation/loss': 0.29231302565111494, 'validation/num_examples': 3554, 'test/ssim': 0.7352252311461533, 'test/loss': 0.2940699735256388, 'test/num_examples': 3581, 'score': 650.5814337730408, 'total_duration': 963.7169024944305, 'accumulated_submission_time': 650.5814337730408, 'accumulated_eval_time': 274.9468936920166, 'accumulated_logging_time': 0.1816411018371582}
I0520 07:51:58.776016 139691772856064 logging_writer.py:48] [2243] accumulated_eval_time=274.946894, accumulated_logging_time=0.181641, accumulated_submission_time=650.581434, global_step=2243, preemption_count=0, score=650.581434, test/loss=0.294070, test/num_examples=3581, test/ssim=0.735225, total_duration=963.716902, train/loss=0.271234, train/ssim=0.738985, validation/loss=0.292313, validation/num_examples=3554, validation/ssim=0.718199
I0520 07:53:04.981661 139691764463360 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.102419, loss=0.243926
I0520 07:53:04.985886 139757714609984 submission.py:296] 2500) loss = 0.244, grad_norm = 0.102
I0520 07:53:19.094037 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:53:21.172630 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:53:23.287682 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:53:25.341245 139757714609984 submission_runner.py:421] Time since start: 1050.29s, 	Step: 2553, 	{'train/ssim': 0.7374082292829242, 'train/loss': 0.27132257393428255, 'validation/ssim': 0.7166252071829277, 'validation/loss': 0.2924714697567881, 'validation/num_examples': 3554, 'test/ssim': 0.7337734091385087, 'test/loss': 0.29410685709953577, 'test/num_examples': 3581, 'score': 724.776547908783, 'total_duration': 1050.2926383018494, 'accumulated_submission_time': 724.776547908783, 'accumulated_eval_time': 281.19412088394165, 'accumulated_logging_time': 0.20060467720031738}
I0520 07:53:25.352030 139691772856064 logging_writer.py:48] [2553] accumulated_eval_time=281.194121, accumulated_logging_time=0.200605, accumulated_submission_time=724.776548, global_step=2553, preemption_count=0, score=724.776548, test/loss=0.294107, test/num_examples=3581, test/ssim=0.733773, total_duration=1050.292638, train/loss=0.271323, train/ssim=0.737408, validation/loss=0.292471, validation/num_examples=3554, validation/ssim=0.716625
I0520 07:54:45.463268 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:54:47.549262 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:54:49.658960 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:54:51.710544 139757714609984 submission_runner.py:421] Time since start: 1136.66s, 	Step: 2861, 	{'train/ssim': 0.738177844456264, 'train/loss': 0.2703939505985805, 'validation/ssim': 0.7177934276299592, 'validation/loss': 0.2910148350120463, 'validation/num_examples': 3554, 'test/ssim': 0.7350903777096133, 'test/loss': 0.2925323511894373, 'test/num_examples': 3581, 'score': 798.8147954940796, 'total_duration': 1136.661961555481, 'accumulated_submission_time': 798.8147954940796, 'accumulated_eval_time': 287.44148111343384, 'accumulated_logging_time': 0.21931052207946777}
I0520 07:54:51.721872 139691764463360 logging_writer.py:48] [2861] accumulated_eval_time=287.441481, accumulated_logging_time=0.219311, accumulated_submission_time=798.814795, global_step=2861, preemption_count=0, score=798.814795, test/loss=0.292532, test/num_examples=3581, test/ssim=0.735090, total_duration=1136.661962, train/loss=0.270394, train/ssim=0.738178, validation/loss=0.291015, validation/num_examples=3554, validation/ssim=0.717793
I0520 07:55:26.670412 139691772856064 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.053930, loss=0.283992
I0520 07:55:26.674063 139757714609984 submission.py:296] 3000) loss = 0.284, grad_norm = 0.054
I0520 07:56:11.883991 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:56:13.946608 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:56:16.064292 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:56:18.124154 139757714609984 submission_runner.py:421] Time since start: 1223.08s, 	Step: 3167, 	{'train/ssim': 0.7396174839564732, 'train/loss': 0.26968285015651156, 'validation/ssim': 0.7192932369337366, 'validation/loss': 0.2902154702447946, 'validation/num_examples': 3554, 'test/ssim': 0.736596195633203, 'test/loss': 0.2917344115426906, 'test/num_examples': 3581, 'score': 872.9055123329163, 'total_duration': 1223.0755801200867, 'accumulated_submission_time': 872.9055123329163, 'accumulated_eval_time': 293.68173599243164, 'accumulated_logging_time': 0.23949503898620605}
I0520 07:56:18.134393 139691764463360 logging_writer.py:48] [3167] accumulated_eval_time=293.681736, accumulated_logging_time=0.239495, accumulated_submission_time=872.905512, global_step=3167, preemption_count=0, score=872.905512, test/loss=0.291734, test/num_examples=3581, test/ssim=0.736596, total_duration=1223.075580, train/loss=0.269683, train/ssim=0.739617, validation/loss=0.290215, validation/num_examples=3554, validation/ssim=0.719293
I0520 07:57:38.243845 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:57:40.304290 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:57:42.419863 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:57:44.482796 139757714609984 submission_runner.py:421] Time since start: 1309.43s, 	Step: 3475, 	{'train/ssim': 0.7419493538992745, 'train/loss': 0.26838007995060514, 'validation/ssim': 0.7208508184000422, 'validation/loss': 0.28956070762037495, 'validation/num_examples': 3554, 'test/ssim': 0.7380315870654147, 'test/loss': 0.2911169696073897, 'test/num_examples': 3581, 'score': 946.8371095657349, 'total_duration': 1309.434208393097, 'accumulated_submission_time': 946.8371095657349, 'accumulated_eval_time': 299.9207148551941, 'accumulated_logging_time': 0.2595381736755371}
I0520 07:57:44.493401 139691772856064 logging_writer.py:48] [3475] accumulated_eval_time=299.920715, accumulated_logging_time=0.259538, accumulated_submission_time=946.837110, global_step=3475, preemption_count=0, score=946.837110, test/loss=0.291117, test/num_examples=3581, test/ssim=0.738032, total_duration=1309.434208, train/loss=0.268380, train/ssim=0.741949, validation/loss=0.289561, validation/num_examples=3554, validation/ssim=0.720851
I0520 07:57:48.814549 139691764463360 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.056242, loss=0.264179
I0520 07:57:48.817926 139757714609984 submission.py:296] 3500) loss = 0.264, grad_norm = 0.056
I0520 07:59:04.741394 139757714609984 spec.py:298] Evaluating on the training split.
I0520 07:59:06.752515 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 07:59:08.869624 139757714609984 spec.py:326] Evaluating on the test split.
I0520 07:59:10.929884 139757714609984 submission_runner.py:421] Time since start: 1395.88s, 	Step: 3783, 	{'train/ssim': 0.7420832770211356, 'train/loss': 0.26851502486637663, 'validation/ssim': 0.7216182058288196, 'validation/loss': 0.2892218371355339, 'validation/num_examples': 3554, 'test/ssim': 0.7386382911765219, 'test/loss': 0.2907651098571454, 'test/num_examples': 3581, 'score': 1020.9630627632141, 'total_duration': 1395.8812918663025, 'accumulated_submission_time': 1020.9630627632141, 'accumulated_eval_time': 306.1092019081116, 'accumulated_logging_time': 0.278489351272583}
I0520 07:59:10.941086 139691772856064 logging_writer.py:48] [3783] accumulated_eval_time=306.109202, accumulated_logging_time=0.278489, accumulated_submission_time=1020.963063, global_step=3783, preemption_count=0, score=1020.963063, test/loss=0.290765, test/num_examples=3581, test/ssim=0.738638, total_duration=1395.881292, train/loss=0.268515, train/ssim=0.742083, validation/loss=0.289222, validation/num_examples=3554, validation/ssim=0.721618
I0520 08:00:06.990077 139691764463360 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.195887, loss=0.270128
I0520 08:00:06.993856 139757714609984 submission.py:296] 4000) loss = 0.270, grad_norm = 0.196
I0520 08:00:31.107785 139757714609984 spec.py:298] Evaluating on the training split.
I0520 08:00:33.217114 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 08:00:35.314582 139757714609984 spec.py:326] Evaluating on the test split.
I0520 08:00:37.369189 139757714609984 submission_runner.py:421] Time since start: 1482.32s, 	Step: 4091, 	{'train/ssim': 0.7397937774658203, 'train/loss': 0.26846412249973844, 'validation/ssim': 0.7192911074009919, 'validation/loss': 0.28908468835730866, 'validation/num_examples': 3554, 'test/ssim': 0.7365574712894443, 'test/loss': 0.29061201916364143, 'test/num_examples': 3581, 'score': 1094.973206281662, 'total_duration': 1482.320590019226, 'accumulated_submission_time': 1094.973206281662, 'accumulated_eval_time': 312.37057542800903, 'accumulated_logging_time': 0.29804468154907227}
I0520 08:00:37.379759 139691772856064 logging_writer.py:48] [4091] accumulated_eval_time=312.370575, accumulated_logging_time=0.298045, accumulated_submission_time=1094.973206, global_step=4091, preemption_count=0, score=1094.973206, test/loss=0.290612, test/num_examples=3581, test/ssim=0.736557, total_duration=1482.320590, train/loss=0.268464, train/ssim=0.739794, validation/loss=0.289085, validation/num_examples=3554, validation/ssim=0.719291
I0520 08:01:57.589572 139757714609984 spec.py:298] Evaluating on the training split.
I0520 08:01:59.663536 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 08:02:01.801922 139757714609984 spec.py:326] Evaluating on the test split.
I0520 08:02:03.870768 139757714609984 submission_runner.py:421] Time since start: 1568.82s, 	Step: 4396, 	{'train/ssim': 0.7424020086015973, 'train/loss': 0.2673960413251604, 'validation/ssim': 0.7212326917074071, 'validation/loss': 0.2885528547279474, 'validation/num_examples': 3554, 'test/ssim': 0.7383764246195197, 'test/loss': 0.2900883883146293, 'test/num_examples': 3581, 'score': 1169.2557773590088, 'total_duration': 1568.8221848011017, 'accumulated_submission_time': 1169.2557773590088, 'accumulated_eval_time': 318.6517617702484, 'accumulated_logging_time': 0.3165264129638672}
I0520 08:02:03.881363 139691764463360 logging_writer.py:48] [4396] accumulated_eval_time=318.651762, accumulated_logging_time=0.316526, accumulated_submission_time=1169.255777, global_step=4396, preemption_count=0, score=1169.255777, test/loss=0.290088, test/num_examples=3581, test/ssim=0.738376, total_duration=1568.822185, train/loss=0.267396, train/ssim=0.742402, validation/loss=0.288553, validation/num_examples=3554, validation/ssim=0.721233
I0520 08:02:29.246625 139691772856064 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.105216, loss=0.198127
I0520 08:02:29.249994 139757714609984 submission.py:296] 4500) loss = 0.198, grad_norm = 0.105
I0520 08:03:24.155217 139757714609984 spec.py:298] Evaluating on the training split.
I0520 08:03:26.235837 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 08:03:28.353049 139757714609984 spec.py:326] Evaluating on the test split.
I0520 08:03:30.402948 139757714609984 submission_runner.py:421] Time since start: 1655.35s, 	Step: 4706, 	{'train/ssim': 0.7410988807678223, 'train/loss': 0.26859889711652485, 'validation/ssim': 0.7201081610289463, 'validation/loss': 0.28979227713272016, 'validation/num_examples': 3554, 'test/ssim': 0.7372091719971726, 'test/loss': 0.2914536941387706, 'test/num_examples': 3581, 'score': 1243.4334602355957, 'total_duration': 1655.354364156723, 'accumulated_submission_time': 1243.4334602355957, 'accumulated_eval_time': 324.8995747566223, 'accumulated_logging_time': 0.33621835708618164}
I0520 08:03:30.413224 139691764463360 logging_writer.py:48] [4706] accumulated_eval_time=324.899575, accumulated_logging_time=0.336218, accumulated_submission_time=1243.433460, global_step=4706, preemption_count=0, score=1243.433460, test/loss=0.291454, test/num_examples=3581, test/ssim=0.737209, total_duration=1655.354364, train/loss=0.268599, train/ssim=0.741099, validation/loss=0.289792, validation/num_examples=3554, validation/ssim=0.720108
I0520 08:04:46.839292 139691772856064 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.042855, loss=0.339414
I0520 08:04:46.843028 139757714609984 submission.py:296] 5000) loss = 0.339, grad_norm = 0.043
I0520 08:04:50.596552 139757714609984 spec.py:298] Evaluating on the training split.
I0520 08:04:52.655256 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 08:04:54.766896 139757714609984 spec.py:326] Evaluating on the test split.
I0520 08:04:56.821404 139757714609984 submission_runner.py:421] Time since start: 1741.77s, 	Step: 5015, 	{'train/ssim': 0.7429064341953823, 'train/loss': 0.26702829769679476, 'validation/ssim': 0.7220976254748172, 'validation/loss': 0.2878865170626407, 'validation/num_examples': 3554, 'test/ssim': 0.7393573504258587, 'test/loss': 0.2892873125414514, 'test/num_examples': 3581, 'score': 1317.4748556613922, 'total_duration': 1741.7728018760681, 'accumulated_submission_time': 1317.4748556613922, 'accumulated_eval_time': 331.1244411468506, 'accumulated_logging_time': 0.3545370101928711}
I0520 08:04:56.831636 139691764463360 logging_writer.py:48] [5015] accumulated_eval_time=331.124441, accumulated_logging_time=0.354537, accumulated_submission_time=1317.474856, global_step=5015, preemption_count=0, score=1317.474856, test/loss=0.289287, test/num_examples=3581, test/ssim=0.739357, total_duration=1741.772802, train/loss=0.267028, train/ssim=0.742906, validation/loss=0.287887, validation/num_examples=3554, validation/ssim=0.722098
I0520 08:06:17.054403 139757714609984 spec.py:298] Evaluating on the training split.
I0520 08:06:19.121478 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 08:06:21.236928 139757714609984 spec.py:326] Evaluating on the test split.
I0520 08:06:23.297867 139757714609984 submission_runner.py:421] Time since start: 1828.25s, 	Step: 5322, 	{'train/ssim': 0.7429274831499372, 'train/loss': 0.26694033827100483, 'validation/ssim': 0.7222754758063098, 'validation/loss': 0.28785835227472567, 'validation/num_examples': 3554, 'test/ssim': 0.739523019713418, 'test/loss': 0.2892916758477904, 'test/num_examples': 3581, 'score': 1391.5894544124603, 'total_duration': 1828.2492730617523, 'accumulated_submission_time': 1391.5894544124603, 'accumulated_eval_time': 337.367892742157, 'accumulated_logging_time': 0.3739185333251953}
I0520 08:06:23.308620 139691772856064 logging_writer.py:48] [5322] accumulated_eval_time=337.367893, accumulated_logging_time=0.373919, accumulated_submission_time=1391.589454, global_step=5322, preemption_count=0, score=1391.589454, test/loss=0.289292, test/num_examples=3581, test/ssim=0.739523, total_duration=1828.249273, train/loss=0.266940, train/ssim=0.742927, validation/loss=0.287858, validation/num_examples=3554, validation/ssim=0.722275
I0520 08:06:49.243229 139757714609984 spec.py:298] Evaluating on the training split.
I0520 08:06:51.289428 139757714609984 spec.py:310] Evaluating on the validation split.
I0520 08:06:53.354962 139757714609984 spec.py:326] Evaluating on the test split.
I0520 08:06:55.389256 139757714609984 submission_runner.py:421] Time since start: 1860.34s, 	Step: 5428, 	{'train/ssim': 0.7413364137922015, 'train/loss': 0.2669851098741804, 'validation/ssim': 0.7204275222460608, 'validation/loss': 0.2878894881042927, 'validation/num_examples': 3554, 'test/ssim': 0.7376766593654007, 'test/loss': 0.2893598184210067, 'test/num_examples': 3581, 'score': 1415.4245865345001, 'total_duration': 1860.340681552887, 'accumulated_submission_time': 1415.4245865345001, 'accumulated_eval_time': 343.51406478881836, 'accumulated_logging_time': 0.3925023078918457}
I0520 08:06:55.399878 139691764463360 logging_writer.py:48] [5428] accumulated_eval_time=343.514065, accumulated_logging_time=0.392502, accumulated_submission_time=1415.424587, global_step=5428, preemption_count=0, score=1415.424587, test/loss=0.289360, test/num_examples=3581, test/ssim=0.737677, total_duration=1860.340682, train/loss=0.266985, train/ssim=0.741336, validation/loss=0.287889, validation/num_examples=3554, validation/ssim=0.720428
I0520 08:06:55.417417 139691772856064 logging_writer.py:48] [5428] global_step=5428, preemption_count=0, score=1415.424587
I0520 08:06:55.560046 139757714609984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_5428.
I0520 08:06:56.118567 139757714609984 submission_runner.py:584] Tuning trial 1/1
I0520 08:06:56.118808 139757714609984 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0520 08:06:56.127562 139757714609984 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/ssim': 0.20803960732051305, 'train/loss': 1.0006442070007324, 'validation/ssim': 0.20153477142282464, 'validation/loss': 1.016404546373804, 'validation/num_examples': 3554, 'test/ssim': 0.2236656395625349, 'test/loss': 1.0144664058136694, 'test/num_examples': 3581, 'score': 47.19718599319458, 'total_duration': 271.2316882610321, 'accumulated_submission_time': 47.19718599319458, 'accumulated_eval_time': 224.0330147743225, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (303, {'train/ssim': 0.6950137274605888, 'train/loss': 0.3096295084272112, 'validation/ssim': 0.6748840984629995, 'validation/loss': 0.33195280076146594, 'validation/num_examples': 3554, 'test/ssim': 0.693282471725775, 'test/loss': 0.3333463096245811, 'test/num_examples': 3581, 'score': 124.25231504440308, 'total_duration': 357.87564063072205, 'accumulated_submission_time': 124.25231504440308, 'accumulated_eval_time': 230.59247970581055, 'accumulated_logging_time': 0.025751113891601562, 'global_step': 303, 'preemption_count': 0}), (547, {'train/ssim': 0.715353148324149, 'train/loss': 0.2916774409157889, 'validation/ssim': 0.6953204685741418, 'validation/loss': 0.31276845851505347, 'validation/num_examples': 3554, 'test/ssim': 0.7128607636222424, 'test/loss': 0.31496765426469564, 'test/num_examples': 3581, 'score': 200.8668553829193, 'total_duration': 444.6901783943176, 'accumulated_submission_time': 200.8668553829193, 'accumulated_eval_time': 237.11579155921936, 'accumulated_logging_time': 0.05092024803161621, 'global_step': 547, 'preemption_count': 0}), (776, {'train/ssim': 0.7226311138698033, 'train/loss': 0.2850535937717983, 'validation/ssim': 0.7030010062385692, 'validation/loss': 0.30560106954751687, 'validation/num_examples': 3554, 'test/ssim': 0.7203456747635087, 'test/loss': 0.3078970186073199, 'test/num_examples': 3581, 'score': 277.47726678848267, 'total_duration': 531.2682504653931, 'accumulated_submission_time': 277.47726678848267, 'accumulated_eval_time': 243.61060309410095, 'accumulated_logging_time': 0.07763409614562988, 'global_step': 776, 'preemption_count': 0}), (1015, {'train/ssim': 0.7279258455548968, 'train/loss': 0.28092520577566965, 'validation/ssim': 0.7082511287897439, 'validation/loss': 0.301424471930835, 'validation/num_examples': 3554, 'test/ssim': 0.725484490627618, 'test/loss': 0.30348407965870217, 'test/num_examples': 3581, 'score': 354.07655811309814, 'total_duration': 617.7478160858154, 'accumulated_submission_time': 354.07655811309814, 'accumulated_eval_time': 249.86967539787292, 'accumulated_logging_time': 0.10522580146789551, 'global_step': 1015, 'preemption_count': 0}), (1323, {'train/ssim': 0.7270787102835519, 'train/loss': 0.2780165672302246, 'validation/ssim': 0.7087465542786298, 'validation/loss': 0.2982167086711276, 'validation/num_examples': 3554, 'test/ssim': 0.7251933081061506, 'test/loss': 0.3003026498359397, 'test/num_examples': 3581, 'score': 428.2650763988495, 'total_duration': 704.2844815254211, 'accumulated_submission_time': 428.2650763988495, 'accumulated_eval_time': 256.12060928344727, 'accumulated_logging_time': 0.12419795989990234, 'global_step': 1323, 'preemption_count': 0}), (1630, {'train/ssim': 0.7331886972699847, 'train/loss': 0.27462029457092285, 'validation/ssim': 0.7127682112144766, 'validation/loss': 0.29520036292733537, 'validation/num_examples': 3554, 'test/ssim': 0.7299733102005376, 'test/loss': 0.29701772784094876, 'test/num_examples': 3581, 'score': 502.4608108997345, 'total_duration': 790.8344902992249, 'accumulated_submission_time': 502.4608108997345, 'accumulated_eval_time': 262.3941957950592, 'accumulated_logging_time': 0.1435995101928711, 'global_step': 1630, 'preemption_count': 0}), (1938, {'train/ssim': 0.7363000597272601, 'train/loss': 0.27263220718928743, 'validation/ssim': 0.7162287019247678, 'validation/loss': 0.2931461881638647, 'validation/num_examples': 3554, 'test/ssim': 0.7333624402227031, 'test/loss': 0.29494236208679486, 'test/num_examples': 3581, 'score': 576.44158244133, 'total_duration': 877.2353978157043, 'accumulated_submission_time': 576.44158244133, 'accumulated_eval_time': 268.66123700141907, 'accumulated_logging_time': 0.1618952751159668, 'global_step': 1938, 'preemption_count': 0}), (2243, {'train/ssim': 0.7389851978846959, 'train/loss': 0.2712343760899135, 'validation/ssim': 0.7181986571029122, 'validation/loss': 0.29231302565111494, 'validation/num_examples': 3554, 'test/ssim': 0.7352252311461533, 'test/loss': 0.2940699735256388, 'test/num_examples': 3581, 'score': 650.5814337730408, 'total_duration': 963.7169024944305, 'accumulated_submission_time': 650.5814337730408, 'accumulated_eval_time': 274.9468936920166, 'accumulated_logging_time': 0.1816411018371582, 'global_step': 2243, 'preemption_count': 0}), (2553, {'train/ssim': 0.7374082292829242, 'train/loss': 0.27132257393428255, 'validation/ssim': 0.7166252071829277, 'validation/loss': 0.2924714697567881, 'validation/num_examples': 3554, 'test/ssim': 0.7337734091385087, 'test/loss': 0.29410685709953577, 'test/num_examples': 3581, 'score': 724.776547908783, 'total_duration': 1050.2926383018494, 'accumulated_submission_time': 724.776547908783, 'accumulated_eval_time': 281.19412088394165, 'accumulated_logging_time': 0.20060467720031738, 'global_step': 2553, 'preemption_count': 0}), (2861, {'train/ssim': 0.738177844456264, 'train/loss': 0.2703939505985805, 'validation/ssim': 0.7177934276299592, 'validation/loss': 0.2910148350120463, 'validation/num_examples': 3554, 'test/ssim': 0.7350903777096133, 'test/loss': 0.2925323511894373, 'test/num_examples': 3581, 'score': 798.8147954940796, 'total_duration': 1136.661961555481, 'accumulated_submission_time': 798.8147954940796, 'accumulated_eval_time': 287.44148111343384, 'accumulated_logging_time': 0.21931052207946777, 'global_step': 2861, 'preemption_count': 0}), (3167, {'train/ssim': 0.7396174839564732, 'train/loss': 0.26968285015651156, 'validation/ssim': 0.7192932369337366, 'validation/loss': 0.2902154702447946, 'validation/num_examples': 3554, 'test/ssim': 0.736596195633203, 'test/loss': 0.2917344115426906, 'test/num_examples': 3581, 'score': 872.9055123329163, 'total_duration': 1223.0755801200867, 'accumulated_submission_time': 872.9055123329163, 'accumulated_eval_time': 293.68173599243164, 'accumulated_logging_time': 0.23949503898620605, 'global_step': 3167, 'preemption_count': 0}), (3475, {'train/ssim': 0.7419493538992745, 'train/loss': 0.26838007995060514, 'validation/ssim': 0.7208508184000422, 'validation/loss': 0.28956070762037495, 'validation/num_examples': 3554, 'test/ssim': 0.7380315870654147, 'test/loss': 0.2911169696073897, 'test/num_examples': 3581, 'score': 946.8371095657349, 'total_duration': 1309.434208393097, 'accumulated_submission_time': 946.8371095657349, 'accumulated_eval_time': 299.9207148551941, 'accumulated_logging_time': 0.2595381736755371, 'global_step': 3475, 'preemption_count': 0}), (3783, {'train/ssim': 0.7420832770211356, 'train/loss': 0.26851502486637663, 'validation/ssim': 0.7216182058288196, 'validation/loss': 0.2892218371355339, 'validation/num_examples': 3554, 'test/ssim': 0.7386382911765219, 'test/loss': 0.2907651098571454, 'test/num_examples': 3581, 'score': 1020.9630627632141, 'total_duration': 1395.8812918663025, 'accumulated_submission_time': 1020.9630627632141, 'accumulated_eval_time': 306.1092019081116, 'accumulated_logging_time': 0.278489351272583, 'global_step': 3783, 'preemption_count': 0}), (4091, {'train/ssim': 0.7397937774658203, 'train/loss': 0.26846412249973844, 'validation/ssim': 0.7192911074009919, 'validation/loss': 0.28908468835730866, 'validation/num_examples': 3554, 'test/ssim': 0.7365574712894443, 'test/loss': 0.29061201916364143, 'test/num_examples': 3581, 'score': 1094.973206281662, 'total_duration': 1482.320590019226, 'accumulated_submission_time': 1094.973206281662, 'accumulated_eval_time': 312.37057542800903, 'accumulated_logging_time': 0.29804468154907227, 'global_step': 4091, 'preemption_count': 0}), (4396, {'train/ssim': 0.7424020086015973, 'train/loss': 0.2673960413251604, 'validation/ssim': 0.7212326917074071, 'validation/loss': 0.2885528547279474, 'validation/num_examples': 3554, 'test/ssim': 0.7383764246195197, 'test/loss': 0.2900883883146293, 'test/num_examples': 3581, 'score': 1169.2557773590088, 'total_duration': 1568.8221848011017, 'accumulated_submission_time': 1169.2557773590088, 'accumulated_eval_time': 318.6517617702484, 'accumulated_logging_time': 0.3165264129638672, 'global_step': 4396, 'preemption_count': 0}), (4706, {'train/ssim': 0.7410988807678223, 'train/loss': 0.26859889711652485, 'validation/ssim': 0.7201081610289463, 'validation/loss': 0.28979227713272016, 'validation/num_examples': 3554, 'test/ssim': 0.7372091719971726, 'test/loss': 0.2914536941387706, 'test/num_examples': 3581, 'score': 1243.4334602355957, 'total_duration': 1655.354364156723, 'accumulated_submission_time': 1243.4334602355957, 'accumulated_eval_time': 324.8995747566223, 'accumulated_logging_time': 0.33621835708618164, 'global_step': 4706, 'preemption_count': 0}), (5015, {'train/ssim': 0.7429064341953823, 'train/loss': 0.26702829769679476, 'validation/ssim': 0.7220976254748172, 'validation/loss': 0.2878865170626407, 'validation/num_examples': 3554, 'test/ssim': 0.7393573504258587, 'test/loss': 0.2892873125414514, 'test/num_examples': 3581, 'score': 1317.4748556613922, 'total_duration': 1741.7728018760681, 'accumulated_submission_time': 1317.4748556613922, 'accumulated_eval_time': 331.1244411468506, 'accumulated_logging_time': 0.3545370101928711, 'global_step': 5015, 'preemption_count': 0}), (5322, {'train/ssim': 0.7429274831499372, 'train/loss': 0.26694033827100483, 'validation/ssim': 0.7222754758063098, 'validation/loss': 0.28785835227472567, 'validation/num_examples': 3554, 'test/ssim': 0.739523019713418, 'test/loss': 0.2892916758477904, 'test/num_examples': 3581, 'score': 1391.5894544124603, 'total_duration': 1828.2492730617523, 'accumulated_submission_time': 1391.5894544124603, 'accumulated_eval_time': 337.367892742157, 'accumulated_logging_time': 0.3739185333251953, 'global_step': 5322, 'preemption_count': 0}), (5428, {'train/ssim': 0.7413364137922015, 'train/loss': 0.2669851098741804, 'validation/ssim': 0.7204275222460608, 'validation/loss': 0.2878894881042927, 'validation/num_examples': 3554, 'test/ssim': 0.7376766593654007, 'test/loss': 0.2893598184210067, 'test/num_examples': 3581, 'score': 1415.4245865345001, 'total_duration': 1860.340681552887, 'accumulated_submission_time': 1415.4245865345001, 'accumulated_eval_time': 343.51406478881836, 'accumulated_logging_time': 0.3925023078918457, 'global_step': 5428, 'preemption_count': 0})], 'global_step': 5428}
I0520 08:06:56.127758 139757714609984 submission_runner.py:587] Timing: 1415.4245865345001
I0520 08:06:56.127807 139757714609984 submission_runner.py:588] ====================
I0520 08:06:56.127911 139757714609984 submission_runner.py:651] Final fastmri score: 1415.4245865345001
