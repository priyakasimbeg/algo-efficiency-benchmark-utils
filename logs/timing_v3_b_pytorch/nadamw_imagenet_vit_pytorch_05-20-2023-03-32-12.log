torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_b_pytorch/timing_nadamw --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_vit_pytorch_05-20-2023-03-32-12.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0520 03:32:36.290274 140360520668992 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0520 03:32:36.290309 139824075749184 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0520 03:32:36.290335 140710618081088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0520 03:32:36.290962 140682895275840 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0520 03:32:36.291194 139903208003392 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0520 03:32:36.291320 140386545678144 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0520 03:32:36.291548 139903208003392 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:32:36.291403 140433017227072 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0520 03:32:36.291657 140386545678144 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:32:36.291749 140433017227072 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:32:36.291683 139662566455104 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0520 03:32:36.292060 139662566455104 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:32:36.300945 140360520668992 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:32:36.300966 139824075749184 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:32:36.300998 140710618081088 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:32:36.301574 140682895275840 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0520 03:32:38.621913 140433017227072 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_b_pytorch/timing_nadamw/imagenet_vit_pytorch.
W0520 03:32:38.668439 140433017227072 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:32:38.668856 140682895275840 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:32:38.668924 139824075749184 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:32:38.669674 140360520668992 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:32:38.670046 140710618081088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:32:38.670187 140386545678144 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:32:38.670753 139662566455104 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0520 03:32:38.671558 139903208003392 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0520 03:32:38.676710 140433017227072 submission_runner.py:544] Using RNG seed 1256616403
I0520 03:32:38.678668 140433017227072 submission_runner.py:553] --- Tuning run 1/1 ---
I0520 03:32:38.678826 140433017227072 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_b_pytorch/timing_nadamw/imagenet_vit_pytorch/trial_1.
I0520 03:32:38.679265 140433017227072 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0520 03:32:38.681032 140433017227072 submission_runner.py:241] Initializing dataset.
I0520 03:32:45.279278 140433017227072 submission_runner.py:248] Initializing model.
I0520 03:32:49.486542 140433017227072 submission_runner.py:258] Initializing optimizer.
I0520 03:32:49.488102 140433017227072 submission_runner.py:265] Initializing metrics bundle.
I0520 03:32:49.488223 140433017227072 submission_runner.py:283] Initializing checkpoint and logger.
I0520 03:32:49.995014 140433017227072 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0520 03:32:49.995868 140433017227072 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0520 03:32:50.048837 140433017227072 submission_runner.py:319] Starting training loop.
I0520 03:32:56.952447 140404175394560 logging_writer.py:48] [0] global_step=0, grad_norm=0.341634, loss=6.907756
I0520 03:32:56.971048 140433017227072 submission.py:296] 0) loss = 6.908, grad_norm = 0.342
I0520 03:32:56.972201 140433017227072 spec.py:298] Evaluating on the training split.
I0520 03:33:57.346384 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 03:34:51.767016 140433017227072 spec.py:326] Evaluating on the test split.
I0520 03:34:51.788059 140433017227072 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0520 03:34:51.794768 140433017227072 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0520 03:34:51.875154 140433017227072 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0520 03:35:03.910541 140433017227072 submission_runner.py:421] Time since start: 133.86s, 	Step: 1, 	{'train/accuracy': 0.00201171875, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00206, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0021, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.922562122344971, 'total_duration': 133.86207914352417, 'accumulated_submission_time': 6.922562122344971, 'accumulated_eval_time': 126.93824529647827, 'accumulated_logging_time': 0}
I0520 03:35:03.926711 140399159015168 logging_writer.py:48] [1] accumulated_eval_time=126.938245, accumulated_logging_time=0, accumulated_submission_time=6.922562, global_step=1, preemption_count=0, score=6.922562, test/accuracy=0.002100, test/loss=6.907755, test/num_examples=10000, total_duration=133.862079, train/accuracy=0.002012, train/loss=6.907756, validation/accuracy=0.002060, validation/loss=6.907756, validation/num_examples=50000
I0520 03:35:03.947186 140433017227072 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:35:03.947144 140710618081088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:35:03.947228 140360520668992 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:35:03.947389 140682895275840 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:35:03.947399 139824075749184 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:35:03.947489 139662566455104 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:35:03.947504 139903208003392 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:35:03.947926 140386545678144 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0520 03:35:04.547301 140399150622464 logging_writer.py:48] [1] global_step=1, grad_norm=0.352424, loss=6.907756
I0520 03:35:04.551151 140433017227072 submission.py:296] 1) loss = 6.908, grad_norm = 0.352
I0520 03:35:04.973810 140399159015168 logging_writer.py:48] [2] global_step=2, grad_norm=0.348983, loss=6.907753
I0520 03:35:04.977598 140433017227072 submission.py:296] 2) loss = 6.908, grad_norm = 0.349
I0520 03:35:05.387863 140399150622464 logging_writer.py:48] [3] global_step=3, grad_norm=0.347722, loss=6.907752
I0520 03:35:05.392131 140433017227072 submission.py:296] 3) loss = 6.908, grad_norm = 0.348
I0520 03:35:05.802447 140399159015168 logging_writer.py:48] [4] global_step=4, grad_norm=0.348213, loss=6.907752
I0520 03:35:05.807793 140433017227072 submission.py:296] 4) loss = 6.908, grad_norm = 0.348
I0520 03:35:06.219044 140399150622464 logging_writer.py:48] [5] global_step=5, grad_norm=0.344115, loss=6.907751
I0520 03:35:06.224406 140433017227072 submission.py:296] 5) loss = 6.908, grad_norm = 0.344
I0520 03:35:06.643946 140399159015168 logging_writer.py:48] [6] global_step=6, grad_norm=0.355901, loss=6.907740
I0520 03:35:06.649065 140433017227072 submission.py:296] 6) loss = 6.908, grad_norm = 0.356
I0520 03:35:07.064671 140399150622464 logging_writer.py:48] [7] global_step=7, grad_norm=0.356375, loss=6.907739
I0520 03:35:07.070140 140433017227072 submission.py:296] 7) loss = 6.908, grad_norm = 0.356
I0520 03:35:07.504522 140399159015168 logging_writer.py:48] [8] global_step=8, grad_norm=0.353642, loss=6.907732
I0520 03:35:07.509668 140433017227072 submission.py:296] 8) loss = 6.908, grad_norm = 0.354
I0520 03:35:07.927436 140399150622464 logging_writer.py:48] [9] global_step=9, grad_norm=0.357600, loss=6.907736
I0520 03:35:07.932493 140433017227072 submission.py:296] 9) loss = 6.908, grad_norm = 0.358
I0520 03:35:08.348504 140399159015168 logging_writer.py:48] [10] global_step=10, grad_norm=0.348685, loss=6.907718
I0520 03:35:08.353364 140433017227072 submission.py:296] 10) loss = 6.908, grad_norm = 0.349
I0520 03:35:08.766921 140399150622464 logging_writer.py:48] [11] global_step=11, grad_norm=0.350602, loss=6.907702
I0520 03:35:08.771902 140433017227072 submission.py:296] 11) loss = 6.908, grad_norm = 0.351
I0520 03:35:09.183029 140399159015168 logging_writer.py:48] [12] global_step=12, grad_norm=0.342770, loss=6.907711
I0520 03:35:09.187894 140433017227072 submission.py:296] 12) loss = 6.908, grad_norm = 0.343
I0520 03:35:09.635164 140399150622464 logging_writer.py:48] [13] global_step=13, grad_norm=0.340321, loss=6.907688
I0520 03:35:09.639464 140433017227072 submission.py:296] 13) loss = 6.908, grad_norm = 0.340
I0520 03:35:10.055372 140399159015168 logging_writer.py:48] [14] global_step=14, grad_norm=0.356373, loss=6.907708
I0520 03:35:10.060344 140433017227072 submission.py:296] 14) loss = 6.908, grad_norm = 0.356
I0520 03:35:10.473273 140399150622464 logging_writer.py:48] [15] global_step=15, grad_norm=0.345478, loss=6.907685
I0520 03:35:10.477745 140433017227072 submission.py:296] 15) loss = 6.908, grad_norm = 0.345
I0520 03:35:10.887864 140399159015168 logging_writer.py:48] [16] global_step=16, grad_norm=0.345825, loss=6.907653
I0520 03:35:10.892405 140433017227072 submission.py:296] 16) loss = 6.908, grad_norm = 0.346
I0520 03:35:11.320647 140399150622464 logging_writer.py:48] [17] global_step=17, grad_norm=0.343392, loss=6.907644
I0520 03:35:11.324605 140433017227072 submission.py:296] 17) loss = 6.908, grad_norm = 0.343
I0520 03:35:11.737263 140399159015168 logging_writer.py:48] [18] global_step=18, grad_norm=0.355526, loss=6.907641
I0520 03:35:11.741789 140433017227072 submission.py:296] 18) loss = 6.908, grad_norm = 0.356
I0520 03:35:12.156035 140399150622464 logging_writer.py:48] [19] global_step=19, grad_norm=0.343156, loss=6.907670
I0520 03:35:12.160528 140433017227072 submission.py:296] 19) loss = 6.908, grad_norm = 0.343
I0520 03:35:12.580258 140399159015168 logging_writer.py:48] [20] global_step=20, grad_norm=0.349372, loss=6.907526
I0520 03:35:12.587639 140433017227072 submission.py:296] 20) loss = 6.908, grad_norm = 0.349
I0520 03:35:13.006052 140399150622464 logging_writer.py:48] [21] global_step=21, grad_norm=0.348771, loss=6.907515
I0520 03:35:13.010575 140433017227072 submission.py:296] 21) loss = 6.908, grad_norm = 0.349
I0520 03:35:13.447736 140399159015168 logging_writer.py:48] [22] global_step=22, grad_norm=0.352719, loss=6.907638
I0520 03:35:13.452486 140433017227072 submission.py:296] 22) loss = 6.908, grad_norm = 0.353
I0520 03:35:13.875968 140399150622464 logging_writer.py:48] [23] global_step=23, grad_norm=0.341136, loss=6.907533
I0520 03:35:13.880769 140433017227072 submission.py:296] 23) loss = 6.908, grad_norm = 0.341
I0520 03:35:14.294140 140399159015168 logging_writer.py:48] [24] global_step=24, grad_norm=0.347045, loss=6.907390
I0520 03:35:14.298818 140433017227072 submission.py:296] 24) loss = 6.907, grad_norm = 0.347
I0520 03:35:14.713625 140399150622464 logging_writer.py:48] [25] global_step=25, grad_norm=0.339176, loss=6.907458
I0520 03:35:14.718441 140433017227072 submission.py:296] 25) loss = 6.907, grad_norm = 0.339
I0520 03:35:15.135540 140399159015168 logging_writer.py:48] [26] global_step=26, grad_norm=0.348745, loss=6.907466
I0520 03:35:15.140178 140433017227072 submission.py:296] 26) loss = 6.907, grad_norm = 0.349
I0520 03:35:15.559370 140399150622464 logging_writer.py:48] [27] global_step=27, grad_norm=0.354129, loss=6.907418
I0520 03:35:15.564530 140433017227072 submission.py:296] 27) loss = 6.907, grad_norm = 0.354
I0520 03:35:16.001546 140399159015168 logging_writer.py:48] [28] global_step=28, grad_norm=0.343508, loss=6.907403
I0520 03:35:16.006603 140433017227072 submission.py:296] 28) loss = 6.907, grad_norm = 0.344
I0520 03:35:16.423072 140399150622464 logging_writer.py:48] [29] global_step=29, grad_norm=0.346281, loss=6.907454
I0520 03:35:16.428080 140433017227072 submission.py:296] 29) loss = 6.907, grad_norm = 0.346
I0520 03:35:16.872364 140399159015168 logging_writer.py:48] [30] global_step=30, grad_norm=0.351739, loss=6.907215
I0520 03:35:16.878332 140433017227072 submission.py:296] 30) loss = 6.907, grad_norm = 0.352
I0520 03:35:17.303960 140399150622464 logging_writer.py:48] [31] global_step=31, grad_norm=0.335495, loss=6.907407
I0520 03:35:17.308748 140433017227072 submission.py:296] 31) loss = 6.907, grad_norm = 0.335
I0520 03:35:17.723151 140399159015168 logging_writer.py:48] [32] global_step=32, grad_norm=0.346255, loss=6.907485
I0520 03:35:17.727517 140433017227072 submission.py:296] 32) loss = 6.907, grad_norm = 0.346
I0520 03:35:18.145278 140399150622464 logging_writer.py:48] [33] global_step=33, grad_norm=0.356708, loss=6.907268
I0520 03:35:18.149192 140433017227072 submission.py:296] 33) loss = 6.907, grad_norm = 0.357
I0520 03:35:18.567283 140399159015168 logging_writer.py:48] [34] global_step=34, grad_norm=0.345285, loss=6.907067
I0520 03:35:18.571610 140433017227072 submission.py:296] 34) loss = 6.907, grad_norm = 0.345
I0520 03:35:18.989341 140399150622464 logging_writer.py:48] [35] global_step=35, grad_norm=0.362171, loss=6.907038
I0520 03:35:18.994064 140433017227072 submission.py:296] 35) loss = 6.907, grad_norm = 0.362
I0520 03:35:19.406181 140399159015168 logging_writer.py:48] [36] global_step=36, grad_norm=0.355733, loss=6.907107
I0520 03:35:19.411774 140433017227072 submission.py:296] 36) loss = 6.907, grad_norm = 0.356
I0520 03:35:19.826506 140399150622464 logging_writer.py:48] [37] global_step=37, grad_norm=0.343796, loss=6.907110
I0520 03:35:19.830896 140433017227072 submission.py:296] 37) loss = 6.907, grad_norm = 0.344
I0520 03:35:20.250140 140399159015168 logging_writer.py:48] [38] global_step=38, grad_norm=0.348325, loss=6.907027
I0520 03:35:20.260588 140433017227072 submission.py:296] 38) loss = 6.907, grad_norm = 0.348
I0520 03:35:20.698986 140399150622464 logging_writer.py:48] [39] global_step=39, grad_norm=0.352252, loss=6.907000
I0520 03:35:20.705407 140433017227072 submission.py:296] 39) loss = 6.907, grad_norm = 0.352
I0520 03:35:21.118342 140399159015168 logging_writer.py:48] [40] global_step=40, grad_norm=0.355536, loss=6.906811
I0520 03:35:21.125590 140433017227072 submission.py:296] 40) loss = 6.907, grad_norm = 0.356
I0520 03:35:21.540554 140399150622464 logging_writer.py:48] [41] global_step=41, grad_norm=0.351556, loss=6.906768
I0520 03:35:21.544493 140433017227072 submission.py:296] 41) loss = 6.907, grad_norm = 0.352
I0520 03:35:21.956343 140399159015168 logging_writer.py:48] [42] global_step=42, grad_norm=0.345810, loss=6.906701
I0520 03:35:21.960844 140433017227072 submission.py:296] 42) loss = 6.907, grad_norm = 0.346
I0520 03:35:22.374342 140399150622464 logging_writer.py:48] [43] global_step=43, grad_norm=0.361169, loss=6.906617
I0520 03:35:22.378568 140433017227072 submission.py:296] 43) loss = 6.907, grad_norm = 0.361
I0520 03:35:22.831093 140399159015168 logging_writer.py:48] [44] global_step=44, grad_norm=0.375904, loss=6.906184
I0520 03:35:22.835607 140433017227072 submission.py:296] 44) loss = 6.906, grad_norm = 0.376
I0520 03:35:23.271957 140399150622464 logging_writer.py:48] [45] global_step=45, grad_norm=0.369700, loss=6.906178
I0520 03:35:23.276539 140433017227072 submission.py:296] 45) loss = 6.906, grad_norm = 0.370
I0520 03:35:23.719232 140399159015168 logging_writer.py:48] [46] global_step=46, grad_norm=0.363830, loss=6.906348
I0520 03:35:23.724469 140433017227072 submission.py:296] 46) loss = 6.906, grad_norm = 0.364
I0520 03:35:24.143306 140399150622464 logging_writer.py:48] [47] global_step=47, grad_norm=0.372757, loss=6.906141
I0520 03:35:24.147549 140433017227072 submission.py:296] 47) loss = 6.906, grad_norm = 0.373
I0520 03:35:24.564460 140399159015168 logging_writer.py:48] [48] global_step=48, grad_norm=0.358668, loss=6.905932
I0520 03:35:24.569205 140433017227072 submission.py:296] 48) loss = 6.906, grad_norm = 0.359
I0520 03:35:24.981749 140399150622464 logging_writer.py:48] [49] global_step=49, grad_norm=0.373301, loss=6.906291
I0520 03:35:24.987015 140433017227072 submission.py:296] 49) loss = 6.906, grad_norm = 0.373
I0520 03:35:25.419569 140399159015168 logging_writer.py:48] [50] global_step=50, grad_norm=0.381747, loss=6.905928
I0520 03:35:25.423627 140433017227072 submission.py:296] 50) loss = 6.906, grad_norm = 0.382
I0520 03:35:25.837836 140399150622464 logging_writer.py:48] [51] global_step=51, grad_norm=0.386958, loss=6.905584
I0520 03:35:25.842684 140433017227072 submission.py:296] 51) loss = 6.906, grad_norm = 0.387
I0520 03:35:26.255965 140399159015168 logging_writer.py:48] [52] global_step=52, grad_norm=0.371599, loss=6.905866
I0520 03:35:26.261421 140433017227072 submission.py:296] 52) loss = 6.906, grad_norm = 0.372
I0520 03:35:26.682753 140399150622464 logging_writer.py:48] [53] global_step=53, grad_norm=0.388730, loss=6.904961
I0520 03:35:26.686865 140433017227072 submission.py:296] 53) loss = 6.905, grad_norm = 0.389
I0520 03:35:27.121241 140399159015168 logging_writer.py:48] [54] global_step=54, grad_norm=0.407380, loss=6.905005
I0520 03:35:27.126483 140433017227072 submission.py:296] 54) loss = 6.905, grad_norm = 0.407
I0520 03:35:27.560449 140399150622464 logging_writer.py:48] [55] global_step=55, grad_norm=0.374218, loss=6.905401
I0520 03:35:27.565579 140433017227072 submission.py:296] 55) loss = 6.905, grad_norm = 0.374
I0520 03:35:27.982634 140399159015168 logging_writer.py:48] [56] global_step=56, grad_norm=0.376898, loss=6.905167
I0520 03:35:27.988137 140433017227072 submission.py:296] 56) loss = 6.905, grad_norm = 0.377
I0520 03:35:28.408120 140399150622464 logging_writer.py:48] [57] global_step=57, grad_norm=0.383909, loss=6.905172
I0520 03:35:28.412124 140433017227072 submission.py:296] 57) loss = 6.905, grad_norm = 0.384
I0520 03:35:28.830189 140399159015168 logging_writer.py:48] [58] global_step=58, grad_norm=0.396811, loss=6.904861
I0520 03:35:28.834057 140433017227072 submission.py:296] 58) loss = 6.905, grad_norm = 0.397
I0520 03:35:29.254064 140399150622464 logging_writer.py:48] [59] global_step=59, grad_norm=0.413058, loss=6.904388
I0520 03:35:29.257878 140433017227072 submission.py:296] 59) loss = 6.904, grad_norm = 0.413
I0520 03:35:29.674756 140399159015168 logging_writer.py:48] [60] global_step=60, grad_norm=0.398626, loss=6.904828
I0520 03:35:29.678766 140433017227072 submission.py:296] 60) loss = 6.905, grad_norm = 0.399
I0520 03:35:30.094493 140399150622464 logging_writer.py:48] [61] global_step=61, grad_norm=0.397945, loss=6.904515
I0520 03:35:30.098657 140433017227072 submission.py:296] 61) loss = 6.905, grad_norm = 0.398
I0520 03:35:30.520787 140399159015168 logging_writer.py:48] [62] global_step=62, grad_norm=0.406560, loss=6.903975
I0520 03:35:30.526236 140433017227072 submission.py:296] 62) loss = 6.904, grad_norm = 0.407
I0520 03:35:30.947641 140399150622464 logging_writer.py:48] [63] global_step=63, grad_norm=0.415489, loss=6.903722
I0520 03:35:30.951890 140433017227072 submission.py:296] 63) loss = 6.904, grad_norm = 0.415
I0520 03:35:31.368570 140399159015168 logging_writer.py:48] [64] global_step=64, grad_norm=0.404028, loss=6.904436
I0520 03:35:31.372429 140433017227072 submission.py:296] 64) loss = 6.904, grad_norm = 0.404
I0520 03:35:31.787643 140399150622464 logging_writer.py:48] [65] global_step=65, grad_norm=0.403042, loss=6.904157
I0520 03:35:31.793614 140433017227072 submission.py:296] 65) loss = 6.904, grad_norm = 0.403
I0520 03:35:32.210894 140399159015168 logging_writer.py:48] [66] global_step=66, grad_norm=0.397862, loss=6.903605
I0520 03:35:32.215493 140433017227072 submission.py:296] 66) loss = 6.904, grad_norm = 0.398
I0520 03:35:32.635835 140399150622464 logging_writer.py:48] [67] global_step=67, grad_norm=0.406058, loss=6.902765
I0520 03:35:32.640727 140433017227072 submission.py:296] 67) loss = 6.903, grad_norm = 0.406
I0520 03:35:33.058054 140399159015168 logging_writer.py:48] [68] global_step=68, grad_norm=0.418298, loss=6.903243
I0520 03:35:33.062290 140433017227072 submission.py:296] 68) loss = 6.903, grad_norm = 0.418
I0520 03:35:33.480102 140399150622464 logging_writer.py:48] [69] global_step=69, grad_norm=0.410645, loss=6.903020
I0520 03:35:33.484128 140433017227072 submission.py:296] 69) loss = 6.903, grad_norm = 0.411
I0520 03:35:33.899349 140399159015168 logging_writer.py:48] [70] global_step=70, grad_norm=0.413493, loss=6.902709
I0520 03:35:33.903430 140433017227072 submission.py:296] 70) loss = 6.903, grad_norm = 0.413
I0520 03:35:34.322833 140399150622464 logging_writer.py:48] [71] global_step=71, grad_norm=0.430630, loss=6.903134
I0520 03:35:34.327895 140433017227072 submission.py:296] 71) loss = 6.903, grad_norm = 0.431
I0520 03:35:34.742173 140399159015168 logging_writer.py:48] [72] global_step=72, grad_norm=0.432364, loss=6.901245
I0520 03:35:34.746727 140433017227072 submission.py:296] 72) loss = 6.901, grad_norm = 0.432
I0520 03:35:35.170340 140399150622464 logging_writer.py:48] [73] global_step=73, grad_norm=0.436993, loss=6.901958
I0520 03:35:35.175265 140433017227072 submission.py:296] 73) loss = 6.902, grad_norm = 0.437
I0520 03:35:35.594751 140399159015168 logging_writer.py:48] [74] global_step=74, grad_norm=0.411082, loss=6.902412
I0520 03:35:35.600305 140433017227072 submission.py:296] 74) loss = 6.902, grad_norm = 0.411
I0520 03:35:36.027556 140399150622464 logging_writer.py:48] [75] global_step=75, grad_norm=0.436950, loss=6.900657
I0520 03:35:36.032334 140433017227072 submission.py:296] 75) loss = 6.901, grad_norm = 0.437
I0520 03:35:36.447725 140399159015168 logging_writer.py:48] [76] global_step=76, grad_norm=0.421504, loss=6.900933
I0520 03:35:36.452489 140433017227072 submission.py:296] 76) loss = 6.901, grad_norm = 0.422
I0520 03:35:36.888607 140399150622464 logging_writer.py:48] [77] global_step=77, grad_norm=0.432745, loss=6.900465
I0520 03:35:36.892889 140433017227072 submission.py:296] 77) loss = 6.900, grad_norm = 0.433
I0520 03:35:37.319325 140399159015168 logging_writer.py:48] [78] global_step=78, grad_norm=0.423614, loss=6.902461
I0520 03:35:37.323175 140433017227072 submission.py:296] 78) loss = 6.902, grad_norm = 0.424
I0520 03:35:37.742961 140399150622464 logging_writer.py:48] [79] global_step=79, grad_norm=0.428788, loss=6.900178
I0520 03:35:37.747312 140433017227072 submission.py:296] 79) loss = 6.900, grad_norm = 0.429
I0520 03:35:38.171794 140399159015168 logging_writer.py:48] [80] global_step=80, grad_norm=0.436593, loss=6.900448
I0520 03:35:38.175701 140433017227072 submission.py:296] 80) loss = 6.900, grad_norm = 0.437
I0520 03:35:38.606580 140399150622464 logging_writer.py:48] [81] global_step=81, grad_norm=0.431929, loss=6.898958
I0520 03:35:38.610727 140433017227072 submission.py:296] 81) loss = 6.899, grad_norm = 0.432
I0520 03:35:39.028178 140399159015168 logging_writer.py:48] [82] global_step=82, grad_norm=0.438963, loss=6.897767
I0520 03:35:39.033437 140433017227072 submission.py:296] 82) loss = 6.898, grad_norm = 0.439
I0520 03:35:39.462875 140399150622464 logging_writer.py:48] [83] global_step=83, grad_norm=0.438836, loss=6.898971
I0520 03:35:39.467319 140433017227072 submission.py:296] 83) loss = 6.899, grad_norm = 0.439
I0520 03:35:39.884632 140399159015168 logging_writer.py:48] [84] global_step=84, grad_norm=0.423777, loss=6.899595
I0520 03:35:39.888374 140433017227072 submission.py:296] 84) loss = 6.900, grad_norm = 0.424
I0520 03:35:40.312082 140399150622464 logging_writer.py:48] [85] global_step=85, grad_norm=0.435059, loss=6.900732
I0520 03:35:40.317267 140433017227072 submission.py:296] 85) loss = 6.901, grad_norm = 0.435
I0520 03:35:40.753799 140399159015168 logging_writer.py:48] [86] global_step=86, grad_norm=0.411666, loss=6.900441
I0520 03:35:40.758532 140433017227072 submission.py:296] 86) loss = 6.900, grad_norm = 0.412
I0520 03:35:41.192352 140399150622464 logging_writer.py:48] [87] global_step=87, grad_norm=0.428885, loss=6.898523
I0520 03:35:41.196987 140433017227072 submission.py:296] 87) loss = 6.899, grad_norm = 0.429
I0520 03:35:41.636813 140399159015168 logging_writer.py:48] [88] global_step=88, grad_norm=0.441747, loss=6.896173
I0520 03:35:41.643388 140433017227072 submission.py:296] 88) loss = 6.896, grad_norm = 0.442
I0520 03:35:42.083838 140399150622464 logging_writer.py:48] [89] global_step=89, grad_norm=0.442308, loss=6.896074
I0520 03:35:42.088989 140433017227072 submission.py:296] 89) loss = 6.896, grad_norm = 0.442
I0520 03:35:42.526067 140399159015168 logging_writer.py:48] [90] global_step=90, grad_norm=0.443182, loss=6.897525
I0520 03:35:42.530730 140433017227072 submission.py:296] 90) loss = 6.898, grad_norm = 0.443
I0520 03:35:42.948662 140399150622464 logging_writer.py:48] [91] global_step=91, grad_norm=0.422717, loss=6.897683
I0520 03:35:42.953102 140433017227072 submission.py:296] 91) loss = 6.898, grad_norm = 0.423
I0520 03:35:43.384175 140399159015168 logging_writer.py:48] [92] global_step=92, grad_norm=0.443790, loss=6.895720
I0520 03:35:43.387966 140433017227072 submission.py:296] 92) loss = 6.896, grad_norm = 0.444
I0520 03:35:43.802225 140399150622464 logging_writer.py:48] [93] global_step=93, grad_norm=0.417487, loss=6.897828
I0520 03:35:43.806563 140433017227072 submission.py:296] 93) loss = 6.898, grad_norm = 0.417
I0520 03:35:44.225333 140399159015168 logging_writer.py:48] [94] global_step=94, grad_norm=0.436209, loss=6.897666
I0520 03:35:44.232001 140433017227072 submission.py:296] 94) loss = 6.898, grad_norm = 0.436
I0520 03:35:44.654393 140399150622464 logging_writer.py:48] [95] global_step=95, grad_norm=0.435567, loss=6.894361
I0520 03:35:44.658260 140433017227072 submission.py:296] 95) loss = 6.894, grad_norm = 0.436
I0520 03:35:45.077449 140399159015168 logging_writer.py:48] [96] global_step=96, grad_norm=0.441673, loss=6.893370
I0520 03:35:45.082154 140433017227072 submission.py:296] 96) loss = 6.893, grad_norm = 0.442
I0520 03:35:45.500195 140399150622464 logging_writer.py:48] [97] global_step=97, grad_norm=0.441053, loss=6.895242
I0520 03:35:45.504266 140433017227072 submission.py:296] 97) loss = 6.895, grad_norm = 0.441
I0520 03:35:45.922093 140399159015168 logging_writer.py:48] [98] global_step=98, grad_norm=0.439935, loss=6.893420
I0520 03:35:45.927590 140433017227072 submission.py:296] 98) loss = 6.893, grad_norm = 0.440
I0520 03:35:46.356611 140399150622464 logging_writer.py:48] [99] global_step=99, grad_norm=0.426850, loss=6.896643
I0520 03:35:46.361623 140433017227072 submission.py:296] 99) loss = 6.897, grad_norm = 0.427
I0520 03:35:46.804289 140399159015168 logging_writer.py:48] [100] global_step=100, grad_norm=0.461220, loss=6.895329
I0520 03:35:46.808793 140433017227072 submission.py:296] 100) loss = 6.895, grad_norm = 0.461
I0520 03:38:31.386107 140399150622464 logging_writer.py:48] [500] global_step=500, grad_norm=0.969858, loss=6.635734
I0520 03:38:31.392202 140433017227072 submission.py:296] 500) loss = 6.636, grad_norm = 0.970
I0520 03:41:57.005207 140399159015168 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.543972, loss=6.340765
I0520 03:41:57.010504 140433017227072 submission.py:296] 1000) loss = 6.341, grad_norm = 1.544
I0520 03:42:04.016823 140433017227072 spec.py:298] Evaluating on the training split.
I0520 03:42:46.812342 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 03:43:31.028472 140433017227072 spec.py:326] Evaluating on the test split.
I0520 03:43:32.483516 140433017227072 submission_runner.py:421] Time since start: 642.44s, 	Step: 1018, 	{'train/accuracy': 0.042265625, 'train/loss': 5.846652221679688, 'validation/accuracy': 0.03934, 'validation/loss': 5.878519375, 'validation/num_examples': 50000, 'test/accuracy': 0.0316, 'test/loss': 5.986660546875, 'test/num_examples': 10000, 'score': 426.50470876693726, 'total_duration': 642.4351246356964, 'accumulated_submission_time': 426.50470876693726, 'accumulated_eval_time': 215.4049220085144, 'accumulated_logging_time': 0.024768829345703125}
I0520 03:43:32.493704 140389382199040 logging_writer.py:48] [1018] accumulated_eval_time=215.404922, accumulated_logging_time=0.024769, accumulated_submission_time=426.504709, global_step=1018, preemption_count=0, score=426.504709, test/accuracy=0.031600, test/loss=5.986661, test/num_examples=10000, total_duration=642.435125, train/accuracy=0.042266, train/loss=5.846652, validation/accuracy=0.039340, validation/loss=5.878519, validation/num_examples=50000
I0520 03:46:54.057853 140389881190144 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.097638, loss=6.228073
I0520 03:46:54.063162 140433017227072 submission.py:296] 1500) loss = 6.228, grad_norm = 1.098
I0520 03:50:22.020293 140389382199040 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.073219, loss=6.154754
I0520 03:50:22.025213 140433017227072 submission.py:296] 2000) loss = 6.155, grad_norm = 1.073
I0520 03:50:32.888467 140433017227072 spec.py:298] Evaluating on the training split.
I0520 03:51:16.891975 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 03:52:03.425965 140433017227072 spec.py:326] Evaluating on the test split.
I0520 03:52:04.847000 140433017227072 submission_runner.py:421] Time since start: 1154.80s, 	Step: 2027, 	{'train/accuracy': 0.09650390625, 'train/loss': 5.12178466796875, 'validation/accuracy': 0.08724, 'validation/loss': 5.1789365625, 'validation/num_examples': 50000, 'test/accuracy': 0.0688, 'test/loss': 5.389979296875, 'test/num_examples': 10000, 'score': 846.3808209896088, 'total_duration': 1154.7986431121826, 'accumulated_submission_time': 846.3808209896088, 'accumulated_eval_time': 307.36343026161194, 'accumulated_logging_time': 0.0432896614074707}
I0520 03:52:04.856765 140389881190144 logging_writer.py:48] [2027] accumulated_eval_time=307.363430, accumulated_logging_time=0.043290, accumulated_submission_time=846.380821, global_step=2027, preemption_count=0, score=846.380821, test/accuracy=0.068800, test/loss=5.389979, test/num_examples=10000, total_duration=1154.798643, train/accuracy=0.096504, train/loss=5.121785, validation/accuracy=0.087240, validation/loss=5.178937, validation/num_examples=50000
I0520 03:55:20.523318 140389382199040 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.165977, loss=5.683728
I0520 03:55:20.530775 140433017227072 submission.py:296] 2500) loss = 5.684, grad_norm = 1.166
I0520 03:58:50.384814 140389881190144 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.601270, loss=5.695868
I0520 03:58:50.389235 140433017227072 submission.py:296] 3000) loss = 5.696, grad_norm = 1.601
I0520 03:59:04.990030 140433017227072 spec.py:298] Evaluating on the training split.
I0520 03:59:48.958343 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 04:00:43.732004 140433017227072 spec.py:326] Evaluating on the test split.
I0520 04:00:45.148693 140433017227072 submission_runner.py:421] Time since start: 1675.10s, 	Step: 3036, 	{'train/accuracy': 0.16546875, 'train/loss': 4.482292785644531, 'validation/accuracy': 0.15032, 'validation/loss': 4.55971875, 'validation/num_examples': 50000, 'test/accuracy': 0.1152, 'test/loss': 4.866061328125, 'test/num_examples': 10000, 'score': 1265.995033979416, 'total_duration': 1675.1002728939056, 'accumulated_submission_time': 1265.995033979416, 'accumulated_eval_time': 407.5220241546631, 'accumulated_logging_time': 0.06199026107788086}
I0520 04:00:45.160043 140389382199040 logging_writer.py:48] [3036] accumulated_eval_time=407.522024, accumulated_logging_time=0.061990, accumulated_submission_time=1265.995034, global_step=3036, preemption_count=0, score=1265.995034, test/accuracy=0.115200, test/loss=4.866061, test/num_examples=10000, total_duration=1675.100273, train/accuracy=0.165469, train/loss=4.482293, validation/accuracy=0.150320, validation/loss=4.559719, validation/num_examples=50000
I0520 04:03:58.312995 140389881190144 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.239979, loss=5.249648
I0520 04:03:58.317901 140433017227072 submission.py:296] 3500) loss = 5.250, grad_norm = 1.240
I0520 04:07:27.841057 140389382199040 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.110243, loss=5.444784
I0520 04:07:27.845691 140433017227072 submission.py:296] 4000) loss = 5.445, grad_norm = 1.110
I0520 04:07:45.310032 140433017227072 spec.py:298] Evaluating on the training split.
I0520 04:08:29.387672 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 04:09:15.156718 140433017227072 spec.py:326] Evaluating on the test split.
I0520 04:09:16.576417 140433017227072 submission_runner.py:421] Time since start: 2186.53s, 	Step: 4043, 	{'train/accuracy': 0.22939453125, 'train/loss': 3.98714111328125, 'validation/accuracy': 0.2108, 'validation/loss': 4.0939303125, 'validation/num_examples': 50000, 'test/accuracy': 0.1639, 'test/loss': 4.47020703125, 'test/num_examples': 10000, 'score': 1685.6271612644196, 'total_duration': 2186.5280923843384, 'accumulated_submission_time': 1685.6271612644196, 'accumulated_eval_time': 498.7885112762451, 'accumulated_logging_time': 0.08195900917053223}
I0520 04:09:16.586319 140389881190144 logging_writer.py:48] [4043] accumulated_eval_time=498.788511, accumulated_logging_time=0.081959, accumulated_submission_time=1685.627161, global_step=4043, preemption_count=0, score=1685.627161, test/accuracy=0.163900, test/loss=4.470207, test/num_examples=10000, total_duration=2186.528092, train/accuracy=0.229395, train/loss=3.987141, validation/accuracy=0.210800, validation/loss=4.093930, validation/num_examples=50000
I0520 04:12:27.138806 140389382199040 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.949051, loss=5.140271
I0520 04:12:27.144331 140433017227072 submission.py:296] 4500) loss = 5.140, grad_norm = 0.949
I0520 04:15:54.075927 140389881190144 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.959503, loss=4.918431
I0520 04:15:54.081652 140433017227072 submission.py:296] 5000) loss = 4.918, grad_norm = 0.960
I0520 04:16:16.895769 140433017227072 spec.py:298] Evaluating on the training split.
I0520 04:17:00.963681 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 04:17:47.702886 140433017227072 spec.py:326] Evaluating on the test split.
I0520 04:17:49.122216 140433017227072 submission_runner.py:421] Time since start: 2699.07s, 	Step: 5051, 	{'train/accuracy': 0.2836328125, 'train/loss': 3.58359130859375, 'validation/accuracy': 0.257, 'validation/loss': 3.713456875, 'validation/num_examples': 50000, 'test/accuracy': 0.198, 'test/loss': 4.142003125, 'test/num_examples': 10000, 'score': 2105.4183411598206, 'total_duration': 2699.0721962451935, 'accumulated_submission_time': 2105.4183411598206, 'accumulated_eval_time': 591.0133395195007, 'accumulated_logging_time': 0.09991168975830078}
I0520 04:17:49.132213 140389382199040 logging_writer.py:48] [5051] accumulated_eval_time=591.013340, accumulated_logging_time=0.099912, accumulated_submission_time=2105.418341, global_step=5051, preemption_count=0, score=2105.418341, test/accuracy=0.198000, test/loss=4.142003, test/num_examples=10000, total_duration=2699.072196, train/accuracy=0.283633, train/loss=3.583591, validation/accuracy=0.257000, validation/loss=3.713457, validation/num_examples=50000
I0520 04:20:56.175434 140389881190144 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.099496, loss=5.224597
I0520 04:20:56.179965 140433017227072 submission.py:296] 5500) loss = 5.225, grad_norm = 1.099
I0520 04:24:23.741660 140389382199040 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.046008, loss=4.686327
I0520 04:24:23.747930 140433017227072 submission.py:296] 6000) loss = 4.686, grad_norm = 1.046
I0520 04:24:49.273144 140433017227072 spec.py:298] Evaluating on the training split.
I0520 04:25:33.556904 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 04:26:19.195572 140433017227072 spec.py:326] Evaluating on the test split.
I0520 04:26:20.613659 140433017227072 submission_runner.py:421] Time since start: 3210.57s, 	Step: 6063, 	{'train/accuracy': 0.33697265625, 'train/loss': 3.216190185546875, 'validation/accuracy': 0.30738, 'validation/loss': 3.371916875, 'validation/num_examples': 50000, 'test/accuracy': 0.2342, 'test/loss': 3.876329296875, 'test/num_examples': 10000, 'score': 2525.0394496917725, 'total_duration': 3210.5653052330017, 'accumulated_submission_time': 2525.0394496917725, 'accumulated_eval_time': 682.3537981510162, 'accumulated_logging_time': 0.11804366111755371}
I0520 04:26:20.624209 140389881190144 logging_writer.py:48] [6063] accumulated_eval_time=682.353798, accumulated_logging_time=0.118044, accumulated_submission_time=2525.039450, global_step=6063, preemption_count=0, score=2525.039450, test/accuracy=0.234200, test/loss=3.876329, test/num_examples=10000, total_duration=3210.565305, train/accuracy=0.336973, train/loss=3.216190, validation/accuracy=0.307380, validation/loss=3.371917, validation/num_examples=50000
I0520 04:29:24.059208 140389382199040 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.019694, loss=5.114761
I0520 04:29:24.065056 140433017227072 submission.py:296] 6500) loss = 5.115, grad_norm = 1.020
I0520 04:32:52.104759 140389881190144 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.784826, loss=5.332187
I0520 04:32:52.109233 140433017227072 submission.py:296] 7000) loss = 5.332, grad_norm = 0.785
I0520 04:33:20.786593 140433017227072 spec.py:298] Evaluating on the training split.
I0520 04:34:05.408006 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 04:34:51.401408 140433017227072 spec.py:326] Evaluating on the test split.
I0520 04:34:52.826730 140433017227072 submission_runner.py:421] Time since start: 3722.78s, 	Step: 7070, 	{'train/accuracy': 0.3833984375, 'train/loss': 2.9399542236328124, 'validation/accuracy': 0.35244, 'validation/loss': 3.1058503125, 'validation/num_examples': 50000, 'test/accuracy': 0.2715, 'test/loss': 3.637245703125, 'test/num_examples': 10000, 'score': 2944.6810286045074, 'total_duration': 3722.778384923935, 'accumulated_submission_time': 2944.6810286045074, 'accumulated_eval_time': 774.3939237594604, 'accumulated_logging_time': 0.13683366775512695}
I0520 04:34:52.837806 140389382199040 logging_writer.py:48] [7070] accumulated_eval_time=774.393924, accumulated_logging_time=0.136834, accumulated_submission_time=2944.681029, global_step=7070, preemption_count=0, score=2944.681029, test/accuracy=0.271500, test/loss=3.637246, test/num_examples=10000, total_duration=3722.778385, train/accuracy=0.383398, train/loss=2.939954, validation/accuracy=0.352440, validation/loss=3.105850, validation/num_examples=50000
I0520 04:37:51.258945 140389881190144 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.814059, loss=4.861500
I0520 04:37:51.263655 140433017227072 submission.py:296] 7500) loss = 4.861, grad_norm = 0.814
I0520 04:41:21.532400 140389382199040 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.893231, loss=4.528882
I0520 04:41:21.537099 140433017227072 submission.py:296] 8000) loss = 4.529, grad_norm = 0.893
I0520 04:41:53.085308 140433017227072 spec.py:298] Evaluating on the training split.
I0520 04:42:37.604831 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 04:43:23.631457 140433017227072 spec.py:326] Evaluating on the test split.
I0520 04:43:25.050776 140433017227072 submission_runner.py:421] Time since start: 4235.00s, 	Step: 8077, 	{'train/accuracy': 0.425078125, 'train/loss': 2.706744384765625, 'validation/accuracy': 0.39198, 'validation/loss': 2.8744134375, 'validation/num_examples': 50000, 'test/accuracy': 0.2963, 'test/loss': 3.452264453125, 'test/num_examples': 10000, 'score': 3364.4035460948944, 'total_duration': 4235.002417087555, 'accumulated_submission_time': 3364.4035460948944, 'accumulated_eval_time': 866.3593912124634, 'accumulated_logging_time': 0.15781474113464355}
I0520 04:43:25.061254 140389881190144 logging_writer.py:48] [8077] accumulated_eval_time=866.359391, accumulated_logging_time=0.157815, accumulated_submission_time=3364.403546, global_step=8077, preemption_count=0, score=3364.403546, test/accuracy=0.296300, test/loss=3.452264, test/num_examples=10000, total_duration=4235.002417, train/accuracy=0.425078, train/loss=2.706744, validation/accuracy=0.391980, validation/loss=2.874413, validation/num_examples=50000
I0520 04:46:21.065772 140389382199040 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.822800, loss=4.467749
I0520 04:46:21.071402 140433017227072 submission.py:296] 8500) loss = 4.468, grad_norm = 0.823
I0520 04:49:50.611998 140389881190144 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.939771, loss=4.356995
I0520 04:49:50.617534 140433017227072 submission.py:296] 9000) loss = 4.357, grad_norm = 0.940
I0520 04:50:25.120695 140433017227072 spec.py:298] Evaluating on the training split.
I0520 04:51:10.269557 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 04:51:56.244850 140433017227072 spec.py:326] Evaluating on the test split.
I0520 04:51:57.666916 140433017227072 submission_runner.py:421] Time since start: 4747.62s, 	Step: 9084, 	{'train/accuracy': 0.45369140625, 'train/loss': 2.547751617431641, 'validation/accuracy': 0.41874, 'validation/loss': 2.7400421875, 'validation/num_examples': 50000, 'test/accuracy': 0.3268, 'test/loss': 3.30404609375, 'test/num_examples': 10000, 'score': 3783.9549186229706, 'total_duration': 4747.618564605713, 'accumulated_submission_time': 3783.9549186229706, 'accumulated_eval_time': 958.9056537151337, 'accumulated_logging_time': 0.17646145820617676}
I0520 04:51:57.679579 140389382199040 logging_writer.py:48] [9084] accumulated_eval_time=958.905654, accumulated_logging_time=0.176461, accumulated_submission_time=3783.954919, global_step=9084, preemption_count=0, score=3783.954919, test/accuracy=0.326800, test/loss=3.304046, test/num_examples=10000, total_duration=4747.618565, train/accuracy=0.453691, train/loss=2.547752, validation/accuracy=0.418740, validation/loss=2.740042, validation/num_examples=50000
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0520 04:54:51.019086 140389881190144 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.755461, loss=4.498785
I0520 04:54:51.023665 140433017227072 submission.py:296] 9500) loss = 4.499, grad_norm = 0.755
I0520 04:58:18.054026 140389382199040 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.763884, loss=4.247703
I0520 04:58:18.058610 140433017227072 submission.py:296] 10000) loss = 4.248, grad_norm = 0.764
I0520 04:58:57.804830 140433017227072 spec.py:298] Evaluating on the training split.
I0520 04:59:43.179320 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 05:00:29.233341 140433017227072 spec.py:326] Evaluating on the test split.
I0520 05:00:30.651364 140433017227072 submission_runner.py:421] Time since start: 5260.60s, 	Step: 10092, 	{'train/accuracy': 0.47966796875, 'train/loss': 2.384369354248047, 'validation/accuracy': 0.4442, 'validation/loss': 2.57844578125, 'validation/num_examples': 50000, 'test/accuracy': 0.3435, 'test/loss': 3.1636318359375, 'test/num_examples': 10000, 'score': 4203.567542076111, 'total_duration': 5260.603030920029, 'accumulated_submission_time': 4203.567542076111, 'accumulated_eval_time': 1051.7522957324982, 'accumulated_logging_time': 0.19909358024597168}
I0520 05:00:30.661419 140389881190144 logging_writer.py:48] [10092] accumulated_eval_time=1051.752296, accumulated_logging_time=0.199094, accumulated_submission_time=4203.567542, global_step=10092, preemption_count=0, score=4203.567542, test/accuracy=0.343500, test/loss=3.163632, test/num_examples=10000, total_duration=5260.603031, train/accuracy=0.479668, train/loss=2.384369, validation/accuracy=0.444200, validation/loss=2.578446, validation/num_examples=50000
I0520 05:03:20.723058 140389382199040 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.677774, loss=5.065675
I0520 05:03:20.729344 140433017227072 submission.py:296] 10500) loss = 5.066, grad_norm = 0.678
I0520 05:06:48.382635 140389881190144 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.772906, loss=4.153857
I0520 05:06:48.387145 140433017227072 submission.py:296] 11000) loss = 4.154, grad_norm = 0.773
I0520 05:07:31.008467 140433017227072 spec.py:298] Evaluating on the training split.
I0520 05:08:15.971361 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 05:09:02.096982 140433017227072 spec.py:326] Evaluating on the test split.
I0520 05:09:03.515436 140433017227072 submission_runner.py:421] Time since start: 5773.47s, 	Step: 11104, 	{'train/accuracy': 0.51205078125, 'train/loss': 2.2432904052734375, 'validation/accuracy': 0.47448, 'validation/loss': 2.432456875, 'validation/num_examples': 50000, 'test/accuracy': 0.3746, 'test/loss': 3.0058154296875, 'test/num_examples': 10000, 'score': 4623.395784139633, 'total_duration': 5773.46706032753, 'accumulated_submission_time': 4623.395784139633, 'accumulated_eval_time': 1144.2594113349915, 'accumulated_logging_time': 0.21814727783203125}
I0520 05:09:03.526418 140389382199040 logging_writer.py:48] [11104] accumulated_eval_time=1144.259411, accumulated_logging_time=0.218147, accumulated_submission_time=4623.395784, global_step=11104, preemption_count=0, score=4623.395784, test/accuracy=0.374600, test/loss=3.005815, test/num_examples=10000, total_duration=5773.467060, train/accuracy=0.512051, train/loss=2.243290, validation/accuracy=0.474480, validation/loss=2.432457, validation/num_examples=50000
I0520 05:11:50.040660 140389881190144 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.775793, loss=4.298398
I0520 05:11:50.045675 140433017227072 submission.py:296] 11500) loss = 4.298, grad_norm = 0.776
I0520 05:15:18.095864 140389382199040 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.798993, loss=3.687971
I0520 05:15:18.100849 140433017227072 submission.py:296] 12000) loss = 3.688, grad_norm = 0.799
I0520 05:16:03.784584 140433017227072 spec.py:298] Evaluating on the training split.
I0520 05:16:48.486577 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 05:17:35.110114 140433017227072 spec.py:326] Evaluating on the test split.
I0520 05:17:36.527551 140433017227072 submission_runner.py:421] Time since start: 6286.48s, 	Step: 12111, 	{'train/accuracy': 0.53595703125, 'train/loss': 2.121379089355469, 'validation/accuracy': 0.49418, 'validation/loss': 2.33264671875, 'validation/num_examples': 50000, 'test/accuracy': 0.3909, 'test/loss': 2.9277451171875, 'test/num_examples': 10000, 'score': 5043.13570690155, 'total_duration': 6286.4791922569275, 'accumulated_submission_time': 5043.13570690155, 'accumulated_eval_time': 1237.0024359226227, 'accumulated_logging_time': 0.2381751537322998}
I0520 05:17:36.537787 140389881190144 logging_writer.py:48] [12111] accumulated_eval_time=1237.002436, accumulated_logging_time=0.238175, accumulated_submission_time=5043.135707, global_step=12111, preemption_count=0, score=5043.135707, test/accuracy=0.390900, test/loss=2.927745, test/num_examples=10000, total_duration=6286.479192, train/accuracy=0.535957, train/loss=2.121379, validation/accuracy=0.494180, validation/loss=2.332647, validation/num_examples=50000
I0520 05:20:18.213387 140389382199040 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.855463, loss=3.932546
I0520 05:20:18.220332 140433017227072 submission.py:296] 12500) loss = 3.933, grad_norm = 0.855
I0520 05:23:48.121244 140389881190144 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.722614, loss=4.087213
I0520 05:23:48.127089 140433017227072 submission.py:296] 13000) loss = 4.087, grad_norm = 0.723
I0520 05:24:36.790681 140433017227072 spec.py:298] Evaluating on the training split.
I0520 05:25:21.610698 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 05:26:07.507577 140433017227072 spec.py:326] Evaluating on the test split.
I0520 05:26:08.928132 140433017227072 submission_runner.py:421] Time since start: 6798.88s, 	Step: 13118, 	{'train/accuracy': 0.555546875, 'train/loss': 2.0113754272460938, 'validation/accuracy': 0.5139, 'validation/loss': 2.22309640625, 'validation/num_examples': 50000, 'test/accuracy': 0.4056, 'test/loss': 2.820813671875, 'test/num_examples': 10000, 'score': 5462.867415189743, 'total_duration': 6798.879762411118, 'accumulated_submission_time': 5462.867415189743, 'accumulated_eval_time': 1329.1399042606354, 'accumulated_logging_time': 0.2565188407897949}
I0520 05:26:08.939136 140389382199040 logging_writer.py:48] [13118] accumulated_eval_time=1329.139904, accumulated_logging_time=0.256519, accumulated_submission_time=5462.867415, global_step=13118, preemption_count=0, score=5462.867415, test/accuracy=0.405600, test/loss=2.820814, test/num_examples=10000, total_duration=6798.879762, train/accuracy=0.555547, train/loss=2.011375, validation/accuracy=0.513900, validation/loss=2.223096, validation/num_examples=50000
I0520 05:28:47.705047 140389881190144 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.714634, loss=4.258583
I0520 05:28:47.710160 140433017227072 submission.py:296] 13500) loss = 4.259, grad_norm = 0.715
I0520 05:32:17.193695 140389382199040 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.730153, loss=4.548620
I0520 05:32:17.199138 140433017227072 submission.py:296] 14000) loss = 4.549, grad_norm = 0.730
I0520 05:33:09.123009 140433017227072 spec.py:298] Evaluating on the training split.
I0520 05:33:53.546935 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 05:34:39.710916 140433017227072 spec.py:326] Evaluating on the test split.
I0520 05:34:41.130876 140433017227072 submission_runner.py:421] Time since start: 7311.08s, 	Step: 14126, 	{'train/accuracy': 0.572734375, 'train/loss': 1.9147151184082032, 'validation/accuracy': 0.52668, 'validation/loss': 2.12884875, 'validation/num_examples': 50000, 'test/accuracy': 0.4215, 'test/loss': 2.7349916015625, 'test/num_examples': 10000, 'score': 5882.532334089279, 'total_duration': 7311.082528352737, 'accumulated_submission_time': 5882.532334089279, 'accumulated_eval_time': 1421.1478300094604, 'accumulated_logging_time': 0.27570080757141113}
I0520 05:34:41.141174 140389881190144 logging_writer.py:48] [14126] accumulated_eval_time=1421.147830, accumulated_logging_time=0.275701, accumulated_submission_time=5882.532334, global_step=14126, preemption_count=0, score=5882.532334, test/accuracy=0.421500, test/loss=2.734992, test/num_examples=10000, total_duration=7311.082528, train/accuracy=0.572734, train/loss=1.914715, validation/accuracy=0.526680, validation/loss=2.128849, validation/num_examples=50000
I0520 05:37:17.121682 140389382199040 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.722060, loss=4.354156
I0520 05:37:17.127150 140433017227072 submission.py:296] 14500) loss = 4.354, grad_norm = 0.722
I0520 05:40:44.678038 140389881190144 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.757342, loss=3.740257
I0520 05:40:44.684499 140433017227072 submission.py:296] 15000) loss = 3.740, grad_norm = 0.757
I0520 05:41:41.265534 140433017227072 spec.py:298] Evaluating on the training split.
I0520 05:42:26.106883 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 05:43:12.165585 140433017227072 spec.py:326] Evaluating on the test split.
I0520 05:43:13.583353 140433017227072 submission_runner.py:421] Time since start: 7823.54s, 	Step: 15132, 	{'train/accuracy': 0.58841796875, 'train/loss': 1.8723348999023437, 'validation/accuracy': 0.54164, 'validation/loss': 2.10024703125, 'validation/num_examples': 50000, 'test/accuracy': 0.4304, 'test/loss': 2.7118236328125, 'test/num_examples': 10000, 'score': 6302.132746934891, 'total_duration': 7823.53501868248, 'accumulated_submission_time': 6302.132746934891, 'accumulated_eval_time': 1513.4657845497131, 'accumulated_logging_time': 0.2951791286468506}
I0520 05:43:13.593790 140389382199040 logging_writer.py:48] [15132] accumulated_eval_time=1513.465785, accumulated_logging_time=0.295179, accumulated_submission_time=6302.132747, global_step=15132, preemption_count=0, score=6302.132747, test/accuracy=0.430400, test/loss=2.711824, test/num_examples=10000, total_duration=7823.535019, train/accuracy=0.588418, train/loss=1.872335, validation/accuracy=0.541640, validation/loss=2.100247, validation/num_examples=50000
I0520 05:45:47.022466 140389881190144 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.742316, loss=4.026920
I0520 05:45:47.029116 140433017227072 submission.py:296] 15500) loss = 4.027, grad_norm = 0.742
I0520 05:49:14.954194 140389382199040 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.840890, loss=4.149711
I0520 05:49:14.959488 140433017227072 submission.py:296] 16000) loss = 4.150, grad_norm = 0.841
I0520 05:50:13.823329 140433017227072 spec.py:298] Evaluating on the training split.
I0520 05:50:58.907451 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 05:51:45.094751 140433017227072 spec.py:326] Evaluating on the test split.
I0520 05:51:46.516902 140433017227072 submission_runner.py:421] Time since start: 8336.47s, 	Step: 16143, 	{'train/accuracy': 0.606015625, 'train/loss': 1.7935594177246095, 'validation/accuracy': 0.5577, 'validation/loss': 2.02054609375, 'validation/num_examples': 50000, 'test/accuracy': 0.4409, 'test/loss': 2.6275302734375, 'test/num_examples': 10000, 'score': 6721.840544223785, 'total_duration': 8336.468514204025, 'accumulated_submission_time': 6721.840544223785, 'accumulated_eval_time': 1606.1594014167786, 'accumulated_logging_time': 0.31400346755981445}
I0520 05:51:46.529760 140389881190144 logging_writer.py:48] [16143] accumulated_eval_time=1606.159401, accumulated_logging_time=0.314003, accumulated_submission_time=6721.840544, global_step=16143, preemption_count=0, score=6721.840544, test/accuracy=0.440900, test/loss=2.627530, test/num_examples=10000, total_duration=8336.468514, train/accuracy=0.606016, train/loss=1.793559, validation/accuracy=0.557700, validation/loss=2.020546, validation/num_examples=50000
I0520 05:54:17.661553 140389382199040 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.778028, loss=3.621358
I0520 05:54:17.666448 140433017227072 submission.py:296] 16500) loss = 3.621, grad_norm = 0.778
I0520 05:57:46.028799 140389881190144 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.915460, loss=3.561024
I0520 05:57:46.034045 140433017227072 submission.py:296] 17000) loss = 3.561, grad_norm = 0.915
I0520 05:58:46.737619 140433017227072 spec.py:298] Evaluating on the training split.
I0520 05:59:31.161790 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 06:00:17.683291 140433017227072 spec.py:326] Evaluating on the test split.
I0520 06:00:19.101168 140433017227072 submission_runner.py:421] Time since start: 8849.05s, 	Step: 17147, 	{'train/accuracy': 0.617890625, 'train/loss': 1.7189463806152343, 'validation/accuracy': 0.56892, 'validation/loss': 1.9578615625, 'validation/num_examples': 50000, 'test/accuracy': 0.4507, 'test/loss': 2.5705626953125, 'test/num_examples': 10000, 'score': 7141.524657964706, 'total_duration': 8849.05283164978, 'accumulated_submission_time': 7141.524657964706, 'accumulated_eval_time': 1698.523205757141, 'accumulated_logging_time': 0.3365137577056885}
I0520 06:00:19.112035 140389382199040 logging_writer.py:48] [17147] accumulated_eval_time=1698.523206, accumulated_logging_time=0.336514, accumulated_submission_time=7141.524658, global_step=17147, preemption_count=0, score=7141.524658, test/accuracy=0.450700, test/loss=2.570563, test/num_examples=10000, total_duration=8849.052832, train/accuracy=0.617891, train/loss=1.718946, validation/accuracy=0.568920, validation/loss=1.957862, validation/num_examples=50000
I0520 06:02:45.441622 140389881190144 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.729178, loss=3.842505
I0520 06:02:45.448321 140433017227072 submission.py:296] 17500) loss = 3.843, grad_norm = 0.729
I0520 06:06:15.167891 140389382199040 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.718474, loss=4.376120
I0520 06:06:15.174001 140433017227072 submission.py:296] 18000) loss = 4.376, grad_norm = 0.718
I0520 06:07:19.301629 140433017227072 spec.py:298] Evaluating on the training split.
I0520 06:08:04.170409 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 06:08:50.076668 140433017227072 spec.py:326] Evaluating on the test split.
I0520 06:08:51.497606 140433017227072 submission_runner.py:421] Time since start: 9361.45s, 	Step: 18155, 	{'train/accuracy': 0.6296875, 'train/loss': 1.666795654296875, 'validation/accuracy': 0.57532, 'validation/loss': 1.90001328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4583, 'test/loss': 2.524188671875, 'test/num_examples': 10000, 'score': 7561.199949502945, 'total_duration': 9361.449240922928, 'accumulated_submission_time': 7561.199949502945, 'accumulated_eval_time': 1790.7191762924194, 'accumulated_logging_time': 0.3552582263946533}
I0520 06:08:51.508936 140389881190144 logging_writer.py:48] [18155] accumulated_eval_time=1790.719176, accumulated_logging_time=0.355258, accumulated_submission_time=7561.199950, global_step=18155, preemption_count=0, score=7561.199950, test/accuracy=0.458300, test/loss=2.524189, test/num_examples=10000, total_duration=9361.449241, train/accuracy=0.629687, train/loss=1.666796, validation/accuracy=0.575320, validation/loss=1.900013, validation/num_examples=50000
I0520 06:11:15.020988 140389382199040 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.751122, loss=3.873704
I0520 06:11:15.026862 140433017227072 submission.py:296] 18500) loss = 3.874, grad_norm = 0.751
I0520 06:14:44.400554 140389881190144 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.826751, loss=4.409014
I0520 06:14:44.405580 140433017227072 submission.py:296] 19000) loss = 4.409, grad_norm = 0.827
I0520 06:15:51.864762 140433017227072 spec.py:298] Evaluating on the training split.
I0520 06:16:37.124677 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 06:17:23.498730 140433017227072 spec.py:326] Evaluating on the test split.
I0520 06:17:24.917846 140433017227072 submission_runner.py:421] Time since start: 9874.87s, 	Step: 19163, 	{'train/accuracy': 0.64013671875, 'train/loss': 1.6009640502929687, 'validation/accuracy': 0.58996, 'validation/loss': 1.84397171875, 'validation/num_examples': 50000, 'test/accuracy': 0.4728, 'test/loss': 2.4491330078125, 'test/num_examples': 10000, 'score': 7981.040262937546, 'total_duration': 9874.869482755661, 'accumulated_submission_time': 7981.040262937546, 'accumulated_eval_time': 1883.7723050117493, 'accumulated_logging_time': 0.3759927749633789}
I0520 06:17:24.928356 140389382199040 logging_writer.py:48] [19163] accumulated_eval_time=1883.772305, accumulated_logging_time=0.375993, accumulated_submission_time=7981.040263, global_step=19163, preemption_count=0, score=7981.040263, test/accuracy=0.472800, test/loss=2.449133, test/num_examples=10000, total_duration=9874.869483, train/accuracy=0.640137, train/loss=1.600964, validation/accuracy=0.589960, validation/loss=1.843972, validation/num_examples=50000
I0520 06:19:45.262223 140389881190144 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.829535, loss=3.513485
I0520 06:19:45.267390 140433017227072 submission.py:296] 19500) loss = 3.513, grad_norm = 0.830
I0520 06:23:12.526965 140389382199040 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.880762, loss=3.524100
I0520 06:23:12.535314 140433017227072 submission.py:296] 20000) loss = 3.524, grad_norm = 0.881
I0520 06:24:25.182552 140433017227072 spec.py:298] Evaluating on the training split.
I0520 06:25:10.645698 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 06:25:56.659875 140433017227072 spec.py:326] Evaluating on the test split.
I0520 06:25:58.083173 140433017227072 submission_runner.py:421] Time since start: 10388.03s, 	Step: 20171, 	{'train/accuracy': 0.653203125, 'train/loss': 1.5454412841796874, 'validation/accuracy': 0.5995, 'validation/loss': 1.7900171875, 'validation/num_examples': 50000, 'test/accuracy': 0.4757, 'test/loss': 2.423836328125, 'test/num_examples': 10000, 'score': 8400.773209571838, 'total_duration': 10388.034816503525, 'accumulated_submission_time': 8400.773209571838, 'accumulated_eval_time': 1976.6729776859283, 'accumulated_logging_time': 0.3948171138763428}
I0520 06:25:58.096966 140389881190144 logging_writer.py:48] [20171] accumulated_eval_time=1976.672978, accumulated_logging_time=0.394817, accumulated_submission_time=8400.773210, global_step=20171, preemption_count=0, score=8400.773210, test/accuracy=0.475700, test/loss=2.423836, test/num_examples=10000, total_duration=10388.034817, train/accuracy=0.653203, train/loss=1.545441, validation/accuracy=0.599500, validation/loss=1.790017, validation/num_examples=50000
I0520 06:28:15.612858 140389382199040 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.852719, loss=3.830880
I0520 06:28:15.617359 140433017227072 submission.py:296] 20500) loss = 3.831, grad_norm = 0.853
I0520 06:31:43.297473 140389881190144 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.772837, loss=3.498181
I0520 06:31:43.303882 140433017227072 submission.py:296] 21000) loss = 3.498, grad_norm = 0.773
I0520 06:32:58.125904 140433017227072 spec.py:298] Evaluating on the training split.
I0520 06:33:43.158177 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 06:34:29.394150 140433017227072 spec.py:326] Evaluating on the test split.
I0520 06:34:30.812360 140433017227072 submission_runner.py:421] Time since start: 10900.76s, 	Step: 21182, 	{'train/accuracy': 0.65958984375, 'train/loss': 1.4983346557617188, 'validation/accuracy': 0.60534, 'validation/loss': 1.7476725, 'validation/num_examples': 50000, 'test/accuracy': 0.4926, 'test/loss': 2.3612431640625, 'test/num_examples': 10000, 'score': 8820.289219141006, 'total_duration': 10900.763979434967, 'accumulated_submission_time': 8820.289219141006, 'accumulated_eval_time': 2069.3593456745148, 'accumulated_logging_time': 0.4171593189239502}
I0520 06:34:30.822962 140389382199040 logging_writer.py:48] [21182] accumulated_eval_time=2069.359346, accumulated_logging_time=0.417159, accumulated_submission_time=8820.289219, global_step=21182, preemption_count=0, score=8820.289219, test/accuracy=0.492600, test/loss=2.361243, test/num_examples=10000, total_duration=10900.763979, train/accuracy=0.659590, train/loss=1.498335, validation/accuracy=0.605340, validation/loss=1.747672, validation/num_examples=50000
I0520 06:36:44.929229 140389881190144 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.746188, loss=3.906429
I0520 06:36:44.933790 140433017227072 submission.py:296] 21500) loss = 3.906, grad_norm = 0.746
I0520 06:40:13.234727 140389382199040 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.813091, loss=3.362060
I0520 06:40:13.239914 140433017227072 submission.py:296] 22000) loss = 3.362, grad_norm = 0.813
I0520 06:41:31.236557 140433017227072 spec.py:298] Evaluating on the training split.
I0520 06:42:16.321353 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 06:43:02.417244 140433017227072 spec.py:326] Evaluating on the test split.
I0520 06:43:03.836222 140433017227072 submission_runner.py:421] Time since start: 11413.79s, 	Step: 22189, 	{'train/accuracy': 0.6693359375, 'train/loss': 1.4878134155273437, 'validation/accuracy': 0.61096, 'validation/loss': 1.7461984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4936, 'test/loss': 2.354861328125, 'test/num_examples': 10000, 'score': 9240.17881155014, 'total_duration': 11413.787845134735, 'accumulated_submission_time': 9240.17881155014, 'accumulated_eval_time': 2161.9589858055115, 'accumulated_logging_time': 0.439816951751709}
I0520 06:43:03.847985 140389881190144 logging_writer.py:48] [22189] accumulated_eval_time=2161.958986, accumulated_logging_time=0.439817, accumulated_submission_time=9240.178812, global_step=22189, preemption_count=0, score=9240.178812, test/accuracy=0.493600, test/loss=2.354861, test/num_examples=10000, total_duration=11413.787845, train/accuracy=0.669336, train/loss=1.487813, validation/accuracy=0.610960, validation/loss=1.746198, validation/num_examples=50000
I0520 06:45:12.588353 140389382199040 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.843886, loss=3.359433
I0520 06:45:12.595281 140433017227072 submission.py:296] 22500) loss = 3.359, grad_norm = 0.844
I0520 06:48:42.696843 140389881190144 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.772042, loss=3.517884
I0520 06:48:42.702466 140433017227072 submission.py:296] 23000) loss = 3.518, grad_norm = 0.772
I0520 06:50:03.890544 140433017227072 spec.py:298] Evaluating on the training split.
I0520 06:50:48.636710 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 06:51:34.893601 140433017227072 spec.py:326] Evaluating on the test split.
I0520 06:51:36.313669 140433017227072 submission_runner.py:421] Time since start: 11926.27s, 	Step: 23196, 	{'train/accuracy': 0.67759765625, 'train/loss': 1.47083740234375, 'validation/accuracy': 0.62012, 'validation/loss': 1.7250171875, 'validation/num_examples': 50000, 'test/accuracy': 0.4995, 'test/loss': 2.3458791015625, 'test/num_examples': 10000, 'score': 9659.709810256958, 'total_duration': 11926.26534986496, 'accumulated_submission_time': 9659.709810256958, 'accumulated_eval_time': 2254.3821687698364, 'accumulated_logging_time': 0.4603290557861328}
I0520 06:51:36.325719 140389382199040 logging_writer.py:48] [23196] accumulated_eval_time=2254.382169, accumulated_logging_time=0.460329, accumulated_submission_time=9659.709810, global_step=23196, preemption_count=0, score=9659.709810, test/accuracy=0.499500, test/loss=2.345879, test/num_examples=10000, total_duration=11926.265350, train/accuracy=0.677598, train/loss=1.470837, validation/accuracy=0.620120, validation/loss=1.725017, validation/num_examples=50000
I0520 06:53:42.887069 140389881190144 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.847645, loss=3.995025
I0520 06:53:42.893486 140433017227072 submission.py:296] 23500) loss = 3.995, grad_norm = 0.848
I0520 06:57:11.969879 140389382199040 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.843742, loss=3.810895
I0520 06:57:11.981312 140433017227072 submission.py:296] 24000) loss = 3.811, grad_norm = 0.844
I0520 06:58:36.700792 140433017227072 spec.py:298] Evaluating on the training split.
I0520 06:59:21.734720 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 07:00:07.741847 140433017227072 spec.py:326] Evaluating on the test split.
I0520 07:00:09.165245 140433017227072 submission_runner.py:421] Time since start: 12439.12s, 	Step: 24204, 	{'train/accuracy': 0.68158203125, 'train/loss': 1.4087979125976562, 'validation/accuracy': 0.62462, 'validation/loss': 1.6674965625, 'validation/num_examples': 50000, 'test/accuracy': 0.5058, 'test/loss': 2.2828646484375, 'test/num_examples': 10000, 'score': 10079.566493749619, 'total_duration': 12439.11690378189, 'accumulated_submission_time': 10079.566493749619, 'accumulated_eval_time': 2346.846603155136, 'accumulated_logging_time': 0.48124241828918457}
I0520 07:00:09.175888 140389881190144 logging_writer.py:48] [24204] accumulated_eval_time=2346.846603, accumulated_logging_time=0.481242, accumulated_submission_time=10079.566494, global_step=24204, preemption_count=0, score=10079.566494, test/accuracy=0.505800, test/loss=2.282865, test/num_examples=10000, total_duration=12439.116904, train/accuracy=0.681582, train/loss=1.408798, validation/accuracy=0.624620, validation/loss=1.667497, validation/num_examples=50000
I0520 07:02:13.002531 140389382199040 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.813528, loss=3.372908
I0520 07:02:13.008033 140433017227072 submission.py:296] 24500) loss = 3.373, grad_norm = 0.814
I0520 07:05:40.338479 140389881190144 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.824775, loss=3.397138
I0520 07:05:40.348918 140433017227072 submission.py:296] 25000) loss = 3.397, grad_norm = 0.825
I0520 07:07:09.571403 140433017227072 spec.py:298] Evaluating on the training split.
I0520 07:07:54.435977 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 07:08:40.272196 140433017227072 spec.py:326] Evaluating on the test split.
I0520 07:08:41.691242 140433017227072 submission_runner.py:421] Time since start: 12951.64s, 	Step: 25211, 	{'train/accuracy': 0.6883984375, 'train/loss': 1.395908660888672, 'validation/accuracy': 0.63106, 'validation/loss': 1.65503015625, 'validation/num_examples': 50000, 'test/accuracy': 0.5118, 'test/loss': 2.278191015625, 'test/num_examples': 10000, 'score': 10499.4430205822, 'total_duration': 12951.642884254456, 'accumulated_submission_time': 10499.4430205822, 'accumulated_eval_time': 2438.966560125351, 'accumulated_logging_time': 0.5005738735198975}
I0520 07:08:41.707119 140389382199040 logging_writer.py:48] [25211] accumulated_eval_time=2438.966560, accumulated_logging_time=0.500574, accumulated_submission_time=10499.443021, global_step=25211, preemption_count=0, score=10499.443021, test/accuracy=0.511800, test/loss=2.278191, test/num_examples=10000, total_duration=12951.642884, train/accuracy=0.688398, train/loss=1.395909, validation/accuracy=0.631060, validation/loss=1.655030, validation/num_examples=50000
I0520 07:10:42.186077 140389881190144 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.786206, loss=3.752410
I0520 07:10:42.190957 140433017227072 submission.py:296] 25500) loss = 3.752, grad_norm = 0.786
I0520 07:14:09.838437 140389382199040 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.920222, loss=3.500094
I0520 07:14:09.842977 140433017227072 submission.py:296] 26000) loss = 3.500, grad_norm = 0.920
I0520 07:15:42.023887 140433017227072 spec.py:298] Evaluating on the training split.
I0520 07:16:26.961180 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 07:17:13.076567 140433017227072 spec.py:326] Evaluating on the test split.
I0520 07:17:14.494700 140433017227072 submission_runner.py:421] Time since start: 13464.45s, 	Step: 26224, 	{'train/accuracy': 0.6919140625, 'train/loss': 1.3616455078125, 'validation/accuracy': 0.6325, 'validation/loss': 1.623083125, 'validation/num_examples': 50000, 'test/accuracy': 0.5202, 'test/loss': 2.246783203125, 'test/num_examples': 10000, 'score': 10919.239442586899, 'total_duration': 13464.446360111237, 'accumulated_submission_time': 10919.239442586899, 'accumulated_eval_time': 2531.4374363422394, 'accumulated_logging_time': 0.5246033668518066}
I0520 07:17:14.505597 140389881190144 logging_writer.py:48] [26224] accumulated_eval_time=2531.437436, accumulated_logging_time=0.524603, accumulated_submission_time=10919.239443, global_step=26224, preemption_count=0, score=10919.239443, test/accuracy=0.520200, test/loss=2.246783, test/num_examples=10000, total_duration=13464.446360, train/accuracy=0.691914, train/loss=1.361646, validation/accuracy=0.632500, validation/loss=1.623083, validation/num_examples=50000
I0520 07:19:11.457048 140389382199040 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.841413, loss=3.410774
I0520 07:19:11.463562 140433017227072 submission.py:296] 26500) loss = 3.411, grad_norm = 0.841
I0520 07:22:39.584840 140389881190144 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.846950, loss=3.864751
I0520 07:22:39.590346 140433017227072 submission.py:296] 27000) loss = 3.865, grad_norm = 0.847
I0520 07:24:14.621293 140433017227072 spec.py:298] Evaluating on the training split.
I0520 07:24:59.358419 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 07:25:45.865850 140433017227072 spec.py:326] Evaluating on the test split.
I0520 07:25:47.283159 140433017227072 submission_runner.py:421] Time since start: 13977.23s, 	Step: 27230, 	{'train/accuracy': 0.7, 'train/loss': 1.3154541015625, 'validation/accuracy': 0.64278, 'validation/loss': 1.58098609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5192, 'test/loss': 2.2125556640625, 'test/num_examples': 10000, 'score': 11338.827892541885, 'total_duration': 13977.234819173813, 'accumulated_submission_time': 11338.827892541885, 'accumulated_eval_time': 2624.099377632141, 'accumulated_logging_time': 0.5455162525177002}
I0520 07:25:47.295537 140389382199040 logging_writer.py:48] [27230] accumulated_eval_time=2624.099378, accumulated_logging_time=0.545516, accumulated_submission_time=11338.827893, global_step=27230, preemption_count=0, score=11338.827893, test/accuracy=0.519200, test/loss=2.212556, test/num_examples=10000, total_duration=13977.234819, train/accuracy=0.700000, train/loss=1.315454, validation/accuracy=0.642780, validation/loss=1.580986, validation/num_examples=50000
I0520 07:27:39.183740 140389881190144 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.815341, loss=3.436368
I0520 07:27:39.189658 140433017227072 submission.py:296] 27500) loss = 3.436, grad_norm = 0.815
I0520 07:31:08.931189 140433017227072 spec.py:298] Evaluating on the training split.
I0520 07:31:53.835690 140433017227072 spec.py:310] Evaluating on the validation split.
I0520 07:32:40.193121 140433017227072 spec.py:326] Evaluating on the test split.
I0520 07:32:41.611013 140433017227072 submission_runner.py:421] Time since start: 14391.56s, 	Step: 28000, 	{'train/accuracy': 0.6992578125, 'train/loss': 1.3194717407226562, 'validation/accuracy': 0.64238, 'validation/loss': 1.58498109375, 'validation/num_examples': 50000, 'test/accuracy': 0.5165, 'test/loss': 2.2266119140625, 'test/num_examples': 10000, 'score': 11660.060758829117, 'total_duration': 14391.562633275986, 'accumulated_submission_time': 11660.060758829117, 'accumulated_eval_time': 2716.779245376587, 'accumulated_logging_time': 0.5673625469207764}
I0520 07:32:41.622425 140389382199040 logging_writer.py:48] [28000] accumulated_eval_time=2716.779245, accumulated_logging_time=0.567363, accumulated_submission_time=11660.060759, global_step=28000, preemption_count=0, score=11660.060759, test/accuracy=0.516500, test/loss=2.226612, test/num_examples=10000, total_duration=14391.562633, train/accuracy=0.699258, train/loss=1.319472, validation/accuracy=0.642380, validation/loss=1.584981, validation/num_examples=50000
I0520 07:32:41.639686 140389881190144 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=11660.060759
I0520 07:32:42.303903 140433017227072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b_pytorch/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_28000.
I0520 07:32:42.574984 140433017227072 submission_runner.py:584] Tuning trial 1/1
I0520 07:32:42.575187 140433017227072 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0520 07:32:42.575988 140433017227072 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00201171875, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00206, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0021, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.922562122344971, 'total_duration': 133.86207914352417, 'accumulated_submission_time': 6.922562122344971, 'accumulated_eval_time': 126.93824529647827, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1018, {'train/accuracy': 0.042265625, 'train/loss': 5.846652221679688, 'validation/accuracy': 0.03934, 'validation/loss': 5.878519375, 'validation/num_examples': 50000, 'test/accuracy': 0.0316, 'test/loss': 5.986660546875, 'test/num_examples': 10000, 'score': 426.50470876693726, 'total_duration': 642.4351246356964, 'accumulated_submission_time': 426.50470876693726, 'accumulated_eval_time': 215.4049220085144, 'accumulated_logging_time': 0.024768829345703125, 'global_step': 1018, 'preemption_count': 0}), (2027, {'train/accuracy': 0.09650390625, 'train/loss': 5.12178466796875, 'validation/accuracy': 0.08724, 'validation/loss': 5.1789365625, 'validation/num_examples': 50000, 'test/accuracy': 0.0688, 'test/loss': 5.389979296875, 'test/num_examples': 10000, 'score': 846.3808209896088, 'total_duration': 1154.7986431121826, 'accumulated_submission_time': 846.3808209896088, 'accumulated_eval_time': 307.36343026161194, 'accumulated_logging_time': 0.0432896614074707, 'global_step': 2027, 'preemption_count': 0}), (3036, {'train/accuracy': 0.16546875, 'train/loss': 4.482292785644531, 'validation/accuracy': 0.15032, 'validation/loss': 4.55971875, 'validation/num_examples': 50000, 'test/accuracy': 0.1152, 'test/loss': 4.866061328125, 'test/num_examples': 10000, 'score': 1265.995033979416, 'total_duration': 1675.1002728939056, 'accumulated_submission_time': 1265.995033979416, 'accumulated_eval_time': 407.5220241546631, 'accumulated_logging_time': 0.06199026107788086, 'global_step': 3036, 'preemption_count': 0}), (4043, {'train/accuracy': 0.22939453125, 'train/loss': 3.98714111328125, 'validation/accuracy': 0.2108, 'validation/loss': 4.0939303125, 'validation/num_examples': 50000, 'test/accuracy': 0.1639, 'test/loss': 4.47020703125, 'test/num_examples': 10000, 'score': 1685.6271612644196, 'total_duration': 2186.5280923843384, 'accumulated_submission_time': 1685.6271612644196, 'accumulated_eval_time': 498.7885112762451, 'accumulated_logging_time': 0.08195900917053223, 'global_step': 4043, 'preemption_count': 0}), (5051, {'train/accuracy': 0.2836328125, 'train/loss': 3.58359130859375, 'validation/accuracy': 0.257, 'validation/loss': 3.713456875, 'validation/num_examples': 50000, 'test/accuracy': 0.198, 'test/loss': 4.142003125, 'test/num_examples': 10000, 'score': 2105.4183411598206, 'total_duration': 2699.0721962451935, 'accumulated_submission_time': 2105.4183411598206, 'accumulated_eval_time': 591.0133395195007, 'accumulated_logging_time': 0.09991168975830078, 'global_step': 5051, 'preemption_count': 0}), (6063, {'train/accuracy': 0.33697265625, 'train/loss': 3.216190185546875, 'validation/accuracy': 0.30738, 'validation/loss': 3.371916875, 'validation/num_examples': 50000, 'test/accuracy': 0.2342, 'test/loss': 3.876329296875, 'test/num_examples': 10000, 'score': 2525.0394496917725, 'total_duration': 3210.5653052330017, 'accumulated_submission_time': 2525.0394496917725, 'accumulated_eval_time': 682.3537981510162, 'accumulated_logging_time': 0.11804366111755371, 'global_step': 6063, 'preemption_count': 0}), (7070, {'train/accuracy': 0.3833984375, 'train/loss': 2.9399542236328124, 'validation/accuracy': 0.35244, 'validation/loss': 3.1058503125, 'validation/num_examples': 50000, 'test/accuracy': 0.2715, 'test/loss': 3.637245703125, 'test/num_examples': 10000, 'score': 2944.6810286045074, 'total_duration': 3722.778384923935, 'accumulated_submission_time': 2944.6810286045074, 'accumulated_eval_time': 774.3939237594604, 'accumulated_logging_time': 0.13683366775512695, 'global_step': 7070, 'preemption_count': 0}), (8077, {'train/accuracy': 0.425078125, 'train/loss': 2.706744384765625, 'validation/accuracy': 0.39198, 'validation/loss': 2.8744134375, 'validation/num_examples': 50000, 'test/accuracy': 0.2963, 'test/loss': 3.452264453125, 'test/num_examples': 10000, 'score': 3364.4035460948944, 'total_duration': 4235.002417087555, 'accumulated_submission_time': 3364.4035460948944, 'accumulated_eval_time': 866.3593912124634, 'accumulated_logging_time': 0.15781474113464355, 'global_step': 8077, 'preemption_count': 0}), (9084, {'train/accuracy': 0.45369140625, 'train/loss': 2.547751617431641, 'validation/accuracy': 0.41874, 'validation/loss': 2.7400421875, 'validation/num_examples': 50000, 'test/accuracy': 0.3268, 'test/loss': 3.30404609375, 'test/num_examples': 10000, 'score': 3783.9549186229706, 'total_duration': 4747.618564605713, 'accumulated_submission_time': 3783.9549186229706, 'accumulated_eval_time': 958.9056537151337, 'accumulated_logging_time': 0.17646145820617676, 'global_step': 9084, 'preemption_count': 0}), (10092, {'train/accuracy': 0.47966796875, 'train/loss': 2.384369354248047, 'validation/accuracy': 0.4442, 'validation/loss': 2.57844578125, 'validation/num_examples': 50000, 'test/accuracy': 0.3435, 'test/loss': 3.1636318359375, 'test/num_examples': 10000, 'score': 4203.567542076111, 'total_duration': 5260.603030920029, 'accumulated_submission_time': 4203.567542076111, 'accumulated_eval_time': 1051.7522957324982, 'accumulated_logging_time': 0.19909358024597168, 'global_step': 10092, 'preemption_count': 0}), (11104, {'train/accuracy': 0.51205078125, 'train/loss': 2.2432904052734375, 'validation/accuracy': 0.47448, 'validation/loss': 2.432456875, 'validation/num_examples': 50000, 'test/accuracy': 0.3746, 'test/loss': 3.0058154296875, 'test/num_examples': 10000, 'score': 4623.395784139633, 'total_duration': 5773.46706032753, 'accumulated_submission_time': 4623.395784139633, 'accumulated_eval_time': 1144.2594113349915, 'accumulated_logging_time': 0.21814727783203125, 'global_step': 11104, 'preemption_count': 0}), (12111, {'train/accuracy': 0.53595703125, 'train/loss': 2.121379089355469, 'validation/accuracy': 0.49418, 'validation/loss': 2.33264671875, 'validation/num_examples': 50000, 'test/accuracy': 0.3909, 'test/loss': 2.9277451171875, 'test/num_examples': 10000, 'score': 5043.13570690155, 'total_duration': 6286.4791922569275, 'accumulated_submission_time': 5043.13570690155, 'accumulated_eval_time': 1237.0024359226227, 'accumulated_logging_time': 0.2381751537322998, 'global_step': 12111, 'preemption_count': 0}), (13118, {'train/accuracy': 0.555546875, 'train/loss': 2.0113754272460938, 'validation/accuracy': 0.5139, 'validation/loss': 2.22309640625, 'validation/num_examples': 50000, 'test/accuracy': 0.4056, 'test/loss': 2.820813671875, 'test/num_examples': 10000, 'score': 5462.867415189743, 'total_duration': 6798.879762411118, 'accumulated_submission_time': 5462.867415189743, 'accumulated_eval_time': 1329.1399042606354, 'accumulated_logging_time': 0.2565188407897949, 'global_step': 13118, 'preemption_count': 0}), (14126, {'train/accuracy': 0.572734375, 'train/loss': 1.9147151184082032, 'validation/accuracy': 0.52668, 'validation/loss': 2.12884875, 'validation/num_examples': 50000, 'test/accuracy': 0.4215, 'test/loss': 2.7349916015625, 'test/num_examples': 10000, 'score': 5882.532334089279, 'total_duration': 7311.082528352737, 'accumulated_submission_time': 5882.532334089279, 'accumulated_eval_time': 1421.1478300094604, 'accumulated_logging_time': 0.27570080757141113, 'global_step': 14126, 'preemption_count': 0}), (15132, {'train/accuracy': 0.58841796875, 'train/loss': 1.8723348999023437, 'validation/accuracy': 0.54164, 'validation/loss': 2.10024703125, 'validation/num_examples': 50000, 'test/accuracy': 0.4304, 'test/loss': 2.7118236328125, 'test/num_examples': 10000, 'score': 6302.132746934891, 'total_duration': 7823.53501868248, 'accumulated_submission_time': 6302.132746934891, 'accumulated_eval_time': 1513.4657845497131, 'accumulated_logging_time': 0.2951791286468506, 'global_step': 15132, 'preemption_count': 0}), (16143, {'train/accuracy': 0.606015625, 'train/loss': 1.7935594177246095, 'validation/accuracy': 0.5577, 'validation/loss': 2.02054609375, 'validation/num_examples': 50000, 'test/accuracy': 0.4409, 'test/loss': 2.6275302734375, 'test/num_examples': 10000, 'score': 6721.840544223785, 'total_duration': 8336.468514204025, 'accumulated_submission_time': 6721.840544223785, 'accumulated_eval_time': 1606.1594014167786, 'accumulated_logging_time': 0.31400346755981445, 'global_step': 16143, 'preemption_count': 0}), (17147, {'train/accuracy': 0.617890625, 'train/loss': 1.7189463806152343, 'validation/accuracy': 0.56892, 'validation/loss': 1.9578615625, 'validation/num_examples': 50000, 'test/accuracy': 0.4507, 'test/loss': 2.5705626953125, 'test/num_examples': 10000, 'score': 7141.524657964706, 'total_duration': 8849.05283164978, 'accumulated_submission_time': 7141.524657964706, 'accumulated_eval_time': 1698.523205757141, 'accumulated_logging_time': 0.3365137577056885, 'global_step': 17147, 'preemption_count': 0}), (18155, {'train/accuracy': 0.6296875, 'train/loss': 1.666795654296875, 'validation/accuracy': 0.57532, 'validation/loss': 1.90001328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4583, 'test/loss': 2.524188671875, 'test/num_examples': 10000, 'score': 7561.199949502945, 'total_duration': 9361.449240922928, 'accumulated_submission_time': 7561.199949502945, 'accumulated_eval_time': 1790.7191762924194, 'accumulated_logging_time': 0.3552582263946533, 'global_step': 18155, 'preemption_count': 0}), (19163, {'train/accuracy': 0.64013671875, 'train/loss': 1.6009640502929687, 'validation/accuracy': 0.58996, 'validation/loss': 1.84397171875, 'validation/num_examples': 50000, 'test/accuracy': 0.4728, 'test/loss': 2.4491330078125, 'test/num_examples': 10000, 'score': 7981.040262937546, 'total_duration': 9874.869482755661, 'accumulated_submission_time': 7981.040262937546, 'accumulated_eval_time': 1883.7723050117493, 'accumulated_logging_time': 0.3759927749633789, 'global_step': 19163, 'preemption_count': 0}), (20171, {'train/accuracy': 0.653203125, 'train/loss': 1.5454412841796874, 'validation/accuracy': 0.5995, 'validation/loss': 1.7900171875, 'validation/num_examples': 50000, 'test/accuracy': 0.4757, 'test/loss': 2.423836328125, 'test/num_examples': 10000, 'score': 8400.773209571838, 'total_duration': 10388.034816503525, 'accumulated_submission_time': 8400.773209571838, 'accumulated_eval_time': 1976.6729776859283, 'accumulated_logging_time': 0.3948171138763428, 'global_step': 20171, 'preemption_count': 0}), (21182, {'train/accuracy': 0.65958984375, 'train/loss': 1.4983346557617188, 'validation/accuracy': 0.60534, 'validation/loss': 1.7476725, 'validation/num_examples': 50000, 'test/accuracy': 0.4926, 'test/loss': 2.3612431640625, 'test/num_examples': 10000, 'score': 8820.289219141006, 'total_duration': 10900.763979434967, 'accumulated_submission_time': 8820.289219141006, 'accumulated_eval_time': 2069.3593456745148, 'accumulated_logging_time': 0.4171593189239502, 'global_step': 21182, 'preemption_count': 0}), (22189, {'train/accuracy': 0.6693359375, 'train/loss': 1.4878134155273437, 'validation/accuracy': 0.61096, 'validation/loss': 1.7461984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4936, 'test/loss': 2.354861328125, 'test/num_examples': 10000, 'score': 9240.17881155014, 'total_duration': 11413.787845134735, 'accumulated_submission_time': 9240.17881155014, 'accumulated_eval_time': 2161.9589858055115, 'accumulated_logging_time': 0.439816951751709, 'global_step': 22189, 'preemption_count': 0}), (23196, {'train/accuracy': 0.67759765625, 'train/loss': 1.47083740234375, 'validation/accuracy': 0.62012, 'validation/loss': 1.7250171875, 'validation/num_examples': 50000, 'test/accuracy': 0.4995, 'test/loss': 2.3458791015625, 'test/num_examples': 10000, 'score': 9659.709810256958, 'total_duration': 11926.26534986496, 'accumulated_submission_time': 9659.709810256958, 'accumulated_eval_time': 2254.3821687698364, 'accumulated_logging_time': 0.4603290557861328, 'global_step': 23196, 'preemption_count': 0}), (24204, {'train/accuracy': 0.68158203125, 'train/loss': 1.4087979125976562, 'validation/accuracy': 0.62462, 'validation/loss': 1.6674965625, 'validation/num_examples': 50000, 'test/accuracy': 0.5058, 'test/loss': 2.2828646484375, 'test/num_examples': 10000, 'score': 10079.566493749619, 'total_duration': 12439.11690378189, 'accumulated_submission_time': 10079.566493749619, 'accumulated_eval_time': 2346.846603155136, 'accumulated_logging_time': 0.48124241828918457, 'global_step': 24204, 'preemption_count': 0}), (25211, {'train/accuracy': 0.6883984375, 'train/loss': 1.395908660888672, 'validation/accuracy': 0.63106, 'validation/loss': 1.65503015625, 'validation/num_examples': 50000, 'test/accuracy': 0.5118, 'test/loss': 2.278191015625, 'test/num_examples': 10000, 'score': 10499.4430205822, 'total_duration': 12951.642884254456, 'accumulated_submission_time': 10499.4430205822, 'accumulated_eval_time': 2438.966560125351, 'accumulated_logging_time': 0.5005738735198975, 'global_step': 25211, 'preemption_count': 0}), (26224, {'train/accuracy': 0.6919140625, 'train/loss': 1.3616455078125, 'validation/accuracy': 0.6325, 'validation/loss': 1.623083125, 'validation/num_examples': 50000, 'test/accuracy': 0.5202, 'test/loss': 2.246783203125, 'test/num_examples': 10000, 'score': 10919.239442586899, 'total_duration': 13464.446360111237, 'accumulated_submission_time': 10919.239442586899, 'accumulated_eval_time': 2531.4374363422394, 'accumulated_logging_time': 0.5246033668518066, 'global_step': 26224, 'preemption_count': 0}), (27230, {'train/accuracy': 0.7, 'train/loss': 1.3154541015625, 'validation/accuracy': 0.64278, 'validation/loss': 1.58098609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5192, 'test/loss': 2.2125556640625, 'test/num_examples': 10000, 'score': 11338.827892541885, 'total_duration': 13977.234819173813, 'accumulated_submission_time': 11338.827892541885, 'accumulated_eval_time': 2624.099377632141, 'accumulated_logging_time': 0.5455162525177002, 'global_step': 27230, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6992578125, 'train/loss': 1.3194717407226562, 'validation/accuracy': 0.64238, 'validation/loss': 1.58498109375, 'validation/num_examples': 50000, 'test/accuracy': 0.5165, 'test/loss': 2.2266119140625, 'test/num_examples': 10000, 'score': 11660.060758829117, 'total_duration': 14391.562633275986, 'accumulated_submission_time': 11660.060758829117, 'accumulated_eval_time': 2716.779245376587, 'accumulated_logging_time': 0.5673625469207764, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0520 07:32:42.576108 140433017227072 submission_runner.py:587] Timing: 11660.060758829117
I0520 07:32:42.576159 140433017227072 submission_runner.py:588] ====================
I0520 07:32:42.576270 140433017227072 submission_runner.py:651] Final imagenet_vit score: 11660.060758829117
