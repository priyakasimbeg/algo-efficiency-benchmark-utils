WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0314 22:26:41.419213 140236504565568 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0314 22:26:41.419244 140707168585536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0314 22:26:41.419856 140265359931200 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0314 22:26:41.420197 139693072885568 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0314 22:26:41.420230 139893997848384 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0314 22:26:41.420282 140113046873920 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0314 22:26:41.420776 139672064325440 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0314 22:26:41.430251 140332132726592 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0314 22:26:41.430433 140265359931200 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0314 22:26:41.430530 140332132726592 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0314 22:26:41.430922 139693072885568 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0314 22:26:41.430947 140113046873920 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0314 22:26:41.430945 139893997848384 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0314 22:26:41.431268 139672064325440 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0314 22:26:41.440170 140236504565568 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0314 22:26:41.440206 140707168585536 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0314 22:26:41.803220 140332132726592 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing/librispeech_conformer_pytorch.
W0314 22:26:42.150269 139693072885568 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0314 22:26:42.150488 139672064325440 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0314 22:26:42.150735 140113046873920 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0314 22:26:42.150963 140265359931200 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0314 22:26:42.150990 140332132726592 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0314 22:26:42.151234 140707168585536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0314 22:26:42.151672 140236504565568 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0314 22:26:42.152033 139893997848384 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0314 22:26:42.155038 140332132726592 submission_runner.py:484] Using RNG seed 3375943589
I0314 22:26:42.156028 140332132726592 submission_runner.py:493] --- Tuning run 1/1 ---
I0314 22:26:42.156147 140332132726592 submission_runner.py:498] Creating tuning directory at /experiment_runs/timing/librispeech_conformer_pytorch/trial_1.
I0314 22:26:42.156341 140332132726592 logger_utils.py:84] Saving hparams to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/hparams.json.
I0314 22:26:42.157042 140332132726592 submission_runner.py:230] Initializing dataset.
I0314 22:26:42.157167 140332132726592 input_pipeline.py:20] Loading split = train-clean-100
I0314 22:26:42.184562 140332132726592 input_pipeline.py:20] Loading split = train-clean-360
I0314 22:26:42.493751 140332132726592 input_pipeline.py:20] Loading split = train-other-500
I0314 22:26:42.888397 140332132726592 submission_runner.py:237] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0314 22:26:50.700219 140332132726592 submission_runner.py:247] Initializing optimizer.
I0314 22:26:50.701379 140332132726592 submission_runner.py:254] Initializing metrics bundle.
I0314 22:26:50.701501 140332132726592 submission_runner.py:268] Initializing checkpoint and logger.
I0314 22:26:50.702629 140332132726592 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0314 22:26:50.702732 140332132726592 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0314 22:26:51.333976 140332132726592 submission_runner.py:289] Saving meta data to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0314 22:26:51.335005 140332132726592 submission_runner.py:292] Saving flags to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0314 22:26:51.366244 140332132726592 submission_runner.py:302] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0314 22:26:58.545116 140305904359168 logging_writer.py:48] [0] global_step=0, grad_norm=91.163971, loss=32.042809
I0314 22:26:58.569185 140332132726592 pytorch_submission_base.py:86] 0) loss = 32.043, grad_norm = 91.164
I0314 22:26:58.570096 140332132726592 spec.py:298] Evaluating on the training split.
I0314 22:26:58.571080 140332132726592 input_pipeline.py:20] Loading split = train-clean-100
I0314 22:26:58.596477 140332132726592 input_pipeline.py:20] Loading split = train-clean-360
I0314 22:26:59.012470 140332132726592 input_pipeline.py:20] Loading split = train-other-500
I0314 22:27:11.385057 140332132726592 spec.py:310] Evaluating on the validation split.
I0314 22:27:11.386329 140332132726592 input_pipeline.py:20] Loading split = dev-clean
I0314 22:27:11.390286 140332132726592 input_pipeline.py:20] Loading split = dev-other
I0314 22:27:21.192910 140332132726592 spec.py:326] Evaluating on the test split.
I0314 22:27:21.194083 140332132726592 input_pipeline.py:20] Loading split = test-clean
I0314 22:27:26.751881 140332132726592 submission_runner.py:362] Time since start: 7.20s, 	Step: 1, 	{'train/ctc_loss': 30.96843496830616, 'train/wer': 1.047464800898333, 'validation/ctc_loss': 30.504307313642755, 'validation/wer': 0.961125862984599, 'validation/num_examples': 5348, 'test/ctc_loss': 30.473346667526936, 'test/wer': 0.973310584364146, 'test/num_examples': 2472}
I0314 22:27:26.773836 140304661325568 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=7.202465, test/ctc_loss=30.473347, test/num_examples=2472, test/wer=0.973311, total_duration=7.204182, train/ctc_loss=30.968435, train/wer=1.047465, validation/ctc_loss=30.504307, validation/num_examples=5348, validation/wer=0.961126
I0314 22:27:27.356341 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_1.
I0314 22:27:27.391492 140332132726592 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0314 22:27:27.391615 140113046873920 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0314 22:27:27.392120 139893997848384 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0314 22:27:27.392134 140236504565568 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0314 22:27:27.392225 139693072885568 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0314 22:27:27.392256 140707168585536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0314 22:27:27.392352 139672064325440 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0314 22:27:27.392748 140265359931200 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0314 22:27:28.528384 140303512098560 logging_writer.py:48] [1] global_step=1, grad_norm=91.074249, loss=31.401306
I0314 22:27:28.532709 140332132726592 pytorch_submission_base.py:86] 1) loss = 31.401, grad_norm = 91.074
I0314 22:27:29.400226 140304661325568 logging_writer.py:48] [2] global_step=2, grad_norm=86.821320, loss=31.814764
I0314 22:27:29.404664 140332132726592 pytorch_submission_base.py:86] 2) loss = 31.815, grad_norm = 86.821
I0314 22:27:30.424102 140303512098560 logging_writer.py:48] [3] global_step=3, grad_norm=93.977066, loss=31.871220
I0314 22:27:30.427926 140332132726592 pytorch_submission_base.py:86] 3) loss = 31.871, grad_norm = 93.977
I0314 22:27:31.233104 140304661325568 logging_writer.py:48] [4] global_step=4, grad_norm=97.905884, loss=31.422356
I0314 22:27:31.236680 140332132726592 pytorch_submission_base.py:86] 4) loss = 31.422, grad_norm = 97.906
I0314 22:27:32.042752 140303512098560 logging_writer.py:48] [5] global_step=5, grad_norm=102.722282, loss=31.480022
I0314 22:27:32.046422 140332132726592 pytorch_submission_base.py:86] 5) loss = 31.480, grad_norm = 102.722
I0314 22:27:32.851250 140304661325568 logging_writer.py:48] [6] global_step=6, grad_norm=107.973801, loss=31.474836
I0314 22:27:32.854678 140332132726592 pytorch_submission_base.py:86] 6) loss = 31.475, grad_norm = 107.974
I0314 22:27:33.656136 140303512098560 logging_writer.py:48] [7] global_step=7, grad_norm=108.363564, loss=30.348593
I0314 22:27:33.660348 140332132726592 pytorch_submission_base.py:86] 7) loss = 30.349, grad_norm = 108.364
I0314 22:27:34.462199 140304661325568 logging_writer.py:48] [8] global_step=8, grad_norm=120.400040, loss=30.626911
I0314 22:27:34.465982 140332132726592 pytorch_submission_base.py:86] 8) loss = 30.627, grad_norm = 120.400
I0314 22:27:35.271615 140303512098560 logging_writer.py:48] [9] global_step=9, grad_norm=123.195763, loss=30.104654
I0314 22:27:35.275341 140332132726592 pytorch_submission_base.py:86] 9) loss = 30.105, grad_norm = 123.196
I0314 22:27:36.078600 140304661325568 logging_writer.py:48] [10] global_step=10, grad_norm=129.797424, loss=29.827593
I0314 22:27:36.083250 140332132726592 pytorch_submission_base.py:86] 10) loss = 29.828, grad_norm = 129.797
I0314 22:27:36.888091 140303512098560 logging_writer.py:48] [11] global_step=11, grad_norm=137.612320, loss=29.647173
I0314 22:27:36.893073 140332132726592 pytorch_submission_base.py:86] 11) loss = 29.647, grad_norm = 137.612
I0314 22:27:37.693427 140304661325568 logging_writer.py:48] [12] global_step=12, grad_norm=144.603882, loss=29.408535
I0314 22:27:37.696857 140332132726592 pytorch_submission_base.py:86] 12) loss = 29.409, grad_norm = 144.604
I0314 22:27:38.503437 140303512098560 logging_writer.py:48] [13] global_step=13, grad_norm=149.658295, loss=28.571762
I0314 22:27:38.507992 140332132726592 pytorch_submission_base.py:86] 13) loss = 28.572, grad_norm = 149.658
I0314 22:27:39.313082 140304661325568 logging_writer.py:48] [14] global_step=14, grad_norm=152.111542, loss=28.082087
I0314 22:27:39.317247 140332132726592 pytorch_submission_base.py:86] 14) loss = 28.082, grad_norm = 152.112
I0314 22:27:40.122250 140303512098560 logging_writer.py:48] [15] global_step=15, grad_norm=150.779312, loss=26.825699
I0314 22:27:40.126373 140332132726592 pytorch_submission_base.py:86] 15) loss = 26.826, grad_norm = 150.779
I0314 22:27:40.931135 140304661325568 logging_writer.py:48] [16] global_step=16, grad_norm=156.456131, loss=26.693293
I0314 22:27:40.934923 140332132726592 pytorch_submission_base.py:86] 16) loss = 26.693, grad_norm = 156.456
I0314 22:27:41.735954 140303512098560 logging_writer.py:48] [17] global_step=17, grad_norm=157.632950, loss=25.973984
I0314 22:27:41.740074 140332132726592 pytorch_submission_base.py:86] 17) loss = 25.974, grad_norm = 157.633
I0314 22:27:42.543915 140304661325568 logging_writer.py:48] [18] global_step=18, grad_norm=158.720703, loss=25.090794
I0314 22:27:42.547409 140332132726592 pytorch_submission_base.py:86] 18) loss = 25.091, grad_norm = 158.721
I0314 22:27:43.354390 140303512098560 logging_writer.py:48] [19] global_step=19, grad_norm=156.620712, loss=24.033295
I0314 22:27:43.357936 140332132726592 pytorch_submission_base.py:86] 19) loss = 24.033, grad_norm = 156.621
I0314 22:27:44.162426 140304661325568 logging_writer.py:48] [20] global_step=20, grad_norm=154.318878, loss=22.891588
I0314 22:27:44.165871 140332132726592 pytorch_submission_base.py:86] 20) loss = 22.892, grad_norm = 154.319
I0314 22:27:44.969014 140303512098560 logging_writer.py:48] [21] global_step=21, grad_norm=154.799240, loss=22.259634
I0314 22:27:44.972543 140332132726592 pytorch_submission_base.py:86] 21) loss = 22.260, grad_norm = 154.799
I0314 22:27:45.773290 140304661325568 logging_writer.py:48] [22] global_step=22, grad_norm=157.430664, loss=21.764444
I0314 22:27:45.776778 140332132726592 pytorch_submission_base.py:86] 22) loss = 21.764, grad_norm = 157.431
I0314 22:27:46.577802 140303512098560 logging_writer.py:48] [23] global_step=23, grad_norm=150.766525, loss=20.324211
I0314 22:27:46.581216 140332132726592 pytorch_submission_base.py:86] 23) loss = 20.324, grad_norm = 150.767
I0314 22:27:47.386561 140304661325568 logging_writer.py:48] [24] global_step=24, grad_norm=151.448792, loss=19.591000
I0314 22:27:47.390169 140332132726592 pytorch_submission_base.py:86] 24) loss = 19.591, grad_norm = 151.449
I0314 22:27:48.197224 140303512098560 logging_writer.py:48] [25] global_step=25, grad_norm=148.227234, loss=18.674303
I0314 22:27:48.200530 140332132726592 pytorch_submission_base.py:86] 25) loss = 18.674, grad_norm = 148.227
I0314 22:27:49.003492 140304661325568 logging_writer.py:48] [26] global_step=26, grad_norm=144.424881, loss=17.599335
I0314 22:27:49.007254 140332132726592 pytorch_submission_base.py:86] 26) loss = 17.599, grad_norm = 144.425
I0314 22:27:49.809058 140303512098560 logging_writer.py:48] [27] global_step=27, grad_norm=141.268188, loss=16.617182
I0314 22:27:49.812948 140332132726592 pytorch_submission_base.py:86] 27) loss = 16.617, grad_norm = 141.268
I0314 22:27:50.614829 140304661325568 logging_writer.py:48] [28] global_step=28, grad_norm=134.535828, loss=15.613757
I0314 22:27:50.618278 140332132726592 pytorch_submission_base.py:86] 28) loss = 15.614, grad_norm = 134.536
I0314 22:27:51.421240 140303512098560 logging_writer.py:48] [29] global_step=29, grad_norm=126.831879, loss=14.480886
I0314 22:27:51.424684 140332132726592 pytorch_submission_base.py:86] 29) loss = 14.481, grad_norm = 126.832
I0314 22:27:52.228907 140304661325568 logging_writer.py:48] [30] global_step=30, grad_norm=120.288948, loss=13.530557
I0314 22:27:52.232527 140332132726592 pytorch_submission_base.py:86] 30) loss = 13.531, grad_norm = 120.289
I0314 22:27:53.036834 140303512098560 logging_writer.py:48] [31] global_step=31, grad_norm=113.601097, loss=12.665916
I0314 22:27:53.040581 140332132726592 pytorch_submission_base.py:86] 31) loss = 12.666, grad_norm = 113.601
I0314 22:27:53.844515 140304661325568 logging_writer.py:48] [32] global_step=32, grad_norm=104.193314, loss=11.759109
I0314 22:27:53.848654 140332132726592 pytorch_submission_base.py:86] 32) loss = 11.759, grad_norm = 104.193
I0314 22:27:54.650167 140303512098560 logging_writer.py:48] [33] global_step=33, grad_norm=95.861771, loss=11.017030
I0314 22:27:54.653647 140332132726592 pytorch_submission_base.py:86] 33) loss = 11.017, grad_norm = 95.862
I0314 22:27:55.457035 140304661325568 logging_writer.py:48] [34] global_step=34, grad_norm=85.271179, loss=10.288497
I0314 22:27:55.460651 140332132726592 pytorch_submission_base.py:86] 34) loss = 10.288, grad_norm = 85.271
I0314 22:27:56.265036 140303512098560 logging_writer.py:48] [35] global_step=35, grad_norm=74.550797, loss=9.642780
I0314 22:27:56.267973 140332132726592 pytorch_submission_base.py:86] 35) loss = 9.643, grad_norm = 74.551
I0314 22:27:57.073089 140304661325568 logging_writer.py:48] [36] global_step=36, grad_norm=63.821850, loss=9.090414
I0314 22:27:57.076057 140332132726592 pytorch_submission_base.py:86] 36) loss = 9.090, grad_norm = 63.822
I0314 22:27:57.878480 140303512098560 logging_writer.py:48] [37] global_step=37, grad_norm=51.280167, loss=8.567632
I0314 22:27:57.881610 140332132726592 pytorch_submission_base.py:86] 37) loss = 8.568, grad_norm = 51.280
I0314 22:27:58.681116 140304661325568 logging_writer.py:48] [38] global_step=38, grad_norm=43.121887, loss=8.256018
I0314 22:27:58.684071 140332132726592 pytorch_submission_base.py:86] 38) loss = 8.256, grad_norm = 43.122
I0314 22:27:59.487182 140303512098560 logging_writer.py:48] [39] global_step=39, grad_norm=32.838158, loss=7.881604
I0314 22:27:59.490287 140332132726592 pytorch_submission_base.py:86] 39) loss = 7.882, grad_norm = 32.838
I0314 22:28:00.293087 140304661325568 logging_writer.py:48] [40] global_step=40, grad_norm=24.421221, loss=7.686295
I0314 22:28:00.296712 140332132726592 pytorch_submission_base.py:86] 40) loss = 7.686, grad_norm = 24.421
I0314 22:28:01.099633 140303512098560 logging_writer.py:48] [41] global_step=41, grad_norm=16.730955, loss=7.510052
I0314 22:28:01.102625 140332132726592 pytorch_submission_base.py:86] 41) loss = 7.510, grad_norm = 16.731
I0314 22:28:01.905892 140304661325568 logging_writer.py:48] [42] global_step=42, grad_norm=9.801006, loss=7.389600
I0314 22:28:01.909882 140332132726592 pytorch_submission_base.py:86] 42) loss = 7.390, grad_norm = 9.801
I0314 22:28:02.754519 140303512098560 logging_writer.py:48] [43] global_step=43, grad_norm=5.027003, loss=7.324510
I0314 22:28:02.757819 140332132726592 pytorch_submission_base.py:86] 43) loss = 7.325, grad_norm = 5.027
I0314 22:28:03.557165 140304661325568 logging_writer.py:48] [44] global_step=44, grad_norm=3.950860, loss=7.310015
I0314 22:28:03.560316 140332132726592 pytorch_submission_base.py:86] 44) loss = 7.310, grad_norm = 3.951
I0314 22:28:04.360349 140303512098560 logging_writer.py:48] [45] global_step=45, grad_norm=6.774826, loss=7.326859
I0314 22:28:04.363539 140332132726592 pytorch_submission_base.py:86] 45) loss = 7.327, grad_norm = 6.775
I0314 22:28:05.165509 140304661325568 logging_writer.py:48] [46] global_step=46, grad_norm=10.241358, loss=7.401702
I0314 22:28:05.169004 140332132726592 pytorch_submission_base.py:86] 46) loss = 7.402, grad_norm = 10.241
I0314 22:28:05.975199 140303512098560 logging_writer.py:48] [47] global_step=47, grad_norm=12.122224, loss=7.453975
I0314 22:28:05.978311 140332132726592 pytorch_submission_base.py:86] 47) loss = 7.454, grad_norm = 12.122
I0314 22:28:06.779105 140304661325568 logging_writer.py:48] [48] global_step=48, grad_norm=13.883943, loss=7.517311
I0314 22:28:06.782279 140332132726592 pytorch_submission_base.py:86] 48) loss = 7.517, grad_norm = 13.884
I0314 22:28:07.584313 140303512098560 logging_writer.py:48] [49] global_step=49, grad_norm=15.565376, loss=7.613861
I0314 22:28:07.587406 140332132726592 pytorch_submission_base.py:86] 49) loss = 7.614, grad_norm = 15.565
I0314 22:28:08.388962 140304661325568 logging_writer.py:48] [50] global_step=50, grad_norm=17.387466, loss=7.702450
I0314 22:28:08.392168 140332132726592 pytorch_submission_base.py:86] 50) loss = 7.702, grad_norm = 17.387
I0314 22:28:09.196944 140303512098560 logging_writer.py:48] [51] global_step=51, grad_norm=18.662991, loss=7.816303
I0314 22:28:09.200261 140332132726592 pytorch_submission_base.py:86] 51) loss = 7.816, grad_norm = 18.663
I0314 22:28:09.999271 140304661325568 logging_writer.py:48] [52] global_step=52, grad_norm=19.641432, loss=7.916216
I0314 22:28:10.002788 140332132726592 pytorch_submission_base.py:86] 52) loss = 7.916, grad_norm = 19.641
I0314 22:28:10.803134 140303512098560 logging_writer.py:48] [53] global_step=53, grad_norm=20.352806, loss=8.003956
I0314 22:28:10.806012 140332132726592 pytorch_submission_base.py:86] 53) loss = 8.004, grad_norm = 20.353
I0314 22:28:11.606703 140304661325568 logging_writer.py:48] [54] global_step=54, grad_norm=20.820858, loss=8.072833
I0314 22:28:11.609617 140332132726592 pytorch_submission_base.py:86] 54) loss = 8.073, grad_norm = 20.821
I0314 22:28:12.411071 140303512098560 logging_writer.py:48] [55] global_step=55, grad_norm=21.174749, loss=8.130411
I0314 22:28:12.414252 140332132726592 pytorch_submission_base.py:86] 55) loss = 8.130, grad_norm = 21.175
I0314 22:28:13.217952 140304661325568 logging_writer.py:48] [56] global_step=56, grad_norm=21.858191, loss=8.237164
I0314 22:28:13.221215 140332132726592 pytorch_submission_base.py:86] 56) loss = 8.237, grad_norm = 21.858
I0314 22:28:14.025770 140303512098560 logging_writer.py:48] [57] global_step=57, grad_norm=22.106075, loss=8.299396
I0314 22:28:14.028847 140332132726592 pytorch_submission_base.py:86] 57) loss = 8.299, grad_norm = 22.106
I0314 22:28:14.831195 140304661325568 logging_writer.py:48] [58] global_step=58, grad_norm=22.448845, loss=8.397015
I0314 22:28:14.834327 140332132726592 pytorch_submission_base.py:86] 58) loss = 8.397, grad_norm = 22.449
I0314 22:28:15.635416 140303512098560 logging_writer.py:48] [59] global_step=59, grad_norm=22.713633, loss=8.445890
I0314 22:28:15.638391 140332132726592 pytorch_submission_base.py:86] 59) loss = 8.446, grad_norm = 22.714
I0314 22:28:16.442158 140304661325568 logging_writer.py:48] [60] global_step=60, grad_norm=22.841427, loss=8.486680
I0314 22:28:16.445327 140332132726592 pytorch_submission_base.py:86] 60) loss = 8.487, grad_norm = 22.841
I0314 22:28:17.247882 140303512098560 logging_writer.py:48] [61] global_step=61, grad_norm=23.157637, loss=8.544041
I0314 22:28:17.251134 140332132726592 pytorch_submission_base.py:86] 61) loss = 8.544, grad_norm = 23.158
I0314 22:28:18.068697 140304661325568 logging_writer.py:48] [62] global_step=62, grad_norm=23.209305, loss=8.584777
I0314 22:28:18.071929 140332132726592 pytorch_submission_base.py:86] 62) loss = 8.585, grad_norm = 23.209
I0314 22:28:18.875735 140303512098560 logging_writer.py:48] [63] global_step=63, grad_norm=23.235533, loss=8.599489
I0314 22:28:18.879598 140332132726592 pytorch_submission_base.py:86] 63) loss = 8.599, grad_norm = 23.236
I0314 22:28:19.685764 140304661325568 logging_writer.py:48] [64] global_step=64, grad_norm=23.482317, loss=8.641196
I0314 22:28:19.688825 140332132726592 pytorch_submission_base.py:86] 64) loss = 8.641, grad_norm = 23.482
I0314 22:28:20.489528 140303512098560 logging_writer.py:48] [65] global_step=65, grad_norm=23.596087, loss=8.686838
I0314 22:28:20.492779 140332132726592 pytorch_submission_base.py:86] 65) loss = 8.687, grad_norm = 23.596
I0314 22:28:21.296037 140304661325568 logging_writer.py:48] [66] global_step=66, grad_norm=23.679781, loss=8.700992
I0314 22:28:21.298944 140332132726592 pytorch_submission_base.py:86] 66) loss = 8.701, grad_norm = 23.680
I0314 22:28:22.104035 140303512098560 logging_writer.py:48] [67] global_step=67, grad_norm=23.786657, loss=8.719779
I0314 22:28:22.106992 140332132726592 pytorch_submission_base.py:86] 67) loss = 8.720, grad_norm = 23.787
I0314 22:28:22.906442 140304661325568 logging_writer.py:48] [68] global_step=68, grad_norm=23.814350, loss=8.721654
I0314 22:28:22.909552 140332132726592 pytorch_submission_base.py:86] 68) loss = 8.722, grad_norm = 23.814
I0314 22:28:23.710305 140303512098560 logging_writer.py:48] [69] global_step=69, grad_norm=23.753979, loss=8.677002
I0314 22:28:23.713524 140332132726592 pytorch_submission_base.py:86] 69) loss = 8.677, grad_norm = 23.754
I0314 22:28:24.512888 140304661325568 logging_writer.py:48] [70] global_step=70, grad_norm=23.830488, loss=8.698175
I0314 22:28:24.516251 140332132726592 pytorch_submission_base.py:86] 70) loss = 8.698, grad_norm = 23.830
I0314 22:28:25.319090 140303512098560 logging_writer.py:48] [71] global_step=71, grad_norm=23.767717, loss=8.674047
I0314 22:28:25.322369 140332132726592 pytorch_submission_base.py:86] 71) loss = 8.674, grad_norm = 23.768
I0314 22:28:26.126334 140304661325568 logging_writer.py:48] [72] global_step=72, grad_norm=23.850681, loss=8.656551
I0314 22:28:26.129307 140332132726592 pytorch_submission_base.py:86] 72) loss = 8.657, grad_norm = 23.851
I0314 22:28:26.929481 140303512098560 logging_writer.py:48] [73] global_step=73, grad_norm=23.808134, loss=8.620639
I0314 22:28:26.932703 140332132726592 pytorch_submission_base.py:86] 73) loss = 8.621, grad_norm = 23.808
I0314 22:28:27.732456 140304661325568 logging_writer.py:48] [74] global_step=74, grad_norm=23.820959, loss=8.585452
I0314 22:28:27.735425 140332132726592 pytorch_submission_base.py:86] 74) loss = 8.585, grad_norm = 23.821
I0314 22:28:28.548629 140303512098560 logging_writer.py:48] [75] global_step=75, grad_norm=23.692955, loss=8.532214
I0314 22:28:28.551970 140332132726592 pytorch_submission_base.py:86] 75) loss = 8.532, grad_norm = 23.693
I0314 22:28:29.356538 140304661325568 logging_writer.py:48] [76] global_step=76, grad_norm=23.705250, loss=8.493529
I0314 22:28:29.359963 140332132726592 pytorch_submission_base.py:86] 76) loss = 8.494, grad_norm = 23.705
I0314 22:28:30.168292 140303512098560 logging_writer.py:48] [77] global_step=77, grad_norm=23.590862, loss=8.454570
I0314 22:28:30.171782 140332132726592 pytorch_submission_base.py:86] 77) loss = 8.455, grad_norm = 23.591
I0314 22:28:30.975758 140304661325568 logging_writer.py:48] [78] global_step=78, grad_norm=23.497753, loss=8.409045
I0314 22:28:30.979186 140332132726592 pytorch_submission_base.py:86] 78) loss = 8.409, grad_norm = 23.498
I0314 22:28:31.782249 140303512098560 logging_writer.py:48] [79] global_step=79, grad_norm=23.309460, loss=8.337785
I0314 22:28:31.785737 140332132726592 pytorch_submission_base.py:86] 79) loss = 8.338, grad_norm = 23.309
I0314 22:28:32.586028 140304661325568 logging_writer.py:48] [80] global_step=80, grad_norm=23.049980, loss=8.258325
I0314 22:28:32.589708 140332132726592 pytorch_submission_base.py:86] 80) loss = 8.258, grad_norm = 23.050
I0314 22:28:33.391503 140303512098560 logging_writer.py:48] [81] global_step=81, grad_norm=22.868654, loss=8.187332
I0314 22:28:33.394705 140332132726592 pytorch_submission_base.py:86] 81) loss = 8.187, grad_norm = 22.869
I0314 22:28:34.197281 140304661325568 logging_writer.py:48] [82] global_step=82, grad_norm=22.563585, loss=8.126006
I0314 22:28:34.200146 140332132726592 pytorch_submission_base.py:86] 82) loss = 8.126, grad_norm = 22.564
I0314 22:28:35.000100 140303512098560 logging_writer.py:48] [83] global_step=83, grad_norm=22.140276, loss=8.031800
I0314 22:28:35.003279 140332132726592 pytorch_submission_base.py:86] 83) loss = 8.032, grad_norm = 22.140
I0314 22:28:35.802152 140304661325568 logging_writer.py:48] [84] global_step=84, grad_norm=21.756121, loss=7.951501
I0314 22:28:35.805330 140332132726592 pytorch_submission_base.py:86] 84) loss = 7.952, grad_norm = 21.756
I0314 22:28:36.605982 140303512098560 logging_writer.py:48] [85] global_step=85, grad_norm=21.188623, loss=7.849952
I0314 22:28:36.609065 140332132726592 pytorch_submission_base.py:86] 85) loss = 7.850, grad_norm = 21.189
I0314 22:28:37.409451 140304661325568 logging_writer.py:48] [86] global_step=86, grad_norm=20.707039, loss=7.789313
I0314 22:28:37.412433 140332132726592 pytorch_submission_base.py:86] 86) loss = 7.789, grad_norm = 20.707
I0314 22:28:38.212834 140303512098560 logging_writer.py:48] [87] global_step=87, grad_norm=20.054661, loss=7.703773
I0314 22:28:38.215795 140332132726592 pytorch_submission_base.py:86] 87) loss = 7.704, grad_norm = 20.055
I0314 22:28:39.016393 140304661325568 logging_writer.py:48] [88] global_step=88, grad_norm=19.036959, loss=7.594434
I0314 22:28:39.019559 140332132726592 pytorch_submission_base.py:86] 88) loss = 7.594, grad_norm = 19.037
I0314 22:28:39.820693 140303512098560 logging_writer.py:48] [89] global_step=89, grad_norm=17.936871, loss=7.508196
I0314 22:28:39.823843 140332132726592 pytorch_submission_base.py:86] 89) loss = 7.508, grad_norm = 17.937
I0314 22:28:40.627205 140304661325568 logging_writer.py:48] [90] global_step=90, grad_norm=16.990484, loss=7.427524
I0314 22:28:40.630165 140332132726592 pytorch_submission_base.py:86] 90) loss = 7.428, grad_norm = 16.990
I0314 22:28:41.433576 140303512098560 logging_writer.py:48] [91] global_step=91, grad_norm=15.356476, loss=7.326006
I0314 22:28:41.436838 140332132726592 pytorch_submission_base.py:86] 91) loss = 7.326, grad_norm = 15.356
I0314 22:28:42.239273 140304661325568 logging_writer.py:48] [92] global_step=92, grad_norm=13.817310, loss=7.268190
I0314 22:28:42.242268 140332132726592 pytorch_submission_base.py:86] 92) loss = 7.268, grad_norm = 13.817
I0314 22:28:43.042124 140303512098560 logging_writer.py:48] [93] global_step=93, grad_norm=11.918686, loss=7.192474
I0314 22:28:43.045220 140332132726592 pytorch_submission_base.py:86] 93) loss = 7.192, grad_norm = 11.919
I0314 22:28:43.858971 140304661325568 logging_writer.py:48] [94] global_step=94, grad_norm=10.043024, loss=7.126000
I0314 22:28:43.862223 140332132726592 pytorch_submission_base.py:86] 94) loss = 7.126, grad_norm = 10.043
I0314 22:28:44.664503 140303512098560 logging_writer.py:48] [95] global_step=95, grad_norm=7.777717, loss=7.097462
I0314 22:28:44.667647 140332132726592 pytorch_submission_base.py:86] 95) loss = 7.097, grad_norm = 7.778
I0314 22:28:45.470351 140304661325568 logging_writer.py:48] [96] global_step=96, grad_norm=4.863215, loss=7.057766
I0314 22:28:45.473443 140332132726592 pytorch_submission_base.py:86] 96) loss = 7.058, grad_norm = 4.863
I0314 22:28:46.276191 140303512098560 logging_writer.py:48] [97] global_step=97, grad_norm=2.842733, loss=7.036057
I0314 22:28:46.279169 140332132726592 pytorch_submission_base.py:86] 97) loss = 7.036, grad_norm = 2.843
I0314 22:28:47.078118 140304661325568 logging_writer.py:48] [98] global_step=98, grad_norm=3.094814, loss=7.016973
I0314 22:28:47.081678 140332132726592 pytorch_submission_base.py:86] 98) loss = 7.017, grad_norm = 3.095
I0314 22:28:47.882937 140303512098560 logging_writer.py:48] [99] global_step=99, grad_norm=6.159721, loss=7.028172
I0314 22:28:47.885852 140332132726592 pytorch_submission_base.py:86] 99) loss = 7.028, grad_norm = 6.160
I0314 22:28:48.688388 140304661325568 logging_writer.py:48] [100] global_step=100, grad_norm=9.164680, loss=7.065607
I0314 22:28:48.691446 140332132726592 pytorch_submission_base.py:86] 100) loss = 7.066, grad_norm = 9.165
I0314 22:34:05.970838 140303512098560 logging_writer.py:48] [500] global_step=500, grad_norm=0.266513, loss=5.815809
I0314 22:34:05.974809 140332132726592 pytorch_submission_base.py:86] 500) loss = 5.816, grad_norm = 0.267
I0314 22:40:42.614052 140304661325568 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.340884, loss=5.800570
I0314 22:40:42.618944 140332132726592 pytorch_submission_base.py:86] 1000) loss = 5.801, grad_norm = 0.341
I0314 22:47:21.603269 140304661325568 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.401484, loss=5.494646
I0314 22:47:21.610303 140332132726592 pytorch_submission_base.py:86] 1500) loss = 5.495, grad_norm = 0.401
I0314 22:53:58.665054 140303512098560 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.939658, loss=4.077748
I0314 22:53:58.669820 140332132726592 pytorch_submission_base.py:86] 2000) loss = 4.078, grad_norm = 0.940
I0314 23:00:37.628810 140304584083200 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.994454, loss=3.237317
I0314 23:00:37.636369 140332132726592 pytorch_submission_base.py:86] 2500) loss = 3.237, grad_norm = 0.994
I0314 23:07:14.300069 140303512098560 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.916537, loss=2.857765
I0314 23:07:14.306469 140332132726592 pytorch_submission_base.py:86] 3000) loss = 2.858, grad_norm = 0.917
I0314 23:07:27.790345 140332132726592 spec.py:298] Evaluating on the training split.
I0314 23:07:39.415643 140332132726592 spec.py:310] Evaluating on the validation split.
I0314 23:07:49.258507 140332132726592 spec.py:326] Evaluating on the test split.
I0314 23:07:54.797069 140332132726592 submission_runner.py:362] Time since start: 2436.42s, 	Step: 3018, 	{'train/ctc_loss': 3.6094356970950137, 'train/wer': 0.7456756931847629, 'validation/ctc_loss': 3.8125525856439624, 'validation/wer': 0.7277458600878676, 'validation/num_examples': 5348, 'test/ctc_loss': 3.5379093006000386, 'test/wer': 0.7044665163609773, 'test/num_examples': 2472}
I0314 23:07:54.814271 140304584083200 logging_writer.py:48] [3018] global_step=3018, preemption_count=0, score=2402.652750, test/ctc_loss=3.537909, test/num_examples=2472, test/wer=0.704467, total_duration=2436.424472, train/ctc_loss=3.609436, train/wer=0.745676, validation/ctc_loss=3.812553, validation/num_examples=5348, validation/wer=0.727746
I0314 23:07:55.363802 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_3018.
I0314 23:14:20.153312 140304584083200 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.856227, loss=2.578504
I0314 23:14:20.160310 140332132726592 pytorch_submission_base.py:86] 3500) loss = 2.579, grad_norm = 0.856
I0314 23:20:56.293414 140303512098560 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.975970, loss=2.399533
I0314 23:20:56.297694 140332132726592 pytorch_submission_base.py:86] 4000) loss = 2.400, grad_norm = 0.976
I0314 23:27:34.351160 140304584083200 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.828521, loss=2.217814
I0314 23:27:34.357872 140332132726592 pytorch_submission_base.py:86] 4500) loss = 2.218, grad_norm = 0.829
I0314 23:34:10.461235 140303512098560 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.736712, loss=2.075931
I0314 23:34:10.466626 140332132726592 pytorch_submission_base.py:86] 5000) loss = 2.076, grad_norm = 0.737
I0314 23:40:48.405424 140304584083200 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.893758, loss=2.032647
I0314 23:40:48.412461 140332132726592 pytorch_submission_base.py:86] 5500) loss = 2.033, grad_norm = 0.894
I0314 23:47:24.171155 140303512098560 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.813071, loss=1.948408
I0314 23:47:24.175526 140332132726592 pytorch_submission_base.py:86] 6000) loss = 1.948, grad_norm = 0.813
I0314 23:47:55.849812 140332132726592 spec.py:298] Evaluating on the training split.
I0314 23:48:07.591864 140332132726592 spec.py:310] Evaluating on the validation split.
I0314 23:48:17.773690 140332132726592 spec.py:326] Evaluating on the test split.
I0314 23:48:23.330311 140332132726592 submission_runner.py:362] Time since start: 4864.48s, 	Step: 6041, 	{'train/ctc_loss': 0.722153751144029, 'train/wer': 0.23592035933316058, 'validation/ctc_loss': 0.9383261636339663, 'validation/wer': 0.2765799256505576, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6581364120265824, 'test/wer': 0.21666361992972194, 'test/num_examples': 2472}
I0314 23:48:23.346310 140304584083200 logging_writer.py:48] [6041] global_step=6041, preemption_count=0, score=4798.132726, test/ctc_loss=0.658136, test/num_examples=2472, test/wer=0.216664, total_duration=4864.483936, train/ctc_loss=0.722154, train/wer=0.235920, validation/ctc_loss=0.938326, validation/num_examples=5348, validation/wer=0.276580
I0314 23:48:23.887894 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_6041.
I0314 23:54:29.920579 140303528883968 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.875456, loss=1.850515
I0314 23:54:29.926835 140332132726592 pytorch_submission_base.py:86] 6500) loss = 1.851, grad_norm = 0.875
I0315 00:01:05.798343 140303512098560 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.790786, loss=1.781906
I0315 00:01:05.803336 140332132726592 pytorch_submission_base.py:86] 7000) loss = 1.782, grad_norm = 0.791
I0315 00:07:43.398938 140303528883968 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.771999, loss=1.799748
I0315 00:07:43.407303 140332132726592 pytorch_submission_base.py:86] 7500) loss = 1.800, grad_norm = 0.772
I0315 00:14:19.261235 140303512098560 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.675384, loss=1.731234
I0315 00:14:19.266574 140332132726592 pytorch_submission_base.py:86] 8000) loss = 1.731, grad_norm = 0.675
I0315 00:20:57.203873 140303528883968 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.716583, loss=1.719584
I0315 00:20:57.209959 140332132726592 pytorch_submission_base.py:86] 8500) loss = 1.720, grad_norm = 0.717
I0315 00:27:32.695408 140303512098560 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.771277, loss=1.724317
I0315 00:27:32.700120 140332132726592 pytorch_submission_base.py:86] 9000) loss = 1.724, grad_norm = 0.771
I0315 00:28:24.065284 140332132726592 spec.py:298] Evaluating on the training split.
I0315 00:28:36.146249 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 00:28:46.070055 140332132726592 spec.py:326] Evaluating on the test split.
I0315 00:28:51.749778 140332132726592 submission_runner.py:362] Time since start: 7292.70s, 	Step: 9066, 	{'train/ctc_loss': 0.5241767228229551, 'train/wer': 0.17665414183294464, 'validation/ctc_loss': 0.7385148440639442, 'validation/wer': 0.22181238835513928, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4864874346732047, 'test/wer': 0.1630613612820669, 'test/num_examples': 2472}
I0315 00:28:51.766721 140303528883968 logging_writer.py:48] [9066] global_step=9066, preemption_count=0, score=7193.345130, test/ctc_loss=0.486487, test/num_examples=2472, test/wer=0.163061, total_duration=7292.698788, train/ctc_loss=0.524177, train/wer=0.176654, validation/ctc_loss=0.738515, validation/num_examples=5348, validation/wer=0.221812
I0315 00:28:52.318875 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_9066.
I0315 00:34:38.439378 140303528883968 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.831682, loss=1.643079
I0315 00:34:38.447131 140332132726592 pytorch_submission_base.py:86] 9500) loss = 1.643, grad_norm = 0.832
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0315 00:41:13.927445 140303512098560 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.867351, loss=1.729103
I0315 00:41:13.933390 140332132726592 pytorch_submission_base.py:86] 10000) loss = 1.729, grad_norm = 0.867
I0315 00:47:51.197538 140303528883968 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.717974, loss=1.694383
I0315 00:47:51.204411 140332132726592 pytorch_submission_base.py:86] 10500) loss = 1.694, grad_norm = 0.718
I0315 00:54:26.882926 140303512098560 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.755712, loss=1.638917
I0315 00:54:26.887284 140332132726592 pytorch_submission_base.py:86] 11000) loss = 1.639, grad_norm = 0.756
I0315 01:01:04.249189 140303528883968 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.643323, loss=1.659685
I0315 01:01:04.255422 140332132726592 pytorch_submission_base.py:86] 11500) loss = 1.660, grad_norm = 0.643
I0315 01:07:39.595297 140303512098560 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.728444, loss=1.624885
I0315 01:07:39.602567 140332132726592 pytorch_submission_base.py:86] 12000) loss = 1.625, grad_norm = 0.728
I0315 01:08:52.380294 140332132726592 spec.py:298] Evaluating on the training split.
I0315 01:09:03.917853 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 01:09:14.109724 140332132726592 spec.py:326] Evaluating on the test split.
I0315 01:09:19.949256 140332132726592 submission_runner.py:362] Time since start: 9721.01s, 	Step: 12093, 	{'train/ctc_loss': 0.44721111148774617, 'train/wer': 0.15147490714347414, 'validation/ctc_loss': 0.6628024557495731, 'validation/wer': 0.19776951672862453, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4195256044744822, 'test/wer': 0.14171389108931, 'test/num_examples': 2472}
I0315 01:09:19.968387 140303528883968 logging_writer.py:48] [12093] global_step=12093, preemption_count=0, score=9588.323011, test/ctc_loss=0.419526, test/num_examples=2472, test/wer=0.141714, total_duration=9721.014415, train/ctc_loss=0.447211, train/wer=0.151475, validation/ctc_loss=0.662802, validation/num_examples=5348, validation/wer=0.197770
I0315 01:09:20.608082 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_12093.
I0315 01:14:45.553540 140303528883968 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.768312, loss=1.566348
I0315 01:14:45.560179 140332132726592 pytorch_submission_base.py:86] 12500) loss = 1.566, grad_norm = 0.768
I0315 01:21:21.055197 140303512098560 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.732276, loss=1.559209
I0315 01:21:21.059976 140332132726592 pytorch_submission_base.py:86] 13000) loss = 1.559, grad_norm = 0.732
I0315 01:27:58.537949 140303528883968 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.611334, loss=1.485981
I0315 01:27:58.544646 140332132726592 pytorch_submission_base.py:86] 13500) loss = 1.486, grad_norm = 0.611
I0315 01:34:33.862707 140303512098560 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.639885, loss=1.502655
I0315 01:34:33.867212 140332132726592 pytorch_submission_base.py:86] 14000) loss = 1.503, grad_norm = 0.640
I0315 01:41:11.301659 140303528883968 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.664467, loss=1.507701
I0315 01:41:11.308697 140332132726592 pytorch_submission_base.py:86] 14500) loss = 1.508, grad_norm = 0.664
I0315 01:47:46.852314 140303512098560 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.613108, loss=1.544922
I0315 01:47:46.886981 140332132726592 pytorch_submission_base.py:86] 15000) loss = 1.545, grad_norm = 0.613
I0315 01:49:20.930625 140332132726592 spec.py:298] Evaluating on the training split.
I0315 01:49:32.567139 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 01:49:42.522222 140332132726592 spec.py:326] Evaluating on the test split.
I0315 01:49:48.219740 140332132726592 submission_runner.py:362] Time since start: 12149.56s, 	Step: 15120, 	{'train/ctc_loss': 0.39052497690756244, 'train/wer': 0.13473373931070226, 'validation/ctc_loss': 0.6001378803433293, 'validation/wer': 0.18154782020953025, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3759363003258275, 'test/wer': 0.12745516218796335, 'test/num_examples': 2472}
I0315 01:49:48.235251 140303528883968 logging_writer.py:48] [15120] global_step=15120, preemption_count=0, score=11983.654450, test/ctc_loss=0.375936, test/num_examples=2472, test/wer=0.127455, total_duration=12149.564177, train/ctc_loss=0.390525, train/wer=0.134734, validation/ctc_loss=0.600138, validation/num_examples=5348, validation/wer=0.181548
I0315 01:49:48.791131 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_15120.
I0315 01:54:51.990480 140303528883968 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.762596, loss=1.475399
I0315 01:54:51.998180 140332132726592 pytorch_submission_base.py:86] 15500) loss = 1.475, grad_norm = 0.763
I0315 02:01:27.137714 140303512098560 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.754850, loss=1.547805
I0315 02:01:27.168231 140332132726592 pytorch_submission_base.py:86] 16000) loss = 1.548, grad_norm = 0.755
I0315 02:08:04.485401 140303520491264 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.611208, loss=1.444470
I0315 02:08:04.495859 140332132726592 pytorch_submission_base.py:86] 16500) loss = 1.444, grad_norm = 0.611
I0315 02:14:39.804822 140303512098560 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.675272, loss=1.458792
I0315 02:14:39.809361 140332132726592 pytorch_submission_base.py:86] 17000) loss = 1.459, grad_norm = 0.675
I0315 02:21:14.951725 140303520491264 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.613741, loss=1.465877
I0315 02:21:14.983817 140332132726592 pytorch_submission_base.py:86] 17500) loss = 1.466, grad_norm = 0.614
I0315 02:27:52.251596 140303520491264 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.663436, loss=1.464285
I0315 02:27:52.257986 140332132726592 pytorch_submission_base.py:86] 18000) loss = 1.464, grad_norm = 0.663
I0315 02:29:49.314812 140332132726592 spec.py:298] Evaluating on the training split.
I0315 02:30:00.940443 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 02:30:10.976548 140332132726592 spec.py:326] Evaluating on the test split.
I0315 02:30:16.473520 140332132726592 submission_runner.py:362] Time since start: 14577.95s, 	Step: 18149, 	{'train/ctc_loss': 0.3519921199705095, 'train/wer': 0.12198756154444157, 'validation/ctc_loss': 0.5588411772277476, 'validation/wer': 0.16827113407039057, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3412302305793922, 'test/wer': 0.112688643795828, 'test/num_examples': 2472}
I0315 02:30:16.489227 140303520491264 logging_writer.py:48] [18149] global_step=18149, preemption_count=0, score=14379.147094, test/ctc_loss=0.341230, test/num_examples=2472, test/wer=0.112689, total_duration=14577.948523, train/ctc_loss=0.351992, train/wer=0.121988, validation/ctc_loss=0.558841, validation/num_examples=5348, validation/wer=0.168271
I0315 02:30:17.040018 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_18149.
I0315 02:34:55.454889 140303512098560 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.632910, loss=1.489402
I0315 02:34:55.458217 140332132726592 pytorch_submission_base.py:86] 18500) loss = 1.489, grad_norm = 0.633
I0315 02:41:32.525607 140303520491264 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.610893, loss=1.379282
I0315 02:41:32.533475 140332132726592 pytorch_submission_base.py:86] 19000) loss = 1.379, grad_norm = 0.611
I0315 02:48:07.880283 140303512098560 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.639008, loss=1.399723
I0315 02:48:07.884716 140332132726592 pytorch_submission_base.py:86] 19500) loss = 1.400, grad_norm = 0.639
I0315 02:54:45.063867 140303520491264 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.678244, loss=1.431409
I0315 02:54:45.070673 140332132726592 pytorch_submission_base.py:86] 20000) loss = 1.431, grad_norm = 0.678
I0315 03:01:20.469510 140303512098560 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.748016, loss=1.433223
I0315 03:01:20.474686 140332132726592 pytorch_submission_base.py:86] 20500) loss = 1.433, grad_norm = 0.748
I0315 03:07:57.478610 140303520491264 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.691397, loss=1.424889
I0315 03:07:57.486861 140332132726592 pytorch_submission_base.py:86] 21000) loss = 1.425, grad_norm = 0.691
I0315 03:10:17.378699 140332132726592 spec.py:298] Evaluating on the training split.
I0315 03:10:29.015145 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 03:10:39.018636 140332132726592 spec.py:326] Evaluating on the test split.
I0315 03:10:44.644922 140332132726592 submission_runner.py:362] Time since start: 17006.01s, 	Step: 21178, 	{'train/ctc_loss': 0.32471356903155824, 'train/wer': 0.11319858339811696, 'validation/ctc_loss': 0.5322827781231163, 'validation/wer': 0.1578332448220924, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3175757306923027, 'test/wer': 0.1087481973473077, 'test/num_examples': 2472}
I0315 03:10:44.661894 140303520491264 logging_writer.py:48] [21178] global_step=21178, preemption_count=0, score=16774.518631, test/ctc_loss=0.317576, test/num_examples=2472, test/wer=0.108748, total_duration=17006.011899, train/ctc_loss=0.324714, train/wer=0.113199, validation/ctc_loss=0.532283, validation/num_examples=5348, validation/wer=0.157833
I0315 03:10:45.225901 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_21178.
I0315 03:15:00.572767 140303512098560 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.627204, loss=1.431169
I0315 03:15:00.576516 140332132726592 pytorch_submission_base.py:86] 21500) loss = 1.431, grad_norm = 0.627
I0315 03:21:37.910207 140303520491264 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.645531, loss=1.405125
I0315 03:21:37.917448 140332132726592 pytorch_submission_base.py:86] 22000) loss = 1.405, grad_norm = 0.646
I0315 03:28:12.914144 140303512098560 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.661144, loss=1.417519
I0315 03:28:12.919900 140332132726592 pytorch_submission_base.py:86] 22500) loss = 1.418, grad_norm = 0.661
I0315 03:34:49.838766 140303520491264 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.644187, loss=1.390113
I0315 03:34:49.845290 140332132726592 pytorch_submission_base.py:86] 23000) loss = 1.390, grad_norm = 0.644
I0315 03:41:25.041462 140303512098560 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.812054, loss=1.389147
I0315 03:41:25.046658 140332132726592 pytorch_submission_base.py:86] 23500) loss = 1.389, grad_norm = 0.812
I0315 03:48:02.422555 140303520491264 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.850247, loss=1.406448
I0315 03:48:02.428645 140332132726592 pytorch_submission_base.py:86] 24000) loss = 1.406, grad_norm = 0.850
I0315 03:50:45.960528 140332132726592 spec.py:298] Evaluating on the training split.
I0315 03:50:57.245498 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 03:51:07.256996 140332132726592 spec.py:326] Evaluating on the test split.
I0315 03:51:12.934595 140332132726592 submission_runner.py:362] Time since start: 19434.59s, 	Step: 24208, 	{'train/ctc_loss': 0.31330180015084236, 'train/wer': 0.10926297831908094, 'validation/ctc_loss': 0.5262415607890044, 'validation/wer': 0.15551585960507894, 'validation/num_examples': 5348, 'test/ctc_loss': 0.31283797543389896, 'test/wer': 0.10399528771352548, 'test/num_examples': 2472}
I0315 03:51:12.951454 140303520491264 logging_writer.py:48] [24208] global_step=24208, preemption_count=0, score=19170.285363, test/ctc_loss=0.312838, test/num_examples=2472, test/wer=0.103995, total_duration=19434.594646, train/ctc_loss=0.313302, train/wer=0.109263, validation/ctc_loss=0.526242, validation/num_examples=5348, validation/wer=0.155516
I0315 03:51:13.505487 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_24208.
I0315 03:55:05.184541 140303512098560 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.611333, loss=1.352762
I0315 03:55:05.188706 140332132726592 pytorch_submission_base.py:86] 24500) loss = 1.353, grad_norm = 0.611
I0315 04:01:42.438292 140303520491264 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.650948, loss=1.438876
I0315 04:01:42.449972 140332132726592 pytorch_submission_base.py:86] 25000) loss = 1.439, grad_norm = 0.651
I0315 04:08:17.634633 140303512098560 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.771916, loss=1.365108
I0315 04:08:17.642711 140332132726592 pytorch_submission_base.py:86] 25500) loss = 1.365, grad_norm = 0.772
I0315 04:14:54.440695 140303520491264 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.695335, loss=1.396845
I0315 04:14:54.446827 140332132726592 pytorch_submission_base.py:86] 26000) loss = 1.397, grad_norm = 0.695
I0315 04:21:29.544324 140303512098560 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.701038, loss=1.372337
I0315 04:21:29.549272 140332132726592 pytorch_submission_base.py:86] 26500) loss = 1.372, grad_norm = 0.701
I0315 04:28:06.626991 140303520491264 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.561741, loss=1.315156
I0315 04:28:06.633340 140332132726592 pytorch_submission_base.py:86] 27000) loss = 1.315, grad_norm = 0.562
I0315 04:31:13.920761 140332132726592 spec.py:298] Evaluating on the training split.
I0315 04:31:25.437454 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 04:31:35.476849 140332132726592 spec.py:326] Evaluating on the test split.
I0315 04:31:41.277020 140332132726592 submission_runner.py:362] Time since start: 21862.55s, 	Step: 27238, 	{'train/ctc_loss': 0.29714575353377853, 'train/wer': 0.10386434309406582, 'validation/ctc_loss': 0.5061534523652552, 'validation/wer': 0.15011828320378506, 'validation/num_examples': 5348, 'test/ctc_loss': 0.29858735845861023, 'test/wer': 0.09995328336684745, 'test/num_examples': 2472}
I0315 04:31:41.294548 140303520491264 logging_writer.py:48] [27238] global_step=27238, preemption_count=0, score=21565.656719, test/ctc_loss=0.298587, test/num_examples=2472, test/wer=0.099953, total_duration=21862.554295, train/ctc_loss=0.297146, train/wer=0.103864, validation/ctc_loss=0.506153, validation/num_examples=5348, validation/wer=0.150118
I0315 04:31:41.902737 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_27238.
I0315 04:35:09.682702 140303512098560 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.589806, loss=1.330554
I0315 04:35:09.687553 140332132726592 pytorch_submission_base.py:86] 27500) loss = 1.331, grad_norm = 0.590
I0315 04:41:46.744720 140303520491264 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.633356, loss=1.321937
I0315 04:41:46.753900 140332132726592 pytorch_submission_base.py:86] 28000) loss = 1.322, grad_norm = 0.633
I0315 04:48:21.998973 140303512098560 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.667866, loss=1.362430
I0315 04:48:22.003374 140332132726592 pytorch_submission_base.py:86] 28500) loss = 1.362, grad_norm = 0.668
I0315 04:54:58.761496 140303520491264 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.858423, loss=1.351974
I0315 04:54:58.767253 140332132726592 pytorch_submission_base.py:86] 29000) loss = 1.352, grad_norm = 0.858
I0315 05:01:34.022536 140303512098560 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.760599, loss=1.337685
I0315 05:01:34.057022 140332132726592 pytorch_submission_base.py:86] 29500) loss = 1.338, grad_norm = 0.761
I0315 05:08:11.195066 140303520491264 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.698195, loss=1.373525
I0315 05:08:11.201605 140332132726592 pytorch_submission_base.py:86] 30000) loss = 1.374, grad_norm = 0.698
I0315 05:11:41.915822 140332132726592 spec.py:298] Evaluating on the training split.
I0315 05:11:53.803236 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 05:12:04.201307 140332132726592 spec.py:326] Evaluating on the test split.
I0315 05:12:10.153317 140332132726592 submission_runner.py:362] Time since start: 24290.55s, 	Step: 30268, 	{'train/ctc_loss': 0.28198832242974814, 'train/wer': 0.09915133454262762, 'validation/ctc_loss': 0.48798820276396426, 'validation/wer': 0.1444986240525274, 'validation/num_examples': 5348, 'test/ctc_loss': 0.28797787861313634, 'test/wer': 0.09524099689232832, 'test/num_examples': 2472}
I0315 05:12:10.170495 140303520491264 logging_writer.py:48] [30268] global_step=30268, preemption_count=0, score=23960.718621, test/ctc_loss=0.287978, test/num_examples=2472, test/wer=0.095241, total_duration=24290.549506, train/ctc_loss=0.281988, train/wer=0.099151, validation/ctc_loss=0.487988, validation/num_examples=5348, validation/wer=0.144499
I0315 05:12:10.726218 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_30268.
I0315 05:15:14.853278 140303512098560 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.683349, loss=1.380401
I0315 05:15:14.856714 140332132726592 pytorch_submission_base.py:86] 30500) loss = 1.380, grad_norm = 0.683
I0315 05:21:51.844830 140303520491264 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.657773, loss=1.359022
I0315 05:21:51.851617 140332132726592 pytorch_submission_base.py:86] 31000) loss = 1.359, grad_norm = 0.658
I0315 05:28:26.720096 140303512098560 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.645179, loss=1.329406
I0315 05:28:26.751203 140332132726592 pytorch_submission_base.py:86] 31500) loss = 1.329, grad_norm = 0.645
I0315 05:35:03.593339 140303520491264 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.681046, loss=1.326398
I0315 05:35:03.601140 140332132726592 pytorch_submission_base.py:86] 32000) loss = 1.326, grad_norm = 0.681
I0315 05:41:38.786589 140303512098560 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.679365, loss=1.248978
I0315 05:41:38.820806 140332132726592 pytorch_submission_base.py:86] 32500) loss = 1.249, grad_norm = 0.679
I0315 05:48:15.985076 140303520491264 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.723721, loss=1.309208
I0315 05:48:15.993159 140332132726592 pytorch_submission_base.py:86] 33000) loss = 1.309, grad_norm = 0.724
I0315 05:52:11.450639 140332132726592 spec.py:298] Evaluating on the training split.
I0315 05:52:22.991791 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 05:52:32.937118 140332132726592 spec.py:326] Evaluating on the test split.
I0315 05:52:40.142785 140332132726592 submission_runner.py:362] Time since start: 26720.08s, 	Step: 33299, 	{'train/ctc_loss': 0.27310318929866784, 'train/wer': 0.096074112464369, 'validation/ctc_loss': 0.47696223919090314, 'validation/wer': 0.14244194467242793, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2784739892493064, 'test/wer': 0.09282391891617411, 'test/num_examples': 2472}
I0315 05:52:40.160216 140303520491264 logging_writer.py:48] [33299] global_step=33299, preemption_count=0, score=26356.407925, test/ctc_loss=0.278474, test/num_examples=2472, test/wer=0.092824, total_duration=26720.084229, train/ctc_loss=0.273103, train/wer=0.096074, validation/ctc_loss=0.476962, validation/num_examples=5348, validation/wer=0.142442
I0315 05:52:40.710367 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_33299.
I0315 05:55:20.391601 140303512098560 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.660907, loss=1.306346
I0315 05:55:20.395443 140332132726592 pytorch_submission_base.py:86] 33500) loss = 1.306, grad_norm = 0.661
I0315 06:01:57.651986 140303520491264 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.712687, loss=1.263235
I0315 06:01:57.668856 140332132726592 pytorch_submission_base.py:86] 34000) loss = 1.263, grad_norm = 0.713
I0315 06:08:32.897604 140303512098560 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.739366, loss=1.299138
I0315 06:08:32.902017 140332132726592 pytorch_submission_base.py:86] 34500) loss = 1.299, grad_norm = 0.739
I0315 06:15:08.319221 140303520491264 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.675106, loss=1.314727
I0315 06:15:08.324271 140332132726592 pytorch_submission_base.py:86] 35000) loss = 1.315, grad_norm = 0.675
I0315 06:21:45.561923 140303520491264 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.641924, loss=1.359715
I0315 06:21:45.568639 140332132726592 pytorch_submission_base.py:86] 35500) loss = 1.360, grad_norm = 0.642
I0315 06:28:20.808282 140303512098560 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.784731, loss=1.331597
I0315 06:28:20.813499 140332132726592 pytorch_submission_base.py:86] 36000) loss = 1.332, grad_norm = 0.785
I0315 06:32:41.270344 140332132726592 spec.py:298] Evaluating on the training split.
I0315 06:32:52.813695 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 06:33:02.950544 140332132726592 spec.py:326] Evaluating on the test split.
I0315 06:33:08.463119 140332132726592 submission_runner.py:362] Time since start: 29149.90s, 	Step: 36328, 	{'train/ctc_loss': 0.259168677544829, 'train/wer': 0.09120994212663039, 'validation/ctc_loss': 0.46603619815212477, 'validation/wer': 0.13917829382513397, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2724642769291567, 'test/wer': 0.09042715251965146, 'test/num_examples': 2472}
I0315 06:33:08.481694 140303520491264 logging_writer.py:48] [36328] global_step=36328, preemption_count=0, score=28752.118845, test/ctc_loss=0.272464, test/num_examples=2472, test/wer=0.090427, total_duration=29149.903843, train/ctc_loss=0.259169, train/wer=0.091210, validation/ctc_loss=0.466036, validation/num_examples=5348, validation/wer=0.139178
I0315 06:33:09.041577 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_36328.
I0315 06:35:25.779996 140303512098560 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.705392, loss=1.292420
I0315 06:35:25.784217 140332132726592 pytorch_submission_base.py:86] 36500) loss = 1.292, grad_norm = 0.705
I0315 06:42:01.086132 140303520491264 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.777479, loss=1.289142
I0315 06:42:01.090347 140332132726592 pytorch_submission_base.py:86] 37000) loss = 1.289, grad_norm = 0.777
I0315 06:48:38.241222 140303520491264 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.752788, loss=1.319131
I0315 06:48:38.249449 140332132726592 pytorch_submission_base.py:86] 37500) loss = 1.319, grad_norm = 0.753
I0315 06:55:13.363005 140303512098560 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.634530, loss=1.346906
I0315 06:55:13.367216 140332132726592 pytorch_submission_base.py:86] 38000) loss = 1.347, grad_norm = 0.635
I0315 07:01:50.597036 140303520491264 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.861114, loss=1.232054
I0315 07:01:50.605136 140332132726592 pytorch_submission_base.py:86] 38500) loss = 1.232, grad_norm = 0.861
I0315 07:08:25.621631 140303512098560 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.633602, loss=1.249867
I0315 07:08:25.627333 140332132726592 pytorch_submission_base.py:86] 39000) loss = 1.250, grad_norm = 0.634
I0315 07:13:09.073197 140332132726592 spec.py:298] Evaluating on the training split.
I0315 07:13:20.897845 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 07:13:30.949168 140332132726592 spec.py:326] Evaluating on the test split.
I0315 07:13:36.938544 140332132726592 submission_runner.py:362] Time since start: 31577.71s, 	Step: 39357, 	{'train/ctc_loss': 0.2464056038354632, 'train/wer': 0.08791137600414616, 'validation/ctc_loss': 0.45511962449141047, 'validation/wer': 0.1337131270216772, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2604468937512098, 'test/wer': 0.08516645339508054, 'test/num_examples': 2472}
I0315 07:13:36.957016 140303520491264 logging_writer.py:48] [39357] global_step=39357, preemption_count=0, score=31147.169887, test/ctc_loss=0.260447, test/num_examples=2472, test/wer=0.085166, total_duration=31577.707146, train/ctc_loss=0.246406, train/wer=0.087911, validation/ctc_loss=0.455120, validation/num_examples=5348, validation/wer=0.133713
I0315 07:13:37.524614 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_39357.
I0315 07:15:31.427822 140303512098560 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.721924, loss=1.266665
I0315 07:15:31.432268 140332132726592 pytorch_submission_base.py:86] 39500) loss = 1.267, grad_norm = 0.722
I0315 07:22:06.689519 140303520491264 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.629500, loss=1.273242
I0315 07:22:06.694632 140332132726592 pytorch_submission_base.py:86] 40000) loss = 1.273, grad_norm = 0.630
I0315 07:28:44.064431 140303520491264 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.583779, loss=1.276940
I0315 07:28:44.070719 140332132726592 pytorch_submission_base.py:86] 40500) loss = 1.277, grad_norm = 0.584
I0315 07:35:19.215658 140303512098560 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.619094, loss=1.262085
I0315 07:35:19.222136 140332132726592 pytorch_submission_base.py:86] 41000) loss = 1.262, grad_norm = 0.619
I0315 07:41:56.691666 140303520491264 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.711200, loss=1.293719
I0315 07:41:56.697605 140332132726592 pytorch_submission_base.py:86] 41500) loss = 1.294, grad_norm = 0.711
I0315 07:48:32.065989 140303512098560 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.657897, loss=1.314630
I0315 07:48:32.070484 140332132726592 pytorch_submission_base.py:86] 42000) loss = 1.315, grad_norm = 0.658
I0315 07:53:37.933146 140332132726592 spec.py:298] Evaluating on the training split.
I0315 07:53:49.369864 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 07:53:59.359269 140332132726592 spec.py:326] Evaluating on the test split.
I0315 07:54:05.110676 140332132726592 submission_runner.py:362] Time since start: 34006.57s, 	Step: 42385, 	{'train/ctc_loss': 0.23516705471848412, 'train/wer': 0.08371123779908439, 'validation/ctc_loss': 0.4406044170371208, 'validation/wer': 0.13002462221793076, 'validation/num_examples': 5348, 'test/ctc_loss': 0.25815917055132587, 'test/wer': 0.08465866390429183, 'test/num_examples': 2472}
I0315 07:54:05.128222 140303520491264 logging_writer.py:48] [42385] global_step=42385, preemption_count=0, score=33542.576315, test/ctc_loss=0.258159, test/num_examples=2472, test/wer=0.084659, total_duration=34006.567007, train/ctc_loss=0.235167, train/wer=0.083711, validation/ctc_loss=0.440604, validation/num_examples=5348, validation/wer=0.130025
I0315 07:54:05.688591 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_42385.
I0315 07:55:37.465779 140303512098560 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.616285, loss=1.234946
I0315 07:55:37.472656 140332132726592 pytorch_submission_base.py:86] 42500) loss = 1.235, grad_norm = 0.616
I0315 08:02:12.480766 140303520491264 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.669923, loss=1.207807
I0315 08:02:12.486853 140332132726592 pytorch_submission_base.py:86] 43000) loss = 1.208, grad_norm = 0.670
I0315 08:08:49.913044 140303520491264 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.637893, loss=1.209013
I0315 08:08:49.920233 140332132726592 pytorch_submission_base.py:86] 43500) loss = 1.209, grad_norm = 0.638
I0315 08:15:25.223772 140303512098560 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.633819, loss=1.248842
I0315 08:15:25.229272 140332132726592 pytorch_submission_base.py:86] 44000) loss = 1.249, grad_norm = 0.634
I0315 08:22:02.587534 140303520491264 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.637074, loss=1.172667
I0315 08:22:02.594403 140332132726592 pytorch_submission_base.py:86] 44500) loss = 1.173, grad_norm = 0.637
I0315 08:28:38.033076 140303512098560 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.799511, loss=1.218722
I0315 08:28:38.038311 140332132726592 pytorch_submission_base.py:86] 45000) loss = 1.219, grad_norm = 0.800
I0315 08:34:06.465487 140332132726592 spec.py:298] Evaluating on the training split.
I0315 08:34:18.089096 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 08:34:28.120718 140332132726592 spec.py:326] Evaluating on the test split.
I0315 08:34:34.004385 140332132726592 submission_runner.py:362] Time since start: 36435.10s, 	Step: 45414, 	{'train/ctc_loss': 0.22376929489678316, 'train/wer': 0.0797540381791483, 'validation/ctc_loss': 0.432230398504998, 'validation/wer': 0.12696374257712548, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2435015968772179, 'test/wer': 0.08006824690756201, 'test/num_examples': 2472}
I0315 08:34:34.022093 140303520491264 logging_writer.py:48] [45414] global_step=45414, preemption_count=0, score=35938.414461, test/ctc_loss=0.243502, test/num_examples=2472, test/wer=0.080068, total_duration=36435.099213, train/ctc_loss=0.223769, train/wer=0.079754, validation/ctc_loss=0.432230, validation/num_examples=5348, validation/wer=0.126964
I0315 08:34:34.583586 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_45414.
I0315 08:35:43.416978 140303512098560 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.670162, loss=1.305866
I0315 08:35:43.422218 140332132726592 pytorch_submission_base.py:86] 45500) loss = 1.306, grad_norm = 0.670
I0315 08:42:18.911454 140303520491264 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.646461, loss=1.216366
I0315 08:42:18.916452 140332132726592 pytorch_submission_base.py:86] 46000) loss = 1.216, grad_norm = 0.646
I0315 08:48:55.936563 140303520491264 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.779735, loss=1.250790
I0315 08:48:55.945489 140332132726592 pytorch_submission_base.py:86] 46500) loss = 1.251, grad_norm = 0.780
I0315 08:55:31.345165 140303512098560 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.650063, loss=1.267105
I0315 08:55:31.350099 140332132726592 pytorch_submission_base.py:86] 47000) loss = 1.267, grad_norm = 0.650
I0315 09:02:08.457945 140303520491264 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.773523, loss=1.190868
I0315 09:02:08.464373 140332132726592 pytorch_submission_base.py:86] 47500) loss = 1.191, grad_norm = 0.774
I0315 09:08:44.050404 140303512098560 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.684681, loss=1.235025
I0315 09:08:44.086244 140332132726592 pytorch_submission_base.py:86] 48000) loss = 1.235, grad_norm = 0.685
I0315 09:14:34.956733 140332132726592 spec.py:298] Evaluating on the training split.
I0315 09:14:46.746574 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 09:14:56.896240 140332132726592 spec.py:326] Evaluating on the test split.
I0315 09:15:03.749240 140332132726592 submission_runner.py:362] Time since start: 38863.59s, 	Step: 48443, 	{'train/ctc_loss': 0.21353971581471815, 'train/wer': 0.07688736287466529, 'validation/ctc_loss': 0.4212762496546614, 'validation/wer': 0.12289866267561435, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2417548359974837, 'test/wer': 0.07949952267787866, 'test/num_examples': 2472}
I0315 09:15:03.768774 140303520491264 logging_writer.py:48] [48443] global_step=48443, preemption_count=0, score=38333.856361, test/ctc_loss=0.241755, test/num_examples=2472, test/wer=0.079500, total_duration=38863.590665, train/ctc_loss=0.213540, train/wer=0.076887, validation/ctc_loss=0.421276, validation/num_examples=5348, validation/wer=0.122899
I0315 09:15:04.327724 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_48443.
I0315 09:15:50.148231 140303512098560 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.718477, loss=1.250169
I0315 09:15:50.151437 140332132726592 pytorch_submission_base.py:86] 48500) loss = 1.250, grad_norm = 0.718
I0315 09:22:25.544928 140303520491264 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.692721, loss=1.292685
I0315 09:22:25.569710 140332132726592 pytorch_submission_base.py:86] 49000) loss = 1.293, grad_norm = 0.693
I0315 09:29:02.837085 140303520491264 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.733625, loss=1.229123
I0315 09:29:02.844150 140332132726592 pytorch_submission_base.py:86] 49500) loss = 1.229, grad_norm = 0.734
I0315 09:35:37.866830 140303512098560 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.627239, loss=1.235016
I0315 09:35:37.902596 140332132726592 pytorch_submission_base.py:86] 50000) loss = 1.235, grad_norm = 0.627
I0315 09:42:15.133540 140303520491264 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.665348, loss=1.142380
I0315 09:42:15.140714 140332132726592 pytorch_submission_base.py:86] 50500) loss = 1.142, grad_norm = 0.665
I0315 09:48:50.579161 140303512098560 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.740781, loss=1.180810
I0315 09:48:50.585483 140332132726592 pytorch_submission_base.py:86] 51000) loss = 1.181, grad_norm = 0.741
I0315 09:55:04.634201 140332132726592 spec.py:298] Evaluating on the training split.
I0315 09:55:16.232240 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 09:55:26.260190 140332132726592 spec.py:326] Evaluating on the test split.
I0315 09:55:32.082729 140332132726592 submission_runner.py:362] Time since start: 41293.27s, 	Step: 51474, 	{'train/ctc_loss': 0.2000627759440358, 'train/wer': 0.07165608534162564, 'validation/ctc_loss': 0.4071989942799377, 'validation/wer': 0.12028194853473664, 'validation/num_examples': 5348, 'test/ctc_loss': 0.23151692750338732, 'test/wer': 0.07618873519793634, 'test/num_examples': 2472}
I0315 09:55:32.106583 140303520491264 logging_writer.py:48] [51474] global_step=51474, preemption_count=0, score=40729.151984, test/ctc_loss=0.231517, test/num_examples=2472, test/wer=0.076189, total_duration=41293.267985, train/ctc_loss=0.200063, train/wer=0.071656, validation/ctc_loss=0.407199, validation/num_examples=5348, validation/wer=0.120282
I0315 09:55:32.665399 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_51474.
I0315 09:55:55.962420 140303520491264 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.731196, loss=1.225452
I0315 09:55:55.971662 140332132726592 pytorch_submission_base.py:86] 51500) loss = 1.225, grad_norm = 0.731
I0315 10:02:31.473904 140303512098560 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.827633, loss=1.168643
I0315 10:02:31.482431 140332132726592 pytorch_submission_base.py:86] 52000) loss = 1.169, grad_norm = 0.828
I0315 10:09:06.810516 140303520491264 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.676153, loss=1.177333
I0315 10:09:06.828340 140332132726592 pytorch_submission_base.py:86] 52500) loss = 1.177, grad_norm = 0.676
I0315 10:15:44.123518 140303520491264 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.689471, loss=1.209153
I0315 10:15:44.129982 140332132726592 pytorch_submission_base.py:86] 53000) loss = 1.209, grad_norm = 0.689
I0315 10:22:19.460321 140303512098560 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.731700, loss=1.216796
I0315 10:22:19.465371 140332132726592 pytorch_submission_base.py:86] 53500) loss = 1.217, grad_norm = 0.732
I0315 10:28:56.654205 140303520491264 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.715389, loss=1.167240
I0315 10:28:56.661158 140332132726592 pytorch_submission_base.py:86] 54000) loss = 1.167, grad_norm = 0.715
I0315 10:35:31.630602 140303512098560 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.741756, loss=1.176516
I0315 10:35:31.635919 140332132726592 pytorch_submission_base.py:86] 54500) loss = 1.177, grad_norm = 0.742
I0315 10:35:33.216012 140332132726592 spec.py:298] Evaluating on the training split.
I0315 10:35:44.865226 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 10:35:55.069562 140332132726592 spec.py:326] Evaluating on the test split.
I0315 10:36:00.718448 140332132726592 submission_runner.py:362] Time since start: 43721.85s, 	Step: 54503, 	{'train/ctc_loss': 0.1923778801989763, 'train/wer': 0.07023084564222165, 'validation/ctc_loss': 0.40525852515320476, 'validation/wer': 0.11803215371988607, 'validation/num_examples': 5348, 'test/ctc_loss': 0.22675457388379897, 'test/wer': 0.07494972884041191, 'test/num_examples': 2472}
I0315 10:36:00.736865 140303520491264 logging_writer.py:48] [54503] global_step=54503, preemption_count=0, score=43124.698450, test/ctc_loss=0.226755, test/num_examples=2472, test/wer=0.074950, total_duration=43721.849460, train/ctc_loss=0.192378, train/wer=0.070231, validation/ctc_loss=0.405259, validation/num_examples=5348, validation/wer=0.118032
I0315 10:36:01.292160 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_54503.
I0315 10:42:37.115973 140303520491264 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.730412, loss=1.212149
I0315 10:42:37.123930 140332132726592 pytorch_submission_base.py:86] 55000) loss = 1.212, grad_norm = 0.730
I0315 10:49:12.509136 140303512098560 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.727435, loss=1.185725
I0315 10:49:12.513773 140332132726592 pytorch_submission_base.py:86] 55500) loss = 1.186, grad_norm = 0.727
I0315 10:55:49.639727 140303520491264 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.665977, loss=1.117258
I0315 10:55:49.646252 140332132726592 pytorch_submission_base.py:86] 56000) loss = 1.117, grad_norm = 0.666
I0315 11:02:25.097062 140303512098560 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.642681, loss=1.216948
I0315 11:02:25.101845 140332132726592 pytorch_submission_base.py:86] 56500) loss = 1.217, grad_norm = 0.643
I0315 11:09:02.385324 140303520491264 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.784619, loss=1.166509
I0315 11:09:02.391701 140332132726592 pytorch_submission_base.py:86] 57000) loss = 1.167, grad_norm = 0.785
I0315 11:15:37.593664 140303512098560 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.768812, loss=1.183370
I0315 11:15:37.598063 140332132726592 pytorch_submission_base.py:86] 57500) loss = 1.183, grad_norm = 0.769
I0315 11:16:01.321063 140332132726592 spec.py:298] Evaluating on the training split.
I0315 11:16:13.020181 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 11:16:23.247141 140332132726592 spec.py:326] Evaluating on the test split.
I0315 11:16:28.764372 140332132726592 submission_runner.py:362] Time since start: 46149.95s, 	Step: 57531, 	{'train/ctc_loss': 0.17790934258584454, 'train/wer': 0.06448129912758055, 'validation/ctc_loss': 0.38232196227647175, 'validation/wer': 0.111485540481823, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2163039248499903, 'test/wer': 0.07092803607336542, 'test/num_examples': 2472}
I0315 11:16:28.783181 140303520491264 logging_writer.py:48] [57531] global_step=57531, preemption_count=0, score=45519.691215, test/ctc_loss=0.216304, test/num_examples=2472, test/wer=0.070928, total_duration=46149.954609, train/ctc_loss=0.177909, train/wer=0.064481, validation/ctc_loss=0.382322, validation/num_examples=5348, validation/wer=0.111486
I0315 11:16:29.342442 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_57531.
I0315 11:22:42.914186 140303520491264 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.680315, loss=1.106867
I0315 11:22:42.921939 140332132726592 pytorch_submission_base.py:86] 58000) loss = 1.107, grad_norm = 0.680
I0315 11:29:18.213806 140303512098560 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.687226, loss=1.139350
I0315 11:29:18.218751 140332132726592 pytorch_submission_base.py:86] 58500) loss = 1.139, grad_norm = 0.687
I0315 11:35:55.235149 140303520491264 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.735512, loss=1.125686
I0315 11:35:55.242175 140332132726592 pytorch_submission_base.py:86] 59000) loss = 1.126, grad_norm = 0.736
I0315 11:42:30.582661 140303512098560 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.787884, loss=1.129527
I0315 11:42:30.588867 140332132726592 pytorch_submission_base.py:86] 59500) loss = 1.130, grad_norm = 0.788
I0315 11:49:08.332028 140303520491264 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.757803, loss=1.135974
I0315 11:49:08.338016 140332132726592 pytorch_submission_base.py:86] 60000) loss = 1.136, grad_norm = 0.758
I0315 11:55:43.535275 140303512098560 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.729444, loss=1.117859
I0315 11:55:43.541404 140332132726592 pytorch_submission_base.py:86] 60500) loss = 1.118, grad_norm = 0.729
I0315 11:56:29.392468 140332132726592 spec.py:298] Evaluating on the training split.
I0315 11:56:41.222933 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 11:56:51.353175 140332132726592 spec.py:326] Evaluating on the test split.
I0315 11:56:56.945961 140332132726592 submission_runner.py:362] Time since start: 48578.03s, 	Step: 60559, 	{'train/ctc_loss': 0.16836283092098572, 'train/wer': 0.06198173101839855, 'validation/ctc_loss': 0.3733193932323061, 'validation/wer': 0.10896538405832086, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2093021374362862, 'test/wer': 0.0672719517396868, 'test/num_examples': 2472}
I0315 11:56:56.963515 140303520491264 logging_writer.py:48] [60559] global_step=60559, preemption_count=0, score=47914.748745, test/ctc_loss=0.209302, test/num_examples=2472, test/wer=0.067272, total_duration=48578.026089, train/ctc_loss=0.168363, train/wer=0.061982, validation/ctc_loss=0.373319, validation/num_examples=5348, validation/wer=0.108965
I0315 11:56:57.519637 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_60559.
I0315 12:02:48.763350 140303520491264 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.776835, loss=1.155229
I0315 12:02:48.771228 140332132726592 pytorch_submission_base.py:86] 61000) loss = 1.155, grad_norm = 0.777
I0315 12:09:24.149137 140303512098560 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.003161, loss=1.114351
I0315 12:09:24.154220 140332132726592 pytorch_submission_base.py:86] 61500) loss = 1.114, grad_norm = 1.003
I0315 12:16:01.186218 140303520491264 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.704597, loss=1.135847
I0315 12:16:01.192214 140332132726592 pytorch_submission_base.py:86] 62000) loss = 1.136, grad_norm = 0.705
I0315 12:22:36.712050 140303512098560 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.809509, loss=1.126451
I0315 12:22:36.718107 140332132726592 pytorch_submission_base.py:86] 62500) loss = 1.126, grad_norm = 0.810
I0315 12:29:13.807003 140303520491264 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.832411, loss=1.092517
I0315 12:29:13.813454 140332132726592 pytorch_submission_base.py:86] 63000) loss = 1.093, grad_norm = 0.832
I0315 12:35:49.011005 140303512098560 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.705807, loss=1.121483
I0315 12:35:49.016307 140332132726592 pytorch_submission_base.py:86] 63500) loss = 1.121, grad_norm = 0.706
I0315 12:36:57.815169 140332132726592 spec.py:298] Evaluating on the training split.
I0315 12:37:09.375872 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 12:37:19.363949 140332132726592 spec.py:326] Evaluating on the test split.
I0315 12:37:25.343529 140332132726592 submission_runner.py:362] Time since start: 51006.45s, 	Step: 63588, 	{'train/ctc_loss': 0.15577880304904917, 'train/wer': 0.05688002073075926, 'validation/ctc_loss': 0.3649618567676939, 'validation/wer': 0.10646453917829382, 'validation/num_examples': 5348, 'test/ctc_loss': 0.20332710477288857, 'test/wer': 0.06682509698779274, 'test/num_examples': 2472}
I0315 12:37:25.361187 140303520491264 logging_writer.py:48] [63588] global_step=63588, preemption_count=0, score=50310.036663, test/ctc_loss=0.203327, test/num_examples=2472, test/wer=0.066825, total_duration=51006.449259, train/ctc_loss=0.155779, train/wer=0.056880, validation/ctc_loss=0.364962, validation/num_examples=5348, validation/wer=0.106465
I0315 12:37:25.920508 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_63588.
I0315 12:42:54.356815 140303520491264 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.927371, loss=1.064078
I0315 12:42:54.366254 140332132726592 pytorch_submission_base.py:86] 64000) loss = 1.064, grad_norm = 0.927
I0315 12:49:29.749683 140303512098560 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.819239, loss=1.126274
I0315 12:49:29.755233 140332132726592 pytorch_submission_base.py:86] 64500) loss = 1.126, grad_norm = 0.819
I0315 12:56:06.617755 140303520491264 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.776061, loss=1.093799
I0315 12:56:06.624372 140332132726592 pytorch_submission_base.py:86] 65000) loss = 1.094, grad_norm = 0.776
I0315 13:02:42.005564 140303512098560 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.778197, loss=1.054632
I0315 13:02:42.038188 140332132726592 pytorch_submission_base.py:86] 65500) loss = 1.055, grad_norm = 0.778
I0315 13:09:19.104297 140303520491264 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.735177, loss=1.092558
I0315 13:09:19.111383 140332132726592 pytorch_submission_base.py:86] 66000) loss = 1.093, grad_norm = 0.735
I0315 13:15:54.467859 140303512098560 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.818355, loss=1.058116
I0315 13:15:54.501378 140332132726592 pytorch_submission_base.py:86] 66500) loss = 1.058, grad_norm = 0.818
I0315 13:17:26.178093 140332132726592 spec.py:298] Evaluating on the training split.
I0315 13:17:37.840667 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 13:17:48.239742 140332132726592 spec.py:326] Evaluating on the test split.
I0315 13:17:53.804728 140332132726592 submission_runner.py:362] Time since start: 53434.81s, 	Step: 66617, 	{'train/ctc_loss': 0.14485727178739705, 'train/wer': 0.0523235726008465, 'validation/ctc_loss': 0.3555767909336071, 'validation/wer': 0.1017138994834162, 'validation/num_examples': 5348, 'test/ctc_loss': 0.1926459595175495, 'test/wer': 0.06205187577437897, 'test/num_examples': 2472}
I0315 13:17:53.822064 140303520491264 logging_writer.py:48] [66617] global_step=66617, preemption_count=0, score=52705.268858, test/ctc_loss=0.192646, test/num_examples=2472, test/wer=0.062052, total_duration=53434.811527, train/ctc_loss=0.144857, train/wer=0.052324, validation/ctc_loss=0.355577, validation/num_examples=5348, validation/wer=0.101714
I0315 13:17:54.392928 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_66617.
I0315 13:22:59.912878 140303520491264 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.708749, loss=1.047255
I0315 13:22:59.922929 140332132726592 pytorch_submission_base.py:86] 67000) loss = 1.047, grad_norm = 0.709
I0315 13:29:35.236574 140303512098560 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.755579, loss=1.056800
I0315 13:29:35.262948 140332132726592 pytorch_submission_base.py:86] 67500) loss = 1.057, grad_norm = 0.756
I0315 13:36:12.838251 140303520491264 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.690199, loss=1.051663
I0315 13:36:12.845149 140332132726592 pytorch_submission_base.py:86] 68000) loss = 1.052, grad_norm = 0.690
I0315 13:42:47.945059 140303512098560 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.870118, loss=1.050294
I0315 13:42:47.952629 140332132726592 pytorch_submission_base.py:86] 68500) loss = 1.050, grad_norm = 0.870
I0315 13:49:23.379872 140303520491264 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.822571, loss=1.052843
I0315 13:49:23.415323 140332132726592 pytorch_submission_base.py:86] 69000) loss = 1.053, grad_norm = 0.823
I0315 13:56:00.218824 140303520491264 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.789829, loss=1.049187
I0315 13:56:00.226771 140332132726592 pytorch_submission_base.py:86] 69500) loss = 1.049, grad_norm = 0.790
I0315 13:57:54.852714 140332132726592 spec.py:298] Evaluating on the training split.
I0315 13:58:06.487827 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 13:58:16.512085 140332132726592 spec.py:326] Evaluating on the test split.
I0315 13:58:22.156643 140332132726592 submission_runner.py:362] Time since start: 55863.49s, 	Step: 69646, 	{'train/ctc_loss': 0.13499039227483814, 'train/wer': 0.04898181739656215, 'validation/ctc_loss': 0.35488113486098555, 'validation/wer': 0.10086419157051127, 'validation/num_examples': 5348, 'test/ctc_loss': 0.1912254671672366, 'test/wer': 0.06296589685779863, 'test/num_examples': 2472}
I0315 13:58:22.172472 140303520491264 logging_writer.py:48] [69646] global_step=69646, preemption_count=0, score=55100.802099, test/ctc_loss=0.191225, test/num_examples=2472, test/wer=0.062966, total_duration=55863.486613, train/ctc_loss=0.134990, train/wer=0.048982, validation/ctc_loss=0.354881, validation/num_examples=5348, validation/wer=0.100864
I0315 13:58:22.727068 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_69646.
I0315 14:03:03.500925 140303512098560 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.918038, loss=1.131252
I0315 14:03:03.505212 140332132726592 pytorch_submission_base.py:86] 70000) loss = 1.131, grad_norm = 0.918
I0315 14:09:40.548230 140303520491264 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.847117, loss=1.132216
I0315 14:09:40.556861 140332132726592 pytorch_submission_base.py:86] 70500) loss = 1.132, grad_norm = 0.847
I0315 14:16:15.953083 140303512098560 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.881442, loss=1.072268
I0315 14:16:15.957430 140332132726592 pytorch_submission_base.py:86] 71000) loss = 1.072, grad_norm = 0.881
I0315 14:22:53.736804 140303520491264 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.886339, loss=1.067686
I0315 14:22:53.744517 140332132726592 pytorch_submission_base.py:86] 71500) loss = 1.068, grad_norm = 0.886
I0315 14:29:29.575023 140303512098560 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.822487, loss=1.021212
I0315 14:29:29.580685 140332132726592 pytorch_submission_base.py:86] 72000) loss = 1.021, grad_norm = 0.822
I0315 14:36:06.664750 140303520491264 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.826728, loss=0.965198
I0315 14:36:06.671411 140332132726592 pytorch_submission_base.py:86] 72500) loss = 0.965, grad_norm = 0.827
I0315 14:38:23.395308 140332132726592 spec.py:298] Evaluating on the training split.
I0315 14:38:34.983741 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 14:38:45.183950 140332132726592 spec.py:326] Evaluating on the test split.
I0315 14:38:50.843994 140332132726592 submission_runner.py:362] Time since start: 58292.03s, 	Step: 72674, 	{'train/ctc_loss': 0.12308892865919799, 'train/wer': 0.045337738619676944, 'validation/ctc_loss': 0.3356099924888864, 'validation/wer': 0.09717568676676483, 'validation/num_examples': 5348, 'test/ctc_loss': 0.1831917835424866, 'test/wer': 0.059289500944488455, 'test/num_examples': 2472}
I0315 14:38:50.858665 140303520491264 logging_writer.py:48] [72674] global_step=72674, preemption_count=0, score=57496.458150, test/ctc_loss=0.183192, test/num_examples=2472, test/wer=0.059290, total_duration=58292.028710, train/ctc_loss=0.123089, train/wer=0.045338, validation/ctc_loss=0.335610, validation/num_examples=5348, validation/wer=0.097176
I0315 14:38:51.629893 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_72674.
I0315 14:43:10.322307 140303512098560 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.818861, loss=0.995286
I0315 14:43:10.327993 140332132726592 pytorch_submission_base.py:86] 73000) loss = 0.995, grad_norm = 0.819
I0315 14:49:47.720492 140303520491264 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.912936, loss=1.012389
I0315 14:49:47.727511 140332132726592 pytorch_submission_base.py:86] 73500) loss = 1.012, grad_norm = 0.913
I0315 14:56:23.169814 140303512098560 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.869681, loss=1.021315
I0315 14:56:23.174294 140332132726592 pytorch_submission_base.py:86] 74000) loss = 1.021, grad_norm = 0.870
I0315 15:03:00.412903 140303520491264 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.894053, loss=0.995395
I0315 15:03:00.419205 140332132726592 pytorch_submission_base.py:86] 74500) loss = 0.995, grad_norm = 0.894
I0315 15:09:36.072859 140303512098560 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.013278, loss=1.025469
I0315 15:09:36.078304 140332132726592 pytorch_submission_base.py:86] 75000) loss = 1.025, grad_norm = 1.013
I0315 15:16:13.446722 140303520491264 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.159920, loss=0.958552
I0315 15:16:13.453711 140332132726592 pytorch_submission_base.py:86] 75500) loss = 0.959, grad_norm = 1.160
I0315 15:18:52.299388 140332132726592 spec.py:298] Evaluating on the training split.
I0315 15:19:03.933871 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 15:19:13.968564 140332132726592 spec.py:326] Evaluating on the test split.
I0315 15:19:19.582017 140332132726592 submission_runner.py:362] Time since start: 60720.93s, 	Step: 75702, 	{'train/ctc_loss': 0.11245797282295515, 'train/wer': 0.04174764619504189, 'validation/ctc_loss': 0.3277817707548473, 'validation/wer': 0.09424998793028533, 'validation/num_examples': 5348, 'test/ctc_loss': 0.17594132837199175, 'test/wer': 0.05750208193691223, 'test/num_examples': 2472}
I0315 15:19:19.599321 140303520491264 logging_writer.py:48] [75702] global_step=75702, preemption_count=0, score=59892.195049, test/ctc_loss=0.175941, test/num_examples=2472, test/wer=0.057502, total_duration=60720.932986, train/ctc_loss=0.112458, train/wer=0.041748, validation/ctc_loss=0.327782, validation/num_examples=5348, validation/wer=0.094250
I0315 15:19:20.158238 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_75702.
I0315 15:23:16.675610 140303512098560 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.900085, loss=1.011315
I0315 15:23:16.681617 140332132726592 pytorch_submission_base.py:86] 76000) loss = 1.011, grad_norm = 0.900
I0315 15:29:53.749827 140303520491264 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.929547, loss=1.005595
I0315 15:29:53.757140 140332132726592 pytorch_submission_base.py:86] 76500) loss = 1.006, grad_norm = 0.930
I0315 15:36:29.122456 140303512098560 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.933406, loss=1.013261
I0315 15:36:29.128651 140332132726592 pytorch_submission_base.py:86] 77000) loss = 1.013, grad_norm = 0.933
I0315 15:43:06.146088 140303520491264 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.867570, loss=0.945424
I0315 15:43:06.152525 140332132726592 pytorch_submission_base.py:86] 77500) loss = 0.945, grad_norm = 0.868
I0315 15:49:41.445988 140303512098560 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.901775, loss=0.998590
I0315 15:49:41.454354 140332132726592 pytorch_submission_base.py:86] 78000) loss = 0.999, grad_norm = 0.902
I0315 15:56:19.000082 140303520491264 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.960587, loss=0.977483
I0315 15:56:19.006696 140332132726592 pytorch_submission_base.py:86] 78500) loss = 0.977, grad_norm = 0.961
I0315 15:59:20.932134 140332132726592 spec.py:298] Evaluating on the training split.
I0315 15:59:32.703652 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 15:59:42.648494 140332132726592 spec.py:326] Evaluating on the test split.
I0315 15:59:48.236475 140332132726592 submission_runner.py:362] Time since start: 63149.57s, 	Step: 78731, 	{'train/ctc_loss': 0.10111861939002068, 'train/wer': 0.037812041116005876, 'validation/ctc_loss': 0.3151143207975437, 'validation/wer': 0.09110220634384203, 'validation/num_examples': 5348, 'test/ctc_loss': 0.17120107799293502, 'test/wer': 0.05469908394775862, 'test/num_examples': 2472}
I0315 15:59:48.255208 140303520491264 logging_writer.py:48] [78731] global_step=78731, preemption_count=0, score=62288.064406, test/ctc_loss=0.171201, test/num_examples=2472, test/wer=0.054699, total_duration=63149.566210, train/ctc_loss=0.101119, train/wer=0.037812, validation/ctc_loss=0.315114, validation/num_examples=5348, validation/wer=0.091102
I0315 15:59:48.813652 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_78731.
I0315 16:03:22.343063 140303512098560 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.015752, loss=0.995678
I0315 16:03:22.346282 140332132726592 pytorch_submission_base.py:86] 79000) loss = 0.996, grad_norm = 1.016
I0315 16:09:59.989495 140303520491264 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.978357, loss=0.981452
I0315 16:09:59.996702 140332132726592 pytorch_submission_base.py:86] 79500) loss = 0.981, grad_norm = 0.978
I0315 16:16:35.427965 140303512098560 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.028885, loss=0.986850
I0315 16:16:35.432672 140332132726592 pytorch_submission_base.py:86] 80000) loss = 0.987, grad_norm = 1.029
I0315 16:23:12.497233 140303520491264 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.842832, loss=0.978714
I0315 16:23:12.504476 140332132726592 pytorch_submission_base.py:86] 80500) loss = 0.979, grad_norm = 0.843
I0315 16:29:48.218209 140303512098560 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.881463, loss=0.970348
I0315 16:29:48.222610 140332132726592 pytorch_submission_base.py:86] 81000) loss = 0.970, grad_norm = 0.881
I0315 16:36:25.986540 140303520491264 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.052372, loss=0.931129
I0315 16:36:25.993727 140332132726592 pytorch_submission_base.py:86] 81500) loss = 0.931, grad_norm = 1.052
I0315 16:39:49.367149 140332132726592 spec.py:298] Evaluating on the training split.
I0315 16:40:01.172712 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 16:40:11.186890 140332132726592 spec.py:326] Evaluating on the test split.
I0315 16:40:17.081207 140332132726592 submission_runner.py:362] Time since start: 65578.00s, 	Step: 81758, 	{'train/ctc_loss': 0.09163077893545982, 'train/wer': 0.034243543232270884, 'validation/ctc_loss': 0.3088396810876909, 'validation/wer': 0.08715299570318158, 'validation/num_examples': 5348, 'test/ctc_loss': 0.1638395269452868, 'test/wer': 0.053033534417971685, 'test/num_examples': 2472}
I0315 16:40:17.099339 140303520491264 logging_writer.py:48] [81758] global_step=81758, preemption_count=0, score=64683.634120, test/ctc_loss=0.163840, test/num_examples=2472, test/wer=0.053034, total_duration=65578.001233, train/ctc_loss=0.091631, train/wer=0.034244, validation/ctc_loss=0.308840, validation/num_examples=5348, validation/wer=0.087153
I0315 16:40:17.658196 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_81758.
I0315 16:43:29.858310 140303512098560 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.959173, loss=0.980191
I0315 16:43:29.862055 140332132726592 pytorch_submission_base.py:86] 82000) loss = 0.980, grad_norm = 0.959
I0315 16:50:07.070059 140303520491264 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.990632, loss=0.942936
I0315 16:50:07.078097 140332132726592 pytorch_submission_base.py:86] 82500) loss = 0.943, grad_norm = 0.991
I0315 16:56:42.317108 140303512098560 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.319491, loss=0.913064
I0315 16:56:42.347362 140332132726592 pytorch_submission_base.py:86] 83000) loss = 0.913, grad_norm = 1.319
I0315 17:03:19.860223 140303520491264 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.078191, loss=0.955168
I0315 17:03:19.868042 140332132726592 pytorch_submission_base.py:86] 83500) loss = 0.955, grad_norm = 1.078
I0315 17:09:55.152720 140303512098560 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.979222, loss=0.876689
I0315 17:09:55.187039 140332132726592 pytorch_submission_base.py:86] 84000) loss = 0.877, grad_norm = 0.979
I0315 17:16:32.812560 140303520491264 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.235014, loss=0.921565
I0315 17:16:32.820984 140332132726592 pytorch_submission_base.py:86] 84500) loss = 0.922, grad_norm = 1.235
I0315 17:20:18.131719 140332132726592 spec.py:298] Evaluating on the training split.
I0315 17:20:29.899283 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 17:20:40.256762 140332132726592 spec.py:326] Evaluating on the test split.
I0315 17:20:45.817490 140332132726592 submission_runner.py:362] Time since start: 68006.77s, 	Step: 84786, 	{'train/ctc_loss': 0.08299717991551134, 'train/wer': 0.03145244882093807, 'validation/ctc_loss': 0.3017955593778255, 'validation/wer': 0.08471008545357987, 'validation/num_examples': 5348, 'test/ctc_loss': 0.16137671684463514, 'test/wer': 0.050880506977027604, 'test/num_examples': 2472}
I0315 17:20:45.834580 140303520491264 logging_writer.py:48] [84786] global_step=84786, preemption_count=0, score=67079.168122, test/ctc_loss=0.161377, test/num_examples=2472, test/wer=0.050881, total_duration=68006.765520, train/ctc_loss=0.082997, train/wer=0.031452, validation/ctc_loss=0.301796, validation/num_examples=5348, validation/wer=0.084710
I0315 17:20:46.403550 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_84786.
I0315 17:23:36.403064 140303512098560 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.182547, loss=0.860502
I0315 17:23:36.409153 140332132726592 pytorch_submission_base.py:86] 85000) loss = 0.861, grad_norm = 1.183
I0315 17:30:13.846854 140303520491264 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.090125, loss=0.880230
I0315 17:30:13.856534 140332132726592 pytorch_submission_base.py:86] 85500) loss = 0.880, grad_norm = 1.090
I0315 17:36:49.072043 140303512098560 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.101436, loss=0.902681
I0315 17:36:49.080034 140332132726592 pytorch_submission_base.py:86] 86000) loss = 0.903, grad_norm = 1.101
I0315 17:43:24.482129 140303520491264 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.072987, loss=0.989652
I0315 17:43:24.486536 140332132726592 pytorch_submission_base.py:86] 86500) loss = 0.990, grad_norm = 1.073
I0315 17:50:02.056068 140303520491264 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.907787, loss=0.919092
I0315 17:50:02.062486 140332132726592 pytorch_submission_base.py:86] 87000) loss = 0.919, grad_norm = 0.908
I0315 17:56:37.128216 140303512098560 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.964516, loss=0.940765
I0315 17:56:37.135027 140332132726592 pytorch_submission_base.py:86] 87500) loss = 0.941, grad_norm = 0.965
I0315 18:00:46.574277 140332132726592 spec.py:298] Evaluating on the training split.
I0315 18:00:58.343057 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 18:01:08.998399 140332132726592 spec.py:326] Evaluating on the test split.
I0315 18:01:15.136625 140332132726592 submission_runner.py:362] Time since start: 70435.21s, 	Step: 87814, 	{'train/ctc_loss': 0.07567596357962442, 'train/wer': 0.028170078604128877, 'validation/ctc_loss': 0.29189145557847346, 'validation/wer': 0.08223820788876551, 'validation/num_examples': 5348, 'test/ctc_loss': 0.15430257093199562, 'test/wer': 0.048321247943452564, 'test/num_examples': 2472}
I0315 18:01:15.158181 140303520491264 logging_writer.py:48] [87814] global_step=87814, preemption_count=0, score=69474.305493, test/ctc_loss=0.154303, test/num_examples=2472, test/wer=0.048321, total_duration=70435.207877, train/ctc_loss=0.075676, train/wer=0.028170, validation/ctc_loss=0.291891, validation/num_examples=5348, validation/wer=0.082238
I0315 18:01:15.727491 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_87814.
I0315 18:03:43.727945 140303512098560 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.446804, loss=0.911484
I0315 18:03:43.731737 140332132726592 pytorch_submission_base.py:86] 88000) loss = 0.911, grad_norm = 1.447
I0315 18:10:19.351031 140303520491264 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.219995, loss=0.903656
I0315 18:10:19.357051 140332132726592 pytorch_submission_base.py:86] 88500) loss = 0.904, grad_norm = 1.220
I0315 18:16:56.925427 140303520491264 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.086895, loss=0.921025
I0315 18:16:56.935379 140332132726592 pytorch_submission_base.py:86] 89000) loss = 0.921, grad_norm = 1.087
I0315 18:23:32.400553 140303512098560 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.004868, loss=0.915349
I0315 18:23:32.405448 140332132726592 pytorch_submission_base.py:86] 89500) loss = 0.915, grad_norm = 1.005
I0315 18:30:09.510473 140303520491264 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.186773, loss=0.914125
I0315 18:30:09.516320 140332132726592 pytorch_submission_base.py:86] 90000) loss = 0.914, grad_norm = 1.187
I0315 18:36:45.099066 140303512098560 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.032199, loss=0.905928
I0315 18:36:45.104201 140332132726592 pytorch_submission_base.py:86] 90500) loss = 0.906, grad_norm = 1.032
I0315 18:41:16.042673 140332132726592 spec.py:298] Evaluating on the training split.
I0315 18:41:27.472616 140332132726592 spec.py:310] Evaluating on the validation split.
I0315 18:41:39.029751 140332132726592 spec.py:326] Evaluating on the test split.
I0315 18:41:44.620370 140332132726592 submission_runner.py:362] Time since start: 72864.68s, 	Step: 90841, 	{'train/ctc_loss': 0.07059585903867666, 'train/wer': 0.026258961734473524, 'validation/ctc_loss': 0.28667242852276725, 'validation/wer': 0.07954424757398736, 'validation/num_examples': 5348, 'test/ctc_loss': 0.15167883815084846, 'test/wer': 0.04702130684703349, 'test/num_examples': 2472}
I0315 18:41:44.638064 140303520491264 logging_writer.py:48] [90841] global_step=90841, preemption_count=0, score=71869.615755, test/ctc_loss=0.151679, test/num_examples=2472, test/wer=0.047021, total_duration=72864.676430, train/ctc_loss=0.070596, train/wer=0.026259, validation/ctc_loss=0.286672, validation/num_examples=5348, validation/wer=0.079544
I0315 18:41:45.197843 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_90841.
I0315 18:43:51.737179 140303512098560 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.207466, loss=0.870549
I0315 18:43:51.742373 140332132726592 pytorch_submission_base.py:86] 91000) loss = 0.871, grad_norm = 1.207
I0315 18:43:56.497357 140303520491264 logging_writer.py:48] [91007] global_step=91007, preemption_count=0, score=72000.619937
I0315 18:43:57.112236 140332132726592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_pytorch/trial_1/checkpoint_91007.
I0315 18:43:57.230678 140332132726592 submission_runner.py:523] Tuning trial 1/1
I0315 18:43:57.230968 140332132726592 submission_runner.py:524] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, beta1=0.9731333693827139, beta2=0.9981232922116359, warmup_steps=9999, weight_decay=0.16375311233774334)
I0315 18:43:57.231839 140332132726592 submission_runner.py:525] Metrics: {'eval_results': [(1, {'train/ctc_loss': 30.96843496830616, 'train/wer': 1.047464800898333, 'validation/ctc_loss': 30.504307313642755, 'validation/wer': 0.961125862984599, 'validation/num_examples': 5348, 'test/ctc_loss': 30.473346667526936, 'test/wer': 0.973310584364146, 'test/num_examples': 2472, 'score': 7.202465057373047, 'total_duration': 7.204182386398315, 'global_step': 1, 'preemption_count': 0}), (3018, {'train/ctc_loss': 3.6094356970950137, 'train/wer': 0.7456756931847629, 'validation/ctc_loss': 3.8125525856439624, 'validation/wer': 0.7277458600878676, 'validation/num_examples': 5348, 'test/ctc_loss': 3.5379093006000386, 'test/wer': 0.7044665163609773, 'test/num_examples': 2472, 'score': 2402.652750492096, 'total_duration': 2436.424471616745, 'global_step': 3018, 'preemption_count': 0}), (6041, {'train/ctc_loss': 0.722153751144029, 'train/wer': 0.23592035933316058, 'validation/ctc_loss': 0.9383261636339663, 'validation/wer': 0.2765799256505576, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6581364120265824, 'test/wer': 0.21666361992972194, 'test/num_examples': 2472, 'score': 4798.132726430893, 'total_duration': 4864.483935832977, 'global_step': 6041, 'preemption_count': 0}), (9066, {'train/ctc_loss': 0.5241767228229551, 'train/wer': 0.17665414183294464, 'validation/ctc_loss': 0.7385148440639442, 'validation/wer': 0.22181238835513928, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4864874346732047, 'test/wer': 0.1630613612820669, 'test/num_examples': 2472, 'score': 7193.345130205154, 'total_duration': 7292.698787689209, 'global_step': 9066, 'preemption_count': 0}), (12093, {'train/ctc_loss': 0.44721111148774617, 'train/wer': 0.15147490714347414, 'validation/ctc_loss': 0.6628024557495731, 'validation/wer': 0.19776951672862453, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4195256044744822, 'test/wer': 0.14171389108931, 'test/num_examples': 2472, 'score': 9588.323010921478, 'total_duration': 9721.014414787292, 'global_step': 12093, 'preemption_count': 0}), (15120, {'train/ctc_loss': 0.39052497690756244, 'train/wer': 0.13473373931070226, 'validation/ctc_loss': 0.6001378803433293, 'validation/wer': 0.18154782020953025, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3759363003258275, 'test/wer': 0.12745516218796335, 'test/num_examples': 2472, 'score': 11983.654449939728, 'total_duration': 12149.564177274704, 'global_step': 15120, 'preemption_count': 0}), (18149, {'train/ctc_loss': 0.3519921199705095, 'train/wer': 0.12198756154444157, 'validation/ctc_loss': 0.5588411772277476, 'validation/wer': 0.16827113407039057, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3412302305793922, 'test/wer': 0.112688643795828, 'test/num_examples': 2472, 'score': 14379.147094249725, 'total_duration': 14577.948522567749, 'global_step': 18149, 'preemption_count': 0}), (21178, {'train/ctc_loss': 0.32471356903155824, 'train/wer': 0.11319858339811696, 'validation/ctc_loss': 0.5322827781231163, 'validation/wer': 0.1578332448220924, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3175757306923027, 'test/wer': 0.1087481973473077, 'test/num_examples': 2472, 'score': 16774.518630981445, 'total_duration': 17006.011899471283, 'global_step': 21178, 'preemption_count': 0}), (24208, {'train/ctc_loss': 0.31330180015084236, 'train/wer': 0.10926297831908094, 'validation/ctc_loss': 0.5262415607890044, 'validation/wer': 0.15551585960507894, 'validation/num_examples': 5348, 'test/ctc_loss': 0.31283797543389896, 'test/wer': 0.10399528771352548, 'test/num_examples': 2472, 'score': 19170.285362958908, 'total_duration': 19434.59464597702, 'global_step': 24208, 'preemption_count': 0}), (27238, {'train/ctc_loss': 0.29714575353377853, 'train/wer': 0.10386434309406582, 'validation/ctc_loss': 0.5061534523652552, 'validation/wer': 0.15011828320378506, 'validation/num_examples': 5348, 'test/ctc_loss': 0.29858735845861023, 'test/wer': 0.09995328336684745, 'test/num_examples': 2472, 'score': 21565.656718969345, 'total_duration': 21862.55429458618, 'global_step': 27238, 'preemption_count': 0}), (30268, {'train/ctc_loss': 0.28198832242974814, 'train/wer': 0.09915133454262762, 'validation/ctc_loss': 0.48798820276396426, 'validation/wer': 0.1444986240525274, 'validation/num_examples': 5348, 'test/ctc_loss': 0.28797787861313634, 'test/wer': 0.09524099689232832, 'test/num_examples': 2472, 'score': 23960.71862053871, 'total_duration': 24290.54950594902, 'global_step': 30268, 'preemption_count': 0}), (33299, {'train/ctc_loss': 0.27310318929866784, 'train/wer': 0.096074112464369, 'validation/ctc_loss': 0.47696223919090314, 'validation/wer': 0.14244194467242793, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2784739892493064, 'test/wer': 0.09282391891617411, 'test/num_examples': 2472, 'score': 26356.407925367355, 'total_duration': 26720.084228754044, 'global_step': 33299, 'preemption_count': 0}), (36328, {'train/ctc_loss': 0.259168677544829, 'train/wer': 0.09120994212663039, 'validation/ctc_loss': 0.46603619815212477, 'validation/wer': 0.13917829382513397, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2724642769291567, 'test/wer': 0.09042715251965146, 'test/num_examples': 2472, 'score': 28752.118844985962, 'total_duration': 29149.903843402863, 'global_step': 36328, 'preemption_count': 0}), (39357, {'train/ctc_loss': 0.2464056038354632, 'train/wer': 0.08791137600414616, 'validation/ctc_loss': 0.45511962449141047, 'validation/wer': 0.1337131270216772, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2604468937512098, 'test/wer': 0.08516645339508054, 'test/num_examples': 2472, 'score': 31147.16988658905, 'total_duration': 31577.707145929337, 'global_step': 39357, 'preemption_count': 0}), (42385, {'train/ctc_loss': 0.23516705471848412, 'train/wer': 0.08371123779908439, 'validation/ctc_loss': 0.4406044170371208, 'validation/wer': 0.13002462221793076, 'validation/num_examples': 5348, 'test/ctc_loss': 0.25815917055132587, 'test/wer': 0.08465866390429183, 'test/num_examples': 2472, 'score': 33542.57631468773, 'total_duration': 34006.56700706482, 'global_step': 42385, 'preemption_count': 0}), (45414, {'train/ctc_loss': 0.22376929489678316, 'train/wer': 0.0797540381791483, 'validation/ctc_loss': 0.432230398504998, 'validation/wer': 0.12696374257712548, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2435015968772179, 'test/wer': 0.08006824690756201, 'test/num_examples': 2472, 'score': 35938.41446137428, 'total_duration': 36435.099212646484, 'global_step': 45414, 'preemption_count': 0}), (48443, {'train/ctc_loss': 0.21353971581471815, 'train/wer': 0.07688736287466529, 'validation/ctc_loss': 0.4212762496546614, 'validation/wer': 0.12289866267561435, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2417548359974837, 'test/wer': 0.07949952267787866, 'test/num_examples': 2472, 'score': 38333.85636115074, 'total_duration': 38863.59066534042, 'global_step': 48443, 'preemption_count': 0}), (51474, {'train/ctc_loss': 0.2000627759440358, 'train/wer': 0.07165608534162564, 'validation/ctc_loss': 0.4071989942799377, 'validation/wer': 0.12028194853473664, 'validation/num_examples': 5348, 'test/ctc_loss': 0.23151692750338732, 'test/wer': 0.07618873519793634, 'test/num_examples': 2472, 'score': 40729.151983976364, 'total_duration': 41293.267984867096, 'global_step': 51474, 'preemption_count': 0}), (54503, {'train/ctc_loss': 0.1923778801989763, 'train/wer': 0.07023084564222165, 'validation/ctc_loss': 0.40525852515320476, 'validation/wer': 0.11803215371988607, 'validation/num_examples': 5348, 'test/ctc_loss': 0.22675457388379897, 'test/wer': 0.07494972884041191, 'test/num_examples': 2472, 'score': 43124.69844985008, 'total_duration': 43721.84946012497, 'global_step': 54503, 'preemption_count': 0}), (57531, {'train/ctc_loss': 0.17790934258584454, 'train/wer': 0.06448129912758055, 'validation/ctc_loss': 0.38232196227647175, 'validation/wer': 0.111485540481823, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2163039248499903, 'test/wer': 0.07092803607336542, 'test/num_examples': 2472, 'score': 45519.69121479988, 'total_duration': 46149.95460939407, 'global_step': 57531, 'preemption_count': 0}), (60559, {'train/ctc_loss': 0.16836283092098572, 'train/wer': 0.06198173101839855, 'validation/ctc_loss': 0.3733193932323061, 'validation/wer': 0.10896538405832086, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2093021374362862, 'test/wer': 0.0672719517396868, 'test/num_examples': 2472, 'score': 47914.74874544144, 'total_duration': 48578.0260887146, 'global_step': 60559, 'preemption_count': 0}), (63588, {'train/ctc_loss': 0.15577880304904917, 'train/wer': 0.05688002073075926, 'validation/ctc_loss': 0.3649618567676939, 'validation/wer': 0.10646453917829382, 'validation/num_examples': 5348, 'test/ctc_loss': 0.20332710477288857, 'test/wer': 0.06682509698779274, 'test/num_examples': 2472, 'score': 50310.036662817, 'total_duration': 51006.44925928116, 'global_step': 63588, 'preemption_count': 0}), (66617, {'train/ctc_loss': 0.14485727178739705, 'train/wer': 0.0523235726008465, 'validation/ctc_loss': 0.3555767909336071, 'validation/wer': 0.1017138994834162, 'validation/num_examples': 5348, 'test/ctc_loss': 0.1926459595175495, 'test/wer': 0.06205187577437897, 'test/num_examples': 2472, 'score': 52705.26885843277, 'total_duration': 53434.8115272522, 'global_step': 66617, 'preemption_count': 0}), (69646, {'train/ctc_loss': 0.13499039227483814, 'train/wer': 0.04898181739656215, 'validation/ctc_loss': 0.35488113486098555, 'validation/wer': 0.10086419157051127, 'validation/num_examples': 5348, 'test/ctc_loss': 0.1912254671672366, 'test/wer': 0.06296589685779863, 'test/num_examples': 2472, 'score': 55100.80209875107, 'total_duration': 55863.48661279678, 'global_step': 69646, 'preemption_count': 0}), (72674, {'train/ctc_loss': 0.12308892865919799, 'train/wer': 0.045337738619676944, 'validation/ctc_loss': 0.3356099924888864, 'validation/wer': 0.09717568676676483, 'validation/num_examples': 5348, 'test/ctc_loss': 0.1831917835424866, 'test/wer': 0.059289500944488455, 'test/num_examples': 2472, 'score': 57496.45815038681, 'total_duration': 58292.02870965004, 'global_step': 72674, 'preemption_count': 0}), (75702, {'train/ctc_loss': 0.11245797282295515, 'train/wer': 0.04174764619504189, 'validation/ctc_loss': 0.3277817707548473, 'validation/wer': 0.09424998793028533, 'validation/num_examples': 5348, 'test/ctc_loss': 0.17594132837199175, 'test/wer': 0.05750208193691223, 'test/num_examples': 2472, 'score': 59892.19504928589, 'total_duration': 60720.93298625946, 'global_step': 75702, 'preemption_count': 0}), (78731, {'train/ctc_loss': 0.10111861939002068, 'train/wer': 0.037812041116005876, 'validation/ctc_loss': 0.3151143207975437, 'validation/wer': 0.09110220634384203, 'validation/num_examples': 5348, 'test/ctc_loss': 0.17120107799293502, 'test/wer': 0.05469908394775862, 'test/num_examples': 2472, 'score': 62288.06440639496, 'total_duration': 63149.56621026993, 'global_step': 78731, 'preemption_count': 0}), (81758, {'train/ctc_loss': 0.09163077893545982, 'train/wer': 0.034243543232270884, 'validation/ctc_loss': 0.3088396810876909, 'validation/wer': 0.08715299570318158, 'validation/num_examples': 5348, 'test/ctc_loss': 0.1638395269452868, 'test/wer': 0.053033534417971685, 'test/num_examples': 2472, 'score': 64683.63411951065, 'total_duration': 65578.00123310089, 'global_step': 81758, 'preemption_count': 0}), (84786, {'train/ctc_loss': 0.08299717991551134, 'train/wer': 0.03145244882093807, 'validation/ctc_loss': 0.3017955593778255, 'validation/wer': 0.08471008545357987, 'validation/num_examples': 5348, 'test/ctc_loss': 0.16137671684463514, 'test/wer': 0.050880506977027604, 'test/num_examples': 2472, 'score': 67079.16812181473, 'total_duration': 68006.7655198574, 'global_step': 84786, 'preemption_count': 0}), (87814, {'train/ctc_loss': 0.07567596357962442, 'train/wer': 0.028170078604128877, 'validation/ctc_loss': 0.29189145557847346, 'validation/wer': 0.08223820788876551, 'validation/num_examples': 5348, 'test/ctc_loss': 0.15430257093199562, 'test/wer': 0.048321247943452564, 'test/num_examples': 2472, 'score': 69474.30549263954, 'total_duration': 70435.20787668228, 'global_step': 87814, 'preemption_count': 0}), (90841, {'train/ctc_loss': 0.07059585903867666, 'train/wer': 0.026258961734473524, 'validation/ctc_loss': 0.28667242852276725, 'validation/wer': 0.07954424757398736, 'validation/num_examples': 5348, 'test/ctc_loss': 0.15167883815084846, 'test/wer': 0.04702130684703349, 'test/num_examples': 2472, 'score': 71869.6157553196, 'total_duration': 72864.67642998695, 'global_step': 90841, 'preemption_count': 0})], 'global_step': 91007}
I0315 18:43:57.231949 140332132726592 submission_runner.py:526] Timing: 72000.61993741989
I0315 18:43:57.231999 140332132726592 submission_runner.py:527] ====================
I0315 18:43:57.232195 140332132726592 submission_runner.py:586] Finallibrispeech_conformer score: 72000.61993741989
