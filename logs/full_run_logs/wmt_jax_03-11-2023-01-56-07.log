I0311 01:56:21.517228 140391735396160 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing/wmt_jax.
I0311 01:56:21.562175 140391735396160 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0311 01:56:22.430455 140391735396160 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0311 01:56:22.431494 140391735396160 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0311 01:56:22.434320 140391735396160 submission_runner.py:481] Using RNG seed 703618313
I0311 01:56:23.594004 140391735396160 submission_runner.py:490] --- Tuning run 1/1 ---
I0311 01:56:23.594189 140391735396160 submission_runner.py:495] Creating tuning directory at /experiment_runs/timing/wmt_jax/trial_1.
I0311 01:56:23.594359 140391735396160 logger_utils.py:84] Saving hparams to /experiment_runs/timing/wmt_jax/trial_1/hparams.json.
I0311 01:56:23.712025 140391735396160 submission_runner.py:226] Initializing dataset.
I0311 01:56:23.718761 140391735396160 dataset_info.py:539] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0311 01:56:23.721414 140391735396160 dataset_info.py:606] Field info.splits from disk and from code do not match. Keeping the one from code.
I0311 01:56:23.721530 140391735396160 dataset_info.py:606] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0311 01:56:23.791064 140391735396160 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0311 01:56:25.544913 140391735396160 submission_runner.py:233] Initializing model.
I0311 01:56:38.027181 140391735396160 submission_runner.py:243] Initializing optimizer.
I0311 01:56:38.913495 140391735396160 submission_runner.py:250] Initializing metrics bundle.
I0311 01:56:38.913734 140391735396160 submission_runner.py:265] Initializing checkpoint and logger.
I0311 01:56:38.914672 140391735396160 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing/wmt_jax/trial_1 with prefix checkpoint_
I0311 01:56:39.676667 140391735396160 submission_runner.py:286] Saving meta data to /experiment_runs/timing/wmt_jax/trial_1/meta_data_0.json.
I0311 01:56:39.677708 140391735396160 submission_runner.py:289] Saving flags to /experiment_runs/timing/wmt_jax/trial_1/flags_0.json.
I0311 01:56:39.680748 140391735396160 submission_runner.py:299] Starting training loop.
I0311 01:57:16.310055 140215800813312 logging_writer.py:48] [0] global_step=0, grad_norm=4.947281360626221, loss=11.027030944824219
I0311 01:57:16.326272 140391735396160 spec.py:298] Evaluating on the training split.
I0311 01:57:16.329002 140391735396160 dataset_info.py:539] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0311 01:57:16.331620 140391735396160 dataset_info.py:606] Field info.splits from disk and from code do not match. Keeping the one from code.
I0311 01:57:16.331722 140391735396160 dataset_info.py:606] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0311 01:57:16.361756 140391735396160 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0311 01:57:24.832959 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 02:02:29.740665 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 02:02:29.743665 140391735396160 dataset_info.py:539] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0311 02:02:29.746653 140391735396160 dataset_info.py:606] Field info.splits from disk and from code do not match. Keeping the one from code.
I0311 02:02:29.746761 140391735396160 dataset_info.py:606] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0311 02:02:29.776693 140391735396160 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0311 02:02:37.124033 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 02:07:34.137635 140391735396160 spec.py:326] Evaluating on the test split.
I0311 02:07:34.140056 140391735396160 dataset_info.py:539] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0311 02:07:34.142632 140391735396160 dataset_info.py:606] Field info.splits from disk and from code do not match. Keeping the one from code.
I0311 02:07:34.142747 140391735396160 dataset_info.py:606] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0311 02:07:34.170333 140391735396160 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0311 02:07:36.967024 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 02:12:28.790257 140391735396160 submission_runner.py:359] Time since start: 36.65s, 	Step: 1, 	{'train/accuracy': 0.0006287654396146536, 'train/loss': 11.037056922912598, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.039999961853027, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.051443099975586, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0311 02:12:28.797777 140204642989824 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=36.446466, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.051443, test/num_examples=3003, total_duration=36.645486, train/accuracy=0.000629, train/bleu=0.000000, train/loss=11.037057, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.040000, validation/num_examples=3000
I0311 02:12:29.700848 140391735396160 checkpoints.py:356] Saving checkpoint at step: 1
I0311 02:12:33.175010 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_1
I0311 02:12:33.179604 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_1.
I0311 02:13:10.489668 140204634597120 logging_writer.py:48] [100] global_step=100, grad_norm=0.19738763570785522, loss=8.148750305175781
I0311 02:13:47.838374 140204207798016 logging_writer.py:48] [200] global_step=200, grad_norm=0.3880152106285095, loss=7.312774181365967
I0311 02:14:25.269466 140204634597120 logging_writer.py:48] [300] global_step=300, grad_norm=0.5204558968544006, loss=6.7060441970825195
I0311 02:15:02.717490 140204207798016 logging_writer.py:48] [400] global_step=400, grad_norm=0.40663594007492065, loss=6.13159704208374
I0311 02:15:40.142535 140204634597120 logging_writer.py:48] [500] global_step=500, grad_norm=0.5830395817756653, loss=5.711288928985596
I0311 02:16:17.552418 140204207798016 logging_writer.py:48] [600] global_step=600, grad_norm=0.5045801997184753, loss=5.37716817855835
I0311 02:16:54.989649 140204634597120 logging_writer.py:48] [700] global_step=700, grad_norm=0.6494975686073303, loss=5.184989929199219
I0311 02:17:32.439279 140204207798016 logging_writer.py:48] [800] global_step=800, grad_norm=0.4104304909706116, loss=4.8511457443237305
I0311 02:18:09.867784 140204634597120 logging_writer.py:48] [900] global_step=900, grad_norm=0.5462934374809265, loss=4.634673595428467
I0311 02:18:47.293603 140204207798016 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6603615283966064, loss=4.326637268066406
I0311 02:19:24.743964 140204634597120 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6126598715782166, loss=4.057309627532959
I0311 02:20:02.171330 140204207798016 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.4725167751312256, loss=3.818371295928955
I0311 02:20:39.565531 140204634597120 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5389518737792969, loss=3.7477753162384033
I0311 02:21:16.997438 140204207798016 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5184736251831055, loss=3.6811470985412598
I0311 02:21:54.408512 140204634597120 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5217131972312927, loss=3.4659528732299805
I0311 02:22:31.826510 140204207798016 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.4312821924686432, loss=3.3870067596435547
I0311 02:23:09.245246 140204634597120 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.4657277464866638, loss=3.281520128250122
I0311 02:23:46.635277 140204207798016 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.3829577565193176, loss=3.2667019367218018
I0311 02:24:24.016338 140204634597120 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.4051799476146698, loss=3.137500762939453
I0311 02:25:01.463354 140204207798016 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.4971841275691986, loss=3.1191580295562744
I0311 02:25:38.857896 140204634597120 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.3203555643558502, loss=2.942625045776367
I0311 02:26:16.281522 140204207798016 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.34760499000549316, loss=2.9573662281036377
I0311 02:26:33.214471 140391735396160 spec.py:298] Evaluating on the training split.
I0311 02:26:36.194821 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 02:29:24.547230 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 02:29:27.200167 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 02:32:06.913915 140391735396160 spec.py:326] Evaluating on the test split.
I0311 02:32:09.613180 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 02:34:43.177171 140391735396160 submission_runner.py:359] Time since start: 1793.53s, 	Step: 2247, 	{'train/accuracy': 0.5198379755020142, 'train/loss': 2.817359209060669, 'train/bleu': 23.378130569270205, 'validation/accuracy': 0.5187288522720337, 'validation/loss': 2.795701503753662, 'validation/bleu': 19.082468131719637, 'validation/num_examples': 3000, 'test/accuracy': 0.5188542604446411, 'test/loss': 2.830124855041504, 'test/bleu': 17.22877798001409, 'test/num_examples': 3003}
I0311 02:34:43.186369 140204634597120 logging_writer.py:48] [2247] global_step=2247, preemption_count=0, score=872.153926, test/accuracy=0.518854, test/bleu=17.228778, test/loss=2.830125, test/num_examples=3003, total_duration=1793.533673, train/accuracy=0.519838, train/bleu=23.378131, train/loss=2.817359, validation/accuracy=0.518729, validation/bleu=19.082468, validation/loss=2.795702, validation/num_examples=3000
I0311 02:34:44.439365 140391735396160 checkpoints.py:356] Saving checkpoint at step: 2247
I0311 02:34:48.793259 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_2247
I0311 02:34:48.796813 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_2247.
I0311 02:35:08.961444 140204207798016 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.3152468502521515, loss=2.9445974826812744
I0311 02:35:46.247264 140204199405312 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.2889614403247833, loss=2.9107987880706787
I0311 02:36:23.626449 140204207798016 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.27032890915870667, loss=2.741617441177368
I0311 02:37:01.042047 140204199405312 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.24721874296665192, loss=2.764711856842041
I0311 02:37:38.464537 140204207798016 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.2309625744819641, loss=2.6937997341156006
I0311 02:38:15.859213 140204199405312 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.25786375999450684, loss=2.6673331260681152
I0311 02:38:53.258004 140204207798016 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.20803454518318176, loss=2.770439386367798
I0311 02:39:30.626505 140204199405312 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.21887372434139252, loss=2.610962390899658
I0311 02:40:08.037092 140204207798016 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.22813908755779266, loss=2.5650789737701416
I0311 02:40:45.361482 140204199405312 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.22612252831459045, loss=2.4620702266693115
I0311 02:41:22.782681 140204207798016 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.21393705904483795, loss=2.5546836853027344
I0311 02:42:00.178511 140204199405312 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.18798328936100006, loss=2.552563428878784
I0311 02:42:37.565321 140204207798016 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.19604845345020294, loss=2.4567129611968994
I0311 02:43:14.984296 140204199405312 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.1930423527956009, loss=2.5256948471069336
I0311 02:43:52.351021 140204207798016 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.23080603778362274, loss=2.4333372116088867
I0311 02:44:29.684994 140204199405312 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.19125942885875702, loss=2.392885684967041
I0311 02:45:07.086333 140204207798016 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.1926257461309433, loss=2.3543667793273926
I0311 02:45:44.476251 140204199405312 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.17911966145038605, loss=2.473855972290039
I0311 02:46:21.888823 140204207798016 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.16891838610172272, loss=2.348621368408203
I0311 02:46:59.330466 140204199405312 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.16435670852661133, loss=2.427269458770752
I0311 02:47:36.731206 140204207798016 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.1682012528181076, loss=2.315423011779785
I0311 02:48:14.109738 140204199405312 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.17544075846672058, loss=2.3323638439178467
I0311 02:48:48.990822 140391735396160 spec.py:298] Evaluating on the training split.
I0311 02:48:51.987453 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 02:51:23.934894 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 02:51:26.577303 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 02:53:47.031870 140391735396160 spec.py:326] Evaluating on the test split.
I0311 02:53:49.726307 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 02:56:05.633475 140391735396160 submission_runner.py:359] Time since start: 3129.31s, 	Step: 4495, 	{'train/accuracy': 0.5779771208763123, 'train/loss': 2.2502853870391846, 'train/bleu': 27.50548948137948, 'validation/accuracy': 0.5897012948989868, 'validation/loss': 2.1443467140197754, 'validation/bleu': 23.348739176161885, 'validation/num_examples': 3000, 'test/accuracy': 0.5949915647506714, 'test/loss': 2.11604380607605, 'test/bleu': 22.146500211483012, 'test/num_examples': 3003}
I0311 02:56:05.640566 140204207798016 logging_writer.py:48] [4495] global_step=4495, preemption_count=0, score=1707.484046, test/accuracy=0.594992, test/bleu=22.146500, test/loss=2.116044, test/num_examples=3003, total_duration=3129.310027, train/accuracy=0.577977, train/bleu=27.505489, train/loss=2.250285, validation/accuracy=0.589701, validation/bleu=23.348739, validation/loss=2.144347, validation/num_examples=3000
I0311 02:56:06.749064 140391735396160 checkpoints.py:356] Saving checkpoint at step: 4495
I0311 02:56:10.171424 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_4495
I0311 02:56:10.175883 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_4495.
I0311 02:56:12.439114 140204199405312 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.21049553155899048, loss=2.2861688137054443
I0311 02:56:49.758649 140204191012608 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.15879777073860168, loss=2.3236420154571533
I0311 02:57:27.083908 140204199405312 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.15425428748130798, loss=2.3117570877075195
I0311 02:58:04.425260 140204191012608 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.22419099509716034, loss=2.2556893825531006
I0311 02:58:41.783879 140204199405312 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.14717675745487213, loss=2.2492220401763916
I0311 02:59:19.128143 140204191012608 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.15217803418636322, loss=2.183809995651245
I0311 02:59:56.538241 140204199405312 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.1509612500667572, loss=2.2368125915527344
I0311 03:00:33.920789 140204191012608 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.14748477935791016, loss=2.2808454036712646
I0311 03:01:11.285912 140204199405312 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.16078788042068481, loss=2.241147756576538
I0311 03:01:48.641467 140204191012608 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.15380500257015228, loss=2.174873113632202
I0311 03:02:26.076992 140204199405312 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.1564716100692749, loss=2.317338466644287
I0311 03:03:03.453618 140204191012608 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.1550895869731903, loss=2.1978063583374023
I0311 03:03:40.852128 140204199405312 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.18093512952327728, loss=2.2423627376556396
I0311 03:04:18.203524 140204191012608 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.16320809721946716, loss=2.1282098293304443
I0311 03:04:55.611772 140204199405312 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.1413276344537735, loss=2.1924166679382324
I0311 03:05:33.000143 140204191012608 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.15049345791339874, loss=2.1954119205474854
I0311 03:06:10.380994 140204199405312 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.16750749945640564, loss=2.258558511734009
I0311 03:06:47.706450 140204191012608 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.1366826593875885, loss=2.1100966930389404
I0311 03:07:25.075086 140204626204416 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.18369203805923462, loss=2.1345651149749756
I0311 03:08:02.498212 140204617811712 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.1517973393201828, loss=2.170046806335449
I0311 03:08:39.835653 140204626204416 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.18901225924491882, loss=2.1244680881500244
I0311 03:09:17.199086 140204617811712 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.14592371881008148, loss=2.157637596130371
I0311 03:09:54.594897 140204626204416 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.13535381853580475, loss=2.0910542011260986
I0311 03:10:10.386123 140391735396160 spec.py:298] Evaluating on the training split.
I0311 03:10:13.370516 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 03:13:00.227203 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 03:13:02.875647 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 03:15:23.444947 140391735396160 spec.py:326] Evaluating on the test split.
I0311 03:15:26.135126 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 03:17:39.703624 140391735396160 submission_runner.py:359] Time since start: 4410.71s, 	Step: 6744, 	{'train/accuracy': 0.6082260012626648, 'train/loss': 1.9924029111862183, 'train/bleu': 30.17388072897711, 'validation/accuracy': 0.6161981821060181, 'validation/loss': 1.9323006868362427, 'validation/bleu': 25.55113566757477, 'validation/num_examples': 3000, 'test/accuracy': 0.6234385371208191, 'test/loss': 1.8746626377105713, 'test/bleu': 24.361249881773627, 'test/num_examples': 3003}
I0311 03:17:39.711669 140204617811712 logging_writer.py:48] [6744] global_step=6744, preemption_count=0, score=2544.211182, test/accuracy=0.623439, test/bleu=24.361250, test/loss=1.874663, test/num_examples=3003, total_duration=4410.705321, train/accuracy=0.608226, train/bleu=30.173881, train/loss=1.992403, validation/accuracy=0.616198, validation/bleu=25.551136, validation/loss=1.932301, validation/num_examples=3000
I0311 03:17:40.757306 140391735396160 checkpoints.py:356] Saving checkpoint at step: 6744
I0311 03:17:44.243855 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_6744
I0311 03:17:44.248314 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_6744.
I0311 03:18:05.513649 140204626204416 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.15294945240020752, loss=2.129575252532959
I0311 03:18:42.801677 140204296910592 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.16198469698429108, loss=2.1898210048675537
I0311 03:19:20.142805 140204626204416 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.15588326752185822, loss=2.21066951751709
I0311 03:19:57.458797 140204296910592 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.1807146817445755, loss=2.1859757900238037
I0311 03:20:34.827209 140204626204416 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.13861320912837982, loss=2.1707727909088135
I0311 03:21:12.181278 140204296910592 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.13331705331802368, loss=2.0559465885162354
I0311 03:21:49.539196 140204626204416 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.15546296536922455, loss=2.083331823348999
I0311 03:22:26.986060 140204296910592 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.2377910166978836, loss=2.110515832901001
I0311 03:23:04.340470 140204626204416 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.1448848843574524, loss=2.1311542987823486
I0311 03:23:41.745878 140204296910592 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.16817781329154968, loss=2.167496919631958
I0311 03:24:19.150037 140204626204416 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.14328762888908386, loss=2.0649235248565674
I0311 03:24:56.505737 140204296910592 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.1433606594800949, loss=2.1110055446624756
I0311 03:25:33.858207 140204626204416 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.1557237058877945, loss=2.1725244522094727
I0311 03:26:11.236086 140204296910592 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.1394893378019333, loss=1.9606132507324219
I0311 03:26:48.582312 140204626204416 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.14460472762584686, loss=1.993507981300354
I0311 03:27:26.013350 140204296910592 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.15345242619514465, loss=2.1170413494110107
I0311 03:28:03.361436 140204626204416 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.14902327954769135, loss=2.0003859996795654
I0311 03:28:40.686536 140204296910592 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.14852045476436615, loss=2.0942089557647705
I0311 03:29:18.074599 140204626204416 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.18487048149108887, loss=2.0280659198760986
I0311 03:29:55.394257 140204296910592 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.14989487826824188, loss=2.116673707962036
I0311 03:30:32.775879 140204626204416 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.2466362863779068, loss=2.0880422592163086
I0311 03:31:10.138991 140204296910592 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.15175580978393555, loss=1.9998637437820435
I0311 03:31:44.617554 140391735396160 spec.py:298] Evaluating on the training split.
I0311 03:31:47.594962 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 03:35:03.941968 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 03:35:06.594441 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 03:37:25.177525 140391735396160 spec.py:326] Evaluating on the test split.
I0311 03:37:27.864314 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 03:39:41.348701 140391735396160 submission_runner.py:359] Time since start: 5704.94s, 	Step: 8994, 	{'train/accuracy': 0.6123753190040588, 'train/loss': 1.9473698139190674, 'train/bleu': 29.388182231186878, 'validation/accuracy': 0.6295768022537231, 'validation/loss': 1.817801594734192, 'validation/bleu': 26.216295834304685, 'validation/num_examples': 3000, 'test/accuracy': 0.6351867914199829, 'test/loss': 1.7686195373535156, 'test/bleu': 24.84412490772963, 'test/num_examples': 3003}
I0311 03:39:41.357100 140204626204416 logging_writer.py:48] [8994] global_step=8994, preemption_count=0, score=3381.080398, test/accuracy=0.635187, test/bleu=24.844125, test/loss=1.768620, test/num_examples=3003, total_duration=5704.936761, train/accuracy=0.612375, train/bleu=29.388182, train/loss=1.947370, validation/accuracy=0.629577, validation/bleu=26.216296, validation/loss=1.817802, validation/num_examples=3000
I0311 03:39:42.313610 140391735396160 checkpoints.py:356] Saving checkpoint at step: 8994
I0311 03:39:45.624672 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_8994
I0311 03:39:45.629116 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_8994.
I0311 03:39:48.261138 140204296910592 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.13890431821346283, loss=2.094332218170166
I0311 03:40:25.527074 140204288517888 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.15332406759262085, loss=1.9931013584136963
I0311 03:41:02.817842 140204296910592 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.17538194358348846, loss=2.1383328437805176
I0311 03:41:40.116377 140204288517888 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.13507471978664398, loss=1.96940279006958
I0311 03:42:17.471670 140204296910592 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.16739095747470856, loss=2.0074636936187744
I0311 03:42:54.870125 140204288517888 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.17461492121219635, loss=1.9769182205200195
I0311 03:43:32.220752 140204296910592 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.14911916851997375, loss=1.9659141302108765
I0311 03:44:09.577260 140204288517888 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.1643962562084198, loss=1.9562724828720093
I0311 03:44:46.990551 140204296910592 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.16375546157360077, loss=2.080620050430298
I0311 03:45:24.355559 140204288517888 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.15816359221935272, loss=2.022545576095581
I0311 03:46:01.754204 140204296910592 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.16900686919689178, loss=1.9265785217285156
I0311 03:46:39.123696 140204288517888 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.1960986852645874, loss=2.023535966873169
I0311 03:47:16.513674 140204296910592 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.16381396353244781, loss=1.9299955368041992
I0311 03:47:53.891672 140204288517888 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.13980740308761597, loss=1.9862481355667114
I0311 03:48:31.239706 140204296910592 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.15681135654449463, loss=1.9575154781341553
I0311 03:49:08.653537 140204288517888 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.15128357708454132, loss=2.0185060501098633
I0311 03:49:46.030627 140204296910592 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.20556069910526276, loss=1.9882174730300903
I0311 03:50:23.401849 140204288517888 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.2561505138874054, loss=2.060218095779419
I0311 03:51:00.815627 140204296910592 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.2092941254377365, loss=1.9525055885314941
I0311 03:51:38.217559 140204288517888 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.18354620039463043, loss=2.0671396255493164
I0311 03:52:15.572003 140204296910592 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.16317610442638397, loss=2.01196551322937
I0311 03:52:52.963993 140204288517888 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.14947839081287384, loss=1.9231823682785034
I0311 03:53:30.346400 140204296910592 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.16310757398605347, loss=1.9487297534942627
I0311 03:53:45.764681 140391735396160 spec.py:298] Evaluating on the training split.
I0311 03:53:48.753636 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 03:56:19.791258 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 03:56:22.429759 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 03:58:34.890504 140391735396160 spec.py:326] Evaluating on the test split.
I0311 03:58:37.575639 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 04:00:46.877637 140391735396160 submission_runner.py:359] Time since start: 7026.08s, 	Step: 11243, 	{'train/accuracy': 0.6191498637199402, 'train/loss': 1.89480459690094, 'train/bleu': 29.81087653375899, 'validation/accuracy': 0.6388017535209656, 'validation/loss': 1.756168007850647, 'validation/bleu': 26.68195837966921, 'validation/num_examples': 3000, 'test/accuracy': 0.6459125280380249, 'test/loss': 1.6911065578460693, 'test/bleu': 25.65808468992218, 'test/num_examples': 3003}
I0311 04:00:46.885562 140204288517888 logging_writer.py:48] [11243] global_step=11243, preemption_count=0, score=4217.635367, test/accuracy=0.645913, test/bleu=25.658085, test/loss=1.691107, test/num_examples=3003, total_duration=7026.083872, train/accuracy=0.619150, train/bleu=29.810877, train/loss=1.894805, validation/accuracy=0.638802, validation/bleu=26.681958, validation/loss=1.756168, validation/num_examples=3000
I0311 04:00:47.841580 140391735396160 checkpoints.py:356] Saving checkpoint at step: 11243
I0311 04:00:51.111904 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_11243
I0311 04:00:51.116331 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_11243.
I0311 04:01:12.763066 140204296910592 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.219142884016037, loss=2.0130226612091064
I0311 04:01:50.084521 140204280125184 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.25365149974823, loss=1.9925285577774048
I0311 04:02:27.417611 140204296910592 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.17352986335754395, loss=2.0486299991607666
I0311 04:03:04.790621 140204280125184 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.1591518074274063, loss=1.9248740673065186
I0311 04:03:42.140870 140204296910592 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.2188015878200531, loss=1.9405543804168701
I0311 04:04:19.487236 140204280125184 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.1941232681274414, loss=1.8779627084732056
I0311 04:04:56.906384 140204296910592 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.2571329176425934, loss=2.0624806880950928
I0311 04:05:34.266245 140204280125184 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.22871530055999756, loss=2.0395872592926025
I0311 04:06:11.631626 140204296910592 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.17330841720104218, loss=1.9448422193527222
I0311 04:06:48.967812 140204280125184 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.1635439395904541, loss=1.9876042604446411
I0311 04:07:26.372460 140204296910592 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.1788671463727951, loss=1.9272609949111938
I0311 04:08:03.728141 140204280125184 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.17054817080497742, loss=1.8744546175003052
I0311 04:08:41.093981 140204296910592 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.16131146252155304, loss=1.954971432685852
I0311 04:09:18.431459 140204280125184 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.15551301836967468, loss=1.9956659078598022
I0311 04:09:55.797132 140204296910592 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.15264786779880524, loss=1.9086036682128906
I0311 04:10:33.154170 140204280125184 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.18611907958984375, loss=2.012936592102051
I0311 04:11:10.509410 140204296910592 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.1586650162935257, loss=1.9120289087295532
I0311 04:11:47.909965 140204280125184 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.2183481752872467, loss=1.971880555152893
I0311 04:12:25.228437 140204296910592 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.16648265719413757, loss=1.9639259576797485
I0311 04:13:02.600594 140204280125184 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.18595834076404572, loss=1.9025260210037231
I0311 04:13:39.986127 140204296910592 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.17874211072921753, loss=1.9389081001281738
I0311 04:14:17.390375 140204280125184 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.1884366124868393, loss=1.9944069385528564
I0311 04:14:51.454308 140391735396160 spec.py:298] Evaluating on the training split.
I0311 04:14:54.433171 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 04:17:34.948260 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 04:17:37.577108 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 04:20:05.175204 140391735396160 spec.py:326] Evaluating on the test split.
I0311 04:20:07.855755 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 04:22:25.489503 140391735396160 submission_runner.py:359] Time since start: 8291.77s, 	Step: 13493, 	{'train/accuracy': 0.6322474479675293, 'train/loss': 1.787447452545166, 'train/bleu': 30.370245323736118, 'validation/accuracy': 0.6454972624778748, 'validation/loss': 1.7069510221481323, 'validation/bleu': 27.26821913156087, 'validation/num_examples': 3000, 'test/accuracy': 0.6533961296081543, 'test/loss': 1.6444133520126343, 'test/bleu': 26.45427409480538, 'test/num_examples': 3003}
I0311 04:22:25.498205 140204296910592 logging_writer.py:48] [13493] global_step=13493, preemption_count=0, score=5054.452053, test/accuracy=0.653396, test/bleu=26.454274, test/loss=1.644413, test/num_examples=3003, total_duration=8291.773514, train/accuracy=0.632247, train/bleu=30.370245, train/loss=1.787447, validation/accuracy=0.645497, validation/bleu=27.268219, validation/loss=1.706951, validation/num_examples=3000
I0311 04:22:26.428126 140391735396160 checkpoints.py:356] Saving checkpoint at step: 13493
I0311 04:22:29.709424 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_13493
I0311 04:22:29.713942 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_13493.
I0311 04:22:32.706722 140204280125184 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.17585253715515137, loss=1.9612163305282593
I0311 04:23:09.926312 140204271732480 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.25099316239356995, loss=1.8547357320785522
I0311 04:23:47.223491 140204280125184 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.18418513238430023, loss=1.9358948469161987
I0311 04:24:24.598259 140204271732480 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.18721294403076172, loss=1.9169058799743652
I0311 04:25:01.962434 140204280125184 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.17184284329414368, loss=1.9688663482666016
I0311 04:25:39.316478 140204271732480 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.20031896233558655, loss=1.9440687894821167
I0311 04:26:16.651214 140204280125184 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.16165126860141754, loss=1.825242280960083
I0311 04:26:54.008703 140204271732480 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.1808588057756424, loss=1.957488775253296
I0311 04:27:31.373913 140204280125184 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.1488465666770935, loss=1.8647009134292603
I0311 04:28:08.764254 140204271732480 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.18858911097049713, loss=1.9302698373794556
I0311 04:28:46.089387 140204280125184 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.15351858735084534, loss=1.943619966506958
I0311 04:29:23.520736 140204271732480 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.17928875982761383, loss=1.9021117687225342
I0311 04:30:00.854727 140204280125184 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.18482156097888947, loss=1.9454593658447266
I0311 04:30:38.237504 140204271732480 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.23683317005634308, loss=1.8450955152511597
I0311 04:31:15.617135 140204280125184 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.15726694464683533, loss=1.9208251237869263
I0311 04:31:52.976891 140204271732480 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.1930161714553833, loss=2.011944055557251
I0311 04:32:30.433379 140204280125184 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.16527749598026276, loss=1.930380940437317
I0311 04:33:07.827616 140204271732480 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.19762928783893585, loss=1.9050379991531372
I0311 04:33:45.235863 140204280125184 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.20763523876667023, loss=1.9523675441741943
I0311 04:34:22.604156 140204271732480 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.1802685260772705, loss=1.9801987409591675
I0311 04:35:00.007987 140204280125184 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.1573941707611084, loss=1.9372857809066772
I0311 04:35:37.356857 140204271732480 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.17412562668323517, loss=1.8819029331207275
I0311 04:36:14.712047 140204280125184 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.1619430035352707, loss=1.9208261966705322
I0311 04:36:29.745182 140391735396160 spec.py:298] Evaluating on the training split.
I0311 04:36:32.727710 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 04:40:35.363160 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 04:40:38.000535 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 04:44:35.752378 140391735396160 spec.py:326] Evaluating on the test split.
I0311 04:44:38.439536 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 04:48:28.338492 140391735396160 submission_runner.py:359] Time since start: 9590.06s, 	Step: 15742, 	{'train/accuracy': 0.6286909580230713, 'train/loss': 1.8207414150238037, 'train/bleu': 30.479847915533863, 'validation/accuracy': 0.6473199129104614, 'validation/loss': 1.6877312660217285, 'validation/bleu': 27.23935768612174, 'validation/num_examples': 3000, 'test/accuracy': 0.6550229787826538, 'test/loss': 1.6249533891677856, 'test/bleu': 26.57388936027434, 'test/num_examples': 3003}
I0311 04:48:28.347753 140204271732480 logging_writer.py:48] [15742] global_step=15742, preemption_count=0, score=5891.036471, test/accuracy=0.655023, test/bleu=26.573889, test/loss=1.624953, test/num_examples=3003, total_duration=9590.064391, train/accuracy=0.628691, train/bleu=30.479848, train/loss=1.820741, validation/accuracy=0.647320, validation/bleu=27.239358, validation/loss=1.687731, validation/num_examples=3000
I0311 04:48:29.271722 140391735396160 checkpoints.py:356] Saving checkpoint at step: 15742
I0311 04:48:33.194129 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_15742
I0311 04:48:33.198661 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_15742.
I0311 04:48:55.174966 140204280125184 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.18429948389530182, loss=1.8760416507720947
I0311 04:49:32.434903 140204263339776 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.19995428621768951, loss=1.8232425451278687
I0311 04:50:09.758768 140204280125184 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.17941857874393463, loss=1.973588466644287
I0311 04:50:47.100382 140204263339776 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.20496335625648499, loss=1.9390605688095093
I0311 04:51:24.459417 140204280125184 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.1905335932970047, loss=1.8655662536621094
I0311 04:52:01.841116 140204263339776 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.18088071048259735, loss=1.8657714128494263
I0311 04:52:39.179812 140204280125184 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.17290331423282623, loss=1.887846827507019
I0311 04:53:16.557785 140204263339776 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.22295719385147095, loss=1.9219552278518677
I0311 04:53:53.950822 140204280125184 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.1804029643535614, loss=1.8683316707611084
I0311 04:54:31.345427 140204263339776 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.1762283593416214, loss=1.9023711681365967
I0311 04:55:08.747341 140204280125184 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.16726934909820557, loss=1.8069838285446167
I0311 04:55:46.105768 140204263339776 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.2807401418685913, loss=1.9282106161117554
I0311 04:56:23.469471 140204280125184 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.17914074659347534, loss=1.952819585800171
I0311 04:57:00.872814 140204263339776 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.17006810009479523, loss=1.9094619750976562
I0311 04:57:38.236773 140204280125184 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.15234751999378204, loss=1.828361988067627
I0311 04:58:15.617035 140204263339776 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.18257561326026917, loss=1.8057852983474731
I0311 04:58:52.983402 140204280125184 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.263074666261673, loss=1.8433455228805542
I0311 04:59:30.394068 140204263339776 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.22514081001281738, loss=1.913226842880249
I0311 05:00:07.759687 140204280125184 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.24089448153972626, loss=1.9328962564468384
I0311 05:00:45.137367 140204263339776 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.19593104720115662, loss=1.9069746732711792
I0311 05:01:22.492466 140204280125184 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.1804424226284027, loss=1.8834381103515625
I0311 05:01:59.882897 140204263339776 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.2881605327129364, loss=1.8439589738845825
I0311 05:02:33.235696 140391735396160 spec.py:298] Evaluating on the training split.
I0311 05:02:36.219735 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 05:05:36.472101 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 05:05:39.105222 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 05:07:58.542465 140391735396160 spec.py:326] Evaluating on the test split.
I0311 05:08:01.226063 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 05:10:20.199997 140391735396160 submission_runner.py:359] Time since start: 11153.55s, 	Step: 17991, 	{'train/accuracy': 0.6307916641235352, 'train/loss': 1.8037077188491821, 'train/bleu': 30.902600817098595, 'validation/accuracy': 0.6520687937736511, 'validation/loss': 1.6513620615005493, 'validation/bleu': 27.73049081228079, 'validation/num_examples': 3000, 'test/accuracy': 0.6610888838768005, 'test/loss': 1.5822477340698242, 'test/bleu': 26.918190229727333, 'test/num_examples': 3003}
I0311 05:10:20.209209 140204280125184 logging_writer.py:48] [17991] global_step=17991, preemption_count=0, score=6727.561628, test/accuracy=0.661089, test/bleu=26.918190, test/loss=1.582248, test/num_examples=3003, total_duration=11153.554866, train/accuracy=0.630792, train/bleu=30.902601, train/loss=1.803708, validation/accuracy=0.652069, validation/bleu=27.730491, validation/loss=1.651362, validation/num_examples=3000
I0311 05:10:21.394348 140391735396160 checkpoints.py:356] Saving checkpoint at step: 17991
I0311 05:10:25.302865 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_17991
I0311 05:10:25.307503 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_17991.
I0311 05:10:29.050259 140204263339776 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.18632368743419647, loss=1.8499866724014282
I0311 05:11:06.316481 140204254947072 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.19838397204875946, loss=1.8638949394226074
I0311 05:11:43.599476 140204263339776 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.16829802095890045, loss=1.8319367170333862
I0311 05:12:20.949704 140204254947072 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.23611094057559967, loss=1.8355828523635864
I0311 05:12:58.329425 140204263339776 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.17298093438148499, loss=1.8564146757125854
I0311 05:13:35.706166 140204254947072 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.19549775123596191, loss=1.9030574560165405
I0311 05:14:13.060116 140204263339776 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.18164080381393433, loss=1.913029432296753
I0311 05:14:50.434161 140204254947072 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.18124723434448242, loss=1.8117735385894775
I0311 05:15:27.825407 140204263339776 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.18958449363708496, loss=1.9145506620407104
I0311 05:16:05.161910 140204254947072 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.23280541598796844, loss=1.8734867572784424
I0311 05:16:42.537540 140204263339776 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.17264239490032196, loss=1.8070183992385864
I0311 05:17:19.904513 140204254947072 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.23674161732196808, loss=1.7991427183151245
I0311 05:17:57.314323 140204263339776 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.24312350153923035, loss=1.9299486875534058
I0311 05:18:34.715712 140204254947072 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.1730058491230011, loss=1.8330470323562622
I0311 05:19:12.061575 140204263339776 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.23683249950408936, loss=1.9009907245635986
I0311 05:19:49.401970 140204254947072 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.194570392370224, loss=1.8223615884780884
I0311 05:20:26.766933 140204263339776 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.1769711822271347, loss=1.8801947832107544
I0311 05:21:04.210050 140204254947072 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.2109351009130478, loss=1.8435043096542358
I0311 05:21:41.588245 140204263339776 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.19892166554927826, loss=1.8954129219055176
I0311 05:22:18.953948 140204254947072 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.18391911685466766, loss=1.8654879331588745
I0311 05:22:56.258637 140204263339776 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.22722645103931427, loss=1.844474196434021
I0311 05:23:33.677988 140204254947072 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.22098025679588318, loss=1.8048204183578491
I0311 05:24:11.064467 140204263339776 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.1747979074716568, loss=1.86482572555542
I0311 05:24:25.387257 140391735396160 spec.py:298] Evaluating on the training split.
I0311 05:24:28.376554 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 05:27:39.056656 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 05:27:41.709274 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 05:30:38.322639 140391735396160 spec.py:326] Evaluating on the test split.
I0311 05:30:41.010965 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 05:33:01.162639 140391735396160 submission_runner.py:359] Time since start: 12465.71s, 	Step: 20240, 	{'train/accuracy': 0.6386517286300659, 'train/loss': 1.7386045455932617, 'train/bleu': 31.02671844970933, 'validation/accuracy': 0.6544122099876404, 'validation/loss': 1.6381105184555054, 'validation/bleu': 27.91161228130194, 'validation/num_examples': 3000, 'test/accuracy': 0.6635175347328186, 'test/loss': 1.5663893222808838, 'test/bleu': 27.084027213150073, 'test/num_examples': 3003}
I0311 05:33:01.172194 140204254947072 logging_writer.py:48] [20240] global_step=20240, preemption_count=0, score=7564.166583, test/accuracy=0.663518, test/bleu=27.084027, test/loss=1.566389, test/num_examples=3003, total_duration=12465.706451, train/accuracy=0.638652, train/bleu=31.026718, train/loss=1.738605, validation/accuracy=0.654412, validation/bleu=27.911612, validation/loss=1.638111, validation/num_examples=3000
I0311 05:33:02.214307 140391735396160 checkpoints.py:356] Saving checkpoint at step: 20240
I0311 05:33:05.945673 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_20240
I0311 05:33:05.949814 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_20240.
I0311 05:33:28.683787 140204263339776 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.1801915466785431, loss=1.8166873455047607
I0311 05:34:05.962955 140204246554368 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.1703406721353531, loss=1.853102445602417
I0311 05:34:43.297094 140204263339776 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.1588059663772583, loss=1.8229249715805054
I0311 05:35:20.653311 140204246554368 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.2244458943605423, loss=1.879880666732788
I0311 05:35:58.040673 140204263339776 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.20930242538452148, loss=1.9202404022216797
I0311 05:36:35.459077 140204246554368 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.1895672231912613, loss=1.9182885885238647
I0311 05:37:12.878349 140204263339776 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.19064001739025116, loss=1.8251532316207886
I0311 05:37:50.265490 140204246554368 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.21550504863262177, loss=1.8004947900772095
I0311 05:38:27.611241 140204263339776 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.1810304969549179, loss=1.886797547340393
I0311 05:39:04.991818 140204246554368 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.2508730888366699, loss=1.8981058597564697
I0311 05:39:42.379709 140204263339776 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.23472407460212708, loss=1.8474758863449097
I0311 05:40:19.758383 140204246554368 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.2038249522447586, loss=1.7113772630691528
I0311 05:40:57.137825 140204263339776 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.22738057374954224, loss=1.8241400718688965
I0311 05:41:34.536201 140204246554368 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.18718622624874115, loss=1.8112061023712158
I0311 05:42:11.978900 140204263339776 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.22441136837005615, loss=1.778842806816101
I0311 05:42:49.358584 140204246554368 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.15581446886062622, loss=1.8497551679611206
I0311 05:43:26.765259 140204263339776 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.23136556148529053, loss=1.8588135242462158
I0311 05:44:04.133050 140204246554368 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.19462382793426514, loss=1.8092457056045532
I0311 05:44:41.452250 140204263339776 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.19982431828975677, loss=1.897386908531189
I0311 05:45:18.836386 140204246554368 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.17266808450222015, loss=1.8489434719085693
I0311 05:45:56.207381 140204263339776 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.1743931919336319, loss=1.817307710647583
I0311 05:46:33.601826 140204246554368 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.17663118243217468, loss=1.9427179098129272
I0311 05:47:06.219856 140391735396160 spec.py:298] Evaluating on the training split.
I0311 05:47:09.195846 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 05:50:31.492783 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 05:50:34.123246 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 05:53:12.258238 140391735396160 spec.py:326] Evaluating on the test split.
I0311 05:53:14.938524 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 05:56:21.392961 140391735396160 submission_runner.py:359] Time since start: 13826.54s, 	Step: 22489, 	{'train/accuracy': 0.6358563899993896, 'train/loss': 1.7583489418029785, 'train/bleu': 31.037824877571992, 'validation/accuracy': 0.653271496295929, 'validation/loss': 1.6486068964004517, 'validation/bleu': 27.881172697717684, 'validation/num_examples': 3000, 'test/accuracy': 0.6654000282287598, 'test/loss': 1.5614794492721558, 'test/bleu': 26.808755779679828, 'test/num_examples': 3003}
I0311 05:56:21.403040 140204263339776 logging_writer.py:48] [22489] global_step=22489, preemption_count=0, score=8400.837740, test/accuracy=0.665400, test/bleu=26.808756, test/loss=1.561479, test/num_examples=3003, total_duration=13826.539062, train/accuracy=0.635856, train/bleu=31.037825, train/loss=1.758349, validation/accuracy=0.653271, validation/bleu=27.881173, validation/loss=1.648607, validation/num_examples=3000
I0311 05:56:22.458996 140391735396160 checkpoints.py:356] Saving checkpoint at step: 22489
I0311 05:56:25.857504 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_22489
I0311 05:56:25.861324 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_22489.
I0311 05:56:30.345500 140204246554368 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.17534314095973969, loss=1.815516471862793
I0311 05:57:07.618096 140204238161664 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.17024338245391846, loss=1.8143640756607056
I0311 05:57:44.950766 140204246554368 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.1902700960636139, loss=1.8522067070007324
I0311 05:58:22.276706 140204238161664 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.2097560167312622, loss=1.815451741218567
I0311 05:58:59.639368 140204246554368 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.21140305697917938, loss=1.8722279071807861
I0311 05:59:36.968288 140204238161664 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.19182513654232025, loss=1.8811125755310059
I0311 06:00:14.305214 140204246554368 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.2921309173107147, loss=1.8129210472106934
I0311 06:00:51.650168 140204238161664 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.16091804206371307, loss=1.8241806030273438
I0311 06:01:29.040686 140204246554368 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.1957697570323944, loss=1.8604182004928589
I0311 06:02:06.447615 140204238161664 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.17090490460395813, loss=1.9213602542877197
I0311 06:02:43.829039 140204246554368 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.19204534590244293, loss=1.7778292894363403
I0311 06:03:21.228468 140204238161664 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.19640186429023743, loss=1.8348338603973389
I0311 06:03:58.599306 140204246554368 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.1909734308719635, loss=1.8893139362335205
I0311 06:04:35.988443 140204238161664 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.27439749240875244, loss=1.7984912395477295
I0311 06:05:13.390580 140204246554368 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.187611922621727, loss=1.8883183002471924
I0311 06:05:50.736520 140204238161664 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.1793128103017807, loss=1.810931921005249
I0311 06:06:28.068878 140204246554368 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.20526455342769623, loss=1.8849166631698608
I0311 06:07:05.479982 140204238161664 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.19743886590003967, loss=1.833808422088623
I0311 06:07:42.893314 140204246554368 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.23987828195095062, loss=1.8783693313598633
I0311 06:08:20.322093 140204238161664 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.18360765278339386, loss=1.7833901643753052
I0311 06:08:57.722344 140204246554368 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.1998768001794815, loss=1.754248023033142
I0311 06:09:35.039629 140204238161664 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.17111647129058838, loss=1.8137564659118652
I0311 06:10:12.460304 140204246554368 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.1823682337999344, loss=1.8188023567199707
I0311 06:10:26.015236 140391735396160 spec.py:298] Evaluating on the training split.
I0311 06:10:28.996335 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 06:14:29.708518 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 06:14:32.355144 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 06:17:06.085922 140391735396160 spec.py:326] Evaluating on the test split.
I0311 06:17:08.772482 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 06:19:25.500179 140391735396160 submission_runner.py:359] Time since start: 15226.33s, 	Step: 24738, 	{'train/accuracy': 0.6413343548774719, 'train/loss': 1.7319859266281128, 'train/bleu': 30.84868003933824, 'validation/accuracy': 0.6577971577644348, 'validation/loss': 1.6136661767959595, 'validation/bleu': 28.0539440896339, 'validation/num_examples': 3000, 'test/accuracy': 0.6678287386894226, 'test/loss': 1.5361779928207397, 'test/bleu': 27.584999863271307, 'test/num_examples': 3003}
I0311 06:19:25.509861 140204238161664 logging_writer.py:48] [24738] global_step=24738, preemption_count=0, score=9237.494067, test/accuracy=0.667829, test/bleu=27.585000, test/loss=1.536178, test/num_examples=3003, total_duration=15226.334443, train/accuracy=0.641334, train/bleu=30.848680, train/loss=1.731986, validation/accuracy=0.657797, validation/bleu=28.053944, validation/loss=1.613666, validation/num_examples=3000
I0311 06:19:26.568756 140391735396160 checkpoints.py:356] Saving checkpoint at step: 24738
I0311 06:19:29.930475 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_24738
I0311 06:19:29.934094 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_24738.
I0311 06:19:53.437797 140204246554368 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.17853623628616333, loss=1.8798478841781616
I0311 06:20:30.744909 140204207798016 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.1788066029548645, loss=1.8380546569824219
I0311 06:21:08.062379 140204246554368 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.17251348495483398, loss=1.804282546043396
I0311 06:21:45.442847 140204207798016 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.2241201251745224, loss=1.838523507118225
I0311 06:22:22.817562 140204246554368 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.2637734115123749, loss=1.813484787940979
I0311 06:23:00.203622 140204207798016 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.19321183860301971, loss=1.869286298751831
I0311 06:23:37.611457 140204246554368 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.2826809287071228, loss=1.9078776836395264
I0311 06:24:14.989482 140204207798016 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.2400101274251938, loss=1.8284558057785034
I0311 06:24:52.352874 140204246554368 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.21554581820964813, loss=1.8975098133087158
I0311 06:25:29.714187 140204207798016 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.18945741653442383, loss=1.7881662845611572
I0311 06:26:07.076752 140204246554368 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.20778551697731018, loss=1.8608977794647217
I0311 06:26:44.427784 140204207798016 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.17570406198501587, loss=1.7939536571502686
I0311 06:27:21.787425 140204246554368 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.19856853783130646, loss=1.8413128852844238
I0311 06:27:59.153025 140204207798016 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.2189633548259735, loss=1.8250149488449097
I0311 06:28:36.520244 140204246554368 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.190810889005661, loss=1.7929458618164062
I0311 06:29:13.849139 140204207798016 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.1913953572511673, loss=1.857952356338501
I0311 06:29:51.192440 140204246554368 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.167410209774971, loss=1.823681354522705
I0311 06:30:28.566525 140204207798016 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.19242680072784424, loss=1.7884790897369385
I0311 06:31:05.951044 140204246554368 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.28842535614967346, loss=1.7399673461914062
I0311 06:31:43.417856 140204207798016 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.17094844579696655, loss=1.753885269165039
I0311 06:32:20.798484 140204246554368 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.1726103127002716, loss=1.814121961593628
I0311 06:32:58.135880 140204207798016 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.1885843127965927, loss=1.790582299232483
I0311 06:33:30.020487 140391735396160 spec.py:298] Evaluating on the training split.
I0311 06:33:33.017404 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 06:37:45.156186 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 06:37:47.804880 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 06:40:29.682803 140391735396160 spec.py:326] Evaluating on the test split.
I0311 06:40:32.367152 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 06:43:09.703441 140391735396160 submission_runner.py:359] Time since start: 16610.34s, 	Step: 26987, 	{'train/accuracy': 0.6406738758087158, 'train/loss': 1.7162266969680786, 'train/bleu': 31.007006662560997, 'validation/accuracy': 0.6594090461730957, 'validation/loss': 1.5988857746124268, 'validation/bleu': 28.17337801131836, 'validation/num_examples': 3000, 'test/accuracy': 0.6716518402099609, 'test/loss': 1.5258216857910156, 'test/bleu': 27.885028874070905, 'test/num_examples': 3003}
I0311 06:43:09.713742 140204246554368 logging_writer.py:48] [26987] global_step=26987, preemption_count=0, score=10074.080024, test/accuracy=0.671652, test/bleu=27.885029, test/loss=1.525822, test/num_examples=3003, total_duration=16610.339667, train/accuracy=0.640674, train/bleu=31.007007, train/loss=1.716227, validation/accuracy=0.659409, validation/bleu=28.173378, validation/loss=1.598886, validation/num_examples=3000
I0311 06:43:10.732220 140391735396160 checkpoints.py:356] Saving checkpoint at step: 26987
I0311 06:43:14.036655 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_26987
I0311 06:43:14.040276 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_26987.
I0311 06:43:19.266138 140204207798016 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.19253131747245789, loss=1.8019315004348755
I0311 06:43:56.543468 140204051543808 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.165559783577919, loss=1.831180214881897
I0311 06:44:33.861463 140204207798016 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.17555536329746246, loss=1.7740353345870972
I0311 06:45:11.182067 140204051543808 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.17861942946910858, loss=1.840421199798584
I0311 06:45:48.511174 140204207798016 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.18057604134082794, loss=1.8301609754562378
I0311 06:46:25.881300 140204051543808 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.18608902394771576, loss=1.8863974809646606
I0311 06:47:03.275807 140204207798016 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.180405855178833, loss=1.8254538774490356
I0311 06:47:40.646662 140204051543808 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.20650614798069, loss=1.8467024564743042
I0311 06:48:18.049860 140204207798016 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.2533372938632965, loss=1.7864881753921509
I0311 06:48:55.438292 140204051543808 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.200891375541687, loss=1.8632510900497437
I0311 06:49:32.830591 140204207798016 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.20378437638282776, loss=1.8374360799789429
I0311 06:50:10.179270 140204051543808 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.16948367655277252, loss=1.889649748802185
I0311 06:50:47.579003 140204207798016 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.1901084929704666, loss=1.7983644008636475
I0311 06:51:24.930989 140204051543808 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.1835777759552002, loss=1.8106639385223389
I0311 06:52:02.296123 140204207798016 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.17644861340522766, loss=1.7603782415390015
I0311 06:52:39.692053 140204051543808 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.21163180470466614, loss=1.7675747871398926
I0311 06:53:17.030898 140204207798016 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.18480056524276733, loss=1.7293891906738281
I0311 06:53:54.391242 140204051543808 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.1982445865869522, loss=1.882390022277832
I0311 06:54:31.749208 140204207798016 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1893603652715683, loss=1.7585604190826416
I0311 06:55:09.125720 140204051543808 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.17951160669326782, loss=1.7505228519439697
I0311 06:55:46.480583 140204207798016 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.19167475402355194, loss=1.7957127094268799
I0311 06:56:23.897119 140204051543808 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.21133758127689362, loss=1.7410792112350464
I0311 06:57:01.303883 140204207798016 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.22380277514457703, loss=1.7788491249084473
I0311 06:57:14.095649 140391735396160 spec.py:298] Evaluating on the training split.
I0311 06:57:17.086052 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 07:01:05.396256 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 07:01:08.039901 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 07:03:52.159996 140391735396160 spec.py:326] Evaluating on the test split.
I0311 07:03:54.854564 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 07:06:43.525432 140391735396160 submission_runner.py:359] Time since start: 18034.41s, 	Step: 29236, 	{'train/accuracy': 0.6400923132896423, 'train/loss': 1.7278815507888794, 'train/bleu': 31.178232460447017, 'validation/accuracy': 0.659508228302002, 'validation/loss': 1.5913872718811035, 'validation/bleu': 28.187521796185962, 'validation/num_examples': 3000, 'test/accuracy': 0.6713381409645081, 'test/loss': 1.5129661560058594, 'test/bleu': 27.57593607823483, 'test/num_examples': 3003}
I0311 07:06:43.535940 140204051543808 logging_writer.py:48] [29236] global_step=29236, preemption_count=0, score=10910.637886, test/accuracy=0.671338, test/bleu=27.575936, test/loss=1.512966, test/num_examples=3003, total_duration=18034.414825, train/accuracy=0.640092, train/bleu=31.178232, train/loss=1.727882, validation/accuracy=0.659508, validation/bleu=28.187522, validation/loss=1.591387, validation/num_examples=3000
I0311 07:06:44.540639 140391735396160 checkpoints.py:356] Saving checkpoint at step: 29236
I0311 07:06:48.350390 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_29236
I0311 07:06:48.353967 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_29236.
I0311 07:07:12.572261 140204207798016 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.19938784837722778, loss=1.7984098196029663
I0311 07:07:49.827687 140204043151104 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.18301305174827576, loss=1.7838413715362549
I0311 07:08:27.160646 140204207798016 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.1873859167098999, loss=1.7551085948944092
I0311 07:09:04.482651 140204043151104 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.26453864574432373, loss=1.7479971647262573
I0311 07:09:41.818374 140204207798016 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.19232560694217682, loss=1.8101602792739868
I0311 07:10:19.197748 140204043151104 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.0633950233459473, loss=1.748775839805603
I0311 07:10:56.532092 140204207798016 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.1866832971572876, loss=1.7088289260864258
I0311 07:11:33.935049 140204043151104 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.189985990524292, loss=1.8163669109344482
I0311 07:12:11.307630 140204207798016 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.1817798614501953, loss=1.8254328966140747
I0311 07:12:48.684208 140204043151104 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.1860295683145523, loss=1.8062465190887451
I0311 07:13:26.050586 140204207798016 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.18604670464992523, loss=1.8106169700622559
I0311 07:14:03.463888 140204043151104 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.25924646854400635, loss=1.7787150144577026
I0311 07:14:40.833336 140204207798016 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.19984014332294464, loss=1.788372278213501
I0311 07:15:18.219887 140204043151104 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.19925476610660553, loss=1.7200336456298828
I0311 07:15:55.588727 140204207798016 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.18978509306907654, loss=1.776503324508667
I0311 07:16:32.993958 140204043151104 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.24962720274925232, loss=1.7863014936447144
I0311 07:17:10.392770 140204207798016 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.18362706899642944, loss=1.7529083490371704
I0311 07:17:47.746310 140204043151104 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.2128700166940689, loss=1.8381049633026123
I0311 07:18:25.170773 140204207798016 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.18064656853675842, loss=1.8145865201950073
I0311 07:19:02.552134 140204043151104 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.2078658640384674, loss=1.774897813796997
I0311 07:19:39.923014 140204207798016 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.19836857914924622, loss=1.9047280550003052
I0311 07:20:17.334533 140204043151104 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.2010495662689209, loss=1.8183156251907349
I0311 07:20:48.428345 140391735396160 spec.py:298] Evaluating on the training split.
I0311 07:20:51.408702 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 07:24:29.440313 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 07:24:32.076222 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 07:26:56.703395 140391735396160 spec.py:326] Evaluating on the test split.
I0311 07:26:59.389640 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 07:29:04.482757 140391735396160 submission_runner.py:359] Time since start: 19448.75s, 	Step: 31485, 	{'train/accuracy': 0.6648632884025574, 'train/loss': 1.5574485063552856, 'train/bleu': 32.579745030489526, 'validation/accuracy': 0.6617896556854248, 'validation/loss': 1.5763901472091675, 'validation/bleu': 28.263989181706112, 'validation/num_examples': 3000, 'test/accuracy': 0.6717564463615417, 'test/loss': 1.505411982536316, 'test/bleu': 27.771302944509074, 'test/num_examples': 3003}
I0311 07:29:04.494998 140204207798016 logging_writer.py:48] [31485] global_step=31485, preemption_count=0, score=11747.211518, test/accuracy=0.671756, test/bleu=27.771303, test/loss=1.505412, test/num_examples=3003, total_duration=19448.747543, train/accuracy=0.664863, train/bleu=32.579745, train/loss=1.557449, validation/accuracy=0.661790, validation/bleu=28.263989, validation/loss=1.576390, validation/num_examples=3000
I0311 07:29:05.505501 140391735396160 checkpoints.py:356] Saving checkpoint at step: 31485
I0311 07:29:09.949354 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_31485
I0311 07:29:09.953056 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_31485.
I0311 07:29:15.923640 140204043151104 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.17483839392662048, loss=1.7708526849746704
I0311 07:29:53.191684 140204034758400 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.19123908877372742, loss=1.795376181602478
I0311 07:30:30.500515 140204043151104 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.19236721098423004, loss=1.8365792036056519
I0311 07:31:07.837258 140204034758400 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.23118439316749573, loss=1.7876907587051392
I0311 07:31:45.216693 140204043151104 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.20432798564434052, loss=1.7873711585998535
I0311 07:32:22.558084 140204034758400 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.2749762237071991, loss=1.72498619556427
I0311 07:32:59.894881 140204043151104 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.19438600540161133, loss=1.8600094318389893
I0311 07:33:37.265092 140204034758400 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.22943297028541565, loss=1.8491709232330322
I0311 07:34:14.628114 140204043151104 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.2397087812423706, loss=1.813198208808899
I0311 07:34:52.001387 140204034758400 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.24822111427783966, loss=1.805086612701416
I0311 07:35:29.334976 140204043151104 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.1859007030725479, loss=1.8220995664596558
I0311 07:36:06.719504 140204034758400 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.1984686255455017, loss=1.8371537923812866
I0311 07:36:44.107732 140204043151104 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.2032371163368225, loss=1.7582706212997437
I0311 07:37:21.527061 140204034758400 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.2240433394908905, loss=1.829613447189331
I0311 07:37:58.835416 140204043151104 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.20486214756965637, loss=1.756783127784729
I0311 07:38:36.144402 140204034758400 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.20238742232322693, loss=1.891505479812622
I0311 07:39:13.557840 140204043151104 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.19865384697914124, loss=1.7374024391174316
I0311 07:39:50.926415 140204034758400 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.2150030881166458, loss=1.7984583377838135
I0311 07:40:28.258357 140204043151104 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.20036861300468445, loss=1.8112143278121948
I0311 07:41:05.639612 140204034758400 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.4273794889450073, loss=1.7497856616973877
I0311 07:41:43.041608 140204043151104 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.1883288323879242, loss=1.732551097869873
I0311 07:42:20.428884 140204034758400 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.17314286530017853, loss=1.7287342548370361
I0311 07:42:57.800888 140204043151104 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.2153051346540451, loss=1.7368959188461304
I0311 07:43:10.242465 140391735396160 spec.py:298] Evaluating on the training split.
I0311 07:43:13.226975 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 07:47:23.382908 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 07:47:26.034888 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 07:50:22.656139 140391735396160 spec.py:326] Evaluating on the test split.
I0311 07:50:25.337801 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 07:53:49.395233 140391735396160 submission_runner.py:359] Time since start: 20790.56s, 	Step: 33735, 	{'train/accuracy': 0.6436406970024109, 'train/loss': 1.7011619806289673, 'train/bleu': 31.781315805573833, 'validation/accuracy': 0.6623600125312805, 'validation/loss': 1.5663892030715942, 'validation/bleu': 27.83605423550204, 'validation/num_examples': 3000, 'test/accuracy': 0.6760560274124146, 'test/loss': 1.486278772354126, 'test/bleu': 27.969794197444912, 'test/num_examples': 3003}
I0311 07:53:49.406428 140204034758400 logging_writer.py:48] [33735] global_step=33735, preemption_count=0, score=12584.025842, test/accuracy=0.676056, test/bleu=27.969794, test/loss=1.486279, test/num_examples=3003, total_duration=20790.561651, train/accuracy=0.643641, train/bleu=31.781316, train/loss=1.701162, validation/accuracy=0.662360, validation/bleu=27.836054, validation/loss=1.566389, validation/num_examples=3000
I0311 07:53:50.435781 140391735396160 checkpoints.py:356] Saving checkpoint at step: 33735
I0311 07:53:54.101106 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_33735
I0311 07:53:54.104706 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_33735.
I0311 07:54:18.687672 140204043151104 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.19002114236354828, loss=1.7646057605743408
I0311 07:54:56.023156 140204026365696 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.19168265163898468, loss=1.859708547592163
I0311 07:55:33.329535 140204043151104 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.2066793441772461, loss=1.8831619024276733
I0311 07:56:10.670091 140204026365696 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.2253195196390152, loss=1.7505261898040771
I0311 07:56:48.013139 140204043151104 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.22259771823883057, loss=1.810901403427124
I0311 07:57:25.355939 140204026365696 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.1853170543909073, loss=1.819726586341858
I0311 07:58:02.700430 140204043151104 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.1821277141571045, loss=1.8248564004898071
I0311 07:58:40.052173 140204026365696 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.1786874234676361, loss=1.794138789176941
I0311 07:59:17.417521 140204043151104 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.2224172204732895, loss=1.7941235303878784
I0311 07:59:54.794697 140204026365696 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.19115813076496124, loss=1.873504877090454
I0311 08:00:32.158275 140204043151104 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.18250788748264313, loss=1.7154899835586548
I0311 08:01:09.519861 140204026365696 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.19254831969738007, loss=1.696013331413269
I0311 08:01:46.899967 140204043151104 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.18053585290908813, loss=1.7009159326553345
I0311 08:02:24.269686 140204026365696 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.1733769327402115, loss=1.791711449623108
I0311 08:03:01.670911 140204043151104 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.21782678365707397, loss=1.8562510013580322
I0311 08:03:39.067596 140204026365696 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.17734003067016602, loss=1.715430736541748
I0311 08:04:16.467071 140204043151104 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.19512008130550385, loss=1.75801682472229
I0311 08:04:53.850891 140204026365696 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.18673862516880035, loss=1.8562768697738647
I0311 08:05:31.234869 140204043151104 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.20113389194011688, loss=1.732572078704834
I0311 08:06:08.640487 140204026365696 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.19107308983802795, loss=1.7485913038253784
I0311 08:06:46.071351 140204043151104 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.18150848150253296, loss=1.7956264019012451
I0311 08:07:23.397467 140204026365696 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.1945699006319046, loss=1.7848315238952637
I0311 08:07:54.152942 140391735396160 spec.py:298] Evaluating on the training split.
I0311 08:07:57.133405 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 08:11:04.036528 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 08:11:06.685235 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 08:13:23.862509 140391735396160 spec.py:326] Evaluating on the test split.
I0311 08:13:26.546824 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 08:15:39.870893 140391735396160 submission_runner.py:359] Time since start: 22274.47s, 	Step: 35984, 	{'train/accuracy': 0.6443754434585571, 'train/loss': 1.695351481437683, 'train/bleu': 31.717029992736872, 'validation/accuracy': 0.6647034883499146, 'validation/loss': 1.5625381469726562, 'validation/bleu': 28.408022996133173, 'validation/num_examples': 3000, 'test/accuracy': 0.6777991056442261, 'test/loss': 1.4802703857421875, 'test/bleu': 27.895970801827712, 'test/num_examples': 3003}
I0311 08:15:39.881963 140204043151104 logging_writer.py:48] [35984] global_step=35984, preemption_count=0, score=13420.586697, test/accuracy=0.677799, test/bleu=27.895971, test/loss=1.480270, test/num_examples=3003, total_duration=22274.472133, train/accuracy=0.644375, train/bleu=31.717030, train/loss=1.695351, validation/accuracy=0.664703, validation/bleu=28.408023, validation/loss=1.562538, validation/num_examples=3000
I0311 08:15:40.926121 140391735396160 checkpoints.py:356] Saving checkpoint at step: 35984
I0311 08:15:44.309531 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_35984
I0311 08:15:44.313101 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_35984.
I0311 08:15:50.661630 140204026365696 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.21570581197738647, loss=1.6931709051132202
I0311 08:16:27.939233 140204017972992 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.21211974322795868, loss=1.7254862785339355
I0311 08:17:05.233878 140204026365696 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.26347607374191284, loss=1.7851223945617676
I0311 08:17:42.520667 140204017972992 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.18150706589221954, loss=1.706374168395996
I0311 08:18:19.877191 140204026365696 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.2093007117509842, loss=1.8264390230178833
I0311 08:18:57.235643 140204017972992 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.20256318151950836, loss=1.799850344657898
I0311 08:19:34.602686 140204026365696 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.38811543583869934, loss=1.707990288734436
I0311 08:20:11.956785 140204017972992 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.27500513195991516, loss=1.8452517986297607
I0311 08:20:49.379928 140204026365696 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.17641110718250275, loss=1.7576934099197388
I0311 08:21:26.716102 140204017972992 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.19687867164611816, loss=1.7439976930618286
I0311 08:22:04.115203 140204026365696 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.17408901453018188, loss=1.653412103652954
I0311 08:22:41.501540 140204017972992 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.1761922538280487, loss=1.7470985651016235
I0311 08:23:18.877589 140204026365696 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.2074313461780548, loss=1.7999969720840454
I0311 08:23:56.262239 140204017972992 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.1995658427476883, loss=1.7159324884414673
I0311 08:24:33.631936 140204026365696 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.22433915734291077, loss=1.7739366292953491
I0311 08:25:11.024296 140204017972992 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.18419477343559265, loss=1.7954199314117432
I0311 08:25:48.413118 140204026365696 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.18072950839996338, loss=1.7731750011444092
I0311 08:26:25.858218 140204017972992 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.19957533478736877, loss=1.7477574348449707
I0311 08:27:03.267669 140204026365696 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.1816599816083908, loss=1.7566689252853394
I0311 08:27:40.648525 140204017972992 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.19027747213840485, loss=1.7817409038543701
I0311 08:28:18.028279 140204026365696 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.19091856479644775, loss=1.781293511390686
I0311 08:28:55.396295 140204017972992 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.21370665729045868, loss=1.735743761062622
I0311 08:29:32.774459 140204026365696 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.2078680694103241, loss=1.8398113250732422
I0311 08:29:44.461551 140391735396160 spec.py:298] Evaluating on the training split.
I0311 08:29:47.444638 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 08:32:42.606881 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 08:32:45.243947 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 08:35:03.011067 140391735396160 spec.py:326] Evaluating on the test split.
I0311 08:35:05.697902 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 08:37:24.182881 140391735396160 submission_runner.py:359] Time since start: 23584.78s, 	Step: 38233, 	{'train/accuracy': 0.6534589529037476, 'train/loss': 1.6294481754302979, 'train/bleu': 32.608645923433365, 'validation/accuracy': 0.6665137410163879, 'validation/loss': 1.556545376777649, 'validation/bleu': 28.72387899019524, 'validation/num_examples': 3000, 'test/accuracy': 0.6777061223983765, 'test/loss': 1.4755134582519531, 'test/bleu': 27.96578900584244, 'test/num_examples': 3003}
I0311 08:37:24.195080 140204017972992 logging_writer.py:48] [38233] global_step=38233, preemption_count=0, score=14257.268062, test/accuracy=0.677706, test/bleu=27.965789, test/loss=1.475513, test/num_examples=3003, total_duration=23584.780749, train/accuracy=0.653459, train/bleu=32.608646, train/loss=1.629448, validation/accuracy=0.666514, validation/bleu=28.723879, validation/loss=1.556545, validation/num_examples=3000
I0311 08:37:25.445888 140391735396160 checkpoints.py:356] Saving checkpoint at step: 38233
I0311 08:37:30.327760 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_38233
I0311 08:37:30.332272 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_38233.
I0311 08:37:55.717276 140204026365696 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.21747790277004242, loss=1.7273542881011963
I0311 08:38:33.059497 140204009580288 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.25885504484176636, loss=1.834743618965149
I0311 08:39:10.422007 140204026365696 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.2064511626958847, loss=1.6720125675201416
I0311 08:39:47.783867 140204009580288 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.20418381690979004, loss=1.7522865533828735
I0311 08:40:25.204084 140204026365696 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.20956960320472717, loss=1.7176414728164673
I0311 08:41:02.563674 140204009580288 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.21726179122924805, loss=1.771525502204895
I0311 08:41:39.968663 140204026365696 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.2309664636850357, loss=1.7091447114944458
I0311 08:42:17.339870 140204009580288 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.2445782721042633, loss=1.767143964767456
I0311 08:42:54.694724 140204026365696 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.30061399936676025, loss=1.7219867706298828
I0311 08:43:32.128837 140204009580288 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.18794290721416473, loss=1.7866078615188599
I0311 08:44:09.456904 140204026365696 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.20516148209571838, loss=1.8269249200820923
I0311 08:44:46.839643 140204009580288 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.20419009029865265, loss=1.7404766082763672
I0311 08:45:24.210967 140204026365696 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.2120940387248993, loss=1.810762643814087
I0311 08:46:01.616374 140204009580288 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.1804955005645752, loss=1.6741302013397217
I0311 08:46:38.989115 140204026365696 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.19129815697669983, loss=1.7014883756637573
I0311 08:47:16.331148 140204009580288 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.1899188756942749, loss=1.6390066146850586
I0311 08:47:53.671505 140204026365696 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.18475539982318878, loss=1.7572063207626343
I0311 08:48:30.996271 140204009580288 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.20141002535820007, loss=1.6584417819976807
I0311 08:49:08.359708 140204026365696 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.201288104057312, loss=1.6394555568695068
I0311 08:49:45.722689 140204009580288 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.20405122637748718, loss=1.7433950901031494
I0311 08:50:23.117962 140204026365696 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.18806110322475433, loss=1.78220796585083
I0311 08:51:00.473390 140204009580288 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.18701884150505066, loss=1.7397316694259644
I0311 08:51:30.449453 140391735396160 spec.py:298] Evaluating on the training split.
I0311 08:51:33.433104 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 08:55:31.085011 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 08:55:33.730726 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 08:59:07.303257 140391735396160 spec.py:326] Evaluating on the test split.
I0311 08:59:10.018962 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 09:02:42.207193 140391735396160 submission_runner.py:359] Time since start: 24890.77s, 	Step: 40482, 	{'train/accuracy': 0.6554723381996155, 'train/loss': 1.6399073600769043, 'train/bleu': 31.856214891381747, 'validation/accuracy': 0.6672700643539429, 'validation/loss': 1.5479999780654907, 'validation/bleu': 28.651235426790976, 'validation/num_examples': 3000, 'test/accuracy': 0.6831561326980591, 'test/loss': 1.4542810916900635, 'test/bleu': 28.669700930269222, 'test/num_examples': 3003}
I0311 09:02:42.218421 140204026365696 logging_writer.py:48] [40482] global_step=40482, preemption_count=0, score=15093.890905, test/accuracy=0.683156, test/bleu=28.669701, test/loss=1.454281, test/num_examples=3003, total_duration=24890.768648, train/accuracy=0.655472, train/bleu=31.856215, train/loss=1.639907, validation/accuracy=0.667270, validation/bleu=28.651235, validation/loss=1.548000, validation/num_examples=3000
I0311 09:02:43.252761 140391735396160 checkpoints.py:356] Saving checkpoint at step: 40482
I0311 09:02:46.634737 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_40482
I0311 09:02:46.638453 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_40482.
I0311 09:02:53.728902 140204009580288 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.23498588800430298, loss=1.7253330945968628
I0311 09:03:30.941925 140204001187584 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.2695960998535156, loss=1.6759438514709473
I0311 09:04:08.262911 140204009580288 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.18943381309509277, loss=1.724821925163269
I0311 09:04:45.597613 140204001187584 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.21648889780044556, loss=1.769188404083252
I0311 09:05:22.957084 140204009580288 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.18313376605510712, loss=1.7651673555374146
I0311 09:06:00.319291 140204001187584 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.23391388356685638, loss=1.7639871835708618
I0311 09:06:37.680629 140204009580288 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.18625208735466003, loss=1.8050785064697266
I0311 09:07:15.060961 140204001187584 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.19232311844825745, loss=1.7527203559875488
I0311 09:07:52.427467 140204009580288 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.19493575394153595, loss=1.6455271244049072
I0311 09:08:29.776048 140204001187584 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.1891300231218338, loss=1.7991704940795898
I0311 09:09:07.167420 140204009580288 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.1999107003211975, loss=1.7811349630355835
I0311 09:09:44.535497 140204001187584 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.18127351999282837, loss=1.7846038341522217
I0311 09:10:21.924039 140204009580288 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.18832187354564667, loss=1.7369017601013184
I0311 09:10:59.305410 140204001187584 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.18847493827342987, loss=1.7219209671020508
I0311 09:11:36.700973 140204009580288 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.19383575022220612, loss=1.7632904052734375
I0311 09:12:14.039953 140204001187584 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.1948365569114685, loss=1.6678494215011597
I0311 09:12:51.399046 140204009580288 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.20032654702663422, loss=1.7882797718048096
I0311 09:13:28.761804 140204001187584 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.18242385983467102, loss=1.7260175943374634
I0311 09:14:06.103370 140204009580288 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.1961200088262558, loss=1.7609773874282837
I0311 09:14:43.476436 140204001187584 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.19929541647434235, loss=1.7166391611099243
I0311 09:15:20.871496 140204009580288 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.18449361622333527, loss=1.7495390176773071
I0311 09:15:58.228954 140204001187584 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.2434428483247757, loss=1.7054401636123657
I0311 09:16:35.626628 140204009580288 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.18278902769088745, loss=1.8261808156967163
I0311 09:16:46.931847 140391735396160 spec.py:298] Evaluating on the training split.
I0311 09:16:49.932989 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 09:20:09.275655 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 09:20:11.926680 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 09:22:56.206608 140391735396160 spec.py:326] Evaluating on the test split.
I0311 09:22:58.900227 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 09:25:15.721819 140391735396160 submission_runner.py:359] Time since start: 26407.25s, 	Step: 42732, 	{'train/accuracy': 0.6502752304077148, 'train/loss': 1.6726518869400024, 'train/bleu': 31.84263952624437, 'validation/accuracy': 0.669563889503479, 'validation/loss': 1.5316383838653564, 'validation/bleu': 28.921991540365624, 'validation/num_examples': 3000, 'test/accuracy': 0.6807622909545898, 'test/loss': 1.448974847793579, 'test/bleu': 28.322028220423057, 'test/num_examples': 3003}
I0311 09:25:15.733132 140204001187584 logging_writer.py:48] [42732] global_step=42732, preemption_count=0, score=15930.704853, test/accuracy=0.680762, test/bleu=28.322028, test/loss=1.448975, test/num_examples=3003, total_duration=26407.251031, train/accuracy=0.650275, train/bleu=31.842640, train/loss=1.672652, validation/accuracy=0.669564, validation/bleu=28.921992, validation/loss=1.531638, validation/num_examples=3000
I0311 09:25:16.745911 140391735396160 checkpoints.py:356] Saving checkpoint at step: 42732
I0311 09:25:20.392842 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_42732
I0311 09:25:20.396415 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_42732.
I0311 09:25:46.101300 140204009580288 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.20494699478149414, loss=1.7734562158584595
I0311 09:26:23.386060 140203992794880 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.19549225270748138, loss=1.7525609731674194
I0311 09:27:00.692977 140204009580288 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.20973826944828033, loss=1.754940390586853
I0311 09:27:38.096848 140203992794880 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.1948680579662323, loss=1.7342543601989746
I0311 09:28:15.464349 140204009580288 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.22211088240146637, loss=1.7147337198257446
I0311 09:28:52.838868 140203992794880 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.20131336152553558, loss=1.6693968772888184
I0311 09:29:30.230520 140204009580288 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.18931415677070618, loss=1.7715448141098022
I0311 09:30:07.611469 140203992794880 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.2611856460571289, loss=1.7800016403198242
I0311 09:30:44.968176 140204009580288 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.20361827313899994, loss=1.7170981168746948
I0311 09:31:22.395638 140203992794880 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.20266656577587128, loss=1.7806950807571411
I0311 09:31:59.784658 140204009580288 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.2044559121131897, loss=1.815399169921875
I0311 09:32:37.140038 140203992794880 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.26312917470932007, loss=1.7043836116790771
I0311 09:33:14.497158 140204009580288 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.1841593235731125, loss=1.6803921461105347
I0311 09:33:51.905995 140203992794880 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.18440212309360504, loss=1.7180742025375366
I0311 09:34:29.290973 140204009580288 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.18902850151062012, loss=1.6451387405395508
I0311 09:35:06.651456 140203992794880 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.19202998280525208, loss=1.684065818786621
I0311 09:35:44.025598 140204009580288 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.18716812133789062, loss=1.6739799976348877
I0311 09:36:21.398707 140203992794880 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.19353768229484558, loss=1.8294991254806519
I0311 09:36:58.785105 140204009580288 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.18346017599105835, loss=1.7028895616531372
I0311 09:37:36.151240 140203992794880 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.20328444242477417, loss=1.757328987121582
I0311 09:38:13.595946 140204009580288 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.18171390891075134, loss=1.7072014808654785
I0311 09:38:50.991492 140203992794880 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.17232190072536469, loss=1.6954706907272339
I0311 09:39:20.637550 140391735396160 spec.py:298] Evaluating on the training split.
I0311 09:39:23.623226 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 09:43:31.354672 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 09:43:34.015177 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 09:46:25.951890 140391735396160 spec.py:326] Evaluating on the test split.
I0311 09:46:28.650809 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 09:49:08.745789 140391735396160 submission_runner.py:359] Time since start: 27760.96s, 	Step: 44981, 	{'train/accuracy': 0.6607688665390015, 'train/loss': 1.588027834892273, 'train/bleu': 32.24235917820795, 'validation/accuracy': 0.6691919565200806, 'validation/loss': 1.5278775691986084, 'validation/bleu': 28.655200953666778, 'validation/num_examples': 3000, 'test/accuracy': 0.6833072304725647, 'test/loss': 1.442453384399414, 'test/bleu': 28.69376492302118, 'test/num_examples': 3003}
I0311 09:49:08.758651 140204009580288 logging_writer.py:48] [44981] global_step=44981, preemption_count=0, score=16767.339332, test/accuracy=0.683307, test/bleu=28.693765, test/loss=1.442453, test/num_examples=3003, total_duration=27760.956735, train/accuracy=0.660769, train/bleu=32.242359, train/loss=1.588028, validation/accuracy=0.669192, validation/bleu=28.655201, validation/loss=1.527878, validation/num_examples=3000
I0311 09:49:10.029888 140391735396160 checkpoints.py:356] Saving checkpoint at step: 44981
I0311 09:49:13.725740 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_44981
I0311 09:49:13.730270 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_44981.
I0311 09:49:21.205192 140203992794880 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.20250943303108215, loss=1.7183489799499512
I0311 09:49:58.468782 140203984402176 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.1930784285068512, loss=1.8253241777420044
I0311 09:50:35.790587 140203992794880 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.19852066040039062, loss=1.6376830339431763
I0311 09:51:13.158299 140203984402176 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.221955344080925, loss=1.7271631956100464
I0311 09:51:50.538099 140203992794880 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.1935758888721466, loss=1.7123609781265259
I0311 09:52:27.921517 140203984402176 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.19897621870040894, loss=1.7326250076293945
I0311 09:53:05.280661 140203992794880 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.20697170495986938, loss=1.8279211521148682
I0311 09:53:42.650389 140203984402176 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.195005863904953, loss=1.662570595741272
I0311 09:54:20.031191 140203992794880 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.20793020725250244, loss=1.750269889831543
I0311 09:54:57.424486 140203984402176 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.18833661079406738, loss=1.7651925086975098
I0311 09:55:34.767268 140203992794880 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.19248755276203156, loss=1.6825275421142578
I0311 09:56:12.169556 140203984402176 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.211959108710289, loss=1.7358421087265015
I0311 09:56:49.546275 140203992794880 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.18714821338653564, loss=1.6639667749404907
I0311 09:57:26.905282 140203984402176 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.1861795037984848, loss=1.733776330947876
I0311 09:58:04.289499 140203992794880 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.1922650784254074, loss=1.6891742944717407
I0311 09:58:41.658757 140203984402176 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.18511690199375153, loss=1.7107264995574951
I0311 09:59:19.021095 140203992794880 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.1739838421344757, loss=1.7399218082427979
I0311 09:59:56.420027 140203984402176 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.22097481787204742, loss=1.7438958883285522
I0311 10:00:33.797502 140203992794880 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.22829444706439972, loss=1.7100366353988647
I0311 10:01:11.121658 140203984402176 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.19239170849323273, loss=1.7305020093917847
I0311 10:01:48.473786 140203992794880 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.21109741926193237, loss=1.6368211507797241
I0311 10:02:25.802185 140203984402176 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.2109929621219635, loss=1.7185453176498413
I0311 10:03:03.176923 140203992794880 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.1864224374294281, loss=1.7264248132705688
I0311 10:03:14.100928 140391735396160 spec.py:298] Evaluating on the training split.
I0311 10:03:17.096087 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 10:07:40.554882 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 10:07:43.203045 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 10:11:30.949003 140391735396160 spec.py:326] Evaluating on the test split.
I0311 10:11:33.643107 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 10:14:58.668807 140391735396160 submission_runner.py:359] Time since start: 29194.42s, 	Step: 47231, 	{'train/accuracy': 0.6550760865211487, 'train/loss': 1.6239455938339233, 'train/bleu': 32.094072917945915, 'validation/accuracy': 0.6710889935493469, 'validation/loss': 1.513609528541565, 'validation/bleu': 28.093779698567847, 'validation/num_examples': 3000, 'test/accuracy': 0.6857242584228516, 'test/loss': 1.427291989326477, 'test/bleu': 28.682205889901244, 'test/num_examples': 3003}
I0311 10:14:58.680555 140203984402176 logging_writer.py:48] [47231] global_step=47231, preemption_count=0, score=17604.170595, test/accuracy=0.685724, test/bleu=28.682206, test/loss=1.427292, test/num_examples=3003, total_duration=29194.420136, train/accuracy=0.655076, train/bleu=32.094073, train/loss=1.623946, validation/accuracy=0.671089, validation/bleu=28.093780, validation/loss=1.513610, validation/num_examples=3000
I0311 10:14:59.704436 140391735396160 checkpoints.py:356] Saving checkpoint at step: 47231
I0311 10:15:03.744576 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_47231
I0311 10:15:03.748319 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_47231.
I0311 10:15:29.835504 140203992794880 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.19955886900424957, loss=1.7473739385604858
I0311 10:16:07.080876 140203976009472 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.20348960161209106, loss=1.6935231685638428
I0311 10:16:44.381322 140203992794880 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.22836194932460785, loss=1.6654913425445557
I0311 10:17:21.717972 140203976009472 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.19180674850940704, loss=1.642493486404419
I0311 10:17:59.077968 140203992794880 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.19797980785369873, loss=1.6751559972763062
I0311 10:18:36.484622 140203976009472 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.2022782415151596, loss=1.6883816719055176
I0311 10:19:13.860480 140203992794880 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.23292163014411926, loss=1.6775641441345215
I0311 10:19:51.198887 140203976009472 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.19642335176467896, loss=1.679988145828247
I0311 10:20:28.546296 140203992794880 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.22502703964710236, loss=1.7341848611831665
I0311 10:21:05.933047 140203976009472 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.18916887044906616, loss=1.7713305950164795
I0311 10:21:43.314895 140203992794880 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.18314550817012787, loss=1.7187122106552124
I0311 10:22:20.682627 140203976009472 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.20608320832252502, loss=1.6350351572036743
I0311 10:22:58.001674 140203992794880 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.18439725041389465, loss=1.7334891557693481
I0311 10:23:35.389922 140203976009472 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.220923513174057, loss=1.7628446817398071
I0311 10:24:12.748636 140203992794880 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.2541515529155731, loss=1.7464922666549683
I0311 10:24:50.077965 140203976009472 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.19328497350215912, loss=1.7430946826934814
I0311 10:25:27.510457 140203992794880 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.19118916988372803, loss=1.7688210010528564
I0311 10:26:04.907352 140203976009472 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.2242414355278015, loss=1.6505210399627686
I0311 10:26:42.289158 140203992794880 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.18209147453308105, loss=1.6929783821105957
I0311 10:27:19.697144 140203976009472 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.18081820011138916, loss=1.6973859071731567
I0311 10:27:57.045930 140203992794880 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.1848318725824356, loss=1.8018337488174438
I0311 10:28:34.447951 140203976009472 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.18100017309188843, loss=1.6601911783218384
I0311 10:29:04.057869 140391735396160 spec.py:298] Evaluating on the training split.
I0311 10:29:07.041996 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 10:33:26.049709 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 10:33:28.691907 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 10:37:10.223450 140391735396160 spec.py:326] Evaluating on the test split.
I0311 10:37:12.905063 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 10:40:36.435479 140391735396160 submission_runner.py:359] Time since start: 30744.38s, 	Step: 49481, 	{'train/accuracy': 0.6547921895980835, 'train/loss': 1.6410382986068726, 'train/bleu': 32.40762972447454, 'validation/accuracy': 0.6735192537307739, 'validation/loss': 1.5032299757003784, 'validation/bleu': 28.96508232715672, 'validation/num_examples': 3000, 'test/accuracy': 0.6866771578788757, 'test/loss': 1.4205994606018066, 'test/bleu': 28.64766000569732, 'test/num_examples': 3003}
I0311 10:40:36.447514 140203992794880 logging_writer.py:48] [49481] global_step=49481, preemption_count=0, score=18441.014641, test/accuracy=0.686677, test/bleu=28.647660, test/loss=1.420599, test/num_examples=3003, total_duration=30744.377077, train/accuracy=0.654792, train/bleu=32.407630, train/loss=1.641038, validation/accuracy=0.673519, validation/bleu=28.965082, validation/loss=1.503230, validation/num_examples=3000
I0311 10:40:37.459201 140391735396160 checkpoints.py:356] Saving checkpoint at step: 49481
I0311 10:40:40.786495 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_49481
I0311 10:40:40.790046 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_49481.
I0311 10:40:48.239778 140203976009472 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.216323122382164, loss=1.708487629890442
I0311 10:41:25.464383 140203967616768 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.22504909336566925, loss=1.789749026298523
I0311 10:42:02.727885 140203976009472 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.1826164573431015, loss=1.6780095100402832
I0311 10:42:40.013462 140203967616768 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.19127030670642853, loss=1.658021092414856
I0311 10:43:17.335967 140203976009472 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.2022441029548645, loss=1.7149428129196167
I0311 10:43:54.721592 140203967616768 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.18252143263816833, loss=1.6809953451156616
I0311 10:44:32.088844 140203976009472 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.21345233917236328, loss=1.6432039737701416
I0311 10:45:09.426204 140203967616768 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.2040003389120102, loss=1.6248583793640137
I0311 10:45:46.769473 140203976009472 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.2792518734931946, loss=1.6974762678146362
I0311 10:46:24.163688 140203967616768 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.18979069590568542, loss=1.7688963413238525
I0311 10:47:01.543664 140203976009472 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.18986789882183075, loss=1.7278896570205688
I0311 10:47:38.979270 140203967616768 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.18496283888816833, loss=1.6528217792510986
I0311 10:48:16.327073 140203976009472 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.2164679616689682, loss=1.7333074808120728
I0311 10:48:53.708946 140203967616768 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.24707546830177307, loss=1.7111343145370483
I0311 10:49:31.044019 140203976009472 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.2163364738225937, loss=1.7357985973358154
I0311 10:50:08.453536 140203967616768 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.20475758612155914, loss=1.6768423318862915
I0311 10:50:45.810709 140203976009472 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.2125585526227951, loss=1.6806564331054688
I0311 10:51:23.165440 140203967616768 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.21007953584194183, loss=1.6603755950927734
I0311 10:52:00.530580 140203976009472 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.19395796954631805, loss=1.7047162055969238
I0311 10:52:37.853988 140203967616768 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.21023797988891602, loss=1.7026253938674927
I0311 10:53:15.236000 140203976009472 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.19751763343811035, loss=1.6896699666976929
I0311 10:53:52.642093 140203967616768 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.19026918709278107, loss=1.7106908559799194
I0311 10:54:29.974149 140203976009472 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.20313307642936707, loss=1.7070785760879517
I0311 10:54:40.877238 140391735396160 spec.py:298] Evaluating on the training split.
I0311 10:54:43.852105 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 10:58:33.494144 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 10:58:36.133628 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 11:01:18.714032 140391735396160 spec.py:326] Evaluating on the test split.
I0311 11:01:21.405990 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 11:03:56.720331 140391735396160 submission_runner.py:359] Time since start: 32281.20s, 	Step: 51731, 	{'train/accuracy': 0.6619307398796082, 'train/loss': 1.5822252035140991, 'train/bleu': 32.746873329437925, 'validation/accuracy': 0.6737672090530396, 'validation/loss': 1.49741530418396, 'validation/bleu': 29.232165430645612, 'validation/num_examples': 3000, 'test/accuracy': 0.6865028142929077, 'test/loss': 1.4137426614761353, 'test/bleu': 29.036734519215344, 'test/num_examples': 3003}
I0311 11:03:56.732072 140203967616768 logging_writer.py:48] [51731] global_step=51731, preemption_count=0, score=19277.575584, test/accuracy=0.686503, test/bleu=29.036735, test/loss=1.413743, test/num_examples=3003, total_duration=32281.196443, train/accuracy=0.661931, train/bleu=32.746873, train/loss=1.582225, validation/accuracy=0.673767, validation/bleu=29.232165, validation/loss=1.497415, validation/num_examples=3000
I0311 11:03:57.750292 140391735396160 checkpoints.py:356] Saving checkpoint at step: 51731
I0311 11:04:01.103475 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_51731
I0311 11:04:01.107242 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_51731.
I0311 11:04:27.198691 140203976009472 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.18540334701538086, loss=1.6074953079223633
I0311 11:05:04.476064 140203959224064 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.21663780510425568, loss=1.7245087623596191
I0311 11:05:41.806390 140203976009472 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.2063523828983307, loss=1.6979223489761353
I0311 11:06:19.123663 140203959224064 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.20781108736991882, loss=1.7184715270996094
I0311 11:06:56.477159 140203976009472 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.19497181475162506, loss=1.6564842462539673
I0311 11:07:33.850717 140203959224064 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.18562966585159302, loss=1.7345672845840454
I0311 11:08:11.259616 140203976009472 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.22728511691093445, loss=1.777016282081604
I0311 11:08:48.639344 140203959224064 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.19422215223312378, loss=1.5932047367095947
I0311 11:09:26.030617 140203976009472 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.21030625700950623, loss=1.6886695623397827
I0311 11:10:03.432469 140203959224064 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.21639399230480194, loss=1.7209054231643677
I0311 11:10:40.804477 140203976009472 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.19608625769615173, loss=1.6853423118591309
I0311 11:11:18.165322 140203959224064 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.18610183894634247, loss=1.642200231552124
I0311 11:11:55.502443 140203976009472 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.2146952599287033, loss=1.6442046165466309
I0311 11:12:32.904709 140203959224064 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.20876654982566833, loss=1.7605293989181519
I0311 11:13:10.277471 140203976009472 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.2124706655740738, loss=1.753607988357544
I0311 11:13:47.630908 140203959224064 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.20212888717651367, loss=1.63591468334198
I0311 11:14:24.987594 140203976009472 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.19190123677253723, loss=1.7634848356246948
I0311 11:15:02.352698 140203959224064 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.20544734597206116, loss=1.706334114074707
I0311 11:15:39.741076 140203976009472 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.18978877365589142, loss=1.6750348806381226
I0311 11:16:17.109286 140203959224064 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.2102249264717102, loss=1.7534273862838745
I0311 11:16:54.495993 140203976009472 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.2024439573287964, loss=1.7137531042099
I0311 11:17:31.889537 140203959224064 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.17931388318538666, loss=1.612989068031311
I0311 11:18:01.114598 140391735396160 spec.py:298] Evaluating on the training split.
I0311 11:18:04.096111 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 11:22:18.799401 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 11:22:21.441426 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 11:25:46.472049 140391735396160 spec.py:326] Evaluating on the test split.
I0311 11:25:49.161557 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 11:29:17.185066 140391735396160 submission_runner.py:359] Time since start: 33681.43s, 	Step: 53980, 	{'train/accuracy': 0.65395188331604, 'train/loss': 1.629076600074768, 'train/bleu': 32.72745121177344, 'validation/accuracy': 0.677920937538147, 'validation/loss': 1.488610863685608, 'validation/bleu': 29.32732174403276, 'validation/num_examples': 3000, 'test/accuracy': 0.6899192333221436, 'test/loss': 1.398556113243103, 'test/bleu': 29.03438321915541, 'test/num_examples': 3003}
I0311 11:29:17.197074 140203976009472 logging_writer.py:48] [53980] global_step=53980, preemption_count=0, score=20114.039040, test/accuracy=0.689919, test/bleu=29.034383, test/loss=1.398556, test/num_examples=3003, total_duration=33681.433806, train/accuracy=0.653952, train/bleu=32.727451, train/loss=1.629077, validation/accuracy=0.677921, validation/bleu=29.327322, validation/loss=1.488611, validation/num_examples=3000
I0311 11:29:18.213820 140391735396160 checkpoints.py:356] Saving checkpoint at step: 53980
I0311 11:29:21.551115 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_53980
I0311 11:29:21.554788 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_53980.
I0311 11:29:29.370211 140203959224064 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.20462848246097565, loss=1.7201682329177856
I0311 11:30:06.586787 140203950831360 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.2088806927204132, loss=1.767331838607788
I0311 11:30:43.865156 140203959224064 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.20312297344207764, loss=1.5994511842727661
I0311 11:31:21.196519 140203950831360 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.20545801520347595, loss=1.7742514610290527
I0311 11:31:58.532851 140203959224064 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.18935520946979523, loss=1.6574431657791138
I0311 11:32:35.889260 140203950831360 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.19712695479393005, loss=1.705716848373413
I0311 11:33:13.253868 140203959224064 logging_writer.py:48] [54600] global_step=54600, grad_norm=4.805877685546875, loss=1.7461788654327393
I0311 11:33:50.630815 140203950831360 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.18874940276145935, loss=1.6134836673736572
I0311 11:34:28.012516 140203959224064 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.19557294249534607, loss=1.7530531883239746
I0311 11:35:05.391893 140203950831360 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.1956288367509842, loss=1.5927804708480835
I0311 11:35:42.751330 140203959224064 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2006290704011917, loss=1.7172356843948364
I0311 11:36:20.143760 140203950831360 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.23457607626914978, loss=1.646310806274414
I0311 11:36:57.495219 140203959224064 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.24856920540332794, loss=1.6560413837432861
I0311 11:37:34.855276 140203950831360 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.19992636144161224, loss=1.6502912044525146
I0311 11:38:12.193372 140203959224064 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.1938728392124176, loss=1.7068109512329102
I0311 11:38:49.559335 140203950831360 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2142413705587387, loss=1.7422449588775635
I0311 11:39:26.933310 140203959224064 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.19608071446418762, loss=1.6688261032104492
I0311 11:40:04.334555 140203950831360 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.18567520380020142, loss=1.6682121753692627
I0311 11:40:41.689293 140203959224064 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.20002199709415436, loss=1.6019740104675293
I0311 11:41:19.100736 140203950831360 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2109181135892868, loss=1.673915147781372
I0311 11:41:56.456008 140203959224064 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.21116314828395844, loss=1.7547317743301392
I0311 11:42:33.871551 140203950831360 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.21494437754154205, loss=1.6899120807647705
I0311 11:43:11.221825 140203959224064 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.19245408475399017, loss=1.6722384691238403
I0311 11:43:21.750827 140391735396160 spec.py:298] Evaluating on the training split.
I0311 11:43:24.732446 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 11:47:02.156497 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 11:47:04.788766 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 11:49:17.263512 140391735396160 spec.py:326] Evaluating on the test split.
I0311 11:49:19.963652 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 11:51:34.389985 140391735396160 submission_runner.py:359] Time since start: 35202.07s, 	Step: 56230, 	{'train/accuracy': 0.658459484577179, 'train/loss': 1.611425757408142, 'train/bleu': 32.26509812044639, 'validation/accuracy': 0.6774621605873108, 'validation/loss': 1.4801615476608276, 'validation/bleu': 29.459838166697168, 'validation/num_examples': 3000, 'test/accuracy': 0.6899076104164124, 'test/loss': 1.3914011716842651, 'test/bleu': 29.065301651803, 'test/num_examples': 3003}
I0311 11:51:34.402030 140203950831360 logging_writer.py:48] [56230] global_step=56230, preemption_count=0, score=20950.756632, test/accuracy=0.689908, test/bleu=29.065302, test/loss=1.391401, test/num_examples=3003, total_duration=35202.070037, train/accuracy=0.658459, train/bleu=32.265098, train/loss=1.611426, validation/accuracy=0.677462, validation/bleu=29.459838, validation/loss=1.480162, validation/num_examples=3000
I0311 11:51:35.422313 140391735396160 checkpoints.py:356] Saving checkpoint at step: 56230
I0311 11:51:38.898479 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_56230
I0311 11:51:38.902030 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_56230.
I0311 11:52:05.374248 140203959224064 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.1992092728614807, loss=1.6446421146392822
I0311 11:52:42.710095 140203942438656 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.2700757384300232, loss=1.62290620803833
I0311 11:53:20.075270 140203959224064 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.1849505752325058, loss=1.6236642599105835
I0311 11:53:57.412587 140203942438656 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.20077186822891235, loss=1.7248811721801758
I0311 11:54:34.757830 140203959224064 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.20053499937057495, loss=1.6563128232955933
I0311 11:55:12.065695 140203942438656 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.2094675600528717, loss=1.6573104858398438
I0311 11:55:49.409957 140203959224064 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.23730693757534027, loss=1.6793862581253052
I0311 11:56:26.779371 140203942438656 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.1966838836669922, loss=1.6380822658538818
I0311 11:57:04.153480 140203959224064 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.19550840556621552, loss=1.6246157884597778
I0311 11:57:41.540148 140203942438656 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.21418941020965576, loss=1.7096660137176514
I0311 11:58:18.890253 140203959224064 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.18616577982902527, loss=1.6752516031265259
I0311 11:58:56.268879 140203942438656 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.19079460203647614, loss=1.6881605386734009
I0311 11:59:33.616439 140203959224064 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.19560252130031586, loss=1.6214232444763184
I0311 12:00:10.963671 140203942438656 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.19477425515651703, loss=1.604872465133667
I0311 12:00:48.336662 140203959224064 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.2071644365787506, loss=1.6366528272628784
I0311 12:01:25.692350 140203942438656 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.2007562518119812, loss=1.655360460281372
I0311 12:02:03.075006 140203959224064 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.1894664168357849, loss=1.607324481010437
I0311 12:02:40.428371 140203942438656 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2298489660024643, loss=1.5834903717041016
I0311 12:03:17.789095 140203959224064 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.21265660226345062, loss=1.6505547761917114
I0311 12:03:55.198144 140203942438656 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.21176622807979584, loss=1.6685409545898438
I0311 12:04:32.586552 140203959224064 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.20000578463077545, loss=1.5973602533340454
I0311 12:05:09.938830 140203942438656 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.19489222764968872, loss=1.6259490251541138
I0311 12:05:39.146416 140391735396160 spec.py:298] Evaluating on the training split.
I0311 12:05:42.130013 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 12:09:40.427200 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 12:09:43.062435 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 12:12:24.079478 140391735396160 spec.py:326] Evaluating on the test split.
I0311 12:12:26.780077 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 12:15:01.133119 140391735396160 submission_runner.py:359] Time since start: 36539.47s, 	Step: 58480, 	{'train/accuracy': 0.6677833795547485, 'train/loss': 1.5405551195144653, 'train/bleu': 32.89231354865563, 'validation/accuracy': 0.6791360378265381, 'validation/loss': 1.4704777002334595, 'validation/bleu': 29.74237073463129, 'validation/num_examples': 3000, 'test/accuracy': 0.6929057240486145, 'test/loss': 1.3788301944732666, 'test/bleu': 29.27869449899086, 'test/num_examples': 3003}
I0311 12:15:01.145499 140203959224064 logging_writer.py:48] [58480] global_step=58480, preemption_count=0, score=21787.464631, test/accuracy=0.692906, test/bleu=29.278694, test/loss=1.378830, test/num_examples=3003, total_duration=36539.465620, train/accuracy=0.667783, train/bleu=32.892314, train/loss=1.540555, validation/accuracy=0.679136, validation/bleu=29.742371, validation/loss=1.470478, validation/num_examples=3000
I0311 12:15:02.161697 140391735396160 checkpoints.py:356] Saving checkpoint at step: 58480
I0311 12:15:06.351029 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_58480
I0311 12:15:06.354769 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_58480.
I0311 12:15:14.185231 140203942438656 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.21323850750923157, loss=1.647682785987854
I0311 12:15:51.422794 140203934045952 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.19645334780216217, loss=1.7133457660675049
I0311 12:16:28.724218 140203942438656 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.21911320090293884, loss=1.6844022274017334
I0311 12:17:06.074428 140203934045952 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.2016105204820633, loss=1.5863165855407715
I0311 12:17:43.404639 140203942438656 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.19542482495307922, loss=1.7174662351608276
I0311 12:18:20.742486 140203934045952 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.1936817765235901, loss=1.6025458574295044
I0311 12:18:58.110476 140203942438656 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.19327688217163086, loss=1.6107747554779053
I0311 12:19:35.440219 140203934045952 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.19491255283355713, loss=1.703942060470581
I0311 12:20:12.811860 140203942438656 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.19643093645572662, loss=1.695294737815857
I0311 12:20:50.170977 140203934045952 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.1997295767068863, loss=1.595766305923462
I0311 12:21:27.484431 140203942438656 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.20323248207569122, loss=1.622849464416504
I0311 12:22:04.898427 140203934045952 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.20201794803142548, loss=1.665379285812378
I0311 12:22:42.247885 140203942438656 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.220020592212677, loss=1.6367942094802856
I0311 12:23:19.607639 140203934045952 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.20874474942684174, loss=1.6590927839279175
I0311 12:23:56.990657 140203942438656 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.19363854825496674, loss=1.5997334718704224
I0311 12:24:34.347171 140203934045952 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.20382502675056458, loss=1.6939470767974854
I0311 12:25:11.710529 140203942438656 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.19695502519607544, loss=1.628609538078308
I0311 12:25:49.069778 140203934045952 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3143669068813324, loss=1.6989432573318481
I0311 12:26:26.436570 140203942438656 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.20260892808437347, loss=1.6707276105880737
I0311 12:27:03.852295 140203934045952 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.20158593356609344, loss=1.6895582675933838
I0311 12:27:41.230578 140203942438656 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.19813689589500427, loss=1.654526710510254
I0311 12:28:18.593264 140203934045952 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.18956266343593597, loss=1.6399351358413696
I0311 12:28:56.002767 140203942438656 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.19182907044887543, loss=1.6680866479873657
I0311 12:29:06.569916 140391735396160 spec.py:298] Evaluating on the training split.
I0311 12:29:09.545091 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 12:33:13.251592 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 12:33:15.898827 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 12:35:54.459853 140391735396160 spec.py:326] Evaluating on the test split.
I0311 12:35:57.149674 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 12:38:32.289840 140391735396160 submission_runner.py:359] Time since start: 37946.89s, 	Step: 60730, 	{'train/accuracy': 0.665516197681427, 'train/loss': 1.5675028562545776, 'train/bleu': 32.53189036798213, 'validation/accuracy': 0.6792476177215576, 'validation/loss': 1.4685629606246948, 'validation/bleu': 29.559650210203717, 'validation/num_examples': 3000, 'test/accuracy': 0.6925687193870544, 'test/loss': 1.3763048648834229, 'test/bleu': 29.376539758710788, 'test/num_examples': 3003}
I0311 12:38:32.302245 140203934045952 logging_writer.py:48] [60730] global_step=60730, preemption_count=0, score=22624.174674, test/accuracy=0.692569, test/bleu=29.376540, test/loss=1.376305, test/num_examples=3003, total_duration=37946.889128, train/accuracy=0.665516, train/bleu=32.531890, train/loss=1.567503, validation/accuracy=0.679248, validation/bleu=29.559650, validation/loss=1.468563, validation/num_examples=3000
I0311 12:38:33.315206 140391735396160 checkpoints.py:356] Saving checkpoint at step: 60730
I0311 12:38:37.626015 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_60730
I0311 12:38:37.629791 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_60730.
I0311 12:39:04.087513 140203942438656 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.18968962132930756, loss=1.627637267112732
I0311 12:39:41.373354 140203925653248 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.20191824436187744, loss=1.6536977291107178
I0311 12:40:18.641708 140203942438656 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.24074134230613708, loss=1.7659305334091187
I0311 12:40:55.979699 140203925653248 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.19246172904968262, loss=1.6005077362060547
I0311 12:41:33.330421 140203942438656 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.22493727505207062, loss=1.7334861755371094
I0311 12:42:10.701750 140203925653248 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.20949730277061462, loss=1.5252799987792969
I0311 12:42:48.051902 140203942438656 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.2379210740327835, loss=1.6064693927764893
I0311 12:43:25.458621 140203925653248 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.19545745849609375, loss=1.623988151550293
I0311 12:44:02.821528 140203942438656 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.1933806985616684, loss=1.5940041542053223
I0311 12:44:40.202879 140203925653248 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.197721466422081, loss=1.6831625699996948
I0311 12:45:17.565792 140203942438656 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.1983751803636551, loss=1.5959950685501099
I0311 12:45:54.979087 140203925653248 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.20834781229496002, loss=1.617148518562317
I0311 12:46:32.380964 140203942438656 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.19454289972782135, loss=1.6806919574737549
I0311 12:47:09.767820 140203925653248 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.19471684098243713, loss=1.5745586156845093
I0311 12:47:47.152204 140203942438656 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.212975412607193, loss=1.763326644897461
I0311 12:48:24.536449 140203925653248 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.18473906815052032, loss=1.5352145433425903
I0311 12:49:01.937188 140203942438656 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.19526566565036774, loss=1.5750138759613037
I0311 12:49:39.340905 140203925653248 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.2065427005290985, loss=1.7069658041000366
I0311 12:50:16.690525 140203942438656 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.19712866842746735, loss=1.6772189140319824
I0311 12:50:54.060552 140203925653248 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.2164240926504135, loss=1.6916626691818237
I0311 12:51:31.441555 140203942438656 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.22961260378360748, loss=1.6521310806274414
I0311 12:52:08.808351 140203925653248 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.21166062355041504, loss=1.6323645114898682
I0311 12:52:37.692168 140391735396160 spec.py:298] Evaluating on the training split.
I0311 12:52:40.672447 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 12:56:34.003494 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 12:56:36.659203 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 12:59:00.539250 140391735396160 spec.py:326] Evaluating on the test split.
I0311 12:59:03.242475 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 13:01:16.710232 140391735396160 submission_runner.py:359] Time since start: 39358.01s, 	Step: 62979, 	{'train/accuracy': 0.6781721115112305, 'train/loss': 1.4681769609451294, 'train/bleu': 33.658778329045774, 'validation/accuracy': 0.6823350191116333, 'validation/loss': 1.4543193578720093, 'validation/bleu': 29.896432338278213, 'validation/num_examples': 3000, 'test/accuracy': 0.6947882175445557, 'test/loss': 1.3666539192199707, 'test/bleu': 29.77311757983071, 'test/num_examples': 3003}
I0311 13:01:16.723296 140203942438656 logging_writer.py:48] [62979] global_step=62979, preemption_count=0, score=23460.727657, test/accuracy=0.694788, test/bleu=29.773118, test/loss=1.366654, test/num_examples=3003, total_duration=39358.011357, train/accuracy=0.678172, train/bleu=33.658778, train/loss=1.468177, validation/accuracy=0.682335, validation/bleu=29.896432, validation/loss=1.454319, validation/num_examples=3000
I0311 13:01:17.745233 140391735396160 checkpoints.py:356] Saving checkpoint at step: 62979
I0311 13:01:21.152574 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_62979
I0311 13:01:21.156155 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_62979.
I0311 13:01:29.367496 140203925653248 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.20050621032714844, loss=1.6478960514068604
I0311 13:02:06.621317 140203917260544 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.20316649973392487, loss=1.6083654165267944
I0311 13:02:43.917096 140203925653248 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.19569283723831177, loss=1.5999252796173096
I0311 13:03:21.268278 140203917260544 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.1847028136253357, loss=1.5658724308013916
I0311 13:03:58.649300 140203925653248 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.23065735399723053, loss=1.6300363540649414
I0311 13:04:35.925677 140203917260544 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.20893259346485138, loss=1.6330277919769287
I0311 13:05:13.302242 140203925653248 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.20443639159202576, loss=1.6080033779144287
I0311 13:05:50.669201 140203917260544 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.20440304279327393, loss=1.6006611585617065
I0311 13:06:28.046229 140203925653248 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.21716977655887604, loss=1.7122348546981812
I0311 13:07:05.393835 140203917260544 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.19556526839733124, loss=1.596839189529419
I0311 13:07:42.784487 140203925653248 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.1935344785451889, loss=1.6032450199127197
I0311 13:08:20.106636 140203917260544 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.1954156756401062, loss=1.5805654525756836
I0311 13:08:57.471986 140203925653248 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.23467737436294556, loss=1.6513786315917969
I0311 13:09:34.816334 140203917260544 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.23058024048805237, loss=1.6296297311782837
I0311 13:10:12.158206 140203925653248 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.22934816777706146, loss=1.6197009086608887
I0311 13:10:49.516504 140203917260544 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.20245379209518433, loss=1.6203383207321167
I0311 13:11:26.863670 140203925653248 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.20983456075191498, loss=1.7131329774856567
I0311 13:12:04.219944 140203917260544 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.19220015406608582, loss=1.6349701881408691
I0311 13:12:41.593293 140203925653248 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.20012830197811127, loss=1.6136425733566284
I0311 13:13:19.017719 140203917260544 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.20741482079029083, loss=1.670575499534607
I0311 13:13:56.375491 140203925653248 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.2030288428068161, loss=1.5876954793930054
I0311 13:14:33.727343 140203917260544 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.1962132751941681, loss=1.5577261447906494
I0311 13:15:11.061370 140203925653248 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.20660625398159027, loss=1.6870547533035278
I0311 13:15:21.225956 140391735396160 spec.py:298] Evaluating on the training split.
I0311 13:15:24.221000 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 13:19:39.677296 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 13:19:42.320394 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 13:23:14.473248 140391735396160 spec.py:326] Evaluating on the test split.
I0311 13:23:17.180393 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 13:26:57.050218 140391735396160 submission_runner.py:359] Time since start: 40721.55s, 	Step: 65229, 	{'train/accuracy': 0.6683319211006165, 'train/loss': 1.5389748811721802, 'train/bleu': 33.463131091627346, 'validation/accuracy': 0.6814670562744141, 'validation/loss': 1.4475693702697754, 'validation/bleu': 29.59354774730482, 'validation/num_examples': 3000, 'test/accuracy': 0.697681725025177, 'test/loss': 1.3529905080795288, 'test/bleu': 29.59358149692596, 'test/num_examples': 3003}
I0311 13:26:57.063085 140203917260544 logging_writer.py:48] [65229] global_step=65229, preemption_count=0, score=24297.289657, test/accuracy=0.697682, test/bleu=29.593581, test/loss=1.352991, test/num_examples=3003, total_duration=40721.545164, train/accuracy=0.668332, train/bleu=33.463131, train/loss=1.538975, validation/accuracy=0.681467, validation/bleu=29.593548, validation/loss=1.447569, validation/num_examples=3000
I0311 13:26:58.074328 140391735396160 checkpoints.py:356] Saving checkpoint at step: 65229
I0311 13:27:01.447297 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_65229
I0311 13:27:01.450972 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_65229.
I0311 13:27:28.270779 140203925653248 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.21297210454940796, loss=1.594704508781433
I0311 13:28:05.577353 140203908867840 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.19299744069576263, loss=1.579919457435608
I0311 13:28:42.899231 140203925653248 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.1993284970521927, loss=1.5782583951950073
I0311 13:29:20.255156 140203908867840 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.2181631177663803, loss=1.7072458267211914
I0311 13:29:57.573488 140203925653248 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.20094572007656097, loss=1.556093454360962
I0311 13:30:34.961567 140203908867840 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.19166158139705658, loss=1.6181297302246094
I0311 13:31:12.360222 140203925653248 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.19076408445835114, loss=1.6142462491989136
I0311 13:31:49.740202 140203908867840 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.20181074738502502, loss=1.6566914319992065
I0311 13:32:27.069307 140203925653248 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.1968110054731369, loss=1.5229650735855103
I0311 13:33:04.420461 140203908867840 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.21274979412555695, loss=1.5995714664459229
I0311 13:33:41.772396 140203925653248 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.2137545645236969, loss=1.6266316175460815
I0311 13:34:19.163580 140203908867840 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.19570429623126984, loss=1.5623711347579956
I0311 13:34:56.486269 140203925653248 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.19223880767822266, loss=1.5372498035430908
I0311 13:35:33.866898 140203908867840 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.20102263987064362, loss=1.6156761646270752
I0311 13:36:11.253170 140203925653248 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.20669186115264893, loss=1.5891332626342773
I0311 13:36:48.640779 140203908867840 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2155986726284027, loss=1.657061219215393
I0311 13:37:26.045010 140203925653248 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.2005445510149002, loss=1.6102442741394043
I0311 13:38:03.394562 140203908867840 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.20261341333389282, loss=1.5528420209884644
I0311 13:38:40.783272 140203925653248 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.2113109976053238, loss=1.674270749092102
I0311 13:39:18.115932 140203908867840 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.1966995894908905, loss=1.5929474830627441
I0311 13:39:55.478142 140203925653248 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.20418518781661987, loss=1.6505764722824097
I0311 13:40:32.856584 140203908867840 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.19513176381587982, loss=1.5995737314224243
I0311 13:41:01.726576 140391735396160 spec.py:298] Evaluating on the training split.
I0311 13:41:04.711978 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 13:45:24.208176 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 13:45:26.854977 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 13:47:55.295178 140391735396160 spec.py:326] Evaluating on the test split.
I0311 13:47:57.980521 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 13:50:35.108200 140391735396160 submission_runner.py:359] Time since start: 42262.05s, 	Step: 67479, 	{'train/accuracy': 0.6673305034637451, 'train/loss': 1.5460699796676636, 'train/bleu': 33.56100032715051, 'validation/accuracy': 0.6833640933036804, 'validation/loss': 1.4409172534942627, 'validation/bleu': 29.727466139711133, 'validation/num_examples': 3000, 'test/accuracy': 0.6977514624595642, 'test/loss': 1.3484975099563599, 'test/bleu': 29.594684613861077, 'test/num_examples': 3003}
I0311 13:50:35.121229 140203925653248 logging_writer.py:48] [67479] global_step=67479, preemption_count=0, score=25134.096145, test/accuracy=0.697751, test/bleu=29.594685, test/loss=1.348498, test/num_examples=3003, total_duration=42262.045758, train/accuracy=0.667331, train/bleu=33.561000, train/loss=1.546070, validation/accuracy=0.683364, validation/bleu=29.727466, validation/loss=1.440917, validation/num_examples=3000
I0311 13:50:36.117951 140391735396160 checkpoints.py:356] Saving checkpoint at step: 67479
I0311 13:50:39.464754 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_67479
I0311 13:50:39.468392 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_67479.
I0311 13:50:47.697404 140203908867840 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.32279685139656067, loss=1.5847352743148804
I0311 13:51:24.928268 140203900475136 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.1995638608932495, loss=1.5416730642318726
I0311 13:52:02.244359 140203908867840 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.1912621408700943, loss=1.5730421543121338
I0311 13:52:39.572413 140203900475136 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.20953378081321716, loss=1.663875699043274
I0311 13:53:16.969415 140203908867840 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.21323272585868835, loss=1.6464145183563232
I0311 13:53:54.355076 140203900475136 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.1922546774148941, loss=1.5377100706100464
I0311 13:54:31.729341 140203908867840 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.21829967200756073, loss=1.6466048955917358
I0311 13:55:09.087898 140203900475136 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.20474787056446075, loss=1.6431719064712524
I0311 13:55:46.457121 140203908867840 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.21606162190437317, loss=1.5837534666061401
I0311 13:56:23.850359 140203900475136 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.19568049907684326, loss=1.5858982801437378
I0311 13:57:01.243398 140203908867840 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.19488035142421722, loss=1.5388203859329224
I0311 13:57:38.580824 140203900475136 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.2147231101989746, loss=1.625744104385376
I0311 13:58:15.960366 140203908867840 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.2008342146873474, loss=1.5677707195281982
I0311 13:58:53.304489 140203900475136 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.20483902096748352, loss=1.5903187990188599
I0311 13:59:30.636678 140203908867840 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.22150582075119019, loss=1.5868154764175415
I0311 14:00:08.043001 140203900475136 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.2118755578994751, loss=1.5784575939178467
I0311 14:00:45.427311 140203908867840 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.19392207264900208, loss=1.5721515417099
I0311 14:01:22.802430 140203900475136 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.19544583559036255, loss=1.4917312860488892
I0311 14:02:00.173197 140203908867840 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.1906348615884781, loss=1.5652154684066772
I0311 14:02:37.542050 140203900475136 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.22235530614852905, loss=1.608649492263794
I0311 14:03:14.919683 140203908867840 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.2026447206735611, loss=1.5668079853057861
I0311 14:03:52.269342 140203900475136 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.19088149070739746, loss=1.6079676151275635
I0311 14:04:29.602648 140203908867840 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.20183077454566956, loss=1.491876244544983
I0311 14:04:39.782834 140391735396160 spec.py:298] Evaluating on the training split.
I0311 14:04:42.759734 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 14:08:46.266199 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 14:08:48.896708 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 14:12:11.440174 140391735396160 spec.py:326] Evaluating on the test split.
I0311 14:12:14.128663 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 14:16:02.013574 140391735396160 submission_runner.py:359] Time since start: 43680.10s, 	Step: 69729, 	{'train/accuracy': 0.6765223145484924, 'train/loss': 1.4774832725524902, 'train/bleu': 34.25141826351276, 'validation/accuracy': 0.6848644018173218, 'validation/loss': 1.4274961948394775, 'validation/bleu': 29.97824908965115, 'validation/num_examples': 3000, 'test/accuracy': 0.698901891708374, 'test/loss': 1.3344674110412598, 'test/bleu': 29.929980963972664, 'test/num_examples': 3003}
I0311 14:16:02.027902 140203900475136 logging_writer.py:48] [69729] global_step=69729, preemption_count=0, score=25970.911387, test/accuracy=0.698902, test/bleu=29.929981, test/loss=1.334467, test/num_examples=3003, total_duration=43680.102049, train/accuracy=0.676522, train/bleu=34.251418, train/loss=1.477483, validation/accuracy=0.684864, validation/bleu=29.978249, validation/loss=1.427496, validation/num_examples=3000
I0311 14:16:03.037246 140391735396160 checkpoints.py:356] Saving checkpoint at step: 69729
I0311 14:16:06.393273 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_69729
I0311 14:16:06.396870 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_69729.
I0311 14:16:33.303177 140203908867840 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.21126072108745575, loss=1.608843207359314
I0311 14:17:10.601228 140203892082432 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.2087150514125824, loss=1.6470770835876465
I0311 14:17:47.937566 140203908867840 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.19331741333007812, loss=1.5071232318878174
I0311 14:18:25.272074 140203892082432 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.21102289855480194, loss=1.5675349235534668
I0311 14:19:02.584293 140203908867840 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.20158541202545166, loss=1.564361810684204
I0311 14:19:39.957664 140203892082432 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.1931360960006714, loss=1.5689045190811157
I0311 14:20:17.299978 140203908867840 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.19780148565769196, loss=1.6073832511901855
I0311 14:20:54.653458 140203892082432 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.2055077850818634, loss=1.52867591381073
I0311 14:21:32.036787 140203908867840 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.1988043636083603, loss=1.6107569932937622
I0311 14:22:09.395938 140203892082432 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.20447146892547607, loss=1.6270002126693726
I0311 14:22:46.721940 140203908867840 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.20680436491966248, loss=1.5502198934555054
I0311 14:23:24.053166 140203892082432 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.19640474021434784, loss=1.599796175956726
I0311 14:24:01.413135 140203908867840 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.20725437998771667, loss=1.5928421020507812
I0311 14:24:38.739130 140203892082432 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.21648259460926056, loss=1.6102734804153442
I0311 14:25:16.101278 140203908867840 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.21122029423713684, loss=1.5980550050735474
I0311 14:25:53.427857 140203892082432 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.19247421622276306, loss=1.5634450912475586
I0311 14:26:30.830138 140203908867840 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.2269221991300583, loss=1.5743032693862915
I0311 14:27:08.205971 140203892082432 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.209723100066185, loss=1.5668214559555054
I0311 14:27:45.589780 140203908867840 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.20765972137451172, loss=1.5979021787643433
I0311 14:28:22.928611 140203892082432 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.20130325853824615, loss=1.5506285429000854
I0311 14:29:00.329690 140203908867840 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.20322637259960175, loss=1.6293978691101074
I0311 14:29:37.719748 140203892082432 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.19786988198757172, loss=1.5559805631637573
I0311 14:30:06.565546 140391735396160 spec.py:298] Evaluating on the training split.
I0311 14:30:09.568188 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 14:34:15.789845 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 14:34:18.437136 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 14:38:00.785305 140391735396160 spec.py:326] Evaluating on the test split.
I0311 14:38:03.473545 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 14:41:45.317624 140391735396160 submission_runner.py:359] Time since start: 45206.88s, 	Step: 71979, 	{'train/accuracy': 0.6767017245292664, 'train/loss': 1.483557939529419, 'train/bleu': 34.213315669464365, 'validation/accuracy': 0.6854719519615173, 'validation/loss': 1.422886848449707, 'validation/bleu': 29.858244243630022, 'validation/num_examples': 3000, 'test/accuracy': 0.7004357576370239, 'test/loss': 1.322145700454712, 'test/bleu': 30.1580656203387, 'test/num_examples': 3003}
I0311 14:41:45.331285 140203908867840 logging_writer.py:48] [71979] global_step=71979, preemption_count=0, score=26807.593782, test/accuracy=0.700436, test/bleu=30.158066, test/loss=1.322146, test/num_examples=3003, total_duration=45206.884733, train/accuracy=0.676702, train/bleu=34.213316, train/loss=1.483558, validation/accuracy=0.685472, validation/bleu=29.858244, validation/loss=1.422887, validation/num_examples=3000
I0311 14:41:46.336952 140391735396160 checkpoints.py:356] Saving checkpoint at step: 71979
I0311 14:41:50.170194 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_71979
I0311 14:41:50.173801 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_71979.
I0311 14:41:58.365479 140203892082432 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.2014540284872055, loss=1.6177773475646973
I0311 14:42:35.630772 140203883689728 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.21879203617572784, loss=1.6001403331756592
I0311 14:43:12.930579 140203892082432 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.2092626988887787, loss=1.600592017173767
I0311 14:43:50.256944 140203883689728 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.27038663625717163, loss=1.486972451210022
I0311 14:44:27.647127 140203892082432 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.20885297656059265, loss=1.5889602899551392
I0311 14:45:05.033602 140203883689728 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.2005535364151001, loss=1.6090741157531738
I0311 14:45:42.406198 140203892082432 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.21134589612483978, loss=1.6009360551834106
I0311 14:46:19.780643 140203883689728 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.19953173398971558, loss=1.5037027597427368
I0311 14:46:57.159713 140203892082432 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.2165951430797577, loss=1.6026777029037476
I0311 14:47:34.516426 140203883689728 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.2059800624847412, loss=1.5097813606262207
I0311 14:48:11.861618 140203892082432 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.21282166242599487, loss=1.554969072341919
I0311 14:48:49.239255 140203883689728 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.21234440803527832, loss=1.4869428873062134
I0311 14:49:26.611922 140203892082432 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.21251462399959564, loss=1.6026911735534668
I0311 14:50:03.977314 140203883689728 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.22037392854690552, loss=1.5220592021942139
I0311 14:50:41.269556 140203892082432 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.19929777085781097, loss=1.5392886400222778
I0311 14:51:18.674113 140203883689728 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.2129371464252472, loss=1.5450102090835571
I0311 14:51:56.067158 140203892082432 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.20652933418750763, loss=1.5640369653701782
I0311 14:52:33.435157 140203883689728 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.21007409691810608, loss=1.5334715843200684
I0311 14:53:10.778809 140203892082432 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.21816182136535645, loss=1.5499385595321655
I0311 14:53:48.141670 140203883689728 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.21480488777160645, loss=1.5732598304748535
I0311 14:54:25.519042 140203892082432 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.20988555252552032, loss=1.6122663021087646
I0311 14:55:02.877527 140203883689728 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.21928004920482635, loss=1.5496618747711182
I0311 14:55:40.233052 140203892082432 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.3783569931983948, loss=1.5512062311172485
I0311 14:55:50.402617 140391735396160 spec.py:298] Evaluating on the training split.
I0311 14:55:53.381503 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 15:00:00.795996 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 15:00:03.444339 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 15:02:47.365458 140391735396160 spec.py:326] Evaluating on the test split.
I0311 15:02:50.064104 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 15:05:26.320074 140391735396160 submission_runner.py:359] Time since start: 46750.72s, 	Step: 74229, 	{'train/accuracy': 0.673919677734375, 'train/loss': 1.5076984167099, 'train/bleu': 34.24305028880997, 'validation/accuracy': 0.6865878701210022, 'validation/loss': 1.415334939956665, 'validation/bleu': 30.287005487782032, 'validation/num_examples': 3000, 'test/accuracy': 0.7037708759307861, 'test/loss': 1.3120838403701782, 'test/bleu': 30.473754679760344, 'test/num_examples': 3003}
I0311 15:05:26.335906 140203883689728 logging_writer.py:48] [74229] global_step=74229, preemption_count=0, score=27644.347570, test/accuracy=0.703771, test/bleu=30.473755, test/loss=1.312084, test/num_examples=3003, total_duration=46750.721818, train/accuracy=0.673920, train/bleu=34.243050, train/loss=1.507698, validation/accuracy=0.686588, validation/bleu=30.287005, validation/loss=1.415335, validation/num_examples=3000
I0311 15:05:27.599248 140391735396160 checkpoints.py:356] Saving checkpoint at step: 74229
I0311 15:05:31.250970 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_74229
I0311 15:05:31.255601 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_74229.
I0311 15:05:58.072113 140203892082432 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.19990810751914978, loss=1.5322911739349365
I0311 15:06:35.392256 140203875297024 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.2251325100660324, loss=1.5262553691864014
I0311 15:07:12.687919 140203892082432 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.21861906349658966, loss=1.6172220706939697
I0311 15:07:49.999370 140203875297024 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.25818106532096863, loss=1.5165730714797974
I0311 15:08:27.345885 140203892082432 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.21542774140834808, loss=1.543958067893982
I0311 15:09:04.697210 140203875297024 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.2015591561794281, loss=1.5451183319091797
I0311 15:09:42.049208 140203892082432 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.2086312621831894, loss=1.5364595651626587
I0311 15:10:19.420556 140203875297024 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.21488140523433685, loss=1.6474815607070923
I0311 15:10:56.727815 140203892082432 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.21135646104812622, loss=1.5527818202972412
I0311 15:11:34.062321 140203875297024 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.212507963180542, loss=1.6145883798599243
I0311 15:12:11.408202 140203892082432 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.22876273095607758, loss=1.5654168128967285
I0311 15:12:48.812726 140203875297024 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.20610906183719635, loss=1.5512083768844604
I0311 15:13:26.200991 140203892082432 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.20953680574893951, loss=1.5747153759002686
I0311 15:14:03.582024 140203875297024 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.20563918352127075, loss=1.5395026206970215
I0311 15:14:40.925066 140203892082432 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.2044716626405716, loss=1.589094638824463
I0311 15:15:18.297880 140203875297024 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.20629017055034637, loss=1.5400768518447876
I0311 15:15:55.684758 140203892082432 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.19332411885261536, loss=1.5706218481063843
I0311 15:16:33.017549 140203875297024 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.21326369047164917, loss=1.5158997774124146
I0311 15:17:10.401477 140203892082432 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.21755443513393402, loss=1.519394874572754
I0311 15:17:47.762099 140203875297024 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.19616219401359558, loss=1.536623477935791
I0311 15:18:25.206127 140203892082432 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.20431703329086304, loss=1.5493476390838623
I0311 15:19:02.533263 140203875297024 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.20431186258792877, loss=1.5388180017471313
I0311 15:19:31.390505 140391735396160 spec.py:298] Evaluating on the training split.
I0311 15:19:34.378040 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 15:23:31.289820 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 15:23:33.942270 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 15:26:13.198695 140391735396160 spec.py:326] Evaluating on the test split.
I0311 15:26:15.879576 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 15:29:01.079979 140391735396160 submission_runner.py:359] Time since start: 48171.71s, 	Step: 76479, 	{'train/accuracy': 0.682380735874176, 'train/loss': 1.4522696733474731, 'train/bleu': 34.47130022557366, 'validation/accuracy': 0.6897372603416443, 'validation/loss': 1.4075356721878052, 'validation/bleu': 30.441052493026962, 'validation/num_examples': 3000, 'test/accuracy': 0.7038406133651733, 'test/loss': 1.306911587715149, 'test/bleu': 30.338031842341756, 'test/num_examples': 3003}
I0311 15:29:01.093503 140203892082432 logging_writer.py:48] [76479] global_step=76479, preemption_count=0, score=28480.929538, test/accuracy=0.703841, test/bleu=30.338032, test/loss=1.306912, test/num_examples=3003, total_duration=48171.709709, train/accuracy=0.682381, train/bleu=34.471300, train/loss=1.452270, validation/accuracy=0.689737, validation/bleu=30.441052, validation/loss=1.407536, validation/num_examples=3000
I0311 15:29:02.123669 140391735396160 checkpoints.py:356] Saving checkpoint at step: 76479
I0311 15:29:05.968759 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_76479
I0311 15:29:05.972380 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_76479.
I0311 15:29:14.185949 140203875297024 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.21467730402946472, loss=1.5548324584960938
I0311 15:29:51.441272 140203866904320 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.22462403774261475, loss=1.4984745979309082
I0311 15:30:28.777149 140203875297024 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.2156064659357071, loss=1.5931336879730225
I0311 15:31:06.134800 140203866904320 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.21189267933368683, loss=1.6322176456451416
I0311 15:31:43.440393 140203875297024 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.1981852948665619, loss=1.4972715377807617
I0311 15:32:20.806092 140203866904320 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.19494344294071198, loss=1.476567268371582
I0311 15:32:58.221212 140203875297024 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.20532996952533722, loss=1.4932615756988525
I0311 15:33:35.565316 140203866904320 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.23054832220077515, loss=1.5984797477722168
I0311 15:34:12.950858 140203875297024 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.21406470239162445, loss=1.5664769411087036
I0311 15:34:50.357772 140203866904320 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.20283928513526917, loss=1.5526235103607178
I0311 15:35:27.728772 140203875297024 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.22112885117530823, loss=1.5368293523788452
I0311 15:36:05.116764 140203866904320 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.20497514307498932, loss=1.5526968240737915
I0311 15:36:42.540308 140203875297024 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.2227928340435028, loss=1.5310935974121094
I0311 15:37:19.881952 140203866904320 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.22614672780036926, loss=1.5577330589294434
I0311 15:37:57.245538 140203875297024 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.1996348798274994, loss=1.5326944589614868
I0311 15:38:34.598792 140203866904320 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.20756098628044128, loss=1.6527185440063477
I0311 15:39:12.020163 140203875297024 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.22863832116127014, loss=1.4960825443267822
I0311 15:39:49.417927 140203866904320 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.21262896060943604, loss=1.5904191732406616
I0311 15:40:26.810011 140203875297024 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.20548370480537415, loss=1.565263271331787
I0311 15:41:04.222919 140203866904320 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.20958146452903748, loss=1.495131492614746
I0311 15:41:41.593563 140203875297024 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2029237300157547, loss=1.4885201454162598
I0311 15:42:18.936467 140203866904320 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.21874266862869263, loss=1.5484154224395752
I0311 15:42:56.312600 140203875297024 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.21234725415706635, loss=1.5586059093475342
I0311 15:43:06.097543 140391735396160 spec.py:298] Evaluating on the training split.
I0311 15:43:09.088131 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 15:47:04.790353 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 15:47:07.445503 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 15:50:32.689180 140391735396160 spec.py:326] Evaluating on the test split.
I0311 15:50:35.389002 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 15:53:56.796668 140391735396160 submission_runner.py:359] Time since start: 49586.42s, 	Step: 78728, 	{'train/accuracy': 0.6807045340538025, 'train/loss': 1.456209659576416, 'train/bleu': 34.280651641184235, 'validation/accuracy': 0.6890305280685425, 'validation/loss': 1.4007242918014526, 'validation/bleu': 30.242948208481355, 'validation/num_examples': 3000, 'test/accuracy': 0.7058160901069641, 'test/loss': 1.2985038757324219, 'test/bleu': 30.43142047542504, 'test/num_examples': 3003}
I0311 15:53:56.811868 140203866904320 logging_writer.py:48] [78728] global_step=78728, preemption_count=0, score=29317.511628, test/accuracy=0.705816, test/bleu=30.431420, test/loss=1.298504, test/num_examples=3003, total_duration=49586.416753, train/accuracy=0.680705, train/bleu=34.280652, train/loss=1.456210, validation/accuracy=0.689031, validation/bleu=30.242948, validation/loss=1.400724, validation/num_examples=3000
I0311 15:53:58.082667 140391735396160 checkpoints.py:356] Saving checkpoint at step: 78728
I0311 15:54:02.981580 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_78728
I0311 15:54:02.986163 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_78728.
I0311 15:54:30.224676 140203875297024 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.20573048293590546, loss=1.5164875984191895
I0311 15:55:07.522250 140203858511616 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.20697887241840363, loss=1.5924094915390015
I0311 15:55:44.845597 140203875297024 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.21933972835540771, loss=1.5420756340026855
I0311 15:56:22.216117 140203858511616 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.21756036579608917, loss=1.5464725494384766
I0311 15:56:59.584893 140203875297024 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.20204007625579834, loss=1.445269227027893
I0311 15:57:36.969547 140203858511616 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.21490545570850372, loss=1.580776572227478
I0311 15:58:14.308336 140203875297024 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.24071910977363586, loss=1.607579231262207
I0311 15:58:51.687576 140203858511616 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.2134190797805786, loss=1.5929388999938965
I0311 15:59:29.009018 140203875297024 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.20761311054229736, loss=1.4845292568206787
I0311 16:00:06.336010 140203858511616 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.2145814597606659, loss=1.541823387145996
I0311 16:00:43.652181 140203875297024 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.2125479131937027, loss=1.5052074193954468
I0311 16:01:20.994961 140203858511616 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.21524286270141602, loss=1.519461750984192
I0311 16:01:58.392106 140203875297024 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.20334328711032867, loss=1.558509349822998
I0311 16:02:35.735224 140203858511616 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.22004744410514832, loss=1.5140652656555176
I0311 16:03:13.104570 140203875297024 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.21735890209674835, loss=1.4978264570236206
I0311 16:03:50.436906 140203858511616 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.22300629317760468, loss=1.5612128973007202
I0311 16:04:27.785388 140203875297024 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.23910632729530334, loss=1.5162211656570435
I0311 16:05:05.174648 140203858511616 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.218425452709198, loss=1.536786675453186
I0311 16:05:42.566282 140203875297024 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.2204175442457199, loss=1.4898169040679932
I0311 16:06:19.909871 140203858511616 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.2259092479944229, loss=1.5557689666748047
I0311 16:06:57.252150 140203875297024 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.21783848106861115, loss=1.5194308757781982
I0311 16:07:34.580609 140203858511616 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.20806190371513367, loss=1.5314346551895142
I0311 16:08:03.064723 140391735396160 spec.py:298] Evaluating on the training split.
I0311 16:08:06.060523 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 16:11:40.686210 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 16:11:43.333459 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 16:14:20.960577 140391735396160 spec.py:326] Evaluating on the test split.
I0311 16:14:23.644581 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 16:16:57.231477 140391735396160 submission_runner.py:359] Time since start: 51083.38s, 	Step: 80978, 	{'train/accuracy': 0.6817448139190674, 'train/loss': 1.4554756879806519, 'train/bleu': 34.48976499985642, 'validation/accuracy': 0.6912003755569458, 'validation/loss': 1.3951085805892944, 'validation/bleu': 30.527759223907882, 'validation/num_examples': 3000, 'test/accuracy': 0.7058160901069641, 'test/loss': 1.2920541763305664, 'test/bleu': 30.49940086644642, 'test/num_examples': 3003}
I0311 16:16:57.245197 140203875297024 logging_writer.py:48] [80978] global_step=80978, preemption_count=0, score=30154.034724, test/accuracy=0.705816, test/bleu=30.499401, test/loss=1.292054, test/num_examples=3003, total_duration=51083.383937, train/accuracy=0.681745, train/bleu=34.489765, train/loss=1.455476, validation/accuracy=0.691200, validation/bleu=30.527759, validation/loss=1.395109, validation/num_examples=3000
I0311 16:16:58.267755 140391735396160 checkpoints.py:356] Saving checkpoint at step: 80978
I0311 16:17:01.627252 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_80978
I0311 16:17:01.630889 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_80978.
I0311 16:17:10.212994 140203858511616 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.20302289724349976, loss=1.485451579093933
I0311 16:17:47.486957 140203850118912 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.23495502769947052, loss=1.5386085510253906
I0311 16:18:24.758433 140203858511616 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.20798306167125702, loss=1.520258903503418
I0311 16:19:02.091307 140203850118912 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.21845602989196777, loss=1.608836054801941
I0311 16:19:39.408396 140203858511616 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.21852834522724152, loss=1.5194823741912842
I0311 16:20:16.743853 140203850118912 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.21165527403354645, loss=1.532284140586853
I0311 16:20:54.162793 140203858511616 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.21912704408168793, loss=1.5576032400131226
I0311 16:21:31.524821 140203850118912 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.2328346222639084, loss=1.4579483270645142
I0311 16:22:08.896722 140203858511616 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.2149796038866043, loss=1.4595263004302979
I0311 16:22:46.248308 140203850118912 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.21727217733860016, loss=1.579939603805542
I0311 16:23:23.610111 140203858511616 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.21502923965454102, loss=1.5363153219223022
I0311 16:24:00.947165 140203850118912 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.21242420375347137, loss=1.4535176753997803
I0311 16:24:38.283962 140203858511616 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.20984309911727905, loss=1.5218273401260376
I0311 16:25:15.679508 140203850118912 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.22139687836170197, loss=1.5542482137680054
I0311 16:25:53.035264 140203858511616 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.22018270194530487, loss=1.5324275493621826
I0311 16:26:30.369894 140203850118912 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.2098693549633026, loss=1.537522792816162
I0311 16:27:07.742818 140203858511616 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.20571041107177734, loss=1.4780093431472778
I0311 16:27:45.126201 140203850118912 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.20965127646923065, loss=1.5483945608139038
I0311 16:28:22.446607 140203858511616 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.22718387842178345, loss=1.5363515615463257
I0311 16:28:59.810245 140203850118912 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.21734707057476044, loss=1.5475047826766968
I0311 16:29:37.159876 140203858511616 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.23525884747505188, loss=1.4970948696136475
I0311 16:30:14.519069 140203850118912 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.22316336631774902, loss=1.5729414224624634
I0311 16:30:51.880049 140203858511616 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.21671028435230255, loss=1.5165351629257202
I0311 16:31:01.719789 140391735396160 spec.py:298] Evaluating on the training split.
I0311 16:31:04.710480 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 16:35:20.373319 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 16:35:23.020689 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 16:39:07.406435 140391735396160 spec.py:326] Evaluating on the test split.
I0311 16:39:10.096614 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 16:42:56.463295 140391735396160 submission_runner.py:359] Time since start: 52462.04s, 	Step: 83228, 	{'train/accuracy': 0.6859668493270874, 'train/loss': 1.4274961948394775, 'train/bleu': 34.81318755008213, 'validation/accuracy': 0.6921426653862, 'validation/loss': 1.3862097263336182, 'validation/bleu': 30.601693176419346, 'validation/num_examples': 3000, 'test/accuracy': 0.7078961133956909, 'test/loss': 1.2853219509124756, 'test/bleu': 30.627931250660627, 'test/num_examples': 3003}
I0311 16:42:56.477956 140203850118912 logging_writer.py:48] [83228] global_step=83228, preemption_count=0, score=30990.511691, test/accuracy=0.707896, test/bleu=30.627931, test/loss=1.285322, test/num_examples=3003, total_duration=52462.038973, train/accuracy=0.685967, train/bleu=34.813188, train/loss=1.427496, validation/accuracy=0.692143, validation/bleu=30.601693, validation/loss=1.386210, validation/num_examples=3000
I0311 16:42:57.479633 140391735396160 checkpoints.py:356] Saving checkpoint at step: 83228
I0311 16:43:00.807505 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_83228
I0311 16:43:00.811136 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_83228.
I0311 16:43:27.981397 140203858511616 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.22294308245182037, loss=1.5094826221466064
I0311 16:44:05.264783 140203841726208 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.21149811148643494, loss=1.5180002450942993
I0311 16:44:42.618016 140203858511616 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.2239457666873932, loss=1.5155961513519287
I0311 16:45:19.972392 140203841726208 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.21763618290424347, loss=1.5853432416915894
I0311 16:45:57.361994 140203858511616 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.20876330137252808, loss=1.4709824323654175
I0311 16:46:34.746199 140203841726208 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.22111783921718597, loss=1.5310795307159424
I0311 16:47:12.119428 140203858511616 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.2074270397424698, loss=1.5445245504379272
I0311 16:47:49.449536 140203841726208 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.22786974906921387, loss=1.513369083404541
I0311 16:48:26.807347 140203858511616 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.22414332628250122, loss=1.5264393091201782
I0311 16:49:04.144171 140203841726208 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.21943387389183044, loss=1.583773136138916
I0311 16:49:41.477525 140203858511616 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.21250367164611816, loss=1.4819059371948242
I0311 16:50:18.875468 140203841726208 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.21090315282344818, loss=1.5143373012542725
I0311 16:50:56.250389 140203858511616 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.21878622472286224, loss=1.5164499282836914
I0311 16:51:33.639183 140203841726208 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.21881265938282013, loss=1.4344264268875122
I0311 16:52:11.034802 140203858511616 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.22039419412612915, loss=1.467971682548523
I0311 16:52:48.430001 140203841726208 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.22701196372509003, loss=1.5385390520095825
I0311 16:53:25.804050 140203858511616 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.21352475881576538, loss=1.439995527267456
I0311 16:54:03.153419 140203841726208 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.20108680427074432, loss=1.4616165161132812
I0311 16:54:40.504088 140203858511616 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.2189824879169464, loss=1.4460861682891846
I0311 16:55:17.859851 140203841726208 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.2212715595960617, loss=1.4688485860824585
I0311 16:55:55.238627 140203858511616 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.2151261866092682, loss=1.5563489198684692
I0311 16:56:32.636067 140203841726208 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.2286868691444397, loss=1.5134397745132446
I0311 16:57:01.127455 140391735396160 spec.py:298] Evaluating on the training split.
I0311 16:57:04.111608 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 17:00:40.174289 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 17:00:42.815980 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 17:04:24.678880 140391735396160 spec.py:326] Evaluating on the test split.
I0311 17:04:27.358350 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 17:08:13.091397 140391735396160 submission_runner.py:359] Time since start: 54021.45s, 	Step: 85478, 	{'train/accuracy': 0.6822166442871094, 'train/loss': 1.4433878660202026, 'train/bleu': 34.79406336475668, 'validation/accuracy': 0.6916590929031372, 'validation/loss': 1.3844685554504395, 'validation/bleu': 30.701389399814072, 'validation/num_examples': 3000, 'test/accuracy': 0.7090930342674255, 'test/loss': 1.2804052829742432, 'test/bleu': 30.717892200030366, 'test/num_examples': 3003}
I0311 17:08:13.105301 140203858511616 logging_writer.py:48] [85478] global_step=85478, preemption_count=0, score=31827.341714, test/accuracy=0.709093, test/bleu=30.717892, test/loss=1.280405, test/num_examples=3003, total_duration=54021.446640, train/accuracy=0.682217, train/bleu=34.794063, train/loss=1.443388, validation/accuracy=0.691659, validation/bleu=30.701389, validation/loss=1.384469, validation/num_examples=3000
I0311 17:08:14.112248 140391735396160 checkpoints.py:356] Saving checkpoint at step: 85478
I0311 17:08:17.657378 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_85478
I0311 17:08:17.661021 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_85478.
I0311 17:08:26.232810 140203841726208 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.2133283019065857, loss=1.4804761409759521
I0311 17:09:03.434033 140203833333504 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.2168997973203659, loss=1.4689487218856812
I0311 17:09:40.744803 140203841726208 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.23568853735923767, loss=1.483063817024231
I0311 17:10:18.110242 140203833333504 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.21271584928035736, loss=1.5060197114944458
I0311 17:10:55.464160 140203841726208 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.4205647706985474, loss=1.525825023651123
I0311 17:11:32.807616 140203833333504 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.33000263571739197, loss=1.5278499126434326
I0311 17:12:10.190567 140203841726208 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.2172013819217682, loss=1.5644468069076538
I0311 17:12:47.542802 140203833333504 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.22165940701961517, loss=1.5404837131500244
I0311 17:13:24.881411 140203841726208 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.21581386029720306, loss=1.5282591581344604
I0311 17:14:02.282157 140203833333504 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.21172146499156952, loss=1.5134732723236084
I0311 17:14:39.652249 140203841726208 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.22885249555110931, loss=1.4550585746765137
I0311 17:15:17.015274 140203833333504 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.22041712701320648, loss=1.578076720237732
I0311 17:15:54.364633 140203841726208 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.23590706288814545, loss=1.4167320728302002
I0311 17:16:31.716534 140203833333504 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.3611140549182892, loss=1.466631531715393
I0311 17:17:09.077799 140203841726208 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.2242278903722763, loss=1.5256370306015015
I0311 17:17:46.429049 140203833333504 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.22039197385311127, loss=1.4621893167495728
I0311 17:18:23.768987 140203841726208 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.2257225662469864, loss=1.4417474269866943
I0311 17:19:01.123776 140203833333504 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.2102324664592743, loss=1.4873145818710327
I0311 17:19:38.451764 140203841726208 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.21505041420459747, loss=1.4509953260421753
I0311 17:20:15.815416 140203833333504 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.21923083066940308, loss=1.5514330863952637
I0311 17:20:53.187092 140203841726208 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.2301899641752243, loss=1.5317661762237549
I0311 17:21:30.553162 140203833333504 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.23094554245471954, loss=1.4901164770126343
I0311 17:22:07.919898 140203841726208 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.20658445358276367, loss=1.5124629735946655
I0311 17:22:17.726213 140391735396160 spec.py:298] Evaluating on the training split.
I0311 17:22:20.716072 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 17:26:30.706818 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 17:26:33.352393 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 17:29:38.767774 140391735396160 spec.py:326] Evaluating on the test split.
I0311 17:29:41.466266 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 17:33:01.504516 140391735396160 submission_runner.py:359] Time since start: 55538.05s, 	Step: 87728, 	{'train/accuracy': 0.6914902329444885, 'train/loss': 1.3971409797668457, 'train/bleu': 35.30333188174412, 'validation/accuracy': 0.6923658847808838, 'validation/loss': 1.379604697227478, 'validation/bleu': 30.496544646813923, 'validation/num_examples': 3000, 'test/accuracy': 0.709557831287384, 'test/loss': 1.2771984338760376, 'test/bleu': 30.618708815507016, 'test/num_examples': 3003}
I0311 17:33:01.519724 140203833333504 logging_writer.py:48] [87728] global_step=87728, preemption_count=0, score=32663.768563, test/accuracy=0.709558, test/bleu=30.618709, test/loss=1.277198, test/num_examples=3003, total_duration=55538.045414, train/accuracy=0.691490, train/bleu=35.303332, train/loss=1.397141, validation/accuracy=0.692366, validation/bleu=30.496545, validation/loss=1.379605, validation/num_examples=3000
I0311 17:33:02.518829 140391735396160 checkpoints.py:356] Saving checkpoint at step: 87728
I0311 17:33:06.754162 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_87728
I0311 17:33:06.757826 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_87728.
I0311 17:33:33.958211 140203841726208 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.2163795828819275, loss=1.4145623445510864
I0311 17:34:11.268733 140203824940800 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.23647178709506989, loss=1.4430792331695557
I0311 17:34:48.566347 140203841726208 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.21397705376148224, loss=1.4992637634277344
I0311 17:35:25.874662 140203824940800 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.22087042033672333, loss=1.45197594165802
I0311 17:36:03.247880 140203841726208 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.21794214844703674, loss=1.5048449039459229
I0311 17:36:40.612504 140203824940800 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.2230282872915268, loss=1.4940787553787231
I0311 17:37:17.977583 140203841726208 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.2171122431755066, loss=1.5006413459777832
I0311 17:37:55.349983 140203824940800 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.21826957166194916, loss=1.555129885673523
I0311 17:38:32.700493 140203841726208 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.22355234622955322, loss=1.5660935640335083
I0311 17:39:10.074199 140203824940800 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.2270546555519104, loss=1.535131812095642
I0311 17:39:47.444490 140203841726208 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.22482696175575256, loss=1.5378222465515137
I0311 17:40:24.810540 140203824940800 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.21908365190029144, loss=1.4973998069763184
I0311 17:41:02.203832 140203841726208 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.21277855336666107, loss=1.4677073955535889
I0311 17:41:39.595375 140203824940800 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.212144136428833, loss=1.5016897916793823
I0311 17:42:16.988759 140203841726208 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.22024859488010406, loss=1.5790191888809204
I0311 17:42:54.350483 140203824940800 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.22157415747642517, loss=1.5020815134048462
I0311 17:43:31.732841 140203841726208 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.22085870802402496, loss=1.4964452981948853
I0311 17:44:09.074308 140203824940800 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.20868191123008728, loss=1.4639965295791626
I0311 17:44:46.442264 140203841726208 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.23117659986019135, loss=1.4953711032867432
I0311 17:45:23.783616 140203824940800 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.23453988134860992, loss=1.4728611707687378
I0311 17:46:01.114553 140203841726208 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.2304905503988266, loss=1.5060956478118896
I0311 17:46:38.454589 140203824940800 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.2119515985250473, loss=1.4531376361846924
I0311 17:47:06.940698 140391735396160 spec.py:298] Evaluating on the training split.
I0311 17:47:09.954217 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 17:51:24.428404 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 17:51:27.066570 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 17:54:54.408504 140391735396160 spec.py:326] Evaluating on the test split.
I0311 17:54:57.091965 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 17:58:41.030746 140391735396160 submission_runner.py:359] Time since start: 57027.26s, 	Step: 89978, 	{'train/accuracy': 0.6903082132339478, 'train/loss': 1.4035587310791016, 'train/bleu': 35.2467082513542, 'validation/accuracy': 0.6933701634407043, 'validation/loss': 1.3752750158309937, 'validation/bleu': 30.606563118779707, 'validation/num_examples': 3000, 'test/accuracy': 0.7105804681777954, 'test/loss': 1.2725517749786377, 'test/bleu': 30.839185565110792, 'test/num_examples': 3003}
I0311 17:58:41.045508 140203841726208 logging_writer.py:48] [89978] global_step=89978, preemption_count=0, score=33500.277763, test/accuracy=0.710580, test/bleu=30.839186, test/loss=1.272552, test/num_examples=3003, total_duration=57027.259890, train/accuracy=0.690308, train/bleu=35.246708, train/loss=1.403559, validation/accuracy=0.693370, validation/bleu=30.606563, validation/loss=1.375275, validation/num_examples=3000
I0311 17:58:42.048097 140391735396160 checkpoints.py:356] Saving checkpoint at step: 89978
I0311 17:58:46.259108 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_89978
I0311 17:58:46.262571 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_89978.
I0311 17:58:54.812179 140203824940800 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.22382009029388428, loss=1.46773099899292
I0311 17:59:32.092021 140203816548096 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.2476944774389267, loss=1.53437077999115
I0311 18:00:09.415314 140203824940800 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.23127543926239014, loss=1.4850566387176514
I0311 18:00:46.740937 140203816548096 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.22342105209827423, loss=1.532547116279602
I0311 18:01:24.084479 140203824940800 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.22296097874641418, loss=1.4736056327819824
I0311 18:02:01.488845 140203816548096 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.21921496093273163, loss=1.4930036067962646
I0311 18:02:38.860677 140203824940800 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.22259452939033508, loss=1.4843639135360718
I0311 18:03:16.191937 140203816548096 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.22785009443759918, loss=1.4789491891860962
I0311 18:03:53.565172 140203824940800 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.21592682600021362, loss=1.533440351486206
I0311 18:04:30.938955 140203816548096 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.23483353853225708, loss=1.5390828847885132
I0311 18:05:08.303463 140203824940800 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.2164383828639984, loss=1.5576776266098022
I0311 18:05:45.681636 140203816548096 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.22812330722808838, loss=1.500491976737976
I0311 18:06:23.029084 140203824940800 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.21621526777744293, loss=1.5446540117263794
I0311 18:07:00.414283 140203816548096 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.310543954372406, loss=1.459578037261963
I0311 18:07:37.772841 140203824940800 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.24202848970890045, loss=1.4780889749526978
I0311 18:08:15.161882 140203816548096 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.2300787717103958, loss=1.5907914638519287
I0311 18:08:52.498502 140203824940800 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.21983326971530914, loss=1.5106892585754395
I0311 18:09:29.892052 140203816548096 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.21657881140708923, loss=1.5567634105682373
I0311 18:10:07.302612 140203824940800 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.26306596398353577, loss=1.5354959964752197
I0311 18:10:44.674311 140203816548096 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.21924792230129242, loss=1.4924770593643188
I0311 18:11:22.107212 140203824940800 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.22409433126449585, loss=1.5018675327301025
I0311 18:11:59.498610 140203816548096 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.21549491584300995, loss=1.410412311553955
I0311 18:12:36.870516 140203824940800 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.21977974474430084, loss=1.4505420923233032
I0311 18:12:46.308116 140391735396160 spec.py:298] Evaluating on the training split.
I0311 18:12:49.297429 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 18:16:39.840928 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 18:16:42.481065 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 18:19:48.617187 140391735396160 spec.py:326] Evaluating on the test split.
I0311 18:19:51.301332 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 18:23:05.627540 140391735396160 submission_runner.py:359] Time since start: 58566.63s, 	Step: 92227, 	{'train/accuracy': 0.6903812885284424, 'train/loss': 1.3982020616531372, 'train/bleu': 35.49914716995388, 'validation/accuracy': 0.6935809850692749, 'validation/loss': 1.374484658241272, 'validation/bleu': 30.704950089245195, 'validation/num_examples': 3000, 'test/accuracy': 0.7103829383850098, 'test/loss': 1.2687493562698364, 'test/bleu': 30.874784071524292, 'test/num_examples': 3003}
I0311 18:23:05.642859 140203816548096 logging_writer.py:48] [92227] global_step=92227, preemption_count=0, score=34336.830654, test/accuracy=0.710383, test/bleu=30.874784, test/loss=1.268749, test/num_examples=3003, total_duration=58566.627289, train/accuracy=0.690381, train/bleu=35.499147, train/loss=1.398202, validation/accuracy=0.693581, validation/bleu=30.704950, validation/loss=1.374485, validation/num_examples=3000
I0311 18:23:06.646525 140391735396160 checkpoints.py:356] Saving checkpoint at step: 92227
I0311 18:23:09.954561 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_92227
I0311 18:23:09.958122 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_92227.
I0311 18:23:37.542251 140203824940800 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.23588423430919647, loss=1.4709333181381226
I0311 18:24:14.799738 140203808155392 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.22852830588817596, loss=1.5020725727081299
I0311 18:24:52.158615 140203824940800 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.2146516591310501, loss=1.5073940753936768
I0311 18:25:29.532387 140203808155392 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.21889865398406982, loss=1.4623152017593384
I0311 18:26:06.854567 140203824940800 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.2161032110452652, loss=1.485816240310669
I0311 18:26:44.249829 140203808155392 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.22227446734905243, loss=1.4507973194122314
I0311 18:27:21.640304 140203824940800 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.21493147313594818, loss=1.5090680122375488
I0311 18:27:59.039643 140203808155392 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.21966484189033508, loss=1.487825632095337
I0311 18:28:36.430898 140203824940800 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.21469417214393616, loss=1.5027681589126587
I0311 18:29:13.811944 140203808155392 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.21738800406455994, loss=1.4155694246292114
I0311 18:29:51.189238 140203824940800 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.22075670957565308, loss=1.5594631433486938
I0311 18:30:28.569154 140203808155392 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.21354220807552338, loss=1.4876972436904907
I0311 18:31:05.952121 140203824940800 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.21104858815670013, loss=1.4680180549621582
I0311 18:31:43.297820 140203808155392 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.2299070656299591, loss=1.4689072370529175
I0311 18:32:20.673483 140203824940800 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.2096969038248062, loss=1.474875807762146
I0311 18:32:58.051811 140203808155392 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.21894222497940063, loss=1.4552778005599976
I0311 18:33:35.453892 140203824940800 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.23487496376037598, loss=1.4906681776046753
I0311 18:34:12.837831 140203808155392 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.216352641582489, loss=1.4266927242279053
I0311 18:34:50.194166 140203824940800 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.22072140872478485, loss=1.4822229146957397
I0311 18:35:27.578446 140203808155392 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.22348609566688538, loss=1.514990210533142
I0311 18:36:04.919095 140203824940800 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.2270452380180359, loss=1.514047622680664
I0311 18:36:42.285131 140203808155392 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.21405941247940063, loss=1.4834047555923462
I0311 18:37:10.052733 140391735396160 spec.py:298] Evaluating on the training split.
I0311 18:37:13.048600 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 18:41:00.530118 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 18:41:03.176641 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 18:44:25.141416 140391735396160 spec.py:326] Evaluating on the test split.
I0311 18:44:27.841876 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 18:47:36.617842 140391735396160 submission_runner.py:359] Time since start: 60030.37s, 	Step: 94476, 	{'train/accuracy': 0.6918635368347168, 'train/loss': 1.391923189163208, 'train/bleu': 35.201651907023745, 'validation/accuracy': 0.6936429738998413, 'validation/loss': 1.3719408512115479, 'validation/bleu': 30.82023337933146, 'validation/num_examples': 3000, 'test/accuracy': 0.710719883441925, 'test/loss': 1.2680233716964722, 'test/bleu': 30.792794776949965, 'test/num_examples': 3003}
I0311 18:47:36.632661 140203824940800 logging_writer.py:48] [94476] global_step=94476, preemption_count=0, score=35173.362765, test/accuracy=0.710720, test/bleu=30.792795, test/loss=1.268023, test/num_examples=3003, total_duration=60030.371926, train/accuracy=0.691864, train/bleu=35.201652, train/loss=1.391923, validation/accuracy=0.693643, validation/bleu=30.820233, validation/loss=1.371941, validation/num_examples=3000
I0311 18:47:37.636190 140391735396160 checkpoints.py:356] Saving checkpoint at step: 94476
I0311 18:47:40.971910 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_94476
I0311 18:47:40.975355 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_94476.
I0311 18:47:50.277904 140203808155392 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.21573154628276825, loss=1.5027167797088623
I0311 18:48:27.556668 140203799762688 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.20803596079349518, loss=1.4758552312850952
I0311 18:49:04.863809 140203808155392 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.2283058762550354, loss=1.5399783849716187
I0311 18:49:42.177751 140203799762688 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.21361972391605377, loss=1.5395982265472412
I0311 18:50:19.572541 140203808155392 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.2248818725347519, loss=1.5756139755249023
I0311 18:50:56.926560 140203799762688 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.21281051635742188, loss=1.4495748281478882
I0311 18:51:34.303747 140203808155392 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.21476340293884277, loss=1.4722994565963745
I0311 18:52:11.682189 140203799762688 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.22672420740127563, loss=1.4591201543807983
I0311 18:52:49.033369 140203808155392 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.23258158564567566, loss=1.4619702100753784
I0311 18:53:26.422653 140203799762688 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.22456856071949005, loss=1.4188700914382935
I0311 18:54:03.780126 140203808155392 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.22648638486862183, loss=1.5443248748779297
I0311 18:54:41.160512 140203799762688 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.22948728501796722, loss=1.3947739601135254
I0311 18:55:18.521898 140203808155392 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.2150907814502716, loss=1.5010484457015991
I0311 18:55:55.892207 140203799762688 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.2140093296766281, loss=1.5381840467453003
I0311 18:56:33.282516 140203808155392 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.22395777702331543, loss=1.5755879878997803
I0311 18:57:10.651998 140203799762688 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.21379248797893524, loss=1.4014027118682861
I0311 18:57:48.028663 140203808155392 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.2281501591205597, loss=1.4990426301956177
I0311 18:58:25.388383 140203799762688 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.2143976092338562, loss=1.4243355989456177
I0311 18:59:02.762639 140203808155392 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.21576781570911407, loss=1.4733996391296387
I0311 18:59:40.090366 140203799762688 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.21364262700080872, loss=1.513252854347229
I0311 19:00:17.465976 140203808155392 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.22803641855716705, loss=1.4561591148376465
I0311 19:00:54.803950 140203799762688 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.21687723696231842, loss=1.4407057762145996
I0311 19:01:32.206398 140203808155392 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.21629059314727783, loss=1.4700446128845215
I0311 19:01:41.267107 140391735396160 spec.py:298] Evaluating on the training split.
I0311 19:01:44.251376 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 19:05:43.915078 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 19:05:46.560067 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 19:09:16.337096 140391735396160 spec.py:326] Evaluating on the test split.
I0311 19:09:19.032091 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 19:12:53.374156 140391735396160 submission_runner.py:359] Time since start: 61501.59s, 	Step: 96726, 	{'train/accuracy': 0.689703106880188, 'train/loss': 1.4098316431045532, 'train/bleu': 35.83295074465345, 'validation/accuracy': 0.6939777731895447, 'validation/loss': 1.3714425563812256, 'validation/bleu': 30.71248806223062, 'validation/num_examples': 3000, 'test/accuracy': 0.7108709812164307, 'test/loss': 1.2669519186019897, 'test/bleu': 30.868209545604145, 'test/num_examples': 3003}
I0311 19:12:53.390121 140203799762688 logging_writer.py:48] [96726] global_step=96726, preemption_count=0, score=36010.079800, test/accuracy=0.710871, test/bleu=30.868210, test/loss=1.266952, test/num_examples=3003, total_duration=61501.586310, train/accuracy=0.689703, train/bleu=35.832951, train/loss=1.409832, validation/accuracy=0.693978, validation/bleu=30.712488, validation/loss=1.371443, validation/num_examples=3000
I0311 19:12:54.391657 140391735396160 checkpoints.py:356] Saving checkpoint at step: 96726
I0311 19:12:57.757534 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_96726
I0311 19:12:57.761121 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_96726.
I0311 19:13:25.712160 140203808155392 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.21852728724479675, loss=1.5351722240447998
I0311 19:14:03.027081 140203791369984 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.20490342378616333, loss=1.4861924648284912
I0311 19:14:40.345479 140203808155392 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.2275620847940445, loss=1.5013959407806396
I0311 19:15:17.691259 140203791369984 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.21448999643325806, loss=1.4681512117385864
I0311 19:15:55.070471 140203808155392 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.21174201369285583, loss=1.499916672706604
I0311 19:16:32.397595 140203791369984 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.21441400051116943, loss=1.4301203489303589
I0311 19:17:09.719268 140203808155392 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.22093190252780914, loss=1.518293857574463
I0311 19:17:47.069337 140203791369984 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.21261385083198547, loss=1.456790804862976
I0311 19:18:24.461934 140203808155392 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.20871980488300323, loss=1.470741629600525
I0311 19:19:01.823227 140203791369984 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.21243895590305328, loss=1.4843510389328003
I0311 19:19:39.210710 140203808155392 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.22491373121738434, loss=1.4150514602661133
I0311 19:20:16.547040 140203791369984 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.21682284772396088, loss=1.4510740041732788
I0311 19:20:53.888201 140203808155392 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.21859538555145264, loss=1.4765926599502563
I0311 19:21:31.248683 140203791369984 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.21825462579727173, loss=1.3761636018753052
I0311 19:22:08.617110 140203808155392 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.21256037056446075, loss=1.477847933769226
I0311 19:22:45.976834 140203791369984 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.2073899805545807, loss=1.4835469722747803
I0311 19:23:23.316232 140203808155392 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.22693027555942535, loss=1.4266550540924072
I0311 19:24:00.717574 140203791369984 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.2262077033519745, loss=1.4665534496307373
I0311 19:24:38.071983 140203808155392 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.21977943181991577, loss=1.413298487663269
I0311 19:25:15.482590 140203791369984 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.21650704741477966, loss=1.4334100484848022
I0311 19:25:52.864585 140203808155392 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.218996062874794, loss=1.4952917098999023
I0311 19:26:30.191711 140203791369984 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.22955606877803802, loss=1.5706881284713745
I0311 19:26:57.926764 140391735396160 spec.py:298] Evaluating on the training split.
I0311 19:27:00.921879 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 19:30:59.979640 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 19:31:02.629237 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 19:34:27.706250 140391735396160 spec.py:326] Evaluating on the test split.
I0311 19:34:30.400737 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 19:37:56.924886 140391735396160 submission_runner.py:359] Time since start: 63018.25s, 	Step: 98976, 	{'train/accuracy': 0.6885139346122742, 'train/loss': 1.4143635034561157, 'train/bleu': 35.69056247505244, 'validation/accuracy': 0.6941885352134705, 'validation/loss': 1.371662974357605, 'validation/bleu': 30.691709682764603, 'validation/num_examples': 3000, 'test/accuracy': 0.7110801339149475, 'test/loss': 1.2663038969039917, 'test/bleu': 30.944697817919742, 'test/num_examples': 3003}
I0311 19:37:56.939924 140203808155392 logging_writer.py:48] [98976] global_step=98976, preemption_count=0, score=36846.668121, test/accuracy=0.711080, test/bleu=30.944698, test/loss=1.266304, test/num_examples=3003, total_duration=63018.245972, train/accuracy=0.688514, train/bleu=35.690562, train/loss=1.414364, validation/accuracy=0.694189, validation/bleu=30.691710, validation/loss=1.371663, validation/num_examples=3000
I0311 19:37:57.941091 140391735396160 checkpoints.py:356] Saving checkpoint at step: 98976
I0311 19:38:01.317169 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_98976
I0311 19:38:01.320656 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_98976.
I0311 19:38:10.639632 140203791369984 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.22843776643276215, loss=1.5019744634628296
I0311 19:38:47.934662 140203782977280 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.20972080528736115, loss=1.4212591648101807
I0311 19:39:25.248169 140203791369984 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.21237525343894958, loss=1.460548758506775
I0311 19:40:02.617073 140203782977280 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.22185949981212616, loss=1.4978954792022705
I0311 19:40:39.973869 140203791369984 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.21617086231708527, loss=1.4439817667007446
I0311 19:41:17.342664 140203782977280 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.23324215412139893, loss=1.3918874263763428
I0311 19:41:54.710558 140203791369984 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.2281731516122818, loss=1.4481863975524902
I0311 19:42:32.104560 140203782977280 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.22547829151153564, loss=1.3949285745620728
I0311 19:43:09.489238 140203791369984 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.2406754046678543, loss=1.460739254951477
I0311 19:43:46.870951 140203782977280 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.211899533867836, loss=1.4315670728683472
I0311 19:44:24.267724 140203791369984 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.22278422117233276, loss=1.4882705211639404
I0311 19:45:01.620458 140203782977280 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.2140771746635437, loss=1.4631175994873047
I0311 19:45:39.006754 140203791369984 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.21570666134357452, loss=1.4539750814437866
I0311 19:46:16.377959 140203782977280 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.2349414825439453, loss=1.4570051431655884
I0311 19:46:53.771910 140203791369984 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.21760398149490356, loss=1.4889823198318481
I0311 19:47:31.165912 140203782977280 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.22200851142406464, loss=1.5385557413101196
I0311 19:48:08.541048 140203791369984 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.21901777386665344, loss=1.4666051864624023
I0311 19:48:45.954959 140203782977280 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.2156403362751007, loss=1.466638207435608
I0311 19:49:23.349529 140203791369984 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.21367067098617554, loss=1.4939546585083008
I0311 19:50:00.742067 140203782977280 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.22275759279727936, loss=1.46079421043396
I0311 19:50:38.124601 140203791369984 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.22229830920696259, loss=1.4302499294281006
I0311 19:51:15.492598 140203782977280 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.2114206701517105, loss=1.4409692287445068
I0311 19:51:52.929536 140203791369984 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.21190963685512543, loss=1.5238312482833862
I0311 19:52:01.615006 140391735396160 spec.py:298] Evaluating on the training split.
I0311 19:52:04.607540 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 19:55:56.390468 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 19:55:59.026039 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 19:59:27.992988 140391735396160 spec.py:326] Evaluating on the test split.
I0311 19:59:30.692977 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 20:02:52.619226 140391735396160 submission_runner.py:359] Time since start: 64521.93s, 	Step: 101225, 	{'train/accuracy': 0.6931129097938538, 'train/loss': 1.3867099285125732, 'train/bleu': 35.717257293316706, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0311 20:02:52.635313 140203782977280 logging_writer.py:48] [101225] global_step=101225, preemption_count=0, score=37683.344819, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=64521.934218, train/accuracy=0.693113, train/bleu=35.717257, train/loss=1.386710, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0311 20:02:53.638012 140391735396160 checkpoints.py:356] Saving checkpoint at step: 101225
I0311 20:02:57.512675 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_101225
I0311 20:02:57.516318 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_101225.
I0311 20:03:25.849957 140203791369984 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.2126922607421875, loss=1.4639947414398193
I0311 20:04:03.129722 140203774584576 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.21637843549251556, loss=1.4251091480255127
I0311 20:04:40.463598 140203791369984 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.2065250724554062, loss=1.4856102466583252
I0311 20:05:17.804851 140203774584576 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.22414936125278473, loss=1.5193681716918945
I0311 20:05:55.182003 140203791369984 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.2206704020500183, loss=1.5412803888320923
I0311 20:06:32.558730 140203774584576 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.21482272446155548, loss=1.4628826379776
I0311 20:07:09.923420 140203791369984 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.22137203812599182, loss=1.56972336769104
I0311 20:07:47.313959 140203774584576 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.21631354093551636, loss=1.4446070194244385
I0311 20:08:24.651462 140203791369984 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.2188480645418167, loss=1.5164260864257812
I0311 20:09:01.999882 140203774584576 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.22868458926677704, loss=1.513058066368103
I0311 20:09:39.369666 140203791369984 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.22044771909713745, loss=1.4562561511993408
I0311 20:10:16.734880 140203774584576 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.2152058482170105, loss=1.493191123008728
I0311 20:10:54.135856 140203791369984 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.21641667187213898, loss=1.50485098361969
I0311 20:11:31.547436 140203774584576 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.2117643505334854, loss=1.443825125694275
I0311 20:12:08.897587 140203791369984 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.20969100296497345, loss=1.5829962491989136
I0311 20:12:46.291277 140203774584576 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.22282473742961884, loss=1.4800835847854614
I0311 20:13:23.642239 140203791369984 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.21362221240997314, loss=1.4368840456008911
I0311 20:14:01.008455 140203774584576 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.22155079245567322, loss=1.4593948125839233
I0311 20:14:38.417286 140203791369984 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.22700701653957367, loss=1.454192042350769
I0311 20:15:15.784622 140203774584576 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.2120949625968933, loss=1.4676861763000488
I0311 20:15:53.138836 140203791369984 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.20919273793697357, loss=1.4929944276809692
I0311 20:16:30.511076 140203774584576 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.21769879758358002, loss=1.5018881559371948
I0311 20:16:57.883473 140391735396160 spec.py:298] Evaluating on the training split.
I0311 20:17:00.867280 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 20:20:56.291857 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 20:20:58.940132 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 20:24:27.872075 140391735396160 spec.py:326] Evaluating on the test split.
I0311 20:24:30.567819 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 20:27:52.432104 140391735396160 submission_runner.py:359] Time since start: 66018.20s, 	Step: 103475, 	{'train/accuracy': 0.6902816295623779, 'train/loss': 1.4033467769622803, 'train/bleu': 35.037440737438686, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0311 20:27:52.447759 140203791369984 logging_writer.py:48] [103475] global_step=103475, preemption_count=0, score=38520.103232, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=66018.202681, train/accuracy=0.690282, train/bleu=35.037441, train/loss=1.403347, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0311 20:27:53.432365 140391735396160 checkpoints.py:356] Saving checkpoint at step: 103475
I0311 20:27:57.930861 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_103475
I0311 20:27:57.934333 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_103475.
I0311 20:28:07.626961 140203774584576 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.20567618310451508, loss=1.4326319694519043
I0311 20:28:44.898481 140203766191872 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.21257713437080383, loss=1.4444280862808228
I0311 20:29:22.227498 140203774584576 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.2200208455324173, loss=1.39982008934021
I0311 20:29:59.565875 140203766191872 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.22538812458515167, loss=1.4848321676254272
I0311 20:30:36.892535 140203774584576 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.22069333493709564, loss=1.468636155128479
I0311 20:31:14.225342 140203766191872 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.3928017318248749, loss=1.4608359336853027
I0311 20:31:51.562327 140203774584576 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.22120767831802368, loss=1.464215874671936
I0311 20:32:28.926590 140203766191872 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.21561475098133087, loss=1.5193440914154053
I0311 20:33:06.272538 140203774584576 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.20688220858573914, loss=1.4694520235061646
I0311 20:33:43.639739 140203766191872 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.2128640115261078, loss=1.4536993503570557
I0311 20:34:21.042346 140203774584576 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.20765715837478638, loss=1.4238508939743042
I0311 20:34:58.451499 140203766191872 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.2336490899324417, loss=1.464469313621521
I0311 20:35:35.813623 140203774584576 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.2086097002029419, loss=1.4805058240890503
I0311 20:36:13.176481 140203766191872 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.2141152173280716, loss=1.43268883228302
I0311 20:36:50.525120 140203774584576 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.24786269664764404, loss=1.4872357845306396
I0311 20:37:27.887807 140203766191872 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.20908892154693604, loss=1.4681532382965088
I0311 20:38:05.233688 140203774584576 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.2239675670862198, loss=1.5600148439407349
I0311 20:38:42.576424 140203766191872 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.216301828622818, loss=1.521521806716919
I0311 20:39:19.953008 140203774584576 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.22385698556900024, loss=1.475603699684143
I0311 20:39:57.327293 140203766191872 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.21855735778808594, loss=1.457865595817566
I0311 20:40:34.696774 140203774584576 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.22231635451316833, loss=1.478973150253296
I0311 20:41:12.030775 140203766191872 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.21733547747135162, loss=1.4931037425994873
I0311 20:41:49.379091 140203774584576 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.21887068450450897, loss=1.468267560005188
I0311 20:41:58.055148 140391735396160 spec.py:298] Evaluating on the training split.
I0311 20:42:01.050258 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 20:46:07.132949 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 20:46:09.778081 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 20:49:38.828635 140391735396160 spec.py:326] Evaluating on the test split.
I0311 20:49:41.522569 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 20:53:03.403443 140391735396160 submission_runner.py:359] Time since start: 67518.37s, 	Step: 105725, 	{'train/accuracy': 0.6951080560684204, 'train/loss': 1.3812083005905151, 'train/bleu': 35.3259972961879, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0311 20:53:03.419510 140203766191872 logging_writer.py:48] [105725] global_step=105725, preemption_count=0, score=39356.511411, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=67518.374342, train/accuracy=0.695108, train/bleu=35.325997, train/loss=1.381208, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0311 20:53:04.418056 140391735396160 checkpoints.py:356] Saving checkpoint at step: 105725
I0311 20:53:08.211901 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_105725
I0311 20:53:08.215661 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_105725.
I0311 20:53:36.546042 140203774584576 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.2165273278951645, loss=1.4654499292373657
I0311 20:54:13.833717 140203757799168 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.2284964621067047, loss=1.4637527465820312
I0311 20:54:51.126332 140203774584576 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.21324780583381653, loss=1.5399937629699707
I0311 20:55:28.465671 140203757799168 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.21400439739227295, loss=1.5103816986083984
I0311 20:56:05.802286 140203774584576 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.21437732875347137, loss=1.5084044933319092
I0311 20:56:43.156565 140203757799168 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.5234674215316772, loss=1.5188486576080322
I0311 20:57:20.571475 140203774584576 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.2208503931760788, loss=1.4880702495574951
I0311 20:57:57.883198 140203757799168 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.4577125608921051, loss=1.4469847679138184
I0311 20:58:35.246469 140203774584576 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.22702385485172272, loss=1.4306750297546387
I0311 20:59:12.638089 140203757799168 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.23047998547554016, loss=1.4447131156921387
I0311 20:59:49.990878 140203774584576 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.20806923508644104, loss=1.5039942264556885
I0311 21:00:27.365391 140203757799168 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.22757913172245026, loss=1.532188892364502
I0311 21:01:04.756144 140203774584576 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.21595025062561035, loss=1.4644966125488281
I0311 21:01:42.184958 140203757799168 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.2151220142841339, loss=1.4946656227111816
I0311 21:02:19.568034 140203774584576 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.22054195404052734, loss=1.4404103755950928
I0311 21:02:56.933793 140203757799168 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.21783028542995453, loss=1.532866358757019
I0311 21:03:34.332108 140203774584576 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.2283308207988739, loss=1.5322554111480713
I0311 21:04:11.676285 140203757799168 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.21859058737754822, loss=1.4680291414260864
I0311 21:04:49.061355 140203774584576 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.22855226695537567, loss=1.5118244886398315
I0311 21:05:26.375543 140203757799168 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.20827284455299377, loss=1.3818731307983398
I0311 21:06:03.752421 140203774584576 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.213042214512825, loss=1.4642268419265747
I0311 21:06:41.126910 140203757799168 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.20516103506088257, loss=1.4547330141067505
I0311 21:07:08.506257 140391735396160 spec.py:298] Evaluating on the training split.
I0311 21:07:11.494662 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 21:11:05.374097 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 21:11:08.029674 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 21:14:36.983340 140391735396160 spec.py:326] Evaluating on the test split.
I0311 21:14:39.686288 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 21:18:01.558472 140391735396160 submission_runner.py:359] Time since start: 69028.83s, 	Step: 107975, 	{'train/accuracy': 0.6911378502845764, 'train/loss': 1.4004404544830322, 'train/bleu': 35.76648938807981, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0311 21:18:01.575363 140203774584576 logging_writer.py:48] [107975] global_step=107975, preemption_count=0, score=40193.350431, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=69028.825454, train/accuracy=0.691138, train/bleu=35.766489, train/loss=1.400440, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0311 21:18:02.578636 140391735396160 checkpoints.py:356] Saving checkpoint at step: 107975
I0311 21:18:05.943035 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_107975
I0311 21:18:05.946624 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_107975.
I0311 21:18:15.642653 140203757799168 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.21275021135807037, loss=1.5175286531448364
I0311 21:18:52.907403 140203749406464 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.22236083447933197, loss=1.499507188796997
I0311 21:19:30.250252 140203757799168 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.20463460683822632, loss=1.4369661808013916
I0311 21:20:07.651320 140203749406464 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.21546082198619843, loss=1.4056096076965332
I0311 21:20:45.028746 140203757799168 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.2194567322731018, loss=1.4514020681381226
I0311 21:21:22.420171 140203749406464 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.21721699833869934, loss=1.4107227325439453
I0311 21:21:59.808351 140203757799168 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.21706539392471313, loss=1.5045289993286133
I0311 21:22:37.184599 140203749406464 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.22509446740150452, loss=1.4869840145111084
I0311 21:23:14.564216 140203757799168 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.21935200691223145, loss=1.4447078704833984
I0311 21:23:51.949678 140203749406464 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.2212456464767456, loss=1.4886351823806763
I0311 21:24:29.305970 140203757799168 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.22619013488292694, loss=1.4494810104370117
I0311 21:25:06.735491 140203749406464 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.21867258846759796, loss=1.3953179121017456
I0311 21:25:44.108315 140203757799168 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.22106590867042542, loss=1.5137929916381836
I0311 21:26:21.520631 140203749406464 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.21791589260101318, loss=1.4314794540405273
I0311 21:26:58.845122 140203757799168 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.2068900763988495, loss=1.4325836896896362
I0311 21:27:36.250606 140203749406464 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.2183365672826767, loss=1.4258302450180054
I0311 21:28:13.643718 140203757799168 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.22404853999614716, loss=1.4175528287887573
I0311 21:28:51.004020 140203749406464 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.2149709165096283, loss=1.49361252784729
I0311 21:29:28.348046 140203757799168 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.21529078483581543, loss=1.4625276327133179
I0311 21:30:05.669220 140203749406464 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.22468045353889465, loss=1.4727696180343628
I0311 21:30:43.051239 140203757799168 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.21734122931957245, loss=1.452276349067688
I0311 21:31:20.427697 140203749406464 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.20603981614112854, loss=1.4157251119613647
I0311 21:31:57.764801 140203757799168 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.22531233727931976, loss=1.5275828838348389
I0311 21:32:06.088692 140391735396160 spec.py:298] Evaluating on the training split.
I0311 21:32:09.066076 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 21:36:04.905461 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 21:36:07.560761 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 21:39:36.777475 140391735396160 spec.py:326] Evaluating on the test split.
I0311 21:39:39.483417 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 21:43:01.293239 140391735396160 submission_runner.py:359] Time since start: 70526.41s, 	Step: 110224, 	{'train/accuracy': 0.6900483965873718, 'train/loss': 1.4049930572509766, 'train/bleu': 35.01723833282688, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0311 21:43:01.309092 140203749406464 logging_writer.py:48] [110224] global_step=110224, preemption_count=0, score=41030.019679, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=70526.407904, train/accuracy=0.690048, train/bleu=35.017238, train/loss=1.404993, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0311 21:43:02.311156 140391735396160 checkpoints.py:356] Saving checkpoint at step: 110224
I0311 21:43:05.689707 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_110224
I0311 21:43:05.693303 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_110224.
I0311 21:43:34.412974 140203757799168 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.21977438032627106, loss=1.4416306018829346
I0311 21:44:11.662271 140203741013760 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.2222997397184372, loss=1.5225133895874023
I0311 21:44:49.002684 140203757799168 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.21433036029338837, loss=1.4495278596878052
I0311 21:45:26.351453 140203741013760 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.38898926973342896, loss=1.5041894912719727
I0311 21:46:03.676096 140203757799168 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.22561083734035492, loss=1.5694892406463623
I0311 21:46:41.051541 140203741013760 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.22371692955493927, loss=1.4214524030685425
I0311 21:47:18.382007 140203757799168 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.20393335819244385, loss=1.4483035802841187
I0311 21:47:55.744570 140203741013760 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.21740005910396576, loss=1.519166350364685
I0311 21:48:33.063082 140203757799168 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.2078358381986618, loss=1.4491065740585327
I0311 21:49:10.431412 140203741013760 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.2170732319355011, loss=1.3810832500457764
I0311 21:49:47.811413 140203757799168 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.22140678763389587, loss=1.4618313312530518
I0311 21:50:25.167748 140203741013760 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.22607269883155823, loss=1.5160799026489258
I0311 21:51:02.575647 140203757799168 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.2003944367170334, loss=1.462971568107605
I0311 21:51:39.927295 140203741013760 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.210777148604393, loss=1.4079272747039795
I0311 21:52:17.328174 140203757799168 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.2203366458415985, loss=1.3878296613693237
I0311 21:52:54.684708 140203741013760 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.21257051825523376, loss=1.4398086071014404
I0311 21:53:32.057285 140203757799168 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.22609943151474, loss=1.4898810386657715
I0311 21:54:09.432330 140203741013760 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.2785690724849701, loss=1.4435291290283203
I0311 21:54:46.792793 140203757799168 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.22476699948310852, loss=1.4307851791381836
I0311 21:55:24.150529 140203741013760 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.217934250831604, loss=1.459932565689087
I0311 21:56:01.503292 140203757799168 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.23477555811405182, loss=1.5562106370925903
I0311 21:56:38.930400 140203741013760 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.23129397630691528, loss=1.4602230787277222
I0311 21:57:05.893419 140391735396160 spec.py:298] Evaluating on the training split.
I0311 21:57:08.879081 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 22:00:55.042031 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 22:00:57.680975 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 22:04:26.705198 140391735396160 spec.py:326] Evaluating on the test split.
I0311 22:04:29.397553 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 22:07:51.285533 140391735396160 submission_runner.py:359] Time since start: 72026.21s, 	Step: 112474, 	{'train/accuracy': 0.6925830245018005, 'train/loss': 1.3915064334869385, 'train/bleu': 35.27959612043446, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0311 22:07:51.301519 140203757799168 logging_writer.py:48] [112474] global_step=112474, preemption_count=0, score=41866.764220, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=72026.212632, train/accuracy=0.692583, train/bleu=35.279596, train/loss=1.391506, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0311 22:07:52.298625 140391735396160 checkpoints.py:356] Saving checkpoint at step: 112474
I0311 22:07:55.653599 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_112474
I0311 22:07:55.657279 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_112474.
I0311 22:08:05.726831 140203741013760 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.22896669805049896, loss=1.4822242259979248
I0311 22:08:42.968395 140203732621056 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.22579547762870789, loss=1.496346116065979
I0311 22:09:20.318315 140203741013760 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.23002974689006805, loss=1.453467845916748
I0311 22:09:57.635713 140203732621056 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.22479163110256195, loss=1.5107780694961548
I0311 22:10:34.978716 140203741013760 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.2138717621564865, loss=1.4612576961517334
I0311 22:11:12.373616 140203732621056 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.21590736508369446, loss=1.4847936630249023
I0311 22:11:49.722490 140203741013760 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.213605597615242, loss=1.4469223022460938
I0311 22:12:27.050136 140203732621056 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.21892423927783966, loss=1.4967771768569946
I0311 22:13:04.421887 140203741013760 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.21584419906139374, loss=1.489074468612671
I0311 22:13:41.756600 140203732621056 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.21679653227329254, loss=1.4533370733261108
I0311 22:14:19.082272 140203741013760 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.2142927199602127, loss=1.4642586708068848
I0311 22:14:56.424273 140203732621056 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.22011157870292664, loss=1.4849728345870972
I0311 22:15:33.761989 140203741013760 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.21151934564113617, loss=1.467917561531067
I0311 22:16:11.212486 140203732621056 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.2145092934370041, loss=1.4807143211364746
I0311 22:16:48.607422 140203741013760 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.21597979962825775, loss=1.4259591102600098
I0311 22:17:26.017467 140203732621056 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.21376514434814453, loss=1.4953601360321045
I0311 22:18:03.424423 140203741013760 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.21179093420505524, loss=1.4463355541229248
I0311 22:18:40.790928 140203732621056 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.22484254837036133, loss=1.5232551097869873
I0311 22:19:18.163433 140203741013760 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.2172105312347412, loss=1.4862282276153564
I0311 22:19:55.510445 140203732621056 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.2128833681344986, loss=1.4277664422988892
I0311 22:20:32.901982 140203741013760 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.2175663411617279, loss=1.4598119258880615
I0311 22:21:10.245613 140203732621056 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.23010674118995667, loss=1.4438416957855225
I0311 22:21:47.581287 140203741013760 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.2185649424791336, loss=1.473304033279419
I0311 22:21:55.901118 140391735396160 spec.py:298] Evaluating on the training split.
I0311 22:21:58.897192 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 22:25:48.847879 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 22:25:51.497540 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 22:29:20.397823 140391735396160 spec.py:326] Evaluating on the test split.
I0311 22:29:23.080602 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 22:32:44.863430 140391735396160 submission_runner.py:359] Time since start: 73516.22s, 	Step: 114724, 	{'train/accuracy': 0.6869683861732483, 'train/loss': 1.4257354736328125, 'train/bleu': 35.07010530580806, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0311 22:32:44.884034 140203732621056 logging_writer.py:48] [114724] global_step=114724, preemption_count=0, score=42703.494933, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=73516.220331, train/accuracy=0.686968, train/bleu=35.070105, train/loss=1.425735, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0311 22:32:45.886022 140391735396160 checkpoints.py:356] Saving checkpoint at step: 114724
I0311 22:32:49.430665 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_114724
I0311 22:32:49.434362 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_114724.
I0311 22:33:18.129176 140203741013760 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.2130126804113388, loss=1.5250579118728638
I0311 22:33:55.433388 140203724228352 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.22548222541809082, loss=1.5327473878860474
I0311 22:34:32.762163 140203741013760 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.21840205788612366, loss=1.4941654205322266
I0311 22:35:10.096689 140203724228352 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.2133031189441681, loss=1.429675817489624
I0311 22:35:47.463510 140203741013760 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.20794358849525452, loss=1.5172648429870605
I0311 22:36:24.862216 140203724228352 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.21804049611091614, loss=1.4261349439620972
I0311 22:37:02.268506 140203741013760 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.24717509746551514, loss=1.4407014846801758
I0311 22:37:39.630875 140203724228352 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.21873612701892853, loss=1.4455182552337646
I0311 22:38:16.972769 140203741013760 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.21979650855064392, loss=1.4223275184631348
I0311 22:38:54.325137 140203724228352 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.21616163849830627, loss=1.4909731149673462
I0311 22:39:31.681352 140203741013760 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.21630556881427765, loss=1.4635839462280273
I0311 22:40:09.013393 140203724228352 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.22950400412082672, loss=1.524796724319458
I0311 22:40:46.435307 140203741013760 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.2188512682914734, loss=1.4960862398147583
I0311 22:41:23.790884 140203724228352 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.21428050100803375, loss=1.4129655361175537
I0311 22:42:01.128770 140203741013760 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.22012503445148468, loss=1.48043692111969
I0311 22:42:38.519909 140203724228352 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.2227817177772522, loss=1.4705356359481812
I0311 22:43:15.929210 140203741013760 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.21640244126319885, loss=1.460162878036499
I0311 22:43:53.276117 140203724228352 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.22559380531311035, loss=1.5100802183151245
I0311 22:44:30.633480 140203741013760 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.21343864500522614, loss=1.4752947092056274
I0311 22:45:07.987519 140203724228352 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.21467725932598114, loss=1.4161081314086914
I0311 22:45:45.345136 140203741013760 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.22446905076503754, loss=1.4946200847625732
I0311 22:46:22.731062 140203724228352 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.20717333257198334, loss=1.4178156852722168
I0311 22:46:49.723949 140391735396160 spec.py:298] Evaluating on the training split.
I0311 22:46:52.709637 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 22:50:49.737962 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 22:50:52.368729 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 22:54:21.121789 140391735396160 spec.py:326] Evaluating on the test split.
I0311 22:54:23.823836 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 22:57:45.629615 140391735396160 submission_runner.py:359] Time since start: 75010.04s, 	Step: 116974, 	{'train/accuracy': 0.6905570030212402, 'train/loss': 1.4027093648910522, 'train/bleu': 35.4081560106899, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0311 22:57:45.646147 140203741013760 logging_writer.py:48] [116974] global_step=116974, preemption_count=0, score=43540.293375, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=75010.043146, train/accuracy=0.690557, train/bleu=35.408156, train/loss=1.402709, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0311 22:57:46.641614 140391735396160 checkpoints.py:356] Saving checkpoint at step: 116974
I0311 22:57:50.888354 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_116974
I0311 22:57:50.891928 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_116974.
I0311 22:58:00.954206 140203724228352 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.21424974501132965, loss=1.5205196142196655
I0311 22:58:38.231533 140203715835648 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.22338387370109558, loss=1.4594066143035889
I0311 22:59:15.533850 140203724228352 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.21360386908054352, loss=1.4173048734664917
I0311 22:59:52.830597 140203715835648 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.2131049931049347, loss=1.5147150754928589
I0311 23:00:30.196578 140203724228352 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.21506106853485107, loss=1.497970700263977
I0311 23:01:07.569990 140203715835648 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.2175455093383789, loss=1.52556574344635
I0311 23:01:44.929494 140203724228352 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.21604923903942108, loss=1.4914155006408691
I0311 23:02:22.290498 140203715835648 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.21061639487743378, loss=1.4369961023330688
I0311 23:02:59.657105 140203724228352 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.23620223999023438, loss=1.5628803968429565
I0311 23:03:36.990307 140203715835648 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.20058110356330872, loss=1.4528087377548218
I0311 23:04:14.347037 140203724228352 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.22619979083538055, loss=1.4914281368255615
I0311 23:04:51.708032 140203715835648 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.226264089345932, loss=1.4449317455291748
I0311 23:05:29.068223 140203724228352 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.22891263663768768, loss=1.4531816244125366
I0311 23:06:06.485107 140203715835648 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.20787517726421356, loss=1.426887035369873
I0311 23:06:43.858743 140203724228352 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.221220001578331, loss=1.516758918762207
I0311 23:07:21.215533 140203715835648 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.2235303521156311, loss=1.4626758098602295
I0311 23:07:58.602399 140203724228352 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.21122467517852783, loss=1.4816339015960693
I0311 23:08:35.960257 140203715835648 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.2191259115934372, loss=1.566542387008667
I0311 23:09:13.297351 140203724228352 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.23089227080345154, loss=1.50625479221344
I0311 23:09:50.677725 140203715835648 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.22555038332939148, loss=1.4646097421646118
I0311 23:10:28.009698 140203724228352 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.21578969061374664, loss=1.5295270681381226
I0311 23:11:05.347772 140203715835648 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.21374653279781342, loss=1.3529611825942993
I0311 23:11:42.690348 140203724228352 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.2233792543411255, loss=1.4907461404800415
I0311 23:11:50.987324 140391735396160 spec.py:298] Evaluating on the training split.
I0311 23:11:53.976480 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 23:15:56.465248 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 23:15:59.104377 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 23:19:28.015936 140391735396160 spec.py:326] Evaluating on the test split.
I0311 23:19:30.712096 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 23:22:52.420781 140391735396160 submission_runner.py:359] Time since start: 76511.31s, 	Step: 119224, 	{'train/accuracy': 0.692175030708313, 'train/loss': 1.395777702331543, 'train/bleu': 35.260293399713966, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0311 23:22:52.437315 140203715835648 logging_writer.py:48] [119224] global_step=119224, preemption_count=0, score=44376.955479, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=76511.306539, train/accuracy=0.692175, train/bleu=35.260293, train/loss=1.395778, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0311 23:22:53.428182 140391735396160 checkpoints.py:356] Saving checkpoint at step: 119224
I0311 23:22:57.641616 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_119224
I0311 23:22:57.645241 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_119224.
I0311 23:23:26.362986 140203724228352 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.21497796475887299, loss=1.468916893005371
I0311 23:24:03.676850 140203707442944 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.20583541691303253, loss=1.385278344154358
I0311 23:24:41.026912 140203724228352 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.21274781227111816, loss=1.4552667140960693
I0311 23:25:18.415720 140203707442944 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.22051021456718445, loss=1.4767224788665771
I0311 23:25:55.777143 140203724228352 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.2241269201040268, loss=1.463648796081543
I0311 23:26:33.146984 140203707442944 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.23301281034946442, loss=1.5256035327911377
I0311 23:27:10.520757 140203724228352 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.21521534025669098, loss=1.4999502897262573
I0311 23:27:47.935976 140203707442944 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.2108631134033203, loss=1.4540504217147827
I0311 23:28:25.353838 140203724228352 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.21699081361293793, loss=1.4636318683624268
I0311 23:29:02.745874 140203707442944 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.21442349255084991, loss=1.5005757808685303
I0311 23:29:40.121259 140203724228352 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.21648328006267548, loss=1.4417681694030762
I0311 23:30:17.539658 140203707442944 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.23700308799743652, loss=1.4953620433807373
I0311 23:30:54.938360 140203724228352 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.22274482250213623, loss=1.4200013875961304
I0311 23:31:32.309842 140203707442944 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.2133423238992691, loss=1.4744129180908203
I0311 23:32:09.723493 140203724228352 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.2222667932510376, loss=1.4487282037734985
I0311 23:32:47.086178 140203707442944 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.21857205033302307, loss=1.4831619262695312
I0311 23:33:24.447916 140203724228352 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.22242453694343567, loss=1.418358325958252
I0311 23:34:01.874302 140203707442944 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.2126854807138443, loss=1.501232624053955
I0311 23:34:39.256749 140203724228352 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.21919938921928406, loss=1.4169213771820068
I0311 23:35:16.621648 140203707442944 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.20719978213310242, loss=1.4467575550079346
I0311 23:35:54.021185 140203724228352 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.2209947407245636, loss=1.5151221752166748
I0311 23:36:31.360532 140203707442944 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.21437062323093414, loss=1.4554177522659302
I0311 23:36:58.005739 140391735396160 spec.py:298] Evaluating on the training split.
I0311 23:37:00.987747 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 23:41:07.983869 140391735396160 spec.py:310] Evaluating on the validation split.
I0311 23:41:10.618395 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 23:44:39.651287 140391735396160 spec.py:326] Evaluating on the test split.
I0311 23:44:42.346832 140391735396160 workload.py:179] Translating evaluation dataset.
I0311 23:48:04.203667 140391735396160 submission_runner.py:359] Time since start: 78018.32s, 	Step: 121473, 	{'train/accuracy': 0.692061722278595, 'train/loss': 1.3947802782058716, 'train/bleu': 35.136608380243075, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0311 23:48:04.220492 140203724228352 logging_writer.py:48] [121473] global_step=121473, preemption_count=0, score=45213.776741, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=78018.324944, train/accuracy=0.692062, train/bleu=35.136608, train/loss=1.394780, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0311 23:48:05.219050 140391735396160 checkpoints.py:356] Saving checkpoint at step: 121473
I0311 23:48:08.589390 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_121473
I0311 23:48:08.593058 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_121473.
I0311 23:48:19.057017 140203707442944 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.21937410533428192, loss=1.4617174863815308
I0311 23:48:56.340652 140203699050240 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.22312510013580322, loss=1.4321436882019043
I0311 23:49:33.636234 140203707442944 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.3547458052635193, loss=1.5197854042053223
I0311 23:50:11.010775 140203699050240 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.23394203186035156, loss=1.600250482559204
I0311 23:50:48.365150 140203707442944 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.21187183260917664, loss=1.5300328731536865
I0311 23:51:25.725842 140203699050240 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.23009024560451508, loss=1.5310577154159546
I0311 23:52:03.084644 140203707442944 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.219238743185997, loss=1.4191774129867554
I0311 23:52:40.404143 140203699050240 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.21800467371940613, loss=1.48267662525177
I0311 23:53:17.777775 140203707442944 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.21600885689258575, loss=1.498064398765564
I0311 23:53:55.156271 140203699050240 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.21446086466312408, loss=1.4638091325759888
I0311 23:54:32.485634 140203707442944 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.22419725358486176, loss=1.5265367031097412
I0311 23:55:09.831068 140203699050240 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.4126622974872589, loss=1.4343445301055908
I0311 23:55:47.217910 140203707442944 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.2116277664899826, loss=1.4361541271209717
I0311 23:56:24.556926 140203699050240 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.2184673547744751, loss=1.5000367164611816
I0311 23:57:01.882340 140203707442944 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.21416829526424408, loss=1.4568111896514893
I0311 23:57:39.280908 140203699050240 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.2126198559999466, loss=1.5001580715179443
I0311 23:58:16.676016 140203707442944 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.23650118708610535, loss=1.5368566513061523
I0311 23:58:54.040966 140203699050240 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.22119182348251343, loss=1.4698926210403442
I0311 23:59:31.420582 140203707442944 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.22078680992126465, loss=1.5355935096740723
I0312 00:00:08.789825 140203699050240 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.21584336459636688, loss=1.4374127388000488
I0312 00:00:46.137741 140203707442944 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.23448289930820465, loss=1.4559706449508667
I0312 00:01:23.510149 140203699050240 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.21196334064006805, loss=1.3746473789215088
I0312 00:02:00.897852 140203707442944 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.21147243678569794, loss=1.536155104637146
I0312 00:02:08.842308 140391735396160 spec.py:298] Evaluating on the training split.
I0312 00:02:11.834955 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 00:06:02.523008 140391735396160 spec.py:310] Evaluating on the validation split.
I0312 00:06:05.166956 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 00:09:34.223129 140391735396160 spec.py:326] Evaluating on the test split.
I0312 00:09:36.917608 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 00:12:58.859136 140391735396160 submission_runner.py:359] Time since start: 79529.16s, 	Step: 123723, 	{'train/accuracy': 0.694069504737854, 'train/loss': 1.3849505186080933, 'train/bleu': 35.523450530129615, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0312 00:12:58.878126 140203699050240 logging_writer.py:48] [123723] global_step=123723, preemption_count=0, score=46050.375118, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=79529.161501, train/accuracy=0.694070, train/bleu=35.523451, train/loss=1.384951, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0312 00:12:59.882036 140391735396160 checkpoints.py:356] Saving checkpoint at step: 123723
I0312 00:13:03.246485 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_123723
I0312 00:13:03.250297 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_123723.
I0312 00:13:32.321515 140203707442944 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.21314461529254913, loss=1.453061819076538
I0312 00:14:09.601450 140203690657536 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.22618240118026733, loss=1.4717541933059692
I0312 00:14:46.986426 140203707442944 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.21485815942287445, loss=1.50493586063385
I0312 00:15:24.409978 140203690657536 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.21348679065704346, loss=1.4189460277557373
I0312 00:16:01.776076 140203707442944 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.20758309960365295, loss=1.430946707725525
I0312 00:16:39.145920 140203690657536 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.21593716740608215, loss=1.4609957933425903
I0312 00:17:16.520045 140203707442944 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.2221560776233673, loss=1.4944802522659302
I0312 00:17:53.930003 140203690657536 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.21975089609622955, loss=1.4072299003601074
I0312 00:18:31.279077 140203707442944 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.21104328334331512, loss=1.41584312915802
I0312 00:19:08.668381 140203690657536 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.2104954719543457, loss=1.4507825374603271
I0312 00:19:46.055984 140203707442944 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.21847093105316162, loss=1.451318621635437
I0312 00:20:23.470846 140203690657536 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.2243540734052658, loss=1.5075688362121582
I0312 00:21:00.861673 140203707442944 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.21836046874523163, loss=1.5288649797439575
I0312 00:21:38.216139 140203690657536 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.22407476603984833, loss=1.4618948698043823
I0312 00:22:15.633100 140203707442944 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.2165926992893219, loss=1.4912827014923096
I0312 00:22:53.039278 140203690657536 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.22650282084941864, loss=1.4780476093292236
I0312 00:23:30.456846 140203707442944 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.21309924125671387, loss=1.4626221656799316
I0312 00:24:07.809104 140203690657536 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.22131331264972687, loss=1.4274332523345947
I0312 00:24:45.231622 140203707442944 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.20867902040481567, loss=1.4813697338104248
I0312 00:25:22.589934 140203690657536 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.21297898888587952, loss=1.4981913566589355
I0312 00:25:59.975432 140203707442944 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.22161775827407837, loss=1.4584903717041016
I0312 00:26:37.397457 140203690657536 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.22374196350574493, loss=1.4804515838623047
I0312 00:27:03.265534 140391735396160 spec.py:298] Evaluating on the training split.
I0312 00:27:06.243978 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 00:30:57.757353 140391735396160 spec.py:310] Evaluating on the validation split.
I0312 00:31:00.389044 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 00:34:29.259250 140391735396160 spec.py:326] Evaluating on the test split.
I0312 00:34:31.941071 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 00:37:53.797552 140391735396160 submission_runner.py:359] Time since start: 81023.58s, 	Step: 125971, 	{'train/accuracy': 0.6935049295425415, 'train/loss': 1.387210726737976, 'train/bleu': 35.02386409941134, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0312 00:37:53.814665 140203707442944 logging_writer.py:48] [125971] global_step=125971, preemption_count=0, score=46886.901731, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=81023.584742, train/accuracy=0.693505, train/bleu=35.023864, train/loss=1.387211, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0312 00:37:54.812521 140391735396160 checkpoints.py:356] Saving checkpoint at step: 125971
I0312 00:37:58.122979 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_125971
I0312 00:37:58.126637 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_125971.
I0312 00:38:09.325400 140203690657536 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.20488981902599335, loss=1.4374020099639893
I0312 00:38:46.553397 140203682264832 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.21184030175209045, loss=1.4337581396102905
I0312 00:39:23.850407 140203690657536 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.2097996026277542, loss=1.4177263975143433
I0312 00:40:01.191887 140203682264832 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.2082330584526062, loss=1.3736945390701294
I0312 00:40:38.590741 140203690657536 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.24227558076381683, loss=1.4868452548980713
I0312 00:41:15.971992 140203682264832 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.22322337329387665, loss=1.5917298793792725
I0312 00:41:53.334665 140203690657536 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.21960587799549103, loss=1.4875458478927612
I0312 00:42:30.718396 140203682264832 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.22253841161727905, loss=1.5170749425888062
I0312 00:43:08.121096 140203690657536 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.21587957441806793, loss=1.4617106914520264
I0312 00:43:45.494913 140203682264832 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.21342507004737854, loss=1.488966703414917
I0312 00:44:22.908149 140203690657536 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.937511146068573, loss=1.3762867450714111
I0312 00:45:00.317409 140203682264832 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.21997542679309845, loss=1.4793668985366821
I0312 00:45:37.662702 140203690657536 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.21718677878379822, loss=1.500792145729065
I0312 00:46:15.080135 140203682264832 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.22259779274463654, loss=1.3579925298690796
I0312 00:46:52.459818 140203690657536 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.21461769938468933, loss=1.438988208770752
I0312 00:47:29.841033 140203682264832 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.3089348375797272, loss=1.4435290098190308
I0312 00:48:07.223027 140203690657536 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.2248670905828476, loss=1.4462991952896118
I0312 00:48:44.606440 140203682264832 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.22387652099132538, loss=1.4946842193603516
I0312 00:49:21.987471 140203690657536 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.21882696449756622, loss=1.442735195159912
I0312 00:49:59.360391 140203682264832 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.22143439948558807, loss=1.5256813764572144
I0312 00:50:36.766669 140203690657536 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.22626833617687225, loss=1.4934982061386108
I0312 00:51:14.106283 140203682264832 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.22222089767456055, loss=1.4512451887130737
I0312 00:51:51.492043 140203690657536 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.21301321685314178, loss=1.4861414432525635
I0312 00:51:58.320532 140391735396160 spec.py:298] Evaluating on the training split.
I0312 00:52:01.308138 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 00:56:07.227869 140391735396160 spec.py:310] Evaluating on the validation split.
I0312 00:56:09.864104 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 00:59:38.645091 140391735396160 spec.py:326] Evaluating on the test split.
I0312 00:59:41.340910 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 01:03:03.188999 140391735396160 submission_runner.py:359] Time since start: 82518.64s, 	Step: 128220, 	{'train/accuracy': 0.692229688167572, 'train/loss': 1.392063021659851, 'train/bleu': 35.0425844454587, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0312 01:03:03.205816 140203682264832 logging_writer.py:48] [128220] global_step=128220, preemption_count=0, score=47723.484293, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=82518.639740, train/accuracy=0.692230, train/bleu=35.042584, train/loss=1.392063, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0312 01:03:04.204203 140391735396160 checkpoints.py:356] Saving checkpoint at step: 128220
I0312 01:03:07.532734 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_128220
I0312 01:03:07.536284 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_128220.
I0312 01:03:37.760583 140203690657536 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.21374213695526123, loss=1.4132343530654907
I0312 01:04:15.068520 140203673872128 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.212491974234581, loss=1.417407751083374
I0312 01:04:52.395620 140203690657536 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.22333042323589325, loss=1.4776535034179688
I0312 01:05:29.767553 140203673872128 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.21671341359615326, loss=1.4104441404342651
I0312 01:06:07.129052 140203690657536 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.21748778223991394, loss=1.4989631175994873
I0312 01:06:44.530986 140203673872128 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.22377286851406097, loss=1.4851619005203247
I0312 01:07:21.943850 140203690657536 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.21293659508228302, loss=1.4049572944641113
I0312 01:07:59.290747 140203673872128 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.21148106455802917, loss=1.4302784204483032
I0312 01:08:36.685185 140203690657536 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.22116528451442719, loss=1.4302341938018799
I0312 01:09:14.059561 140203673872128 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.22248612344264984, loss=1.4463471174240112
I0312 01:09:51.438693 140203690657536 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.8928731679916382, loss=1.5224827527999878
I0312 01:10:28.787692 140203673872128 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.21225251257419586, loss=1.4803924560546875
I0312 01:11:06.212203 140203690657536 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.20788992941379547, loss=1.476321816444397
I0312 01:11:43.584695 140203673872128 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.21698656678199768, loss=1.5101011991500854
I0312 01:12:20.942563 140203690657536 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.2211432009935379, loss=1.4631719589233398
I0312 01:12:58.320181 140203673872128 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.21863515675067902, loss=1.4908593893051147
I0312 01:13:35.684128 140203690657536 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.21883872151374817, loss=1.5035438537597656
I0312 01:14:13.104734 140203673872128 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.21279293298721313, loss=1.4219365119934082
I0312 01:14:50.466865 140203690657536 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.2141077071428299, loss=1.4401603937149048
I0312 01:15:27.856948 140203673872128 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.2180939018726349, loss=1.5194278955459595
I0312 01:16:05.165639 140203690657536 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.20566710829734802, loss=1.4389506578445435
I0312 01:16:42.552845 140203673872128 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.21238432824611664, loss=1.4823981523513794
I0312 01:17:07.659203 140391735396160 spec.py:298] Evaluating on the training split.
I0312 01:17:10.653948 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 01:20:55.357108 140391735396160 spec.py:310] Evaluating on the validation split.
I0312 01:20:58.007354 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 01:24:26.815290 140391735396160 spec.py:326] Evaluating on the test split.
I0312 01:24:29.509930 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 01:27:51.471616 140391735396160 submission_runner.py:359] Time since start: 84027.98s, 	Step: 130469, 	{'train/accuracy': 0.6888855695724487, 'train/loss': 1.4139291048049927, 'train/bleu': 35.47462497760243, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0312 01:27:51.489502 140203690657536 logging_writer.py:48] [130469] global_step=130469, preemption_count=0, score=48560.167624, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=84027.978398, train/accuracy=0.688886, train/bleu=35.474625, train/loss=1.413929, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0312 01:27:52.486928 140391735396160 checkpoints.py:356] Saving checkpoint at step: 130469
I0312 01:27:56.305247 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_130469
I0312 01:27:56.308827 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_130469.
I0312 01:28:08.257605 140203673872128 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.21126995980739594, loss=1.4820364713668823
I0312 01:28:45.555164 140203665479424 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.22977472841739655, loss=1.483467698097229
I0312 01:29:22.887990 140203673872128 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.22005854547023773, loss=1.4968916177749634
I0312 01:30:00.236487 140203665479424 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.21956712007522583, loss=1.5006024837493896
I0312 01:30:37.596870 140203673872128 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.21220232546329498, loss=1.4732829332351685
I0312 01:31:14.932712 140203665479424 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.2211494892835617, loss=1.50395667552948
I0312 01:31:52.336040 140203673872128 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.22323770821094513, loss=1.496596336364746
I0312 01:32:29.685481 140203665479424 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.20775900781154633, loss=1.4961251020431519
I0312 01:33:07.061281 140203673872128 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.20736823976039886, loss=1.4167709350585938
I0312 01:33:44.444521 140203665479424 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.2142031341791153, loss=1.4663089513778687
I0312 01:34:21.774684 140203673872128 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.21848900616168976, loss=1.4294472932815552
I0312 01:34:59.164598 140203665479424 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.21591976284980774, loss=1.3934547901153564
I0312 01:35:36.560984 140203673872128 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.21444298326969147, loss=1.4235825538635254
I0312 01:36:13.949672 140203665479424 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.22135403752326965, loss=1.4543241262435913
I0312 01:36:51.328976 140203673872128 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.20861831307411194, loss=1.432502269744873
I0312 01:37:28.675561 140203665479424 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.2108614593744278, loss=1.4642850160598755
I0312 01:38:06.043782 140203673872128 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.21976135671138763, loss=1.5079022645950317
I0312 01:38:43.448340 140203665479424 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.2115800380706787, loss=1.4405508041381836
I0312 01:39:20.867497 140203673872128 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.21124492585659027, loss=1.4935153722763062
I0312 01:39:58.198195 140203665479424 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.2165939062833786, loss=1.4486840963363647
I0312 01:40:35.540285 140203673872128 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.2231505960226059, loss=1.4936554431915283
I0312 01:41:12.922975 140203665479424 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.2228587120771408, loss=1.4759923219680786
I0312 01:41:50.288372 140203673872128 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.21331626176834106, loss=1.3927490711212158
I0312 01:41:56.361660 140391735396160 spec.py:298] Evaluating on the training split.
I0312 01:41:59.341944 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 01:46:02.547009 140391735396160 spec.py:310] Evaluating on the validation split.
I0312 01:46:05.177775 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 01:49:34.146234 140391735396160 spec.py:326] Evaluating on the test split.
I0312 01:49:36.847362 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 01:52:58.765501 140391735396160 submission_runner.py:359] Time since start: 85516.68s, 	Step: 132718, 	{'train/accuracy': 0.6890657544136047, 'train/loss': 1.4094282388687134, 'train/bleu': 35.36007369963588, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0312 01:52:58.783133 140203665479424 logging_writer.py:48] [132718] global_step=132718, preemption_count=0, score=49396.698473, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=85516.680871, train/accuracy=0.689066, train/bleu=35.360074, train/loss=1.409428, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0312 01:52:59.785362 140391735396160 checkpoints.py:356] Saving checkpoint at step: 132718
I0312 01:53:04.289755 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_132718
I0312 01:53:04.293528 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_132718.
I0312 01:53:35.220327 140203673872128 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.23717260360717773, loss=1.403983473777771
I0312 01:54:12.456634 140203657086720 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.2584395706653595, loss=1.4411766529083252
I0312 01:54:49.812036 140203673872128 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.22249573469161987, loss=1.5631574392318726
I0312 01:55:27.158882 140203657086720 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.21661144495010376, loss=1.4680787324905396
I0312 01:56:04.491809 140203673872128 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.21929694712162018, loss=1.4844495058059692
I0312 01:56:41.853708 140203657086720 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.22716160118579865, loss=1.5092533826828003
I0312 01:56:53.510894 140391735396160 spec.py:298] Evaluating on the training split.
I0312 01:56:56.505428 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 02:00:54.517737 140391735396160 spec.py:310] Evaluating on the validation split.
I0312 02:00:57.155118 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 02:04:26.370782 140391735396160 spec.py:326] Evaluating on the test split.
I0312 02:04:29.078408 140391735396160 workload.py:179] Translating evaluation dataset.
I0312 02:07:50.931944 140391735396160 submission_runner.py:359] Time since start: 86413.83s, 	Step: 133333, 	{'train/accuracy': 0.6927968859672546, 'train/loss': 1.3904540538787842, 'train/bleu': 35.430308005106255, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003}
I0312 02:07:50.948705 140203673872128 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=49624.962126, test/accuracy=0.710999, test/bleu=30.905427, test/loss=1.266358, test/num_examples=3003, total_duration=86413.830103, train/accuracy=0.692797, train/bleu=35.430308, train/loss=1.390454, validation/accuracy=0.694176, validation/bleu=30.711707, validation/loss=1.371636, validation/num_examples=3000
I0312 02:07:51.956400 140391735396160 checkpoints.py:356] Saving checkpoint at step: 133333
I0312 02:07:55.774286 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333
I0312 02:07:55.777877 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333.
I0312 02:07:55.796567 140203657086720 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=49624.962126
I0312 02:07:56.371345 140391735396160 checkpoints.py:356] Saving checkpoint at step: 133333
I0312 02:08:02.115206 140391735396160 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333
I0312 02:08:02.119443 140391735396160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333.
I0312 02:08:02.150955 140391735396160 submission_runner.py:520] Tuning trial 1/1
I0312 02:08:02.151101 140391735396160 submission_runner.py:521] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, beta1=0.9326607383586145, beta2=0.9955159689799007, warmup_steps=1999, weight_decay=0.08121616522670176, label_smoothing=0.0)
I0312 02:08:02.155339 140391735396160 submission_runner.py:522] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006287654396146536, 'train/loss': 11.037056922912598, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.039999961853027, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.051443099975586, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 36.44646644592285, 'total_duration': 36.64548587799072, 'global_step': 1, 'preemption_count': 0}), (2247, {'train/accuracy': 0.5198379755020142, 'train/loss': 2.817359209060669, 'train/bleu': 23.378130569270205, 'validation/accuracy': 0.5187288522720337, 'validation/loss': 2.795701503753662, 'validation/bleu': 19.082468131719637, 'validation/num_examples': 3000, 'test/accuracy': 0.5188542604446411, 'test/loss': 2.830124855041504, 'test/bleu': 17.22877798001409, 'test/num_examples': 3003, 'score': 872.1539256572723, 'total_duration': 1793.5336725711823, 'global_step': 2247, 'preemption_count': 0}), (4495, {'train/accuracy': 0.5779771208763123, 'train/loss': 2.2502853870391846, 'train/bleu': 27.50548948137948, 'validation/accuracy': 0.5897012948989868, 'validation/loss': 2.1443467140197754, 'validation/bleu': 23.348739176161885, 'validation/num_examples': 3000, 'test/accuracy': 0.5949915647506714, 'test/loss': 2.11604380607605, 'test/bleu': 22.146500211483012, 'test/num_examples': 3003, 'score': 1707.4840457439423, 'total_duration': 3129.3100266456604, 'global_step': 4495, 'preemption_count': 0}), (6744, {'train/accuracy': 0.6082260012626648, 'train/loss': 1.9924029111862183, 'train/bleu': 30.17388072897711, 'validation/accuracy': 0.6161981821060181, 'validation/loss': 1.9323006868362427, 'validation/bleu': 25.55113566757477, 'validation/num_examples': 3000, 'test/accuracy': 0.6234385371208191, 'test/loss': 1.8746626377105713, 'test/bleu': 24.361249881773627, 'test/num_examples': 3003, 'score': 2544.211181640625, 'total_duration': 4410.7053208351135, 'global_step': 6744, 'preemption_count': 0}), (8994, {'train/accuracy': 0.6123753190040588, 'train/loss': 1.9473698139190674, 'train/bleu': 29.388182231186878, 'validation/accuracy': 0.6295768022537231, 'validation/loss': 1.817801594734192, 'validation/bleu': 26.216295834304685, 'validation/num_examples': 3000, 'test/accuracy': 0.6351867914199829, 'test/loss': 1.7686195373535156, 'test/bleu': 24.84412490772963, 'test/num_examples': 3003, 'score': 3381.0803978443146, 'total_duration': 5704.936761140823, 'global_step': 8994, 'preemption_count': 0}), (11243, {'train/accuracy': 0.6191498637199402, 'train/loss': 1.89480459690094, 'train/bleu': 29.81087653375899, 'validation/accuracy': 0.6388017535209656, 'validation/loss': 1.756168007850647, 'validation/bleu': 26.68195837966921, 'validation/num_examples': 3000, 'test/accuracy': 0.6459125280380249, 'test/loss': 1.6911065578460693, 'test/bleu': 25.65808468992218, 'test/num_examples': 3003, 'score': 4217.6353669166565, 'total_duration': 7026.083872318268, 'global_step': 11243, 'preemption_count': 0}), (13493, {'train/accuracy': 0.6322474479675293, 'train/loss': 1.787447452545166, 'train/bleu': 30.370245323736118, 'validation/accuracy': 0.6454972624778748, 'validation/loss': 1.7069510221481323, 'validation/bleu': 27.26821913156087, 'validation/num_examples': 3000, 'test/accuracy': 0.6533961296081543, 'test/loss': 1.6444133520126343, 'test/bleu': 26.45427409480538, 'test/num_examples': 3003, 'score': 5054.45205283165, 'total_duration': 8291.773514270782, 'global_step': 13493, 'preemption_count': 0}), (15742, {'train/accuracy': 0.6286909580230713, 'train/loss': 1.8207414150238037, 'train/bleu': 30.479847915533863, 'validation/accuracy': 0.6473199129104614, 'validation/loss': 1.6877312660217285, 'validation/bleu': 27.23935768612174, 'validation/num_examples': 3000, 'test/accuracy': 0.6550229787826538, 'test/loss': 1.6249533891677856, 'test/bleu': 26.57388936027434, 'test/num_examples': 3003, 'score': 5891.036470890045, 'total_duration': 9590.064391374588, 'global_step': 15742, 'preemption_count': 0}), (17991, {'train/accuracy': 0.6307916641235352, 'train/loss': 1.8037077188491821, 'train/bleu': 30.902600817098595, 'validation/accuracy': 0.6520687937736511, 'validation/loss': 1.6513620615005493, 'validation/bleu': 27.73049081228079, 'validation/num_examples': 3000, 'test/accuracy': 0.6610888838768005, 'test/loss': 1.5822477340698242, 'test/bleu': 26.918190229727333, 'test/num_examples': 3003, 'score': 6727.561627626419, 'total_duration': 11153.554865837097, 'global_step': 17991, 'preemption_count': 0}), (20240, {'train/accuracy': 0.6386517286300659, 'train/loss': 1.7386045455932617, 'train/bleu': 31.02671844970933, 'validation/accuracy': 0.6544122099876404, 'validation/loss': 1.6381105184555054, 'validation/bleu': 27.91161228130194, 'validation/num_examples': 3000, 'test/accuracy': 0.6635175347328186, 'test/loss': 1.5663893222808838, 'test/bleu': 27.084027213150073, 'test/num_examples': 3003, 'score': 7564.1665828228, 'total_duration': 12465.70645070076, 'global_step': 20240, 'preemption_count': 0}), (22489, {'train/accuracy': 0.6358563899993896, 'train/loss': 1.7583489418029785, 'train/bleu': 31.037824877571992, 'validation/accuracy': 0.653271496295929, 'validation/loss': 1.6486068964004517, 'validation/bleu': 27.881172697717684, 'validation/num_examples': 3000, 'test/accuracy': 0.6654000282287598, 'test/loss': 1.5614794492721558, 'test/bleu': 26.808755779679828, 'test/num_examples': 3003, 'score': 8400.837740182877, 'total_duration': 13826.5390625, 'global_step': 22489, 'preemption_count': 0}), (24738, {'train/accuracy': 0.6413343548774719, 'train/loss': 1.7319859266281128, 'train/bleu': 30.84868003933824, 'validation/accuracy': 0.6577971577644348, 'validation/loss': 1.6136661767959595, 'validation/bleu': 28.0539440896339, 'validation/num_examples': 3000, 'test/accuracy': 0.6678287386894226, 'test/loss': 1.5361779928207397, 'test/bleu': 27.584999863271307, 'test/num_examples': 3003, 'score': 9237.494067430496, 'total_duration': 15226.334443330765, 'global_step': 24738, 'preemption_count': 0}), (26987, {'train/accuracy': 0.6406738758087158, 'train/loss': 1.7162266969680786, 'train/bleu': 31.007006662560997, 'validation/accuracy': 0.6594090461730957, 'validation/loss': 1.5988857746124268, 'validation/bleu': 28.17337801131836, 'validation/num_examples': 3000, 'test/accuracy': 0.6716518402099609, 'test/loss': 1.5258216857910156, 'test/bleu': 27.885028874070905, 'test/num_examples': 3003, 'score': 10074.08002448082, 'total_duration': 16610.339666843414, 'global_step': 26987, 'preemption_count': 0}), (29236, {'train/accuracy': 0.6400923132896423, 'train/loss': 1.7278815507888794, 'train/bleu': 31.178232460447017, 'validation/accuracy': 0.659508228302002, 'validation/loss': 1.5913872718811035, 'validation/bleu': 28.187521796185962, 'validation/num_examples': 3000, 'test/accuracy': 0.6713381409645081, 'test/loss': 1.5129661560058594, 'test/bleu': 27.57593607823483, 'test/num_examples': 3003, 'score': 10910.637885808945, 'total_duration': 18034.414824962616, 'global_step': 29236, 'preemption_count': 0}), (31485, {'train/accuracy': 0.6648632884025574, 'train/loss': 1.5574485063552856, 'train/bleu': 32.579745030489526, 'validation/accuracy': 0.6617896556854248, 'validation/loss': 1.5763901472091675, 'validation/bleu': 28.263989181706112, 'validation/num_examples': 3000, 'test/accuracy': 0.6717564463615417, 'test/loss': 1.505411982536316, 'test/bleu': 27.771302944509074, 'test/num_examples': 3003, 'score': 11747.211518287659, 'total_duration': 19448.747543096542, 'global_step': 31485, 'preemption_count': 0}), (33735, {'train/accuracy': 0.6436406970024109, 'train/loss': 1.7011619806289673, 'train/bleu': 31.781315805573833, 'validation/accuracy': 0.6623600125312805, 'validation/loss': 1.5663892030715942, 'validation/bleu': 27.83605423550204, 'validation/num_examples': 3000, 'test/accuracy': 0.6760560274124146, 'test/loss': 1.486278772354126, 'test/bleu': 27.969794197444912, 'test/num_examples': 3003, 'score': 12584.02584195137, 'total_duration': 20790.56165122986, 'global_step': 33735, 'preemption_count': 0}), (35984, {'train/accuracy': 0.6443754434585571, 'train/loss': 1.695351481437683, 'train/bleu': 31.717029992736872, 'validation/accuracy': 0.6647034883499146, 'validation/loss': 1.5625381469726562, 'validation/bleu': 28.408022996133173, 'validation/num_examples': 3000, 'test/accuracy': 0.6777991056442261, 'test/loss': 1.4802703857421875, 'test/bleu': 27.895970801827712, 'test/num_examples': 3003, 'score': 13420.586697340012, 'total_duration': 22274.4721326828, 'global_step': 35984, 'preemption_count': 0}), (38233, {'train/accuracy': 0.6534589529037476, 'train/loss': 1.6294481754302979, 'train/bleu': 32.608645923433365, 'validation/accuracy': 0.6665137410163879, 'validation/loss': 1.556545376777649, 'validation/bleu': 28.72387899019524, 'validation/num_examples': 3000, 'test/accuracy': 0.6777061223983765, 'test/loss': 1.4755134582519531, 'test/bleu': 27.96578900584244, 'test/num_examples': 3003, 'score': 14257.268061637878, 'total_duration': 23584.780748844147, 'global_step': 38233, 'preemption_count': 0}), (40482, {'train/accuracy': 0.6554723381996155, 'train/loss': 1.6399073600769043, 'train/bleu': 31.856214891381747, 'validation/accuracy': 0.6672700643539429, 'validation/loss': 1.5479999780654907, 'validation/bleu': 28.651235426790976, 'validation/num_examples': 3000, 'test/accuracy': 0.6831561326980591, 'test/loss': 1.4542810916900635, 'test/bleu': 28.669700930269222, 'test/num_examples': 3003, 'score': 15093.890904903412, 'total_duration': 24890.768647909164, 'global_step': 40482, 'preemption_count': 0}), (42732, {'train/accuracy': 0.6502752304077148, 'train/loss': 1.6726518869400024, 'train/bleu': 31.84263952624437, 'validation/accuracy': 0.669563889503479, 'validation/loss': 1.5316383838653564, 'validation/bleu': 28.921991540365624, 'validation/num_examples': 3000, 'test/accuracy': 0.6807622909545898, 'test/loss': 1.448974847793579, 'test/bleu': 28.322028220423057, 'test/num_examples': 3003, 'score': 15930.70485329628, 'total_duration': 26407.251031398773, 'global_step': 42732, 'preemption_count': 0}), (44981, {'train/accuracy': 0.6607688665390015, 'train/loss': 1.588027834892273, 'train/bleu': 32.24235917820795, 'validation/accuracy': 0.6691919565200806, 'validation/loss': 1.5278775691986084, 'validation/bleu': 28.655200953666778, 'validation/num_examples': 3000, 'test/accuracy': 0.6833072304725647, 'test/loss': 1.442453384399414, 'test/bleu': 28.69376492302118, 'test/num_examples': 3003, 'score': 16767.339332342148, 'total_duration': 27760.956735134125, 'global_step': 44981, 'preemption_count': 0}), (47231, {'train/accuracy': 0.6550760865211487, 'train/loss': 1.6239455938339233, 'train/bleu': 32.094072917945915, 'validation/accuracy': 0.6710889935493469, 'validation/loss': 1.513609528541565, 'validation/bleu': 28.093779698567847, 'validation/num_examples': 3000, 'test/accuracy': 0.6857242584228516, 'test/loss': 1.427291989326477, 'test/bleu': 28.682205889901244, 'test/num_examples': 3003, 'score': 17604.170595169067, 'total_duration': 29194.420135736465, 'global_step': 47231, 'preemption_count': 0}), (49481, {'train/accuracy': 0.6547921895980835, 'train/loss': 1.6410382986068726, 'train/bleu': 32.40762972447454, 'validation/accuracy': 0.6735192537307739, 'validation/loss': 1.5032299757003784, 'validation/bleu': 28.96508232715672, 'validation/num_examples': 3000, 'test/accuracy': 0.6866771578788757, 'test/loss': 1.4205994606018066, 'test/bleu': 28.64766000569732, 'test/num_examples': 3003, 'score': 18441.014641046524, 'total_duration': 30744.377076625824, 'global_step': 49481, 'preemption_count': 0}), (51731, {'train/accuracy': 0.6619307398796082, 'train/loss': 1.5822252035140991, 'train/bleu': 32.746873329437925, 'validation/accuracy': 0.6737672090530396, 'validation/loss': 1.49741530418396, 'validation/bleu': 29.232165430645612, 'validation/num_examples': 3000, 'test/accuracy': 0.6865028142929077, 'test/loss': 1.4137426614761353, 'test/bleu': 29.036734519215344, 'test/num_examples': 3003, 'score': 19277.575583696365, 'total_duration': 32281.19644331932, 'global_step': 51731, 'preemption_count': 0}), (53980, {'train/accuracy': 0.65395188331604, 'train/loss': 1.629076600074768, 'train/bleu': 32.72745121177344, 'validation/accuracy': 0.677920937538147, 'validation/loss': 1.488610863685608, 'validation/bleu': 29.32732174403276, 'validation/num_examples': 3000, 'test/accuracy': 0.6899192333221436, 'test/loss': 1.398556113243103, 'test/bleu': 29.03438321915541, 'test/num_examples': 3003, 'score': 20114.039039850235, 'total_duration': 33681.43380570412, 'global_step': 53980, 'preemption_count': 0}), (56230, {'train/accuracy': 0.658459484577179, 'train/loss': 1.611425757408142, 'train/bleu': 32.26509812044639, 'validation/accuracy': 0.6774621605873108, 'validation/loss': 1.4801615476608276, 'validation/bleu': 29.459838166697168, 'validation/num_examples': 3000, 'test/accuracy': 0.6899076104164124, 'test/loss': 1.3914011716842651, 'test/bleu': 29.065301651803, 'test/num_examples': 3003, 'score': 20950.756632089615, 'total_duration': 35202.07003736496, 'global_step': 56230, 'preemption_count': 0}), (58480, {'train/accuracy': 0.6677833795547485, 'train/loss': 1.5405551195144653, 'train/bleu': 32.89231354865563, 'validation/accuracy': 0.6791360378265381, 'validation/loss': 1.4704777002334595, 'validation/bleu': 29.74237073463129, 'validation/num_examples': 3000, 'test/accuracy': 0.6929057240486145, 'test/loss': 1.3788301944732666, 'test/bleu': 29.27869449899086, 'test/num_examples': 3003, 'score': 21787.46463084221, 'total_duration': 36539.46562004089, 'global_step': 58480, 'preemption_count': 0}), (60730, {'train/accuracy': 0.665516197681427, 'train/loss': 1.5675028562545776, 'train/bleu': 32.53189036798213, 'validation/accuracy': 0.6792476177215576, 'validation/loss': 1.4685629606246948, 'validation/bleu': 29.559650210203717, 'validation/num_examples': 3000, 'test/accuracy': 0.6925687193870544, 'test/loss': 1.3763048648834229, 'test/bleu': 29.376539758710788, 'test/num_examples': 3003, 'score': 22624.17467403412, 'total_duration': 37946.88912844658, 'global_step': 60730, 'preemption_count': 0}), (62979, {'train/accuracy': 0.6781721115112305, 'train/loss': 1.4681769609451294, 'train/bleu': 33.658778329045774, 'validation/accuracy': 0.6823350191116333, 'validation/loss': 1.4543193578720093, 'validation/bleu': 29.896432338278213, 'validation/num_examples': 3000, 'test/accuracy': 0.6947882175445557, 'test/loss': 1.3666539192199707, 'test/bleu': 29.77311757983071, 'test/num_examples': 3003, 'score': 23460.72765660286, 'total_duration': 39358.011357307434, 'global_step': 62979, 'preemption_count': 0}), (65229, {'train/accuracy': 0.6683319211006165, 'train/loss': 1.5389748811721802, 'train/bleu': 33.463131091627346, 'validation/accuracy': 0.6814670562744141, 'validation/loss': 1.4475693702697754, 'validation/bleu': 29.59354774730482, 'validation/num_examples': 3000, 'test/accuracy': 0.697681725025177, 'test/loss': 1.3529905080795288, 'test/bleu': 29.59358149692596, 'test/num_examples': 3003, 'score': 24297.289657354355, 'total_duration': 40721.54516386986, 'global_step': 65229, 'preemption_count': 0}), (67479, {'train/accuracy': 0.6673305034637451, 'train/loss': 1.5460699796676636, 'train/bleu': 33.56100032715051, 'validation/accuracy': 0.6833640933036804, 'validation/loss': 1.4409172534942627, 'validation/bleu': 29.727466139711133, 'validation/num_examples': 3000, 'test/accuracy': 0.6977514624595642, 'test/loss': 1.3484975099563599, 'test/bleu': 29.594684613861077, 'test/num_examples': 3003, 'score': 25134.096144914627, 'total_duration': 42262.04575777054, 'global_step': 67479, 'preemption_count': 0}), (69729, {'train/accuracy': 0.6765223145484924, 'train/loss': 1.4774832725524902, 'train/bleu': 34.25141826351276, 'validation/accuracy': 0.6848644018173218, 'validation/loss': 1.4274961948394775, 'validation/bleu': 29.97824908965115, 'validation/num_examples': 3000, 'test/accuracy': 0.698901891708374, 'test/loss': 1.3344674110412598, 'test/bleu': 29.929980963972664, 'test/num_examples': 3003, 'score': 25970.911387443542, 'total_duration': 43680.10204935074, 'global_step': 69729, 'preemption_count': 0}), (71979, {'train/accuracy': 0.6767017245292664, 'train/loss': 1.483557939529419, 'train/bleu': 34.213315669464365, 'validation/accuracy': 0.6854719519615173, 'validation/loss': 1.422886848449707, 'validation/bleu': 29.858244243630022, 'validation/num_examples': 3000, 'test/accuracy': 0.7004357576370239, 'test/loss': 1.322145700454712, 'test/bleu': 30.1580656203387, 'test/num_examples': 3003, 'score': 26807.59378194809, 'total_duration': 45206.88473343849, 'global_step': 71979, 'preemption_count': 0}), (74229, {'train/accuracy': 0.673919677734375, 'train/loss': 1.5076984167099, 'train/bleu': 34.24305028880997, 'validation/accuracy': 0.6865878701210022, 'validation/loss': 1.415334939956665, 'validation/bleu': 30.287005487782032, 'validation/num_examples': 3000, 'test/accuracy': 0.7037708759307861, 'test/loss': 1.3120838403701782, 'test/bleu': 30.473754679760344, 'test/num_examples': 3003, 'score': 27644.347569704056, 'total_duration': 46750.72181773186, 'global_step': 74229, 'preemption_count': 0}), (76479, {'train/accuracy': 0.682380735874176, 'train/loss': 1.4522696733474731, 'train/bleu': 34.47130022557366, 'validation/accuracy': 0.6897372603416443, 'validation/loss': 1.4075356721878052, 'validation/bleu': 30.441052493026962, 'validation/num_examples': 3000, 'test/accuracy': 0.7038406133651733, 'test/loss': 1.306911587715149, 'test/bleu': 30.338031842341756, 'test/num_examples': 3003, 'score': 28480.929537534714, 'total_duration': 48171.70970916748, 'global_step': 76479, 'preemption_count': 0}), (78728, {'train/accuracy': 0.6807045340538025, 'train/loss': 1.456209659576416, 'train/bleu': 34.280651641184235, 'validation/accuracy': 0.6890305280685425, 'validation/loss': 1.4007242918014526, 'validation/bleu': 30.242948208481355, 'validation/num_examples': 3000, 'test/accuracy': 0.7058160901069641, 'test/loss': 1.2985038757324219, 'test/bleu': 30.43142047542504, 'test/num_examples': 3003, 'score': 29317.51162838936, 'total_duration': 49586.41675257683, 'global_step': 78728, 'preemption_count': 0}), (80978, {'train/accuracy': 0.6817448139190674, 'train/loss': 1.4554756879806519, 'train/bleu': 34.48976499985642, 'validation/accuracy': 0.6912003755569458, 'validation/loss': 1.3951085805892944, 'validation/bleu': 30.527759223907882, 'validation/num_examples': 3000, 'test/accuracy': 0.7058160901069641, 'test/loss': 1.2920541763305664, 'test/bleu': 30.49940086644642, 'test/num_examples': 3003, 'score': 30154.03472352028, 'total_duration': 51083.3839366436, 'global_step': 80978, 'preemption_count': 0}), (83228, {'train/accuracy': 0.6859668493270874, 'train/loss': 1.4274961948394775, 'train/bleu': 34.81318755008213, 'validation/accuracy': 0.6921426653862, 'validation/loss': 1.3862097263336182, 'validation/bleu': 30.601693176419346, 'validation/num_examples': 3000, 'test/accuracy': 0.7078961133956909, 'test/loss': 1.2853219509124756, 'test/bleu': 30.627931250660627, 'test/num_examples': 3003, 'score': 30990.511690855026, 'total_duration': 52462.038972616196, 'global_step': 83228, 'preemption_count': 0}), (85478, {'train/accuracy': 0.6822166442871094, 'train/loss': 1.4433878660202026, 'train/bleu': 34.79406336475668, 'validation/accuracy': 0.6916590929031372, 'validation/loss': 1.3844685554504395, 'validation/bleu': 30.701389399814072, 'validation/num_examples': 3000, 'test/accuracy': 0.7090930342674255, 'test/loss': 1.2804052829742432, 'test/bleu': 30.717892200030366, 'test/num_examples': 3003, 'score': 31827.34171438217, 'total_duration': 54021.44664025307, 'global_step': 85478, 'preemption_count': 0}), (87728, {'train/accuracy': 0.6914902329444885, 'train/loss': 1.3971409797668457, 'train/bleu': 35.30333188174412, 'validation/accuracy': 0.6923658847808838, 'validation/loss': 1.379604697227478, 'validation/bleu': 30.496544646813923, 'validation/num_examples': 3000, 'test/accuracy': 0.709557831287384, 'test/loss': 1.2771984338760376, 'test/bleu': 30.618708815507016, 'test/num_examples': 3003, 'score': 32663.76856279373, 'total_duration': 55538.04541373253, 'global_step': 87728, 'preemption_count': 0}), (89978, {'train/accuracy': 0.6903082132339478, 'train/loss': 1.4035587310791016, 'train/bleu': 35.2467082513542, 'validation/accuracy': 0.6933701634407043, 'validation/loss': 1.3752750158309937, 'validation/bleu': 30.606563118779707, 'validation/num_examples': 3000, 'test/accuracy': 0.7105804681777954, 'test/loss': 1.2725517749786377, 'test/bleu': 30.839185565110792, 'test/num_examples': 3003, 'score': 33500.2777633667, 'total_duration': 57027.25988960266, 'global_step': 89978, 'preemption_count': 0}), (92227, {'train/accuracy': 0.6903812885284424, 'train/loss': 1.3982020616531372, 'train/bleu': 35.49914716995388, 'validation/accuracy': 0.6935809850692749, 'validation/loss': 1.374484658241272, 'validation/bleu': 30.704950089245195, 'validation/num_examples': 3000, 'test/accuracy': 0.7103829383850098, 'test/loss': 1.2687493562698364, 'test/bleu': 30.874784071524292, 'test/num_examples': 3003, 'score': 34336.83065414429, 'total_duration': 58566.62728857994, 'global_step': 92227, 'preemption_count': 0}), (94476, {'train/accuracy': 0.6918635368347168, 'train/loss': 1.391923189163208, 'train/bleu': 35.201651907023745, 'validation/accuracy': 0.6936429738998413, 'validation/loss': 1.3719408512115479, 'validation/bleu': 30.82023337933146, 'validation/num_examples': 3000, 'test/accuracy': 0.710719883441925, 'test/loss': 1.2680233716964722, 'test/bleu': 30.792794776949965, 'test/num_examples': 3003, 'score': 35173.362765073776, 'total_duration': 60030.37192559242, 'global_step': 94476, 'preemption_count': 0}), (96726, {'train/accuracy': 0.689703106880188, 'train/loss': 1.4098316431045532, 'train/bleu': 35.83295074465345, 'validation/accuracy': 0.6939777731895447, 'validation/loss': 1.3714425563812256, 'validation/bleu': 30.71248806223062, 'validation/num_examples': 3000, 'test/accuracy': 0.7108709812164307, 'test/loss': 1.2669519186019897, 'test/bleu': 30.868209545604145, 'test/num_examples': 3003, 'score': 36010.07980012894, 'total_duration': 61501.5863096714, 'global_step': 96726, 'preemption_count': 0}), (98976, {'train/accuracy': 0.6885139346122742, 'train/loss': 1.4143635034561157, 'train/bleu': 35.69056247505244, 'validation/accuracy': 0.6941885352134705, 'validation/loss': 1.371662974357605, 'validation/bleu': 30.691709682764603, 'validation/num_examples': 3000, 'test/accuracy': 0.7110801339149475, 'test/loss': 1.2663038969039917, 'test/bleu': 30.944697817919742, 'test/num_examples': 3003, 'score': 36846.66812109947, 'total_duration': 63018.245972156525, 'global_step': 98976, 'preemption_count': 0}), (101225, {'train/accuracy': 0.6931129097938538, 'train/loss': 1.3867099285125732, 'train/bleu': 35.717257293316706, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 37683.34481930733, 'total_duration': 64521.93421792984, 'global_step': 101225, 'preemption_count': 0}), (103475, {'train/accuracy': 0.6902816295623779, 'train/loss': 1.4033467769622803, 'train/bleu': 35.037440737438686, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 38520.10323214531, 'total_duration': 66018.20268130302, 'global_step': 103475, 'preemption_count': 0}), (105725, {'train/accuracy': 0.6951080560684204, 'train/loss': 1.3812083005905151, 'train/bleu': 35.3259972961879, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 39356.511410713196, 'total_duration': 67518.3743417263, 'global_step': 105725, 'preemption_count': 0}), (107975, {'train/accuracy': 0.6911378502845764, 'train/loss': 1.4004404544830322, 'train/bleu': 35.76648938807981, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 40193.35043144226, 'total_duration': 69028.82545423508, 'global_step': 107975, 'preemption_count': 0}), (110224, {'train/accuracy': 0.6900483965873718, 'train/loss': 1.4049930572509766, 'train/bleu': 35.01723833282688, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 41030.0196788311, 'total_duration': 70526.40790438652, 'global_step': 110224, 'preemption_count': 0}), (112474, {'train/accuracy': 0.6925830245018005, 'train/loss': 1.3915064334869385, 'train/bleu': 35.27959612043446, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 41866.76421999931, 'total_duration': 72026.21263217926, 'global_step': 112474, 'preemption_count': 0}), (114724, {'train/accuracy': 0.6869683861732483, 'train/loss': 1.4257354736328125, 'train/bleu': 35.07010530580806, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 42703.49493288994, 'total_duration': 73516.22033143044, 'global_step': 114724, 'preemption_count': 0}), (116974, {'train/accuracy': 0.6905570030212402, 'train/loss': 1.4027093648910522, 'train/bleu': 35.4081560106899, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 43540.29337525368, 'total_duration': 75010.04314613342, 'global_step': 116974, 'preemption_count': 0}), (119224, {'train/accuracy': 0.692175030708313, 'train/loss': 1.395777702331543, 'train/bleu': 35.260293399713966, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 44376.95547938347, 'total_duration': 76511.3065392971, 'global_step': 119224, 'preemption_count': 0}), (121473, {'train/accuracy': 0.692061722278595, 'train/loss': 1.3947802782058716, 'train/bleu': 35.136608380243075, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 45213.776740550995, 'total_duration': 78018.3249437809, 'global_step': 121473, 'preemption_count': 0}), (123723, {'train/accuracy': 0.694069504737854, 'train/loss': 1.3849505186080933, 'train/bleu': 35.523450530129615, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 46050.375118494034, 'total_duration': 79529.16150093079, 'global_step': 123723, 'preemption_count': 0}), (125971, {'train/accuracy': 0.6935049295425415, 'train/loss': 1.387210726737976, 'train/bleu': 35.02386409941134, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 46886.90173077583, 'total_duration': 81023.58474206924, 'global_step': 125971, 'preemption_count': 0}), (128220, {'train/accuracy': 0.692229688167572, 'train/loss': 1.392063021659851, 'train/bleu': 35.0425844454587, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 47723.48429274559, 'total_duration': 82518.6397395134, 'global_step': 128220, 'preemption_count': 0}), (130469, {'train/accuracy': 0.6888855695724487, 'train/loss': 1.4139291048049927, 'train/bleu': 35.47462497760243, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 48560.16762447357, 'total_duration': 84027.97839808464, 'global_step': 130469, 'preemption_count': 0}), (132718, {'train/accuracy': 0.6890657544136047, 'train/loss': 1.4094282388687134, 'train/bleu': 35.36007369963588, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 49396.698472976685, 'total_duration': 85516.68087124825, 'global_step': 132718, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6927968859672546, 'train/loss': 1.3904540538787842, 'train/bleu': 35.430308005106255, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.3716360330581665, 'validation/bleu': 30.7117066960924, 'validation/num_examples': 3000, 'test/accuracy': 0.7109987735748291, 'test/loss': 1.2663582563400269, 'test/bleu': 30.905426623067743, 'test/num_examples': 3003, 'score': 49624.962126493454, 'total_duration': 86413.83010292053, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0312 02:08:02.155476 140391735396160 submission_runner.py:523] Timing: 49624.962126493454
I0312 02:08:02.155520 140391735396160 submission_runner.py:524] ====================
I0312 02:08:02.155730 140391735396160 submission_runner.py:583] Final wmt score: 49624.962126493454
