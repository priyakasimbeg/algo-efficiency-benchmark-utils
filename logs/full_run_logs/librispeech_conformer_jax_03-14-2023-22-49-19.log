I0314 22:49:32.840614 139935937595200 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing/librispeech_conformer_jax.
I0314 22:49:32.890433 139935937595200 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0314 22:49:33.746800 139935937595200 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0314 22:49:33.747389 139935937595200 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0314 22:49:33.750131 139935937595200 submission_runner.py:484] Using RNG seed 789490227
I0314 22:49:34.905751 139935937595200 submission_runner.py:493] --- Tuning run 1/1 ---
I0314 22:49:34.905968 139935937595200 submission_runner.py:498] Creating tuning directory at /experiment_runs/timing/librispeech_conformer_jax/trial_1.
I0314 22:49:34.906151 139935937595200 logger_utils.py:84] Saving hparams to /experiment_runs/timing/librispeech_conformer_jax/trial_1/hparams.json.
I0314 22:49:35.025901 139935937595200 submission_runner.py:230] Initializing dataset.
I0314 22:49:35.026109 139935937595200 submission_runner.py:237] Initializing model.
I0314 22:49:41.274035 139935937595200 submission_runner.py:247] Initializing optimizer.
I0314 22:49:42.058835 139935937595200 submission_runner.py:254] Initializing metrics bundle.
I0314 22:49:42.059049 139935937595200 submission_runner.py:268] Initializing checkpoint and logger.
I0314 22:49:42.059998 139935937595200 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0314 22:49:42.060272 139935937595200 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0314 22:49:42.060345 139935937595200 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0314 22:49:43.050720 139935937595200 submission_runner.py:289] Saving meta data to /experiment_runs/timing/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0314 22:49:43.051675 139935937595200 submission_runner.py:292] Saving flags to /experiment_runs/timing/librispeech_conformer_jax/trial_1/flags_0.json.
I0314 22:49:43.054717 139935937595200 submission_runner.py:302] Starting training loop.
I0314 22:49:43.245709 139935937595200 input_pipeline.py:20] Loading split = train-clean-100
I0314 22:49:43.274461 139935937595200 input_pipeline.py:20] Loading split = train-clean-360
I0314 22:49:43.634832 139935937595200 input_pipeline.py:20] Loading split = train-other-500
2023-03-14 22:50:43.083831: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-03-14 22:50:43.292514: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0314 22:50:45.164116 139760014194432 logging_writer.py:48] [0] global_step=0, grad_norm=42.840248107910156, loss=32.591670989990234
I0314 22:50:45.190074 139935937595200 spec.py:298] Evaluating on the training split.
I0314 22:50:45.294895 139935937595200 input_pipeline.py:20] Loading split = train-clean-100
I0314 22:50:45.543928 139935937595200 input_pipeline.py:20] Loading split = train-clean-360
I0314 22:50:45.640007 139935937595200 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0314 22:51:39.986730 139935937595200 spec.py:310] Evaluating on the validation split.
I0314 22:51:40.046216 139935937595200 input_pipeline.py:20] Loading split = dev-clean
I0314 22:51:40.051438 139935937595200 input_pipeline.py:20] Loading split = dev-other
I0314 22:52:21.602788 139935937595200 spec.py:326] Evaluating on the test split.
I0314 22:52:21.661518 139935937595200 input_pipeline.py:20] Loading split = test-clean
I0314 22:52:53.635236 139935937595200 submission_runner.py:362] Time since start: 62.14s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.783794, dtype=float32), 'train/wer': 1.3892770130978906, 'validation/ctc_loss': DeviceArray(31.04576, dtype=float32), 'validation/wer': 1.3554689384364538, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.139847, dtype=float32), 'test/wer': 1.3896573436516158, 'test/num_examples': 2472}
I0314 22:52:53.657872 139756591642368 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=61.944379, test/ctc_loss=31.139846801757812, test/num_examples=2472, test/wer=1.389657, total_duration=62.135304, train/ctc_loss=31.783794403076172, train/wer=1.389277, validation/ctc_loss=31.045759201049805, validation/num_examples=5348, validation/wer=1.355469
I0314 22:52:53.989180 139935937595200 checkpoints.py:356] Saving checkpoint at step: 1
I0314 22:52:55.350022 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_1
I0314 22:52:55.370266 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_1.
I0314 22:54:30.292507 139761935263488 logging_writer.py:48] [100] global_step=100, grad_norm=15.942691802978516, loss=7.163873195648193
I0314 22:55:46.717938 139761943656192 logging_writer.py:48] [200] global_step=200, grad_norm=1.358547568321228, loss=6.232685089111328
I0314 22:57:02.971289 139761935263488 logging_writer.py:48] [300] global_step=300, grad_norm=0.38398948311805725, loss=5.946052551269531
I0314 22:58:19.415744 139761943656192 logging_writer.py:48] [400] global_step=400, grad_norm=0.545195996761322, loss=5.850461959838867
I0314 22:59:35.976181 139761935263488 logging_writer.py:48] [500] global_step=500, grad_norm=0.5238192081451416, loss=5.826157569885254
I0314 23:00:52.051457 139761943656192 logging_writer.py:48] [600] global_step=600, grad_norm=0.2749680280685425, loss=5.8044233322143555
I0314 23:02:08.252397 139761935263488 logging_writer.py:48] [700] global_step=700, grad_norm=0.2683272659778595, loss=5.823644161224365
I0314 23:03:24.549266 139761943656192 logging_writer.py:48] [800] global_step=800, grad_norm=1.0308902263641357, loss=5.830152988433838
I0314 23:04:41.098599 139761935263488 logging_writer.py:48] [900] global_step=900, grad_norm=0.4420686662197113, loss=5.7878737449646
I0314 23:05:57.592478 139761943656192 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.34936660528182983, loss=5.802099227905273
I0314 23:07:17.265416 139763405444864 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6307553052902222, loss=5.7981977462768555
I0314 23:08:33.659033 139763397052160 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.4808681011199951, loss=5.772940635681152
I0314 23:09:50.219915 139763405444864 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7107312083244324, loss=5.783646106719971
I0314 23:11:06.967408 139763397052160 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7010269165039062, loss=5.788985252380371
I0314 23:12:23.565219 139763405444864 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.0026862621307373, loss=5.755716323852539
I0314 23:13:40.150767 139763397052160 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.6376893520355225, loss=5.618978500366211
I0314 23:14:56.496985 139763405444864 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.883705735206604, loss=5.536043643951416
I0314 23:16:12.861141 139763397052160 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5269246101379395, loss=5.512808322906494
I0314 23:17:28.955630 139763405444864 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8951805233955383, loss=5.379992485046387
I0314 23:18:45.107583 139763397052160 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5710023045539856, loss=5.151007652282715
I0314 23:20:04.464249 139763405444864 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7388352155685425, loss=4.667654991149902
I0314 23:21:20.687999 139763397052160 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8005160689353943, loss=4.272579193115234
I0314 23:22:36.841160 139763405444864 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1473276615142822, loss=3.9461700916290283
I0314 23:23:52.823138 139763397052160 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8675824999809265, loss=3.6960160732269287
I0314 23:25:09.047065 139763405444864 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9367885589599609, loss=3.5707967281341553
I0314 23:26:25.182741 139763397052160 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.990291953086853, loss=3.377248525619507
I0314 23:27:41.111354 139763405444864 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.1582123041152954, loss=3.2925498485565186
I0314 23:28:57.022677 139763397052160 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.0437103509902954, loss=3.211026906967163
I0314 23:30:12.883317 139763405444864 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8844096064567566, loss=3.0870070457458496
I0314 23:31:35.284570 139763397052160 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9209257960319519, loss=2.99157452583313
I0314 23:32:55.728198 139935937595200 spec.py:298] Evaluating on the training split.
I0314 23:33:30.132990 139935937595200 spec.py:310] Evaluating on the validation split.
I0314 23:34:09.480954 139935937595200 spec.py:326] Evaluating on the test split.
I0314 23:34:30.614205 139935937595200 submission_runner.py:362] Time since start: 2592.67s, 	Step: 3097, 	{'train/ctc_loss': DeviceArray(3.382217, dtype=float32), 'train/wer': 0.7113756668245762, 'validation/ctc_loss': DeviceArray(3.6820817, dtype=float32), 'validation/wer': 0.7188877847350191, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3.4262977, dtype=float32), 'test/wer': 0.6972762171714094, 'test/num_examples': 2472}
I0314 23:34:30.635537 139762821764864 logging_writer.py:48] [3097] global_step=3097, preemption_count=0, score=2454.894264, test/ctc_loss=3.426297664642334, test/num_examples=2472, test/wer=0.697276, total_duration=2592.673230, train/ctc_loss=3.3822169303894043, train/wer=0.711376, validation/ctc_loss=3.682081699371338, validation/num_examples=5348, validation/wer=0.718888
I0314 23:34:31.052025 139935937595200 checkpoints.py:356] Saving checkpoint at step: 3097
I0314 23:34:32.595633 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_3097
I0314 23:34:32.625824 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_3097.
I0314 23:34:35.734399 139762813372160 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9104982018470764, loss=2.9054653644561768
I0314 23:35:51.602668 139762771408640 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.1783883571624756, loss=2.8311421871185303
I0314 23:37:07.346568 139762813372160 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.2084509134292603, loss=2.8374571800231934
I0314 23:38:23.060775 139762771408640 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9660578370094299, loss=2.8128561973571777
I0314 23:39:38.895990 139762813372160 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.364302396774292, loss=2.757699966430664
I0314 23:40:54.640233 139762771408640 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.0174089670181274, loss=2.643156051635742
I0314 23:42:10.433250 139762813372160 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9987132549285889, loss=2.625915050506592
I0314 23:43:26.488694 139762771408640 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9720398783683777, loss=2.5684337615966797
I0314 23:44:42.301017 139762813372160 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9160040020942688, loss=2.524402141571045
I0314 23:45:57.960677 139762771408640 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8103843331336975, loss=2.5366475582122803
I0314 23:47:19.394753 139762813372160 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8535169959068298, loss=2.5292906761169434
I0314 23:48:39.180453 139762166404864 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7204668521881104, loss=2.4285974502563477
I0314 23:49:54.953548 139762158012160 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9702072739601135, loss=2.3397176265716553
I0314 23:51:10.619111 139762166404864 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8014881014823914, loss=2.312129259109497
I0314 23:52:26.188145 139762158012160 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.3370286226272583, loss=2.328693389892578
I0314 23:53:41.871456 139762166404864 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.939610481262207, loss=2.353517532348633
I0314 23:54:57.563505 139762158012160 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.0996203422546387, loss=2.2296817302703857
I0314 23:56:13.221413 139762166404864 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.056418776512146, loss=2.2138278484344482
I0314 23:57:28.910555 139762158012160 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7512580752372742, loss=2.259679079055786
I0314 23:58:46.029848 139762166404864 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8679274320602417, loss=2.1864969730377197
I0315 00:00:02.212834 139762158012160 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.0397990942001343, loss=2.1513805389404297
I0315 00:01:22.525298 139762166404864 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6765677332878113, loss=2.0615313053131104
I0315 00:02:38.179857 139762158012160 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8549959063529968, loss=2.1156883239746094
I0315 00:03:53.880754 139762166404864 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8010949492454529, loss=2.0059962272644043
I0315 00:05:09.450568 139762158012160 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8388801217079163, loss=2.087979555130005
I0315 00:06:25.307680 139762166404864 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.0295639038085938, loss=2.0210282802581787
I0315 00:07:41.297847 139762158012160 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7962687611579895, loss=2.0235908031463623
I0315 00:08:58.775689 139762166404864 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8573771119117737, loss=2.0252273082733154
I0315 00:10:19.582379 139762158012160 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.697724461555481, loss=1.9671748876571655
I0315 00:11:42.596968 139762166404864 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.2892967462539673, loss=1.911764144897461
I0315 00:13:06.514582 139762158012160 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7031943798065186, loss=1.914294719696045
I0315 00:14:30.747952 139762821764864 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8905721306800842, loss=1.925112009048462
I0315 00:14:32.726382 139935937595200 spec.py:298] Evaluating on the training split.
I0315 00:15:09.203293 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 00:15:47.028658 139935937595200 spec.py:326] Evaluating on the test split.
I0315 00:16:07.027994 139935937595200 submission_runner.py:362] Time since start: 5089.67s, 	Step: 6204, 	{'train/ctc_loss': DeviceArray(0.62502086, dtype=float32), 'train/wer': 0.21431958021468717, 'validation/ctc_loss': DeviceArray(0.94187, dtype=float32), 'validation/wer': 0.27905720267441075, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.66660255, dtype=float32), 'test/wer': 0.21717140942051064, 'test/num_examples': 2472}
I0315 00:16:07.047466 139762821764864 logging_writer.py:48] [6204] global_step=6204, preemption_count=0, score=4847.354572, test/ctc_loss=0.6666025519371033, test/num_examples=2472, test/wer=0.217171, total_duration=5089.671578, train/ctc_loss=0.6250208616256714, train/wer=0.214320, validation/ctc_loss=0.9418699741363525, validation/num_examples=5348, validation/wer=0.279057
I0315 00:16:07.395841 139935937595200 checkpoints.py:356] Saving checkpoint at step: 6204
I0315 00:16:08.835595 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_6204
I0315 00:16:08.868637 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_6204.
I0315 00:17:22.059931 139762813372160 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9765991568565369, loss=1.8516041040420532
I0315 00:18:37.704015 139762005255936 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7524048089981079, loss=1.8925368785858154
I0315 00:19:53.297374 139762813372160 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8213993310928345, loss=1.8985621929168701
I0315 00:21:08.931339 139762005255936 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7255322933197021, loss=1.9104255437850952
I0315 00:22:24.472104 139762813372160 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.870005190372467, loss=1.8874619007110596
I0315 00:23:39.990929 139762005255936 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.8586965203285217, loss=1.83272123336792
I0315 00:24:55.507830 139762813372160 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8321372866630554, loss=1.8298965692520142
I0315 00:26:13.832962 139762005255936 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8254408240318298, loss=1.8148835897445679
I0315 00:27:35.560014 139762813372160 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7242418527603149, loss=1.8612568378448486
I0315 00:28:58.122980 139762005255936 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7322872877120972, loss=1.8186722993850708
I0315 00:30:17.896980 139762494084864 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7505902051925659, loss=1.7727426290512085
I0315 00:31:33.537029 139762485692160 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.6544788479804993, loss=1.7117303609848022
I0315 00:32:49.359004 139762494084864 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7117556929588318, loss=1.7742103338241577
I0315 00:34:04.951648 139762485692160 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6652085781097412, loss=1.7424304485321045
I0315 00:35:20.542739 139762494084864 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7468216419219971, loss=1.7197445631027222
I0315 00:36:36.401759 139762485692160 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7998854517936707, loss=1.7778232097625732
I0315 00:37:53.040060 139762494084864 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6921884417533875, loss=1.80215585231781
I0315 00:39:15.022495 139762485692160 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7044410109519958, loss=1.751450777053833
I0315 00:40:36.397197 139762494084864 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.753491997718811, loss=1.7368175983428955
I0315 00:41:59.731245 139762485692160 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6400238871574402, loss=1.769676923751831
I0315 00:43:21.366370 139762494084864 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6847293972969055, loss=1.6960729360580444
I0315 00:44:37.060044 139762485692160 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7027756571769714, loss=1.6665955781936646
I0315 00:45:52.755950 139762494084864 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6525522470474243, loss=1.7334630489349365
I0315 00:47:08.186828 139762485692160 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8082549571990967, loss=1.6997439861297607
I0315 00:48:23.691700 139762494084864 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6602716445922852, loss=1.7635287046432495
I0315 00:49:39.083448 139762485692160 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6731851696968079, loss=1.7485641241073608
I0315 00:50:54.581485 139762494084864 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6821764707565308, loss=1.7556235790252686
I0315 00:52:14.236320 139762485692160 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8133257031440735, loss=1.7267974615097046
I0315 00:53:35.207079 139762494084864 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6032310128211975, loss=1.6701174974441528
I0315 00:54:56.565317 139762485692160 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6807417869567871, loss=1.7837764024734497
I0315 00:56:09.433660 139935937595200 spec.py:298] Evaluating on the training split.
I0315 00:56:46.497373 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 00:57:24.420821 139935937595200 spec.py:326] Evaluating on the test split.
I0315 00:57:44.029755 139935937595200 submission_runner.py:362] Time since start: 7586.38s, 	Step: 9290, 	{'train/ctc_loss': DeviceArray(0.42288408, dtype=float32), 'train/wer': 0.15223155220308784, 'validation/ctc_loss': DeviceArray(0.75306743, dtype=float32), 'validation/wer': 0.2246041929975205, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.49526188, dtype=float32), 'test/wer': 0.16385351288769728, 'test/num_examples': 2472}
I0315 00:57:44.049527 139762494084864 logging_writer.py:48] [9290] global_step=9290, preemption_count=0, score=7240.238755, test/ctc_loss=0.49526187777519226, test/num_examples=2472, test/wer=0.163854, total_duration=7586.378685, train/ctc_loss=0.422884076833725, train/wer=0.152232, validation/ctc_loss=0.7530674338340759, validation/num_examples=5348, validation/wer=0.224604
I0315 00:57:44.413760 139935937595200 checkpoints.py:356] Saving checkpoint at step: 9290
I0315 00:57:45.827768 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_9290
I0315 00:57:45.860765 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_9290.
I0315 00:57:54.237363 139762485692160 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6045204401016235, loss=1.6272081136703491
I0315 00:59:09.774360 139762426943232 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5888932943344116, loss=1.6707007884979248
I0315 01:00:25.377824 139762485692160 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7890274524688721, loss=1.7232953310012817
I0315 01:01:40.986278 139762426943232 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6239449977874756, loss=1.7163786888122559
I0315 01:02:56.723005 139762485692160 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6533975005149841, loss=1.6962926387786865
I0315 01:04:12.476644 139762426943232 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7512754201889038, loss=1.6604129076004028
I0315 01:05:28.059345 139762485692160 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6733154654502869, loss=1.745287299156189
I0315 01:06:46.887741 139762426943232 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5720870494842529, loss=1.680891990661621
I0315 01:08:08.469471 139762485692160 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.701782763004303, loss=1.6691553592681885
I0315 01:09:31.466425 139762426943232 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7292289733886719, loss=1.7061691284179688
I0315 01:10:57.881975 139762166404864 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7133323550224304, loss=1.6341594457626343
I0315 01:12:13.292019 139762158012160 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6030803918838501, loss=1.6233543157577515
I0315 01:13:28.702878 139762166404864 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7165049314498901, loss=1.694968581199646
I0315 01:14:44.474927 139762158012160 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6555282473564148, loss=1.651984453201294
I0315 01:16:00.068377 139762166404864 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5943037867546082, loss=1.6530989408493042
I0315 01:17:15.809198 139762158012160 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6662436723709106, loss=1.5957939624786377
I0315 01:18:31.461895 139762166404864 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6933108568191528, loss=1.6973152160644531
I0315 01:19:46.925277 139762158012160 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6043217778205872, loss=1.6597297191619873
I0315 01:21:08.331153 139762166404864 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5716253519058228, loss=1.5935916900634766
I0315 01:22:30.325929 139762158012160 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6159888505935669, loss=1.6159093379974365
I0315 01:23:52.915928 139762166404864 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6190941333770752, loss=1.5736862421035767
I0315 01:25:13.500945 139762166404864 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6316733360290527, loss=1.61434805393219
I0315 01:26:29.031156 139762158012160 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6966857314109802, loss=1.6128835678100586
I0315 01:27:44.306255 139762166404864 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6707860231399536, loss=1.606607437133789
I0315 01:28:59.746365 139762158012160 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5924996733665466, loss=1.631835699081421
I0315 01:30:15.129417 139762166404864 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6752878427505493, loss=1.638007640838623
I0315 01:31:30.676816 139762158012160 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.6735174059867859, loss=1.6604795455932617
I0315 01:32:51.836436 139762166404864 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5525871515274048, loss=1.595643162727356
I0315 01:34:15.173576 139762158012160 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.8162851929664612, loss=1.5432376861572266
I0315 01:35:38.783735 139762166404864 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.593373715877533, loss=1.644165277481079
I0315 01:37:02.342609 139762158012160 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6100121736526489, loss=1.6147263050079346
I0315 01:37:46.131535 139935937595200 spec.py:298] Evaluating on the training split.
I0315 01:38:22.962616 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 01:39:00.773150 139935937595200 spec.py:326] Evaluating on the test split.
I0315 01:39:20.476901 139935937595200 submission_runner.py:362] Time since start: 10083.08s, 	Step: 12354, 	{'train/ctc_loss': DeviceArray(0.37204802, dtype=float32), 'train/wer': 0.13383216915037335, 'validation/ctc_loss': DeviceArray(0.66134274, dtype=float32), 'validation/wer': 0.2025296915551525, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4276823, dtype=float32), 'test/wer': 0.14508561330814698, 'test/num_examples': 2472}
I0315 01:39:20.497353 139763405444864 logging_writer.py:48] [12354] global_step=12354, preemption_count=0, score=9632.916443, test/ctc_loss=0.4276823103427887, test/num_examples=2472, test/wer=0.145086, total_duration=10083.076741, train/ctc_loss=0.372048020362854, train/wer=0.133832, validation/ctc_loss=0.6613427400588989, validation/num_examples=5348, validation/wer=0.202530
I0315 01:39:20.843641 139935937595200 checkpoints.py:356] Saving checkpoint at step: 12354
I0315 01:39:22.257874 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_12354
I0315 01:39:22.290539 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_12354.
I0315 01:40:01.329916 139762535044864 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5943244695663452, loss=1.6114357709884644
I0315 01:41:16.797318 139762526652160 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6459628343582153, loss=1.598090648651123
I0315 01:42:32.238928 139762535044864 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.614228367805481, loss=1.5560745000839233
I0315 01:43:47.767565 139762526652160 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.753624677658081, loss=1.5779836177825928
I0315 01:45:03.100492 139762535044864 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7111968398094177, loss=1.6930615901947021
I0315 01:46:18.565676 139762526652160 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6004819869995117, loss=1.65421462059021
I0315 01:47:41.779405 139762535044864 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6434298753738403, loss=1.5883190631866455
I0315 01:49:03.464198 139762526652160 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.7348415851593018, loss=1.6120305061340332
I0315 01:50:27.079506 139762535044864 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6824618577957153, loss=1.5944424867630005
I0315 01:51:49.672589 139762526652160 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6427598595619202, loss=1.6078768968582153
I0315 01:53:14.666882 139763405444864 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.7352842092514038, loss=1.596964716911316
I0315 01:54:30.118002 139763397052160 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7639316320419312, loss=1.5300681591033936
I0315 01:55:45.518177 139763405444864 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.698885440826416, loss=1.5595176219940186
I0315 01:57:00.819900 139763397052160 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6060376167297363, loss=1.5884313583374023
I0315 01:58:16.224358 139763405444864 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6227613687515259, loss=1.5990909337997437
I0315 01:59:31.570127 139763397052160 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.5471354126930237, loss=1.5642708539962769
I0315 02:00:47.370804 139763405444864 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6419187188148499, loss=1.5436033010482788
I0315 02:02:06.711751 139763397052160 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5594989061355591, loss=1.5380696058273315
I0315 02:03:29.661363 139763405444864 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.8067187070846558, loss=1.5632184743881226
I0315 02:04:54.595830 139763397052160 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.5943117141723633, loss=1.4605443477630615
I0315 02:06:17.759117 139763405444864 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6552721261978149, loss=1.5797064304351807
I0315 02:07:38.100539 139763405444864 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.5894932746887207, loss=1.5135225057601929
I0315 02:08:53.633545 139763397052160 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.5608651638031006, loss=1.538395881652832
I0315 02:10:09.027238 139763405444864 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7153259515762329, loss=1.6375455856323242
I0315 02:11:24.317298 139763397052160 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6413187384605408, loss=1.5517127513885498
I0315 02:12:39.571518 139763405444864 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6432087421417236, loss=1.564713716506958
I0315 02:13:54.964659 139763397052160 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6178908348083496, loss=1.5716999769210815
I0315 02:15:15.174838 139763405444864 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6137403249740601, loss=1.5668381452560425
I0315 02:16:37.441671 139763397052160 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.7113247513771057, loss=1.529410719871521
I0315 02:17:56.664448 139763405444864 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.625737726688385, loss=1.4588130712509155
I0315 02:19:17.893147 139763397052160 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6207905411720276, loss=1.5098828077316284
I0315 02:19:22.981375 139935937595200 spec.py:298] Evaluating on the training split.
I0315 02:19:59.633197 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 02:20:38.093618 139935937595200 spec.py:326] Evaluating on the test split.
I0315 02:20:58.201694 139935937595200 submission_runner.py:362] Time since start: 12579.93s, 	Step: 15408, 	{'train/ctc_loss': DeviceArray(0.33014566, dtype=float32), 'train/wer': 0.11944704168388093, 'validation/ctc_loss': DeviceArray(0.61305237, dtype=float32), 'validation/wer': 0.18375478779341817, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38567758, dtype=float32), 'test/wer': 0.13011597911969613, 'test/num_examples': 2472}
I0315 02:20:58.221485 139762898564864 logging_writer.py:48] [15408] global_step=15408, preemption_count=0, score=12026.035772, test/ctc_loss=0.3856775760650635, test/num_examples=2472, test/wer=0.130116, total_duration=12579.926593, train/ctc_loss=0.3301456570625305, train/wer=0.119447, validation/ctc_loss=0.6130523681640625, validation/num_examples=5348, validation/wer=0.183755
I0315 02:20:58.577153 139935937595200 checkpoints.py:356] Saving checkpoint at step: 15408
I0315 02:21:00.003876 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_15408
I0315 02:21:00.037148 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_15408.
I0315 02:22:13.842168 139762898564864 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6353570222854614, loss=1.4969245195388794
I0315 02:23:29.376490 139762890172160 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.5701413750648499, loss=1.4978379011154175
I0315 02:24:44.938014 139762898564864 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6246402859687805, loss=1.4937310218811035
I0315 02:26:00.213110 139762890172160 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.610101580619812, loss=1.510520100593567
I0315 02:27:15.557580 139762898564864 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6189121603965759, loss=1.5000903606414795
I0315 02:28:31.115259 139762890172160 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6030630469322205, loss=1.4624087810516357
I0315 02:29:47.366427 139762898564864 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.5822808742523193, loss=1.5229244232177734
I0315 02:31:07.083665 139762890172160 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.538598358631134, loss=1.4722368717193604
I0315 02:32:28.536883 139762898564864 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.5623914003372192, loss=1.5449987649917603
I0315 02:33:50.336641 139762890172160 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.6573609113693237, loss=1.5416316986083984
I0315 02:35:17.033705 139762898564864 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6632250547409058, loss=1.5193711519241333
I0315 02:36:32.399416 139762890172160 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.554344892501831, loss=1.554903507232666
I0315 02:37:47.810536 139762898564864 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5210164785385132, loss=1.4485080242156982
I0315 02:39:03.069786 139762890172160 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6041446328163147, loss=1.4499433040618896
I0315 02:40:18.304769 139762898564864 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.6874668598175049, loss=1.5216208696365356
I0315 02:41:34.237608 139762890172160 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7557258605957031, loss=1.5477914810180664
I0315 02:42:59.948291 139762898564864 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5822141766548157, loss=1.46635103225708
I0315 02:44:25.019392 139762890172160 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.691008448600769, loss=1.4894365072250366
I0315 02:45:49.577771 139762898564864 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.6646725535392761, loss=1.5364763736724854
I0315 02:47:10.304510 139762890172160 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6331321597099304, loss=1.5040594339370728
I0315 02:48:33.221809 139762898564864 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.606048583984375, loss=1.4611148834228516
I0315 02:49:52.561884 139762898564864 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.6726195216178894, loss=1.4462863206863403
I0315 02:51:07.978334 139762890172160 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.6157379150390625, loss=1.5004758834838867
I0315 02:52:23.246151 139762898564864 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.5720090270042419, loss=1.4942867755889893
I0315 02:53:38.589844 139762890172160 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6156302094459534, loss=1.4473321437835693
I0315 02:54:54.001987 139762898564864 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6537410020828247, loss=1.4195945262908936
I0315 02:56:09.807882 139762890172160 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5954867005348206, loss=1.4354274272918701
I0315 02:57:31.338281 139762898564864 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6466848850250244, loss=1.4520775079727173
I0315 02:58:51.692864 139762890172160 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.7453311681747437, loss=1.5162240266799927
I0315 03:00:12.419723 139762898564864 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.5735301971435547, loss=1.4664905071258545
I0315 03:01:00.715278 139935937595200 spec.py:298] Evaluating on the training split.
I0315 03:01:37.910041 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 03:02:16.938251 139935937595200 spec.py:326] Evaluating on the test split.
I0315 03:02:37.181755 139935937595200 submission_runner.py:362] Time since start: 15077.66s, 	Step: 18460, 	{'train/ctc_loss': DeviceArray(0.32438862, dtype=float32), 'train/wer': 0.11455793780774709, 'validation/ctc_loss': DeviceArray(0.5705625, dtype=float32), 'validation/wer': 0.172891200108057, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.35358003, dtype=float32), 'test/wer': 0.11975707350760668, 'test/num_examples': 2472}
I0315 03:02:37.204697 139762279040768 logging_writer.py:48] [18460] global_step=18460, preemption_count=0, score=14419.042432, test/ctc_loss=0.3535800278186798, test/num_examples=2472, test/wer=0.119757, total_duration=15077.660468, train/ctc_loss=0.32438862323760986, train/wer=0.114558, validation/ctc_loss=0.570562481880188, validation/num_examples=5348, validation/wer=0.172891
I0315 03:02:37.555130 139935937595200 checkpoints.py:356] Saving checkpoint at step: 18460
I0315 03:02:38.975956 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_18460
I0315 03:02:39.009008 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_18460.
I0315 03:03:09.832518 139762270648064 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7407175302505493, loss=1.481427788734436
I0315 03:04:28.959341 139761623680768 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.6757586598396301, loss=1.4758317470550537
I0315 03:05:44.354784 139761615288064 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6833162903785706, loss=1.461805820465088
I0315 03:06:59.735772 139761623680768 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6679310202598572, loss=1.4425957202911377
I0315 03:08:15.026989 139761615288064 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6224633455276489, loss=1.4364609718322754
I0315 03:09:30.482051 139761623680768 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5651106834411621, loss=1.4941242933273315
I0315 03:10:47.797098 139761615288064 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6681390404701233, loss=1.4725667238235474
I0315 03:12:07.504693 139761623680768 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6363490223884583, loss=1.5264793634414673
I0315 03:13:29.335413 139761615288064 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.604859471321106, loss=1.5132931470870972
I0315 03:14:51.100907 139761623680768 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5753644704818726, loss=1.4251407384872437
I0315 03:16:14.867348 139761615288064 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6057137250900269, loss=1.4395290613174438
I0315 03:17:38.204850 139761623680768 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.6100925803184509, loss=1.539756417274475
I0315 03:18:53.743950 139761615288064 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6773275136947632, loss=1.480899691581726
I0315 03:20:09.149860 139761623680768 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.7496287822723389, loss=1.472265362739563
I0315 03:21:24.542577 139761615288064 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6575842499732971, loss=1.4684048891067505
I0315 03:22:39.911033 139761623680768 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6448704600334167, loss=1.444709062576294
I0315 03:23:55.474866 139761615288064 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6236075758934021, loss=1.5268924236297607
I0315 03:25:13.584407 139761623680768 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6568176746368408, loss=1.4497243165969849
I0315 03:26:35.144559 139761615288064 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5957030653953552, loss=1.4685091972351074
I0315 03:27:57.715058 139761623680768 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6130722761154175, loss=1.4225335121154785
I0315 03:29:21.342684 139761615288064 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6961593627929688, loss=1.447837471961975
I0315 03:30:44.672888 139761623680768 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6467190384864807, loss=1.4517251253128052
I0315 03:31:59.933134 139761615288064 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.7110076546669006, loss=1.4573683738708496
I0315 03:33:15.443597 139761623680768 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.7177222371101379, loss=1.4957481622695923
I0315 03:34:31.018248 139761615288064 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.6078994870185852, loss=1.4382843971252441
I0315 03:35:46.253922 139761623680768 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.5690964460372925, loss=1.4427121877670288
I0315 03:37:01.607492 139761615288064 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.69910728931427, loss=1.4527008533477783
I0315 03:38:16.916208 139761623680768 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.7007936239242554, loss=1.4376115798950195
I0315 03:39:39.266580 139761615288064 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.5845285654067993, loss=1.424242377281189
I0315 03:41:01.557936 139761623680768 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.7340860366821289, loss=1.3754405975341797
I0315 03:42:24.362397 139761615288064 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6062661409378052, loss=1.408268690109253
I0315 03:42:39.254169 139935937595200 spec.py:298] Evaluating on the training split.
I0315 03:43:16.642360 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 03:43:54.883188 139935937595200 spec.py:326] Evaluating on the test split.
I0315 03:44:15.447266 139935937595200 submission_runner.py:362] Time since start: 17576.20s, 	Step: 21520, 	{'train/ctc_loss': DeviceArray(0.27776644, dtype=float32), 'train/wer': 0.1005413396294642, 'validation/ctc_loss': DeviceArray(0.5416949, dtype=float32), 'validation/wer': 0.16300205501259057, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3335092, dtype=float32), 'test/wer': 0.11281051327361728, 'test/num_examples': 2472}
I0315 03:44:15.468418 139761623680768 logging_writer.py:48] [21520] global_step=21520, preemption_count=0, score=16811.673843, test/ctc_loss=0.3335092067718506, test/num_examples=2472, test/wer=0.112811, total_duration=17576.199377, train/ctc_loss=0.2777664363384247, train/wer=0.100541, validation/ctc_loss=0.5416948795318604, validation/num_examples=5348, validation/wer=0.163002
I0315 03:44:15.824081 139935937595200 checkpoints.py:356] Saving checkpoint at step: 21520
I0315 03:44:17.242714 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_21520
I0315 03:44:17.275357 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_21520.
I0315 03:45:18.185013 139761615288064 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6202176809310913, loss=1.422149419784546
I0315 03:46:36.940280 139761623680768 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.5805335640907288, loss=1.4080209732055664
I0315 03:47:52.519642 139761615288064 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6610839366912842, loss=1.4141982793807983
I0315 03:49:08.121994 139761623680768 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5908405780792236, loss=1.4601622819900513
I0315 03:50:23.468225 139761615288064 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6308094263076782, loss=1.399021863937378
I0315 03:51:38.780453 139761623680768 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.6565800309181213, loss=1.462044596672058
I0315 03:52:55.918511 139761615288064 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.6776483058929443, loss=1.4740291833877563
I0315 03:54:19.228745 139761623680768 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6561344861984253, loss=1.448710560798645
I0315 03:55:42.213690 139761615288064 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6115274429321289, loss=1.4229806661605835
I0315 03:57:05.630726 139761623680768 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6960494518280029, loss=1.4020845890045166
I0315 03:58:27.515043 139761615288064 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6043047308921814, loss=1.4527029991149902
I0315 03:59:48.556673 139762279040768 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.6329070329666138, loss=1.3540171384811401
I0315 04:01:03.939971 139762270648064 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.5964736938476562, loss=1.385201096534729
I0315 04:02:19.537057 139762279040768 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.635191023349762, loss=1.4206875562667847
I0315 04:03:34.774640 139762270648064 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7460612058639526, loss=1.4483288526535034
I0315 04:04:50.027432 139762279040768 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.6993519067764282, loss=1.4351866245269775
I0315 04:06:11.872607 139762270648064 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.6957226395606995, loss=1.4368150234222412
I0315 04:07:36.951687 139762279040768 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.7284497022628784, loss=1.399744987487793
I0315 04:09:01.327804 139762270648064 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.6525576114654541, loss=1.439163327217102
I0315 04:10:26.231220 139762279040768 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.533149778842926, loss=1.3878902196884155
I0315 04:11:46.882389 139762270648064 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6853886842727661, loss=1.3753910064697266
I0315 04:13:12.300401 139761623680768 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.6085778474807739, loss=1.3990272283554077
I0315 04:14:27.859438 139761615288064 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.5414881110191345, loss=1.3510090112686157
I0315 04:15:43.079745 139761623680768 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.6487655639648438, loss=1.4117538928985596
I0315 04:16:58.414070 139761615288064 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6839941740036011, loss=1.4130421876907349
I0315 04:18:13.626637 139761623680768 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.6672658324241638, loss=1.4250853061676025
I0315 04:19:28.913284 139761615288064 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6783950328826904, loss=1.351832389831543
I0315 04:20:53.373006 139761623680768 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.632893979549408, loss=1.4505550861358643
I0315 04:22:18.416346 139761615288064 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8269574642181396, loss=1.3766928911209106
I0315 04:23:40.159192 139761623680768 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.5818209648132324, loss=1.3841770887374878
I0315 04:24:17.716725 139935937595200 spec.py:298] Evaluating on the training split.
I0315 04:24:55.466098 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 04:25:34.741801 139935937595200 spec.py:326] Evaluating on the test split.
I0315 04:25:54.720390 139935937595200 submission_runner.py:362] Time since start: 20074.66s, 	Step: 24548, 	{'train/ctc_loss': DeviceArray(0.24292839, dtype=float32), 'train/wer': 0.09057437790894199, 'validation/ctc_loss': DeviceArray(0.5270403, dtype=float32), 'validation/wer': 0.1577535721521674, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.31746113, dtype=float32), 'test/wer': 0.1072654520342047, 'test/num_examples': 2472}
I0315 04:25:54.742485 139761623680768 logging_writer.py:48] [24548] global_step=24548, preemption_count=0, score=19204.569430, test/ctc_loss=0.31746113300323486, test/num_examples=2472, test/wer=0.107265, total_duration=20074.661944, train/ctc_loss=0.2429283857345581, train/wer=0.090574, validation/ctc_loss=0.5270403027534485, validation/num_examples=5348, validation/wer=0.157754
I0315 04:25:55.031823 139935937595200 checkpoints.py:356] Saving checkpoint at step: 24548
I0315 04:25:56.458512 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_24548
I0315 04:25:56.494385 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_24548.
I0315 04:26:36.357833 139761615288064 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.7650908827781677, loss=1.4136772155761719
I0315 04:27:51.591362 139761514575616 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.5750988721847534, loss=1.4142712354660034
I0315 04:29:10.426615 139761623680768 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6912446022033691, loss=1.3986831903457642
I0315 04:30:25.601176 139761615288064 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6622530221939087, loss=1.3507310152053833
I0315 04:31:40.854814 139761623680768 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6741664409637451, loss=1.4155688285827637
I0315 04:32:56.163289 139761615288064 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.6643538475036621, loss=1.408453106880188
I0315 04:34:11.380253 139761623680768 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6403289437294006, loss=1.3262872695922852
I0315 04:35:26.861153 139761615288064 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.5835379362106323, loss=1.3998568058013916
I0315 04:36:49.136872 139761623680768 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.65788334608078, loss=1.4036914110183716
I0315 04:38:12.182573 139761615288064 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6035597324371338, loss=1.3636444807052612
I0315 04:39:37.536826 139761623680768 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.7178552150726318, loss=1.4197278022766113
I0315 04:41:01.061815 139761615288064 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.6942446827888489, loss=1.4040240049362183
I0315 04:42:22.445683 139761623680768 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6661532521247864, loss=1.3201054334640503
I0315 04:43:37.700364 139761615288064 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.5778281092643738, loss=1.34403395652771
I0315 04:44:53.000581 139761623680768 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5465469360351562, loss=1.4064081907272339
I0315 04:46:08.381160 139761615288064 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.6751388907432556, loss=1.3928930759429932
I0315 04:47:23.650471 139761623680768 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.7684779763221741, loss=1.3187875747680664
I0315 04:48:41.933478 139761615288064 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.5816919803619385, loss=1.3603407144546509
I0315 04:50:03.718122 139761623680768 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.5771607160568237, loss=1.3744045495986938
I0315 04:51:28.283040 139761615288064 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6651068925857544, loss=1.4044275283813477
I0315 04:52:48.625275 139761623680768 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.6237093210220337, loss=1.3849126100540161
I0315 04:54:09.823399 139761615288064 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.6195099949836731, loss=1.3902223110198975
I0315 04:55:34.786017 139761623680768 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.5617705583572388, loss=1.3673551082611084
I0315 04:56:50.142225 139761615288064 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.6521305441856384, loss=1.3531458377838135
I0315 04:58:05.488838 139761623680768 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.5851324200630188, loss=1.3722575902938843
I0315 04:59:21.003924 139761615288064 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6230857968330383, loss=1.3846133947372437
I0315 05:00:36.219985 139761623680768 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.6689978241920471, loss=1.4071290493011475
I0315 05:01:51.412362 139761615288064 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6248878836631775, loss=1.3997936248779297
I0315 05:03:06.957381 139761623680768 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.6697843670845032, loss=1.3903453350067139
I0315 05:04:31.780601 139761615288064 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.590264618396759, loss=1.3805139064788818
I0315 05:05:55.810888 139761623680768 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.6752839088439941, loss=1.3457558155059814
I0315 05:05:57.042702 139935937595200 spec.py:298] Evaluating on the training split.
I0315 05:06:34.579786 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 05:07:12.966607 139935937595200 spec.py:326] Evaluating on the test split.
I0315 05:07:33.556506 139935937595200 submission_runner.py:362] Time since start: 22573.99s, 	Step: 27603, 	{'train/ctc_loss': DeviceArray(0.22396263, dtype=float32), 'train/wer': 0.08077927743404498, 'validation/ctc_loss': DeviceArray(0.50849587, dtype=float32), 'validation/wer': 0.15240860982739823, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.30482325, dtype=float32), 'test/wer': 0.10212662238742307, 'test/num_examples': 2472}
I0315 05:07:33.577418 139761623680768 logging_writer.py:48] [27603] global_step=27603, preemption_count=0, score=21597.555044, test/ctc_loss=0.30482324957847595, test/num_examples=2472, test/wer=0.102127, total_duration=22573.987916, train/ctc_loss=0.22396263480186462, train/wer=0.080779, validation/ctc_loss=0.5084958672523499, validation/num_examples=5348, validation/wer=0.152409
I0315 05:07:33.919321 139935937595200 checkpoints.py:356] Saving checkpoint at step: 27603
I0315 05:07:35.344642 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_27603
I0315 05:07:35.377606 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_27603.
I0315 05:08:49.048756 139761615288064 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.6076884269714355, loss=1.347110390663147
I0315 05:10:04.257018 139761506182912 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6801438331604004, loss=1.3592934608459473
I0315 05:11:23.382786 139761623680768 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6482003331184387, loss=1.3583526611328125
I0315 05:12:39.097616 139761615288064 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.640713095664978, loss=1.3228262662887573
I0315 05:13:54.627271 139761623680768 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.8895229697227478, loss=1.3789159059524536
I0315 05:15:10.060778 139761615288064 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.7529674172401428, loss=1.2886884212493896
I0315 05:16:25.581534 139761623680768 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5971235632896423, loss=1.295210599899292
I0315 05:17:44.577447 139761615288064 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.5960933566093445, loss=1.3874998092651367
I0315 05:19:08.913130 139761623680768 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6382623314857483, loss=1.3966765403747559
I0315 05:20:34.134425 139761615288064 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.5893550515174866, loss=1.3123528957366943
I0315 05:21:59.141352 139761623680768 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.5940583944320679, loss=1.3568652868270874
I0315 05:23:21.217941 139761615288064 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.5747523903846741, loss=1.3117830753326416
I0315 05:24:43.294802 139761623680768 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.598153293132782, loss=1.3494598865509033
I0315 05:25:58.567138 139761615288064 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.5899168252944946, loss=1.3331345319747925
I0315 05:27:13.890674 139761623680768 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.7111749649047852, loss=1.3055485486984253
I0315 05:28:29.305934 139761615288064 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.5920522809028625, loss=1.3014721870422363
I0315 05:29:44.574974 139761623680768 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.7209250330924988, loss=1.4002957344055176
I0315 05:30:59.786195 139761615288064 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.5802310109138489, loss=1.3528214693069458
I0315 05:32:23.229140 139761623680768 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.6954690217971802, loss=1.3142355680465698
I0315 05:33:47.975154 139761615288064 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.61613529920578, loss=1.3179936408996582
I0315 05:35:12.002765 139761623680768 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.6763858795166016, loss=1.3721754550933838
I0315 05:36:35.234654 139761615288064 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.6614867448806763, loss=1.3583992719650269
I0315 05:37:59.439157 139761623680768 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.6402707099914551, loss=1.3085108995437622
I0315 05:39:15.006445 139761615288064 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6930351257324219, loss=1.3148404359817505
I0315 05:40:30.672251 139761623680768 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.7135526537895203, loss=1.2705776691436768
I0315 05:41:46.029511 139761615288064 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.5603720545768738, loss=1.2949256896972656
I0315 05:43:01.273859 139761623680768 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.5740072727203369, loss=1.311989188194275
I0315 05:44:20.976917 139761615288064 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.7361807823181152, loss=1.3249356746673584
I0315 05:45:43.693010 139761623680768 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.656028151512146, loss=1.3894058465957642
I0315 05:47:07.758366 139761615288064 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.6187121272087097, loss=1.3263914585113525
I0315 05:47:35.450159 139935937595200 spec.py:298] Evaluating on the training split.
I0315 05:48:12.638385 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 05:48:50.886251 139935937595200 spec.py:326] Evaluating on the test split.
I0315 05:49:11.561641 139935937595200 submission_runner.py:362] Time since start: 25072.40s, 	Step: 30635, 	{'train/ctc_loss': DeviceArray(0.21750984, dtype=float32), 'train/wer': 0.08221293848241244, 'validation/ctc_loss': DeviceArray(0.4860431, dtype=float32), 'validation/wer': 0.1466101940202028, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.2918611, dtype=float32), 'test/wer': 0.0983283569963236, 'test/num_examples': 2472}
I0315 05:49:11.584410 139763113604864 logging_writer.py:48] [30635] global_step=30635, preemption_count=0, score=23990.057044, test/ctc_loss=0.29186108708381653, test/num_examples=2472, test/wer=0.098328, total_duration=25072.395372, train/ctc_loss=0.21750983595848083, train/wer=0.082213, validation/ctc_loss=0.4860430955886841, validation/num_examples=5348, validation/wer=0.146610
I0315 05:49:11.932659 139935937595200 checkpoints.py:356] Saving checkpoint at step: 30635
I0315 05:49:13.366442 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_30635
I0315 05:49:13.399181 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_30635.
I0315 05:50:02.974668 139763105212160 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6212292909622192, loss=1.3602211475372314
I0315 05:51:18.303218 139762987714304 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.7162070870399475, loss=1.3676687479019165
I0315 05:52:37.494737 139761951360768 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.6716130375862122, loss=1.292216420173645
I0315 05:53:52.899704 139761942968064 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.711775004863739, loss=1.3271534442901611
I0315 05:55:08.277704 139761951360768 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.6448391675949097, loss=1.3343732357025146
I0315 05:56:23.627136 139761942968064 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.6067717671394348, loss=1.3138281106948853
I0315 05:57:38.970558 139761951360768 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.7010110020637512, loss=1.3647247552871704
I0315 05:58:54.303211 139761942968064 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6733809113502502, loss=1.369356632232666
I0315 06:00:14.491957 139761951360768 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.7172730565071106, loss=1.3153865337371826
I0315 06:01:34.898540 139761942968064 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.598861038684845, loss=1.2983927726745605
I0315 06:03:00.895974 139761951360768 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6702677607536316, loss=1.3312629461288452
I0315 06:04:23.553007 139761942968064 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.5840514898300171, loss=1.2934811115264893
I0315 06:05:45.094156 139761951360768 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.7021504640579224, loss=1.3272769451141357
I0315 06:07:05.283138 139763113604864 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6501168608665466, loss=1.3615447282791138
I0315 06:08:20.598870 139763105212160 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.6237201690673828, loss=1.3689426183700562
I0315 06:09:35.962836 139763113604864 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.6702476143836975, loss=1.3282781839370728
I0315 06:10:51.235352 139763105212160 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.7449494004249573, loss=1.3320950269699097
I0315 06:12:06.603011 139763113604864 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.6754727959632874, loss=1.2887957096099854
I0315 06:13:22.464425 139763105212160 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7737285494804382, loss=1.3522926568984985
I0315 06:14:44.049841 139763113604864 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.7156861424446106, loss=1.3472102880477905
I0315 06:16:03.653380 139763105212160 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.735691487789154, loss=1.2896775007247925
I0315 06:17:26.620581 139763113604864 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.6774386763572693, loss=1.3161588907241821
I0315 06:18:49.052699 139763105212160 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6697776913642883, loss=1.3732868432998657
I0315 06:20:11.352388 139761623680768 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.636421263217926, loss=1.3067282438278198
I0315 06:21:26.624791 139761615288064 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.6215370297431946, loss=1.2896783351898193
I0315 06:22:41.995919 139761623680768 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.7249249815940857, loss=1.375341773033142
I0315 06:23:57.349163 139761615288064 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.7385448217391968, loss=1.3026798963546753
I0315 06:25:12.884380 139761623680768 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.5951370000839233, loss=1.271975040435791
I0315 06:26:28.278261 139761615288064 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6263713240623474, loss=1.361571192741394
I0315 06:27:50.201399 139761623680768 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.6997973322868347, loss=1.3502984046936035
I0315 06:29:13.630477 139761615288064 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.6437106728553772, loss=1.3137929439544678
I0315 06:29:13.638717 139935937595200 spec.py:298] Evaluating on the training split.
I0315 06:29:51.628088 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 06:30:29.843382 139935937595200 spec.py:326] Evaluating on the test split.
I0315 06:30:49.884827 139935937595200 submission_runner.py:362] Time since start: 27570.58s, 	Step: 33701, 	{'train/ctc_loss': DeviceArray(0.23125577, dtype=float32), 'train/wer': 0.08354530791401148, 'validation/ctc_loss': DeviceArray(0.48458257, dtype=float32), 'validation/wer': 0.14441046223311368, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.28890395, dtype=float32), 'test/wer': 0.09749558223143014, 'test/num_examples': 2472}
I0315 06:30:49.905843 139762355844864 logging_writer.py:48] [33701] global_step=33701, preemption_count=0, score=26382.577127, test/ctc_loss=0.28890395164489746, test/num_examples=2472, test/wer=0.097496, total_duration=27570.583963, train/ctc_loss=0.23125576972961426, train/wer=0.083545, validation/ctc_loss=0.4845825731754303, validation/num_examples=5348, validation/wer=0.144410
I0315 06:30:50.239643 139935937595200 checkpoints.py:356] Saving checkpoint at step: 33701
I0315 06:30:51.672182 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_33701
I0315 06:30:51.705064 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_33701.
I0315 06:32:07.077923 139762347452160 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.6921434998512268, loss=1.299531102180481
I0315 06:33:22.352634 139761598502656 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.6278396248817444, loss=1.3422644138336182
I0315 06:34:41.414950 139763113604864 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.6415800452232361, loss=1.282153606414795
I0315 06:35:56.644814 139763105212160 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.6414360404014587, loss=1.3188070058822632
I0315 06:37:11.856283 139763113604864 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.6087728142738342, loss=1.3296711444854736
I0315 06:38:27.407118 139763105212160 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.5842912793159485, loss=1.2746763229370117
I0315 06:39:42.662559 139763113604864 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.6919357776641846, loss=1.28120756149292
I0315 06:40:57.856122 139763105212160 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.6432725191116333, loss=1.3247114419937134
I0315 06:42:15.090878 139763113604864 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.5612615942955017, loss=1.3036757707595825
I0315 06:43:38.504491 139763105212160 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.5778928399085999, loss=1.3338849544525146
I0315 06:44:59.790685 139763113604864 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.6660969853401184, loss=1.3160896301269531
I0315 06:46:21.370131 139763105212160 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.8113418221473694, loss=1.3140321969985962
I0315 06:47:43.389260 139763113604864 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6355828642845154, loss=1.3190616369247437
I0315 06:49:03.821617 139763113604864 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.5841492414474487, loss=1.2980375289916992
I0315 06:50:19.511453 139763105212160 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6782524585723877, loss=1.3296458721160889
I0315 06:51:35.104139 139763113604864 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.5954645872116089, loss=1.310247778892517
I0315 06:52:50.728020 139763105212160 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.7183413505554199, loss=1.276213526725769
I0315 06:54:06.502842 139763113604864 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6058353781700134, loss=1.3264142274856567
I0315 06:55:22.022173 139763105212160 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.7159697413444519, loss=1.3057942390441895
I0315 06:56:39.824111 139763113604864 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.6250089406967163, loss=1.278867483139038
I0315 06:58:01.260641 139763105212160 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.6748412251472473, loss=1.2759732007980347
I0315 06:59:22.920595 139763113604864 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.5994223952293396, loss=1.313523292541504
I0315 07:00:45.015182 139763105212160 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.6595799326896667, loss=1.2971943616867065
I0315 07:02:09.498679 139763113604864 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.6134226322174072, loss=1.2285789251327515
I0315 07:03:24.907398 139763105212160 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.6574712991714478, loss=1.344353199005127
I0315 07:04:40.543768 139763113604864 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.7534841299057007, loss=1.2700505256652832
I0315 07:05:55.977177 139763105212160 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.6598345041275024, loss=1.2707041501998901
I0315 07:07:11.363655 139763113604864 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.8137433528900146, loss=1.340105652809143
I0315 07:08:32.841908 139763105212160 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.7422052621841431, loss=1.322403907775879
I0315 07:09:58.030670 139763113604864 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.7497833967208862, loss=1.244216799736023
I0315 07:10:51.960127 139935937595200 spec.py:298] Evaluating on the training split.
I0315 07:11:29.832173 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 07:12:07.818162 139935937595200 spec.py:326] Evaluating on the test split.
I0315 07:12:27.784422 139935937595200 submission_runner.py:362] Time since start: 30068.91s, 	Step: 36765, 	{'train/ctc_loss': DeviceArray(0.21903068, dtype=float32), 'train/wer': 0.08019626981823938, 'validation/ctc_loss': DeviceArray(0.46387938, dtype=float32), 'validation/wer': 0.1382164806220996, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.26881742, dtype=float32), 'test/wer': 0.09022403672333597, 'test/num_examples': 2472}
I0315 07:12:27.806898 139762821764864 logging_writer.py:48] [36765] global_step=36765, preemption_count=0, score=28775.156538, test/ctc_loss=0.2688174247741699, test/num_examples=2472, test/wer=0.090224, total_duration=30068.905347, train/ctc_loss=0.21903067827224731, train/wer=0.080196, validation/ctc_loss=0.46387937664985657, validation/num_examples=5348, validation/wer=0.138216
I0315 07:12:28.147912 139935937595200 checkpoints.py:356] Saving checkpoint at step: 36765
I0315 07:12:29.599835 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_36765
I0315 07:12:29.632401 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_36765.
I0315 07:12:56.780906 139762813372160 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6159867644309998, loss=1.300120234489441
I0315 07:14:12.363769 139762679088896 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.6947963833808899, loss=1.3051286935806274
I0315 07:15:27.566344 139762813372160 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.6238996982574463, loss=1.2956525087356567
I0315 07:16:46.383240 139761736324864 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.793155312538147, loss=1.3077194690704346
I0315 07:18:01.795652 139761727932160 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.6552029252052307, loss=1.265884280204773
I0315 07:19:17.129815 139761736324864 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.6228598356246948, loss=1.248337745666504
I0315 07:20:32.484825 139761727932160 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6091471314430237, loss=1.214091181755066
I0315 07:21:47.726607 139761736324864 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.7062866687774658, loss=1.3056812286376953
I0315 07:23:10.154072 139761727932160 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6760917901992798, loss=1.3574509620666504
I0315 07:24:34.750688 139761736324864 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.6177636384963989, loss=1.3498917818069458
I0315 07:25:56.172407 139761727932160 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.5746742486953735, loss=1.2772891521453857
I0315 07:27:19.846331 139761736324864 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6935849785804749, loss=1.3243708610534668
I0315 07:28:42.514501 139761727932160 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.6489433646202087, loss=1.2732815742492676
I0315 07:30:07.936775 139761736324864 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.6155411005020142, loss=1.2972992658615112
I0315 07:31:27.336305 139761736324864 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.5627586245536804, loss=1.255645513534546
I0315 07:32:42.671101 139761727932160 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.6372102499008179, loss=1.2399629354476929
I0315 07:33:57.928605 139761736324864 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.782659113407135, loss=1.2565969228744507
I0315 07:35:13.661157 139761727932160 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.6471582651138306, loss=1.2938193082809448
I0315 07:36:29.132993 139761736324864 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.7318876385688782, loss=1.2570295333862305
I0315 07:37:55.097832 139761727932160 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.7081212401390076, loss=1.3140335083007812
I0315 07:39:18.791423 139761736324864 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.5729063749313354, loss=1.279961109161377
I0315 07:40:41.574879 139761727932160 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.6348761320114136, loss=1.2592401504516602
I0315 07:42:03.880678 139761736324864 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.6669082045555115, loss=1.2748150825500488
I0315 07:43:27.329153 139761727932160 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.6033986210823059, loss=1.2607934474945068
I0315 07:44:49.744385 139762821764864 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.7284701466560364, loss=1.2859174013137817
I0315 07:46:05.177746 139762813372160 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.6270180344581604, loss=1.315444827079773
I0315 07:47:20.702875 139762821764864 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.631626307964325, loss=1.260638952255249
I0315 07:48:36.078984 139762813372160 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.6467857360839844, loss=1.296647310256958
I0315 07:49:51.370301 139762821764864 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.764142632484436, loss=1.3086352348327637
I0315 07:51:06.737958 139762813372160 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7884160280227661, loss=1.292633295059204
I0315 07:52:27.800886 139762821764864 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.6434617638587952, loss=1.3222720623016357
I0315 07:52:29.999154 139935937595200 spec.py:298] Evaluating on the training split.
I0315 07:53:08.199375 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 07:53:46.952070 139935937595200 spec.py:326] Evaluating on the test split.
I0315 07:54:06.980945 139935937595200 submission_runner.py:362] Time since start: 32566.94s, 	Step: 39804, 	{'train/ctc_loss': DeviceArray(0.20274656, dtype=float32), 'train/wer': 0.07532153513735981, 'validation/ctc_loss': DeviceArray(0.45643613, dtype=float32), 'validation/wer': 0.13585273374562223, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.2656183, dtype=float32), 'test/wer': 0.08896471878618, 'test/num_examples': 2472}
I0315 07:54:07.005136 139763190404864 logging_writer.py:48] [39804] global_step=39804, preemption_count=0, score=31167.975384, test/ctc_loss=0.26561829447746277, test/num_examples=2472, test/wer=0.088965, total_duration=32566.944352, train/ctc_loss=0.20274655520915985, train/wer=0.075322, validation/ctc_loss=0.4564361274242401, validation/num_examples=5348, validation/wer=0.135853
I0315 07:54:07.382843 139935937595200 checkpoints.py:356] Saving checkpoint at step: 39804
I0315 07:54:08.973862 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_39804
I0315 07:54:09.004366 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_39804.
I0315 07:55:22.213513 139763182012160 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.6860199570655823, loss=1.3366622924804688
I0315 07:56:38.036969 139762452121344 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.5965204238891602, loss=1.2425293922424316
I0315 07:57:53.312049 139763182012160 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.6406431198120117, loss=1.2726213932037354
I0315 07:59:12.119045 139763190404864 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.6263759136199951, loss=1.223160743713379
I0315 08:00:27.430201 139763182012160 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.7272923588752747, loss=1.2725160121917725
I0315 08:01:42.818077 139763190404864 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.6580336093902588, loss=1.3225208520889282
I0315 08:02:57.993674 139763182012160 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.69010329246521, loss=1.2570759057998657
I0315 08:04:13.649526 139763190404864 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.6794377565383911, loss=1.2588564157485962
I0315 08:05:35.302066 139763182012160 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.7911706566810608, loss=1.306862235069275
I0315 08:07:00.947643 139763190404864 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.6591674089431763, loss=1.290661334991455
I0315 08:08:24.967818 139763182012160 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.5895281434059143, loss=1.2800078392028809
I0315 08:09:47.400094 139763190404864 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.7690017819404602, loss=1.2933356761932373
I0315 08:11:09.324385 139763182012160 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.6168747544288635, loss=1.2512115240097046
I0315 08:12:35.241124 139763190404864 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.6731277704238892, loss=1.3217027187347412
I0315 08:13:50.580799 139763182012160 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.6499541401863098, loss=1.3074049949645996
I0315 08:15:05.877098 139763190404864 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.6415613889694214, loss=1.301025152206421
I0315 08:16:21.213283 139763182012160 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.7202642560005188, loss=1.3455810546875
I0315 08:17:36.462636 139763190404864 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.6489726305007935, loss=1.2729517221450806
I0315 08:18:51.840683 139763182012160 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.8228327035903931, loss=1.2699803113937378
I0315 08:20:14.797129 139763190404864 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.7979223132133484, loss=1.2631536722183228
I0315 08:21:39.255920 139763182012160 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.7104536890983582, loss=1.340628981590271
I0315 08:23:02.728457 139763190404864 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.6763255000114441, loss=1.305562138557434
I0315 08:24:26.003116 139763182012160 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.5868734121322632, loss=1.2618136405944824
I0315 08:25:52.058149 139763190404864 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.6446316838264465, loss=1.1988431215286255
I0315 08:27:13.653958 139763190404864 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.7438778877258301, loss=1.3042337894439697
I0315 08:28:29.062918 139763182012160 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.6991569995880127, loss=1.249725580215454
I0315 08:29:44.296800 139763190404864 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.7423833608627319, loss=1.2428090572357178
I0315 08:30:59.655858 139763182012160 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.6598812937736511, loss=1.255636215209961
I0315 08:32:14.926788 139763190404864 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.7799964547157288, loss=1.2784000635147095
I0315 08:33:33.355471 139763182012160 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.6253693103790283, loss=1.2760486602783203
I0315 08:34:09.137636 139935937595200 spec.py:298] Evaluating on the training split.
I0315 08:34:46.621334 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 08:35:25.023720 139935937595200 spec.py:326] Evaluating on the test split.
I0315 08:35:45.125492 139935937595200 submission_runner.py:362] Time since start: 35066.08s, 	Step: 42845, 	{'train/ctc_loss': DeviceArray(0.18709019, dtype=float32), 'train/wer': 0.06900574300071788, 'validation/ctc_loss': DeviceArray(0.44811746, dtype=float32), 'validation/wer': 0.13061389883163368, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.25967354, dtype=float32), 'test/wer': 0.08620234395628948, 'test/num_examples': 2472}
I0315 08:35:45.148774 139762606724864 logging_writer.py:48] [42845] global_step=42845, preemption_count=0, score=33560.548149, test/ctc_loss=0.259673535823822, test/num_examples=2472, test/wer=0.086202, total_duration=35066.082852, train/ctc_loss=0.1870901882648468, train/wer=0.069006, validation/ctc_loss=0.4481174647808075, validation/num_examples=5348, validation/wer=0.130614
I0315 08:35:45.484455 139935937595200 checkpoints.py:356] Saving checkpoint at step: 42845
I0315 08:35:46.930724 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_42845
I0315 08:35:46.964571 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_42845.
I0315 08:36:29.049475 139762598332160 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.6782833933830261, loss=1.2724374532699585
I0315 08:37:44.293788 139761901008640 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6826028227806091, loss=1.2189984321594238
I0315 08:38:59.534625 139762598332160 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.5980843305587769, loss=1.2442095279693604
I0315 08:40:16.898824 139761901008640 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.631624698638916, loss=1.2788703441619873
I0315 08:41:37.625378 139762279044864 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.599304735660553, loss=1.2405465841293335
I0315 08:42:53.134647 139762270652160 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.6887439489364624, loss=1.2636504173278809
I0315 08:44:08.534645 139762279044864 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.6445598602294922, loss=1.2479658126831055
I0315 08:45:23.876856 139762270652160 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.6770893931388855, loss=1.2590537071228027
I0315 08:46:39.116669 139762279044864 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.7914355397224426, loss=1.2720320224761963
I0315 08:47:54.871528 139762270652160 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.7471307516098022, loss=1.225742220878601
I0315 08:49:10.349847 139762279044864 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.6073476672172546, loss=1.215366244316101
I0315 08:50:26.926964 139762270652160 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.7003144025802612, loss=1.2054206132888794
I0315 08:51:48.873246 139762279044864 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.7583470344543457, loss=1.2849857807159424
I0315 08:53:11.124910 139762270652160 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.6665756702423096, loss=1.30694580078125
I0315 08:54:33.488991 139762606724864 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7354046106338501, loss=1.2170605659484863
I0315 08:55:48.796746 139762598332160 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.6402899622917175, loss=1.2231197357177734
I0315 08:57:03.984743 139762606724864 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.6632535457611084, loss=1.2181694507598877
I0315 08:58:19.284595 139762598332160 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.6137482523918152, loss=1.2292567491531372
I0315 08:59:34.508316 139762606724864 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.5866798162460327, loss=1.2283735275268555
I0315 09:00:49.809492 139762598332160 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.6138798594474792, loss=1.2613651752471924
I0315 09:02:05.344422 139762606724864 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.6175026893615723, loss=1.278038740158081
I0315 09:03:29.693205 139762598332160 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.6254917979240417, loss=1.247844934463501
I0315 09:04:54.302343 139762606724864 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.6655014157295227, loss=1.2249451875686646
I0315 09:06:16.869189 139762598332160 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.6511422395706177, loss=1.3110897541046143
I0315 09:07:39.213466 139762606724864 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.6598517298698425, loss=1.218307614326477
I0315 09:08:59.787181 139762279044864 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.6480090618133545, loss=1.2530429363250732
I0315 09:10:15.102362 139762270652160 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.6683815121650696, loss=1.2138912677764893
I0315 09:11:30.349979 139762279044864 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.6799520254135132, loss=1.223334550857544
I0315 09:12:45.640864 139762270652160 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.6493649482727051, loss=1.2311614751815796
I0315 09:14:00.919276 139762279044864 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.6698759198188782, loss=1.239768147468567
I0315 09:15:16.360556 139762270652160 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.6993141174316406, loss=1.2133347988128662
I0315 09:15:47.465085 139935937595200 spec.py:298] Evaluating on the training split.
I0315 09:16:25.089738 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 09:17:03.308216 139935937595200 spec.py:326] Evaluating on the test split.
I0315 09:17:23.265389 139935937595200 submission_runner.py:362] Time since start: 37564.41s, 	Step: 45941, 	{'train/ctc_loss': DeviceArray(0.17499085, dtype=float32), 'train/wer': 0.06643887425337124, 'validation/ctc_loss': DeviceArray(0.43550894, dtype=float32), 'validation/wer': 0.1286071259732366, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.25229603, dtype=float32), 'test/wer': 0.08258688278187394, 'test/num_examples': 2472}
I0315 09:17:23.288400 139762975364864 logging_writer.py:48] [45941] global_step=45941, preemption_count=0, score=35953.362479, test/ctc_loss=0.2522960305213928, test/num_examples=2472, test/wer=0.082587, total_duration=37564.410295, train/ctc_loss=0.17499084770679474, train/wer=0.066439, validation/ctc_loss=0.43550893664360046, validation/num_examples=5348, validation/wer=0.128607
I0315 09:17:23.640504 139935937595200 checkpoints.py:356] Saving checkpoint at step: 45941
I0315 09:17:25.070577 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_45941
I0315 09:17:25.103391 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_45941.
I0315 09:18:10.262347 139762966972160 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.16778564453125, loss=1.2706420421600342
I0315 09:19:25.723197 139762807510784 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.7032504081726074, loss=1.2082501649856567
I0315 09:20:41.090940 139762966972160 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.5631094574928284, loss=1.212090015411377
I0315 09:21:56.477116 139762807510784 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.6870918869972229, loss=1.2286430597305298
I0315 09:23:15.660317 139762975364864 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.5945881009101868, loss=1.2347782850265503
I0315 09:24:31.342904 139762966972160 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.6522995233535767, loss=1.2425721883773804
I0315 09:25:46.721920 139762975364864 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.7356184720993042, loss=1.2734153270721436
I0315 09:27:01.899366 139762966972160 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.6514320373535156, loss=1.1994415521621704
I0315 09:28:17.164928 139762975364864 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.7895286679267883, loss=1.2393523454666138
I0315 09:29:32.374201 139762966972160 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.8121371865272522, loss=1.2300407886505127
I0315 09:30:53.625197 139762975364864 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.7066125273704529, loss=1.2366611957550049
I0315 09:32:15.412787 139762966972160 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.6758121848106384, loss=1.2686598300933838
I0315 09:33:35.759736 139762975364864 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.6837364435195923, loss=1.2667866945266724
I0315 09:34:59.368837 139762966972160 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.8291224837303162, loss=1.292494297027588
I0315 09:36:26.119226 139762975364864 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.6557786464691162, loss=1.2205889225006104
I0315 09:37:41.367012 139762966972160 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.6406429409980774, loss=1.2080714702606201
I0315 09:38:56.696454 139762975364864 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.7239134311676025, loss=1.203236699104309
I0315 09:40:11.944916 139762966972160 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.6203817129135132, loss=1.1806882619857788
I0315 09:41:27.365914 139762975364864 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.7252294421195984, loss=1.2048486471176147
I0315 09:42:42.962380 139762966972160 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.6598433256149292, loss=1.2126357555389404
I0315 09:44:05.228992 139762975364864 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.778290331363678, loss=1.1922430992126465
I0315 09:45:25.114616 139762966972160 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.6723396182060242, loss=1.2582846879959106
I0315 09:46:47.058032 139762975364864 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.7071161866188049, loss=1.274350881576538
I0315 09:48:07.094252 139762966972160 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.7508466243743896, loss=1.2400319576263428
I0315 09:49:31.575243 139762975364864 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.8897664546966553, loss=1.24740731716156
I0315 09:50:51.794418 139762320004864 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.7841094136238098, loss=1.207385540008545
I0315 09:52:07.264255 139762311612160 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.7972633838653564, loss=1.2557200193405151
I0315 09:53:22.947850 139762320004864 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.5918546915054321, loss=1.1521741151809692
I0315 09:54:38.511353 139762311612160 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.9789376854896545, loss=1.1824861764907837
I0315 09:55:54.104425 139762320004864 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.7135555148124695, loss=1.2415597438812256
I0315 09:57:10.846317 139762311612160 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.6037005186080933, loss=1.2543890476226807
I0315 09:57:25.673224 139935937595200 spec.py:298] Evaluating on the training split.
I0315 09:58:03.834745 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 09:58:42.552913 139935937595200 spec.py:326] Evaluating on the test split.
I0315 09:59:02.969856 139935937595200 submission_runner.py:362] Time since start: 40062.62s, 	Step: 49020, 	{'train/ctc_loss': DeviceArray(0.16939811, dtype=float32), 'train/wer': 0.06155710793589101, 'validation/ctc_loss': DeviceArray(0.41998357, dtype=float32), 'validation/wer': 0.12394716784532413, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.24302162, dtype=float32), 'test/wer': 0.08077915219466618, 'test/num_examples': 2472}
I0315 09:59:02.993134 139763405444864 logging_writer.py:48] [49020] global_step=49020, preemption_count=0, score=38346.243358, test/ctc_loss=0.243021622300148, test/num_examples=2472, test/wer=0.080779, total_duration=40062.618440, train/ctc_loss=0.16939811408519745, train/wer=0.061557, validation/ctc_loss=0.41998356580734253, validation/num_examples=5348, validation/wer=0.123947
I0315 09:59:03.301300 139935937595200 checkpoints.py:356] Saving checkpoint at step: 49020
I0315 09:59:04.725216 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_49020
I0315 09:59:04.758665 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_49020.
I0315 10:00:05.795706 139763397052160 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.6224197149276733, loss=1.2524484395980835
I0315 10:01:21.309609 139763229198080 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.7195361852645874, loss=1.2045679092407227
I0315 10:02:36.566202 139763397052160 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.703154981136322, loss=1.2221277952194214
I0315 10:03:51.846947 139763229198080 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.6710312962532043, loss=1.2312506437301636
I0315 10:05:12.087172 139763077764864 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.8441287875175476, loss=1.2081366777420044
I0315 10:06:27.393295 139763069372160 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.6544071435928345, loss=1.2707239389419556
I0315 10:07:42.814697 139763077764864 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.7063735127449036, loss=1.1857283115386963
I0315 10:08:58.123759 139763069372160 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.6806904673576355, loss=1.2249844074249268
I0315 10:10:13.434241 139763077764864 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.7538400292396545, loss=1.2555458545684814
I0315 10:11:28.771808 139763069372160 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.6569923162460327, loss=1.220755934715271
I0315 10:12:44.375211 139763077764864 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.7015714049339294, loss=1.1962162256240845
I0315 10:14:00.784996 139763069372160 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.6751113533973694, loss=1.206362009048462
I0315 10:15:20.903027 139763077764864 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.6392275094985962, loss=1.2052252292633057
I0315 10:16:42.023900 139763069372160 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.7667928338050842, loss=1.2330467700958252
I0315 10:18:03.137163 139763077764864 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.6224197745323181, loss=1.1909692287445068
I0315 10:19:18.533921 139763069372160 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.7293419241905212, loss=1.2140578031539917
I0315 10:20:33.814723 139763077764864 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.7374354600906372, loss=1.215657114982605
I0315 10:21:49.238192 139763069372160 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.6761595010757446, loss=1.2053627967834473
I0315 10:23:04.595582 139763077764864 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.7203541994094849, loss=1.2202177047729492
I0315 10:24:19.853933 139763069372160 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.6431577205657959, loss=1.2282681465148926
I0315 10:25:35.929049 139763077764864 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.7284479737281799, loss=1.264219045639038
I0315 10:26:58.072698 139763069372160 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.6677453517913818, loss=1.2162691354751587
I0315 10:28:22.790429 139763077764864 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.6666601896286011, loss=1.2006113529205322
I0315 10:29:47.420705 139763069372160 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.9248394966125488, loss=1.147842526435852
I0315 10:31:14.181675 139763405444864 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.7559510469436646, loss=1.1566290855407715
I0315 10:32:29.751843 139763397052160 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.7144466638565063, loss=1.220547080039978
I0315 10:33:45.102566 139763405444864 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.666098952293396, loss=1.2120362520217896
I0315 10:35:00.475650 139763397052160 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.7151912450790405, loss=1.1816482543945312
I0315 10:36:15.811758 139763405444864 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.789897620677948, loss=1.1918126344680786
I0315 10:37:31.515714 139763397052160 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.7153645753860474, loss=1.208694577217102
I0315 10:38:46.932816 139763405444864 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.6331089735031128, loss=1.1654127836227417
I0315 10:39:05.406348 139935937595200 spec.py:298] Evaluating on the training split.
I0315 10:39:42.312134 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 10:40:20.177071 139935937595200 spec.py:326] Evaluating on the test split.
I0315 10:40:39.768254 139935937595200 submission_runner.py:362] Time since start: 42562.35s, 	Step: 52126, 	{'train/ctc_loss': DeviceArray(0.17175573, dtype=float32), 'train/wer': 0.06300377015080603, 'validation/ctc_loss': DeviceArray(0.40576738, dtype=float32), 'validation/wer': 0.12082123320051327, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.23534396, dtype=float32), 'test/wer': 0.07598561940162087, 'test/num_examples': 2472}
I0315 10:40:39.790866 139763405444864 logging_writer.py:48] [52126] global_step=52126, preemption_count=0, score=40739.105608, test/ctc_loss=0.23534396290779114, test/num_examples=2472, test/wer=0.075986, total_duration=42562.351561, train/ctc_loss=0.17175573110580444, train/wer=0.063004, validation/ctc_loss=0.40576738119125366, validation/num_examples=5348, validation/wer=0.120821
I0315 10:40:40.109829 139935937595200 checkpoints.py:356] Saving checkpoint at step: 52126
I0315 10:40:41.559007 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_52126
I0315 10:40:41.591759 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_52126.
I0315 10:41:38.272494 139763397052160 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.6203897595405579, loss=1.1664555072784424
I0315 10:42:53.572208 139759863191296 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.6989226341247559, loss=1.1651338338851929
I0315 10:44:09.073447 139763397052160 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.7853530049324036, loss=1.191410779953003
I0315 10:45:24.754020 139759863191296 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.8473355174064636, loss=1.1979455947875977
I0315 10:46:43.965833 139763077764864 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.6475457549095154, loss=1.1979304552078247
I0315 10:47:59.353935 139763069372160 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.732843816280365, loss=1.1727789640426636
I0315 10:49:14.965447 139763077764864 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.6778075695037842, loss=1.2250183820724487
I0315 10:50:30.736045 139763069372160 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.6770405769348145, loss=1.1812902688980103
I0315 10:51:45.963598 139763077764864 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.6961882710456848, loss=1.1688052415847778
I0315 10:53:01.382854 139763069372160 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.910008430480957, loss=1.158570647239685
I0315 10:54:17.105515 139763077764864 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.6663814783096313, loss=1.1908422708511353
I0315 10:55:36.291177 139763069372160 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.6934882402420044, loss=1.2472513914108276
I0315 10:56:53.320374 139763077764864 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.7275748252868652, loss=1.2001593112945557
I0315 10:58:14.005132 139763069372160 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.7406517267227173, loss=1.1976991891860962
I0315 10:59:35.693832 139763077764864 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.8722309470176697, loss=1.1195077896118164
I0315 11:00:51.105356 139763069372160 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.7219246029853821, loss=1.1707292795181274
I0315 11:02:06.409214 139763077764864 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.7314972281455994, loss=1.2130881547927856
I0315 11:03:21.615059 139763069372160 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.8528446555137634, loss=1.1571389436721802
I0315 11:04:36.944089 139763077764864 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.6884963512420654, loss=1.1434094905853271
I0315 11:05:52.196122 139763069372160 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.7609233260154724, loss=1.1935325860977173
I0315 11:07:07.707395 139763077764864 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.658454179763794, loss=1.1751821041107178
I0315 11:08:23.178514 139763069372160 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.6715742945671082, loss=1.1180672645568848
I0315 11:09:43.402192 139763077764864 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.789721667766571, loss=1.1796557903289795
I0315 11:11:04.822379 139763069372160 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.6949118971824646, loss=1.1443824768066406
I0315 11:12:29.174097 139763405444864 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.6397036910057068, loss=1.1128231287002563
I0315 11:13:44.445183 139763397052160 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.7508249878883362, loss=1.23777437210083
I0315 11:14:59.721225 139763405444864 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.6749017238616943, loss=1.193144679069519
I0315 11:16:14.909322 139763397052160 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.7408881187438965, loss=1.1907057762145996
I0315 11:17:30.134821 139763405444864 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.7190777063369751, loss=1.1325792074203491
I0315 11:18:45.301207 139763397052160 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.6445245146751404, loss=1.131351113319397
I0315 11:20:04.889246 139763405444864 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.7245727777481079, loss=1.1859608888626099
I0315 11:20:41.672741 139935937595200 spec.py:298] Evaluating on the training split.
I0315 11:21:18.631572 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 11:21:57.820829 139935937595200 spec.py:326] Evaluating on the test split.
I0315 11:22:17.503017 139935937595200 submission_runner.py:362] Time since start: 45058.62s, 	Step: 55249, 	{'train/ctc_loss': DeviceArray(0.15877353, dtype=float32), 'train/wer': 0.056869543513529644, 'validation/ctc_loss': DeviceArray(0.39962924, dtype=float32), 'validation/wer': 0.11635423400129283, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.22373688, dtype=float32), 'test/wer': 0.07330449089025654, 'test/num_examples': 2472}
I0315 11:22:17.526853 139763405444864 logging_writer.py:48] [55249] global_step=55249, preemption_count=0, score=43131.312398, test/ctc_loss=0.22373688220977783, test/num_examples=2472, test/wer=0.073304, total_duration=45058.617962, train/ctc_loss=0.1587735265493393, train/wer=0.056870, validation/ctc_loss=0.39962923526763916, validation/num_examples=5348, validation/wer=0.116354
I0315 11:22:17.802103 139935937595200 checkpoints.py:356] Saving checkpoint at step: 55249
I0315 11:22:19.245130 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_55249
I0315 11:22:19.278075 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_55249.
I0315 11:22:58.315809 139763397052160 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.7671008110046387, loss=1.151273250579834
I0315 11:24:13.634293 139761870972672 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.7875968813896179, loss=1.2088134288787842
I0315 11:25:28.953525 139763397052160 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.6987323760986328, loss=1.1742366552352905
I0315 11:26:44.241549 139761870972672 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.7305948138237, loss=1.183845043182373
I0315 11:28:03.128037 139763077764864 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.7899544835090637, loss=1.1880797147750854
I0315 11:29:18.574079 139763069372160 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.771105170249939, loss=1.1878564357757568
I0315 11:30:33.931740 139763077764864 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.7345860004425049, loss=1.1856796741485596
I0315 11:31:49.213353 139763069372160 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.7282022833824158, loss=1.1987690925598145
I0315 11:33:04.409422 139763077764864 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.7354638576507568, loss=1.1912806034088135
I0315 11:34:19.813976 139763069372160 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.837607204914093, loss=1.1401416063308716
I0315 11:35:37.802631 139763077764864 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.8002785444259644, loss=1.157625436782837
I0315 11:36:56.982062 139763069372160 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.8062671422958374, loss=1.2062559127807617
I0315 11:38:17.171275 139763077764864 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.8072406053543091, loss=1.1529772281646729
I0315 11:39:35.984380 139763069372160 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.7627866268157959, loss=1.1803126335144043
I0315 11:40:56.438463 139763405444864 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.6738582849502563, loss=1.1115806102752686
I0315 11:42:11.699485 139763397052160 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.6763788461685181, loss=1.1593806743621826
I0315 11:43:27.238835 139763405444864 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.7606918811798096, loss=1.1408849954605103
I0315 11:44:42.686292 139763397052160 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.7450945377349854, loss=1.2163983583450317
I0315 11:45:58.062462 139763405444864 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.7301033139228821, loss=1.0992138385772705
I0315 11:47:13.474890 139763397052160 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.6406708359718323, loss=1.1581090688705444
I0315 11:48:28.780489 139763405444864 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.790060818195343, loss=1.2016322612762451
I0315 11:49:43.993144 139763397052160 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.6814708709716797, loss=1.1840192079544067
I0315 11:50:59.364455 139763405444864 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.6755589246749878, loss=1.1529980897903442
I0315 11:52:18.927945 139763397052160 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.730766773223877, loss=1.198485255241394
I0315 11:53:37.792201 139763405444864 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.734889030456543, loss=1.0920714139938354
I0315 11:54:53.066208 139763397052160 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.7155724167823792, loss=1.1739566326141357
I0315 11:56:08.357449 139763405444864 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.8542065024375916, loss=1.1594270467758179
I0315 11:57:23.528507 139763397052160 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.6996989846229553, loss=1.1700446605682373
I0315 11:58:38.734876 139763405444864 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.7123206853866577, loss=1.1702078580856323
I0315 11:59:54.050696 139763397052160 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.7507434487342834, loss=1.0926177501678467
I0315 12:01:09.387015 139763405444864 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.7095531225204468, loss=1.1822277307510376
I0315 12:02:19.978009 139935937595200 spec.py:298] Evaluating on the training split.
I0315 12:02:57.512213 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 12:03:36.001855 139935937595200 spec.py:326] Evaluating on the test split.
I0315 12:03:56.679395 139935937595200 submission_runner.py:362] Time since start: 47556.92s, 	Step: 58395, 	{'train/ctc_loss': DeviceArray(0.16121131, dtype=float32), 'train/wer': 0.05815512485295553, 'validation/ctc_loss': DeviceArray(0.38644284, dtype=float32), 'validation/wer': 0.11311252399926676, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.21834558, dtype=float32), 'test/wer': 0.07176081083825889, 'test/num_examples': 2472}
I0315 12:03:56.702570 139763405444864 logging_writer.py:48] [58395] global_step=58395, preemption_count=0, score=45524.081457, test/ctc_loss=0.21834558248519897, test/num_examples=2472, test/wer=0.071761, total_duration=47556.923217, train/ctc_loss=0.1612113118171692, train/wer=0.058155, validation/ctc_loss=0.3864428400993347, validation/num_examples=5348, validation/wer=0.113113
I0315 12:03:57.020243 139935937595200 checkpoints.py:356] Saving checkpoint at step: 58395
I0315 12:03:58.457573 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_58395
I0315 12:03:58.490371 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_58395.
I0315 12:04:03.074986 139763397052160 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.8005772829055786, loss=1.0893845558166504
I0315 12:05:18.240100 139762876339968 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.7595943808555603, loss=1.1838165521621704
I0315 12:06:33.688563 139763397052160 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.8276016116142273, loss=1.15999174118042
I0315 12:07:49.045986 139762876339968 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.8032686114311218, loss=1.1237651109695435
I0315 12:09:07.968550 139762422404864 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.7764284610748291, loss=1.1457457542419434
I0315 12:10:23.254250 139762414012160 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.7178473472595215, loss=1.1552350521087646
I0315 12:11:38.475006 139762422404864 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.7074490189552307, loss=1.151347279548645
I0315 12:12:53.672342 139762414012160 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.7326253056526184, loss=1.1243821382522583
I0315 12:14:08.860508 139762422404864 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.7684739828109741, loss=1.1144332885742188
I0315 12:15:24.079688 139762414012160 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.6578254103660583, loss=1.1094480752944946
I0315 12:16:41.922908 139762422404864 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.7028366327285767, loss=1.1541390419006348
I0315 12:18:05.602666 139762414012160 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.7717586755752563, loss=1.1410620212554932
I0315 12:19:27.109457 139762422404864 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.6913576722145081, loss=1.1207777261734009
I0315 12:20:46.893708 139762414012160 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.7579480409622192, loss=1.133738398551941
I0315 12:22:07.047764 139763405444864 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.8289104700088501, loss=1.1320148706436157
I0315 12:23:22.632626 139763397052160 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.7540714144706726, loss=1.1762382984161377
I0315 12:24:38.070133 139763405444864 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.8089404106140137, loss=1.1531654596328735
I0315 12:25:53.462309 139763397052160 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.6481017470359802, loss=1.179707407951355
I0315 12:27:08.893704 139763405444864 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.6869004964828491, loss=1.125171422958374
I0315 12:28:24.536142 139763397052160 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.7261850833892822, loss=1.1935874223709106
I0315 12:29:41.321036 139763405444864 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.7491284012794495, loss=1.1930198669433594
I0315 12:31:04.945652 139763397052160 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.702509880065918, loss=1.1424864530563354
I0315 12:32:26.550803 139763405444864 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.6938199996948242, loss=1.08476722240448
I0315 12:33:49.389184 139763397052160 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.6882054805755615, loss=1.1023362874984741
I0315 12:35:14.333669 139763405444864 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.8236554861068726, loss=1.1361666917800903
I0315 12:36:29.924415 139763397052160 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.7750316858291626, loss=1.2520740032196045
I0315 12:37:45.380647 139763405444864 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.7357039451599121, loss=1.0257726907730103
I0315 12:39:00.596450 139763397052160 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.7426725029945374, loss=1.0979217290878296
I0315 12:40:15.911770 139763405444864 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.708345353603363, loss=1.1099308729171753
I0315 12:41:31.215105 139763397052160 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.7104350924491882, loss=1.178714394569397
I0315 12:42:53.869503 139763405444864 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.7341716289520264, loss=1.1223703622817993
I0315 12:43:59.017940 139935937595200 spec.py:298] Evaluating on the training split.
I0315 12:44:36.350065 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 12:45:14.943782 139935937595200 spec.py:326] Evaluating on the test split.
I0315 12:45:35.720127 139935937595200 submission_runner.py:362] Time since start: 50055.96s, 	Step: 61484, 	{'train/ctc_loss': DeviceArray(0.15064636, dtype=float32), 'train/wer': 0.05401920351247376, 'validation/ctc_loss': DeviceArray(0.3754372, dtype=float32), 'validation/wer': 0.11098997578365445, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.21323049, dtype=float32), 'test/wer': 0.07072492027704995, 'test/num_examples': 2472}
I0315 12:45:35.744501 139762821764864 logging_writer.py:48] [61484] global_step=61484, preemption_count=0, score=47916.900683, test/ctc_loss=0.21323049068450928, test/num_examples=2472, test/wer=0.070725, total_duration=50055.963148, train/ctc_loss=0.1506463587284088, train/wer=0.054019, validation/ctc_loss=0.3754372000694275, validation/num_examples=5348, validation/wer=0.110990
I0315 12:45:36.047901 139935937595200 checkpoints.py:356] Saving checkpoint at step: 61484
I0315 12:45:37.496903 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_61484
I0315 12:45:37.529651 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_61484.
I0315 12:45:50.386283 139762813372160 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.6940805315971375, loss=1.1250793933868408
I0315 12:47:06.005270 139761737008896 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.8186877965927124, loss=1.11903977394104
I0315 12:48:21.562781 139762813372160 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.7100554704666138, loss=1.1636565923690796
I0315 12:49:40.583992 139762494084864 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.7676438689231873, loss=1.0625479221343994
I0315 12:50:56.139074 139762485692160 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.8670387268066406, loss=1.10287344455719
I0315 12:52:11.688545 139762494084864 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.7897122502326965, loss=1.1134223937988281
I0315 12:53:26.919066 139762485692160 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.7120180726051331, loss=1.1064140796661377
I0315 12:54:42.280762 139762494084864 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.7364071011543274, loss=1.0740547180175781
I0315 12:55:57.862851 139762485692160 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.8043040633201599, loss=1.1835601329803467
I0315 12:57:13.348897 139762494084864 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.8056643605232239, loss=1.1477885246276855
I0315 12:58:28.945904 139762485692160 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.6432014107704163, loss=1.112170696258545
I0315 12:59:44.442338 139762494084864 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.7449563145637512, loss=1.1333717107772827
I0315 13:01:01.874390 139762485692160 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.785485565662384, loss=1.1124792098999023
I0315 13:02:21.794327 139762494084864 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.6724450588226318, loss=1.1406230926513672
I0315 13:03:41.972834 139762494084864 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.7260140180587769, loss=1.092916488647461
I0315 13:04:57.450655 139762485692160 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.7131635546684265, loss=1.1423157453536987
I0315 13:06:12.660369 139762494084864 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.7038417458534241, loss=1.1633484363555908
I0315 13:07:27.952291 139762485692160 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.7407668232917786, loss=1.0858287811279297
I0315 13:08:43.246182 139762494084864 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.7684256434440613, loss=1.152540922164917
I0315 13:09:58.495034 139762485692160 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.8555775284767151, loss=1.1355655193328857
I0315 13:11:13.701540 139762494084864 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.8288024663925171, loss=1.1396304368972778
I0315 13:12:29.227650 139762485692160 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.7861253619194031, loss=1.1295669078826904
I0315 13:13:44.514427 139762494084864 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.7957046627998352, loss=1.1157302856445312
I0315 13:14:59.767275 139762485692160 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.8180575370788574, loss=1.1131511926651
I0315 13:16:18.446962 139762494084864 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.828971266746521, loss=1.0975477695465088
I0315 13:17:34.045782 139762485692160 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.8385133743286133, loss=1.042283535003662
I0315 13:18:49.551333 139762494084864 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.7406381964683533, loss=1.1038720607757568
I0315 13:20:04.809885 139762485692160 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.675033450126648, loss=1.0890536308288574
I0315 13:21:20.154900 139762494084864 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.7914105653762817, loss=1.1103146076202393
I0315 13:22:35.422180 139762485692160 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.8397702574729919, loss=1.1266859769821167
I0315 13:23:50.801996 139762494084864 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.824337363243103, loss=1.1879469156265259
I0315 13:25:06.155856 139762485692160 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.7721149921417236, loss=1.0686182975769043
I0315 13:25:38.198835 139935937595200 spec.py:298] Evaluating on the training split.
I0315 13:26:16.085677 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 13:26:53.625172 139935937595200 spec.py:326] Evaluating on the test split.
I0315 13:27:13.362222 139935937595200 submission_runner.py:362] Time since start: 52555.14s, 	Step: 64644, 	{'train/ctc_loss': DeviceArray(0.10945161, dtype=float32), 'train/wer': 0.04149626395252298, 'validation/ctc_loss': DeviceArray(0.36210132, dtype=float32), 'validation/wer': 0.10688959854894886, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.20593657, dtype=float32), 'test/wer': 0.06658135803221417, 'test/num_examples': 2472}
I0315 13:27:13.386708 139763405444864 logging_writer.py:48] [64644] global_step=64644, preemption_count=0, score=50309.630600, test/ctc_loss=0.20593656599521637, test/num_examples=2472, test/wer=0.066581, total_duration=52555.144044, train/ctc_loss=0.10945161432027817, train/wer=0.041496, validation/ctc_loss=0.36210131645202637, validation/num_examples=5348, validation/wer=0.106890
I0315 13:27:13.679573 139935937595200 checkpoints.py:356] Saving checkpoint at step: 64644
I0315 13:27:15.080675 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_64644
I0315 13:27:15.113274 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_64644.
I0315 13:27:57.971733 139763397052160 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.777170717716217, loss=1.0755259990692139
I0315 13:29:13.596211 139763187234560 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.7482589483261108, loss=1.1200658082962036
I0315 13:30:32.536384 139762535044864 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.7711068391799927, loss=1.0578968524932861
I0315 13:31:47.827639 139762526652160 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.7247787714004517, loss=1.060495138168335
I0315 13:33:03.453995 139762535044864 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.7479491233825684, loss=1.090590476989746
I0315 13:34:19.080832 139762526652160 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.7880668044090271, loss=1.0903561115264893
I0315 13:35:34.834528 139762535044864 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.7626528143882751, loss=1.0718648433685303
I0315 13:36:50.224324 139762526652160 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.7858970165252686, loss=1.0760352611541748
I0315 13:38:05.703046 139762535044864 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.7755863666534424, loss=1.0425004959106445
I0315 13:39:21.357197 139762526652160 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.7554031014442444, loss=1.1114580631256104
I0315 13:40:37.174965 139762535044864 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.8118681907653809, loss=1.143993616104126
I0315 13:41:52.697151 139762526652160 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.7333172559738159, loss=1.0504015684127808
I0315 13:43:11.837302 139762535044864 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.9132401347160339, loss=1.0985352993011475
I0315 13:44:31.202702 139763405444864 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.7384478449821472, loss=1.0934752225875854
I0315 13:45:46.522316 139763397052160 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.8153586983680725, loss=1.0628342628479004
I0315 13:47:01.930329 139763405444864 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.8078883290290833, loss=1.1016690731048584
I0315 13:48:17.228811 139763397052160 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.7671289443969727, loss=1.070310354232788
I0315 13:49:32.522360 139763405444864 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.7586938738822937, loss=1.0968644618988037
I0315 13:50:48.420941 139763397052160 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.8285040855407715, loss=1.094866156578064
I0315 13:52:07.476279 139763405444864 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.6847689151763916, loss=1.0605559349060059
I0315 13:53:28.790809 139763397052160 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.8123877048492432, loss=1.1011251211166382
I0315 13:54:52.183875 139763405444864 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.9450734257698059, loss=1.1141555309295654
I0315 13:56:13.552083 139763397052160 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.8662826418876648, loss=1.0887668132781982
I0315 13:57:34.105710 139763405444864 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.7694188356399536, loss=1.0059369802474976
I0315 13:58:49.285702 139763397052160 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.7794932126998901, loss=1.0893763303756714
I0315 14:00:04.812823 139763405444864 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.7468897104263306, loss=1.0417828559875488
I0315 14:01:20.106844 139763397052160 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.8173595666885376, loss=1.0535458326339722
I0315 14:02:35.505686 139763405444864 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.8079189658164978, loss=1.0805305242538452
I0315 14:03:50.756435 139763397052160 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.76289963722229, loss=1.0886788368225098
I0315 14:05:12.845536 139763405444864 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.8128430247306824, loss=1.016122817993164
I0315 14:06:36.524223 139763397052160 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.8664553165435791, loss=1.0801125764846802
I0315 14:07:15.744899 139935937595200 spec.py:298] Evaluating on the training split.
I0315 14:07:53.319794 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 14:08:32.232049 139935937595200 spec.py:326] Evaluating on the test split.
I0315 14:08:52.181340 139935937595200 submission_runner.py:362] Time since start: 55052.69s, 	Step: 67749, 	{'train/ctc_loss': DeviceArray(0.1206243, dtype=float32), 'train/wer': 0.04498849021139997, 'validation/ctc_loss': DeviceArray(0.35496044, dtype=float32), 'validation/wer': 0.10332950631458095, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.19711342, dtype=float32), 'test/wer': 0.063758048463429, 'test/num_examples': 2472}
I0315 14:08:52.205604 139762898564864 logging_writer.py:48] [67749] global_step=67749, preemption_count=0, score=52702.531948, test/ctc_loss=0.19711342453956604, test/num_examples=2472, test/wer=0.063758, total_duration=55052.690098, train/ctc_loss=0.12062429636716843, train/wer=0.044988, validation/ctc_loss=0.35496044158935547, validation/num_examples=5348, validation/wer=0.103330
I0315 14:08:52.521261 139935937595200 checkpoints.py:356] Saving checkpoint at step: 67749
I0315 14:08:53.942298 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_67749
I0315 14:08:53.975380 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_67749.
I0315 14:09:33.137273 139762890172160 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.7298877835273743, loss=1.059210181236267
I0315 14:10:48.585498 139761464223488 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.7379900813102722, loss=1.0493204593658447
I0315 14:12:07.516831 139762570884864 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.8142510652542114, loss=1.061057686805725
I0315 14:13:22.703306 139762562492160 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.8404161930084229, loss=1.1014524698257446
I0315 14:14:38.072292 139762570884864 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.0315804481506348, loss=1.0518791675567627
I0315 14:15:53.480845 139762562492160 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.7842280268669128, loss=1.075535535812378
I0315 14:17:08.969485 139762570884864 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.8026701807975769, loss=1.0741074085235596
I0315 14:18:24.603373 139762562492160 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.8373569846153259, loss=1.0681706666946411
I0315 14:19:44.663307 139762570884864 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.8127288818359375, loss=1.0614405870437622
I0315 14:21:10.131033 139762562492160 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.8265715837478638, loss=1.1240133047103882
I0315 14:22:34.500075 139762570884864 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.9473592042922974, loss=1.1068408489227295
I0315 14:23:55.830940 139762562492160 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.849203884601593, loss=1.0692412853240967
I0315 14:25:20.915050 139762570884864 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.8540439009666443, loss=1.074949026107788
I0315 14:26:40.152350 139762570884864 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.0027852058410645, loss=1.0998507738113403
I0315 14:27:55.606364 139762562492160 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.844158947467804, loss=1.089594841003418
I0315 14:29:10.892546 139762570884864 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.8695551753044128, loss=1.01982581615448
I0315 14:30:26.270149 139762562492160 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.8211932182312012, loss=1.0785102844238281
I0315 14:31:41.475630 139762570884864 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.7799512147903442, loss=1.0412609577178955
I0315 14:32:56.704945 139762562492160 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.7772883772850037, loss=1.0994163751602173
I0315 14:34:14.049455 139762570884864 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.73948734998703, loss=1.0892157554626465
I0315 14:35:36.242658 139762562492160 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.7732030153274536, loss=1.0377979278564453
I0315 14:36:59.200514 139762570884864 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.8054739832878113, loss=1.0906322002410889
I0315 14:38:20.113980 139762562492160 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.7973310947418213, loss=1.0250658988952637
I0315 14:39:40.420676 139762898564864 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.8182645440101624, loss=1.046076774597168
I0315 14:40:55.977871 139762890172160 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.9940683245658875, loss=1.090660572052002
I0315 14:42:11.230355 139762898564864 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.9765859246253967, loss=1.0512070655822754
I0315 14:43:26.475611 139762890172160 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.8035393953323364, loss=1.0340999364852905
I0315 14:44:41.836710 139762898564864 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.8427820205688477, loss=1.087546944618225
I0315 14:45:57.150661 139762890172160 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.7633615732192993, loss=1.0385342836380005
I0315 14:47:12.530651 139762898564864 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.8848724961280823, loss=1.0453269481658936
I0315 14:48:28.136902 139762890172160 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.7371374368667603, loss=1.0459884405136108
I0315 14:48:54.472814 139935937595200 spec.py:298] Evaluating on the training split.
I0315 14:49:31.611115 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 14:50:10.017435 139935937595200 spec.py:326] Evaluating on the test split.
I0315 14:50:29.710220 139935937595200 submission_runner.py:362] Time since start: 57551.42s, 	Step: 70836, 	{'train/ctc_loss': DeviceArray(0.1393159, dtype=float32), 'train/wer': 0.052700379143320514, 'validation/ctc_loss': DeviceArray(0.34659326, dtype=float32), 'validation/wer': 0.10038688265202751, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.18663132, dtype=float32), 'test/wer': 0.06209249893364207, 'test/num_examples': 2472}
I0315 14:50:29.734260 139762898564864 logging_writer.py:48] [70836] global_step=70836, preemption_count=0, score=55095.378886, test/ctc_loss=0.18663132190704346, test/num_examples=2472, test/wer=0.062092, total_duration=57551.418034, train/ctc_loss=0.1393159031867981, train/wer=0.052700, validation/ctc_loss=0.3465932607650757, validation/num_examples=5348, validation/wer=0.100387
I0315 14:50:30.062736 139935937595200 checkpoints.py:356] Saving checkpoint at step: 70836
I0315 14:50:31.488369 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_70836
I0315 14:50:31.521489 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_70836.
I0315 14:51:20.402999 139762890172160 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.7853304147720337, loss=1.0427377223968506
I0315 14:52:36.019957 139761499059968 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.7813212871551514, loss=1.021931529045105
I0315 14:53:54.935966 139762898564864 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.9186276197433472, loss=1.0620431900024414
I0315 14:55:10.476665 139762890172160 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.9131854176521301, loss=1.0676320791244507
I0315 14:56:25.803484 139762898564864 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.8317633271217346, loss=1.0356107950210571
I0315 14:57:41.396448 139762890172160 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.8435575366020203, loss=1.0645827054977417
I0315 14:58:56.861888 139762898564864 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.8380751013755798, loss=1.0081186294555664
I0315 15:00:12.177702 139762890172160 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.8827193975448608, loss=1.0064465999603271
I0315 15:01:27.961077 139762898564864 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.9600827097892761, loss=1.069916009902954
I0315 15:02:43.833544 139762890172160 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.8527212142944336, loss=1.0315196514129639
I0315 15:03:59.313711 139762898564864 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.8529916405677795, loss=1.0295921564102173
I0315 15:05:14.777646 139762890172160 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.8096629977226257, loss=1.0224425792694092
I0315 15:06:35.099182 139762898564864 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.8593165278434753, loss=1.0258597135543823
I0315 15:07:50.430364 139762890172160 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.7923817038536072, loss=1.0060935020446777
I0315 15:09:05.810657 139762898564864 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.8168502449989319, loss=1.0302983522415161
I0315 15:10:21.275855 139762890172160 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.79913729429245, loss=1.0038244724273682
I0315 15:11:36.785582 139762898564864 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.8437034487724304, loss=0.9934873580932617
I0315 15:12:52.205983 139762890172160 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.763754665851593, loss=1.043764352798462
I0315 15:14:07.549335 139762898564864 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.852759599685669, loss=1.05026376247406
I0315 15:15:23.009191 139762890172160 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.8450325727462769, loss=1.060468077659607
I0315 15:16:40.999991 139762898564864 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.8461881875991821, loss=1.0290374755859375
I0315 15:18:03.286775 139762890172160 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.9841904640197754, loss=1.0124763250350952
I0315 15:19:25.062859 139762898564864 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.9015814661979675, loss=1.036924123764038
I0315 15:20:45.933887 139762898564864 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.8867864012718201, loss=1.005162239074707
I0315 15:22:01.730681 139762890172160 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.8920613527297974, loss=0.9626790285110474
I0315 15:23:17.228733 139762898564864 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.8160350918769836, loss=1.0320658683776855
I0315 15:24:32.500895 139762890172160 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.8198484182357788, loss=0.9928945302963257
I0315 15:25:47.755928 139762898564864 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.8461523652076721, loss=1.0352360010147095
I0315 15:27:03.099606 139762890172160 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.8630698919296265, loss=1.019270658493042
I0315 15:28:18.423351 139762898564864 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.7978518605232239, loss=1.025349497795105
I0315 15:29:34.125576 139762890172160 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.7650873064994812, loss=0.9802406430244446
I0315 15:30:32.290722 139935937595200 spec.py:298] Evaluating on the training split.
I0315 15:31:08.658797 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 15:31:46.145583 139935937595200 spec.py:326] Evaluating on the test split.
I0315 15:32:06.552203 139935937595200 submission_runner.py:362] Time since start: 60049.24s, 	Step: 73975, 	{'train/ctc_loss': DeviceArray(0.1310518, dtype=float32), 'train/wer': 0.047841365183166146, 'validation/ctc_loss': DeviceArray(0.33485386, dtype=float32), 'validation/wer': 0.0954953738096846, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.18101572, dtype=float32), 'test/wer': 0.05922856620559381, 'test/num_examples': 2472}
I0315 15:32:06.576915 139762391684864 logging_writer.py:48] [73975] global_step=73975, preemption_count=0, score=57488.292620, test/ctc_loss=0.1810157150030136, test/num_examples=2472, test/wer=0.059229, total_duration=60049.235926, train/ctc_loss=0.13105179369449615, train/wer=0.047841, validation/ctc_loss=0.334853857755661, validation/num_examples=5348, validation/wer=0.095495
I0315 15:32:06.852587 139935937595200 checkpoints.py:356] Saving checkpoint at step: 73975
I0315 15:32:08.296314 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_73975
I0315 15:32:08.328967 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_73975.
I0315 15:32:27.903017 139762383292160 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.8414261937141418, loss=1.0306369066238403
I0315 15:33:43.122928 139762148296448 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.960883378982544, loss=0.9976928234100342
I0315 15:35:02.033750 139761736324864 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.8652323484420776, loss=1.0316789150238037
I0315 15:36:17.405189 139761727932160 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.8340551853179932, loss=1.0215826034545898
I0315 15:37:32.655617 139761736324864 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.8651866912841797, loss=0.9535933136940002
I0315 15:38:47.975212 139761727932160 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.008587121963501, loss=1.0217399597167969
I0315 15:40:03.322544 139761736324864 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.8454630374908447, loss=1.0450834035873413
I0315 15:41:18.569520 139761727932160 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.8419876098632812, loss=1.0333150625228882
I0315 15:42:33.850007 139761736324864 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.929970920085907, loss=1.0064862966537476
I0315 15:43:53.663717 139761727932160 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.8523867726325989, loss=0.97110915184021
I0315 15:45:15.791363 139761736324864 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.8316658735275269, loss=0.999581515789032
I0315 15:46:36.699282 139761727932160 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.9209397435188293, loss=1.0014762878417969
I0315 15:48:00.984546 139762391684864 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.9188192486763, loss=1.0028611421585083
I0315 15:49:16.380594 139762383292160 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.8458914756774902, loss=0.9794048070907593
I0315 15:50:31.813577 139762391684864 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.0800743103027344, loss=1.0054339170455933
I0315 15:51:47.287889 139762383292160 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.8015674948692322, loss=1.0439943075180054
I0315 15:53:02.600850 139762391684864 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.9559240937232971, loss=1.053181529045105
I0315 15:54:18.006778 139762383292160 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.8028593063354492, loss=0.9948925971984863
I0315 15:55:33.329236 139762391684864 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.8009597063064575, loss=0.986031711101532
I0315 15:56:49.368335 139762383292160 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.8593031167984009, loss=1.0274698734283447
I0315 15:58:09.258282 139762391684864 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.836828351020813, loss=0.9401403665542603
I0315 15:59:33.133210 139762383292160 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.0462340116500854, loss=1.0228313207626343
I0315 16:00:52.878360 139762391684864 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.8770164847373962, loss=0.9808777570724487
I0315 16:02:12.257247 139761736324864 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.8476430177688599, loss=0.9865506887435913
I0315 16:03:27.582777 139761727932160 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.7856393456459045, loss=0.9731375575065613
I0315 16:04:42.979532 139761736324864 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.8353073000907898, loss=1.0309137105941772
I0315 16:05:58.299664 139761727932160 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.8228244781494141, loss=0.9942891001701355
I0315 16:07:13.727254 139761736324864 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.8065866827964783, loss=0.9517849087715149
I0315 16:08:29.134643 139761727932160 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.0739485025405884, loss=0.9933379292488098
I0315 16:09:50.733407 139761736324864 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.8728578686714172, loss=1.0008116960525513
I0315 16:11:10.570008 139761727932160 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.949301540851593, loss=1.063683271408081
I0315 16:12:08.359409 139935937595200 spec.py:298] Evaluating on the training split.
I0315 16:12:44.031721 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 16:13:22.231044 139935937595200 spec.py:326] Evaluating on the test split.
I0315 16:13:42.604559 139935937595200 submission_runner.py:362] Time since start: 62545.30s, 	Step: 77073, 	{'train/ctc_loss': DeviceArray(0.14083833, dtype=float32), 'train/wer': 0.053939432936923444, 'validation/ctc_loss': DeviceArray(0.3182592, dtype=float32), 'validation/wer': 0.09271676523651941, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.17458922, dtype=float32), 'test/wer': 0.05579590924786221, 'test/num_examples': 2472}
I0315 16:13:42.629428 139761736324864 logging_writer.py:48] [77073] global_step=77073, preemption_count=0, score=59880.522376, test/ctc_loss=0.17458921670913696, test/num_examples=2472, test/wer=0.055796, total_duration=62545.304626, train/ctc_loss=0.14083832502365112, train/wer=0.053939, validation/ctc_loss=0.31825920939445496, validation/num_examples=5348, validation/wer=0.092717
I0315 16:13:42.964641 139935937595200 checkpoints.py:356] Saving checkpoint at step: 77073
I0315 16:13:44.393737 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_77073
I0315 16:13:44.426725 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_77073.
I0315 16:14:05.512800 139761727932160 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.8367564082145691, loss=1.0313689708709717
I0315 16:15:20.859240 139761484543744 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.8364936709403992, loss=1.0312548875808716
I0315 16:16:39.849976 139761736324864 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.9302003979682922, loss=0.9888123273849487
I0315 16:17:55.181456 139761727932160 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.9240292906761169, loss=0.9398337602615356
I0315 16:19:10.470621 139761736324864 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.0129220485687256, loss=0.996721625328064
I0315 16:20:25.973623 139761727932160 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.9862475395202637, loss=0.9855647087097168
I0315 16:21:41.682666 139761736324864 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.894469141960144, loss=0.9799879193305969
I0315 16:22:57.390459 139761727932160 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.1842960119247437, loss=1.0148069858551025
I0315 16:24:14.877542 139761736324864 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.0445424318313599, loss=0.9514805674552917
I0315 16:25:35.834156 139761727932160 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.9150398969650269, loss=1.0098495483398438
I0315 16:26:55.600261 139761736324864 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.9503428339958191, loss=1.048540711402893
I0315 16:28:13.631633 139761727932160 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.0449769496917725, loss=0.9368520379066467
I0315 16:29:37.221166 139762391684864 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.8806682229042053, loss=0.9923686981201172
I0315 16:30:52.468433 139762383292160 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.0297636985778809, loss=0.9890891313552856
I0315 16:32:07.704758 139762391684864 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.0887831449508667, loss=0.9888681769371033
I0315 16:33:23.193663 139762383292160 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.823807954788208, loss=0.974227249622345
I0315 16:34:38.963280 139762391684864 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.1808785200119019, loss=1.0247646570205688
I0315 16:35:54.566155 139762383292160 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.9890642762184143, loss=0.9751124978065491
I0315 16:37:10.090652 139762391684864 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.1781178712844849, loss=0.9394745826721191
I0315 16:38:25.754783 139762383292160 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.9653382897377014, loss=1.0030828714370728
I0315 16:39:41.309660 139762391684864 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.9509447813034058, loss=1.0583873987197876
I0315 16:40:57.026124 139762383292160 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.9354868531227112, loss=0.9792960286140442
I0315 16:42:12.520848 139762391684864 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.9809495210647583, loss=1.0299724340438843
I0315 16:43:31.173186 139761736324864 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.9705813527107239, loss=0.9277345538139343
I0315 16:44:46.731137 139761727932160 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.0177476406097412, loss=0.9626331925392151
I0315 16:46:02.163132 139761736324864 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.840918779373169, loss=0.9458529949188232
I0315 16:47:17.438117 139761727932160 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.0139983892440796, loss=0.9911798238754272
I0315 16:48:32.717317 139761736324864 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.0366982221603394, loss=0.9624334573745728
I0315 16:49:48.473529 139761727932160 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.130729079246521, loss=0.9961421489715576
I0315 16:51:05.043586 139761736324864 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.9898624420166016, loss=0.9611208438873291
I0315 16:52:23.473387 139761727932160 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.8462927341461182, loss=0.9679975509643555
I0315 16:53:44.148350 139761736324864 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.0124064683914185, loss=1.0198276042938232
I0315 16:53:44.608931 139935937595200 spec.py:298] Evaluating on the training split.
I0315 16:54:20.364011 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 16:54:59.079636 139935937595200 spec.py:326] Evaluating on the test split.
I0315 16:55:19.411915 139935937595200 submission_runner.py:362] Time since start: 65041.55s, 	Step: 80202, 	{'train/ctc_loss': DeviceArray(0.11894266, dtype=float32), 'train/wer': 0.043061778403169874, 'validation/ctc_loss': DeviceArray(0.31821582, dtype=float32), 'validation/wer': 0.09008287585987322, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.16986497, dtype=float32), 'test/wer': 0.05421160603660147, 'test/num_examples': 2472}
I0315 16:55:19.436995 139761736324864 logging_writer.py:48] [80202] global_step=80202, preemption_count=0, score=62272.907842, test/ctc_loss=0.1698649674654007, test/num_examples=2472, test/wer=0.054212, total_duration=65041.554144, train/ctc_loss=0.11894266307353973, train/wer=0.043062, validation/ctc_loss=0.31821581721305847, validation/num_examples=5348, validation/wer=0.090083
I0315 16:55:19.771382 139935937595200 checkpoints.py:356] Saving checkpoint at step: 80202
I0315 16:55:21.253344 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_80202
I0315 16:55:21.287844 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_80202.
I0315 16:56:35.749990 139761727932160 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.9485889077186584, loss=0.9750303030014038
I0315 16:57:54.672245 139762391684864 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.924132227897644, loss=0.9297945499420166
I0315 16:59:10.127736 139762383292160 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.0332592725753784, loss=0.98562091588974
I0315 17:00:25.461690 139762391684864 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.0674827098846436, loss=0.9534667730331421
I0315 17:01:40.690610 139762383292160 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.9856923222541809, loss=0.9912011623382568
I0315 17:02:56.225080 139762391684864 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.0366456508636475, loss=0.9486356973648071
I0315 17:04:11.675161 139762383292160 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.9413436055183411, loss=0.9776435494422913
I0315 17:05:30.223692 139762391684864 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.886472225189209, loss=0.9814271926879883
I0315 17:06:50.940104 139762383292160 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.9522818326950073, loss=1.0047242641448975
I0315 17:08:13.083356 139762391684864 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.1169769763946533, loss=0.9868426322937012
I0315 17:09:34.027346 139762383292160 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.9319924712181091, loss=0.9940942525863647
I0315 17:10:58.557818 139762391684864 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.9668405055999756, loss=0.9221052527427673
I0315 17:12:14.036795 139762383292160 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.8845314979553223, loss=0.9336035251617432
I0315 17:13:29.478298 139762391684864 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.021522045135498, loss=0.9188088178634644
I0315 17:14:44.963690 139762383292160 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.8697233200073242, loss=0.9043723344802856
I0315 17:16:00.369976 139762391684864 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.9347577691078186, loss=0.9725224375724792
I0315 17:17:15.765918 139762383292160 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.9478321075439453, loss=0.946017861366272
I0315 17:18:32.365745 139762391684864 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.8932844400405884, loss=0.9659599661827087
I0315 17:19:54.970667 139762383292160 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.0030114650726318, loss=0.9650875329971313
I0315 17:21:19.525515 139762391684864 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.9890845417976379, loss=1.0016711950302124
I0315 17:22:41.038763 139762383292160 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.874630331993103, loss=0.9323524832725525
I0315 17:24:04.466653 139762391684864 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.2033812999725342, loss=0.9801287055015564
I0315 17:25:19.742364 139762383292160 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.0088839530944824, loss=0.9727338552474976
I0315 17:26:35.303489 139762391684864 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.1505237817764282, loss=0.9758340716362
I0315 17:27:50.703119 139762383292160 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.930006742477417, loss=0.9539848566055298
I0315 17:29:06.146825 139762391684864 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.8811674118041992, loss=0.9347664713859558
I0315 17:30:21.472973 139762383292160 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.8454516530036926, loss=0.9043494462966919
I0315 17:31:37.037545 139762391684864 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.9731244444847107, loss=0.9582692384719849
I0315 17:32:52.458716 139762383292160 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.9389917254447937, loss=0.9346886873245239
I0315 17:34:14.532970 139762391684864 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.9370600581169128, loss=0.9403822422027588
I0315 17:35:21.969197 139935937595200 spec.py:298] Evaluating on the training split.
I0315 17:35:58.870368 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 17:36:37.631995 139935937595200 spec.py:326] Evaluating on the test split.
I0315 17:36:58.114020 139935937595200 submission_runner.py:362] Time since start: 67538.91s, 	Step: 83283, 	{'train/ctc_loss': DeviceArray(0.10204645, dtype=float32), 'train/wer': 0.03870820482558204, 'validation/ctc_loss': DeviceArray(0.3070815, dtype=float32), 'validation/wer': 0.08703412478653919, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.16460858, dtype=float32), 'test/wer': 0.052546056506814535, 'test/num_examples': 2472}
I0315 17:36:58.138101 139762391684864 logging_writer.py:48] [83283] global_step=83283, preemption_count=0, score=64665.837786, test/ctc_loss=0.16460858285427094, test/num_examples=2472, test/wer=0.052546, total_duration=67538.914419, train/ctc_loss=0.10204645246267319, train/wer=0.038708, validation/ctc_loss=0.3070814907550812, validation/num_examples=5348, validation/wer=0.087034
I0315 17:36:58.444378 139935937595200 checkpoints.py:356] Saving checkpoint at step: 83283
I0315 17:36:59.879215 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_83283
I0315 17:36:59.912215 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_83283.
I0315 17:37:13.508286 139762383292160 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.101157307624817, loss=0.9453812837600708
I0315 17:38:28.977074 139761467758336 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.9476427435874939, loss=0.9425901770591736
I0315 17:39:48.003136 139761736324864 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.8300719261169434, loss=0.9175465106964111
I0315 17:41:03.289474 139761727932160 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.9489307999610901, loss=0.9125007390975952
I0315 17:42:18.668563 139761736324864 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.0178210735321045, loss=0.9238643050193787
I0315 17:43:34.216582 139761727932160 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.0573735237121582, loss=0.9644809365272522
I0315 17:44:49.951864 139761736324864 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.4496129751205444, loss=0.946303129196167
I0315 17:46:05.404887 139761727932160 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.1243879795074463, loss=0.9152426719665527
I0315 17:47:20.680824 139761736324864 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.9844759702682495, loss=0.9714064598083496
I0315 17:48:36.904773 139761727932160 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.0780174732208252, loss=0.9421854615211487
I0315 17:49:59.200048 139761736324864 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.0340427160263062, loss=0.9320805072784424
I0315 17:51:20.680001 139761727932160 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.0406677722930908, loss=0.967100203037262
I0315 17:52:42.781474 139762391684864 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.0820424556732178, loss=0.9101521968841553
I0315 17:53:58.037419 139762383292160 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.9282771944999695, loss=0.9216079115867615
I0315 17:55:13.302213 139762391684864 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.0535870790481567, loss=0.9612158536911011
I0315 17:56:28.659300 139762383292160 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.253874659538269, loss=0.9052859544754028
I0315 17:57:44.045080 139762391684864 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.965591549873352, loss=0.8864490985870361
I0315 17:58:59.373608 139762383292160 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.9585792422294617, loss=0.9276489615440369
I0315 18:00:14.763940 139762391684864 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.1661758422851562, loss=0.9398358464241028
I0315 18:01:30.093903 139762383292160 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.035399079322815, loss=0.9433282613754272
I0315 18:02:45.879818 139762391684864 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.1229348182678223, loss=0.9100775718688965
I0315 18:04:08.440163 139762383292160 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.9564654231071472, loss=0.9135981798171997
I0315 18:05:31.819954 139761736324864 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.0513989925384521, loss=0.9586995840072632
I0315 18:06:47.516451 139761727932160 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.0173187255859375, loss=0.9179137349128723
I0315 18:08:03.326294 139761736324864 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.0188806056976318, loss=0.9007309675216675
I0315 18:09:18.984721 139761727932160 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.0752376317977905, loss=0.8988631367683411
I0315 18:10:34.673179 139761736324864 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.2543187141418457, loss=0.9166692495346069
I0315 18:11:50.479882 139761727932160 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.1279876232147217, loss=0.9401942491531372
I0315 18:13:05.961976 139761736324864 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.927727997303009, loss=0.9297589063644409
I0315 18:14:21.739871 139761727932160 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.9299013018608093, loss=0.8833051323890686
I0315 18:15:44.268863 139761736324864 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.0220950841903687, loss=0.9515655040740967
I0315 18:17:00.538313 139935937595200 spec.py:298] Evaluating on the training split.
I0315 18:17:37.554996 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 18:18:15.935837 139935937595200 spec.py:326] Evaluating on the test split.
I0315 18:18:35.544605 139935937595200 submission_runner.py:362] Time since start: 70037.48s, 	Step: 86392, 	{'train/ctc_loss': DeviceArray(0.07941885, dtype=float32), 'train/wer': 0.030554981565868035, 'validation/ctc_loss': DeviceArray(0.30075538, dtype=float32), 'validation/wer': 0.08391783808816293, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.1557824, dtype=float32), 'test/wer': 0.05057583328255438, 'test/num_examples': 2472}
I0315 18:18:35.569647 139761736324864 logging_writer.py:48] [86392] global_step=86392, preemption_count=0, score=67058.483201, test/ctc_loss=0.15578240156173706, test/num_examples=2472, test/wer=0.050576, total_duration=70037.483526, train/ctc_loss=0.07941884547472, train/wer=0.030555, validation/ctc_loss=0.3007553815841675, validation/num_examples=5348, validation/wer=0.083918
I0315 18:18:35.874349 139935937595200 checkpoints.py:356] Saving checkpoint at step: 86392
I0315 18:18:37.323163 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_86392
I0315 18:18:37.356710 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_86392.
I0315 18:18:44.228334 139761727932160 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.0793579816818237, loss=0.9187886118888855
I0315 18:19:59.963568 139761459365632 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.1185550689697266, loss=0.9756188988685608
I0315 18:21:19.324444 139762391684864 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.0060523748397827, loss=0.8970276713371277
I0315 18:22:34.694609 139762383292160 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.9840983748435974, loss=0.8992834091186523
I0315 18:23:50.086253 139762391684864 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.0197995901107788, loss=0.9369189143180847
I0315 18:25:05.420530 139762383292160 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.0578384399414062, loss=0.9125414490699768
I0315 18:26:20.694458 139762391684864 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.0247002840042114, loss=0.939693808555603
I0315 18:27:38.820568 139762383292160 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.0813815593719482, loss=0.9423421025276184
I0315 18:28:57.153496 139762391684864 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.0601505041122437, loss=0.9006398320198059
I0315 18:30:16.843176 139762383292160 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.9868184924125671, loss=0.9358373880386353
I0315 18:31:34.334449 139762391684864 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.9983505010604858, loss=0.885100245475769
I0315 18:32:54.268102 139762383292160 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.1853901147842407, loss=0.919720470905304
I0315 18:34:16.811350 139761736324864 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.0072779655456543, loss=0.9421672821044922
I0315 18:35:32.149811 139761727932160 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.9822387099266052, loss=0.9120826721191406
I0315 18:36:47.617956 139761736324864 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.932902455329895, loss=0.8746607899665833
I0315 18:38:03.208604 139761727932160 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.0019794702529907, loss=0.9015475511550903
I0315 18:39:18.868700 139761736324864 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.8889676928520203, loss=0.9387722611427307
I0315 18:40:34.267635 139761727932160 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.1958436965942383, loss=0.8778174519538879
I0315 18:41:49.593905 139761736324864 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.057892918586731, loss=0.9210802912712097
I0315 18:43:04.785835 139761727932160 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.1353789567947388, loss=0.8721500635147095
I0315 18:44:27.412946 139761736324864 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.0368163585662842, loss=0.9070367813110352
I0315 18:45:48.503996 139761727932160 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.0296893119812012, loss=0.8967173099517822
I0315 18:47:13.908395 139761736324864 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.0854078531265259, loss=0.9342693090438843
I0315 18:48:29.395386 139761727932160 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.9433884620666504, loss=0.8806849122047424
I0315 18:49:44.942026 139761736324864 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.2186110019683838, loss=0.9094642996788025
I0315 18:51:00.569586 139761727932160 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.1792222261428833, loss=0.8908661007881165
I0315 18:52:16.064642 139761736324864 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.068997859954834, loss=0.8978012204170227
I0315 18:53:31.810907 139761727932160 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.276534914970398, loss=0.8795057535171509
I0315 18:54:47.309973 139761736324864 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.1416528224945068, loss=0.9175018668174744
I0315 18:56:02.601868 139761727932160 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.9387759566307068, loss=0.9460262656211853
I0315 18:57:22.554048 139761736324864 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.232231616973877, loss=0.8826802372932434
I0315 18:58:37.945034 139935937595200 spec.py:298] Evaluating on the training split.
I0315 18:59:15.120910 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 18:59:53.063034 139935937595200 spec.py:326] Evaluating on the test split.
I0315 19:00:12.880837 139935937595200 submission_runner.py:362] Time since start: 72534.89s, 	Step: 89496, 	{'train/ctc_loss': DeviceArray(0.08349711, dtype=float32), 'train/wer': 0.031700945543021934, 'validation/ctc_loss': DeviceArray(0.2957625, dtype=float32), 'validation/wer': 0.08190141728333124, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.1534131, dtype=float32), 'test/wer': 0.047915016350821606, 'test/num_examples': 2472}
I0315 19:00:12.907586 139761736324864 logging_writer.py:48] [89496] global_step=89496, preemption_count=0, score=69451.342222, test/ctc_loss=0.1534131020307541, test/num_examples=2472, test/wer=0.047915, total_duration=72534.890243, train/ctc_loss=0.08349710702896118, train/wer=0.031701, validation/ctc_loss=0.2957625091075897, validation/num_examples=5348, validation/wer=0.081901
I0315 19:00:13.209218 139935937595200 checkpoints.py:356] Saving checkpoint at step: 89496
I0315 19:00:14.658449 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_89496
I0315 19:00:14.691066 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_89496.
I0315 19:00:18.515023 139761727932160 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.9432045817375183, loss=0.8564152121543884
I0315 19:01:33.757848 139761450972928 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.9213303923606873, loss=0.903274416923523
I0315 19:02:53.292460 139761736324864 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.1913563013076782, loss=0.8960033655166626
I0315 19:04:09.025337 139761727932160 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.2337387800216675, loss=0.9077501893043518
I0315 19:05:24.451762 139761736324864 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.9391520619392395, loss=0.872495710849762
I0315 19:06:39.732156 139761727932160 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.925957977771759, loss=0.9316977262496948
I0315 19:07:55.023489 139761736324864 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.0274434089660645, loss=0.871626079082489
I0315 19:09:11.329257 139761727932160 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.1250883340835571, loss=0.9159109592437744
I0315 19:10:34.830144 139761736324864 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.045045256614685, loss=0.8981730341911316
I0315 19:11:59.395639 139761727932160 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.0001699924468994, loss=0.8863298296928406
I0315 19:13:25.213483 139761736324864 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.10338294506073, loss=0.8996885418891907
I0315 19:14:46.794339 139761727932160 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.1329565048217773, loss=0.88189297914505
I0315 19:16:09.278200 139761736324864 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.0711145401000977, loss=0.8418238162994385
I0315 19:17:24.733793 139761727932160 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.2355409860610962, loss=0.8893324136734009
I0315 19:18:40.381877 139761736324864 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.4195709228515625, loss=0.9239348769187927
I0315 19:19:55.861663 139761727932160 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.1038219928741455, loss=0.9145068526268005
I0315 19:21:11.105888 139761736324864 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.2059179544448853, loss=0.9067059755325317
I0315 19:22:26.474819 139761727932160 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.1520872116088867, loss=0.8792622089385986
I0315 19:23:42.168702 139761736324864 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.2036206722259521, loss=0.8824314475059509
I0315 19:24:57.581531 139761727932160 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.1549110412597656, loss=0.9053103923797607
I0315 19:26:15.702784 139761736324864 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.0696265697479248, loss=0.9102910161018372
I0315 19:27:35.962481 139761727932160 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.2399590015411377, loss=0.8709118366241455
I0315 19:28:56.751985 139761736324864 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.0891987085342407, loss=0.9207330346107483
I0315 19:30:12.381222 139761727932160 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.0301297903060913, loss=0.872185468673706
I0315 19:31:27.866708 139761736324864 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.0045456886291504, loss=0.8849509954452515
I0315 19:32:43.246877 139761727932160 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.0826503038406372, loss=0.8872955441474915
I0315 19:33:58.523333 139761736324864 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.1445586681365967, loss=0.8801453113555908
I0315 19:35:13.807748 139761727932160 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.0309569835662842, loss=0.8931188583374023
I0315 19:36:29.241549 139761736324864 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.0560020208358765, loss=0.8893066644668579
I0315 19:37:44.949442 139761727932160 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.2998188734054565, loss=0.9124355912208557
I0315 19:39:00.823318 139761736324864 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.1379718780517578, loss=0.8556801080703735
I0315 19:40:15.208172 139935937595200 spec.py:298] Evaluating on the training split.
I0315 19:40:52.524971 139935937595200 spec.py:310] Evaluating on the validation split.
I0315 19:41:30.005800 139935937595200 spec.py:326] Evaluating on the test split.
I0315 19:41:49.951863 139935937595200 submission_runner.py:362] Time since start: 75032.15s, 	Step: 92600, 	{'train/ctc_loss': DeviceArray(0.07478549, dtype=float32), 'train/wer': 0.027545739563648856, 'validation/ctc_loss': DeviceArray(0.2873434, dtype=float32), 'validation/wer': 0.07938330326390028, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.15168855, dtype=float32), 'test/wer': 0.04775252371376922, 'test/num_examples': 2472}
I0315 19:41:49.979911 139761736324864 logging_writer.py:48] [92600] global_step=92600, preemption_count=0, score=71844.024841, test/ctc_loss=0.15168854594230652, test/num_examples=2472, test/wer=0.047753, total_duration=75032.153384, train/ctc_loss=0.07478548586368561, train/wer=0.027546, validation/ctc_loss=0.28734341263771057, validation/num_examples=5348, validation/wer=0.079383
I0315 19:41:50.337527 139935937595200 checkpoints.py:356] Saving checkpoint at step: 92600
I0315 19:41:51.965926 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_92600
I0315 19:41:51.995993 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_92600.
I0315 19:41:52.845556 139761727932160 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.1548216342926025, loss=0.944069504737854
I0315 19:43:11.505867 139761736324864 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.9562960267066956, loss=0.8974765539169312
I0315 19:44:27.000023 139761727932160 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.229323387145996, loss=0.8946415185928345
I0315 19:44:29.012223 139761736324864 logging_writer.py:48] [92804] global_step=92804, preemption_count=0, score=72000.490123
I0315 19:44:29.786450 139935937595200 checkpoints.py:356] Saving checkpoint at step: 92804
I0315 19:44:31.214831 139935937595200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_92804
I0315 19:44:31.249173 139935937595200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_conformer_jax/trial_1/checkpoint_92804.
I0315 19:44:34.650616 139935937595200 submission_runner.py:523] Tuning trial 1/1
I0315 19:44:34.650845 139935937595200 submission_runner.py:524] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, beta1=0.9731333693827139, beta2=0.9981232922116359, warmup_steps=9999, weight_decay=0.16375311233774334)
I0315 19:44:34.663604 139935937595200 submission_runner.py:525] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.783794, dtype=float32), 'train/wer': 1.3892770130978906, 'validation/ctc_loss': DeviceArray(31.04576, dtype=float32), 'validation/wer': 1.3554689384364538, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.139847, dtype=float32), 'test/wer': 1.3896573436516158, 'test/num_examples': 2472, 'score': 61.9443793296814, 'total_duration': 62.13530397415161, 'global_step': 1, 'preemption_count': 0}), (3097, {'train/ctc_loss': DeviceArray(3.382217, dtype=float32), 'train/wer': 0.7113756668245762, 'validation/ctc_loss': DeviceArray(3.6820817, dtype=float32), 'validation/wer': 0.7188877847350191, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3.4262977, dtype=float32), 'test/wer': 0.6972762171714094, 'test/num_examples': 2472, 'score': 2454.8942635059357, 'total_duration': 2592.6732296943665, 'global_step': 3097, 'preemption_count': 0}), (6204, {'train/ctc_loss': DeviceArray(0.62502086, dtype=float32), 'train/wer': 0.21431958021468717, 'validation/ctc_loss': DeviceArray(0.94187, dtype=float32), 'validation/wer': 0.27905720267441075, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.66660255, dtype=float32), 'test/wer': 0.21717140942051064, 'test/num_examples': 2472, 'score': 4847.354571819305, 'total_duration': 5089.671578407288, 'global_step': 6204, 'preemption_count': 0}), (9290, {'train/ctc_loss': DeviceArray(0.42288408, dtype=float32), 'train/wer': 0.15223155220308784, 'validation/ctc_loss': DeviceArray(0.75306743, dtype=float32), 'validation/wer': 0.2246041929975205, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.49526188, dtype=float32), 'test/wer': 0.16385351288769728, 'test/num_examples': 2472, 'score': 7240.238754749298, 'total_duration': 7586.378684997559, 'global_step': 9290, 'preemption_count': 0}), (12354, {'train/ctc_loss': DeviceArray(0.37204802, dtype=float32), 'train/wer': 0.13383216915037335, 'validation/ctc_loss': DeviceArray(0.66134274, dtype=float32), 'validation/wer': 0.2025296915551525, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4276823, dtype=float32), 'test/wer': 0.14508561330814698, 'test/num_examples': 2472, 'score': 9632.916442632675, 'total_duration': 10083.076741456985, 'global_step': 12354, 'preemption_count': 0}), (15408, {'train/ctc_loss': DeviceArray(0.33014566, dtype=float32), 'train/wer': 0.11944704168388093, 'validation/ctc_loss': DeviceArray(0.61305237, dtype=float32), 'validation/wer': 0.18375478779341817, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38567758, dtype=float32), 'test/wer': 0.13011597911969613, 'test/num_examples': 2472, 'score': 12026.035771846771, 'total_duration': 12579.926593065262, 'global_step': 15408, 'preemption_count': 0}), (18460, {'train/ctc_loss': DeviceArray(0.32438862, dtype=float32), 'train/wer': 0.11455793780774709, 'validation/ctc_loss': DeviceArray(0.5705625, dtype=float32), 'validation/wer': 0.172891200108057, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.35358003, dtype=float32), 'test/wer': 0.11975707350760668, 'test/num_examples': 2472, 'score': 14419.042431592941, 'total_duration': 15077.660467863083, 'global_step': 18460, 'preemption_count': 0}), (21520, {'train/ctc_loss': DeviceArray(0.27776644, dtype=float32), 'train/wer': 0.1005413396294642, 'validation/ctc_loss': DeviceArray(0.5416949, dtype=float32), 'validation/wer': 0.16300205501259057, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3335092, dtype=float32), 'test/wer': 0.11281051327361728, 'test/num_examples': 2472, 'score': 16811.673842668533, 'total_duration': 17576.199376821518, 'global_step': 21520, 'preemption_count': 0}), (24548, {'train/ctc_loss': DeviceArray(0.24292839, dtype=float32), 'train/wer': 0.09057437790894199, 'validation/ctc_loss': DeviceArray(0.5270403, dtype=float32), 'validation/wer': 0.1577535721521674, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.31746113, dtype=float32), 'test/wer': 0.1072654520342047, 'test/num_examples': 2472, 'score': 19204.56942987442, 'total_duration': 20074.661943674088, 'global_step': 24548, 'preemption_count': 0}), (27603, {'train/ctc_loss': DeviceArray(0.22396263, dtype=float32), 'train/wer': 0.08077927743404498, 'validation/ctc_loss': DeviceArray(0.50849587, dtype=float32), 'validation/wer': 0.15240860982739823, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.30482325, dtype=float32), 'test/wer': 0.10212662238742307, 'test/num_examples': 2472, 'score': 21597.555043697357, 'total_duration': 22573.9879155159, 'global_step': 27603, 'preemption_count': 0}), (30635, {'train/ctc_loss': DeviceArray(0.21750984, dtype=float32), 'train/wer': 0.08221293848241244, 'validation/ctc_loss': DeviceArray(0.4860431, dtype=float32), 'validation/wer': 0.1466101940202028, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.2918611, dtype=float32), 'test/wer': 0.0983283569963236, 'test/num_examples': 2472, 'score': 23990.0570435524, 'total_duration': 25072.39537167549, 'global_step': 30635, 'preemption_count': 0}), (33701, {'train/ctc_loss': DeviceArray(0.23125577, dtype=float32), 'train/wer': 0.08354530791401148, 'validation/ctc_loss': DeviceArray(0.48458257, dtype=float32), 'validation/wer': 0.14441046223311368, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.28890395, dtype=float32), 'test/wer': 0.09749558223143014, 'test/num_examples': 2472, 'score': 26382.577127218246, 'total_duration': 27570.583962917328, 'global_step': 33701, 'preemption_count': 0}), (36765, {'train/ctc_loss': DeviceArray(0.21903068, dtype=float32), 'train/wer': 0.08019626981823938, 'validation/ctc_loss': DeviceArray(0.46387938, dtype=float32), 'validation/wer': 0.1382164806220996, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.26881742, dtype=float32), 'test/wer': 0.09022403672333597, 'test/num_examples': 2472, 'score': 28775.156537771225, 'total_duration': 30068.905346632004, 'global_step': 36765, 'preemption_count': 0}), (39804, {'train/ctc_loss': DeviceArray(0.20274656, dtype=float32), 'train/wer': 0.07532153513735981, 'validation/ctc_loss': DeviceArray(0.45643613, dtype=float32), 'validation/wer': 0.13585273374562223, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.2656183, dtype=float32), 'test/wer': 0.08896471878618, 'test/num_examples': 2472, 'score': 31167.975383996964, 'total_duration': 32566.944352149963, 'global_step': 39804, 'preemption_count': 0}), (42845, {'train/ctc_loss': DeviceArray(0.18709019, dtype=float32), 'train/wer': 0.06900574300071788, 'validation/ctc_loss': DeviceArray(0.44811746, dtype=float32), 'validation/wer': 0.13061389883163368, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.25967354, dtype=float32), 'test/wer': 0.08620234395628948, 'test/num_examples': 2472, 'score': 33560.548149347305, 'total_duration': 35066.08285164833, 'global_step': 42845, 'preemption_count': 0}), (45941, {'train/ctc_loss': DeviceArray(0.17499085, dtype=float32), 'train/wer': 0.06643887425337124, 'validation/ctc_loss': DeviceArray(0.43550894, dtype=float32), 'validation/wer': 0.1286071259732366, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.25229603, dtype=float32), 'test/wer': 0.08258688278187394, 'test/num_examples': 2472, 'score': 35953.3624792099, 'total_duration': 37564.410294771194, 'global_step': 45941, 'preemption_count': 0}), (49020, {'train/ctc_loss': DeviceArray(0.16939811, dtype=float32), 'train/wer': 0.06155710793589101, 'validation/ctc_loss': DeviceArray(0.41998357, dtype=float32), 'validation/wer': 0.12394716784532413, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.24302162, dtype=float32), 'test/wer': 0.08077915219466618, 'test/num_examples': 2472, 'score': 38346.24335813522, 'total_duration': 40062.61844038963, 'global_step': 49020, 'preemption_count': 0}), (52126, {'train/ctc_loss': DeviceArray(0.17175573, dtype=float32), 'train/wer': 0.06300377015080603, 'validation/ctc_loss': DeviceArray(0.40576738, dtype=float32), 'validation/wer': 0.12082123320051327, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.23534396, dtype=float32), 'test/wer': 0.07598561940162087, 'test/num_examples': 2472, 'score': 40739.10560822487, 'total_duration': 42562.35156083107, 'global_step': 52126, 'preemption_count': 0}), (55249, {'train/ctc_loss': DeviceArray(0.15877353, dtype=float32), 'train/wer': 0.056869543513529644, 'validation/ctc_loss': DeviceArray(0.39962924, dtype=float32), 'validation/wer': 0.11635423400129283, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.22373688, dtype=float32), 'test/wer': 0.07330449089025654, 'test/num_examples': 2472, 'score': 43131.312398433685, 'total_duration': 45058.617961883545, 'global_step': 55249, 'preemption_count': 0}), (58395, {'train/ctc_loss': DeviceArray(0.16121131, dtype=float32), 'train/wer': 0.05815512485295553, 'validation/ctc_loss': DeviceArray(0.38644284, dtype=float32), 'validation/wer': 0.11311252399926676, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.21834558, dtype=float32), 'test/wer': 0.07176081083825889, 'test/num_examples': 2472, 'score': 45524.081456661224, 'total_duration': 47556.92321681976, 'global_step': 58395, 'preemption_count': 0}), (61484, {'train/ctc_loss': DeviceArray(0.15064636, dtype=float32), 'train/wer': 0.05401920351247376, 'validation/ctc_loss': DeviceArray(0.3754372, dtype=float32), 'validation/wer': 0.11098997578365445, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.21323049, dtype=float32), 'test/wer': 0.07072492027704995, 'test/num_examples': 2472, 'score': 47916.9006831646, 'total_duration': 50055.963148117065, 'global_step': 61484, 'preemption_count': 0}), (64644, {'train/ctc_loss': DeviceArray(0.10945161, dtype=float32), 'train/wer': 0.04149626395252298, 'validation/ctc_loss': DeviceArray(0.36210132, dtype=float32), 'validation/wer': 0.10688959854894886, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.20593657, dtype=float32), 'test/wer': 0.06658135803221417, 'test/num_examples': 2472, 'score': 50309.63060045242, 'total_duration': 52555.144043922424, 'global_step': 64644, 'preemption_count': 0}), (67749, {'train/ctc_loss': DeviceArray(0.1206243, dtype=float32), 'train/wer': 0.04498849021139997, 'validation/ctc_loss': DeviceArray(0.35496044, dtype=float32), 'validation/wer': 0.10332950631458095, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.19711342, dtype=float32), 'test/wer': 0.063758048463429, 'test/num_examples': 2472, 'score': 52702.5319480896, 'total_duration': 55052.69009804726, 'global_step': 67749, 'preemption_count': 0}), (70836, {'train/ctc_loss': DeviceArray(0.1393159, dtype=float32), 'train/wer': 0.052700379143320514, 'validation/ctc_loss': DeviceArray(0.34659326, dtype=float32), 'validation/wer': 0.10038688265202751, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.18663132, dtype=float32), 'test/wer': 0.06209249893364207, 'test/num_examples': 2472, 'score': 55095.37888622284, 'total_duration': 57551.41803359985, 'global_step': 70836, 'preemption_count': 0}), (73975, {'train/ctc_loss': DeviceArray(0.1310518, dtype=float32), 'train/wer': 0.047841365183166146, 'validation/ctc_loss': DeviceArray(0.33485386, dtype=float32), 'validation/wer': 0.0954953738096846, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.18101572, dtype=float32), 'test/wer': 0.05922856620559381, 'test/num_examples': 2472, 'score': 57488.29262018204, 'total_duration': 60049.235926389694, 'global_step': 73975, 'preemption_count': 0}), (77073, {'train/ctc_loss': DeviceArray(0.14083833, dtype=float32), 'train/wer': 0.053939432936923444, 'validation/ctc_loss': DeviceArray(0.3182592, dtype=float32), 'validation/wer': 0.09271676523651941, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.17458922, dtype=float32), 'test/wer': 0.05579590924786221, 'test/num_examples': 2472, 'score': 59880.52237582207, 'total_duration': 62545.30462551117, 'global_step': 77073, 'preemption_count': 0}), (80202, {'train/ctc_loss': DeviceArray(0.11894266, dtype=float32), 'train/wer': 0.043061778403169874, 'validation/ctc_loss': DeviceArray(0.31821582, dtype=float32), 'validation/wer': 0.09008287585987322, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.16986497, dtype=float32), 'test/wer': 0.05421160603660147, 'test/num_examples': 2472, 'score': 62272.90784239769, 'total_duration': 65041.55414366722, 'global_step': 80202, 'preemption_count': 0}), (83283, {'train/ctc_loss': DeviceArray(0.10204645, dtype=float32), 'train/wer': 0.03870820482558204, 'validation/ctc_loss': DeviceArray(0.3070815, dtype=float32), 'validation/wer': 0.08703412478653919, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.16460858, dtype=float32), 'test/wer': 0.052546056506814535, 'test/num_examples': 2472, 'score': 64665.83778619766, 'total_duration': 67538.91441893578, 'global_step': 83283, 'preemption_count': 0}), (86392, {'train/ctc_loss': DeviceArray(0.07941885, dtype=float32), 'train/wer': 0.030554981565868035, 'validation/ctc_loss': DeviceArray(0.30075538, dtype=float32), 'validation/wer': 0.08391783808816293, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.1557824, dtype=float32), 'test/wer': 0.05057583328255438, 'test/num_examples': 2472, 'score': 67058.48320126534, 'total_duration': 70037.48352622986, 'global_step': 86392, 'preemption_count': 0}), (89496, {'train/ctc_loss': DeviceArray(0.08349711, dtype=float32), 'train/wer': 0.031700945543021934, 'validation/ctc_loss': DeviceArray(0.2957625, dtype=float32), 'validation/wer': 0.08190141728333124, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.1534131, dtype=float32), 'test/wer': 0.047915016350821606, 'test/num_examples': 2472, 'score': 69451.34222221375, 'total_duration': 72534.8902425766, 'global_step': 89496, 'preemption_count': 0}), (92600, {'train/ctc_loss': DeviceArray(0.07478549, dtype=float32), 'train/wer': 0.027545739563648856, 'validation/ctc_loss': DeviceArray(0.2873434, dtype=float32), 'validation/wer': 0.07938330326390028, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.15168855, dtype=float32), 'test/wer': 0.04775252371376922, 'test/num_examples': 2472, 'score': 71844.02484107018, 'total_duration': 75032.1533844471, 'global_step': 92600, 'preemption_count': 0})], 'global_step': 92804}
I0315 19:44:34.663850 139935937595200 submission_runner.py:526] Timing: 72000.49012255669
I0315 19:44:34.663917 139935937595200 submission_runner.py:527] ====================
I0315 19:44:34.665782 139935937595200 submission_runner.py:586] Finallibrispeech_conformer score: 72000.49012255669
