python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_conformer/adamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=60000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_10-05-2023-22-06-47.log
2023-10-05 22:06:52.826900: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1005 22:07:11.034482 139940046329664 logger_utils.py:61] Removing existing experiment directory /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax because --overwrite was set.
I1005 22:07:11.051316 139940046329664 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax.
I1005 22:07:12.114938 139940046329664 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I1005 22:07:12.116260 139940046329664 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1005 22:07:12.116422 139940046329664 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1005 22:07:12.121373 139940046329664 submission_runner.py:507] Using RNG seed 208631761
I1005 22:07:18.178236 139940046329664 submission_runner.py:516] --- Tuning run 1/1 ---
I1005 22:07:18.178443 139940046329664 submission_runner.py:521] Creating tuning directory at /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1.
I1005 22:07:18.178630 139940046329664 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/hparams.json.
I1005 22:07:18.358663 139940046329664 submission_runner.py:191] Initializing dataset.
I1005 22:07:18.358869 139940046329664 submission_runner.py:198] Initializing model.
I1005 22:07:23.259160 139940046329664 submission_runner.py:232] Initializing optimizer.
I1005 22:07:24.590917 139940046329664 submission_runner.py:239] Initializing metrics bundle.
I1005 22:07:24.591140 139940046329664 submission_runner.py:257] Initializing checkpoint and logger.
I1005 22:07:24.592758 139940046329664 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I1005 22:07:24.592905 139940046329664 submission_runner.py:277] Saving meta data to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/meta_data_0.json.
I1005 22:07:24.593153 139940046329664 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1005 22:07:24.593219 139940046329664 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1005 22:07:24.923699 139940046329664 logger_utils.py:220] Unable to record git information. Continuing without it.
I1005 22:07:25.229140 139940046329664 submission_runner.py:280] Saving flags to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/flags_0.json.
I1005 22:07:25.246841 139940046329664 submission_runner.py:290] Starting training loop.
I1005 22:07:25.539871 139940046329664 input_pipeline.py:20] Loading split = train-clean-100
I1005 22:07:25.578957 139940046329664 input_pipeline.py:20] Loading split = train-clean-360
I1005 22:07:25.982892 139940046329664 input_pipeline.py:20] Loading split = train-other-500
2023-10-05 22:08:36.548809: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-05 22:08:39.475551: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1005 22:08:41.493432 139763839399680 logging_writer.py:48] [0] global_step=0, grad_norm=39.58601379394531, loss=32.93434143066406
I1005 22:08:41.531723 139940046329664 spec.py:321] Evaluating on the training split.
I1005 22:08:41.700916 139940046329664 input_pipeline.py:20] Loading split = train-clean-100
I1005 22:08:41.736228 139940046329664 input_pipeline.py:20] Loading split = train-clean-360
I1005 22:08:42.141619 139940046329664 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1005 22:10:01.553509 139940046329664 spec.py:333] Evaluating on the validation split.
I1005 22:10:01.702475 139940046329664 input_pipeline.py:20] Loading split = dev-clean
I1005 22:10:01.708115 139940046329664 input_pipeline.py:20] Loading split = dev-other
I1005 22:10:53.292864 139940046329664 spec.py:349] Evaluating on the test split.
I1005 22:10:53.414321 139940046329664 input_pipeline.py:20] Loading split = test-clean
I1005 22:11:30.284063 139940046329664 submission_runner.py:381] Time since start: 245.03s, 	Step: 1, 	{'train/ctc_loss': Array(31.94961, dtype=float32), 'train/wer': 1.3229025675197301, 'validation/ctc_loss': Array(30.927902, dtype=float32), 'validation/wer': 0.9735356829298883, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.94005, dtype=float32), 'test/wer': 1.0188288343184448, 'test/num_examples': 2472, 'score': 76.28481006622314, 'total_duration': 245.03448510169983, 'accumulated_submission_time': 76.28481006622314, 'accumulated_eval_time': 168.74962043762207, 'accumulated_logging_time': 0}
I1005 22:11:30.308649 139758068037376 logging_writer.py:48] [1] accumulated_eval_time=168.749620, accumulated_logging_time=0, accumulated_submission_time=76.284810, global_step=1, preemption_count=0, score=76.284810, test/ctc_loss=30.94005012512207, test/num_examples=2472, test/wer=1.018829, total_duration=245.034485, train/ctc_loss=31.949609756469727, train/wer=1.322903, validation/ctc_loss=30.927902221679688, validation/num_examples=5348, validation/wer=0.973536
I1005 22:11:53.089860 139766012786432 logging_writer.py:48] [1] global_step=1, grad_norm=40.8138313293457, loss=33.0535888671875
I1005 22:11:53.918935 139766021179136 logging_writer.py:48] [2] global_step=2, grad_norm=42.871116638183594, loss=32.926151275634766
I1005 22:11:54.739210 139766012786432 logging_writer.py:48] [3] global_step=3, grad_norm=59.00932312011719, loss=32.00989532470703
I1005 22:11:55.626254 139766021179136 logging_writer.py:48] [4] global_step=4, grad_norm=61.47758483886719, loss=31.4366397857666
I1005 22:11:56.512684 139766012786432 logging_writer.py:48] [5] global_step=5, grad_norm=67.40351867675781, loss=30.279619216918945
I1005 22:11:57.395717 139766021179136 logging_writer.py:48] [6] global_step=6, grad_norm=69.18956756591797, loss=29.200483322143555
I1005 22:11:58.277707 139766012786432 logging_writer.py:48] [7] global_step=7, grad_norm=68.42890930175781, loss=28.019454956054688
I1005 22:11:59.173846 139766021179136 logging_writer.py:48] [8] global_step=8, grad_norm=66.79206848144531, loss=26.46653175354004
I1005 22:12:00.062476 139766012786432 logging_writer.py:48] [9] global_step=9, grad_norm=62.73528289794922, loss=23.91237449645996
I1005 22:12:00.947925 139766021179136 logging_writer.py:48] [10] global_step=10, grad_norm=58.69977951049805, loss=22.532480239868164
I1005 22:12:01.827331 139766012786432 logging_writer.py:48] [11] global_step=11, grad_norm=63.90239334106445, loss=21.710569381713867
I1005 22:12:02.731755 139766021179136 logging_writer.py:48] [12] global_step=12, grad_norm=67.87503814697266, loss=20.409414291381836
I1005 22:12:03.619225 139766012786432 logging_writer.py:48] [13] global_step=13, grad_norm=77.46808624267578, loss=18.829866409301758
I1005 22:12:04.520334 139766021179136 logging_writer.py:48] [14] global_step=14, grad_norm=82.83203125, loss=16.27117156982422
I1005 22:12:05.410612 139766012786432 logging_writer.py:48] [15] global_step=15, grad_norm=97.65803527832031, loss=13.746003150939941
I1005 22:12:06.307632 139766021179136 logging_writer.py:48] [16] global_step=16, grad_norm=94.74571228027344, loss=10.253055572509766
I1005 22:12:07.196816 139766012786432 logging_writer.py:48] [17] global_step=17, grad_norm=39.935306549072266, loss=7.8723015785217285
I1005 22:12:08.098856 139766021179136 logging_writer.py:48] [18] global_step=18, grad_norm=4.939671993255615, loss=7.1986470222473145
I1005 22:12:08.983505 139766012786432 logging_writer.py:48] [19] global_step=19, grad_norm=14.81469440460205, loss=7.35519552230835
I1005 22:12:09.886063 139766021179136 logging_writer.py:48] [20] global_step=20, grad_norm=19.953115463256836, loss=7.632798671722412
I1005 22:12:10.775678 139766012786432 logging_writer.py:48] [21] global_step=21, grad_norm=21.974023818969727, loss=7.852924346923828
I1005 22:12:11.672153 139766021179136 logging_writer.py:48] [22] global_step=22, grad_norm=22.44634437561035, loss=7.928756237030029
I1005 22:12:12.560942 139766012786432 logging_writer.py:48] [23] global_step=23, grad_norm=21.804887771606445, loss=7.801271915435791
I1005 22:12:13.459398 139766021179136 logging_writer.py:48] [24] global_step=24, grad_norm=20.568601608276367, loss=7.604844093322754
I1005 22:12:14.348389 139766012786432 logging_writer.py:48] [25] global_step=25, grad_norm=17.97760009765625, loss=7.333439826965332
I1005 22:12:15.247252 139766021179136 logging_writer.py:48] [26] global_step=26, grad_norm=12.12370491027832, loss=7.02457332611084
I1005 22:12:16.134796 139766012786432 logging_writer.py:48] [27] global_step=27, grad_norm=3.446974515914917, loss=6.856653690338135
I1005 22:12:17.036737 139766021179136 logging_writer.py:48] [28] global_step=28, grad_norm=11.513507843017578, loss=6.913245677947998
I1005 22:12:17.926953 139766012786432 logging_writer.py:48] [29] global_step=29, grad_norm=21.801359176635742, loss=7.0702619552612305
I1005 22:12:18.822272 139766021179136 logging_writer.py:48] [30] global_step=30, grad_norm=22.113840103149414, loss=7.0622711181640625
I1005 22:12:19.706681 139766012786432 logging_writer.py:48] [31] global_step=31, grad_norm=13.108912467956543, loss=6.851774215698242
I1005 22:12:20.608935 139766021179136 logging_writer.py:48] [32] global_step=32, grad_norm=4.173276901245117, loss=6.716833114624023
I1005 22:12:21.498965 139766012786432 logging_writer.py:48] [33] global_step=33, grad_norm=4.970746040344238, loss=6.694594383239746
I1005 22:12:22.393634 139766021179136 logging_writer.py:48] [34] global_step=34, grad_norm=8.061722755432129, loss=6.709537506103516
I1005 22:12:23.282240 139766012786432 logging_writer.py:48] [35] global_step=35, grad_norm=9.454004287719727, loss=6.7114667892456055
I1005 22:12:24.185549 139766021179136 logging_writer.py:48] [36] global_step=36, grad_norm=8.621014595031738, loss=6.663563251495361
I1005 22:12:25.078224 139766012786432 logging_writer.py:48] [37] global_step=37, grad_norm=5.373311996459961, loss=6.585628032684326
I1005 22:12:25.974031 139766021179136 logging_writer.py:48] [38] global_step=38, grad_norm=1.9345154762268066, loss=6.507232666015625
I1005 22:12:26.860979 139766012786432 logging_writer.py:48] [39] global_step=39, grad_norm=5.4011383056640625, loss=6.49893045425415
I1005 22:12:27.755940 139766021179136 logging_writer.py:48] [40] global_step=40, grad_norm=8.241910934448242, loss=6.49444055557251
I1005 22:12:28.643191 139766012786432 logging_writer.py:48] [41] global_step=41, grad_norm=6.967083930969238, loss=6.417523384094238
I1005 22:12:29.538645 139766021179136 logging_writer.py:48] [42] global_step=42, grad_norm=3.1105546951293945, loss=6.382075309753418
I1005 22:12:30.436196 139766012786432 logging_writer.py:48] [43] global_step=43, grad_norm=2.27146577835083, loss=6.325110912322998
I1005 22:12:31.334338 139766021179136 logging_writer.py:48] [44] global_step=44, grad_norm=4.142608642578125, loss=6.3042473793029785
I1005 22:12:32.232705 139766012786432 logging_writer.py:48] [45] global_step=45, grad_norm=3.7814321517944336, loss=6.287062168121338
I1005 22:12:33.124126 139766021179136 logging_writer.py:48] [46] global_step=46, grad_norm=1.1619291305541992, loss=6.248551368713379
I1005 22:12:34.025151 139766012786432 logging_writer.py:48] [47] global_step=47, grad_norm=3.238323450088501, loss=6.238059997558594
I1005 22:12:34.925196 139766021179136 logging_writer.py:48] [48] global_step=48, grad_norm=4.125179290771484, loss=6.197656154632568
I1005 22:12:35.820566 139766012786432 logging_writer.py:48] [49] global_step=49, grad_norm=2.0978360176086426, loss=6.187483787536621
I1005 22:12:36.715757 139766021179136 logging_writer.py:48] [50] global_step=50, grad_norm=2.7039995193481445, loss=6.150317668914795
I1005 22:12:37.619092 139766012786432 logging_writer.py:48] [51] global_step=51, grad_norm=3.0739824771881104, loss=6.139758110046387
I1005 22:12:38.520731 139766021179136 logging_writer.py:48] [52] global_step=52, grad_norm=0.8027873635292053, loss=6.109993934631348
I1005 22:12:39.415053 139766012786432 logging_writer.py:48] [53] global_step=53, grad_norm=2.7685883045196533, loss=6.098771572113037
I1005 22:12:40.315208 139766021179136 logging_writer.py:48] [54] global_step=54, grad_norm=1.5145004987716675, loss=6.043721675872803
I1005 22:12:41.215036 139766012786432 logging_writer.py:48] [55] global_step=55, grad_norm=1.4452967643737793, loss=6.031503677368164
I1005 22:12:42.108743 139766021179136 logging_writer.py:48] [56] global_step=56, grad_norm=1.406294345855713, loss=6.027649402618408
I1005 22:12:43.003504 139766012786432 logging_writer.py:48] [57] global_step=57, grad_norm=1.3467408418655396, loss=6.003870964050293
I1005 22:12:43.896808 139766021179136 logging_writer.py:48] [58] global_step=58, grad_norm=1.3256996870040894, loss=5.998232364654541
I1005 22:12:44.797608 139766012786432 logging_writer.py:48] [59] global_step=59, grad_norm=1.5330390930175781, loss=5.982335567474365
I1005 22:12:45.697386 139766021179136 logging_writer.py:48] [60] global_step=60, grad_norm=1.178654432296753, loss=5.96671199798584
I1005 22:12:46.587699 139766012786432 logging_writer.py:48] [61] global_step=61, grad_norm=0.7466261386871338, loss=5.952420711517334
I1005 22:12:47.480201 139766021179136 logging_writer.py:48] [62] global_step=62, grad_norm=0.5323776602745056, loss=5.927072525024414
I1005 22:12:48.378110 139766012786432 logging_writer.py:48] [63] global_step=63, grad_norm=0.7641686201095581, loss=5.918994903564453
I1005 22:12:49.285226 139766021179136 logging_writer.py:48] [64] global_step=64, grad_norm=0.8097935914993286, loss=5.893091678619385
I1005 22:12:50.176051 139766012786432 logging_writer.py:48] [65] global_step=65, grad_norm=1.1860876083374023, loss=5.892799377441406
I1005 22:12:51.066611 139766021179136 logging_writer.py:48] [66] global_step=66, grad_norm=1.543397068977356, loss=5.885105609893799
I1005 22:12:51.958636 139766012786432 logging_writer.py:48] [67] global_step=67, grad_norm=1.774491786956787, loss=5.845864772796631
I1005 22:12:52.859006 139766021179136 logging_writer.py:48] [68] global_step=68, grad_norm=1.180456280708313, loss=5.876891613006592
I1005 22:12:53.751144 139766012786432 logging_writer.py:48] [69] global_step=69, grad_norm=0.46522805094718933, loss=5.841731071472168
I1005 22:12:54.648731 139766021179136 logging_writer.py:48] [70] global_step=70, grad_norm=3.348452091217041, loss=5.844852924346924
I1005 22:12:55.545168 139766012786432 logging_writer.py:48] [71] global_step=71, grad_norm=7.633528709411621, loss=5.9061174392700195
I1005 22:12:56.448380 139766021179136 logging_writer.py:48] [72] global_step=72, grad_norm=8.666183471679688, loss=5.918850898742676
I1005 22:12:57.336835 139766012786432 logging_writer.py:48] [73] global_step=73, grad_norm=3.245394468307495, loss=5.8697190284729
I1005 22:12:58.233761 139766021179136 logging_writer.py:48] [74] global_step=74, grad_norm=4.790818691253662, loss=5.870203495025635
I1005 22:12:59.133212 139766012786432 logging_writer.py:48] [75] global_step=75, grad_norm=6.9376959800720215, loss=5.8881940841674805
I1005 22:13:00.035620 139766021179136 logging_writer.py:48] [76] global_step=76, grad_norm=1.1549670696258545, loss=5.841891288757324
I1005 22:13:00.927398 139766012786432 logging_writer.py:48] [77] global_step=77, grad_norm=4.561793327331543, loss=5.858073711395264
I1005 22:13:01.827091 139766021179136 logging_writer.py:48] [78] global_step=78, grad_norm=2.7619988918304443, loss=5.843212127685547
I1005 22:13:02.714465 139766012786432 logging_writer.py:48] [79] global_step=79, grad_norm=2.565322160720825, loss=5.8484416007995605
I1005 22:13:03.615728 139766021179136 logging_writer.py:48] [80] global_step=80, grad_norm=3.347857713699341, loss=5.85396671295166
I1005 22:13:04.515750 139766012786432 logging_writer.py:48] [81] global_step=81, grad_norm=0.6196226477622986, loss=5.832937240600586
I1005 22:13:05.414074 139766021179136 logging_writer.py:48] [82] global_step=82, grad_norm=4.693092346191406, loss=5.869611740112305
I1005 22:13:06.317306 139766012786432 logging_writer.py:48] [83] global_step=83, grad_norm=4.590268135070801, loss=5.852445125579834
I1005 22:13:07.207305 139766021179136 logging_writer.py:48] [84] global_step=84, grad_norm=0.9099304676055908, loss=5.814778804779053
I1005 22:13:08.108939 139766012786432 logging_writer.py:48] [85] global_step=85, grad_norm=5.619983673095703, loss=5.825607776641846
I1005 22:13:09.001219 139766021179136 logging_writer.py:48] [86] global_step=86, grad_norm=3.566513776779175, loss=5.841541290283203
I1005 22:13:09.901108 139766012786432 logging_writer.py:48] [87] global_step=87, grad_norm=2.202122688293457, loss=5.8446760177612305
I1005 22:13:10.788613 139766021179136 logging_writer.py:48] [88] global_step=88, grad_norm=4.780686378479004, loss=5.833339691162109
I1005 22:13:11.676106 139766012786432 logging_writer.py:48] [89] global_step=89, grad_norm=1.8760956525802612, loss=5.83237886428833
I1005 22:13:12.568684 139766021179136 logging_writer.py:48] [90] global_step=90, grad_norm=2.904179096221924, loss=5.842647552490234
I1005 22:13:13.465889 139766012786432 logging_writer.py:48] [91] global_step=91, grad_norm=4.919486045837402, loss=5.85440731048584
I1005 22:13:14.356731 139766021179136 logging_writer.py:48] [92] global_step=92, grad_norm=2.246640682220459, loss=5.844020843505859
I1005 22:13:15.245921 139766012786432 logging_writer.py:48] [93] global_step=93, grad_norm=2.972881317138672, loss=5.79671573638916
I1005 22:13:16.141335 139766021179136 logging_writer.py:48] [94] global_step=94, grad_norm=6.561995506286621, loss=5.876070022583008
I1005 22:13:17.035933 139766012786432 logging_writer.py:48] [95] global_step=95, grad_norm=4.583015441894531, loss=5.8301100730896
I1005 22:13:17.921891 139766021179136 logging_writer.py:48] [96] global_step=96, grad_norm=2.284100294113159, loss=5.8231611251831055
I1005 22:13:18.810991 139766012786432 logging_writer.py:48] [97] global_step=97, grad_norm=6.626131057739258, loss=5.854617595672607
I1005 22:13:19.703323 139766021179136 logging_writer.py:48] [98] global_step=98, grad_norm=2.6889288425445557, loss=5.82315731048584
I1005 22:13:20.602203 139766012786432 logging_writer.py:48] [99] global_step=99, grad_norm=4.340303421020508, loss=5.83104133605957
I1005 22:13:21.489667 139766021179136 logging_writer.py:48] [100] global_step=100, grad_norm=5.269335746765137, loss=5.84014368057251
I1005 22:18:27.902496 139766012786432 logging_writer.py:48] [500] global_step=500, grad_norm=0.9220912456512451, loss=5.49644660949707
I1005 22:25:29.907506 139766021179136 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6871592402458191, loss=3.5782949924468994
I1005 22:31:58.911156 139766071535360 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9882235527038574, loss=2.7662277221679688
I1005 22:35:30.573927 139940046329664 spec.py:321] Evaluating on the training split.
I1005 22:36:20.028051 139940046329664 spec.py:333] Evaluating on the validation split.
I1005 22:37:09.971193 139940046329664 spec.py:349] Evaluating on the test split.
I1005 22:37:34.847535 139940046329664 submission_runner.py:381] Time since start: 1809.59s, 	Step: 1751, 	{'train/ctc_loss': Array(2.7256203, dtype=float32), 'train/wer': 0.6026188955943235, 'validation/ctc_loss': Array(3.2158542, dtype=float32), 'validation/wer': 0.6444152862063309, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.8580754, dtype=float32), 'test/wer': 0.596287043243353, 'test/num_examples': 2472, 'score': 1516.502548456192, 'total_duration': 1809.5943443775177, 'accumulated_submission_time': 1516.502548456192, 'accumulated_eval_time': 293.01696825027466, 'accumulated_logging_time': 0.03876447677612305}
I1005 22:37:34.899935 139766071535360 logging_writer.py:48] [1751] accumulated_eval_time=293.016968, accumulated_logging_time=0.038764, accumulated_submission_time=1516.502548, global_step=1751, preemption_count=0, score=1516.502548, test/ctc_loss=2.8580753803253174, test/num_examples=2472, test/wer=0.596287, total_duration=1809.594344, train/ctc_loss=2.7256202697753906, train/wer=0.602619, validation/ctc_loss=3.2158541679382324, validation/num_examples=5348, validation/wer=0.644415
I1005 22:40:44.721282 139766063142656 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.2277706861495972, loss=2.498420476913452
I1005 22:47:08.746856 139766071535360 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7655406594276428, loss=2.2276976108551025
I1005 22:54:13.781863 139766063142656 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8463326692581177, loss=2.169772148132324
I1005 23:00:44.965721 139766726895360 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.0308865308761597, loss=2.03836727142334
I1005 23:01:35.457501 139940046329664 spec.py:321] Evaluating on the training split.
I1005 23:02:28.279126 139940046329664 spec.py:333] Evaluating on the validation split.
I1005 23:03:19.360853 139940046329664 spec.py:349] Evaluating on the test split.
I1005 23:03:45.148648 139940046329664 submission_runner.py:381] Time since start: 3379.90s, 	Step: 3562, 	{'train/ctc_loss': Array(0.8245935, dtype=float32), 'train/wer': 0.2797645366646246, 'validation/ctc_loss': Array(1.1686907, dtype=float32), 'validation/wer': 0.34073652423081746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8596292, dtype=float32), 'test/wer': 0.28279812321004205, 'test/num_examples': 2472, 'score': 2957.0092322826385, 'total_duration': 3379.896076440811, 'accumulated_submission_time': 2957.0092322826385, 'accumulated_eval_time': 422.70243549346924, 'accumulated_logging_time': 0.10651803016662598}
I1005 23:03:45.183679 139766726895360 logging_writer.py:48] [3562] accumulated_eval_time=422.702435, accumulated_logging_time=0.106518, accumulated_submission_time=2957.009232, global_step=3562, preemption_count=0, score=2957.009232, test/ctc_loss=0.859629213809967, test/num_examples=2472, test/wer=0.282798, total_duration=3379.896076, train/ctc_loss=0.8245934844017029, train/wer=0.279765, validation/ctc_loss=1.1686906814575195, validation/num_examples=5348, validation/wer=0.340737
I1005 23:09:31.239701 139766718502656 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7150818109512329, loss=2.0543158054351807
I1005 23:16:05.425388 139766071535360 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8984445333480835, loss=1.9686079025268555
I1005 23:23:16.296489 139766063142656 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6409501433372498, loss=2.0077431201934814
I1005 23:27:45.585147 139940046329664 spec.py:321] Evaluating on the training split.
I1005 23:28:39.452828 139940046329664 spec.py:333] Evaluating on the validation split.
I1005 23:29:30.648762 139940046329664 spec.py:349] Evaluating on the test split.
I1005 23:29:57.091135 139940046329664 submission_runner.py:381] Time since start: 4951.84s, 	Step: 5335, 	{'train/ctc_loss': Array(0.6061016, dtype=float32), 'train/wer': 0.21673492839107905, 'validation/ctc_loss': Array(0.9558972, dtype=float32), 'validation/wer': 0.2871711256259105, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6753387, dtype=float32), 'test/wer': 0.22704283712144294, 'test/num_examples': 2472, 'score': 4397.359958410263, 'total_duration': 4951.837518215179, 'accumulated_submission_time': 4397.359958410263, 'accumulated_eval_time': 554.2018940448761, 'accumulated_logging_time': 0.15749287605285645}
I1005 23:29:57.121021 139766143215360 logging_writer.py:48] [5335] accumulated_eval_time=554.201894, accumulated_logging_time=0.157493, accumulated_submission_time=4397.359958, global_step=5335, preemption_count=0, score=4397.359958, test/ctc_loss=0.6753386855125427, test/num_examples=2472, test/wer=0.227043, total_duration=4951.837518, train/ctc_loss=0.6061015725135803, train/wer=0.216735, validation/ctc_loss=0.9558972120285034, validation/num_examples=5348, validation/wer=0.287171
I1005 23:32:04.877345 139766134822656 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.830278754234314, loss=1.9040253162384033
I1005 23:39:03.506618 139766143215360 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9020941257476807, loss=1.9058266878128052
I1005 23:45:47.111254 139765487855360 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7820577621459961, loss=1.8265331983566284
I1005 23:53:00.112402 139765479462656 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6821114420890808, loss=1.836199402809143
I1005 23:53:57.459646 139940046329664 spec.py:321] Evaluating on the training split.
I1005 23:54:51.627364 139940046329664 spec.py:333] Evaluating on the validation split.
I1005 23:55:42.641519 139940046329664 spec.py:349] Evaluating on the test split.
I1005 23:56:08.904347 139940046329664 submission_runner.py:381] Time since start: 6523.65s, 	Step: 7066, 	{'train/ctc_loss': Array(0.5741389, dtype=float32), 'train/wer': 0.20477227743509724, 'validation/ctc_loss': Array(0.903562, dtype=float32), 'validation/wer': 0.2718887784735019, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.618618, dtype=float32), 'test/wer': 0.2110170007921516, 'test/num_examples': 2472, 'score': 5837.648542642593, 'total_duration': 6523.650816202164, 'accumulated_submission_time': 5837.648542642593, 'accumulated_eval_time': 685.6399669647217, 'accumulated_logging_time': 0.20297455787658691}
I1005 23:56:08.938861 139765487855360 logging_writer.py:48] [7066] accumulated_eval_time=685.639967, accumulated_logging_time=0.202975, accumulated_submission_time=5837.648543, global_step=7066, preemption_count=0, score=5837.648543, test/ctc_loss=0.6186180114746094, test/num_examples=2472, test/wer=0.211017, total_duration=6523.650816, train/ctc_loss=0.574138879776001, train/wer=0.204772, validation/ctc_loss=0.9035620093345642, validation/num_examples=5348, validation/wer=0.271889
I1006 00:01:45.987912 139766143215360 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6433973908424377, loss=1.8006715774536133
I1006 00:09:13.200959 139766134822656 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8014573454856873, loss=1.8063322305679321
I1006 00:16:11.760252 139767382255360 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8512009978294373, loss=1.8448858261108398
I1006 00:20:09.552326 139940046329664 spec.py:321] Evaluating on the training split.
I1006 00:21:02.730728 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 00:21:53.845356 139940046329664 spec.py:349] Evaluating on the test split.
I1006 00:22:19.780506 139940046329664 submission_runner.py:381] Time since start: 8094.53s, 	Step: 8781, 	{'train/ctc_loss': Array(0.56861055, dtype=float32), 'train/wer': 0.20198308291113093, 'validation/ctc_loss': Array(0.89218795, dtype=float32), 'validation/wer': 0.2682032629354842, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.60752237, dtype=float32), 'test/wer': 0.20583754798610687, 'test/num_examples': 2472, 'score': 7278.211523532867, 'total_duration': 8094.526681423187, 'accumulated_submission_time': 7278.211523532867, 'accumulated_eval_time': 815.8612408638, 'accumulated_logging_time': 0.25385284423828125}
I1006 00:22:19.817772 139767382255360 logging_writer.py:48] [8781] accumulated_eval_time=815.861241, accumulated_logging_time=0.253853, accumulated_submission_time=7278.211524, global_step=8781, preemption_count=0, score=7278.211524, test/ctc_loss=0.6075223684310913, test/num_examples=2472, test/wer=0.205838, total_duration=8094.526681, train/ctc_loss=0.5686105489730835, train/wer=0.201983, validation/ctc_loss=0.8921879529953003, validation/num_examples=5348, validation/wer=0.268203
I1006 00:25:06.901736 139767373862656 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.0235596895217896, loss=1.9138215780258179
I1006 00:32:06.300342 139767382255360 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7213912606239319, loss=1.7911697626113892
I1006 00:39:14.847612 139767373862656 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8737516403198242, loss=1.8486766815185547
I1006 00:46:20.764750 139767382255360 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.8399879932403564, loss=1.79802668094635
I1006 00:46:20.776958 139940046329664 spec.py:321] Evaluating on the training split.
I1006 00:47:14.537419 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 00:48:06.490074 139940046329664 spec.py:349] Evaluating on the test split.
I1006 00:48:32.672629 139940046329664 submission_runner.py:381] Time since start: 9667.42s, 	Step: 10501, 	{'train/ctc_loss': Array(0.5637794, dtype=float32), 'train/wer': 0.1968461091165158, 'validation/ctc_loss': Array(0.8574523, dtype=float32), 'validation/wer': 0.25825623016141014, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5773397, dtype=float32), 'test/wer': 0.193386549671968, 'test/num_examples': 2472, 'score': 8719.117112636566, 'total_duration': 9667.419454574585, 'accumulated_submission_time': 8719.117112636566, 'accumulated_eval_time': 947.7506108283997, 'accumulated_logging_time': 0.3097681999206543}
I1006 00:48:32.708963 139766368495360 logging_writer.py:48] [10501] accumulated_eval_time=947.750611, accumulated_logging_time=0.309768, accumulated_submission_time=8719.117113, global_step=10501, preemption_count=0, score=8719.117113, test/ctc_loss=0.5773397088050842, test/num_examples=2472, test/wer=0.193387, total_duration=9667.419455, train/ctc_loss=0.5637794137001038, train/wer=0.196846, validation/ctc_loss=0.8574522733688354, validation/num_examples=5348, validation/wer=0.258256
I1006 00:55:20.754658 139766360102656 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6860331892967224, loss=1.7698472738265991
I1006 01:02:29.426812 139766040815360 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6559005975723267, loss=1.876125693321228
I1006 01:09:33.951912 139766032422656 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7743821740150452, loss=1.8083709478378296
I1006 01:12:33.267705 139940046329664 spec.py:321] Evaluating on the training split.
I1006 01:13:27.193998 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 01:14:18.656556 139940046329664 spec.py:349] Evaluating on the test split.
I1006 01:14:44.877446 139940046329664 submission_runner.py:381] Time since start: 11239.62s, 	Step: 12199, 	{'train/ctc_loss': Array(0.49871895, dtype=float32), 'train/wer': 0.1786633484355391, 'validation/ctc_loss': Array(0.7981695, dtype=float32), 'validation/wer': 0.24418952425976131, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.536767, dtype=float32), 'test/wer': 0.18266203562651068, 'test/num_examples': 2472, 'score': 10159.62468123436, 'total_duration': 11239.621859788895, 'accumulated_submission_time': 10159.62468123436, 'accumulated_eval_time': 1079.3516917228699, 'accumulated_logging_time': 0.363588809967041}
I1006 01:14:44.911862 139767382255360 logging_writer.py:48] [12199] accumulated_eval_time=1079.351692, accumulated_logging_time=0.363589, accumulated_submission_time=10159.624681, global_step=12199, preemption_count=0, score=10159.624681, test/ctc_loss=0.5367670059204102, test/num_examples=2472, test/wer=0.182662, total_duration=11239.621860, train/ctc_loss=0.4987189471721649, train/wer=0.178663, validation/ctc_loss=0.7981694936752319, validation/num_examples=5348, validation/wer=0.244190
I1006 01:18:37.441588 139766296815360 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.7672829031944275, loss=1.7652299404144287
I1006 01:25:41.547771 139766288422656 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8523078560829163, loss=1.7926381826400757
I1006 01:32:56.838681 139766296815360 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7626497149467468, loss=1.7413413524627686
I1006 01:38:45.737432 139940046329664 spec.py:321] Evaluating on the training split.
I1006 01:39:41.141230 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 01:40:33.413347 139940046329664 spec.py:349] Evaluating on the test split.
I1006 01:40:59.787303 139940046329664 submission_runner.py:381] Time since start: 12814.53s, 	Step: 13928, 	{'train/ctc_loss': Array(0.4494891, dtype=float32), 'train/wer': 0.16481447775553187, 'validation/ctc_loss': Array(0.7822567, dtype=float32), 'validation/wer': 0.2379376549701396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5177803, dtype=float32), 'test/wer': 0.17717790912599274, 'test/num_examples': 2472, 'score': 11600.400096654892, 'total_duration': 12814.532217264175, 'accumulated_submission_time': 11600.400096654892, 'accumulated_eval_time': 1213.3933968544006, 'accumulated_logging_time': 0.4132654666900635}
I1006 01:40:59.836295 139767054575360 logging_writer.py:48] [13928] accumulated_eval_time=1213.393397, accumulated_logging_time=0.413265, accumulated_submission_time=11600.400097, global_step=13928, preemption_count=0, score=11600.400097, test/ctc_loss=0.5177803039550781, test/num_examples=2472, test/wer=0.177178, total_duration=12814.532217, train/ctc_loss=0.4494890868663788, train/wer=0.164814, validation/ctc_loss=0.7822567224502563, validation/num_examples=5348, validation/wer=0.237938
I1006 01:41:55.113057 139767046182656 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6216216683387756, loss=1.752335786819458
I1006 01:48:50.692190 139766726895360 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.928392767906189, loss=1.7896926403045654
I1006 01:55:39.765431 139766718502656 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7185155749320984, loss=1.7415868043899536
I1006 02:03:00.647168 139766726895360 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.833633303642273, loss=1.9268347024917603
I1006 02:04:59.861577 139940046329664 spec.py:321] Evaluating on the training split.
I1006 02:05:53.866447 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 02:06:45.436513 139940046329664 spec.py:349] Evaluating on the test split.
I1006 02:07:11.255189 139940046329664 submission_runner.py:381] Time since start: 14386.00s, 	Step: 15658, 	{'train/ctc_loss': Array(0.45198792, dtype=float32), 'train/wer': 0.16214907777346266, 'validation/ctc_loss': Array(0.787391, dtype=float32), 'validation/wer': 0.23738772202336733, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5246585, dtype=float32), 'test/wer': 0.17776694493530762, 'test/num_examples': 2472, 'score': 13040.367740154266, 'total_duration': 14386.001607179642, 'accumulated_submission_time': 13040.367740154266, 'accumulated_eval_time': 1344.7803456783295, 'accumulated_logging_time': 0.4857804775238037}
I1006 02:07:11.294390 139766798575360 logging_writer.py:48] [15658] accumulated_eval_time=1344.780346, accumulated_logging_time=0.485780, accumulated_submission_time=13040.367740, global_step=15658, preemption_count=0, score=13040.367740, test/ctc_loss=0.5246585011482239, test/num_examples=2472, test/wer=0.177767, total_duration=14386.001607, train/ctc_loss=0.4519879221916199, train/wer=0.162149, validation/ctc_loss=0.7873910069465637, validation/num_examples=5348, validation/wer=0.237388
I1006 02:11:39.387055 139766790182656 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.9288259744644165, loss=1.7048907279968262
I1006 02:19:04.986359 139766798575360 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.8140977621078491, loss=1.7135343551635742
I1006 02:25:53.068474 139766790182656 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.960625946521759, loss=1.7620874643325806
I1006 02:31:11.369779 139940046329664 spec.py:321] Evaluating on the training split.
I1006 02:32:05.271547 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 02:32:56.232782 139940046329664 spec.py:349] Evaluating on the test split.
I1006 02:33:22.818774 139940046329664 submission_runner.py:381] Time since start: 15957.56s, 	Step: 17355, 	{'train/ctc_loss': Array(0.4404222, dtype=float32), 'train/wer': 0.16333184960341507, 'validation/ctc_loss': Array(0.7596358, dtype=float32), 'validation/wer': 0.22991056353655123, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.497719, dtype=float32), 'test/wer': 0.1716125363069486, 'test/num_examples': 2472, 'score': 14480.394287109375, 'total_duration': 15957.563787937164, 'accumulated_submission_time': 14480.394287109375, 'accumulated_eval_time': 1476.2212626934052, 'accumulated_logging_time': 0.5393185615539551}
I1006 02:33:22.857568 139766952175360 logging_writer.py:48] [17355] accumulated_eval_time=1476.221263, accumulated_logging_time=0.539319, accumulated_submission_time=14480.394287, global_step=17355, preemption_count=0, score=14480.394287, test/ctc_loss=0.4977189898490906, test/num_examples=2472, test/wer=0.171613, total_duration=15957.563788, train/ctc_loss=0.4404222071170807, train/wer=0.163332, validation/ctc_loss=0.7596358060836792, validation/num_examples=5348, validation/wer=0.229911
I1006 02:35:13.567720 139766943782656 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8558117747306824, loss=1.7217414379119873
I1006 02:42:05.958669 139765641455360 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.8843591809272766, loss=1.660866618156433
I1006 02:49:28.758706 139765633062656 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7633025050163269, loss=1.7458370923995972
I1006 02:56:17.286343 139766952175360 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6946806907653809, loss=1.6842916011810303
I1006 02:57:23.277218 139940046329664 spec.py:321] Evaluating on the training split.
I1006 02:58:17.757794 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 02:59:09.121047 139940046329664 spec.py:349] Evaluating on the test split.
I1006 02:59:36.180452 139940046329664 submission_runner.py:381] Time since start: 17530.93s, 	Step: 19075, 	{'train/ctc_loss': Array(0.45637897, dtype=float32), 'train/wer': 0.16236812184351782, 'validation/ctc_loss': Array(0.76461565, dtype=float32), 'validation/wer': 0.2316375459483449, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50437593, dtype=float32), 'test/wer': 0.17088131944021287, 'test/num_examples': 2472, 'score': 15920.762355089188, 'total_duration': 17530.92565011978, 'accumulated_submission_time': 15920.762355089188, 'accumulated_eval_time': 1609.1165940761566, 'accumulated_logging_time': 0.5944809913635254}
I1006 02:59:36.218140 139767382255360 logging_writer.py:48] [19075] accumulated_eval_time=1609.116594, accumulated_logging_time=0.594481, accumulated_submission_time=15920.762355, global_step=19075, preemption_count=0, score=15920.762355, test/ctc_loss=0.5043759346008301, test/num_examples=2472, test/wer=0.170881, total_duration=17530.925650, train/ctc_loss=0.4563789665699005, train/wer=0.162368, validation/ctc_loss=0.7646156549453735, validation/num_examples=5348, validation/wer=0.231638
I1006 03:05:22.508814 139767373862656 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.9240291118621826, loss=1.7419989109039307
I1006 03:12:10.978314 139766399215360 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.8490087985992432, loss=1.7231417894363403
I1006 03:19:36.817064 139766390822656 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.8668757677078247, loss=1.7205151319503784
I1006 03:23:36.413982 139940046329664 spec.py:321] Evaluating on the training split.
I1006 03:24:32.368902 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 03:25:25.062253 139940046329664 spec.py:349] Evaluating on the test split.
I1006 03:25:51.349167 139940046329664 submission_runner.py:381] Time since start: 19106.10s, 	Step: 20796, 	{'train/ctc_loss': Array(0.4362161, dtype=float32), 'train/wer': 0.1533753660273828, 'validation/ctc_loss': Array(0.71568817, dtype=float32), 'validation/wer': 0.21615259192080966, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4647225, dtype=float32), 'test/wer': 0.158653748502021, 'test/num_examples': 2472, 'score': 17360.907459020615, 'total_duration': 19106.095099449158, 'accumulated_submission_time': 17360.907459020615, 'accumulated_eval_time': 1744.044807434082, 'accumulated_logging_time': 0.647216796875}
I1006 03:25:51.381332 139766399215360 logging_writer.py:48] [20796] accumulated_eval_time=1744.044807, accumulated_logging_time=0.647217, accumulated_submission_time=17360.907459, global_step=20796, preemption_count=0, score=17360.907459, test/ctc_loss=0.46472251415252686, test/num_examples=2472, test/wer=0.158654, total_duration=19106.095099, train/ctc_loss=0.4362160861492157, train/wer=0.153375, validation/ctc_loss=0.715688169002533, validation/num_examples=5348, validation/wer=0.216153
I1006 03:28:26.952522 139766390822656 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8634470105171204, loss=1.6286832094192505
I1006 03:35:45.505535 139766399215360 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8043419122695923, loss=1.5984784364700317
I1006 03:42:37.615128 139767382255360 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.0529117584228516, loss=1.6226043701171875
I1006 03:49:52.775164 139940046329664 spec.py:321] Evaluating on the training split.
I1006 03:50:48.166940 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 03:51:39.831947 139940046329664 spec.py:349] Evaluating on the test split.
I1006 03:52:06.492844 139940046329664 submission_runner.py:381] Time since start: 20681.24s, 	Step: 22494, 	{'train/ctc_loss': Array(0.40687972, dtype=float32), 'train/wer': 0.1458843502417944, 'validation/ctc_loss': Array(0.6818462, dtype=float32), 'validation/wer': 0.20696774691506914, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43468878, dtype=float32), 'test/wer': 0.14715739443056486, 'test/num_examples': 2472, 'score': 18802.25049972534, 'total_duration': 20681.236469745636, 'accumulated_submission_time': 18802.25049972534, 'accumulated_eval_time': 1877.7530903816223, 'accumulated_logging_time': 0.6952602863311768}
I1006 03:52:06.530264 139766798575360 logging_writer.py:48] [22494] accumulated_eval_time=1877.753090, accumulated_logging_time=0.695260, accumulated_submission_time=18802.250500, global_step=22494, preemption_count=0, score=18802.250500, test/ctc_loss=0.4346887767314911, test/num_examples=2472, test/wer=0.147157, total_duration=20681.236470, train/ctc_loss=0.406879723072052, train/wer=0.145884, validation/ctc_loss=0.6818462014198303, validation/num_examples=5348, validation/wer=0.206968
I1006 03:52:11.963018 139766790182656 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8029245138168335, loss=1.6483097076416016
I1006 03:58:42.911082 139766470895360 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.8446764349937439, loss=1.6091134548187256
I1006 04:06:10.285714 139766462502656 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.7883581519126892, loss=1.6033731698989868
I1006 04:13:07.543713 139766798575360 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7952093482017517, loss=1.6892071962356567
I1006 04:16:06.840630 139940046329664 spec.py:321] Evaluating on the training split.
I1006 04:17:01.371476 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 04:17:53.920195 139940046329664 spec.py:349] Evaluating on the test split.
I1006 04:18:20.533900 139940046329664 submission_runner.py:381] Time since start: 22255.28s, 	Step: 24208, 	{'train/ctc_loss': Array(0.37764597, dtype=float32), 'train/wer': 0.1365915713018876, 'validation/ctc_loss': Array(0.6665316, dtype=float32), 'validation/wer': 0.20487414253876063, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42705417, dtype=float32), 'test/wer': 0.14707614811203867, 'test/num_examples': 2472, 'score': 20242.508354902267, 'total_duration': 22255.280361413956, 'accumulated_submission_time': 20242.508354902267, 'accumulated_eval_time': 2011.4397547245026, 'accumulated_logging_time': 0.7499113082885742}
I1006 04:18:20.568329 139766798575360 logging_writer.py:48] [24208] accumulated_eval_time=2011.439755, accumulated_logging_time=0.749911, accumulated_submission_time=20242.508355, global_step=24208, preemption_count=0, score=20242.508355, test/ctc_loss=0.42705416679382324, test/num_examples=2472, test/wer=0.147076, total_duration=22255.280361, train/ctc_loss=0.37764596939086914, train/wer=0.136592, validation/ctc_loss=0.6665316224098206, validation/num_examples=5348, validation/wer=0.204874
I1006 04:22:11.786410 139766790182656 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0273730754852295, loss=1.67042076587677
I1006 04:29:06.086587 139766470895360 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.0296344757080078, loss=1.62229323387146
I1006 04:36:21.830227 139766462502656 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8559656143188477, loss=1.6075143814086914
I1006 04:42:20.873723 139940046329664 spec.py:321] Evaluating on the training split.
I1006 04:43:15.672938 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 04:44:08.774786 139940046329664 spec.py:349] Evaluating on the test split.
I1006 04:44:35.493538 139940046329664 submission_runner.py:381] Time since start: 23830.24s, 	Step: 25927, 	{'train/ctc_loss': Array(0.3637589, dtype=float32), 'train/wer': 0.132757766243251, 'validation/ctc_loss': Array(0.6636227, dtype=float32), 'validation/wer': 0.20230778878715666, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4158733, dtype=float32), 'test/wer': 0.14287165112830824, 'test/num_examples': 2472, 'score': 21682.76216006279, 'total_duration': 23830.239624500275, 'accumulated_submission_time': 21682.76216006279, 'accumulated_eval_time': 2146.0527267456055, 'accumulated_logging_time': 0.801091194152832}
I1006 04:44:35.528683 139766470895360 logging_writer.py:48] [25927] accumulated_eval_time=2146.052727, accumulated_logging_time=0.801091, accumulated_submission_time=21682.762160, global_step=25927, preemption_count=0, score=21682.762160, test/ctc_loss=0.41587328910827637, test/num_examples=2472, test/wer=0.142872, total_duration=23830.239625, train/ctc_loss=0.3637588918209076, train/wer=0.132758, validation/ctc_loss=0.6636226773262024, validation/num_examples=5348, validation/wer=0.202308
I1006 04:45:31.729826 139766462502656 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8480309844017029, loss=1.5009453296661377
I1006 04:52:26.556159 139766470895360 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6501005291938782, loss=1.6257723569869995
I1006 04:59:33.716096 139766470895360 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.7184389233589172, loss=1.5643845796585083
I1006 05:06:48.447904 139766462502656 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8693349361419678, loss=1.5577551126480103
I1006 05:08:36.165897 139940046329664 spec.py:321] Evaluating on the training split.
I1006 05:09:32.238576 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 05:10:24.148649 139940046329664 spec.py:349] Evaluating on the test split.
I1006 05:10:51.032798 139940046329664 submission_runner.py:381] Time since start: 25405.78s, 	Step: 27621, 	{'train/ctc_loss': Array(0.34638444, dtype=float32), 'train/wer': 0.1265559317179941, 'validation/ctc_loss': Array(0.6427649, dtype=float32), 'validation/wer': 0.1971171936053411, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40922084, dtype=float32), 'test/wer': 0.13990616050210225, 'test/num_examples': 2472, 'score': 23123.348477840424, 'total_duration': 25405.778505802155, 'accumulated_submission_time': 23123.348477840424, 'accumulated_eval_time': 2280.912257909775, 'accumulated_logging_time': 0.8531873226165771}
I1006 05:10:51.067550 139766470895360 logging_writer.py:48] [27621] accumulated_eval_time=2280.912258, accumulated_logging_time=0.853187, accumulated_submission_time=23123.348478, global_step=27621, preemption_count=0, score=23123.348478, test/ctc_loss=0.4092208445072174, test/num_examples=2472, test/wer=0.139906, total_duration=25405.778506, train/ctc_loss=0.3463844358921051, train/wer=0.126556, validation/ctc_loss=0.6427649259567261, validation/num_examples=5348, validation/wer=0.197117
I1006 05:15:44.241732 139766470895360 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.976617693901062, loss=1.5629311800003052
I1006 05:23:01.646824 139766462502656 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7976651787757874, loss=1.582341194152832
I1006 05:30:18.937951 139766470895360 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.0151722431182861, loss=1.5562477111816406
I1006 05:34:51.148111 139940046329664 spec.py:321] Evaluating on the training split.
I1006 05:35:44.734598 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 05:36:36.920667 139940046329664 spec.py:349] Evaluating on the test split.
I1006 05:37:03.983490 139940046329664 submission_runner.py:381] Time since start: 26978.73s, 	Step: 29326, 	{'train/ctc_loss': Array(0.35499054, dtype=float32), 'train/wer': 0.12919808459005028, 'validation/ctc_loss': Array(0.6285142, dtype=float32), 'validation/wer': 0.19124159422666887, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39048994, dtype=float32), 'test/wer': 0.13304084658663903, 'test/num_examples': 2472, 'score': 24563.378422498703, 'total_duration': 26978.729504823685, 'accumulated_submission_time': 24563.378422498703, 'accumulated_eval_time': 2413.7405927181244, 'accumulated_logging_time': 0.9040446281433105}
I1006 05:37:04.026213 139766470895360 logging_writer.py:48] [29326] accumulated_eval_time=2413.740593, accumulated_logging_time=0.904045, accumulated_submission_time=24563.378422, global_step=29326, preemption_count=0, score=24563.378422, test/ctc_loss=0.39048993587493896, test/num_examples=2472, test/wer=0.133041, total_duration=26978.729505, train/ctc_loss=0.35499054193496704, train/wer=0.129198, validation/ctc_loss=0.6285141706466675, validation/num_examples=5348, validation/wer=0.191242
I1006 05:39:16.841599 139766462502656 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.8137212991714478, loss=1.5674118995666504
I1006 05:46:25.824123 139766470895360 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7580814957618713, loss=1.5332920551300049
I1006 05:53:22.892001 139766462502656 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0354703664779663, loss=1.4911670684814453
I1006 06:00:43.026859 139766798575360 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.8036682605743408, loss=1.4731518030166626
I1006 06:01:03.988929 139940046329664 spec.py:321] Evaluating on the training split.
I1006 06:01:57.873612 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 06:02:49.054291 139940046329664 spec.py:349] Evaluating on the test split.
I1006 06:03:15.618273 139940046329664 submission_runner.py:381] Time since start: 28550.37s, 	Step: 31029, 	{'train/ctc_loss': Array(0.32396176, dtype=float32), 'train/wer': 0.1144932962197299, 'validation/ctc_loss': Array(0.6037779, dtype=float32), 'validation/wer': 0.1812077299346834, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37634137, dtype=float32), 'test/wer': 0.12652082952491214, 'test/num_examples': 2472, 'score': 26003.289304971695, 'total_duration': 28550.36519265175, 'accumulated_submission_time': 26003.289304971695, 'accumulated_eval_time': 2545.3637714385986, 'accumulated_logging_time': 0.9640021324157715}
I1006 06:03:15.659340 139766660335360 logging_writer.py:48] [31029] accumulated_eval_time=2545.363771, accumulated_logging_time=0.964002, accumulated_submission_time=26003.289305, global_step=31029, preemption_count=0, score=26003.289305, test/ctc_loss=0.3763413727283478, test/num_examples=2472, test/wer=0.126521, total_duration=28550.365193, train/ctc_loss=0.3239617645740509, train/wer=0.114493, validation/ctc_loss=0.6037778854370117, validation/num_examples=5348, validation/wer=0.181208
I1006 06:09:35.958566 139766651942656 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.8109095096588135, loss=1.4944472312927246
I1006 06:16:58.707451 139766660335360 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6835333704948425, loss=1.4981706142425537
I1006 06:23:47.970541 139766651942656 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7875575423240662, loss=1.4862521886825562
I1006 06:27:16.106440 139940046329664 spec.py:321] Evaluating on the training split.
I1006 06:28:11.222885 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 06:29:03.045147 139940046329664 spec.py:349] Evaluating on the test split.
I1006 06:29:29.165517 139940046329664 submission_runner.py:381] Time since start: 30123.91s, 	Step: 32735, 	{'train/ctc_loss': Array(0.3376016, dtype=float32), 'train/wer': 0.1194745953387545, 'validation/ctc_loss': Array(0.5861454, dtype=float32), 'validation/wer': 0.1798087777016662, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3692605, dtype=float32), 'test/wer': 0.12548493896370322, 'test/num_examples': 2472, 'score': 27443.686097621918, 'total_duration': 30123.912828445435, 'accumulated_submission_time': 27443.686097621918, 'accumulated_eval_time': 2678.417067527771, 'accumulated_logging_time': 1.0207006931304932}
I1006 06:29:29.207675 139767090415360 logging_writer.py:48] [32735] accumulated_eval_time=2678.417068, accumulated_logging_time=1.020701, accumulated_submission_time=27443.686098, global_step=32735, preemption_count=0, score=27443.686098, test/ctc_loss=0.3692604899406433, test/num_examples=2472, test/wer=0.125485, total_duration=30123.912828, train/ctc_loss=0.33760160207748413, train/wer=0.119475, validation/ctc_loss=0.5861454010009766, validation/num_examples=5348, validation/wer=0.179809
I1006 06:32:56.128847 139767090415360 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.9061597585678101, loss=1.406952142715454
I1006 06:39:51.612028 139767082022656 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7127915024757385, loss=1.449059247970581
I1006 06:47:27.387923 139766762735360 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.8376080989837646, loss=1.4119333028793335
I1006 06:53:29.872411 139940046329664 spec.py:321] Evaluating on the training split.
I1006 06:54:23.749631 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 06:55:14.522234 139940046329664 spec.py:349] Evaluating on the test split.
I1006 06:55:41.027066 139940046329664 submission_runner.py:381] Time since start: 31695.77s, 	Step: 34455, 	{'train/ctc_loss': Array(0.3137835, dtype=float32), 'train/wer': 0.10909342796551467, 'validation/ctc_loss': Array(0.5602287, dtype=float32), 'validation/wer': 0.1721193643932889, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34445658, dtype=float32), 'test/wer': 0.11803058923892511, 'test/num_examples': 2472, 'score': 28884.30054783821, 'total_duration': 31695.773631811142, 'accumulated_submission_time': 28884.30054783821, 'accumulated_eval_time': 2809.5651977062225, 'accumulated_logging_time': 1.0777907371520996}
I1006 06:55:41.072812 139766798575360 logging_writer.py:48] [34455] accumulated_eval_time=2809.565198, accumulated_logging_time=1.077791, accumulated_submission_time=28884.300548, global_step=34455, preemption_count=0, score=28884.300548, test/ctc_loss=0.34445658326148987, test/num_examples=2472, test/wer=0.118031, total_duration=31695.773632, train/ctc_loss=0.3137834966182709, train/wer=0.109093, validation/ctc_loss=0.560228705406189, validation/num_examples=5348, validation/wer=0.172119
I1006 06:56:15.865586 139766790182656 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.9157050251960754, loss=1.4712042808532715
I1006 07:03:20.630846 139766798575360 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7813394665718079, loss=1.4277889728546143
I1006 07:10:14.536456 139766798575360 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7820999622344971, loss=1.3712800741195679
I1006 07:17:49.661653 139766790182656 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.1545788049697876, loss=1.4335044622421265
I1006 07:19:41.345598 139940046329664 spec.py:321] Evaluating on the training split.
I1006 07:20:36.435218 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 07:21:29.709923 139940046329664 spec.py:349] Evaluating on the test split.
I1006 07:21:56.248039 139940046329664 submission_runner.py:381] Time since start: 33270.99s, 	Step: 36136, 	{'train/ctc_loss': Array(0.23970518, dtype=float32), 'train/wer': 0.08809692198886873, 'validation/ctc_loss': Array(0.5367253, dtype=float32), 'validation/wer': 0.16284768786963694, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3270721, dtype=float32), 'test/wer': 0.11041374687709463, 'test/num_examples': 2472, 'score': 30324.511627197266, 'total_duration': 33270.994863033295, 'accumulated_submission_time': 30324.511627197266, 'accumulated_eval_time': 2944.4615342617035, 'accumulated_logging_time': 1.1506857872009277}
I1006 07:21:56.281887 139766368495360 logging_writer.py:48] [36136] accumulated_eval_time=2944.461534, accumulated_logging_time=1.150686, accumulated_submission_time=30324.511627, global_step=36136, preemption_count=0, score=30324.511627, test/ctc_loss=0.3270721137523651, test/num_examples=2472, test/wer=0.110414, total_duration=33270.994863, train/ctc_loss=0.2397051751613617, train/wer=0.088097, validation/ctc_loss=0.5367252826690674, validation/num_examples=5348, validation/wer=0.162848
I1006 07:26:35.553421 139766360102656 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.0346689224243164, loss=1.3870618343353271
I1006 07:34:12.912027 139766368495360 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.4954921007156372, loss=1.451443076133728
I1006 07:40:58.367573 139766368495360 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.0088720321655273, loss=1.3350850343704224
I1006 07:45:56.865785 139940046329664 spec.py:321] Evaluating on the training split.
I1006 07:46:51.695401 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 07:47:43.889121 139940046329664 spec.py:349] Evaluating on the test split.
I1006 07:48:10.054702 139940046329664 submission_runner.py:381] Time since start: 34844.80s, 	Step: 37831, 	{'train/ctc_loss': Array(0.26820892, dtype=float32), 'train/wer': 0.09595890266309054, 'validation/ctc_loss': Array(0.5272781, dtype=float32), 'validation/wer': 0.16069619581472083, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31610107, dtype=float32), 'test/wer': 0.10608738041557492, 'test/num_examples': 2472, 'score': 31765.045680999756, 'total_duration': 34844.80156493187, 'accumulated_submission_time': 31765.045680999756, 'accumulated_eval_time': 3077.6442205905914, 'accumulated_logging_time': 1.199528455734253}
I1006 07:48:10.092901 139766798575360 logging_writer.py:48] [37831] accumulated_eval_time=3077.644221, accumulated_logging_time=1.199528, accumulated_submission_time=31765.045681, global_step=37831, preemption_count=0, score=31765.045681, test/ctc_loss=0.31610107421875, test/num_examples=2472, test/wer=0.106087, total_duration=34844.801565, train/ctc_loss=0.26820892095565796, train/wer=0.095959, validation/ctc_loss=0.5272781252861023, validation/num_examples=5348, validation/wer=0.160696
I1006 07:50:19.108992 139766790182656 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.815945029258728, loss=1.3592710494995117
I1006 07:57:00.265805 139766040815360 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.97669517993927, loss=1.3480026721954346
I1006 08:04:29.234169 139766032422656 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.3132710456848145, loss=1.3800427913665771
I1006 08:11:17.721048 139766040815360 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.019500970840454, loss=1.358361005783081
I1006 08:12:10.796507 139940046329664 spec.py:321] Evaluating on the training split.
I1006 08:13:03.658155 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 08:13:55.192272 139940046329664 spec.py:349] Evaluating on the test split.
I1006 08:14:22.016201 139940046329664 submission_runner.py:381] Time since start: 36416.76s, 	Step: 39563, 	{'train/ctc_loss': Array(0.32121426, dtype=float32), 'train/wer': 0.11493629415880627, 'validation/ctc_loss': Array(0.49552232, dtype=float32), 'validation/wer': 0.15311290991712415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30072522, dtype=float32), 'test/wer': 0.10362967928015762, 'test/num_examples': 2472, 'score': 33205.69780421257, 'total_duration': 36416.76326060295, 'accumulated_submission_time': 33205.69780421257, 'accumulated_eval_time': 3208.8578763008118, 'accumulated_logging_time': 1.2537486553192139}
I1006 08:14:22.055823 139766952175360 logging_writer.py:48] [39563] accumulated_eval_time=3208.857876, accumulated_logging_time=1.253749, accumulated_submission_time=33205.697804, global_step=39563, preemption_count=0, score=33205.697804, test/ctc_loss=0.30072522163391113, test/num_examples=2472, test/wer=0.103630, total_duration=36416.763261, train/ctc_loss=0.3212142586708069, train/wer=0.114936, validation/ctc_loss=0.49552232027053833, validation/num_examples=5348, validation/wer=0.153113
I1006 08:20:25.169031 139766943782656 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9180527925491333, loss=1.383811354637146
I1006 08:27:19.323044 139766952175360 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.1238209009170532, loss=1.3311678171157837
I1006 08:34:50.910738 139766943782656 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.0212697982788086, loss=1.3369320631027222
I1006 08:38:22.054174 139940046329664 spec.py:321] Evaluating on the training split.
I1006 08:39:14.965883 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 08:40:06.355645 139940046329664 spec.py:349] Evaluating on the test split.
I1006 08:40:32.308112 139940046329664 submission_runner.py:381] Time since start: 37987.05s, 	Step: 41242, 	{'train/ctc_loss': Array(0.32201982, dtype=float32), 'train/wer': 0.1118759610740714, 'validation/ctc_loss': Array(0.47793457, dtype=float32), 'validation/wer': 0.14408243205433724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2893217, dtype=float32), 'test/wer': 0.09613470639611643, 'test/num_examples': 2472, 'score': 34645.64494895935, 'total_duration': 37987.054493665695, 'accumulated_submission_time': 34645.64494895935, 'accumulated_eval_time': 3339.1053099632263, 'accumulated_logging_time': 1.310318946838379}
I1006 08:40:32.342865 139766004975360 logging_writer.py:48] [41242] accumulated_eval_time=3339.105310, accumulated_logging_time=1.310319, accumulated_submission_time=34645.644949, global_step=41242, preemption_count=0, score=34645.644949, test/ctc_loss=0.2893216907978058, test/num_examples=2472, test/wer=0.096135, total_duration=37987.054494, train/ctc_loss=0.3220198154449463, train/wer=0.111876, validation/ctc_loss=0.4779345691204071, validation/num_examples=5348, validation/wer=0.144082
I1006 08:43:49.427889 139765996582656 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.0429844856262207, loss=1.2686299085617065
I1006 08:50:55.472979 139766004975360 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.1278281211853027, loss=1.312961220741272
I1006 08:57:55.480389 139766004975360 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.9727683663368225, loss=1.3440920114517212
I1006 09:04:32.496137 139940046329664 spec.py:321] Evaluating on the training split.
I1006 09:05:23.958716 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 09:06:15.455214 139940046329664 spec.py:349] Evaluating on the test split.
I1006 09:06:41.876127 139940046329664 submission_runner.py:381] Time since start: 39556.62s, 	Step: 42955, 	{'train/ctc_loss': Array(0.3502565, dtype=float32), 'train/wer': 0.12712999631473346, 'validation/ctc_loss': Array(0.46123832, dtype=float32), 'validation/wer': 0.14133276732047584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.275181, dtype=float32), 'test/wer': 0.09400199053480389, 'test/num_examples': 2472, 'score': 36085.74735164642, 'total_duration': 39556.61580777168, 'accumulated_submission_time': 36085.74735164642, 'accumulated_eval_time': 3468.471888780594, 'accumulated_logging_time': 1.3613312244415283}
I1006 09:06:41.913370 139766004975360 logging_writer.py:48] [42955] accumulated_eval_time=3468.471889, accumulated_logging_time=1.361331, accumulated_submission_time=36085.747352, global_step=42955, preemption_count=0, score=36085.747352, test/ctc_loss=0.27518099546432495, test/num_examples=2472, test/wer=0.094002, total_duration=39556.615808, train/ctc_loss=0.3502565026283264, train/wer=0.127130, validation/ctc_loss=0.4612383246421814, validation/num_examples=5348, validation/wer=0.141333
I1006 09:07:16.728820 139765996582656 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.007032871246338, loss=1.287527084350586
I1006 09:14:00.139445 139766952175360 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.9946527481079102, loss=1.225841760635376
I1006 09:21:19.225476 139766943782656 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.8468658924102783, loss=1.2745472192764282
I1006 09:28:26.741983 139765677295360 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.1852949857711792, loss=1.2402018308639526
I1006 09:30:42.393310 139940046329664 spec.py:321] Evaluating on the training split.
I1006 09:31:33.846033 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 09:32:26.697726 139940046329664 spec.py:349] Evaluating on the test split.
I1006 09:32:53.490847 139940046329664 submission_runner.py:381] Time since start: 41128.24s, 	Step: 44667, 	{'train/ctc_loss': Array(0.2957178, dtype=float32), 'train/wer': 0.10172565043067666, 'validation/ctc_loss': Array(0.4375282, dtype=float32), 'validation/wer': 0.13236982508273115, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25597444, dtype=float32), 'test/wer': 0.08488209128023887, 'test/num_examples': 2472, 'score': 37526.17189335823, 'total_duration': 41128.23708367348, 'accumulated_submission_time': 37526.17189335823, 'accumulated_eval_time': 3599.562591314316, 'accumulated_logging_time': 1.4192545413970947}
I1006 09:32:53.533849 139767382255360 logging_writer.py:48] [44667] accumulated_eval_time=3599.562591, accumulated_logging_time=1.419255, accumulated_submission_time=37526.171893, global_step=44667, preemption_count=0, score=37526.171893, test/ctc_loss=0.2559744417667389, test/num_examples=2472, test/wer=0.084882, total_duration=41128.237084, train/ctc_loss=0.2957178056240082, train/wer=0.101726, validation/ctc_loss=0.43752819299697876, validation/num_examples=5348, validation/wer=0.132370
I1006 09:37:22.453594 139767373862656 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.2106406688690186, loss=1.2383500337600708
I1006 09:44:32.655088 139766726895360 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.028526782989502, loss=1.267608404159546
I1006 09:51:42.612233 139766718502656 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.3329554796218872, loss=1.244438886642456
I1006 09:56:53.533097 139940046329664 spec.py:321] Evaluating on the training split.
I1006 09:57:45.407368 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 09:58:36.662073 139940046329664 spec.py:349] Evaluating on the test split.
I1006 09:59:02.842990 139940046329664 submission_runner.py:381] Time since start: 42697.59s, 	Step: 46341, 	{'train/ctc_loss': Array(0.25991777, dtype=float32), 'train/wer': 0.0945631478739158, 'validation/ctc_loss': Array(0.42294112, dtype=float32), 'validation/wer': 0.12815367249081033, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24309343, dtype=float32), 'test/wer': 0.0820587817114537, 'test/num_examples': 2472, 'score': 38966.12124705315, 'total_duration': 42697.58953118324, 'accumulated_submission_time': 38966.12124705315, 'accumulated_eval_time': 3728.865946292877, 'accumulated_logging_time': 1.4777500629425049}
I1006 09:59:02.888216 139766726895360 logging_writer.py:48] [46341] accumulated_eval_time=3728.865946, accumulated_logging_time=1.477750, accumulated_submission_time=38966.121247, global_step=46341, preemption_count=0, score=38966.121247, test/ctc_loss=0.24309343099594116, test/num_examples=2472, test/wer=0.082059, total_duration=42697.589531, train/ctc_loss=0.2599177658557892, train/wer=0.094563, validation/ctc_loss=0.422941118478775, validation/num_examples=5348, validation/wer=0.128154
I1006 10:01:08.534507 139767382255360 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.8761016726493835, loss=1.1663891077041626
I1006 10:08:19.287846 139767373862656 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.9833433628082275, loss=1.1955190896987915
I1006 10:15:34.940730 139766726895360 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.6716856956481934, loss=1.2003483772277832
I1006 10:22:34.598346 139766718502656 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9828718900680542, loss=1.173523187637329
I1006 10:23:03.428137 139940046329664 spec.py:321] Evaluating on the training split.
I1006 10:23:56.594111 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 10:24:48.636289 139940046329664 spec.py:349] Evaluating on the test split.
I1006 10:25:14.967620 139940046329664 submission_runner.py:381] Time since start: 44269.71s, 	Step: 48033, 	{'train/ctc_loss': Array(0.21460372, dtype=float32), 'train/wer': 0.07891331063959368, 'validation/ctc_loss': Array(0.40765736, dtype=float32), 'validation/wer': 0.12438132543488119, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23308186, dtype=float32), 'test/wer': 0.07832145105924888, 'test/num_examples': 2472, 'score': 40406.61080646515, 'total_duration': 44269.71370792389, 'accumulated_submission_time': 40406.61080646515, 'accumulated_eval_time': 3860.3984134197235, 'accumulated_logging_time': 1.5393648147583008}
I1006 10:25:15.001578 139766726895360 logging_writer.py:48] [48033] accumulated_eval_time=3860.398413, accumulated_logging_time=1.539365, accumulated_submission_time=40406.610806, global_step=48033, preemption_count=0, score=40406.610806, test/ctc_loss=0.2330818623304367, test/num_examples=2472, test/wer=0.078321, total_duration=44269.713708, train/ctc_loss=0.2146037220954895, train/wer=0.078913, validation/ctc_loss=0.40765735507011414, validation/num_examples=5348, validation/wer=0.124381
I1006 10:31:32.271502 139766726895360 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0807511806488037, loss=1.1228631734848022
I1006 10:38:21.437511 139766718502656 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.99409419298172, loss=1.1343721151351929
I1006 10:45:49.388651 139767382255360 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.0578641891479492, loss=1.1577461957931519
I1006 10:49:15.007691 139940046329664 spec.py:321] Evaluating on the training split.
I1006 10:50:09.015903 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 10:51:01.328096 139940046329664 spec.py:349] Evaluating on the test split.
I1006 10:51:27.869893 139940046329664 submission_runner.py:381] Time since start: 45842.62s, 	Step: 49766, 	{'train/ctc_loss': Array(0.22010417, dtype=float32), 'train/wer': 0.08053137236798515, 'validation/ctc_loss': Array(0.38535455, dtype=float32), 'validation/wer': 0.11605514766182018, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21998067, dtype=float32), 'test/wer': 0.07338573720878273, 'test/num_examples': 2472, 'score': 41846.56562137604, 'total_duration': 45842.61648106575, 'accumulated_submission_time': 41846.56562137604, 'accumulated_eval_time': 3993.2541365623474, 'accumulated_logging_time': 1.5888762474060059}
I1006 10:51:27.903073 139767382255360 logging_writer.py:48] [49766] accumulated_eval_time=3993.254137, accumulated_logging_time=1.588876, accumulated_submission_time=41846.565621, global_step=49766, preemption_count=0, score=41846.565621, test/ctc_loss=0.21998067200183868, test/num_examples=2472, test/wer=0.073386, total_duration=45842.616481, train/ctc_loss=0.2201041728258133, train/wer=0.080531, validation/ctc_loss=0.38535454869270325, validation/num_examples=5348, validation/wer=0.116055
I1006 10:54:27.903064 139767373862656 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.9840393662452698, loss=1.1285879611968994
I1006 11:01:53.707869 139767382255360 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.50319504737854, loss=1.1599500179290771
I1006 11:08:40.763848 139767373862656 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.9504820704460144, loss=1.1229327917099
I1006 11:15:28.381601 139940046329664 spec.py:321] Evaluating on the training split.
I1006 11:16:22.289205 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 11:17:14.297473 139940046329664 spec.py:349] Evaluating on the test split.
I1006 11:17:40.758600 139940046329664 submission_runner.py:381] Time since start: 47415.50s, 	Step: 51457, 	{'train/ctc_loss': Array(0.19126765, dtype=float32), 'train/wer': 0.06960788524308538, 'validation/ctc_loss': Array(0.37512258, dtype=float32), 'validation/wer': 0.11263977462397129, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2132377, dtype=float32), 'test/wer': 0.06993276867141958, 'test/num_examples': 2472, 'score': 43286.99607396126, 'total_duration': 47415.50420475006, 'accumulated_submission_time': 43286.99607396126, 'accumulated_eval_time': 4125.623663187027, 'accumulated_logging_time': 1.635772705078125}
I1006 11:17:40.799910 139767382255360 logging_writer.py:48] [51457] accumulated_eval_time=4125.623663, accumulated_logging_time=1.635773, accumulated_submission_time=43286.996074, global_step=51457, preemption_count=0, score=43286.996074, test/ctc_loss=0.2132377028465271, test/num_examples=2472, test/wer=0.069933, total_duration=47415.504205, train/ctc_loss=0.19126765429973602, train/wer=0.069608, validation/ctc_loss=0.3751225769519806, validation/num_examples=5348, validation/wer=0.112640
I1006 11:18:17.919806 139766726895360 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.2544071674346924, loss=1.093173861503601
I1006 11:25:05.929961 139766718502656 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.491793155670166, loss=1.0912055969238281
I1006 11:32:43.124516 139766726895360 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.106390118598938, loss=1.0828198194503784
I1006 11:39:26.875568 139766726895360 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.1474542617797852, loss=1.0839061737060547
I1006 11:41:41.308001 139940046329664 spec.py:321] Evaluating on the training split.
I1006 11:42:35.414051 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 11:43:26.824590 139940046329664 spec.py:349] Evaluating on the test split.
I1006 11:43:52.862121 139940046329664 submission_runner.py:381] Time since start: 48987.61s, 	Step: 53152, 	{'train/ctc_loss': Array(0.18736342, dtype=float32), 'train/wer': 0.0684754662124779, 'validation/ctc_loss': Array(0.35893753, dtype=float32), 'validation/wer': 0.10818242337118544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20606536, dtype=float32), 'test/wer': 0.06755631385452847, 'test/num_examples': 2472, 'score': 44727.45292019844, 'total_duration': 48987.60837483406, 'accumulated_submission_time': 44727.45292019844, 'accumulated_eval_time': 4257.1709525585175, 'accumulated_logging_time': 1.6938915252685547}
I1006 11:43:52.898658 139765713135360 logging_writer.py:48] [53152] accumulated_eval_time=4257.170953, accumulated_logging_time=1.693892, accumulated_submission_time=44727.452920, global_step=53152, preemption_count=0, score=44727.452920, test/ctc_loss=0.2060653567314148, test/num_examples=2472, test/wer=0.067556, total_duration=48987.608375, train/ctc_loss=0.1873634159564972, train/wer=0.068475, validation/ctc_loss=0.358937531709671, validation/num_examples=5348, validation/wer=0.108182
I1006 11:48:31.442839 139765704742656 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.425411343574524, loss=1.0830110311508179
I1006 11:55:15.284164 139765713135360 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.3913335800170898, loss=1.0756505727767944
I1006 12:02:42.841675 139765704742656 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.3600269556045532, loss=1.1171284914016724
I1006 12:07:53.100834 139940046329664 spec.py:321] Evaluating on the training split.
I1006 12:08:46.102171 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 12:09:37.716769 139940046329664 spec.py:349] Evaluating on the test split.
I1006 12:10:03.681724 139940046329664 submission_runner.py:381] Time since start: 50558.43s, 	Step: 54884, 	{'train/ctc_loss': Array(0.17216952, dtype=float32), 'train/wer': 0.06314975845410628, 'validation/ctc_loss': Array(0.3504784, dtype=float32), 'validation/wer': 0.10432324479734488, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19877562, dtype=float32), 'test/wer': 0.06507830113947961, 'test/num_examples': 2472, 'score': 46167.60228896141, 'total_duration': 50558.42852473259, 'accumulated_submission_time': 46167.60228896141, 'accumulated_eval_time': 4387.745643615723, 'accumulated_logging_time': 1.7477047443389893}
I1006 12:10:03.723276 139767382255360 logging_writer.py:48] [54884] accumulated_eval_time=4387.745644, accumulated_logging_time=1.747705, accumulated_submission_time=46167.602289, global_step=54884, preemption_count=0, score=46167.602289, test/ctc_loss=0.19877561926841736, test/num_examples=2472, test/wer=0.065078, total_duration=50558.428525, train/ctc_loss=0.1721695214509964, train/wer=0.063150, validation/ctc_loss=0.3504784107208252, validation/num_examples=5348, validation/wer=0.104323
I1006 12:11:32.523576 139767373862656 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.6404749155044556, loss=1.0174461603164673
I1006 12:18:42.259938 139767382255360 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.2250195741653442, loss=1.0490918159484863
I1006 12:25:28.730268 139767382255360 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.1134943962097168, loss=1.0314372777938843
I1006 12:32:51.698212 139767373862656 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.2674258947372437, loss=1.0406203269958496
I1006 12:34:03.968880 139940046329664 spec.py:321] Evaluating on the training split.
I1006 12:34:56.408329 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 12:35:48.742714 139940046329664 spec.py:349] Evaluating on the test split.
I1006 12:36:15.198361 139940046329664 submission_runner.py:381] Time since start: 52129.95s, 	Step: 56586, 	{'train/ctc_loss': Array(0.17619194, dtype=float32), 'train/wer': 0.06460973180866042, 'validation/ctc_loss': Array(0.3455636, dtype=float32), 'validation/wer': 0.10276992542137406, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19548336, dtype=float32), 'test/wer': 0.06355493266711353, 'test/num_examples': 2472, 'score': 47607.79773974419, 'total_duration': 52129.945148944855, 'accumulated_submission_time': 47607.79773974419, 'accumulated_eval_time': 4518.968811035156, 'accumulated_logging_time': 1.8048992156982422}
I1006 12:36:15.233524 139766798575360 logging_writer.py:48] [56586] accumulated_eval_time=4518.968811, accumulated_logging_time=1.804899, accumulated_submission_time=47607.797740, global_step=56586, preemption_count=0, score=47607.797740, test/ctc_loss=0.19548335671424866, test/num_examples=2472, test/wer=0.063555, total_duration=52129.945149, train/ctc_loss=0.17619194090366364, train/wer=0.064610, validation/ctc_loss=0.3455635905265808, validation/num_examples=5348, validation/wer=0.102770
I1006 12:41:43.182751 139766798575360 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.261051893234253, loss=1.0460312366485596
I1006 12:49:15.033460 139766790182656 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.3840452432632446, loss=0.9951763153076172
I1006 12:56:07.241239 139765713135360 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.6280148029327393, loss=1.03214430809021
I1006 13:00:15.548964 139940046329664 spec.py:321] Evaluating on the training split.
I1006 13:01:09.223771 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 13:02:01.944849 139940046329664 spec.py:349] Evaluating on the test split.
I1006 13:02:28.339263 139940046329664 submission_runner.py:381] Time since start: 53703.09s, 	Step: 58286, 	{'train/ctc_loss': Array(0.18434367, dtype=float32), 'train/wer': 0.065422189307224, 'validation/ctc_loss': Array(0.34211627, dtype=float32), 'validation/wer': 0.10136132524192226, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19359203, dtype=float32), 'test/wer': 0.06251904210590457, 'test/num_examples': 2472, 'score': 49048.06335878372, 'total_duration': 53703.08632874489, 'accumulated_submission_time': 49048.06335878372, 'accumulated_eval_time': 4651.7530863285065, 'accumulated_logging_time': 1.8548481464385986}
I1006 13:02:28.378630 139766470895360 logging_writer.py:48] [58286] accumulated_eval_time=4651.753086, accumulated_logging_time=1.854848, accumulated_submission_time=49048.063359, global_step=58286, preemption_count=0, score=49048.063359, test/ctc_loss=0.19359202682971954, test/num_examples=2472, test/wer=0.062519, total_duration=53703.086329, train/ctc_loss=0.18434366583824158, train/wer=0.065422, validation/ctc_loss=0.34211626648902893, validation/num_examples=5348, validation/wer=0.101361
I1006 13:05:13.900683 139766462502656 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.020658254623413, loss=1.08182692527771
I1006 13:12:11.289626 139766798575360 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.1541448831558228, loss=1.0523924827575684
I1006 13:19:36.695064 139766790182656 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.2471259832382202, loss=1.0480183362960815
I1006 13:26:28.582882 139940046329664 spec.py:321] Evaluating on the training split.
I1006 13:27:21.620595 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 13:28:12.999910 139940046329664 spec.py:349] Evaluating on the test split.
I1006 13:28:39.459009 139940046329664 submission_runner.py:381] Time since start: 55274.21s, 	Step: 59992, 	{'train/ctc_loss': Array(0.1628805, dtype=float32), 'train/wer': 0.060253996009594905, 'validation/ctc_loss': Array(0.34108773, dtype=float32), 'validation/wer': 0.10110083068818802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19300593, dtype=float32), 'test/wer': 0.06253935368553612, 'test/num_examples': 2472, 'score': 50488.21764969826, 'total_duration': 55274.20524048805, 'accumulated_submission_time': 50488.21764969826, 'accumulated_eval_time': 4782.622443914413, 'accumulated_logging_time': 1.910083532333374}
I1006 13:28:39.500615 139766952175360 logging_writer.py:48] [59992] accumulated_eval_time=4782.622444, accumulated_logging_time=1.910084, accumulated_submission_time=50488.217650, global_step=59992, preemption_count=0, score=50488.217650, test/ctc_loss=0.19300593435764313, test/num_examples=2472, test/wer=0.062539, total_duration=55274.205240, train/ctc_loss=0.16288049519062042, train/wer=0.060254, validation/ctc_loss=0.3410877287387848, validation/num_examples=5348, validation/wer=0.101101
I1006 13:28:45.225043 139940046329664 spec.py:321] Evaluating on the training split.
I1006 13:29:36.204940 139940046329664 spec.py:333] Evaluating on the validation split.
I1006 13:30:22.214393 139940046329664 spec.py:349] Evaluating on the test split.
I1006 13:30:45.639828 139940046329664 submission_runner.py:381] Time since start: 55400.39s, 	Step: 60000, 	{'train/ctc_loss': Array(0.1675994, dtype=float32), 'train/wer': 0.061647117885993336, 'validation/ctc_loss': Array(0.341086, dtype=float32), 'validation/wer': 0.10112012658105722, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19300595, dtype=float32), 'test/wer': 0.06253935368553612, 'test/num_examples': 2472, 'score': 50493.92419672012, 'total_duration': 55400.389587402344, 'accumulated_submission_time': 50493.92419672012, 'accumulated_eval_time': 4903.033866405487, 'accumulated_logging_time': 1.9686484336853027}
I1006 13:30:45.673539 139767382255360 logging_writer.py:48] [60000] accumulated_eval_time=4903.033866, accumulated_logging_time=1.968648, accumulated_submission_time=50493.924197, global_step=60000, preemption_count=0, score=50493.924197, test/ctc_loss=0.19300594925880432, test/num_examples=2472, test/wer=0.062539, total_duration=55400.389587, train/ctc_loss=0.1675993949174881, train/wer=0.061647, validation/ctc_loss=0.3410860002040863, validation/num_examples=5348, validation/wer=0.101120
I1006 13:30:45.697063 139767373862656 logging_writer.py:48] [60000] global_step=60000, preemption_count=0, score=50493.924197
I1006 13:30:46.247163 139940046329664 checkpoints.py:490] Saving checkpoint at step: 60000
I1006 13:30:47.958736 139940046329664 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/checkpoint_60000
I1006 13:30:47.993644 139940046329664 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/checkpoint_60000.
I1006 13:30:49.601395 139940046329664 submission_runner.py:549] Tuning trial 1/1
I1006 13:30:49.601700 139940046329664 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.002106913873888147, beta1=0.8231189937738506, beta2=0.8774571227688758, warmup_steps=1199, weight_decay=0.27590534177690645)
I1006 13:30:49.624456 139940046329664 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.94961, dtype=float32), 'train/wer': 1.3229025675197301, 'validation/ctc_loss': Array(30.927902, dtype=float32), 'validation/wer': 0.9735356829298883, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.94005, dtype=float32), 'test/wer': 1.0188288343184448, 'test/num_examples': 2472, 'score': 76.28481006622314, 'total_duration': 245.03448510169983, 'accumulated_submission_time': 76.28481006622314, 'accumulated_eval_time': 168.74962043762207, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1751, {'train/ctc_loss': Array(2.7256203, dtype=float32), 'train/wer': 0.6026188955943235, 'validation/ctc_loss': Array(3.2158542, dtype=float32), 'validation/wer': 0.6444152862063309, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.8580754, dtype=float32), 'test/wer': 0.596287043243353, 'test/num_examples': 2472, 'score': 1516.502548456192, 'total_duration': 1809.5943443775177, 'accumulated_submission_time': 1516.502548456192, 'accumulated_eval_time': 293.01696825027466, 'accumulated_logging_time': 0.03876447677612305, 'global_step': 1751, 'preemption_count': 0}), (3562, {'train/ctc_loss': Array(0.8245935, dtype=float32), 'train/wer': 0.2797645366646246, 'validation/ctc_loss': Array(1.1686907, dtype=float32), 'validation/wer': 0.34073652423081746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8596292, dtype=float32), 'test/wer': 0.28279812321004205, 'test/num_examples': 2472, 'score': 2957.0092322826385, 'total_duration': 3379.896076440811, 'accumulated_submission_time': 2957.0092322826385, 'accumulated_eval_time': 422.70243549346924, 'accumulated_logging_time': 0.10651803016662598, 'global_step': 3562, 'preemption_count': 0}), (5335, {'train/ctc_loss': Array(0.6061016, dtype=float32), 'train/wer': 0.21673492839107905, 'validation/ctc_loss': Array(0.9558972, dtype=float32), 'validation/wer': 0.2871711256259105, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6753387, dtype=float32), 'test/wer': 0.22704283712144294, 'test/num_examples': 2472, 'score': 4397.359958410263, 'total_duration': 4951.837518215179, 'accumulated_submission_time': 4397.359958410263, 'accumulated_eval_time': 554.2018940448761, 'accumulated_logging_time': 0.15749287605285645, 'global_step': 5335, 'preemption_count': 0}), (7066, {'train/ctc_loss': Array(0.5741389, dtype=float32), 'train/wer': 0.20477227743509724, 'validation/ctc_loss': Array(0.903562, dtype=float32), 'validation/wer': 0.2718887784735019, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.618618, dtype=float32), 'test/wer': 0.2110170007921516, 'test/num_examples': 2472, 'score': 5837.648542642593, 'total_duration': 6523.650816202164, 'accumulated_submission_time': 5837.648542642593, 'accumulated_eval_time': 685.6399669647217, 'accumulated_logging_time': 0.20297455787658691, 'global_step': 7066, 'preemption_count': 0}), (8781, {'train/ctc_loss': Array(0.56861055, dtype=float32), 'train/wer': 0.20198308291113093, 'validation/ctc_loss': Array(0.89218795, dtype=float32), 'validation/wer': 0.2682032629354842, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.60752237, dtype=float32), 'test/wer': 0.20583754798610687, 'test/num_examples': 2472, 'score': 7278.211523532867, 'total_duration': 8094.526681423187, 'accumulated_submission_time': 7278.211523532867, 'accumulated_eval_time': 815.8612408638, 'accumulated_logging_time': 0.25385284423828125, 'global_step': 8781, 'preemption_count': 0}), (10501, {'train/ctc_loss': Array(0.5637794, dtype=float32), 'train/wer': 0.1968461091165158, 'validation/ctc_loss': Array(0.8574523, dtype=float32), 'validation/wer': 0.25825623016141014, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5773397, dtype=float32), 'test/wer': 0.193386549671968, 'test/num_examples': 2472, 'score': 8719.117112636566, 'total_duration': 9667.419454574585, 'accumulated_submission_time': 8719.117112636566, 'accumulated_eval_time': 947.7506108283997, 'accumulated_logging_time': 0.3097681999206543, 'global_step': 10501, 'preemption_count': 0}), (12199, {'train/ctc_loss': Array(0.49871895, dtype=float32), 'train/wer': 0.1786633484355391, 'validation/ctc_loss': Array(0.7981695, dtype=float32), 'validation/wer': 0.24418952425976131, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.536767, dtype=float32), 'test/wer': 0.18266203562651068, 'test/num_examples': 2472, 'score': 10159.62468123436, 'total_duration': 11239.621859788895, 'accumulated_submission_time': 10159.62468123436, 'accumulated_eval_time': 1079.3516917228699, 'accumulated_logging_time': 0.363588809967041, 'global_step': 12199, 'preemption_count': 0}), (13928, {'train/ctc_loss': Array(0.4494891, dtype=float32), 'train/wer': 0.16481447775553187, 'validation/ctc_loss': Array(0.7822567, dtype=float32), 'validation/wer': 0.2379376549701396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5177803, dtype=float32), 'test/wer': 0.17717790912599274, 'test/num_examples': 2472, 'score': 11600.400096654892, 'total_duration': 12814.532217264175, 'accumulated_submission_time': 11600.400096654892, 'accumulated_eval_time': 1213.3933968544006, 'accumulated_logging_time': 0.4132654666900635, 'global_step': 13928, 'preemption_count': 0}), (15658, {'train/ctc_loss': Array(0.45198792, dtype=float32), 'train/wer': 0.16214907777346266, 'validation/ctc_loss': Array(0.787391, dtype=float32), 'validation/wer': 0.23738772202336733, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5246585, dtype=float32), 'test/wer': 0.17776694493530762, 'test/num_examples': 2472, 'score': 13040.367740154266, 'total_duration': 14386.001607179642, 'accumulated_submission_time': 13040.367740154266, 'accumulated_eval_time': 1344.7803456783295, 'accumulated_logging_time': 0.4857804775238037, 'global_step': 15658, 'preemption_count': 0}), (17355, {'train/ctc_loss': Array(0.4404222, dtype=float32), 'train/wer': 0.16333184960341507, 'validation/ctc_loss': Array(0.7596358, dtype=float32), 'validation/wer': 0.22991056353655123, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.497719, dtype=float32), 'test/wer': 0.1716125363069486, 'test/num_examples': 2472, 'score': 14480.394287109375, 'total_duration': 15957.563787937164, 'accumulated_submission_time': 14480.394287109375, 'accumulated_eval_time': 1476.2212626934052, 'accumulated_logging_time': 0.5393185615539551, 'global_step': 17355, 'preemption_count': 0}), (19075, {'train/ctc_loss': Array(0.45637897, dtype=float32), 'train/wer': 0.16236812184351782, 'validation/ctc_loss': Array(0.76461565, dtype=float32), 'validation/wer': 0.2316375459483449, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50437593, dtype=float32), 'test/wer': 0.17088131944021287, 'test/num_examples': 2472, 'score': 15920.762355089188, 'total_duration': 17530.92565011978, 'accumulated_submission_time': 15920.762355089188, 'accumulated_eval_time': 1609.1165940761566, 'accumulated_logging_time': 0.5944809913635254, 'global_step': 19075, 'preemption_count': 0}), (20796, {'train/ctc_loss': Array(0.4362161, dtype=float32), 'train/wer': 0.1533753660273828, 'validation/ctc_loss': Array(0.71568817, dtype=float32), 'validation/wer': 0.21615259192080966, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4647225, dtype=float32), 'test/wer': 0.158653748502021, 'test/num_examples': 2472, 'score': 17360.907459020615, 'total_duration': 19106.095099449158, 'accumulated_submission_time': 17360.907459020615, 'accumulated_eval_time': 1744.044807434082, 'accumulated_logging_time': 0.647216796875, 'global_step': 20796, 'preemption_count': 0}), (22494, {'train/ctc_loss': Array(0.40687972, dtype=float32), 'train/wer': 0.1458843502417944, 'validation/ctc_loss': Array(0.6818462, dtype=float32), 'validation/wer': 0.20696774691506914, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43468878, dtype=float32), 'test/wer': 0.14715739443056486, 'test/num_examples': 2472, 'score': 18802.25049972534, 'total_duration': 20681.236469745636, 'accumulated_submission_time': 18802.25049972534, 'accumulated_eval_time': 1877.7530903816223, 'accumulated_logging_time': 0.6952602863311768, 'global_step': 22494, 'preemption_count': 0}), (24208, {'train/ctc_loss': Array(0.37764597, dtype=float32), 'train/wer': 0.1365915713018876, 'validation/ctc_loss': Array(0.6665316, dtype=float32), 'validation/wer': 0.20487414253876063, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42705417, dtype=float32), 'test/wer': 0.14707614811203867, 'test/num_examples': 2472, 'score': 20242.508354902267, 'total_duration': 22255.280361413956, 'accumulated_submission_time': 20242.508354902267, 'accumulated_eval_time': 2011.4397547245026, 'accumulated_logging_time': 0.7499113082885742, 'global_step': 24208, 'preemption_count': 0}), (25927, {'train/ctc_loss': Array(0.3637589, dtype=float32), 'train/wer': 0.132757766243251, 'validation/ctc_loss': Array(0.6636227, dtype=float32), 'validation/wer': 0.20230778878715666, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4158733, dtype=float32), 'test/wer': 0.14287165112830824, 'test/num_examples': 2472, 'score': 21682.76216006279, 'total_duration': 23830.239624500275, 'accumulated_submission_time': 21682.76216006279, 'accumulated_eval_time': 2146.0527267456055, 'accumulated_logging_time': 0.801091194152832, 'global_step': 25927, 'preemption_count': 0}), (27621, {'train/ctc_loss': Array(0.34638444, dtype=float32), 'train/wer': 0.1265559317179941, 'validation/ctc_loss': Array(0.6427649, dtype=float32), 'validation/wer': 0.1971171936053411, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40922084, dtype=float32), 'test/wer': 0.13990616050210225, 'test/num_examples': 2472, 'score': 23123.348477840424, 'total_duration': 25405.778505802155, 'accumulated_submission_time': 23123.348477840424, 'accumulated_eval_time': 2280.912257909775, 'accumulated_logging_time': 0.8531873226165771, 'global_step': 27621, 'preemption_count': 0}), (29326, {'train/ctc_loss': Array(0.35499054, dtype=float32), 'train/wer': 0.12919808459005028, 'validation/ctc_loss': Array(0.6285142, dtype=float32), 'validation/wer': 0.19124159422666887, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39048994, dtype=float32), 'test/wer': 0.13304084658663903, 'test/num_examples': 2472, 'score': 24563.378422498703, 'total_duration': 26978.729504823685, 'accumulated_submission_time': 24563.378422498703, 'accumulated_eval_time': 2413.7405927181244, 'accumulated_logging_time': 0.9040446281433105, 'global_step': 29326, 'preemption_count': 0}), (31029, {'train/ctc_loss': Array(0.32396176, dtype=float32), 'train/wer': 0.1144932962197299, 'validation/ctc_loss': Array(0.6037779, dtype=float32), 'validation/wer': 0.1812077299346834, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37634137, dtype=float32), 'test/wer': 0.12652082952491214, 'test/num_examples': 2472, 'score': 26003.289304971695, 'total_duration': 28550.36519265175, 'accumulated_submission_time': 26003.289304971695, 'accumulated_eval_time': 2545.3637714385986, 'accumulated_logging_time': 0.9640021324157715, 'global_step': 31029, 'preemption_count': 0}), (32735, {'train/ctc_loss': Array(0.3376016, dtype=float32), 'train/wer': 0.1194745953387545, 'validation/ctc_loss': Array(0.5861454, dtype=float32), 'validation/wer': 0.1798087777016662, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3692605, dtype=float32), 'test/wer': 0.12548493896370322, 'test/num_examples': 2472, 'score': 27443.686097621918, 'total_duration': 30123.912828445435, 'accumulated_submission_time': 27443.686097621918, 'accumulated_eval_time': 2678.417067527771, 'accumulated_logging_time': 1.0207006931304932, 'global_step': 32735, 'preemption_count': 0}), (34455, {'train/ctc_loss': Array(0.3137835, dtype=float32), 'train/wer': 0.10909342796551467, 'validation/ctc_loss': Array(0.5602287, dtype=float32), 'validation/wer': 0.1721193643932889, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34445658, dtype=float32), 'test/wer': 0.11803058923892511, 'test/num_examples': 2472, 'score': 28884.30054783821, 'total_duration': 31695.773631811142, 'accumulated_submission_time': 28884.30054783821, 'accumulated_eval_time': 2809.5651977062225, 'accumulated_logging_time': 1.0777907371520996, 'global_step': 34455, 'preemption_count': 0}), (36136, {'train/ctc_loss': Array(0.23970518, dtype=float32), 'train/wer': 0.08809692198886873, 'validation/ctc_loss': Array(0.5367253, dtype=float32), 'validation/wer': 0.16284768786963694, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3270721, dtype=float32), 'test/wer': 0.11041374687709463, 'test/num_examples': 2472, 'score': 30324.511627197266, 'total_duration': 33270.994863033295, 'accumulated_submission_time': 30324.511627197266, 'accumulated_eval_time': 2944.4615342617035, 'accumulated_logging_time': 1.1506857872009277, 'global_step': 36136, 'preemption_count': 0}), (37831, {'train/ctc_loss': Array(0.26820892, dtype=float32), 'train/wer': 0.09595890266309054, 'validation/ctc_loss': Array(0.5272781, dtype=float32), 'validation/wer': 0.16069619581472083, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31610107, dtype=float32), 'test/wer': 0.10608738041557492, 'test/num_examples': 2472, 'score': 31765.045680999756, 'total_duration': 34844.80156493187, 'accumulated_submission_time': 31765.045680999756, 'accumulated_eval_time': 3077.6442205905914, 'accumulated_logging_time': 1.199528455734253, 'global_step': 37831, 'preemption_count': 0}), (39563, {'train/ctc_loss': Array(0.32121426, dtype=float32), 'train/wer': 0.11493629415880627, 'validation/ctc_loss': Array(0.49552232, dtype=float32), 'validation/wer': 0.15311290991712415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30072522, dtype=float32), 'test/wer': 0.10362967928015762, 'test/num_examples': 2472, 'score': 33205.69780421257, 'total_duration': 36416.76326060295, 'accumulated_submission_time': 33205.69780421257, 'accumulated_eval_time': 3208.8578763008118, 'accumulated_logging_time': 1.2537486553192139, 'global_step': 39563, 'preemption_count': 0}), (41242, {'train/ctc_loss': Array(0.32201982, dtype=float32), 'train/wer': 0.1118759610740714, 'validation/ctc_loss': Array(0.47793457, dtype=float32), 'validation/wer': 0.14408243205433724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2893217, dtype=float32), 'test/wer': 0.09613470639611643, 'test/num_examples': 2472, 'score': 34645.64494895935, 'total_duration': 37987.054493665695, 'accumulated_submission_time': 34645.64494895935, 'accumulated_eval_time': 3339.1053099632263, 'accumulated_logging_time': 1.310318946838379, 'global_step': 41242, 'preemption_count': 0}), (42955, {'train/ctc_loss': Array(0.3502565, dtype=float32), 'train/wer': 0.12712999631473346, 'validation/ctc_loss': Array(0.46123832, dtype=float32), 'validation/wer': 0.14133276732047584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.275181, dtype=float32), 'test/wer': 0.09400199053480389, 'test/num_examples': 2472, 'score': 36085.74735164642, 'total_duration': 39556.61580777168, 'accumulated_submission_time': 36085.74735164642, 'accumulated_eval_time': 3468.471888780594, 'accumulated_logging_time': 1.3613312244415283, 'global_step': 42955, 'preemption_count': 0}), (44667, {'train/ctc_loss': Array(0.2957178, dtype=float32), 'train/wer': 0.10172565043067666, 'validation/ctc_loss': Array(0.4375282, dtype=float32), 'validation/wer': 0.13236982508273115, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25597444, dtype=float32), 'test/wer': 0.08488209128023887, 'test/num_examples': 2472, 'score': 37526.17189335823, 'total_duration': 41128.23708367348, 'accumulated_submission_time': 37526.17189335823, 'accumulated_eval_time': 3599.562591314316, 'accumulated_logging_time': 1.4192545413970947, 'global_step': 44667, 'preemption_count': 0}), (46341, {'train/ctc_loss': Array(0.25991777, dtype=float32), 'train/wer': 0.0945631478739158, 'validation/ctc_loss': Array(0.42294112, dtype=float32), 'validation/wer': 0.12815367249081033, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24309343, dtype=float32), 'test/wer': 0.0820587817114537, 'test/num_examples': 2472, 'score': 38966.12124705315, 'total_duration': 42697.58953118324, 'accumulated_submission_time': 38966.12124705315, 'accumulated_eval_time': 3728.865946292877, 'accumulated_logging_time': 1.4777500629425049, 'global_step': 46341, 'preemption_count': 0}), (48033, {'train/ctc_loss': Array(0.21460372, dtype=float32), 'train/wer': 0.07891331063959368, 'validation/ctc_loss': Array(0.40765736, dtype=float32), 'validation/wer': 0.12438132543488119, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23308186, dtype=float32), 'test/wer': 0.07832145105924888, 'test/num_examples': 2472, 'score': 40406.61080646515, 'total_duration': 44269.71370792389, 'accumulated_submission_time': 40406.61080646515, 'accumulated_eval_time': 3860.3984134197235, 'accumulated_logging_time': 1.5393648147583008, 'global_step': 48033, 'preemption_count': 0}), (49766, {'train/ctc_loss': Array(0.22010417, dtype=float32), 'train/wer': 0.08053137236798515, 'validation/ctc_loss': Array(0.38535455, dtype=float32), 'validation/wer': 0.11605514766182018, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21998067, dtype=float32), 'test/wer': 0.07338573720878273, 'test/num_examples': 2472, 'score': 41846.56562137604, 'total_duration': 45842.61648106575, 'accumulated_submission_time': 41846.56562137604, 'accumulated_eval_time': 3993.2541365623474, 'accumulated_logging_time': 1.5888762474060059, 'global_step': 49766, 'preemption_count': 0}), (51457, {'train/ctc_loss': Array(0.19126765, dtype=float32), 'train/wer': 0.06960788524308538, 'validation/ctc_loss': Array(0.37512258, dtype=float32), 'validation/wer': 0.11263977462397129, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2132377, dtype=float32), 'test/wer': 0.06993276867141958, 'test/num_examples': 2472, 'score': 43286.99607396126, 'total_duration': 47415.50420475006, 'accumulated_submission_time': 43286.99607396126, 'accumulated_eval_time': 4125.623663187027, 'accumulated_logging_time': 1.635772705078125, 'global_step': 51457, 'preemption_count': 0}), (53152, {'train/ctc_loss': Array(0.18736342, dtype=float32), 'train/wer': 0.0684754662124779, 'validation/ctc_loss': Array(0.35893753, dtype=float32), 'validation/wer': 0.10818242337118544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20606536, dtype=float32), 'test/wer': 0.06755631385452847, 'test/num_examples': 2472, 'score': 44727.45292019844, 'total_duration': 48987.60837483406, 'accumulated_submission_time': 44727.45292019844, 'accumulated_eval_time': 4257.1709525585175, 'accumulated_logging_time': 1.6938915252685547, 'global_step': 53152, 'preemption_count': 0}), (54884, {'train/ctc_loss': Array(0.17216952, dtype=float32), 'train/wer': 0.06314975845410628, 'validation/ctc_loss': Array(0.3504784, dtype=float32), 'validation/wer': 0.10432324479734488, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19877562, dtype=float32), 'test/wer': 0.06507830113947961, 'test/num_examples': 2472, 'score': 46167.60228896141, 'total_duration': 50558.42852473259, 'accumulated_submission_time': 46167.60228896141, 'accumulated_eval_time': 4387.745643615723, 'accumulated_logging_time': 1.7477047443389893, 'global_step': 54884, 'preemption_count': 0}), (56586, {'train/ctc_loss': Array(0.17619194, dtype=float32), 'train/wer': 0.06460973180866042, 'validation/ctc_loss': Array(0.3455636, dtype=float32), 'validation/wer': 0.10276992542137406, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19548336, dtype=float32), 'test/wer': 0.06355493266711353, 'test/num_examples': 2472, 'score': 47607.79773974419, 'total_duration': 52129.945148944855, 'accumulated_submission_time': 47607.79773974419, 'accumulated_eval_time': 4518.968811035156, 'accumulated_logging_time': 1.8048992156982422, 'global_step': 56586, 'preemption_count': 0}), (58286, {'train/ctc_loss': Array(0.18434367, dtype=float32), 'train/wer': 0.065422189307224, 'validation/ctc_loss': Array(0.34211627, dtype=float32), 'validation/wer': 0.10136132524192226, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19359203, dtype=float32), 'test/wer': 0.06251904210590457, 'test/num_examples': 2472, 'score': 49048.06335878372, 'total_duration': 53703.08632874489, 'accumulated_submission_time': 49048.06335878372, 'accumulated_eval_time': 4651.7530863285065, 'accumulated_logging_time': 1.8548481464385986, 'global_step': 58286, 'preemption_count': 0}), (59992, {'train/ctc_loss': Array(0.1628805, dtype=float32), 'train/wer': 0.060253996009594905, 'validation/ctc_loss': Array(0.34108773, dtype=float32), 'validation/wer': 0.10110083068818802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19300593, dtype=float32), 'test/wer': 0.06253935368553612, 'test/num_examples': 2472, 'score': 50488.21764969826, 'total_duration': 55274.20524048805, 'accumulated_submission_time': 50488.21764969826, 'accumulated_eval_time': 4782.622443914413, 'accumulated_logging_time': 1.910083532333374, 'global_step': 59992, 'preemption_count': 0}), (60000, {'train/ctc_loss': Array(0.1675994, dtype=float32), 'train/wer': 0.061647117885993336, 'validation/ctc_loss': Array(0.341086, dtype=float32), 'validation/wer': 0.10112012658105722, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19300595, dtype=float32), 'test/wer': 0.06253935368553612, 'test/num_examples': 2472, 'score': 50493.92419672012, 'total_duration': 55400.389587402344, 'accumulated_submission_time': 50493.92419672012, 'accumulated_eval_time': 4903.033866405487, 'accumulated_logging_time': 1.9686484336853027, 'global_step': 60000, 'preemption_count': 0})], 'global_step': 60000}
I1006 13:30:49.624645 139940046329664 submission_runner.py:552] Timing: 50493.92419672012
I1006 13:30:49.624705 139940046329664 submission_runner.py:554] Total number of evals: 37
I1006 13:30:49.624794 139940046329664 submission_runner.py:555] ====================
I1006 13:30:49.629723 139940046329664 submission_runner.py:625] Final librispeech_conformer score: 50493.92419672012
