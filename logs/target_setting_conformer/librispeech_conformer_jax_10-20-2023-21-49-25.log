python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_conformer/adamw_run1 --overwrite=true --save_checkpoints=false --max_global_steps=60000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_10-20-2023-21-49-25.log
I1020 21:49:47.337557 139765020145472 logger_utils.py:61] Removing existing experiment directory /experiment_runs/targets_check_conformer/adamw_run1/librispeech_conformer_jax because --overwrite was set.
I1020 21:49:47.342572 139765020145472 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_conformer/adamw_run1/librispeech_conformer_jax.
I1020 21:49:48.273591 139765020145472 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1020 21:49:48.274323 139765020145472 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1020 21:49:48.274515 139765020145472 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1020 21:49:48.283668 139765020145472 submission_runner.py:525] Using RNG seed 887936416
I1020 21:49:54.090709 139765020145472 submission_runner.py:534] --- Tuning run 1/1 ---
I1020 21:49:54.090953 139765020145472 submission_runner.py:539] Creating tuning directory at /experiment_runs/targets_check_conformer/adamw_run1/librispeech_conformer_jax/trial_1.
I1020 21:49:54.091123 139765020145472 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_conformer/adamw_run1/librispeech_conformer_jax/trial_1/hparams.json.
I1020 21:49:54.273670 139765020145472 submission_runner.py:202] Initializing dataset.
I1020 21:49:54.273942 139765020145472 submission_runner.py:209] Initializing model.
I1020 21:49:59.179728 139765020145472 submission_runner.py:243] Initializing optimizer.
I1020 21:50:00.464527 139765020145472 submission_runner.py:250] Initializing metrics bundle.
I1020 21:50:00.464764 139765020145472 submission_runner.py:268] Initializing checkpoint and logger.
I1020 21:50:00.466066 139765020145472 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_conformer/adamw_run1/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I1020 21:50:00.466201 139765020145472 submission_runner.py:288] Saving meta data to /experiment_runs/targets_check_conformer/adamw_run1/librispeech_conformer_jax/trial_1/meta_data_0.json.
I1020 21:50:00.466450 139765020145472 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1020 21:50:00.466536 139765020145472 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1020 21:50:00.786531 139765020145472 logger_utils.py:220] Unable to record git information. Continuing without it.
I1020 21:50:01.081515 139765020145472 submission_runner.py:291] Saving flags to /experiment_runs/targets_check_conformer/adamw_run1/librispeech_conformer_jax/trial_1/flags_0.json.
I1020 21:50:01.097221 139765020145472 submission_runner.py:301] Starting training loop.
I1020 21:50:01.393519 139765020145472 input_pipeline.py:20] Loading split = train-clean-100
I1020 21:50:01.432250 139765020145472 input_pipeline.py:20] Loading split = train-clean-360
I1020 21:50:01.835095 139765020145472 input_pipeline.py:20] Loading split = train-other-500
2023-10-20 21:51:13.960490: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-20 21:51:17.008995: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1020 21:51:18.957578 139590354593536 logging_writer.py:48] [0] global_step=0, grad_norm=40.86802673339844, loss=32.24833679199219
I1020 21:51:18.995516 139765020145472 spec.py:321] Evaluating on the training split.
I1020 21:51:19.165069 139765020145472 input_pipeline.py:20] Loading split = train-clean-100
I1020 21:51:19.200174 139765020145472 input_pipeline.py:20] Loading split = train-clean-360
I1020 21:51:19.631771 139765020145472 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1020 21:52:35.446863 139765020145472 spec.py:333] Evaluating on the validation split.
I1020 21:52:35.564766 139765020145472 input_pipeline.py:20] Loading split = dev-clean
I1020 21:52:35.570149 139765020145472 input_pipeline.py:20] Loading split = dev-other
I1020 21:53:36.856122 139765020145472 spec.py:349] Evaluating on the test split.
I1020 21:53:36.978071 139765020145472 input_pipeline.py:20] Loading split = test-clean
I1020 21:54:13.543700 139765020145472 submission_runner.py:395] Time since start: 252.44s, 	Step: 1, 	{'train/ctc_loss': Array(31.724596, dtype=float32), 'train/wer': 1.2776221810382524, 'validation/ctc_loss': Array(30.79964, dtype=float32), 'validation/wer': 1.076503470847775, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.920681, dtype=float32), 'test/wer': 1.1024719192411594, 'test/num_examples': 2472, 'score': 77.8982207775116, 'total_duration': 252.44418621063232, 'accumulated_submission_time': 77.8982207775116, 'accumulated_eval_time': 174.54589772224426, 'accumulated_logging_time': 0}
I1020 21:54:13.570376 139583987644160 logging_writer.py:48] [1] accumulated_eval_time=174.545898, accumulated_logging_time=0, accumulated_submission_time=77.898221, global_step=1, preemption_count=0, score=77.898221, test/ctc_loss=30.92068099975586, test/num_examples=2472, test/wer=1.102472, total_duration=252.444186, train/ctc_loss=31.72459602355957, train/wer=1.277622, validation/ctc_loss=30.799640655517578, validation/num_examples=5348, validation/wer=1.076503
I1020 21:54:36.564926 139591606265600 logging_writer.py:48] [1] global_step=1, grad_norm=43.52877426147461, loss=32.7100830078125
I1020 21:54:37.443364 139591614658304 logging_writer.py:48] [2] global_step=2, grad_norm=45.5778694152832, loss=32.26261901855469
I1020 21:54:38.254051 139591606265600 logging_writer.py:48] [3] global_step=3, grad_norm=48.20903396606445, loss=32.225547790527344
I1020 21:54:39.133085 139591614658304 logging_writer.py:48] [4] global_step=4, grad_norm=49.9245719909668, loss=31.647354125976562
I1020 21:54:40.011938 139591606265600 logging_writer.py:48] [5] global_step=5, grad_norm=64.85377502441406, loss=30.79916000366211
I1020 21:54:40.895008 139591614658304 logging_writer.py:48] [6] global_step=6, grad_norm=60.15463638305664, loss=29.48099708557129
I1020 21:54:41.772223 139591606265600 logging_writer.py:48] [7] global_step=7, grad_norm=49.188392639160156, loss=28.667551040649414
I1020 21:54:42.662223 139591614658304 logging_writer.py:48] [8] global_step=8, grad_norm=52.98618698120117, loss=27.002290725708008
I1020 21:54:43.547198 139591606265600 logging_writer.py:48] [9] global_step=9, grad_norm=41.563201904296875, loss=26.238866806030273
I1020 21:54:44.425058 139591614658304 logging_writer.py:48] [10] global_step=10, grad_norm=37.05479049682617, loss=25.668176651000977
I1020 21:54:45.303467 139591606265600 logging_writer.py:48] [11] global_step=11, grad_norm=45.31226348876953, loss=24.835250854492188
I1020 21:54:46.192100 139591614658304 logging_writer.py:48] [12] global_step=12, grad_norm=43.77163314819336, loss=24.493330001831055
I1020 21:54:47.078770 139591606265600 logging_writer.py:48] [13] global_step=13, grad_norm=30.20028305053711, loss=22.677053451538086
I1020 21:54:47.962475 139591614658304 logging_writer.py:48] [14] global_step=14, grad_norm=29.292278289794922, loss=21.75859832763672
I1020 21:54:48.841595 139591606265600 logging_writer.py:48] [15] global_step=15, grad_norm=36.8685417175293, loss=21.19240951538086
I1020 21:54:49.725025 139591614658304 logging_writer.py:48] [16] global_step=16, grad_norm=34.71889114379883, loss=19.673131942749023
I1020 21:54:50.614518 139591606265600 logging_writer.py:48] [17] global_step=17, grad_norm=50.94453048706055, loss=19.349937438964844
I1020 21:54:51.497111 139591614658304 logging_writer.py:48] [18] global_step=18, grad_norm=64.23328399658203, loss=17.846433639526367
I1020 21:54:52.377200 139591606265600 logging_writer.py:48] [19] global_step=19, grad_norm=89.01683044433594, loss=15.007644653320312
I1020 21:54:53.250000 139591614658304 logging_writer.py:48] [20] global_step=20, grad_norm=100.0267105102539, loss=10.917144775390625
I1020 21:54:54.128239 139591606265600 logging_writer.py:48] [21] global_step=21, grad_norm=38.973140716552734, loss=7.939075946807861
I1020 21:54:55.002222 139591614658304 logging_writer.py:48] [22] global_step=22, grad_norm=4.450087070465088, loss=7.240368843078613
I1020 21:54:55.877787 139591606265600 logging_writer.py:48] [23] global_step=23, grad_norm=15.904265403747559, loss=7.461202621459961
I1020 21:54:56.757038 139591614658304 logging_writer.py:48] [24] global_step=24, grad_norm=20.264055252075195, loss=7.763299465179443
I1020 21:54:57.629353 139591606265600 logging_writer.py:48] [25] global_step=25, grad_norm=21.59357452392578, loss=7.908936977386475
I1020 21:54:58.502044 139591614658304 logging_writer.py:48] [26] global_step=26, grad_norm=21.343748092651367, loss=7.843970775604248
I1020 21:54:59.377352 139591606265600 logging_writer.py:48] [27] global_step=27, grad_norm=19.954153060913086, loss=7.656386375427246
I1020 21:55:00.251361 139591614658304 logging_writer.py:48] [28] global_step=28, grad_norm=16.5581111907959, loss=7.367710113525391
I1020 21:55:01.127871 139591606265600 logging_writer.py:48] [29] global_step=29, grad_norm=9.298684120178223, loss=7.087411880493164
I1020 21:55:02.024325 139591614658304 logging_writer.py:48] [30] global_step=30, grad_norm=4.933185577392578, loss=7.008210182189941
I1020 21:55:02.904358 139591606265600 logging_writer.py:48] [31] global_step=31, grad_norm=18.975744247436523, loss=7.178122520446777
I1020 21:55:03.780950 139591614658304 logging_writer.py:48] [32] global_step=32, grad_norm=22.51785659790039, loss=7.226310729980469
I1020 21:55:04.657850 139591606265600 logging_writer.py:48] [33] global_step=33, grad_norm=15.272805213928223, loss=7.048451900482178
I1020 21:55:05.537276 139591614658304 logging_writer.py:48] [34] global_step=34, grad_norm=4.77934455871582, loss=6.890548229217529
I1020 21:55:06.419941 139591606265600 logging_writer.py:48] [35] global_step=35, grad_norm=4.584285259246826, loss=6.838331699371338
I1020 21:55:07.303705 139591614658304 logging_writer.py:48] [36] global_step=36, grad_norm=8.558648109436035, loss=6.866374969482422
I1020 21:55:08.183576 139591606265600 logging_writer.py:48] [37] global_step=37, grad_norm=10.060029029846191, loss=6.860696792602539
I1020 21:55:09.064702 139591614658304 logging_writer.py:48] [38] global_step=38, grad_norm=9.218616485595703, loss=6.785536289215088
I1020 21:55:09.948965 139591606265600 logging_writer.py:48] [39] global_step=39, grad_norm=5.700948715209961, loss=6.690433502197266
I1020 21:55:10.836876 139591614658304 logging_writer.py:48] [40] global_step=40, grad_norm=1.8425917625427246, loss=6.61954927444458
I1020 21:55:11.711938 139591606265600 logging_writer.py:48] [41] global_step=41, grad_norm=6.802320957183838, loss=6.6114983558654785
I1020 21:55:12.600049 139591614658304 logging_writer.py:48] [42] global_step=42, grad_norm=9.722814559936523, loss=6.615340709686279
I1020 21:55:13.483797 139591606265600 logging_writer.py:48] [43] global_step=43, grad_norm=7.564739227294922, loss=6.544834613800049
I1020 21:55:14.372003 139591614658304 logging_writer.py:48] [44] global_step=44, grad_norm=1.7652976512908936, loss=6.438749313354492
I1020 21:55:15.253170 139591606265600 logging_writer.py:48] [45] global_step=45, grad_norm=3.7811429500579834, loss=6.420883655548096
I1020 21:55:16.141141 139591614658304 logging_writer.py:48] [46] global_step=46, grad_norm=5.25549840927124, loss=6.418490886688232
I1020 21:55:17.028719 139591606265600 logging_writer.py:48] [47] global_step=47, grad_norm=3.6438286304473877, loss=6.370095252990723
I1020 21:55:17.914072 139591614658304 logging_writer.py:48] [48] global_step=48, grad_norm=1.2759863138198853, loss=6.323142051696777
I1020 21:55:18.799759 139591606265600 logging_writer.py:48] [49] global_step=49, grad_norm=4.617209434509277, loss=6.30537223815918
I1020 21:55:19.687535 139591614658304 logging_writer.py:48] [50] global_step=50, grad_norm=4.909185409545898, loss=6.276613235473633
I1020 21:55:20.575970 139591606265600 logging_writer.py:48] [51] global_step=51, grad_norm=1.7148923873901367, loss=6.221371173858643
I1020 21:55:21.465734 139591614658304 logging_writer.py:48] [52] global_step=52, grad_norm=2.7159438133239746, loss=6.205158710479736
I1020 21:55:22.354593 139591606265600 logging_writer.py:48] [53] global_step=53, grad_norm=3.195608615875244, loss=6.182469367980957
I1020 21:55:23.238687 139591614658304 logging_writer.py:48] [54] global_step=54, grad_norm=1.067813515663147, loss=6.150703430175781
I1020 21:55:24.120953 139591606265600 logging_writer.py:48] [55] global_step=55, grad_norm=4.179072380065918, loss=6.151023864746094
I1020 21:55:25.012981 139591614658304 logging_writer.py:48] [56] global_step=56, grad_norm=1.0283218622207642, loss=6.115593433380127
I1020 21:55:25.898586 139591606265600 logging_writer.py:48] [57] global_step=57, grad_norm=2.492626428604126, loss=6.08732795715332
I1020 21:55:26.781448 139591614658304 logging_writer.py:48] [58] global_step=58, grad_norm=1.3090168237686157, loss=6.063316822052002
I1020 21:55:27.662636 139591606265600 logging_writer.py:48] [59] global_step=59, grad_norm=2.299377918243408, loss=6.04822301864624
I1020 21:55:28.556869 139591614658304 logging_writer.py:48] [60] global_step=60, grad_norm=2.090308427810669, loss=6.011368751525879
I1020 21:55:29.445337 139591606265600 logging_writer.py:48] [61] global_step=61, grad_norm=2.5839169025421143, loss=5.994943141937256
I1020 21:55:30.332643 139591614658304 logging_writer.py:48] [62] global_step=62, grad_norm=0.7742409706115723, loss=5.987238883972168
I1020 21:55:31.219841 139591606265600 logging_writer.py:48] [63] global_step=63, grad_norm=2.801010847091675, loss=5.974620342254639
I1020 21:55:32.111288 139591614658304 logging_writer.py:48] [64] global_step=64, grad_norm=1.0432734489440918, loss=5.9382195472717285
I1020 21:55:32.996396 139591606265600 logging_writer.py:48] [65] global_step=65, grad_norm=2.2202327251434326, loss=5.933995723724365
I1020 21:55:33.889010 139591614658304 logging_writer.py:48] [66] global_step=66, grad_norm=3.8819150924682617, loss=5.927366733551025
I1020 21:55:34.771827 139591606265600 logging_writer.py:48] [67] global_step=67, grad_norm=2.36643385887146, loss=5.879649639129639
I1020 21:55:35.657290 139591614658304 logging_writer.py:48] [68] global_step=68, grad_norm=0.7002607583999634, loss=5.9066267013549805
I1020 21:55:36.536610 139591606265600 logging_writer.py:48] [69] global_step=69, grad_norm=3.2169861793518066, loss=5.8788042068481445
I1020 21:55:37.425494 139591614658304 logging_writer.py:48] [70] global_step=70, grad_norm=4.087759017944336, loss=5.889287948608398
I1020 21:55:38.306118 139591606265600 logging_writer.py:48] [71] global_step=71, grad_norm=2.475210189819336, loss=5.8694658279418945
I1020 21:55:39.186898 139591614658304 logging_writer.py:48] [72] global_step=72, grad_norm=0.5080128312110901, loss=5.859847068786621
I1020 21:55:40.074689 139591606265600 logging_writer.py:48] [73] global_step=73, grad_norm=2.136404037475586, loss=5.866888523101807
I1020 21:55:40.959856 139591614658304 logging_writer.py:48] [74] global_step=74, grad_norm=2.5951156616210938, loss=5.831241130828857
I1020 21:55:41.840172 139591606265600 logging_writer.py:48] [75] global_step=75, grad_norm=1.9962431192398071, loss=5.866000175476074
I1020 21:55:42.715552 139591614658304 logging_writer.py:48] [76] global_step=76, grad_norm=1.8324462175369263, loss=5.848655700683594
I1020 21:55:43.606321 139591606265600 logging_writer.py:48] [77] global_step=77, grad_norm=2.5314505100250244, loss=5.839160919189453
I1020 21:55:44.492031 139591614658304 logging_writer.py:48] [78] global_step=78, grad_norm=5.01040506362915, loss=5.845666885375977
I1020 21:55:45.367572 139591606265600 logging_writer.py:48] [79] global_step=79, grad_norm=7.679756164550781, loss=5.897878170013428
I1020 21:55:46.245426 139591614658304 logging_writer.py:48] [80] global_step=80, grad_norm=5.57290506362915, loss=5.858323097229004
I1020 21:55:47.134986 139591606265600 logging_writer.py:48] [81] global_step=81, grad_norm=0.4373893439769745, loss=5.841241359710693
I1020 21:55:48.014450 139591614658304 logging_writer.py:48] [82] global_step=82, grad_norm=5.156670093536377, loss=5.8688883781433105
I1020 21:55:48.902011 139591606265600 logging_writer.py:48] [83] global_step=83, grad_norm=5.4487738609313965, loss=5.854497909545898
I1020 21:55:49.784485 139591614658304 logging_writer.py:48] [84] global_step=84, grad_norm=1.213784098625183, loss=5.838409900665283
I1020 21:55:50.669414 139591606265600 logging_writer.py:48] [85] global_step=85, grad_norm=3.3952319622039795, loss=5.850294589996338
I1020 21:55:51.557327 139591614658304 logging_writer.py:48] [86] global_step=86, grad_norm=5.313015937805176, loss=5.858325481414795
I1020 21:55:52.440536 139591606265600 logging_writer.py:48] [87] global_step=87, grad_norm=3.9279775619506836, loss=5.848573207855225
I1020 21:55:53.321363 139591614658304 logging_writer.py:48] [88] global_step=88, grad_norm=0.7620614171028137, loss=5.819334506988525
I1020 21:55:54.206471 139591606265600 logging_writer.py:48] [89] global_step=89, grad_norm=5.710245132446289, loss=5.841817378997803
I1020 21:55:55.098258 139591614658304 logging_writer.py:48] [90] global_step=90, grad_norm=5.322518825531006, loss=5.848191738128662
I1020 21:55:55.981672 139591606265600 logging_writer.py:48] [91] global_step=91, grad_norm=1.3107835054397583, loss=5.834954738616943
I1020 21:55:56.860100 139591614658304 logging_writer.py:48] [92] global_step=92, grad_norm=7.076525688171387, loss=5.8819899559021
I1020 21:55:57.745893 139591606265600 logging_writer.py:48] [93] global_step=93, grad_norm=4.565845489501953, loss=5.8500494956970215
I1020 21:55:58.631866 139591614658304 logging_writer.py:48] [94] global_step=94, grad_norm=3.8064968585968018, loss=5.8322930335998535
I1020 21:55:59.521080 139591606265600 logging_writer.py:48] [95] global_step=95, grad_norm=7.01708984375, loss=5.876078128814697
I1020 21:56:00.401908 139591614658304 logging_writer.py:48] [96] global_step=96, grad_norm=1.4793154001235962, loss=5.825599670410156
I1020 21:56:01.294312 139591606265600 logging_writer.py:48] [97] global_step=97, grad_norm=4.376082420349121, loss=5.843827724456787
I1020 21:56:02.176767 139591614658304 logging_writer.py:48] [98] global_step=98, grad_norm=2.329136848449707, loss=5.800815582275391
I1020 21:56:03.055650 139591606265600 logging_writer.py:48] [99] global_step=99, grad_norm=3.1524713039398193, loss=5.836818218231201
I1020 21:56:03.934418 139591614658304 logging_writer.py:48] [100] global_step=100, grad_norm=3.761439085006714, loss=5.805899143218994
I1020 22:01:06.592599 139591606265600 logging_writer.py:48] [500] global_step=500, grad_norm=2.4209020137786865, loss=4.223554611206055
I1020 22:08:12.973714 139591614658304 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.3011789321899414, loss=2.7242822647094727
I1020 22:14:37.523498 139585750386432 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7845766544342041, loss=2.2382395267486572
I1020 22:18:13.715155 139765020145472 spec.py:321] Evaluating on the training split.
I1020 22:19:04.830257 139765020145472 spec.py:333] Evaluating on the validation split.
I1020 22:19:53.558502 139765020145472 spec.py:349] Evaluating on the test split.
I1020 22:20:18.631539 139765020145472 submission_runner.py:395] Time since start: 1817.53s, 	Step: 1750, 	{'train/ctc_loss': Array(2.3897297, dtype=float32), 'train/wer': 0.5442112417869869, 'validation/ctc_loss': Array(2.9253056, dtype=float32), 'validation/wer': 0.5861243326221072, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5503435, dtype=float32), 'test/wer': 0.5324071253021347, 'test/num_examples': 2472, 'score': 1517.9625606536865, 'total_duration': 1817.529152393341, 'accumulated_submission_time': 1517.9625606536865, 'accumulated_eval_time': 299.45716977119446, 'accumulated_logging_time': 0.04081392288208008}
I1020 22:20:18.664217 139592362338048 logging_writer.py:48] [1750] accumulated_eval_time=299.457170, accumulated_logging_time=0.040814, accumulated_submission_time=1517.962561, global_step=1750, preemption_count=0, score=1517.962561, test/ctc_loss=2.5503435134887695, test/num_examples=2472, test/wer=0.532407, total_duration=1817.529152, train/ctc_loss=2.3897297382354736, train/wer=0.544211, validation/ctc_loss=2.9253056049346924, validation/num_examples=5348, validation/wer=0.586124
I1020 22:23:28.520041 139592353945344 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6161383390426636, loss=2.0900943279266357
I1020 22:29:53.125792 139592362338048 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7315030097961426, loss=1.9927966594696045
I1020 22:37:06.346617 139592353945344 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.224729299545288, loss=1.934971570968628
I1020 22:43:38.581592 139592362338048 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9556214809417725, loss=1.9246411323547363
I1020 22:44:19.118439 139765020145472 spec.py:321] Evaluating on the training split.
I1020 22:45:11.432755 139765020145472 spec.py:333] Evaluating on the validation split.
I1020 22:46:02.092571 139765020145472 spec.py:349] Evaluating on the test split.
I1020 22:46:28.075061 139765020145472 submission_runner.py:395] Time since start: 3386.97s, 	Step: 3550, 	{'train/ctc_loss': Array(0.66659975, dtype=float32), 'train/wer': 0.2331406653703753, 'validation/ctc_loss': Array(0.9881299, dtype=float32), 'validation/wer': 0.293839365882387, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6959028, dtype=float32), 'test/wer': 0.23392846261653769, 'test/num_examples': 2472, 'score': 2958.3294310569763, 'total_duration': 3386.9722702503204, 'accumulated_submission_time': 2958.3294310569763, 'accumulated_eval_time': 428.40828919410706, 'accumulated_logging_time': 0.08768963813781738}
I1020 22:46:28.111644 139592362338048 logging_writer.py:48] [3550] accumulated_eval_time=428.408289, accumulated_logging_time=0.087690, accumulated_submission_time=2958.329431, global_step=3550, preemption_count=0, score=2958.329431, test/ctc_loss=0.6959028244018555, test/num_examples=2472, test/wer=0.233928, total_duration=3386.972270, train/ctc_loss=0.6665997505187988, train/wer=0.233141, validation/ctc_loss=0.9881299138069153, validation/num_examples=5348, validation/wer=0.293839
I1020 22:52:26.561254 139592353945344 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8983836770057678, loss=1.923896074295044
I1020 22:59:01.226403 139592362338048 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.689214289188385, loss=1.825084924697876
I1020 23:06:11.728756 139592353945344 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6335462331771851, loss=1.7710250616073608
I1020 23:10:28.334182 139765020145472 spec.py:321] Evaluating on the training split.
I1020 23:11:21.441120 139765020145472 spec.py:333] Evaluating on the validation split.
I1020 23:12:12.768325 139765020145472 spec.py:349] Evaluating on the test split.
I1020 23:12:38.779592 139765020145472 submission_runner.py:395] Time since start: 4957.68s, 	Step: 5314, 	{'train/ctc_loss': Array(0.49469042, dtype=float32), 'train/wer': 0.1766792647764231, 'validation/ctc_loss': Array(0.8382224, dtype=float32), 'validation/wer': 0.24736186605134344, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5657178, dtype=float32), 'test/wer': 0.18759774947697683, 'test/num_examples': 2472, 'score': 4398.463261127472, 'total_duration': 4957.676019191742, 'accumulated_submission_time': 4398.463261127472, 'accumulated_eval_time': 558.8476555347443, 'accumulated_logging_time': 0.1399669647216797}
I1020 23:12:38.814628 139592362338048 logging_writer.py:48] [5314] accumulated_eval_time=558.847656, accumulated_logging_time=0.139967, accumulated_submission_time=4398.463261, global_step=5314, preemption_count=0, score=4398.463261, test/ctc_loss=0.5657178163528442, test/num_examples=2472, test/wer=0.187598, total_duration=4957.676019, train/ctc_loss=0.4946904182434082, train/wer=0.176679, validation/ctc_loss=0.8382223844528198, validation/num_examples=5348, validation/wer=0.247362
I1020 23:15:00.316576 139592353945344 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6292084455490112, loss=1.6868586540222168
I1020 23:21:54.895932 139592362338048 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6786364316940308, loss=1.7316406965255737
I1020 23:28:38.745207 139592362338048 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6617593765258789, loss=1.7396284341812134
I1020 23:35:42.908553 139592353945344 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7244338393211365, loss=1.7894437313079834
I1020 23:36:39.231787 139765020145472 spec.py:321] Evaluating on the training split.
I1020 23:37:33.866683 139765020145472 spec.py:333] Evaluating on the validation split.
I1020 23:38:25.299502 139765020145472 spec.py:349] Evaluating on the test split.
I1020 23:38:50.891781 139765020145472 submission_runner.py:395] Time since start: 6529.79s, 	Step: 7066, 	{'train/ctc_loss': Array(0.45440343, dtype=float32), 'train/wer': 0.157892532124119, 'validation/ctc_loss': Array(0.75882304, dtype=float32), 'validation/wer': 0.22641126891105168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5047663, dtype=float32), 'test/wer': 0.16880953831779497, 'test/num_examples': 2472, 'score': 5838.795207500458, 'total_duration': 6529.786837816238, 'accumulated_submission_time': 5838.795207500458, 'accumulated_eval_time': 690.4999713897705, 'accumulated_logging_time': 0.1893620491027832}
I1020 23:38:50.927788 139591932258048 logging_writer.py:48] [7066] accumulated_eval_time=690.499971, accumulated_logging_time=0.189362, accumulated_submission_time=5838.795208, global_step=7066, preemption_count=0, score=5838.795208, test/ctc_loss=0.5047662854194641, test/num_examples=2472, test/wer=0.168810, total_duration=6529.786838, train/ctc_loss=0.45440343022346497, train/wer=0.157893, validation/ctc_loss=0.758823037147522, validation/num_examples=5348, validation/wer=0.226411
I1020 23:44:21.879662 139591932258048 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5944471955299377, loss=1.7445303201675415
I1020 23:51:25.156566 139591923865344 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8181740045547485, loss=1.733951210975647
I1020 23:58:15.053642 139591932258048 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5701791644096375, loss=1.709574818611145
I1021 00:02:51.941990 139765020145472 spec.py:321] Evaluating on the training split.
I1021 00:03:45.622900 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 00:04:36.630982 139765020145472 spec.py:349] Evaluating on the test split.
I1021 00:05:02.638974 139765020145472 submission_runner.py:395] Time since start: 8101.54s, 	Step: 8836, 	{'train/ctc_loss': Array(0.4505591, dtype=float32), 'train/wer': 0.15683641740416188, 'validation/ctc_loss': Array(0.7456387, dtype=float32), 'validation/wer': 0.22204736572791256, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4905502, dtype=float32), 'test/wer': 0.1636097739321187, 'test/num_examples': 2472, 'score': 7279.721311330795, 'total_duration': 8101.536092996597, 'accumulated_submission_time': 7279.721311330795, 'accumulated_eval_time': 821.1913588047028, 'accumulated_logging_time': 0.23985028266906738}
I1021 00:05:02.683790 139592362338048 logging_writer.py:48] [8836] accumulated_eval_time=821.191359, accumulated_logging_time=0.239850, accumulated_submission_time=7279.721311, global_step=8836, preemption_count=0, score=7279.721311, test/ctc_loss=0.4905501902103424, test/num_examples=2472, test/wer=0.163610, total_duration=8101.536093, train/ctc_loss=0.4505591094493866, train/wer=0.156836, validation/ctc_loss=0.7456387281417847, validation/num_examples=5348, validation/wer=0.222047
I1021 00:07:06.923107 139592353945344 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8041568398475647, loss=1.7261855602264404
I1021 00:13:52.907516 139592034658048 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.665592610836029, loss=1.7057913541793823
I1021 00:20:59.920140 139592026265344 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6689627766609192, loss=1.661390781402588
I1021 00:27:59.525580 139592034658048 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.956483006477356, loss=1.6581043004989624
I1021 00:29:02.742421 139765020145472 spec.py:321] Evaluating on the training split.
I1021 00:29:56.408065 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 00:30:47.768676 139765020145472 spec.py:349] Evaluating on the test split.
I1021 00:31:13.991311 139765020145472 submission_runner.py:395] Time since start: 9672.89s, 	Step: 10583, 	{'train/ctc_loss': Array(0.45575836, dtype=float32), 'train/wer': 0.1574384772553894, 'validation/ctc_loss': Array(0.73247176, dtype=float32), 'validation/wer': 0.21679523446324955, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47312295, dtype=float32), 'test/wer': 0.15820689375012695, 'test/num_examples': 2472, 'score': 8719.689422130585, 'total_duration': 9672.887839078903, 'accumulated_submission_time': 8719.689422130585, 'accumulated_eval_time': 952.4340887069702, 'accumulated_logging_time': 0.303358793258667}
I1021 00:31:14.026258 139591563618048 logging_writer.py:48] [10583] accumulated_eval_time=952.434089, accumulated_logging_time=0.303359, accumulated_submission_time=8719.689422, global_step=10583, preemption_count=0, score=8719.689422, test/ctc_loss=0.4731229543685913, test/num_examples=2472, test/wer=0.158207, total_duration=9672.887839, train/ctc_loss=0.45575836300849915, train/wer=0.157438, validation/ctc_loss=0.732471764087677, validation/num_examples=5348, validation/wer=0.216795
I1021 00:36:55.941861 139591555225344 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6408953070640564, loss=1.6222889423370361
I1021 00:44:04.791867 139591235938048 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6552499532699585, loss=1.5933276414871216
I1021 00:51:01.446872 139591227545344 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8510215282440186, loss=1.6931506395339966
I1021 00:55:14.321118 139765020145472 spec.py:321] Evaluating on the training split.
I1021 00:56:08.554054 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 00:56:59.764309 139765020145472 spec.py:349] Evaluating on the test split.
I1021 00:57:25.967170 139765020145472 submission_runner.py:395] Time since start: 11244.86s, 	Step: 12284, 	{'train/ctc_loss': Array(0.41217625, dtype=float32), 'train/wer': 0.14335928360240785, 'validation/ctc_loss': Array(0.6922513, dtype=float32), 'validation/wer': 0.20763296870926942, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44658628, dtype=float32), 'test/wer': 0.15002132715861313, 'test/num_examples': 2472, 'score': 10159.897886276245, 'total_duration': 11244.863183498383, 'accumulated_submission_time': 10159.897886276245, 'accumulated_eval_time': 1084.073422908783, 'accumulated_logging_time': 0.3556859493255615}
I1021 00:57:26.004559 139591348578048 logging_writer.py:48] [12284] accumulated_eval_time=1084.073423, accumulated_logging_time=0.355686, accumulated_submission_time=10159.897886, global_step=12284, preemption_count=0, score=10159.897886, test/ctc_loss=0.4465862810611725, test/num_examples=2472, test/wer=0.150021, total_duration=11244.863183, train/ctc_loss=0.412176251411438, train/wer=0.143359, validation/ctc_loss=0.6922513246536255, validation/num_examples=5348, validation/wer=0.207633
I1021 01:00:13.995024 139591348578048 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.7250447273254395, loss=1.7210056781768799
I1021 01:07:13.612117 139591340185344 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7457900643348694, loss=1.6514747142791748
I1021 01:14:31.552910 139591348578048 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6970236301422119, loss=1.6777536869049072
I1021 01:21:23.857460 139591340185344 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.8446561098098755, loss=1.5493197441101074
I1021 01:21:26.125425 139765020145472 spec.py:321] Evaluating on the training split.
I1021 01:22:21.058269 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 01:23:13.515417 139765020145472 spec.py:349] Evaluating on the test split.
I1021 01:23:39.678460 139765020145472 submission_runner.py:395] Time since start: 12818.58s, 	Step: 14004, 	{'train/ctc_loss': Array(0.3658871, dtype=float32), 'train/wer': 0.1305754179775865, 'validation/ctc_loss': Array(0.6747159, dtype=float32), 'validation/wer': 0.2009809127509003, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42511925, dtype=float32), 'test/wer': 0.1445778238173583, 'test/num_examples': 2472, 'score': 11599.932636737823, 'total_duration': 12818.575507879257, 'accumulated_submission_time': 11599.932636737823, 'accumulated_eval_time': 1217.6207690238953, 'accumulated_logging_time': 0.4077177047729492}
I1021 01:23:39.711867 139591778658048 logging_writer.py:48] [14004] accumulated_eval_time=1217.620769, accumulated_logging_time=0.407718, accumulated_submission_time=11599.932637, global_step=14004, preemption_count=0, score=11599.932637, test/ctc_loss=0.4251192510128021, test/num_examples=2472, test/wer=0.144578, total_duration=12818.575508, train/ctc_loss=0.3658871054649353, train/wer=0.130575, validation/ctc_loss=0.6747158765792847, validation/num_examples=5348, validation/wer=0.200981
I1021 01:30:24.602244 139591778658048 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.9570790529251099, loss=1.60420560836792
I1021 01:37:17.080110 139591770265344 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7117350101470947, loss=1.569457769393921
I1021 01:44:39.300934 139591778658048 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6601189970970154, loss=1.6117862462997437
I1021 01:47:39.784307 139765020145472 spec.py:321] Evaluating on the training split.
I1021 01:48:33.869246 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 01:49:26.718279 139765020145472 spec.py:349] Evaluating on the test split.
I1021 01:49:52.798362 139765020145472 submission_runner.py:395] Time since start: 14391.69s, 	Step: 15739, 	{'train/ctc_loss': Array(0.35378382, dtype=float32), 'train/wer': 0.12363740829604881, 'validation/ctc_loss': Array(0.66440237, dtype=float32), 'validation/wer': 0.19839346573080896, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42792174, dtype=float32), 'test/wer': 0.1423029268986249, 'test/num_examples': 2472, 'score': 13039.918533563614, 'total_duration': 14391.694628477097, 'accumulated_submission_time': 13039.918533563614, 'accumulated_eval_time': 1350.6283848285675, 'accumulated_logging_time': 0.4568822383880615}
I1021 01:49:52.834482 139591932258048 logging_writer.py:48] [15739] accumulated_eval_time=1350.628385, accumulated_logging_time=0.456882, accumulated_submission_time=13039.918534, global_step=15739, preemption_count=0, score=13039.918534, test/ctc_loss=0.42792174220085144, test/num_examples=2472, test/wer=0.142303, total_duration=14391.694628, train/ctc_loss=0.35378381609916687, train/wer=0.123637, validation/ctc_loss=0.6644023656845093, validation/num_examples=5348, validation/wer=0.198393
I1021 01:53:15.630979 139591923865344 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6249262690544128, loss=1.5830990076065063
I1021 02:00:38.679387 139591276898048 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6800775527954102, loss=1.5249444246292114
I1021 02:07:17.511618 139591268505344 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6427500247955322, loss=1.571380853652954
I1021 02:13:53.289476 139765020145472 spec.py:321] Evaluating on the training split.
I1021 02:14:47.634025 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 02:15:38.565235 139765020145472 spec.py:349] Evaluating on the test split.
I1021 02:16:05.094787 139765020145472 submission_runner.py:395] Time since start: 15963.99s, 	Step: 17440, 	{'train/ctc_loss': Array(0.3488905, dtype=float32), 'train/wer': 0.12355001153019032, 'validation/ctc_loss': Array(0.6491723, dtype=float32), 'validation/wer': 0.19232068895604237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40822417, dtype=float32), 'test/wer': 0.13478764243495217, 'test/num_examples': 2472, 'score': 14480.287340402603, 'total_duration': 15963.988492965698, 'accumulated_submission_time': 14480.287340402603, 'accumulated_eval_time': 1482.4246878623962, 'accumulated_logging_time': 0.5075671672821045}
I1021 02:16:05.132257 139592362338048 logging_writer.py:48] [17440] accumulated_eval_time=1482.424688, accumulated_logging_time=0.507567, accumulated_submission_time=14480.287340, global_step=17440, preemption_count=0, score=14480.287340, test/ctc_loss=0.4082241654396057, test/num_examples=2472, test/wer=0.134788, total_duration=15963.988493, train/ctc_loss=0.3488905131816864, train/wer=0.123550, validation/ctc_loss=0.649172306060791, validation/num_examples=5348, validation/wer=0.192321
I1021 02:16:50.996847 139592353945344 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6992891430854797, loss=1.6277055740356445
I1021 02:23:33.857272 139592362338048 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6951117515563965, loss=1.5304139852523804
I1021 02:31:01.451694 139592353945344 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.668913722038269, loss=1.524938941001892
I1021 02:37:45.577829 139592362338048 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5944302678108215, loss=1.5363035202026367
I1021 02:40:05.201700 139765020145472 spec.py:321] Evaluating on the training split.
I1021 02:40:59.540448 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 02:41:51.924134 139765020145472 spec.py:349] Evaluating on the test split.
I1021 02:42:18.193404 139765020145472 submission_runner.py:395] Time since start: 17537.09s, 	Step: 19158, 	{'train/ctc_loss': Array(0.352684, dtype=float32), 'train/wer': 0.12276369020862529, 'validation/ctc_loss': Array(0.6297174, dtype=float32), 'validation/wer': 0.18653755177307704, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3977691, dtype=float32), 'test/wer': 0.13186277496800927, 'test/num_examples': 2472, 'score': 15920.269403457642, 'total_duration': 17537.09039068222, 'accumulated_submission_time': 15920.269403457642, 'accumulated_eval_time': 1615.4106562137604, 'accumulated_logging_time': 0.561298131942749}
I1021 02:42:18.230862 139592362338048 logging_writer.py:48] [19158] accumulated_eval_time=1615.410656, accumulated_logging_time=0.561298, accumulated_submission_time=15920.269403, global_step=19158, preemption_count=0, score=15920.269403, test/ctc_loss=0.39776909351348877, test/num_examples=2472, test/wer=0.131863, total_duration=17537.090391, train/ctc_loss=0.35268399119377136, train/wer=0.122764, validation/ctc_loss=0.6297174096107483, validation/num_examples=5348, validation/wer=0.186538
I1021 02:46:50.210934 139592353945344 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6307384371757507, loss=1.5731316804885864
I1021 02:53:34.482806 139592362338048 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7669840455055237, loss=1.5954676866531372
I1021 03:00:57.740037 139592353945344 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6605690717697144, loss=1.5451710224151611
I1021 03:06:18.261930 139765020145472 spec.py:321] Evaluating on the training split.
I1021 03:07:13.154621 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 03:08:04.089900 139765020145472 spec.py:349] Evaluating on the test split.
I1021 03:08:30.411000 139765020145472 submission_runner.py:395] Time since start: 19109.31s, 	Step: 20901, 	{'train/ctc_loss': Array(0.3507246, dtype=float32), 'train/wer': 0.12021526367161738, 'validation/ctc_loss': Array(0.61042434, dtype=float32), 'validation/wer': 0.17924828871274512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3801553, dtype=float32), 'test/wer': 0.12497714947291451, 'test/num_examples': 2472, 'score': 17360.215380191803, 'total_duration': 19109.308428525925, 'accumulated_submission_time': 17360.215380191803, 'accumulated_eval_time': 1747.5546412467957, 'accumulated_logging_time': 0.61212158203125}
I1021 03:08:30.445057 139592362338048 logging_writer.py:48] [20901] accumulated_eval_time=1747.554641, accumulated_logging_time=0.612122, accumulated_submission_time=17360.215380, global_step=20901, preemption_count=0, score=17360.215380, test/ctc_loss=0.3801552951335907, test/num_examples=2472, test/wer=0.124977, total_duration=19109.308429, train/ctc_loss=0.35072460770606995, train/wer=0.120215, validation/ctc_loss=0.6104243397712708, validation/num_examples=5348, validation/wer=0.179248
I1021 03:09:45.613345 139592353945344 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7156087160110474, loss=1.51180899143219
I1021 03:16:51.823329 139592362338048 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7218860983848572, loss=1.5515819787979126
I1021 03:23:35.389412 139592362338048 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7695394158363342, loss=1.4930691719055176
I1021 03:31:00.882373 139592353945344 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6350787878036499, loss=1.5043946504592896
I1021 03:32:30.706818 139765020145472 spec.py:321] Evaluating on the training split.
I1021 03:33:26.198275 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 03:34:18.479623 139765020145472 spec.py:349] Evaluating on the test split.
I1021 03:34:45.216449 139765020145472 submission_runner.py:395] Time since start: 20684.11s, 	Step: 22602, 	{'train/ctc_loss': Array(0.33120733, dtype=float32), 'train/wer': 0.11669924889391912, 'validation/ctc_loss': Array(0.5895108, dtype=float32), 'validation/wer': 0.177172538304836, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36914045, dtype=float32), 'test/wer': 0.12388032417281092, 'test/num_examples': 2472, 'score': 18800.390300273895, 'total_duration': 20684.11161017418, 'accumulated_submission_time': 18800.390300273895, 'accumulated_eval_time': 1882.0567164421082, 'accumulated_logging_time': 0.6611838340759277}
I1021 03:34:45.253743 139591778658048 logging_writer.py:48] [22602] accumulated_eval_time=1882.056716, accumulated_logging_time=0.661184, accumulated_submission_time=18800.390300, global_step=22602, preemption_count=0, score=18800.390300, test/ctc_loss=0.3691404461860657, test/num_examples=2472, test/wer=0.123880, total_duration=20684.111610, train/ctc_loss=0.3312073349952698, train/wer=0.116699, validation/ctc_loss=0.5895107984542847, validation/num_examples=5348, validation/wer=0.177173
I1021 03:39:55.319242 139591778658048 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.867114782333374, loss=1.4256548881530762
I1021 03:47:20.713620 139591770265344 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.7004773616790771, loss=1.462611436843872
I1021 03:54:14.758834 139591778658048 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7616258263587952, loss=1.4547250270843506
I1021 03:58:45.633180 139765020145472 spec.py:321] Evaluating on the training split.
I1021 03:59:40.581808 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 04:00:32.247828 139765020145472 spec.py:349] Evaluating on the test split.
I1021 04:00:59.568212 139765020145472 submission_runner.py:395] Time since start: 22258.46s, 	Step: 24316, 	{'train/ctc_loss': Array(0.30555034, dtype=float32), 'train/wer': 0.10787128921920527, 'validation/ctc_loss': Array(0.57999027, dtype=float32), 'validation/wer': 0.17156318487695144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34827054, dtype=float32), 'test/wer': 0.11687282919992688, 'test/num_examples': 2472, 'score': 20240.683255195618, 'total_duration': 22258.464760541916, 'accumulated_submission_time': 20240.683255195618, 'accumulated_eval_time': 2015.9855983257294, 'accumulated_logging_time': 0.7143306732177734}
I1021 04:00:59.610305 139591932258048 logging_writer.py:48] [24316] accumulated_eval_time=2015.985598, accumulated_logging_time=0.714331, accumulated_submission_time=20240.683255, global_step=24316, preemption_count=0, score=20240.683255, test/ctc_loss=0.3482705354690552, test/num_examples=2472, test/wer=0.116873, total_duration=22258.464761, train/ctc_loss=0.30555033683776855, train/wer=0.107871, validation/ctc_loss=0.5799902677536011, validation/num_examples=5348, validation/wer=0.171563
I1021 04:03:19.329065 139591923865344 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8181833624839783, loss=1.5222219228744507
I1021 04:10:09.447871 139591932258048 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6675018668174744, loss=1.4662939310073853
I1021 04:17:26.237335 139591923865344 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.7226302623748779, loss=1.5183568000793457
I1021 04:24:20.410402 139591932258048 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8072600960731506, loss=1.4218347072601318
I1021 04:25:00.139526 139765020145472 spec.py:321] Evaluating on the training split.
I1021 04:25:55.032404 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 04:26:47.350909 139765020145472 spec.py:349] Evaluating on the test split.
I1021 04:27:13.403308 139765020145472 submission_runner.py:395] Time since start: 23832.30s, 	Step: 26051, 	{'train/ctc_loss': Array(0.28669932, dtype=float32), 'train/wer': 0.10374292495891911, 'validation/ctc_loss': Array(0.5682426, dtype=float32), 'validation/wer': 0.16973845544860344, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34270158, dtype=float32), 'test/wer': 0.11447606280340422, 'test/num_examples': 2472, 'score': 21681.123821496964, 'total_duration': 23832.300597190857, 'accumulated_submission_time': 21681.123821496964, 'accumulated_eval_time': 2149.243949651718, 'accumulated_logging_time': 0.7726695537567139}
I1021 04:27:13.438037 139592362338048 logging_writer.py:48] [26051] accumulated_eval_time=2149.243950, accumulated_logging_time=0.772670, accumulated_submission_time=21681.123821, global_step=26051, preemption_count=0, score=21681.123821, test/ctc_loss=0.34270158410072327, test/num_examples=2472, test/wer=0.114476, total_duration=23832.300597, train/ctc_loss=0.2866993248462677, train/wer=0.103743, validation/ctc_loss=0.568242609500885, validation/num_examples=5348, validation/wer=0.169738
I1021 04:33:18.506329 139592353945344 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6769290566444397, loss=1.3692831993103027
I1021 04:40:16.323241 139592362338048 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.6681600213050842, loss=1.3774638175964355
I1021 04:47:22.835042 139592353945344 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.9853814840316772, loss=1.4639438390731812
I1021 04:51:13.582025 139765020145472 spec.py:321] Evaluating on the training split.
I1021 04:52:08.954372 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 04:53:00.232847 139765020145472 spec.py:349] Evaluating on the test split.
I1021 04:53:26.923144 139765020145472 submission_runner.py:395] Time since start: 25405.82s, 	Step: 27760, 	{'train/ctc_loss': Array(0.27958962, dtype=float32), 'train/wer': 0.09679250493500118, 'validation/ctc_loss': Array(0.5559207, dtype=float32), 'validation/wer': 0.1613195979802466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33768842, dtype=float32), 'test/wer': 0.11187618061056609, 'test/num_examples': 2472, 'score': 23121.179690361023, 'total_duration': 25405.819195747375, 'accumulated_submission_time': 23121.179690361023, 'accumulated_eval_time': 2282.578394174576, 'accumulated_logging_time': 0.8237957954406738}
I1021 04:53:26.961659 139592362338048 logging_writer.py:48] [27760] accumulated_eval_time=2282.578394, accumulated_logging_time=0.823796, accumulated_submission_time=23121.179690, global_step=27760, preemption_count=0, score=23121.179690, test/ctc_loss=0.3376884162425995, test/num_examples=2472, test/wer=0.111876, total_duration=25405.819196, train/ctc_loss=0.27958962321281433, train/wer=0.096793, validation/ctc_loss=0.5559207201004028, validation/num_examples=5348, validation/wer=0.161320
I1021 04:56:33.057998 139592034658048 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.7645971179008484, loss=1.427842617034912
I1021 05:03:39.149405 139592026265344 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.8107469081878662, loss=1.4235211610794067
I1021 05:10:50.095109 139592362338048 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.7556361556053162, loss=1.4083170890808105
I1021 05:17:27.596569 139765020145472 spec.py:321] Evaluating on the training split.
I1021 05:18:20.940579 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 05:19:13.138742 139765020145472 spec.py:349] Evaluating on the test split.
I1021 05:19:39.591469 139765020145472 submission_runner.py:395] Time since start: 26978.49s, 	Step: 29475, 	{'train/ctc_loss': Array(0.28855702, dtype=float32), 'train/wer': 0.10198907956318252, 'validation/ctc_loss': Array(0.52978396, dtype=float32), 'validation/wer': 0.15889628006217596, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32637402, dtype=float32), 'test/wer': 0.1077732415249934, 'test/num_examples': 2472, 'score': 24561.724899053574, 'total_duration': 26978.488636016846, 'accumulated_submission_time': 24561.724899053574, 'accumulated_eval_time': 2414.5677559375763, 'accumulated_logging_time': 0.8794715404510498}
I1021 05:19:39.633921 139592362338048 logging_writer.py:48] [29475] accumulated_eval_time=2414.567756, accumulated_logging_time=0.879472, accumulated_submission_time=24561.724899, global_step=29475, preemption_count=0, score=24561.724899, test/ctc_loss=0.32637402415275574, test/num_examples=2472, test/wer=0.107773, total_duration=26978.488636, train/ctc_loss=0.2885570228099823, train/wer=0.101989, validation/ctc_loss=0.5297839641571045, validation/num_examples=5348, validation/wer=0.158896
I1021 05:19:59.436231 139592353945344 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.7656921148300171, loss=1.407110571861267
I1021 05:26:39.807746 139585750386432 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7409106492996216, loss=1.416911005973816
I1021 05:33:31.850047 139584725841664 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6816935539245605, loss=1.3615264892578125
I1021 05:40:44.453958 139592362338048 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.7326856255531311, loss=1.3655234575271606
I1021 05:43:39.610067 139765020145472 spec.py:321] Evaluating on the training split.
I1021 05:44:33.331434 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 05:45:24.081589 139765020145472 spec.py:349] Evaluating on the test split.
I1021 05:45:49.817183 139765020145472 submission_runner.py:395] Time since start: 28548.72s, 	Step: 31227, 	{'train/ctc_loss': Array(0.26182705, dtype=float32), 'train/wer': 0.09105855159279215, 'validation/ctc_loss': Array(0.5137404, dtype=float32), 'validation/wer': 0.15284281259352944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31195962, dtype=float32), 'test/wer': 0.10318282452826356, 'test/num_examples': 2472, 'score': 26001.605697870255, 'total_duration': 28548.715017795563, 'accumulated_submission_time': 26001.605697870255, 'accumulated_eval_time': 2544.769999027252, 'accumulated_logging_time': 0.9440271854400635}
I1021 05:45:49.853979 139592362338048 logging_writer.py:48] [31227] accumulated_eval_time=2544.769999, accumulated_logging_time=0.944027, accumulated_submission_time=26001.605698, global_step=31227, preemption_count=0, score=26001.605698, test/ctc_loss=0.3119596242904663, test/num_examples=2472, test/wer=0.103183, total_duration=28548.715018, train/ctc_loss=0.2618270516395569, train/wer=0.091059, validation/ctc_loss=0.5137404203414917, validation/num_examples=5348, validation/wer=0.152843
I1021 05:49:23.266773 139592353945344 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6723487377166748, loss=1.4302067756652832
I1021 05:56:43.545948 139591706978048 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.742195725440979, loss=1.3561961650848389
I1021 06:03:35.093930 139591698585344 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7230657935142517, loss=1.3704602718353271
I1021 06:09:50.009685 139765020145472 spec.py:321] Evaluating on the training split.
I1021 06:10:45.500497 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 06:11:36.924977 139765020145472 spec.py:349] Evaluating on the test split.
I1021 06:12:03.462564 139765020145472 submission_runner.py:395] Time since start: 30122.36s, 	Step: 32920, 	{'train/ctc_loss': Array(0.2649976, dtype=float32), 'train/wer': 0.09215129270710797, 'validation/ctc_loss': Array(0.5026061, dtype=float32), 'validation/wer': 0.148855440879732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2970402, dtype=float32), 'test/wer': 0.09737371275364085, 'test/num_examples': 2472, 'score': 27441.675806999207, 'total_duration': 30122.35778260231, 'accumulated_submission_time': 27441.675806999207, 'accumulated_eval_time': 2678.2153701782227, 'accumulated_logging_time': 0.995917558670044}
I1021 06:12:03.504223 139592362338048 logging_writer.py:48] [32920] accumulated_eval_time=2678.215370, accumulated_logging_time=0.995918, accumulated_submission_time=27441.675807, global_step=32920, preemption_count=0, score=27441.675807, test/ctc_loss=0.297040194272995, test/num_examples=2472, test/wer=0.097374, total_duration=30122.357783, train/ctc_loss=0.26499760150909424, train/wer=0.092151, validation/ctc_loss=0.5026060938835144, validation/num_examples=5348, validation/wer=0.148855
I1021 06:13:08.983417 139592362338048 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7529894113540649, loss=1.3105767965316772
I1021 06:19:55.020639 139592353945344 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7771676778793335, loss=1.3535040616989136
I1021 06:27:21.374861 139592362338048 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7234246730804443, loss=1.3277877569198608
I1021 06:33:55.219580 139592353945344 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7566051483154297, loss=1.3384567499160767
I1021 06:36:04.060509 139765020145472 spec.py:321] Evaluating on the training split.
I1021 06:36:58.522146 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 06:37:50.201077 139765020145472 spec.py:349] Evaluating on the test split.
I1021 06:38:16.338699 139765020145472 submission_runner.py:395] Time since start: 31695.24s, 	Step: 34647, 	{'train/ctc_loss': Array(0.25350338, dtype=float32), 'train/wer': 0.08540341230005222, 'validation/ctc_loss': Array(0.483826, dtype=float32), 'validation/wer': 0.14208752908464234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28813714, dtype=float32), 'test/wer': 0.0954034895293807, 'test/num_examples': 2472, 'score': 28882.14173388481, 'total_duration': 31695.236352682114, 'accumulated_submission_time': 28882.14173388481, 'accumulated_eval_time': 2810.4885330200195, 'accumulated_logging_time': 1.0557634830474854}
I1021 06:38:16.375406 139591932258048 logging_writer.py:48] [34647] accumulated_eval_time=2810.488533, accumulated_logging_time=1.055763, accumulated_submission_time=28882.141734, global_step=34647, preemption_count=0, score=28882.141734, test/ctc_loss=0.28813713788986206, test/num_examples=2472, test/wer=0.095403, total_duration=31695.236353, train/ctc_loss=0.25350338220596313, train/wer=0.085403, validation/ctc_loss=0.48382601141929626, validation/num_examples=5348, validation/wer=0.142088
I1021 06:42:56.584569 139591923865344 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7977299094200134, loss=1.3482122421264648
I1021 06:49:36.455295 139591932258048 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.8103274703025818, loss=1.3279423713684082
I1021 06:56:56.054574 139591923865344 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7456607818603516, loss=1.3294978141784668
I1021 07:02:16.916788 139765020145472 spec.py:321] Evaluating on the training split.
I1021 07:03:12.632369 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 07:04:04.004633 139765020145472 spec.py:349] Evaluating on the test split.
I1021 07:04:29.989308 139765020145472 submission_runner.py:395] Time since start: 33268.89s, 	Step: 36411, 	{'train/ctc_loss': Array(0.18878697, dtype=float32), 'train/wer': 0.0681867101257649, 'validation/ctc_loss': Array(0.46347603, dtype=float32), 'validation/wer': 0.13604371626905587, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26919183, dtype=float32), 'test/wer': 0.08963500091402109, 'test/num_examples': 2472, 'score': 30322.592145204544, 'total_duration': 33268.88709831238, 'accumulated_submission_time': 30322.592145204544, 'accumulated_eval_time': 2943.5563192367554, 'accumulated_logging_time': 1.1093976497650146}
I1021 07:04:30.028419 139592362338048 logging_writer.py:48] [36411] accumulated_eval_time=2943.556319, accumulated_logging_time=1.109398, accumulated_submission_time=30322.592145, global_step=36411, preemption_count=0, score=30322.592145, test/ctc_loss=0.26919183135032654, test/num_examples=2472, test/wer=0.089635, total_duration=33268.887098, train/ctc_loss=0.18878696858882904, train/wer=0.068187, validation/ctc_loss=0.4634760320186615, validation/num_examples=5348, validation/wer=0.136044
I1021 07:05:38.544332 139592353945344 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.7792512774467468, loss=1.3362641334533691
I1021 07:12:42.533817 139592362338048 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7820912599563599, loss=1.3066515922546387
I1021 07:19:23.170129 139592362338048 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.8427270650863647, loss=1.271236538887024
I1021 07:26:44.106093 139592353945344 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.7686312794685364, loss=1.2521753311157227
I1021 07:28:30.182253 139765020145472 spec.py:321] Evaluating on the training split.
I1021 07:29:24.788375 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 07:30:15.842836 139765020145472 spec.py:349] Evaluating on the test split.
I1021 07:30:42.395213 139765020145472 submission_runner.py:395] Time since start: 34841.29s, 	Step: 38122, 	{'train/ctc_loss': Array(0.20645283, dtype=float32), 'train/wer': 0.07201537454330426, 'validation/ctc_loss': Array(0.44553778, dtype=float32), 'validation/wer': 0.13231702018787955, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2558761, dtype=float32), 'test/wer': 0.08380557755976682, 'test/num_examples': 2472, 'score': 31762.658802509308, 'total_duration': 34841.29111742973, 'accumulated_submission_time': 31762.658802509308, 'accumulated_eval_time': 3075.7626280784607, 'accumulated_logging_time': 1.1638495922088623}
I1021 07:30:42.437288 139591778658048 logging_writer.py:48] [38122] accumulated_eval_time=3075.762628, accumulated_logging_time=1.163850, accumulated_submission_time=31762.658803, global_step=38122, preemption_count=0, score=31762.658803, test/ctc_loss=0.2558760941028595, test/num_examples=2472, test/wer=0.083806, total_duration=34841.291117, train/ctc_loss=0.20645283162593842, train/wer=0.072015, validation/ctc_loss=0.4455377757549286, validation/num_examples=5348, validation/wer=0.132317
I1021 07:35:27.732867 139591770265344 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7878839373588562, loss=1.2164684534072876
I1021 07:42:44.271274 139591778658048 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.9643463492393494, loss=1.2453689575195312
I1021 07:49:24.236824 139591778658048 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.9700821042060852, loss=1.2126868963241577
I1021 07:54:43.047157 139765020145472 spec.py:321] Evaluating on the training split.
I1021 07:55:36.509747 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 07:56:28.227526 139765020145472 spec.py:349] Evaluating on the test split.
I1021 07:56:54.049540 139765020145472 submission_runner.py:395] Time since start: 36412.95s, 	Step: 39863, 	{'train/ctc_loss': Array(0.2533094, dtype=float32), 'train/wer': 0.08796233517571313, 'validation/ctc_loss': Array(0.41986907, dtype=float32), 'validation/wer': 0.12479604545410661, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24632764, dtype=float32), 'test/wer': 0.08151036906140191, 'test/num_examples': 2472, 'score': 33203.18149089813, 'total_duration': 36412.94720220566, 'accumulated_submission_time': 33203.18149089813, 'accumulated_eval_time': 3206.759986639023, 'accumulated_logging_time': 1.2221400737762451}
I1021 07:56:54.091038 139591481693952 logging_writer.py:48] [39863] accumulated_eval_time=3206.759987, accumulated_logging_time=1.222140, accumulated_submission_time=33203.181491, global_step=39863, preemption_count=0, score=33203.181491, test/ctc_loss=0.24632763862609863, test/num_examples=2472, test/wer=0.081510, total_duration=36412.947202, train/ctc_loss=0.2533093988895416, train/wer=0.087962, validation/ctc_loss=0.419869065284729, validation/num_examples=5348, validation/wer=0.124796
I1021 07:58:38.260500 139591473301248 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.8365299701690674, loss=1.2655872106552124
I1021 08:05:17.162030 139591154013952 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.8581557869911194, loss=1.2020107507705688
I1021 08:12:43.121279 139591145621248 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.8511403203010559, loss=1.2619447708129883
I1021 08:19:33.980388 139591481693952 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8219155073165894, loss=1.1892294883728027
I1021 08:20:54.469793 139765020145472 spec.py:321] Evaluating on the training split.
I1021 08:21:47.273319 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 08:22:39.830423 139765020145472 spec.py:349] Evaluating on the test split.
I1021 08:23:06.796292 139765020145472 submission_runner.py:395] Time since start: 37985.69s, 	Step: 41598, 	{'train/ctc_loss': Array(0.24586582, dtype=float32), 'train/wer': 0.08654524056690185, 'validation/ctc_loss': Array(0.42336622, dtype=float32), 'validation/wer': 0.12315475443389942, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2361669, dtype=float32), 'test/wer': 0.07850425527593281, 'test/num_examples': 2472, 'score': 34643.4685151577, 'total_duration': 37985.69330191612, 'accumulated_submission_time': 34643.4685151577, 'accumulated_eval_time': 3339.0807819366455, 'accumulated_logging_time': 1.2829980850219727}
I1021 08:23:06.832124 139591481693952 logging_writer.py:48] [41598] accumulated_eval_time=3339.080782, accumulated_logging_time=1.282998, accumulated_submission_time=34643.468515, global_step=41598, preemption_count=0, score=34643.468515, test/ctc_loss=0.23616689443588257, test/num_examples=2472, test/wer=0.078504, total_duration=37985.693302, train/ctc_loss=0.2458658218383789, train/wer=0.086545, validation/ctc_loss=0.4233662188053131, validation/num_examples=5348, validation/wer=0.123155
I1021 08:28:38.311308 139591473301248 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.8545040488243103, loss=1.20778226852417
I1021 08:35:32.047160 139591154013952 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.9962095022201538, loss=1.1986756324768066
I1021 08:42:50.795676 139591145621248 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.9486635327339172, loss=1.152369737625122
I1021 08:47:07.219671 139765020145472 spec.py:321] Evaluating on the training split.
I1021 08:47:58.575696 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 08:48:51.176337 139765020145472 spec.py:349] Evaluating on the test split.
I1021 08:49:17.840557 139765020145472 submission_runner.py:395] Time since start: 39556.74s, 	Step: 43283, 	{'train/ctc_loss': Array(0.26693735, dtype=float32), 'train/wer': 0.09477569595966095, 'validation/ctc_loss': Array(0.39970118, dtype=float32), 'validation/wer': 0.11750678239377468, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22665474, dtype=float32), 'test/wer': 0.07427944671257083, 'test/num_examples': 2472, 'score': 36083.76709651947, 'total_duration': 39556.73584342003, 'accumulated_submission_time': 36083.76709651947, 'accumulated_eval_time': 3469.694420814514, 'accumulated_logging_time': 1.337653636932373}
I1021 08:49:17.878421 139591154013952 logging_writer.py:48] [43283] accumulated_eval_time=3469.694421, accumulated_logging_time=1.337654, accumulated_submission_time=36083.767097, global_step=43283, preemption_count=0, score=36083.767097, test/ctc_loss=0.22665473818778992, test/num_examples=2472, test/wer=0.074279, total_duration=39556.735843, train/ctc_loss=0.26693734526634216, train/wer=0.094776, validation/ctc_loss=0.39970117807388306, validation/num_examples=5348, validation/wer=0.117507
I1021 08:52:02.233757 139591145621248 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.8476840257644653, loss=1.159265398979187
I1021 08:59:03.639008 139591154013952 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.9555544853210449, loss=1.1416782140731812
I1021 09:06:01.151840 139591154013952 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.9145426154136658, loss=1.1824302673339844
I1021 09:13:13.608646 139591145621248 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.9996805191040039, loss=1.1984550952911377
I1021 09:13:18.541474 139765020145472 spec.py:321] Evaluating on the training split.
I1021 09:14:10.189145 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 09:15:02.285034 139765020145472 spec.py:349] Evaluating on the test split.
I1021 09:15:28.441051 139765020145472 submission_runner.py:395] Time since start: 41127.34s, 	Step: 45007, 	{'train/ctc_loss': Array(0.23055223, dtype=float32), 'train/wer': 0.0809719221190587, 'validation/ctc_loss': Array(0.38633734, dtype=float32), 'validation/wer': 0.11462969578188208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21498065, dtype=float32), 'test/wer': 0.0721467308512583, 'test/num_examples': 2472, 'score': 37524.34188055992, 'total_duration': 41127.338093042374, 'accumulated_submission_time': 37524.34188055992, 'accumulated_eval_time': 3599.588337659836, 'accumulated_logging_time': 1.3909001350402832}
I1021 09:15:28.482515 139591154013952 logging_writer.py:48] [45007] accumulated_eval_time=3599.588338, accumulated_logging_time=1.390900, accumulated_submission_time=37524.341881, global_step=45007, preemption_count=0, score=37524.341881, test/ctc_loss=0.21498064696788788, test/num_examples=2472, test/wer=0.072147, total_duration=41127.338093, train/ctc_loss=0.23055222630500793, train/wer=0.080972, validation/ctc_loss=0.3863373398780823, validation/num_examples=5348, validation/wer=0.114630
I1021 09:21:57.138571 139591481693952 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.0558648109436035, loss=1.1775151491165161
I1021 09:29:05.561977 139591473301248 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9032834768295288, loss=1.1038190126419067
I1021 09:36:14.925119 139591481693952 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.9842275977134705, loss=1.088269829750061
I1021 09:39:29.149051 139765020145472 spec.py:321] Evaluating on the training split.
I1021 09:40:21.904571 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 09:41:14.776749 139765020145472 spec.py:349] Evaluating on the test split.
I1021 09:41:41.257526 139765020145472 submission_runner.py:395] Time since start: 42700.15s, 	Step: 46747, 	{'train/ctc_loss': Array(0.20073223, dtype=float32), 'train/wer': 0.07153196085199247, 'validation/ctc_loss': Array(0.37127587, dtype=float32), 'validation/wer': 0.10727285015012986, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20473592, dtype=float32), 'test/wer': 0.06759693701379156, 'test/num_examples': 2472, 'score': 38964.92105579376, 'total_duration': 42700.15410685539, 'accumulated_submission_time': 38964.92105579376, 'accumulated_eval_time': 3731.6906871795654, 'accumulated_logging_time': 1.4469618797302246}
I1021 09:41:41.293292 139591932258048 logging_writer.py:48] [46747] accumulated_eval_time=3731.690687, accumulated_logging_time=1.446962, accumulated_submission_time=38964.921056, global_step=46747, preemption_count=0, score=38964.921056, test/ctc_loss=0.2047359198331833, test/num_examples=2472, test/wer=0.067597, total_duration=42700.154107, train/ctc_loss=0.20073223114013672, train/wer=0.071532, validation/ctc_loss=0.3712758719921112, validation/num_examples=5348, validation/wer=0.107273
I1021 09:44:56.209319 139591923865344 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.9322255253791809, loss=1.0876085758209229
I1021 09:52:12.225537 139591604578048 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.9103910326957703, loss=1.0865542888641357
I1021 09:59:10.811009 139591596185344 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0624001026153564, loss=1.1221187114715576
I1021 10:05:41.653415 139765020145472 spec.py:321] Evaluating on the training split.
I1021 10:06:35.305616 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 10:07:27.565076 139765020145472 spec.py:349] Evaluating on the test split.
I1021 10:07:54.068005 139765020145472 submission_runner.py:395] Time since start: 44272.96s, 	Step: 48432, 	{'train/ctc_loss': Array(0.15851687, dtype=float32), 'train/wer': 0.057189311466138835, 'validation/ctc_loss': Array(0.35182622, dtype=float32), 'validation/wer': 0.10227173986502795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1961421, dtype=float32), 'test/wer': 0.06404241057827068, 'test/num_examples': 2472, 'score': 40405.19302892685, 'total_duration': 44272.9643907547, 'accumulated_submission_time': 40405.19302892685, 'accumulated_eval_time': 3864.09903049469, 'accumulated_logging_time': 1.5005207061767578}
I1021 10:07:54.114563 139591312738048 logging_writer.py:48] [48432] accumulated_eval_time=3864.099030, accumulated_logging_time=1.500521, accumulated_submission_time=40405.193029, global_step=48432, preemption_count=0, score=40405.193029, test/ctc_loss=0.19614210724830627, test/num_examples=2472, test/wer=0.064042, total_duration=44272.964391, train/ctc_loss=0.15851686894893646, train/wer=0.057189, validation/ctc_loss=0.3518262207508087, validation/num_examples=5348, validation/wer=0.102272
I1021 10:08:45.913208 139591304345344 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0235118865966797, loss=1.107932448387146
I1021 10:15:24.105746 139591312738048 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9074514508247375, loss=1.073869228363037
I1021 10:22:48.063488 139591312738048 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.0226200819015503, loss=1.0451788902282715
I1021 10:29:37.615531 139591304345344 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.1998709440231323, loss=1.0367075204849243
I1021 10:31:55.259782 139765020145472 spec.py:321] Evaluating on the training split.
I1021 10:32:49.069430 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 10:33:40.431063 139765020145472 spec.py:349] Evaluating on the test split.
I1021 10:34:06.534453 139765020145472 submission_runner.py:395] Time since start: 45845.43s, 	Step: 50155, 	{'train/ctc_loss': Array(0.16453648, dtype=float32), 'train/wer': 0.05955671900373112, 'validation/ctc_loss': Array(0.3386692, dtype=float32), 'validation/wer': 0.0985836623960918, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18856141, dtype=float32), 'test/wer': 0.061320658907643245, 'test/num_examples': 2472, 'score': 41846.24918437004, 'total_duration': 45845.43116354942, 'accumulated_submission_time': 41846.24918437004, 'accumulated_eval_time': 3995.3676929473877, 'accumulated_logging_time': 1.5626299381256104}
I1021 10:34:06.568726 139592362338048 logging_writer.py:48] [50155] accumulated_eval_time=3995.367693, accumulated_logging_time=1.562630, accumulated_submission_time=41846.249184, global_step=50155, preemption_count=0, score=41846.249184, test/ctc_loss=0.18856140971183777, test/num_examples=2472, test/wer=0.061321, total_duration=45845.431164, train/ctc_loss=0.1645364761352539, train/wer=0.059557, validation/ctc_loss=0.33866921067237854, validation/num_examples=5348, validation/wer=0.098584
I1021 10:38:49.495182 139592362338048 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.9992238283157349, loss=1.0643534660339355
I1021 10:45:37.772346 139592353945344 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.062917709350586, loss=1.040370225906372
I1021 10:53:13.457072 139592362338048 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.0776946544647217, loss=0.9900029301643372
I1021 10:58:07.098517 139765020145472 spec.py:321] Evaluating on the training split.
I1021 10:59:00.477607 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 10:59:51.724685 139765020145472 spec.py:349] Evaluating on the test split.
I1021 11:00:18.201758 139765020145472 submission_runner.py:395] Time since start: 47417.10s, 	Step: 51880, 	{'train/ctc_loss': Array(0.14188813, dtype=float32), 'train/wer': 0.05083426387218691, 'validation/ctc_loss': Array(0.32378784, dtype=float32), 'validation/wer': 0.0940170114986918, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17722452, dtype=float32), 'test/wer': 0.05794893668880629, 'test/num_examples': 2472, 'score': 43286.69103932381, 'total_duration': 47417.0975458622, 'accumulated_submission_time': 43286.69103932381, 'accumulated_eval_time': 4126.463999271393, 'accumulated_logging_time': 1.6127378940582275}
I1021 11:00:18.243228 139592362338048 logging_writer.py:48] [51880] accumulated_eval_time=4126.463999, accumulated_logging_time=1.612738, accumulated_submission_time=43286.691039, global_step=51880, preemption_count=0, score=43286.691039, test/ctc_loss=0.1772245168685913, test/num_examples=2472, test/wer=0.057949, total_duration=47417.097546, train/ctc_loss=0.14188812673091888, train/wer=0.050834, validation/ctc_loss=0.3237878382205963, validation/num_examples=5348, validation/wer=0.094017
I1021 11:01:49.470257 139592353945344 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.0644798278808594, loss=1.0001318454742432
I1021 11:09:04.236644 139592362338048 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.0724518299102783, loss=1.0196313858032227
I1021 11:15:49.005420 139591706978048 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.186768889427185, loss=0.984494686126709
I1021 11:23:17.215376 139591698585344 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.3543680906295776, loss=0.9867660999298096
I1021 11:24:18.856016 139765020145472 spec.py:321] Evaluating on the training split.
I1021 11:25:13.057690 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 11:26:04.019225 139765020145472 spec.py:349] Evaluating on the test split.
I1021 11:26:29.944508 139765020145472 submission_runner.py:395] Time since start: 48988.84s, 	Step: 53569, 	{'train/ctc_loss': Array(0.13709657, dtype=float32), 'train/wer': 0.05001681510973215, 'validation/ctc_loss': Array(0.32031515, dtype=float32), 'validation/wer': 0.09185436921324232, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17172539, dtype=float32), 'test/wer': 0.05636463347754555, 'test/num_examples': 2472, 'score': 44727.2161860466, 'total_duration': 48988.84048175812, 'accumulated_submission_time': 44727.2161860466, 'accumulated_eval_time': 4257.54591012001, 'accumulated_logging_time': 1.6709940433502197}
I1021 11:26:29.980167 139591415138048 logging_writer.py:48] [53569] accumulated_eval_time=4257.545910, accumulated_logging_time=1.670994, accumulated_submission_time=44727.216186, global_step=53569, preemption_count=0, score=44727.216186, test/ctc_loss=0.17172539234161377, test/num_examples=2472, test/wer=0.056365, total_duration=48988.840482, train/ctc_loss=0.13709656894207, train/wer=0.050017, validation/ctc_loss=0.3203151524066925, validation/num_examples=5348, validation/wer=0.091854
I1021 11:31:56.940086 139591406745344 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.0959316492080688, loss=1.0066317319869995
I1021 11:39:17.630423 139591415138048 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.346336841583252, loss=1.0456945896148682
I1021 11:46:06.412101 139591087458048 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.136722445487976, loss=1.0083562135696411
I1021 11:50:30.174209 139765020145472 spec.py:321] Evaluating on the training split.
I1021 11:51:23.640941 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 11:52:15.023957 139765020145472 spec.py:349] Evaluating on the test split.
I1021 11:52:41.698543 139765020145472 submission_runner.py:395] Time since start: 50560.60s, 	Step: 55299, 	{'train/ctc_loss': Array(0.12422652, dtype=float32), 'train/wer': 0.04451621808143547, 'validation/ctc_loss': Array(0.30715603, dtype=float32), 'validation/wer': 0.08865867905036832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16657917, dtype=float32), 'test/wer': 0.05402880181991754, 'test/num_examples': 2472, 'score': 46167.32132577896, 'total_duration': 50560.595286369324, 'accumulated_submission_time': 46167.32132577896, 'accumulated_eval_time': 4389.0642812252045, 'accumulated_logging_time': 1.72357177734375}
I1021 11:52:41.736762 139591087458048 logging_writer.py:48] [55299] accumulated_eval_time=4389.064281, accumulated_logging_time=1.723572, accumulated_submission_time=46167.321326, global_step=55299, preemption_count=0, score=46167.321326, test/ctc_loss=0.16657917201519012, test/num_examples=2472, test/wer=0.054029, total_duration=50560.595286, train/ctc_loss=0.12422651797533035, train/wer=0.044516, validation/ctc_loss=0.3071560263633728, validation/num_examples=5348, validation/wer=0.088659
I1021 11:55:14.776161 139591079065344 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.3281636238098145, loss=1.0096936225891113
I1021 12:01:57.117452 139591087458048 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.1880254745483398, loss=0.9665156602859497
I1021 12:09:31.838241 139591079065344 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.2454214096069336, loss=0.9530318379402161
I1021 12:16:20.501877 139591415138048 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1841883659362793, loss=0.9810287356376648
I1021 12:16:42.089268 139765020145472 spec.py:321] Evaluating on the training split.
I1021 12:17:34.133877 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 12:18:25.158934 139765020145472 spec.py:349] Evaluating on the test split.
I1021 12:18:52.442246 139765020145472 submission_runner.py:395] Time since start: 52131.34s, 	Step: 57027, 	{'train/ctc_loss': Array(0.12671377, dtype=float32), 'train/wer': 0.045980563854263246, 'validation/ctc_loss': Array(0.3025658, dtype=float32), 'validation/wer': 0.08685325892814041, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16329002, dtype=float32), 'test/wer': 0.05339914285133955, 'test/num_examples': 2472, 'score': 47607.58441400528, 'total_duration': 52131.33870744705, 'accumulated_submission_time': 47607.58441400528, 'accumulated_eval_time': 4519.41101193428, 'accumulated_logging_time': 1.7783310413360596}
I1021 12:18:52.477139 139591415138048 logging_writer.py:48] [57027] accumulated_eval_time=4519.411012, accumulated_logging_time=1.778331, accumulated_submission_time=47607.584414, global_step=57027, preemption_count=0, score=47607.584414, test/ctc_loss=0.16329002380371094, test/num_examples=2472, test/wer=0.053399, total_duration=52131.338707, train/ctc_loss=0.12671376764774323, train/wer=0.045981, validation/ctc_loss=0.3025658130645752, validation/num_examples=5348, validation/wer=0.086853
I1021 12:25:21.229380 139591406745344 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.9940714240074158, loss=0.964164674282074
I1021 12:32:07.586909 139592362338048 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.1416940689086914, loss=0.9621004462242126
I1021 12:39:28.549724 139592353945344 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.2487001419067383, loss=0.9527379870414734
I1021 12:42:52.954209 139765020145472 spec.py:321] Evaluating on the training split.
I1021 12:43:46.662579 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 12:44:37.824578 139765020145472 spec.py:349] Evaluating on the test split.
I1021 12:45:04.105338 139765020145472 submission_runner.py:395] Time since start: 53703.00s, 	Step: 58730, 	{'train/ctc_loss': Array(0.13518772, dtype=float32), 'train/wer': 0.04794759254620504, 'validation/ctc_loss': Array(0.29995033, dtype=float32), 'validation/wer': 0.08582986570377593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1612366, dtype=float32), 'test/wer': 0.052058578595657386, 'test/num_examples': 2472, 'score': 49047.97480177879, 'total_duration': 53703.002312898636, 'accumulated_submission_time': 49047.97480177879, 'accumulated_eval_time': 4650.556568622589, 'accumulated_logging_time': 1.8286492824554443}
I1021 12:45:04.141904 139591415138048 logging_writer.py:48] [58730] accumulated_eval_time=4650.556569, accumulated_logging_time=1.828649, accumulated_submission_time=49047.974802, global_step=58730, preemption_count=0, score=49047.974802, test/ctc_loss=0.16123659908771515, test/num_examples=2472, test/wer=0.052059, total_duration=53703.002313, train/ctc_loss=0.13518771529197693, train/wer=0.047948, validation/ctc_loss=0.29995033144950867, validation/num_examples=5348, validation/wer=0.085830
I1021 12:48:29.151531 139591406745344 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.2350804805755615, loss=0.9424825310707092
I1021 12:55:32.525537 139591415138048 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.2196106910705566, loss=0.9723580479621887
I1021 13:02:23.136914 139765020145472 spec.py:321] Evaluating on the training split.
I1021 13:03:15.442771 139765020145472 spec.py:333] Evaluating on the validation split.
I1021 13:04:07.384583 139765020145472 spec.py:349] Evaluating on the test split.
I1021 13:04:33.660919 139765020145472 submission_runner.py:395] Time since start: 54872.56s, 	Step: 60000, 	{'train/ctc_loss': Array(0.11520543, dtype=float32), 'train/wer': 0.04230277759096106, 'validation/ctc_loss': Array(0.29943058, dtype=float32), 'validation/wer': 0.08577193778541568, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16139245, dtype=float32), 'test/wer': 0.05191639753823655, 'test/num_examples': 2472, 'score': 50086.90069746971, 'total_duration': 54872.56066775322, 'accumulated_submission_time': 50086.90069746971, 'accumulated_eval_time': 4781.077726840973, 'accumulated_logging_time': 1.881331205368042}
I1021 13:04:33.699393 139591415138048 logging_writer.py:48] [60000] accumulated_eval_time=4781.077727, accumulated_logging_time=1.881331, accumulated_submission_time=50086.900697, global_step=60000, preemption_count=0, score=50086.900697, test/ctc_loss=0.1613924503326416, test/num_examples=2472, test/wer=0.051916, total_duration=54872.560668, train/ctc_loss=0.11520542949438095, train/wer=0.042303, validation/ctc_loss=0.29943057894706726, validation/num_examples=5348, validation/wer=0.085772
I1021 13:04:33.723767 139591406745344 logging_writer.py:48] [60000] global_step=60000, preemption_count=0, score=50086.900697
I1021 13:04:34.173507 139765020145472 checkpoints.py:490] Saving checkpoint at step: 60000
I1021 13:04:35.662003 139765020145472 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_conformer/adamw_run1/librispeech_conformer_jax/trial_1/checkpoint_60000
I1021 13:04:35.697911 139765020145472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_conformer/adamw_run1/librispeech_conformer_jax/trial_1/checkpoint_60000.
I1021 13:04:37.264928 139765020145472 submission_runner.py:565] Tuning trial 1/1
I1021 13:04:37.265200 139765020145472 submission_runner.py:566] Hyperparameters: Hyperparameters(learning_rate=0.002106913873888147, beta1=0.8231189937738506, beta2=0.8774571227688758, warmup_steps=1199, weight_decay=0.27590534177690645)
I1021 13:04:37.286464 139765020145472 submission_runner.py:567] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.724596, dtype=float32), 'train/wer': 1.2776221810382524, 'validation/ctc_loss': Array(30.79964, dtype=float32), 'validation/wer': 1.076503470847775, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.920681, dtype=float32), 'test/wer': 1.1024719192411594, 'test/num_examples': 2472, 'score': 77.8982207775116, 'total_duration': 252.44418621063232, 'accumulated_submission_time': 77.8982207775116, 'accumulated_eval_time': 174.54589772224426, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1750, {'train/ctc_loss': Array(2.3897297, dtype=float32), 'train/wer': 0.5442112417869869, 'validation/ctc_loss': Array(2.9253056, dtype=float32), 'validation/wer': 0.5861243326221072, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5503435, dtype=float32), 'test/wer': 0.5324071253021347, 'test/num_examples': 2472, 'score': 1517.9625606536865, 'total_duration': 1817.529152393341, 'accumulated_submission_time': 1517.9625606536865, 'accumulated_eval_time': 299.45716977119446, 'accumulated_logging_time': 0.04081392288208008, 'global_step': 1750, 'preemption_count': 0}), (3550, {'train/ctc_loss': Array(0.66659975, dtype=float32), 'train/wer': 0.2331406653703753, 'validation/ctc_loss': Array(0.9881299, dtype=float32), 'validation/wer': 0.293839365882387, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6959028, dtype=float32), 'test/wer': 0.23392846261653769, 'test/num_examples': 2472, 'score': 2958.3294310569763, 'total_duration': 3386.9722702503204, 'accumulated_submission_time': 2958.3294310569763, 'accumulated_eval_time': 428.40828919410706, 'accumulated_logging_time': 0.08768963813781738, 'global_step': 3550, 'preemption_count': 0}), (5314, {'train/ctc_loss': Array(0.49469042, dtype=float32), 'train/wer': 0.1766792647764231, 'validation/ctc_loss': Array(0.8382224, dtype=float32), 'validation/wer': 0.24736186605134344, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5657178, dtype=float32), 'test/wer': 0.18759774947697683, 'test/num_examples': 2472, 'score': 4398.463261127472, 'total_duration': 4957.676019191742, 'accumulated_submission_time': 4398.463261127472, 'accumulated_eval_time': 558.8476555347443, 'accumulated_logging_time': 0.1399669647216797, 'global_step': 5314, 'preemption_count': 0}), (7066, {'train/ctc_loss': Array(0.45440343, dtype=float32), 'train/wer': 0.157892532124119, 'validation/ctc_loss': Array(0.75882304, dtype=float32), 'validation/wer': 0.22641126891105168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5047663, dtype=float32), 'test/wer': 0.16880953831779497, 'test/num_examples': 2472, 'score': 5838.795207500458, 'total_duration': 6529.786837816238, 'accumulated_submission_time': 5838.795207500458, 'accumulated_eval_time': 690.4999713897705, 'accumulated_logging_time': 0.1893620491027832, 'global_step': 7066, 'preemption_count': 0}), (8836, {'train/ctc_loss': Array(0.4505591, dtype=float32), 'train/wer': 0.15683641740416188, 'validation/ctc_loss': Array(0.7456387, dtype=float32), 'validation/wer': 0.22204736572791256, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4905502, dtype=float32), 'test/wer': 0.1636097739321187, 'test/num_examples': 2472, 'score': 7279.721311330795, 'total_duration': 8101.536092996597, 'accumulated_submission_time': 7279.721311330795, 'accumulated_eval_time': 821.1913588047028, 'accumulated_logging_time': 0.23985028266906738, 'global_step': 8836, 'preemption_count': 0}), (10583, {'train/ctc_loss': Array(0.45575836, dtype=float32), 'train/wer': 0.1574384772553894, 'validation/ctc_loss': Array(0.73247176, dtype=float32), 'validation/wer': 0.21679523446324955, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47312295, dtype=float32), 'test/wer': 0.15820689375012695, 'test/num_examples': 2472, 'score': 8719.689422130585, 'total_duration': 9672.887839078903, 'accumulated_submission_time': 8719.689422130585, 'accumulated_eval_time': 952.4340887069702, 'accumulated_logging_time': 0.303358793258667, 'global_step': 10583, 'preemption_count': 0}), (12284, {'train/ctc_loss': Array(0.41217625, dtype=float32), 'train/wer': 0.14335928360240785, 'validation/ctc_loss': Array(0.6922513, dtype=float32), 'validation/wer': 0.20763296870926942, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44658628, dtype=float32), 'test/wer': 0.15002132715861313, 'test/num_examples': 2472, 'score': 10159.897886276245, 'total_duration': 11244.863183498383, 'accumulated_submission_time': 10159.897886276245, 'accumulated_eval_time': 1084.073422908783, 'accumulated_logging_time': 0.3556859493255615, 'global_step': 12284, 'preemption_count': 0}), (14004, {'train/ctc_loss': Array(0.3658871, dtype=float32), 'train/wer': 0.1305754179775865, 'validation/ctc_loss': Array(0.6747159, dtype=float32), 'validation/wer': 0.2009809127509003, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42511925, dtype=float32), 'test/wer': 0.1445778238173583, 'test/num_examples': 2472, 'score': 11599.932636737823, 'total_duration': 12818.575507879257, 'accumulated_submission_time': 11599.932636737823, 'accumulated_eval_time': 1217.6207690238953, 'accumulated_logging_time': 0.4077177047729492, 'global_step': 14004, 'preemption_count': 0}), (15739, {'train/ctc_loss': Array(0.35378382, dtype=float32), 'train/wer': 0.12363740829604881, 'validation/ctc_loss': Array(0.66440237, dtype=float32), 'validation/wer': 0.19839346573080896, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42792174, dtype=float32), 'test/wer': 0.1423029268986249, 'test/num_examples': 2472, 'score': 13039.918533563614, 'total_duration': 14391.694628477097, 'accumulated_submission_time': 13039.918533563614, 'accumulated_eval_time': 1350.6283848285675, 'accumulated_logging_time': 0.4568822383880615, 'global_step': 15739, 'preemption_count': 0}), (17440, {'train/ctc_loss': Array(0.3488905, dtype=float32), 'train/wer': 0.12355001153019032, 'validation/ctc_loss': Array(0.6491723, dtype=float32), 'validation/wer': 0.19232068895604237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40822417, dtype=float32), 'test/wer': 0.13478764243495217, 'test/num_examples': 2472, 'score': 14480.287340402603, 'total_duration': 15963.988492965698, 'accumulated_submission_time': 14480.287340402603, 'accumulated_eval_time': 1482.4246878623962, 'accumulated_logging_time': 0.5075671672821045, 'global_step': 17440, 'preemption_count': 0}), (19158, {'train/ctc_loss': Array(0.352684, dtype=float32), 'train/wer': 0.12276369020862529, 'validation/ctc_loss': Array(0.6297174, dtype=float32), 'validation/wer': 0.18653755177307704, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3977691, dtype=float32), 'test/wer': 0.13186277496800927, 'test/num_examples': 2472, 'score': 15920.269403457642, 'total_duration': 17537.09039068222, 'accumulated_submission_time': 15920.269403457642, 'accumulated_eval_time': 1615.4106562137604, 'accumulated_logging_time': 0.561298131942749, 'global_step': 19158, 'preemption_count': 0}), (20901, {'train/ctc_loss': Array(0.3507246, dtype=float32), 'train/wer': 0.12021526367161738, 'validation/ctc_loss': Array(0.61042434, dtype=float32), 'validation/wer': 0.17924828871274512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3801553, dtype=float32), 'test/wer': 0.12497714947291451, 'test/num_examples': 2472, 'score': 17360.215380191803, 'total_duration': 19109.308428525925, 'accumulated_submission_time': 17360.215380191803, 'accumulated_eval_time': 1747.5546412467957, 'accumulated_logging_time': 0.61212158203125, 'global_step': 20901, 'preemption_count': 0}), (22602, {'train/ctc_loss': Array(0.33120733, dtype=float32), 'train/wer': 0.11669924889391912, 'validation/ctc_loss': Array(0.5895108, dtype=float32), 'validation/wer': 0.177172538304836, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36914045, dtype=float32), 'test/wer': 0.12388032417281092, 'test/num_examples': 2472, 'score': 18800.390300273895, 'total_duration': 20684.11161017418, 'accumulated_submission_time': 18800.390300273895, 'accumulated_eval_time': 1882.0567164421082, 'accumulated_logging_time': 0.6611838340759277, 'global_step': 22602, 'preemption_count': 0}), (24316, {'train/ctc_loss': Array(0.30555034, dtype=float32), 'train/wer': 0.10787128921920527, 'validation/ctc_loss': Array(0.57999027, dtype=float32), 'validation/wer': 0.17156318487695144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34827054, dtype=float32), 'test/wer': 0.11687282919992688, 'test/num_examples': 2472, 'score': 20240.683255195618, 'total_duration': 22258.464760541916, 'accumulated_submission_time': 20240.683255195618, 'accumulated_eval_time': 2015.9855983257294, 'accumulated_logging_time': 0.7143306732177734, 'global_step': 24316, 'preemption_count': 0}), (26051, {'train/ctc_loss': Array(0.28669932, dtype=float32), 'train/wer': 0.10374292495891911, 'validation/ctc_loss': Array(0.5682426, dtype=float32), 'validation/wer': 0.16973845544860344, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34270158, dtype=float32), 'test/wer': 0.11447606280340422, 'test/num_examples': 2472, 'score': 21681.123821496964, 'total_duration': 23832.300597190857, 'accumulated_submission_time': 21681.123821496964, 'accumulated_eval_time': 2149.243949651718, 'accumulated_logging_time': 0.7726695537567139, 'global_step': 26051, 'preemption_count': 0}), (27760, {'train/ctc_loss': Array(0.27958962, dtype=float32), 'train/wer': 0.09679250493500118, 'validation/ctc_loss': Array(0.5559207, dtype=float32), 'validation/wer': 0.1613195979802466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33768842, dtype=float32), 'test/wer': 0.11187618061056609, 'test/num_examples': 2472, 'score': 23121.179690361023, 'total_duration': 25405.819195747375, 'accumulated_submission_time': 23121.179690361023, 'accumulated_eval_time': 2282.578394174576, 'accumulated_logging_time': 0.8237957954406738, 'global_step': 27760, 'preemption_count': 0}), (29475, {'train/ctc_loss': Array(0.28855702, dtype=float32), 'train/wer': 0.10198907956318252, 'validation/ctc_loss': Array(0.52978396, dtype=float32), 'validation/wer': 0.15889628006217596, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32637402, dtype=float32), 'test/wer': 0.1077732415249934, 'test/num_examples': 2472, 'score': 24561.724899053574, 'total_duration': 26978.488636016846, 'accumulated_submission_time': 24561.724899053574, 'accumulated_eval_time': 2414.5677559375763, 'accumulated_logging_time': 0.8794715404510498, 'global_step': 29475, 'preemption_count': 0}), (31227, {'train/ctc_loss': Array(0.26182705, dtype=float32), 'train/wer': 0.09105855159279215, 'validation/ctc_loss': Array(0.5137404, dtype=float32), 'validation/wer': 0.15284281259352944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31195962, dtype=float32), 'test/wer': 0.10318282452826356, 'test/num_examples': 2472, 'score': 26001.605697870255, 'total_duration': 28548.715017795563, 'accumulated_submission_time': 26001.605697870255, 'accumulated_eval_time': 2544.769999027252, 'accumulated_logging_time': 0.9440271854400635, 'global_step': 31227, 'preemption_count': 0}), (32920, {'train/ctc_loss': Array(0.2649976, dtype=float32), 'train/wer': 0.09215129270710797, 'validation/ctc_loss': Array(0.5026061, dtype=float32), 'validation/wer': 0.148855440879732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2970402, dtype=float32), 'test/wer': 0.09737371275364085, 'test/num_examples': 2472, 'score': 27441.675806999207, 'total_duration': 30122.35778260231, 'accumulated_submission_time': 27441.675806999207, 'accumulated_eval_time': 2678.2153701782227, 'accumulated_logging_time': 0.995917558670044, 'global_step': 32920, 'preemption_count': 0}), (34647, {'train/ctc_loss': Array(0.25350338, dtype=float32), 'train/wer': 0.08540341230005222, 'validation/ctc_loss': Array(0.483826, dtype=float32), 'validation/wer': 0.14208752908464234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28813714, dtype=float32), 'test/wer': 0.0954034895293807, 'test/num_examples': 2472, 'score': 28882.14173388481, 'total_duration': 31695.236352682114, 'accumulated_submission_time': 28882.14173388481, 'accumulated_eval_time': 2810.4885330200195, 'accumulated_logging_time': 1.0557634830474854, 'global_step': 34647, 'preemption_count': 0}), (36411, {'train/ctc_loss': Array(0.18878697, dtype=float32), 'train/wer': 0.0681867101257649, 'validation/ctc_loss': Array(0.46347603, dtype=float32), 'validation/wer': 0.13604371626905587, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26919183, dtype=float32), 'test/wer': 0.08963500091402109, 'test/num_examples': 2472, 'score': 30322.592145204544, 'total_duration': 33268.88709831238, 'accumulated_submission_time': 30322.592145204544, 'accumulated_eval_time': 2943.5563192367554, 'accumulated_logging_time': 1.1093976497650146, 'global_step': 36411, 'preemption_count': 0}), (38122, {'train/ctc_loss': Array(0.20645283, dtype=float32), 'train/wer': 0.07201537454330426, 'validation/ctc_loss': Array(0.44553778, dtype=float32), 'validation/wer': 0.13231702018787955, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2558761, dtype=float32), 'test/wer': 0.08380557755976682, 'test/num_examples': 2472, 'score': 31762.658802509308, 'total_duration': 34841.29111742973, 'accumulated_submission_time': 31762.658802509308, 'accumulated_eval_time': 3075.7626280784607, 'accumulated_logging_time': 1.1638495922088623, 'global_step': 38122, 'preemption_count': 0}), (39863, {'train/ctc_loss': Array(0.2533094, dtype=float32), 'train/wer': 0.08796233517571313, 'validation/ctc_loss': Array(0.41986907, dtype=float32), 'validation/wer': 0.12479604545410661, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24632764, dtype=float32), 'test/wer': 0.08151036906140191, 'test/num_examples': 2472, 'score': 33203.18149089813, 'total_duration': 36412.94720220566, 'accumulated_submission_time': 33203.18149089813, 'accumulated_eval_time': 3206.759986639023, 'accumulated_logging_time': 1.2221400737762451, 'global_step': 39863, 'preemption_count': 0}), (41598, {'train/ctc_loss': Array(0.24586582, dtype=float32), 'train/wer': 0.08654524056690185, 'validation/ctc_loss': Array(0.42336622, dtype=float32), 'validation/wer': 0.12315475443389942, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2361669, dtype=float32), 'test/wer': 0.07850425527593281, 'test/num_examples': 2472, 'score': 34643.4685151577, 'total_duration': 37985.69330191612, 'accumulated_submission_time': 34643.4685151577, 'accumulated_eval_time': 3339.0807819366455, 'accumulated_logging_time': 1.2829980850219727, 'global_step': 41598, 'preemption_count': 0}), (43283, {'train/ctc_loss': Array(0.26693735, dtype=float32), 'train/wer': 0.09477569595966095, 'validation/ctc_loss': Array(0.39970118, dtype=float32), 'validation/wer': 0.11750678239377468, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22665474, dtype=float32), 'test/wer': 0.07427944671257083, 'test/num_examples': 2472, 'score': 36083.76709651947, 'total_duration': 39556.73584342003, 'accumulated_submission_time': 36083.76709651947, 'accumulated_eval_time': 3469.694420814514, 'accumulated_logging_time': 1.337653636932373, 'global_step': 43283, 'preemption_count': 0}), (45007, {'train/ctc_loss': Array(0.23055223, dtype=float32), 'train/wer': 0.0809719221190587, 'validation/ctc_loss': Array(0.38633734, dtype=float32), 'validation/wer': 0.11462969578188208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21498065, dtype=float32), 'test/wer': 0.0721467308512583, 'test/num_examples': 2472, 'score': 37524.34188055992, 'total_duration': 41127.338093042374, 'accumulated_submission_time': 37524.34188055992, 'accumulated_eval_time': 3599.588337659836, 'accumulated_logging_time': 1.3909001350402832, 'global_step': 45007, 'preemption_count': 0}), (46747, {'train/ctc_loss': Array(0.20073223, dtype=float32), 'train/wer': 0.07153196085199247, 'validation/ctc_loss': Array(0.37127587, dtype=float32), 'validation/wer': 0.10727285015012986, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20473592, dtype=float32), 'test/wer': 0.06759693701379156, 'test/num_examples': 2472, 'score': 38964.92105579376, 'total_duration': 42700.15410685539, 'accumulated_submission_time': 38964.92105579376, 'accumulated_eval_time': 3731.6906871795654, 'accumulated_logging_time': 1.4469618797302246, 'global_step': 46747, 'preemption_count': 0}), (48432, {'train/ctc_loss': Array(0.15851687, dtype=float32), 'train/wer': 0.057189311466138835, 'validation/ctc_loss': Array(0.35182622, dtype=float32), 'validation/wer': 0.10227173986502795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1961421, dtype=float32), 'test/wer': 0.06404241057827068, 'test/num_examples': 2472, 'score': 40405.19302892685, 'total_duration': 44272.9643907547, 'accumulated_submission_time': 40405.19302892685, 'accumulated_eval_time': 3864.09903049469, 'accumulated_logging_time': 1.5005207061767578, 'global_step': 48432, 'preemption_count': 0}), (50155, {'train/ctc_loss': Array(0.16453648, dtype=float32), 'train/wer': 0.05955671900373112, 'validation/ctc_loss': Array(0.3386692, dtype=float32), 'validation/wer': 0.0985836623960918, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18856141, dtype=float32), 'test/wer': 0.061320658907643245, 'test/num_examples': 2472, 'score': 41846.24918437004, 'total_duration': 45845.43116354942, 'accumulated_submission_time': 41846.24918437004, 'accumulated_eval_time': 3995.3676929473877, 'accumulated_logging_time': 1.5626299381256104, 'global_step': 50155, 'preemption_count': 0}), (51880, {'train/ctc_loss': Array(0.14188813, dtype=float32), 'train/wer': 0.05083426387218691, 'validation/ctc_loss': Array(0.32378784, dtype=float32), 'validation/wer': 0.0940170114986918, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17722452, dtype=float32), 'test/wer': 0.05794893668880629, 'test/num_examples': 2472, 'score': 43286.69103932381, 'total_duration': 47417.0975458622, 'accumulated_submission_time': 43286.69103932381, 'accumulated_eval_time': 4126.463999271393, 'accumulated_logging_time': 1.6127378940582275, 'global_step': 51880, 'preemption_count': 0}), (53569, {'train/ctc_loss': Array(0.13709657, dtype=float32), 'train/wer': 0.05001681510973215, 'validation/ctc_loss': Array(0.32031515, dtype=float32), 'validation/wer': 0.09185436921324232, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17172539, dtype=float32), 'test/wer': 0.05636463347754555, 'test/num_examples': 2472, 'score': 44727.2161860466, 'total_duration': 48988.84048175812, 'accumulated_submission_time': 44727.2161860466, 'accumulated_eval_time': 4257.54591012001, 'accumulated_logging_time': 1.6709940433502197, 'global_step': 53569, 'preemption_count': 0}), (55299, {'train/ctc_loss': Array(0.12422652, dtype=float32), 'train/wer': 0.04451621808143547, 'validation/ctc_loss': Array(0.30715603, dtype=float32), 'validation/wer': 0.08865867905036832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16657917, dtype=float32), 'test/wer': 0.05402880181991754, 'test/num_examples': 2472, 'score': 46167.32132577896, 'total_duration': 50560.595286369324, 'accumulated_submission_time': 46167.32132577896, 'accumulated_eval_time': 4389.0642812252045, 'accumulated_logging_time': 1.72357177734375, 'global_step': 55299, 'preemption_count': 0}), (57027, {'train/ctc_loss': Array(0.12671377, dtype=float32), 'train/wer': 0.045980563854263246, 'validation/ctc_loss': Array(0.3025658, dtype=float32), 'validation/wer': 0.08685325892814041, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16329002, dtype=float32), 'test/wer': 0.05339914285133955, 'test/num_examples': 2472, 'score': 47607.58441400528, 'total_duration': 52131.33870744705, 'accumulated_submission_time': 47607.58441400528, 'accumulated_eval_time': 4519.41101193428, 'accumulated_logging_time': 1.7783310413360596, 'global_step': 57027, 'preemption_count': 0}), (58730, {'train/ctc_loss': Array(0.13518772, dtype=float32), 'train/wer': 0.04794759254620504, 'validation/ctc_loss': Array(0.29995033, dtype=float32), 'validation/wer': 0.08582986570377593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1612366, dtype=float32), 'test/wer': 0.052058578595657386, 'test/num_examples': 2472, 'score': 49047.97480177879, 'total_duration': 53703.002312898636, 'accumulated_submission_time': 49047.97480177879, 'accumulated_eval_time': 4650.556568622589, 'accumulated_logging_time': 1.8286492824554443, 'global_step': 58730, 'preemption_count': 0}), (60000, {'train/ctc_loss': Array(0.11520543, dtype=float32), 'train/wer': 0.04230277759096106, 'validation/ctc_loss': Array(0.29943058, dtype=float32), 'validation/wer': 0.08577193778541568, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16139245, dtype=float32), 'test/wer': 0.05191639753823655, 'test/num_examples': 2472, 'score': 50086.90069746971, 'total_duration': 54872.56066775322, 'accumulated_submission_time': 50086.90069746971, 'accumulated_eval_time': 4781.077726840973, 'accumulated_logging_time': 1.881331205368042, 'global_step': 60000, 'preemption_count': 0})], 'global_step': 60000}
I1021 13:04:37.286710 139765020145472 submission_runner.py:568] Timing: 50086.90069746971
I1021 13:04:37.286785 139765020145472 submission_runner.py:570] Total number of evals: 36
I1021 13:04:37.286838 139765020145472 submission_runner.py:571] ====================
I1021 13:04:37.291716 139765020145472 submission_runner.py:647] Final librispeech_conformer score: 50086.90069746971
