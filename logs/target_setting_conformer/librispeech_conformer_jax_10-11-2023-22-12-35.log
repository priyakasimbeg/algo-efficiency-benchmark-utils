python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_conformer/adamw_run_0 --overwrite=true --save_checkpoints=false --max_global_steps=60000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_10-11-2023-22-12-35.log
2023-10-11 22:12:40.820716: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1011 22:12:58.692338 139633106130752 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax.
I1011 22:12:59.641944 139633106130752 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I1011 22:12:59.642671 139633106130752 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1011 22:12:59.642812 139633106130752 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1011 22:12:59.650854 139633106130752 submission_runner.py:523] Using RNG seed 496736683
I1011 22:13:05.220952 139633106130752 submission_runner.py:532] --- Tuning run 1/1 ---
I1011 22:13:05.221195 139633106130752 submission_runner.py:537] Creating tuning directory at /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1.
I1011 22:13:05.221422 139633106130752 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/hparams.json.
I1011 22:13:05.408336 139633106130752 submission_runner.py:200] Initializing dataset.
I1011 22:13:05.408590 139633106130752 submission_runner.py:207] Initializing model.
I1011 22:13:10.760445 139633106130752 submission_runner.py:241] Initializing optimizer.
I1011 22:13:12.067789 139633106130752 submission_runner.py:248] Initializing metrics bundle.
I1011 22:13:12.068042 139633106130752 submission_runner.py:266] Initializing checkpoint and logger.
I1011 22:13:12.069494 139633106130752 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I1011 22:13:12.069670 139633106130752 submission_runner.py:286] Saving meta data to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/meta_data_0.json.
I1011 22:13:12.069897 139633106130752 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1011 22:13:12.069962 139633106130752 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1011 22:13:12.456779 139633106130752 logger_utils.py:220] Unable to record git information. Continuing without it.
I1011 22:13:12.822644 139633106130752 submission_runner.py:289] Saving flags to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/flags_0.json.
I1011 22:13:12.838264 139633106130752 submission_runner.py:299] Starting training loop.
I1011 22:13:13.152420 139633106130752 input_pipeline.py:20] Loading split = train-clean-100
I1011 22:13:13.192783 139633106130752 input_pipeline.py:20] Loading split = train-clean-360
I1011 22:13:13.687237 139633106130752 input_pipeline.py:20] Loading split = train-other-500
2023-10-11 22:14:24.000002: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-11 22:14:26.779241: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1011 22:14:28.725224 139458141738752 logging_writer.py:48] [0] global_step=0, grad_norm=33.15991973876953, loss=31.57537269592285
I1011 22:14:28.761773 139633106130752 spec.py:321] Evaluating on the training split.
I1011 22:14:28.932114 139633106130752 input_pipeline.py:20] Loading split = train-clean-100
I1011 22:14:28.967415 139633106130752 input_pipeline.py:20] Loading split = train-clean-360
I1011 22:14:29.367888 139633106130752 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1011 22:15:45.792784 139633106130752 spec.py:333] Evaluating on the validation split.
I1011 22:15:45.910438 139633106130752 input_pipeline.py:20] Loading split = dev-clean
I1011 22:15:45.915786 139633106130752 input_pipeline.py:20] Loading split = dev-other
I1011 22:16:51.177978 139633106130752 spec.py:349] Evaluating on the test split.
I1011 22:16:51.302543 139633106130752 input_pipeline.py:20] Loading split = test-clean
I1011 22:17:30.522942 139633106130752 submission_runner.py:393] Time since start: 257.68s, 	Step: 1, 	{'train/ctc_loss': Array(31.283686, dtype=float32), 'train/wer': 1.2353195343897716, 'validation/ctc_loss': Array(30.45324, dtype=float32), 'validation/wer': 1.2676462921304923, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.471466, dtype=float32), 'test/wer': 1.2872260475697195, 'test/num_examples': 2472, 'score': 75.92344069480896, 'total_duration': 257.6823902130127, 'accumulated_submission_time': 75.92344069480896, 'accumulated_eval_time': 181.7588791847229, 'accumulated_logging_time': 0}
I1011 22:17:30.552208 139451984508672 logging_writer.py:48] [1] accumulated_eval_time=181.758879, accumulated_logging_time=0, accumulated_submission_time=75.923441, global_step=1, preemption_count=0, score=75.923441, test/ctc_loss=30.471466064453125, test/num_examples=2472, test/wer=1.287226, total_duration=257.682390, train/ctc_loss=31.2836856842041, train/wer=1.235320, validation/ctc_loss=30.45323944091797, validation/num_examples=5348, validation/wer=1.267646
I1011 22:17:53.878879 139459846420224 logging_writer.py:48] [1] global_step=1, grad_norm=35.313114166259766, loss=31.788532257080078
I1011 22:17:54.710850 139459854812928 logging_writer.py:48] [2] global_step=2, grad_norm=36.2273063659668, loss=31.939132690429688
I1011 22:17:55.594377 139459846420224 logging_writer.py:48] [3] global_step=3, grad_norm=37.442222595214844, loss=31.441070556640625
I1011 22:17:56.485089 139459854812928 logging_writer.py:48] [4] global_step=4, grad_norm=43.774314880371094, loss=31.228086471557617
I1011 22:17:57.374465 139459846420224 logging_writer.py:48] [5] global_step=5, grad_norm=47.7711296081543, loss=30.096006393432617
I1011 22:17:58.253751 139459854812928 logging_writer.py:48] [6] global_step=6, grad_norm=67.98027801513672, loss=29.644044876098633
I1011 22:17:59.144013 139459846420224 logging_writer.py:48] [7] global_step=7, grad_norm=84.08881378173828, loss=27.840436935424805
I1011 22:18:00.034407 139459854812928 logging_writer.py:48] [8] global_step=8, grad_norm=100.37989044189453, loss=26.057580947875977
I1011 22:18:00.921880 139459846420224 logging_writer.py:48] [9] global_step=9, grad_norm=114.38978576660156, loss=24.250579833984375
I1011 22:18:01.830118 139459854812928 logging_writer.py:48] [10] global_step=10, grad_norm=112.27386474609375, loss=21.104387283325195
I1011 22:18:02.725727 139459846420224 logging_writer.py:48] [11] global_step=11, grad_norm=136.54779052734375, loss=17.880779266357422
I1011 22:18:03.611087 139459854812928 logging_writer.py:48] [12] global_step=12, grad_norm=144.8287353515625, loss=13.941227912902832
I1011 22:18:04.488579 139459846420224 logging_writer.py:48] [13] global_step=13, grad_norm=100.38871765136719, loss=10.043692588806152
I1011 22:18:05.376187 139459854812928 logging_writer.py:48] [14] global_step=14, grad_norm=42.32083511352539, loss=7.763301849365234
I1011 22:18:06.272109 139459846420224 logging_writer.py:48] [15] global_step=15, grad_norm=4.171846866607666, loss=7.124650001525879
I1011 22:18:07.151894 139459854812928 logging_writer.py:48] [16] global_step=16, grad_norm=14.75786018371582, loss=7.3032426834106445
I1011 22:18:08.031639 139459846420224 logging_writer.py:48] [17] global_step=17, grad_norm=20.40331268310547, loss=7.583488941192627
I1011 22:18:08.924565 139459854812928 logging_writer.py:48] [18] global_step=18, grad_norm=22.64214324951172, loss=7.818045139312744
I1011 22:18:09.812091 139459846420224 logging_writer.py:48] [19] global_step=19, grad_norm=22.961423873901367, loss=7.852637767791748
I1011 22:18:10.691698 139459854812928 logging_writer.py:48] [20] global_step=20, grad_norm=22.494070053100586, loss=7.75370979309082
I1011 22:18:11.568904 139459846420224 logging_writer.py:48] [21] global_step=21, grad_norm=20.734140396118164, loss=7.561861038208008
I1011 22:18:12.455642 139459854812928 logging_writer.py:48] [22] global_step=22, grad_norm=16.41419792175293, loss=7.288782119750977
I1011 22:18:13.338673 139459846420224 logging_writer.py:48] [23] global_step=23, grad_norm=9.150325775146484, loss=7.045709133148193
I1011 22:18:14.221773 139459854812928 logging_writer.py:48] [24] global_step=24, grad_norm=5.6454291343688965, loss=6.991340160369873
I1011 22:18:15.099805 139459846420224 logging_writer.py:48] [25] global_step=25, grad_norm=19.9594783782959, loss=7.153368949890137
I1011 22:18:15.982367 139459854812928 logging_writer.py:48] [26] global_step=26, grad_norm=26.3619441986084, loss=7.277545928955078
I1011 22:18:16.871069 139459846420224 logging_writer.py:48] [27] global_step=27, grad_norm=17.575206756591797, loss=7.054885387420654
I1011 22:18:17.761254 139459854812928 logging_writer.py:48] [28] global_step=28, grad_norm=6.846212387084961, loss=6.887123107910156
I1011 22:18:18.644205 139459846420224 logging_writer.py:48] [29] global_step=29, grad_norm=4.365700721740723, loss=6.8526740074157715
I1011 22:18:19.529535 139459854812928 logging_writer.py:48] [30] global_step=30, grad_norm=9.031214714050293, loss=6.872568607330322
I1011 22:18:20.425758 139459846420224 logging_writer.py:48] [31] global_step=31, grad_norm=11.054577827453613, loss=6.880847930908203
I1011 22:18:21.318297 139459854812928 logging_writer.py:48] [32] global_step=32, grad_norm=10.216168403625488, loss=6.799765110015869
I1011 22:18:22.211876 139459846420224 logging_writer.py:48] [33] global_step=33, grad_norm=6.260593414306641, loss=6.729271411895752
I1011 22:18:23.107335 139459854812928 logging_writer.py:48] [34] global_step=34, grad_norm=1.9127558469772339, loss=6.645495414733887
I1011 22:18:24.002806 139459846420224 logging_writer.py:48] [35] global_step=35, grad_norm=6.931822299957275, loss=6.639617443084717
I1011 22:18:24.886233 139459854812928 logging_writer.py:48] [36] global_step=36, grad_norm=11.230517387390137, loss=6.653032302856445
I1011 22:18:25.771723 139459846420224 logging_writer.py:48] [37] global_step=37, grad_norm=7.728986740112305, loss=6.563356876373291
I1011 22:18:26.659355 139459854812928 logging_writer.py:48] [38] global_step=38, grad_norm=1.8538070917129517, loss=6.512983798980713
I1011 22:18:27.545997 139459846420224 logging_writer.py:48] [39] global_step=39, grad_norm=4.130667209625244, loss=6.494144916534424
I1011 22:18:28.436462 139459854812928 logging_writer.py:48] [40] global_step=40, grad_norm=5.879776477813721, loss=6.459390640258789
I1011 22:18:29.330260 139459846420224 logging_writer.py:48] [41] global_step=41, grad_norm=3.804466962814331, loss=6.421169757843018
I1011 22:18:30.224178 139459854812928 logging_writer.py:48] [42] global_step=42, grad_norm=1.3381550312042236, loss=6.374166011810303
I1011 22:18:31.109384 139459846420224 logging_writer.py:48] [43] global_step=43, grad_norm=5.1381096839904785, loss=6.354024410247803
I1011 22:18:31.994769 139459854812928 logging_writer.py:48] [44] global_step=44, grad_norm=5.483169078826904, loss=6.310190200805664
I1011 22:18:32.880890 139459846420224 logging_writer.py:48] [45] global_step=45, grad_norm=1.6100382804870605, loss=6.278481960296631
I1011 22:18:33.776397 139459854812928 logging_writer.py:48] [46] global_step=46, grad_norm=3.2646872997283936, loss=6.239269733428955
I1011 22:18:34.664008 139459846420224 logging_writer.py:48] [47] global_step=47, grad_norm=4.012678146362305, loss=6.238736629486084
I1011 22:18:35.556545 139459854812928 logging_writer.py:48] [48] global_step=48, grad_norm=1.2317421436309814, loss=6.201253414154053
I1011 22:18:36.450406 139459846420224 logging_writer.py:48] [49] global_step=49, grad_norm=4.46732234954834, loss=6.188632965087891
I1011 22:18:37.349959 139459854812928 logging_writer.py:48] [50] global_step=50, grad_norm=2.287473201751709, loss=6.151869297027588
I1011 22:18:38.240371 139459846420224 logging_writer.py:48] [51] global_step=51, grad_norm=2.893773078918457, loss=6.13851261138916
I1011 22:18:39.135580 139459854812928 logging_writer.py:48] [52] global_step=52, grad_norm=2.299752950668335, loss=6.144152641296387
I1011 22:18:40.037945 139459846420224 logging_writer.py:48] [53] global_step=53, grad_norm=1.9958363771438599, loss=6.086791515350342
I1011 22:18:40.937668 139459854812928 logging_writer.py:48] [54] global_step=54, grad_norm=3.2394542694091797, loss=6.0930047035217285
I1011 22:18:41.828942 139459846420224 logging_writer.py:48] [55] global_step=55, grad_norm=2.3408524990081787, loss=6.06387186050415
I1011 22:18:42.720593 139459854812928 logging_writer.py:48] [56] global_step=56, grad_norm=1.2570784091949463, loss=6.043445587158203
I1011 22:18:43.613149 139459846420224 logging_writer.py:48] [57] global_step=57, grad_norm=1.795289397239685, loss=6.008497714996338
I1011 22:18:44.506541 139459854812928 logging_writer.py:48] [58] global_step=58, grad_norm=1.3631571531295776, loss=6.011539936065674
I1011 22:18:45.398510 139459846420224 logging_writer.py:48] [59] global_step=59, grad_norm=2.469888210296631, loss=5.994093894958496
I1011 22:18:46.284608 139459854812928 logging_writer.py:48] [60] global_step=60, grad_norm=1.929497480392456, loss=5.973856449127197
I1011 22:18:47.182350 139459846420224 logging_writer.py:48] [61] global_step=61, grad_norm=0.6233648061752319, loss=5.969196796417236
I1011 22:18:48.081662 139459854812928 logging_writer.py:48] [62] global_step=62, grad_norm=0.5784416794776917, loss=5.967426776885986
I1011 22:18:48.965687 139459846420224 logging_writer.py:48] [63] global_step=63, grad_norm=0.7403700351715088, loss=5.941073417663574
I1011 22:18:49.846854 139459854812928 logging_writer.py:48] [64] global_step=64, grad_norm=0.9328124523162842, loss=5.9079976081848145
I1011 22:18:50.738388 139459846420224 logging_writer.py:48] [65] global_step=65, grad_norm=0.688014030456543, loss=5.8986310958862305
I1011 22:18:51.628041 139459854812928 logging_writer.py:48] [66] global_step=66, grad_norm=1.0949828624725342, loss=5.895335674285889
I1011 22:18:52.517538 139459846420224 logging_writer.py:48] [67] global_step=67, grad_norm=2.759146213531494, loss=5.872076511383057
I1011 22:18:53.409106 139459854812928 logging_writer.py:48] [68] global_step=68, grad_norm=9.57370662689209, loss=5.955726623535156
I1011 22:18:54.298543 139459846420224 logging_writer.py:48] [69] global_step=69, grad_norm=12.720013618469238, loss=6.050429344177246
I1011 22:18:55.197239 139459854812928 logging_writer.py:48] [70] global_step=70, grad_norm=2.421799898147583, loss=5.881631374359131
I1011 22:18:56.083993 139459846420224 logging_writer.py:48] [71] global_step=71, grad_norm=19.792905807495117, loss=6.082019329071045
I1011 22:18:56.975751 139459854812928 logging_writer.py:48] [72] global_step=72, grad_norm=6.024285316467285, loss=5.877596378326416
I1011 22:18:57.869891 139459846420224 logging_writer.py:48] [73] global_step=73, grad_norm=11.639492988586426, loss=6.002535343170166
I1011 22:18:58.770831 139459854812928 logging_writer.py:48] [74] global_step=74, grad_norm=3.987240791320801, loss=5.843491554260254
I1011 22:18:59.663953 139459846420224 logging_writer.py:48] [75] global_step=75, grad_norm=11.45029354095459, loss=5.93370246887207
I1011 22:19:00.557863 139459854812928 logging_writer.py:48] [76] global_step=76, grad_norm=6.8760247230529785, loss=5.897753715515137
I1011 22:19:01.453539 139459846420224 logging_writer.py:48] [77] global_step=77, grad_norm=5.731967449188232, loss=5.879810810089111
I1011 22:19:02.364601 139459854812928 logging_writer.py:48] [78] global_step=78, grad_norm=7.651723384857178, loss=5.919749736785889
I1011 22:19:03.287214 139459846420224 logging_writer.py:48] [79] global_step=79, grad_norm=1.0320857763290405, loss=5.823648929595947
I1011 22:19:04.172995 139459854812928 logging_writer.py:48] [80] global_step=80, grad_norm=8.051517486572266, loss=5.879026412963867
I1011 22:19:05.061657 139459846420224 logging_writer.py:48] [81] global_step=81, grad_norm=3.295598030090332, loss=5.865869522094727
I1011 22:19:05.951717 139459854812928 logging_writer.py:48] [82] global_step=82, grad_norm=4.6666412353515625, loss=5.862390041351318
I1011 22:19:06.838999 139459846420224 logging_writer.py:48] [83] global_step=83, grad_norm=4.723830699920654, loss=5.857757091522217
I1011 22:19:07.722902 139459854812928 logging_writer.py:48] [84] global_step=84, grad_norm=1.4242326021194458, loss=5.816578388214111
I1011 22:19:08.623690 139459846420224 logging_writer.py:48] [85] global_step=85, grad_norm=5.226039409637451, loss=5.857592582702637
I1011 22:19:09.520132 139459854812928 logging_writer.py:48] [86] global_step=86, grad_norm=0.27144211530685425, loss=5.824209690093994
I1011 22:19:10.413558 139459846420224 logging_writer.py:48] [87] global_step=87, grad_norm=3.7720906734466553, loss=5.828324317932129
I1011 22:19:11.304313 139459854812928 logging_writer.py:48] [88] global_step=88, grad_norm=1.972072958946228, loss=5.827181339263916
I1011 22:19:12.204391 139459846420224 logging_writer.py:48] [89] global_step=89, grad_norm=4.729888916015625, loss=5.8510026931762695
I1011 22:19:13.093748 139459854812928 logging_writer.py:48] [90] global_step=90, grad_norm=0.3010800778865814, loss=5.814955711364746
I1011 22:19:13.985982 139459846420224 logging_writer.py:48] [91] global_step=91, grad_norm=2.8526828289031982, loss=5.835781097412109
I1011 22:19:14.872881 139459854812928 logging_writer.py:48] [92] global_step=92, grad_norm=0.5369176268577576, loss=5.816189289093018
I1011 22:19:15.775450 139459846420224 logging_writer.py:48] [93] global_step=93, grad_norm=2.260390520095825, loss=5.820140361785889
I1011 22:19:16.669418 139459854812928 logging_writer.py:48] [94] global_step=94, grad_norm=0.3288212716579437, loss=5.813551425933838
I1011 22:19:17.558427 139459846420224 logging_writer.py:48] [95] global_step=95, grad_norm=1.8035647869110107, loss=5.809733867645264
I1011 22:19:18.453588 139459854812928 logging_writer.py:48] [96] global_step=96, grad_norm=0.6086132526397705, loss=5.8158063888549805
I1011 22:19:19.347167 139459846420224 logging_writer.py:48] [97] global_step=97, grad_norm=1.7153563499450684, loss=5.804832458496094
I1011 22:19:20.238400 139459854812928 logging_writer.py:48] [98] global_step=98, grad_norm=2.0194084644317627, loss=5.837055683135986
I1011 22:19:21.128174 139459846420224 logging_writer.py:48] [99] global_step=99, grad_norm=0.9123585224151611, loss=5.811395168304443
I1011 22:19:22.019102 139459854812928 logging_writer.py:48] [100] global_step=100, grad_norm=0.38217291235923767, loss=5.82072639465332
I1011 22:24:27.281896 139459846420224 logging_writer.py:48] [500] global_step=500, grad_norm=0.9036929607391357, loss=4.792221546173096
I1011 22:31:10.182544 139459854812928 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9330283403396606, loss=2.7989916801452637
I1011 22:37:36.088705 139460560529152 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.1914006471633911, loss=2.4117817878723145
I1011 22:41:30.866258 139633106130752 spec.py:321] Evaluating on the training split.
I1011 22:42:23.303223 139633106130752 spec.py:333] Evaluating on the validation split.
I1011 22:43:13.111690 139633106130752 spec.py:349] Evaluating on the test split.
I1011 22:43:38.075743 139633106130752 submission_runner.py:393] Time since start: 1825.23s, 	Step: 1795, 	{'train/ctc_loss': Array(2.1724489, dtype=float32), 'train/wer': 0.5042532594591255, 'validation/ctc_loss': Array(2.7234242, dtype=float32), 'validation/wer': 0.541548799443892, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.363636, dtype=float32), 'test/wer': 0.5019803790140759, 'test/num_examples': 2472, 'score': 1516.1533861160278, 'total_duration': 1825.2312757968903, 'accumulated_submission_time': 1516.1533861160278, 'accumulated_eval_time': 308.9622106552124, 'accumulated_logging_time': 0.04378247261047363}
I1011 22:43:38.111402 139460560529152 logging_writer.py:48] [1795] accumulated_eval_time=308.962211, accumulated_logging_time=0.043782, accumulated_submission_time=1516.153386, global_step=1795, preemption_count=0, score=1516.153386, test/ctc_loss=2.363636016845703, test/num_examples=2472, test/wer=0.501980, total_duration=1825.231276, train/ctc_loss=2.1724488735198975, train/wer=0.504253, validation/ctc_loss=2.723424196243286, validation/num_examples=5348, validation/wer=0.541549
I1011 22:46:14.507841 139460552136448 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.956173837184906, loss=2.1354782581329346
I1011 22:52:39.827837 139460560529152 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5804657936096191, loss=1.998108983039856
I1011 22:59:36.460802 139460552136448 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.71747887134552, loss=1.9848899841308594
I1011 23:06:08.459540 139459577489152 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5563068985939026, loss=1.9614570140838623
I1011 23:07:38.185457 139633106130752 spec.py:321] Evaluating on the training split.
I1011 23:08:30.731395 139633106130752 spec.py:333] Evaluating on the validation split.
I1011 23:09:22.409245 139633106130752 spec.py:349] Evaluating on the test split.
I1011 23:09:48.276062 139633106130752 submission_runner.py:393] Time since start: 3395.43s, 	Step: 3607, 	{'train/ctc_loss': Array(0.6835408, dtype=float32), 'train/wer': 0.2335988067016488, 'validation/ctc_loss': Array(1.0022143, dtype=float32), 'validation/wer': 0.29368489143342635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.71744424, dtype=float32), 'test/wer': 0.23616273637600796, 'test/num_examples': 2472, 'score': 2956.1371109485626, 'total_duration': 3395.4316880702972, 'accumulated_submission_time': 2956.1371109485626, 'accumulated_eval_time': 439.0467426776886, 'accumulated_logging_time': 0.09451818466186523}
I1011 23:09:48.315015 139460560529152 logging_writer.py:48] [3607] accumulated_eval_time=439.046743, accumulated_logging_time=0.094518, accumulated_submission_time=2956.137111, global_step=3607, preemption_count=0, score=2956.137111, test/ctc_loss=0.7174442410469055, test/num_examples=2472, test/wer=0.236163, total_duration=3395.431688, train/ctc_loss=0.6835408210754395, train/wer=0.233599, validation/ctc_loss=1.0022143125534058, validation/num_examples=5348, validation/wer=0.293685
I1011 23:14:57.040863 139460552136448 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.683569610118866, loss=1.8831818103790283
I1011 23:21:33.184100 139460560529152 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8336583375930786, loss=1.809140920639038
I1011 23:28:23.733329 139460552136448 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5851700305938721, loss=1.843873143196106
I1011 23:33:48.659171 139633106130752 spec.py:321] Evaluating on the training split.
I1011 23:34:41.897436 139633106130752 spec.py:333] Evaluating on the validation split.
I1011 23:35:32.309853 139633106130752 spec.py:349] Evaluating on the test split.
I1011 23:35:57.811077 139633106130752 submission_runner.py:393] Time since start: 4964.97s, 	Step: 5406, 	{'train/ctc_loss': Array(0.50123256, dtype=float32), 'train/wer': 0.1806273753235184, 'validation/ctc_loss': Array(0.8452047, dtype=float32), 'validation/wer': 0.25117545401006014, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5711044, dtype=float32), 'test/wer': 0.1888976905733959, 'test/num_examples': 2472, 'score': 4396.388687610626, 'total_duration': 4964.966960906982, 'accumulated_submission_time': 4396.388687610626, 'accumulated_eval_time': 568.193030834198, 'accumulated_logging_time': 0.1506650447845459}
I1011 23:35:57.852302 139460560529152 logging_writer.py:48] [5406] accumulated_eval_time=568.193031, accumulated_logging_time=0.150665, accumulated_submission_time=4396.388688, global_step=5406, preemption_count=0, score=4396.388688, test/ctc_loss=0.5711044073104858, test/num_examples=2472, test/wer=0.188898, total_duration=4964.966961, train/ctc_loss=0.5012325644493103, train/wer=0.180627, validation/ctc_loss=0.8452047109603882, validation/num_examples=5348, validation/wer=0.251175
I1011 23:37:10.010440 139460552136448 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7150567173957825, loss=1.809019684791565
I1011 23:43:50.536716 139460560529152 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7061159014701843, loss=1.7635350227355957
I1011 23:50:31.690722 139460560529152 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7672057747840881, loss=1.7687015533447266
I1011 23:57:26.007738 139460552136448 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6799727082252502, loss=1.6657856702804565
I1011 23:59:58.145964 139633106130752 spec.py:321] Evaluating on the training split.
I1012 00:00:51.624702 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 00:01:43.092240 139633106130752 spec.py:349] Evaluating on the test split.
I1012 00:02:09.671371 139633106130752 submission_runner.py:393] Time since start: 6536.83s, 	Step: 7182, 	{'train/ctc_loss': Array(0.4753171, dtype=float32), 'train/wer': 0.1652860538910241, 'validation/ctc_loss': Array(0.7753231, dtype=float32), 'validation/wer': 0.23046622319626944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5148791, dtype=float32), 'test/wer': 0.17061726890500276, 'test/num_examples': 2472, 'score': 5836.592508554459, 'total_duration': 6536.8271048069, 'accumulated_submission_time': 5836.592508554459, 'accumulated_eval_time': 699.7124745845795, 'accumulated_logging_time': 0.20771479606628418}
I1012 00:02:09.708377 139460560529152 logging_writer.py:48] [7182] accumulated_eval_time=699.712475, accumulated_logging_time=0.207715, accumulated_submission_time=5836.592509, global_step=7182, preemption_count=0, score=5836.592509, test/ctc_loss=0.5148791074752808, test/num_examples=2472, test/wer=0.170617, total_duration=6536.827105, train/ctc_loss=0.4753170907497406, train/wer=0.165286, validation/ctc_loss=0.7753230929374695, validation/num_examples=5348, validation/wer=0.230466
I1012 00:06:15.841467 139460560529152 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7439720630645752, loss=1.7940620183944702
I1012 00:13:25.323274 139460552136448 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5911368727684021, loss=1.7698509693145752
I1012 00:20:18.066396 139460560529152 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8632079362869263, loss=1.7067302465438843
I1012 00:26:10.264397 139633106130752 spec.py:321] Evaluating on the training split.
I1012 00:27:04.424723 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 00:27:55.464425 139633106130752 spec.py:349] Evaluating on the test split.
I1012 00:28:22.138811 139633106130752 submission_runner.py:393] Time since start: 8109.29s, 	Step: 8917, 	{'train/ctc_loss': Array(0.45146623, dtype=float32), 'train/wer': 0.1579562504077062, 'validation/ctc_loss': Array(0.75178593, dtype=float32), 'validation/wer': 0.22201840176873244, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48599577, dtype=float32), 'test/wer': 0.16182235492454247, 'test/num_examples': 2472, 'score': 7277.061921596527, 'total_duration': 8109.292993068695, 'accumulated_submission_time': 7277.061921596527, 'accumulated_eval_time': 831.5793786048889, 'accumulated_logging_time': 0.26070261001586914}
I1012 00:28:22.185245 139460345489152 logging_writer.py:48] [8917] accumulated_eval_time=831.579379, accumulated_logging_time=0.260703, accumulated_submission_time=7277.061922, global_step=8917, preemption_count=0, score=7277.061922, test/ctc_loss=0.4859957695007324, test/num_examples=2472, test/wer=0.161822, total_duration=8109.292993, train/ctc_loss=0.45146623253822327, train/wer=0.157956, validation/ctc_loss=0.751785933971405, validation/num_examples=5348, validation/wer=0.222018
I1012 00:29:25.941773 139460337096448 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6751567721366882, loss=1.6975080966949463
I1012 00:36:00.097042 139459690129152 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9466574788093567, loss=1.719285011291504
I1012 00:42:58.012496 139459681736448 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7430576086044312, loss=1.6820647716522217
I1012 00:49:53.311372 139459690129152 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6859186291694641, loss=1.6251908540725708
I1012 00:52:22.338411 139633106130752 spec.py:321] Evaluating on the training split.
I1012 00:53:15.572061 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 00:54:06.411606 139633106130752 spec.py:349] Evaluating on the test split.
I1012 00:54:32.737934 139633106130752 submission_runner.py:393] Time since start: 9679.89s, 	Step: 10694, 	{'train/ctc_loss': Array(0.4585386, dtype=float32), 'train/wer': 0.15907090386287617, 'validation/ctc_loss': Array(0.7237231, dtype=float32), 'validation/wer': 0.21542427372872355, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47273266, dtype=float32), 'test/wer': 0.1602583632929133, 'test/num_examples': 2472, 'score': 8717.120530128479, 'total_duration': 9679.894563674927, 'accumulated_submission_time': 8717.120530128479, 'accumulated_eval_time': 961.9738273620605, 'accumulated_logging_time': 0.32740354537963867}
I1012 00:54:32.773461 139459761809152 logging_writer.py:48] [10694] accumulated_eval_time=961.973827, accumulated_logging_time=0.327404, accumulated_submission_time=8717.120530, global_step=10694, preemption_count=0, score=8717.120530, test/ctc_loss=0.47273266315460205, test/num_examples=2472, test/wer=0.160258, total_duration=9679.894564, train/ctc_loss=0.45853859186172485, train/wer=0.159071, validation/ctc_loss=0.7237231135368347, validation/num_examples=5348, validation/wer=0.215424
I1012 00:58:29.880939 139459753416448 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6624324321746826, loss=1.692055344581604
I1012 01:05:31.251399 139459434129152 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6900289058685303, loss=1.636352300643921
I1012 01:12:25.716313 139459425736448 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7527180910110474, loss=1.6673543453216553
I1012 01:18:32.808799 139633106130752 spec.py:321] Evaluating on the training split.
I1012 01:19:26.871582 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 01:20:19.190479 139633106130752 spec.py:349] Evaluating on the test split.
I1012 01:20:45.414020 139633106130752 submission_runner.py:393] Time since start: 11252.57s, 	Step: 12422, 	{'train/ctc_loss': Array(0.42136386, dtype=float32), 'train/wer': 0.146092570140174, 'validation/ctc_loss': Array(0.7110198, dtype=float32), 'validation/wer': 0.21197756258628847, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45268196, dtype=float32), 'test/wer': 0.15396177360713342, 'test/num_examples': 2472, 'score': 10157.064435720444, 'total_duration': 11252.56951546669, 'accumulated_submission_time': 10157.064435720444, 'accumulated_eval_time': 1094.5730183124542, 'accumulated_logging_time': 0.38143348693847656}
I1012 01:20:45.450109 139459761809152 logging_writer.py:48] [12422] accumulated_eval_time=1094.573018, accumulated_logging_time=0.381433, accumulated_submission_time=10157.064436, global_step=12422, preemption_count=0, score=10157.064436, test/ctc_loss=0.4526819586753845, test/num_examples=2472, test/wer=0.153962, total_duration=11252.569515, train/ctc_loss=0.42136386036872864, train/wer=0.146093, validation/ctc_loss=0.7110198140144348, validation/num_examples=5348, validation/wer=0.211978
I1012 01:21:45.858232 139459753416448 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6958006620407104, loss=1.6993893384933472
I1012 01:28:20.876704 139459761809152 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8025112152099609, loss=1.6069790124893188
I1012 01:35:30.139157 139459761809152 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6059476137161255, loss=1.6387208700180054
I1012 01:42:20.527960 139459753416448 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7030541300773621, loss=1.6401392221450806
I1012 01:44:45.465563 139633106130752 spec.py:321] Evaluating on the training split.
I1012 01:45:40.017979 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 01:46:31.329332 139633106130752 spec.py:349] Evaluating on the test split.
I1012 01:46:57.622813 139633106130752 submission_runner.py:393] Time since start: 12824.78s, 	Step: 14164, 	{'train/ctc_loss': Array(0.3761427, dtype=float32), 'train/wer': 0.1358433656621337, 'validation/ctc_loss': Array(0.6898569, dtype=float32), 'validation/wer': 0.20477519140349693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43603724, dtype=float32), 'test/wer': 0.14669022809903925, 'test/num_examples': 2472, 'score': 11596.988751649857, 'total_duration': 12824.777482032776, 'accumulated_submission_time': 11596.988751649857, 'accumulated_eval_time': 1226.7232501506805, 'accumulated_logging_time': 0.43695807456970215}
I1012 01:46:57.658926 139459546769152 logging_writer.py:48] [14164] accumulated_eval_time=1226.723250, accumulated_logging_time=0.436958, accumulated_submission_time=11596.988752, global_step=14164, preemption_count=0, score=11596.988752, test/ctc_loss=0.43603724241256714, test/num_examples=2472, test/wer=0.146690, total_duration=12824.777482, train/ctc_loss=0.3761427104473114, train/wer=0.135843, validation/ctc_loss=0.6898568868637085, validation/num_examples=5348, validation/wer=0.204775
I1012 01:51:17.905374 139459546769152 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7670360803604126, loss=1.5953049659729004
I1012 01:58:05.037910 139459538376448 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8122979998588562, loss=1.6691898107528687
I1012 02:05:23.171955 139459546769152 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6170923709869385, loss=1.5957484245300293
I1012 02:10:57.619137 139633106130752 spec.py:321] Evaluating on the training split.
I1012 02:11:51.887104 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 02:12:42.645437 139633106130752 spec.py:349] Evaluating on the test split.
I1012 02:13:09.909343 139633106130752 submission_runner.py:393] Time since start: 14397.06s, 	Step: 15925, 	{'train/ctc_loss': Array(0.35632157, dtype=float32), 'train/wer': 0.1272231059376812, 'validation/ctc_loss': Array(0.6625003, dtype=float32), 'validation/wer': 0.199088600751132, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42379397, dtype=float32), 'test/wer': 0.14075924684662727, 'test/num_examples': 2472, 'score': 13036.85973405838, 'total_duration': 14397.064236879349, 'accumulated_submission_time': 13036.85973405838, 'accumulated_eval_time': 1359.0066635608673, 'accumulated_logging_time': 0.4878711700439453}
I1012 02:13:09.957643 139459546769152 logging_writer.py:48] [15925] accumulated_eval_time=1359.006664, accumulated_logging_time=0.487871, accumulated_submission_time=13036.859734, global_step=15925, preemption_count=0, score=13036.859734, test/ctc_loss=0.4237939715385437, test/num_examples=2472, test/wer=0.140759, total_duration=14397.064237, train/ctc_loss=0.3563215732574463, train/wer=0.127223, validation/ctc_loss=0.6625003218650818, validation/num_examples=5348, validation/wer=0.199089
I1012 02:14:07.902384 139459538376448 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7653128504753113, loss=1.5994914770126343
I1012 02:21:07.123342 139459546769152 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7127571702003479, loss=1.5704609155654907
I1012 02:27:43.404619 139459538376448 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.9067281484603882, loss=1.6317720413208008
I1012 02:35:04.041235 139459546769152 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6442025303840637, loss=1.5837851762771606
I1012 02:37:10.381684 139633106130752 spec.py:321] Evaluating on the training split.
I1012 02:38:03.790469 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 02:38:54.617231 139633106130752 spec.py:349] Evaluating on the test split.
I1012 02:39:20.380796 139633106130752 submission_runner.py:393] Time since start: 15967.54s, 	Step: 17661, 	{'train/ctc_loss': Array(0.3570879, dtype=float32), 'train/wer': 0.12983530597907406, 'validation/ctc_loss': Array(0.65580726, dtype=float32), 'validation/wer': 0.1957577454454174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41565606, dtype=float32), 'test/wer': 0.13921556679462962, 'test/num_examples': 2472, 'score': 14477.183139562607, 'total_duration': 15967.536049604416, 'accumulated_submission_time': 14477.183139562607, 'accumulated_eval_time': 1488.9995353221893, 'accumulated_logging_time': 0.5632083415985107}
I1012 02:39:20.415702 139460345489152 logging_writer.py:48] [17661] accumulated_eval_time=1488.999535, accumulated_logging_time=0.563208, accumulated_submission_time=14477.183140, global_step=17661, preemption_count=0, score=14477.183140, test/ctc_loss=0.41565605998039246, test/num_examples=2472, test/wer=0.139216, total_duration=15967.536050, train/ctc_loss=0.3570879101753235, train/wer=0.129835, validation/ctc_loss=0.6558072566986084, validation/num_examples=5348, validation/wer=0.195758
I1012 02:43:38.552738 139460337096448 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.8024235367774963, loss=1.6556044816970825
I1012 02:50:48.023540 139460345489152 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.633075475692749, loss=1.6091381311416626
I1012 02:57:20.468829 139460345489152 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8408414125442505, loss=1.6134227514266968
I1012 03:03:20.888156 139633106130752 spec.py:321] Evaluating on the training split.
I1012 03:04:15.983340 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 03:05:07.794928 139633106130752 spec.py:349] Evaluating on the test split.
I1012 03:05:35.089153 139633106130752 submission_runner.py:393] Time since start: 17542.24s, 	Step: 19420, 	{'train/ctc_loss': Array(0.35986096, dtype=float32), 'train/wer': 0.12371030434437716, 'validation/ctc_loss': Array(0.6383446, dtype=float32), 'validation/wer': 0.18857468356874596, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3996908, dtype=float32), 'test/wer': 0.13261430341437655, 'test/num_examples': 2472, 'score': 15917.56741642952, 'total_duration': 17542.244049310684, 'accumulated_submission_time': 15917.56741642952, 'accumulated_eval_time': 1623.1937549114227, 'accumulated_logging_time': 0.6120152473449707}
I1012 03:05:35.132627 139459761809152 logging_writer.py:48] [19420] accumulated_eval_time=1623.193755, accumulated_logging_time=0.612015, accumulated_submission_time=15917.567416, global_step=19420, preemption_count=0, score=15917.567416, test/ctc_loss=0.39969080686569214, test/num_examples=2472, test/wer=0.132614, total_duration=17542.244049, train/ctc_loss=0.35986095666885376, train/wer=0.123710, validation/ctc_loss=0.6383445858955383, validation/num_examples=5348, validation/wer=0.188575
I1012 03:06:36.453788 139459753416448 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6185609102249146, loss=1.5318121910095215
I1012 03:13:07.336780 139459434129152 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7621960043907166, loss=1.479035496711731
I1012 03:20:23.300988 139459425736448 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6718591451644897, loss=1.5606392621994019
I1012 03:27:01.439899 139459761809152 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8342894911766052, loss=1.5724412202835083
I1012 03:29:35.640897 139633106130752 spec.py:321] Evaluating on the training split.
I1012 03:30:30.069082 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 03:31:21.175246 139633106130752 spec.py:349] Evaluating on the test split.
I1012 03:31:47.186906 139633106130752 submission_runner.py:393] Time since start: 19114.34s, 	Step: 21179, 	{'train/ctc_loss': Array(0.35657898, dtype=float32), 'train/wer': 0.12375023082807925, 'validation/ctc_loss': Array(0.61579514, dtype=float32), 'validation/wer': 0.18322600577348253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3865894, dtype=float32), 'test/wer': 0.12877541486401398, 'test/num_examples': 2472, 'score': 17357.973737955093, 'total_duration': 19114.34250688553, 'accumulated_submission_time': 17357.973737955093, 'accumulated_eval_time': 1754.7336728572845, 'accumulated_logging_time': 0.6818695068359375}
I1012 03:31:47.227137 139459761809152 logging_writer.py:48] [21179] accumulated_eval_time=1754.733673, accumulated_logging_time=0.681870, accumulated_submission_time=17357.973738, global_step=21179, preemption_count=0, score=17357.973738, test/ctc_loss=0.3865894079208374, test/num_examples=2472, test/wer=0.128775, total_duration=19114.342507, train/ctc_loss=0.3565789759159088, train/wer=0.123750, validation/ctc_loss=0.6157951354980469, validation/num_examples=5348, validation/wer=0.183226
I1012 03:35:54.973469 139459753416448 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7404180765151978, loss=1.5511821508407593
I1012 03:42:38.534133 139459434129152 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.715714156627655, loss=1.50886869430542
I1012 03:49:52.410796 139459425736448 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.663177490234375, loss=1.6027618646621704
I1012 03:55:47.510963 139633106130752 spec.py:321] Evaluating on the training split.
I1012 03:56:42.685069 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 03:57:34.830787 139633106130752 spec.py:349] Evaluating on the test split.
I1012 03:58:01.403390 139633106130752 submission_runner.py:393] Time since start: 20688.56s, 	Step: 22940, 	{'train/ctc_loss': Array(0.32963604, dtype=float32), 'train/wer': 0.11558802345920362, 'validation/ctc_loss': Array(0.59680986, dtype=float32), 'validation/wer': 0.17849522577406182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36326215, dtype=float32), 'test/wer': 0.12174760831149839, 'test/num_examples': 2472, 'score': 18798.168061494827, 'total_duration': 20688.558798074722, 'accumulated_submission_time': 18798.168061494827, 'accumulated_eval_time': 1888.6200063228607, 'accumulated_logging_time': 0.7375879287719727}
I1012 03:58:01.450748 139459761809152 logging_writer.py:48] [22940] accumulated_eval_time=1888.620006, accumulated_logging_time=0.737588, accumulated_submission_time=18798.168061, global_step=22940, preemption_count=0, score=18798.168061, test/ctc_loss=0.3632621467113495, test/num_examples=2472, test/wer=0.121748, total_duration=20688.558798, train/ctc_loss=0.3296360373497009, train/wer=0.115588, validation/ctc_loss=0.5968098640441895, validation/num_examples=5348, validation/wer=0.178495
I1012 03:58:47.761790 139459753416448 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7283127307891846, loss=1.4877558946609497
I1012 04:05:37.473219 139459761809152 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8421707153320312, loss=1.5128611326217651
I1012 04:12:25.517315 139459761809152 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.638859212398529, loss=1.456937313079834
I1012 04:19:41.289394 139459753416448 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8354650139808655, loss=1.5152487754821777
I1012 04:22:01.759027 139633106130752 spec.py:321] Evaluating on the training split.
I1012 04:22:56.096905 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 04:23:48.168662 139633106130752 spec.py:349] Evaluating on the test split.
I1012 04:24:14.017191 139633106130752 submission_runner.py:393] Time since start: 22261.17s, 	Step: 24661, 	{'train/ctc_loss': Array(0.30119097, dtype=float32), 'train/wer': 0.10811409991132132, 'validation/ctc_loss': Array(0.57746845, dtype=float32), 'validation/wer': 0.17099356034640895, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3587026, dtype=float32), 'test/wer': 0.12105701460402575, 'test/num_examples': 2472, 'score': 20238.383621692657, 'total_duration': 22261.172021389008, 'accumulated_submission_time': 20238.383621692657, 'accumulated_eval_time': 2020.8713102340698, 'accumulated_logging_time': 0.8044335842132568}
I1012 04:24:14.055543 139460130449152 logging_writer.py:48] [24661] accumulated_eval_time=2020.871310, accumulated_logging_time=0.804434, accumulated_submission_time=20238.383622, global_step=24661, preemption_count=0, score=20238.383622, test/ctc_loss=0.3587026000022888, test/num_examples=2472, test/wer=0.121057, total_duration=22261.172021, train/ctc_loss=0.30119097232818604, train/wer=0.108114, validation/ctc_loss=0.5774684548377991, validation/num_examples=5348, validation/wer=0.170994
I1012 04:28:36.944408 139459802769152 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.9289599657058716, loss=1.388802409172058
I1012 04:35:46.572936 139459794376448 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.7337347269058228, loss=1.492764949798584
I1012 04:42:38.642375 139459802769152 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8970934748649597, loss=1.4315112829208374
I1012 04:48:14.022955 139633106130752 spec.py:321] Evaluating on the training split.
I1012 04:49:08.521970 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 04:50:00.814729 139633106130752 spec.py:349] Evaluating on the test split.
I1012 04:50:27.438013 139633106130752 submission_runner.py:393] Time since start: 23834.59s, 	Step: 26396, 	{'train/ctc_loss': Array(0.28304073, dtype=float32), 'train/wer': 0.1015884608362242, 'validation/ctc_loss': Array(0.55854106, dtype=float32), 'validation/wer': 0.1680585458161561, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34688652, dtype=float32), 'test/wer': 0.11604005443503342, 'test/num_examples': 2472, 'score': 21678.261090755463, 'total_duration': 23834.59403681755, 'accumulated_submission_time': 21678.261090755463, 'accumulated_eval_time': 2154.2807199954987, 'accumulated_logging_time': 0.8591780662536621}
I1012 04:50:27.478426 139459802769152 logging_writer.py:48] [26396] accumulated_eval_time=2154.280720, accumulated_logging_time=0.859178, accumulated_submission_time=21678.261091, global_step=26396, preemption_count=0, score=21678.261091, test/ctc_loss=0.3468865156173706, test/num_examples=2472, test/wer=0.116040, total_duration=23834.594037, train/ctc_loss=0.28304073214530945, train/wer=0.101588, validation/ctc_loss=0.5585410594940186, validation/num_examples=5348, validation/wer=0.168059
I1012 04:51:46.980853 139459794376448 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.658135712146759, loss=1.4189523458480835
I1012 04:58:28.045271 139459802769152 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.6745489239692688, loss=1.4088737964630127
I1012 05:05:37.063491 139459794376448 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8409786820411682, loss=1.4582194089889526
I1012 05:12:43.284415 139459802769152 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6494126915931702, loss=1.424567699432373
I1012 05:14:28.205289 139633106130752 spec.py:321] Evaluating on the training split.
I1012 05:15:23.613968 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 05:16:15.572000 139633106130752 spec.py:349] Evaluating on the test split.
I1012 05:16:42.035622 139633106130752 submission_runner.py:393] Time since start: 25409.19s, 	Step: 28138, 	{'train/ctc_loss': Array(0.27256718, dtype=float32), 'train/wer': 0.09634758773051313, 'validation/ctc_loss': Array(0.53650576, dtype=float32), 'validation/wer': 0.16111685026598568, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32934523, dtype=float32), 'test/wer': 0.11049499319562082, 'test/num_examples': 2472, 'score': 23118.897171020508, 'total_duration': 25409.191937446594, 'accumulated_submission_time': 23118.897171020508, 'accumulated_eval_time': 2288.1056954860687, 'accumulated_logging_time': 0.9155960083007812}
I1012 05:16:42.074307 139459802769152 logging_writer.py:48] [28138] accumulated_eval_time=2288.105695, accumulated_logging_time=0.915596, accumulated_submission_time=23118.897171, global_step=28138, preemption_count=0, score=23118.897171, test/ctc_loss=0.3293452262878418, test/num_examples=2472, test/wer=0.110495, total_duration=25409.191937, train/ctc_loss=0.27256718277931213, train/wer=0.096348, validation/ctc_loss=0.5365057587623596, validation/num_examples=5348, validation/wer=0.161117
I1012 05:21:27.179662 139459794376448 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6851587891578674, loss=1.4526457786560059
I1012 05:28:36.078384 139460130449152 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.8001599907875061, loss=1.382803201675415
I1012 05:35:31.415894 139460122056448 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9158394932746887, loss=1.4508508443832397
I1012 05:40:42.634667 139633106130752 spec.py:321] Evaluating on the training split.
I1012 05:41:35.671771 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 05:42:27.861636 139633106130752 spec.py:349] Evaluating on the test split.
I1012 05:42:53.844763 139633106130752 submission_runner.py:393] Time since start: 26981.00s, 	Step: 29849, 	{'train/ctc_loss': Array(0.2840189, dtype=float32), 'train/wer': 0.10035318079389842, 'validation/ctc_loss': Array(0.5314, dtype=float32), 'validation/wer': 0.15759290189907027, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.320143, dtype=float32), 'test/wer': 0.10720451729531005, 'test/num_examples': 2472, 'score': 24559.365059137344, 'total_duration': 26981.000098466873, 'accumulated_submission_time': 24559.365059137344, 'accumulated_eval_time': 2419.3094305992126, 'accumulated_logging_time': 0.9739279747009277}
I1012 05:42:53.880768 139460560529152 logging_writer.py:48] [29849] accumulated_eval_time=2419.309431, accumulated_logging_time=0.973928, accumulated_submission_time=24559.365059, global_step=29849, preemption_count=0, score=24559.365059, test/ctc_loss=0.3201430141925812, test/num_examples=2472, test/wer=0.107205, total_duration=26981.000098, train/ctc_loss=0.2840189039707184, train/wer=0.100353, validation/ctc_loss=0.5314000248908997, validation/num_examples=5348, validation/wer=0.157593
I1012 05:44:53.520760 139459905169152 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7824283838272095, loss=1.36074960231781
I1012 05:51:45.126312 139459896776448 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.8791718482971191, loss=1.4478907585144043
I1012 05:58:58.989133 139459905169152 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.809848427772522, loss=1.3764500617980957
I1012 06:05:53.735833 139459896776448 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.7171674370765686, loss=1.3651819229125977
I1012 06:06:54.411151 139633106130752 spec.py:321] Evaluating on the training split.
I1012 06:07:47.399834 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 06:08:40.176752 139633106130752 spec.py:349] Evaluating on the test split.
I1012 06:09:07.084785 139633106130752 submission_runner.py:393] Time since start: 28554.24s, 	Step: 31569, 	{'train/ctc_loss': Array(0.25978902, dtype=float32), 'train/wer': 0.09060277049794084, 'validation/ctc_loss': Array(0.518501, dtype=float32), 'validation/wer': 0.15248559043030788, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31301984, dtype=float32), 'test/wer': 0.10237036134300165, 'test/num_examples': 2472, 'score': 25999.80398464203, 'total_duration': 28554.238365650177, 'accumulated_submission_time': 25999.80398464203, 'accumulated_eval_time': 2551.974945783615, 'accumulated_logging_time': 1.0275635719299316}
I1012 06:09:07.125069 139459976849152 logging_writer.py:48] [31569] accumulated_eval_time=2551.974946, accumulated_logging_time=1.027564, accumulated_submission_time=25999.803985, global_step=31569, preemption_count=0, score=25999.803985, test/ctc_loss=0.31301984190940857, test/num_examples=2472, test/wer=0.102370, total_duration=28554.238366, train/ctc_loss=0.25978901982307434, train/wer=0.090603, validation/ctc_loss=0.5185009837150574, validation/num_examples=5348, validation/wer=0.152486
I1012 06:14:56.005109 139459976849152 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.7732805609703064, loss=1.3512974977493286
I1012 06:21:44.966645 139459968456448 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.8543341755867004, loss=1.3330098390579224
I1012 06:29:10.988030 139459976849152 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.8770363926887512, loss=1.3955655097961426
I1012 06:33:07.620522 139633106130752 spec.py:321] Evaluating on the training split.
I1012 06:34:03.857484 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 06:34:54.562312 139633106130752 spec.py:349] Evaluating on the test split.
I1012 06:35:21.256171 139633106130752 submission_runner.py:393] Time since start: 30128.41s, 	Step: 33306, 	{'train/ctc_loss': Array(0.260205, dtype=float32), 'train/wer': 0.09063076298742995, 'validation/ctc_loss': Array(0.49199975, dtype=float32), 'validation/wer': 0.14521563667609605, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29075333, dtype=float32), 'test/wer': 0.09611439481648487, 'test/num_examples': 2472, 'score': 27440.208084106445, 'total_duration': 30128.411539316177, 'accumulated_submission_time': 27440.208084106445, 'accumulated_eval_time': 2685.604304790497, 'accumulated_logging_time': 1.0847833156585693}
I1012 06:35:21.296394 139460345489152 logging_writer.py:48] [33306] accumulated_eval_time=2685.604305, accumulated_logging_time=1.084783, accumulated_submission_time=27440.208084, global_step=33306, preemption_count=0, score=27440.208084, test/ctc_loss=0.2907533347606659, test/num_examples=2472, test/wer=0.096114, total_duration=30128.411539, train/ctc_loss=0.2602050006389618, train/wer=0.090631, validation/ctc_loss=0.4919997453689575, validation/num_examples=5348, validation/wer=0.145216
I1012 06:37:49.790604 139460337096448 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6761667728424072, loss=1.3623688220977783
I1012 06:45:07.329805 139460345489152 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7024024128913879, loss=1.2477885484695435
I1012 06:51:43.419725 139460337096448 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.713895320892334, loss=1.3175125122070312
I1012 06:59:04.426916 139460345489152 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7919524312019348, loss=1.2783825397491455
I1012 06:59:24.905808 139633106130752 spec.py:321] Evaluating on the training split.
I1012 07:00:19.133618 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 07:01:11.366098 139633106130752 spec.py:349] Evaluating on the test split.
I1012 07:01:37.865141 139633106130752 submission_runner.py:393] Time since start: 31705.02s, 	Step: 35021, 	{'train/ctc_loss': Array(0.24406941, dtype=float32), 'train/wer': 0.08340526657928109, 'validation/ctc_loss': Array(0.4747575, dtype=float32), 'validation/wer': 0.14094828002355736, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2776102, dtype=float32), 'test/wer': 0.09272236101801637, 'test/num_examples': 2472, 'score': 28883.72544693947, 'total_duration': 31705.019716262817, 'accumulated_submission_time': 28883.72544693947, 'accumulated_eval_time': 2818.55673623085, 'accumulated_logging_time': 1.1437885761260986}
I1012 07:01:37.906461 139460053649152 logging_writer.py:48] [35021] accumulated_eval_time=2818.556736, accumulated_logging_time=1.143789, accumulated_submission_time=28883.725447, global_step=35021, preemption_count=0, score=28883.725447, test/ctc_loss=0.2776102125644684, test/num_examples=2472, test/wer=0.092722, total_duration=31705.019716, train/ctc_loss=0.2440694123506546, train/wer=0.083405, validation/ctc_loss=0.47475749254226685, validation/num_examples=5348, validation/wer=0.140948
I1012 07:07:42.602380 139460045256448 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7519392967224121, loss=1.320444941520691
I1012 07:15:04.028446 139460053649152 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7774501442909241, loss=1.3632742166519165
I1012 07:21:41.565698 139459434129152 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.7551689743995667, loss=1.2350767850875854
I1012 07:25:38.406400 139633106130752 spec.py:321] Evaluating on the training split.
I1012 07:26:33.668460 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 07:27:26.279982 139633106130752 spec.py:349] Evaluating on the test split.
I1012 07:27:52.852467 139633106130752 submission_runner.py:393] Time since start: 33280.01s, 	Step: 36769, 	{'train/ctc_loss': Array(0.19014135, dtype=float32), 'train/wer': 0.06823795906234946, 'validation/ctc_loss': Array(0.45848003, dtype=float32), 'validation/wer': 0.1356382208405341, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2668064, dtype=float32), 'test/wer': 0.08908658826396929, 'test/num_examples': 2472, 'score': 30324.13405394554, 'total_duration': 33280.008269548416, 'accumulated_submission_time': 30324.13405394554, 'accumulated_eval_time': 2952.996910095215, 'accumulated_logging_time': 1.2019007205963135}
I1012 07:27:52.892321 139459761809152 logging_writer.py:48] [36769] accumulated_eval_time=2952.996910, accumulated_logging_time=1.201901, accumulated_submission_time=30324.134054, global_step=36769, preemption_count=0, score=30324.134054, test/ctc_loss=0.26680639386177063, test/num_examples=2472, test/wer=0.089087, total_duration=33280.008270, train/ctc_loss=0.19014135003089905, train/wer=0.068238, validation/ctc_loss=0.45848003029823303, validation/num_examples=5348, validation/wer=0.135638
I1012 07:30:48.642258 139459753416448 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.708840548992157, loss=1.306887149810791
I1012 07:37:26.885446 139459434129152 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.8087552189826965, loss=1.2778793573379517
I1012 07:44:47.145387 139459425736448 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.0220153331756592, loss=1.2889782190322876
I1012 07:51:32.106122 139459434129152 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7665570378303528, loss=1.3143224716186523
I1012 07:51:53.198513 139633106130752 spec.py:321] Evaluating on the training split.
I1012 07:52:47.330511 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 07:53:39.464551 139633106130752 spec.py:349] Evaluating on the test split.
I1012 07:54:05.866177 139633106130752 submission_runner.py:393] Time since start: 34853.02s, 	Step: 38527, 	{'train/ctc_loss': Array(0.20307493, dtype=float32), 'train/wer': 0.07222656332495618, 'validation/ctc_loss': Array(0.4376409, dtype=float32), 'validation/wer': 0.13151568398389604, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25359207, dtype=float32), 'test/wer': 0.08411025125424004, 'test/num_examples': 2472, 'score': 31764.349419355392, 'total_duration': 34853.02194070816, 'accumulated_submission_time': 31764.349419355392, 'accumulated_eval_time': 3085.658665418625, 'accumulated_logging_time': 1.2569711208343506}
I1012 07:54:05.907960 139459434129152 logging_writer.py:48] [38527] accumulated_eval_time=3085.658665, accumulated_logging_time=1.256971, accumulated_submission_time=31764.349419, global_step=38527, preemption_count=0, score=31764.349419, test/ctc_loss=0.2535920739173889, test/num_examples=2472, test/wer=0.084110, total_duration=34853.021941, train/ctc_loss=0.20307493209838867, train/wer=0.072227, validation/ctc_loss=0.437640905380249, validation/num_examples=5348, validation/wer=0.131516
I1012 08:00:34.469388 139459425736448 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7360995411872864, loss=1.2586911916732788
I1012 08:07:25.240836 139459434129152 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.7367228865623474, loss=1.2842390537261963
I1012 08:14:43.518589 139459425736448 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.088829755783081, loss=1.3390272855758667
I1012 08:18:06.007111 139633106130752 spec.py:321] Evaluating on the training split.
I1012 08:18:59.439416 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 08:19:51.111837 139633106130752 spec.py:349] Evaluating on the test split.
I1012 08:20:18.157672 139633106130752 submission_runner.py:393] Time since start: 36425.31s, 	Step: 40234, 	{'train/ctc_loss': Array(0.24375516, dtype=float32), 'train/wer': 0.08508214771944479, 'validation/ctc_loss': Array(0.42571378, dtype=float32), 'validation/wer': 0.1254139432499493, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24401668, dtype=float32), 'test/wer': 0.08094164483171856, 'test/num_examples': 2472, 'score': 33204.359389066696, 'total_duration': 36425.31294822693, 'accumulated_submission_time': 33204.359389066696, 'accumulated_eval_time': 3217.803034543991, 'accumulated_logging_time': 1.3148212432861328}
I1012 08:20:18.193921 139459761809152 logging_writer.py:48] [40234] accumulated_eval_time=3217.803035, accumulated_logging_time=1.314821, accumulated_submission_time=33204.359389, global_step=40234, preemption_count=0, score=33204.359389, test/ctc_loss=0.24401667714118958, test/num_examples=2472, test/wer=0.080942, total_duration=36425.312948, train/ctc_loss=0.24375516176223755, train/wer=0.085082, validation/ctc_loss=0.42571377754211426, validation/num_examples=5348, validation/wer=0.125414
I1012 08:23:41.409092 139459753416448 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.8422677516937256, loss=1.1844557523727417
I1012 08:30:46.584232 139459761809152 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.9401722550392151, loss=1.208509922027588
I1012 08:37:38.753261 139459761809152 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.78693687915802, loss=1.2263044118881226
I1012 08:44:18.489326 139633106130752 spec.py:321] Evaluating on the training split.
I1012 08:45:11.583168 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 08:46:04.151164 139633106130752 spec.py:349] Evaluating on the test split.
I1012 08:46:30.391582 139633106130752 submission_runner.py:393] Time since start: 37997.55s, 	Step: 41958, 	{'train/ctc_loss': Array(0.246588, dtype=float32), 'train/wer': 0.08537390985749735, 'validation/ctc_loss': Array(0.41846702, dtype=float32), 'validation/wer': 0.12228583565849561, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23728706, dtype=float32), 'test/wer': 0.07706213312209291, 'test/num_examples': 2472, 'score': 34644.56383180618, 'total_duration': 37997.54720067978, 'accumulated_submission_time': 34644.56383180618, 'accumulated_eval_time': 3349.6992213726044, 'accumulated_logging_time': 1.367805004119873}
I1012 08:46:30.431121 139460053649152 logging_writer.py:48] [41958] accumulated_eval_time=3349.699221, accumulated_logging_time=1.367805, accumulated_submission_time=34644.563832, global_step=41958, preemption_count=0, score=34644.563832, test/ctc_loss=0.23728705942630768, test/num_examples=2472, test/wer=0.077062, total_duration=37997.547201, train/ctc_loss=0.24658800661563873, train/wer=0.085374, validation/ctc_loss=0.4184670150279999, validation/num_examples=5348, validation/wer=0.122286
I1012 08:47:03.001816 139460045256448 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.8928309082984924, loss=1.20286226272583
I1012 08:53:34.743404 139459398289152 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.834956169128418, loss=1.1434201002120972
I1012 09:00:48.866773 139459389896448 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.929107129573822, loss=1.1181303262710571
I1012 09:07:49.681513 139460053649152 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.9343342185020447, loss=1.2165881395339966
I1012 09:10:30.546832 139633106130752 spec.py:321] Evaluating on the training split.
I1012 09:11:21.838720 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 09:12:13.082645 139633106130752 spec.py:349] Evaluating on the test split.
I1012 09:12:39.860091 139633106130752 submission_runner.py:393] Time since start: 39567.01s, 	Step: 43699, 	{'train/ctc_loss': Array(0.26955163, dtype=float32), 'train/wer': 0.09448906411778814, 'validation/ctc_loss': Array(0.3980731, dtype=float32), 'validation/wer': 0.11752609169989477, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22429788, dtype=float32), 'test/wer': 0.07338573720878273, 'test/num_examples': 2472, 'score': 36084.5872592926, 'total_duration': 39567.01443386078, 'accumulated_submission_time': 36084.5872592926, 'accumulated_eval_time': 3479.0051443576813, 'accumulated_logging_time': 1.4248120784759521}
I1012 09:12:39.905462 139459761809152 logging_writer.py:48] [43699] accumulated_eval_time=3479.005144, accumulated_logging_time=1.424812, accumulated_submission_time=36084.587259, global_step=43699, preemption_count=0, score=36084.587259, test/ctc_loss=0.2242978811264038, test/num_examples=2472, test/wer=0.073386, total_duration=39567.014434, train/ctc_loss=0.2695516347885132, train/wer=0.094489, validation/ctc_loss=0.39807310700416565, validation/num_examples=5348, validation/wer=0.117526
I1012 09:16:37.491681 139459753416448 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.8201514482498169, loss=1.194166660308838
I1012 09:23:38.865041 139459761809152 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.012282371520996, loss=1.1589261293411255
I1012 09:30:44.083160 139459753416448 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.9425391554832458, loss=1.1496000289916992
I1012 09:36:40.370918 139633106130752 spec.py:321] Evaluating on the training split.
I1012 09:37:32.256610 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 09:38:24.197868 139633106130752 spec.py:349] Evaluating on the test split.
I1012 09:38:50.652931 139633106130752 submission_runner.py:393] Time since start: 41137.81s, 	Step: 45408, 	{'train/ctc_loss': Array(0.22383668, dtype=float32), 'train/wer': 0.07831662241167155, 'validation/ctc_loss': Array(0.3803963, dtype=float32), 'validation/wer': 0.11149193353736833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21352662, dtype=float32), 'test/wer': 0.07072492027704995, 'test/num_examples': 2472, 'score': 37524.958131313324, 'total_duration': 41137.80766797066, 'accumulated_submission_time': 37524.958131313324, 'accumulated_eval_time': 3609.2803947925568, 'accumulated_logging_time': 1.4917166233062744}
I1012 09:38:50.697699 139459761809152 logging_writer.py:48] [45408] accumulated_eval_time=3609.280395, accumulated_logging_time=1.491717, accumulated_submission_time=37524.958131, global_step=45408, preemption_count=0, score=37524.958131, test/ctc_loss=0.2135266214609146, test/num_examples=2472, test/wer=0.070725, total_duration=41137.807668, train/ctc_loss=0.22383667528629303, train/wer=0.078317, validation/ctc_loss=0.38039630651474, validation/num_examples=5348, validation/wer=0.111492
I1012 09:40:01.113481 139459753416448 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.0583915710449219, loss=1.1655794382095337
I1012 09:46:49.046072 139459761809152 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9347197413444519, loss=1.1185535192489624
I1012 09:53:58.436816 139459761809152 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.193718671798706, loss=1.1616195440292358
I1012 10:00:55.009407 139459753416448 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.9711501002311707, loss=1.0599733591079712
I1012 10:02:50.934525 139633106130752 spec.py:321] Evaluating on the training split.
I1012 10:03:45.744169 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 10:04:38.907611 139633106130752 spec.py:349] Evaluating on the test split.
I1012 10:05:05.114880 139633106130752 submission_runner.py:393] Time since start: 42712.27s, 	Step: 47130, 	{'train/ctc_loss': Array(0.19620135, dtype=float32), 'train/wer': 0.07103648692282855, 'validation/ctc_loss': Array(0.36219963, dtype=float32), 'validation/wer': 0.10645220464002626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20107274, dtype=float32), 'test/wer': 0.06698758962484512, 'test/num_examples': 2472, 'score': 38965.105144023895, 'total_duration': 42712.27090001106, 'accumulated_submission_time': 38965.105144023895, 'accumulated_eval_time': 3743.455114841461, 'accumulated_logging_time': 1.5524523258209229}
I1012 10:05:05.158454 139460560529152 logging_writer.py:48] [47130] accumulated_eval_time=3743.455115, accumulated_logging_time=1.552452, accumulated_submission_time=38965.105144, global_step=47130, preemption_count=0, score=38965.105144, test/ctc_loss=0.20107273757457733, test/num_examples=2472, test/wer=0.066988, total_duration=42712.270900, train/ctc_loss=0.196201354265213, train/wer=0.071036, validation/ctc_loss=0.3621996343135834, validation/num_examples=5348, validation/wer=0.106452
I1012 10:09:53.824117 139459905169152 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.0870506763458252, loss=1.1007014513015747
I1012 10:16:52.369726 139459896776448 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0555765628814697, loss=1.1258710622787476
I1012 10:24:10.027668 139460560529152 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.9521639943122864, loss=1.1165189743041992
I1012 10:29:05.461643 139633106130752 spec.py:321] Evaluating on the training split.
I1012 10:29:58.880631 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 10:30:51.972499 139633106130752 spec.py:349] Evaluating on the test split.
I1012 10:31:18.795996 139633106130752 submission_runner.py:393] Time since start: 44285.95s, 	Step: 48870, 	{'train/ctc_loss': Array(0.1566978, dtype=float32), 'train/wer': 0.05633013953387204, 'validation/ctc_loss': Array(0.3515569, dtype=float32), 'validation/wer': 0.10123869198760342, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19438645, dtype=float32), 'test/wer': 0.06288465053927243, 'test/num_examples': 2472, 'score': 40405.32123088837, 'total_duration': 44285.95129823685, 'accumulated_submission_time': 40405.32123088837, 'accumulated_eval_time': 3876.783122062683, 'accumulated_logging_time': 1.610213041305542}
I1012 10:31:18.833364 139460560529152 logging_writer.py:48] [48870] accumulated_eval_time=3876.783122, accumulated_logging_time=1.610213, accumulated_submission_time=40405.321231, global_step=48870, preemption_count=0, score=40405.321231, test/ctc_loss=0.19438645243644714, test/num_examples=2472, test/wer=0.062885, total_duration=44285.951298, train/ctc_loss=0.15669779479503632, train/wer=0.056330, validation/ctc_loss=0.3515568971633911, validation/num_examples=5348, validation/wer=0.101239
I1012 10:32:58.233615 139460552136448 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.1975821256637573, loss=1.0786367654800415
I1012 10:40:06.890551 139460560529152 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1218030452728271, loss=1.0639208555221558
I1012 10:46:54.516552 139460552136448 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.964787483215332, loss=1.032431960105896
I1012 10:54:20.157181 139460560529152 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.1634645462036133, loss=1.047745704650879
I1012 10:55:19.170277 139633106130752 spec.py:321] Evaluating on the training split.
I1012 10:56:13.072938 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 10:57:04.877748 139633106130752 spec.py:349] Evaluating on the test split.
I1012 10:57:31.476045 139633106130752 submission_runner.py:393] Time since start: 45858.63s, 	Step: 50579, 	{'train/ctc_loss': Array(0.1639799, dtype=float32), 'train/wer': 0.058291288078337236, 'validation/ctc_loss': Array(0.34078166, dtype=float32), 'validation/wer': 0.09763750639620765, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18385345, dtype=float32), 'test/wer': 0.059370747263014646, 'test/num_examples': 2472, 'score': 41845.57046985626, 'total_duration': 45858.63100862503, 'accumulated_submission_time': 41845.57046985626, 'accumulated_eval_time': 4009.082178592682, 'accumulated_logging_time': 1.663142442703247}
I1012 10:57:31.511976 139460130449152 logging_writer.py:48] [50579] accumulated_eval_time=4009.082179, accumulated_logging_time=1.663142, accumulated_submission_time=41845.570470, global_step=50579, preemption_count=0, score=41845.570470, test/ctc_loss=0.18385344743728638, test/num_examples=2472, test/wer=0.059371, total_duration=45858.631009, train/ctc_loss=0.1639799028635025, train/wer=0.058291, validation/ctc_loss=0.34078165888786316, validation/num_examples=5348, validation/wer=0.097638
I1012 11:02:59.635564 139460122056448 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.053467869758606, loss=1.044122576713562
I1012 11:10:30.339398 139460130449152 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.3378974199295044, loss=1.028104305267334
I1012 11:17:13.588157 139460122056448 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.1130331754684448, loss=1.0362640619277954
I1012 11:21:31.617026 139633106130752 spec.py:321] Evaluating on the training split.
I1012 11:22:25.639313 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 11:23:17.882070 139633106130752 spec.py:349] Evaluating on the test split.
I1012 11:23:44.689363 139633106130752 submission_runner.py:393] Time since start: 47431.84s, 	Step: 52293, 	{'train/ctc_loss': Array(0.13913022, dtype=float32), 'train/wer': 0.050565839202886105, 'validation/ctc_loss': Array(0.32421482, dtype=float32), 'validation/wer': 0.0936501346824102, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1766318, dtype=float32), 'test/wer': 0.057400524038754494, 'test/num_examples': 2472, 'score': 43285.585735082626, 'total_duration': 47431.84371328354, 'accumulated_submission_time': 43285.585735082626, 'accumulated_eval_time': 4142.147248268127, 'accumulated_logging_time': 1.7157704830169678}
I1012 11:23:44.732613 139460130449152 logging_writer.py:48] [52293] accumulated_eval_time=4142.147248, accumulated_logging_time=1.715770, accumulated_submission_time=43285.585735, global_step=52293, preemption_count=0, score=43285.585735, test/ctc_loss=0.17663179337978363, test/num_examples=2472, test/wer=0.057401, total_duration=47431.843713, train/ctc_loss=0.13913021981716156, train/wer=0.050566, validation/ctc_loss=0.3242148160934448, validation/num_examples=5348, validation/wer=0.093650
I1012 11:26:23.254501 139460122056448 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.0835133790969849, loss=1.0213359594345093
I1012 11:33:01.608130 139460130449152 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.1636828184127808, loss=0.9930033087730408
I1012 11:40:25.730346 139460122056448 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.0587188005447388, loss=1.0410761833190918
I1012 11:47:12.162308 139459802769152 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.3707168102264404, loss=0.986016035079956
I1012 11:47:44.915068 139633106130752 spec.py:321] Evaluating on the training split.
I1012 11:48:39.071556 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 11:49:31.088653 139633106130752 spec.py:349] Evaluating on the test split.
I1012 11:49:58.023135 139633106130752 submission_runner.py:393] Time since start: 49005.18s, 	Step: 54038, 	{'train/ctc_loss': Array(0.13397767, dtype=float32), 'train/wer': 0.04963169485457642, 'validation/ctc_loss': Array(0.32009247, dtype=float32), 'validation/wer': 0.09085993994805797, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1700292, dtype=float32), 'test/wer': 0.05447565657181159, 'test/num_examples': 2472, 'score': 44725.67624735832, 'total_duration': 49005.1785197258, 'accumulated_submission_time': 44725.67624735832, 'accumulated_eval_time': 4275.249014854431, 'accumulated_logging_time': 1.775087833404541}
I1012 11:49:58.066796 139459546769152 logging_writer.py:48] [54038] accumulated_eval_time=4275.249015, accumulated_logging_time=1.775088, accumulated_submission_time=44725.676247, global_step=54038, preemption_count=0, score=44725.676247, test/ctc_loss=0.1700291931629181, test/num_examples=2472, test/wer=0.054476, total_duration=49005.178520, train/ctc_loss=0.13397766649723053, train/wer=0.049632, validation/ctc_loss=0.32009246945381165, validation/num_examples=5348, validation/wer=0.090860
I1012 11:56:17.019114 139459538376448 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.179238200187683, loss=1.03273344039917
I1012 12:02:58.914985 139459546769152 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2426921129226685, loss=0.9542418718338013
I1012 12:10:26.372552 139459538376448 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.28026282787323, loss=0.9907591938972473
I1012 12:13:58.735065 139633106130752 spec.py:321] Evaluating on the training split.
I1012 12:14:51.718633 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 12:15:43.699243 139633106130752 spec.py:349] Evaluating on the test split.
I1012 12:16:10.746540 139633106130752 submission_runner.py:393] Time since start: 50577.90s, 	Step: 55756, 	{'train/ctc_loss': Array(0.12478878, dtype=float32), 'train/wer': 0.04467632850241546, 'validation/ctc_loss': Array(0.3109721, dtype=float32), 'validation/wer': 0.08901590121358989, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16544126, dtype=float32), 'test/wer': 0.05299291125870859, 'test/num_examples': 2472, 'score': 46166.253903865814, 'total_duration': 50577.9007794857, 'accumulated_submission_time': 46166.253903865814, 'accumulated_eval_time': 4407.253207921982, 'accumulated_logging_time': 1.8347620964050293}
I1012 12:16:10.784538 139460560529152 logging_writer.py:48] [55756] accumulated_eval_time=4407.253208, accumulated_logging_time=1.834762, accumulated_submission_time=46166.253904, global_step=55756, preemption_count=0, score=46166.253904, test/ctc_loss=0.16544125974178314, test/num_examples=2472, test/wer=0.052993, total_duration=50577.900779, train/ctc_loss=0.12478877604007721, train/wer=0.044676, validation/ctc_loss=0.31097209453582764, validation/num_examples=5348, validation/wer=0.089016
I1012 12:19:16.465194 139460552136448 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.2377903461456299, loss=0.9735289216041565
I1012 12:26:33.973177 139460560529152 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.185194492340088, loss=0.9991668462753296
I1012 12:33:21.659740 139459475089152 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.672216534614563, loss=1.0109872817993164
I1012 12:40:11.481394 139633106130752 spec.py:321] Evaluating on the training split.
I1012 12:41:03.586832 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 12:41:55.301123 139633106130752 spec.py:349] Evaluating on the test split.
I1012 12:42:21.906263 139633106130752 submission_runner.py:393] Time since start: 52149.06s, 	Step: 57459, 	{'train/ctc_loss': Array(0.12825522, dtype=float32), 'train/wer': 0.04683332761008665, 'validation/ctc_loss': Array(0.3062976, dtype=float32), 'validation/wer': 0.08708497060158144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16248833, dtype=float32), 'test/wer': 0.051632035423394874, 'test/num_examples': 2472, 'score': 47606.86358118057, 'total_duration': 52149.0624089241, 'accumulated_submission_time': 47606.86358118057, 'accumulated_eval_time': 4537.672585248947, 'accumulated_logging_time': 1.887894868850708}
I1012 12:42:21.950724 139460560529152 logging_writer.py:48] [57459] accumulated_eval_time=4537.672585, accumulated_logging_time=1.887895, accumulated_submission_time=47606.863581, global_step=57459, preemption_count=0, score=47606.863581, test/ctc_loss=0.16248832643032074, test/num_examples=2472, test/wer=0.051632, total_duration=52149.062409, train/ctc_loss=0.1282552182674408, train/wer=0.046833, validation/ctc_loss=0.3062976002693176, validation/num_examples=5348, validation/wer=0.087085
I1012 12:42:53.827364 139460552136448 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.229918122291565, loss=0.9846438765525818
I1012 12:49:27.685926 139460560529152 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.098130464553833, loss=0.9503174424171448
I1012 12:56:50.231474 139460552136448 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.1990382671356201, loss=0.9645189642906189
I1012 13:03:45.980577 139460560529152 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.181392788887024, loss=0.9980677962303162
I1012 13:06:22.097814 139633106130752 spec.py:321] Evaluating on the training split.
I1012 13:07:15.550596 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 13:08:07.693555 139633106130752 spec.py:349] Evaluating on the test split.
I1012 13:08:33.802006 139633106130752 submission_runner.py:393] Time since start: 53720.96s, 	Step: 59189, 	{'train/ctc_loss': Array(0.13686039, dtype=float32), 'train/wer': 0.04832333177229114, 'validation/ctc_loss': Array(0.30417097, dtype=float32), 'validation/wer': 0.0868242949689603, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16086234, dtype=float32), 'test/wer': 0.05116486909186928, 'test/num_examples': 2472, 'score': 49046.91881299019, 'total_duration': 53720.95758342743, 'accumulated_submission_time': 49046.91881299019, 'accumulated_eval_time': 4669.370657205582, 'accumulated_logging_time': 1.9503746032714844}
I1012 13:08:33.838519 139460560529152 logging_writer.py:48] [59189] accumulated_eval_time=4669.370657, accumulated_logging_time=1.950375, accumulated_submission_time=49046.918813, global_step=59189, preemption_count=0, score=49046.918813, test/ctc_loss=0.16086234152317047, test/num_examples=2472, test/wer=0.051165, total_duration=53720.957583, train/ctc_loss=0.13686038553714752, train/wer=0.048323, validation/ctc_loss=0.30417096614837646, validation/num_examples=5348, validation/wer=0.086824
I1012 13:12:44.367569 139460552136448 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.664811134338379, loss=0.9497177004814148
I1012 13:19:39.709223 139633106130752 spec.py:321] Evaluating on the training split.
I1012 13:20:31.790881 139633106130752 spec.py:333] Evaluating on the validation split.
I1012 13:21:23.858776 139633106130752 spec.py:349] Evaluating on the test split.
I1012 13:21:50.041038 139633106130752 submission_runner.py:393] Time since start: 54517.20s, 	Step: 60000, 	{'train/ctc_loss': Array(0.11757164, dtype=float32), 'train/wer': 0.04299213127984397, 'validation/ctc_loss': Array(0.30437273, dtype=float32), 'validation/wer': 0.08692084149956071, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16084751, dtype=float32), 'test/wer': 0.05094144171592225, 'test/num_examples': 2472, 'score': 49712.73689985275, 'total_duration': 54517.20010280609, 'accumulated_submission_time': 49712.73689985275, 'accumulated_eval_time': 4799.699951648712, 'accumulated_logging_time': 2.0042941570281982}
I1012 13:21:50.076485 139460560529152 logging_writer.py:48] [60000] accumulated_eval_time=4799.699952, accumulated_logging_time=2.004294, accumulated_submission_time=49712.736900, global_step=60000, preemption_count=0, score=49712.736900, test/ctc_loss=0.1608475148677826, test/num_examples=2472, test/wer=0.050941, total_duration=54517.200103, train/ctc_loss=0.1175716444849968, train/wer=0.042992, validation/ctc_loss=0.30437272787094116, validation/num_examples=5348, validation/wer=0.086921
I1012 13:21:50.098623 139460552136448 logging_writer.py:48] [60000] global_step=60000, preemption_count=0, score=49712.736900
I1012 13:21:50.568770 139633106130752 checkpoints.py:490] Saving checkpoint at step: 60000
I1012 13:21:52.056333 139633106130752 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/checkpoint_60000
I1012 13:21:52.089262 139633106130752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_conformer/adamw_run_0/librispeech_conformer_jax/trial_1/checkpoint_60000.
I1012 13:21:53.397834 139633106130752 submission_runner.py:563] Tuning trial 1/1
I1012 13:21:53.398109 139633106130752 submission_runner.py:564] Hyperparameters: Hyperparameters(learning_rate=0.002106913873888147, beta1=0.8231189937738506, beta2=0.8774571227688758, warmup_steps=1199, weight_decay=0.27590534177690645)
I1012 13:21:53.418565 139633106130752 submission_runner.py:565] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.283686, dtype=float32), 'train/wer': 1.2353195343897716, 'validation/ctc_loss': Array(30.45324, dtype=float32), 'validation/wer': 1.2676462921304923, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.471466, dtype=float32), 'test/wer': 1.2872260475697195, 'test/num_examples': 2472, 'score': 75.92344069480896, 'total_duration': 257.6823902130127, 'accumulated_submission_time': 75.92344069480896, 'accumulated_eval_time': 181.7588791847229, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1795, {'train/ctc_loss': Array(2.1724489, dtype=float32), 'train/wer': 0.5042532594591255, 'validation/ctc_loss': Array(2.7234242, dtype=float32), 'validation/wer': 0.541548799443892, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.363636, dtype=float32), 'test/wer': 0.5019803790140759, 'test/num_examples': 2472, 'score': 1516.1533861160278, 'total_duration': 1825.2312757968903, 'accumulated_submission_time': 1516.1533861160278, 'accumulated_eval_time': 308.9622106552124, 'accumulated_logging_time': 0.04378247261047363, 'global_step': 1795, 'preemption_count': 0}), (3607, {'train/ctc_loss': Array(0.6835408, dtype=float32), 'train/wer': 0.2335988067016488, 'validation/ctc_loss': Array(1.0022143, dtype=float32), 'validation/wer': 0.29368489143342635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.71744424, dtype=float32), 'test/wer': 0.23616273637600796, 'test/num_examples': 2472, 'score': 2956.1371109485626, 'total_duration': 3395.4316880702972, 'accumulated_submission_time': 2956.1371109485626, 'accumulated_eval_time': 439.0467426776886, 'accumulated_logging_time': 0.09451818466186523, 'global_step': 3607, 'preemption_count': 0}), (5406, {'train/ctc_loss': Array(0.50123256, dtype=float32), 'train/wer': 0.1806273753235184, 'validation/ctc_loss': Array(0.8452047, dtype=float32), 'validation/wer': 0.25117545401006014, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5711044, dtype=float32), 'test/wer': 0.1888976905733959, 'test/num_examples': 2472, 'score': 4396.388687610626, 'total_duration': 4964.966960906982, 'accumulated_submission_time': 4396.388687610626, 'accumulated_eval_time': 568.193030834198, 'accumulated_logging_time': 0.1506650447845459, 'global_step': 5406, 'preemption_count': 0}), (7182, {'train/ctc_loss': Array(0.4753171, dtype=float32), 'train/wer': 0.1652860538910241, 'validation/ctc_loss': Array(0.7753231, dtype=float32), 'validation/wer': 0.23046622319626944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5148791, dtype=float32), 'test/wer': 0.17061726890500276, 'test/num_examples': 2472, 'score': 5836.592508554459, 'total_duration': 6536.8271048069, 'accumulated_submission_time': 5836.592508554459, 'accumulated_eval_time': 699.7124745845795, 'accumulated_logging_time': 0.20771479606628418, 'global_step': 7182, 'preemption_count': 0}), (8917, {'train/ctc_loss': Array(0.45146623, dtype=float32), 'train/wer': 0.1579562504077062, 'validation/ctc_loss': Array(0.75178593, dtype=float32), 'validation/wer': 0.22201840176873244, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48599577, dtype=float32), 'test/wer': 0.16182235492454247, 'test/num_examples': 2472, 'score': 7277.061921596527, 'total_duration': 8109.292993068695, 'accumulated_submission_time': 7277.061921596527, 'accumulated_eval_time': 831.5793786048889, 'accumulated_logging_time': 0.26070261001586914, 'global_step': 8917, 'preemption_count': 0}), (10694, {'train/ctc_loss': Array(0.4585386, dtype=float32), 'train/wer': 0.15907090386287617, 'validation/ctc_loss': Array(0.7237231, dtype=float32), 'validation/wer': 0.21542427372872355, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47273266, dtype=float32), 'test/wer': 0.1602583632929133, 'test/num_examples': 2472, 'score': 8717.120530128479, 'total_duration': 9679.894563674927, 'accumulated_submission_time': 8717.120530128479, 'accumulated_eval_time': 961.9738273620605, 'accumulated_logging_time': 0.32740354537963867, 'global_step': 10694, 'preemption_count': 0}), (12422, {'train/ctc_loss': Array(0.42136386, dtype=float32), 'train/wer': 0.146092570140174, 'validation/ctc_loss': Array(0.7110198, dtype=float32), 'validation/wer': 0.21197756258628847, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45268196, dtype=float32), 'test/wer': 0.15396177360713342, 'test/num_examples': 2472, 'score': 10157.064435720444, 'total_duration': 11252.56951546669, 'accumulated_submission_time': 10157.064435720444, 'accumulated_eval_time': 1094.5730183124542, 'accumulated_logging_time': 0.38143348693847656, 'global_step': 12422, 'preemption_count': 0}), (14164, {'train/ctc_loss': Array(0.3761427, dtype=float32), 'train/wer': 0.1358433656621337, 'validation/ctc_loss': Array(0.6898569, dtype=float32), 'validation/wer': 0.20477519140349693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43603724, dtype=float32), 'test/wer': 0.14669022809903925, 'test/num_examples': 2472, 'score': 11596.988751649857, 'total_duration': 12824.777482032776, 'accumulated_submission_time': 11596.988751649857, 'accumulated_eval_time': 1226.7232501506805, 'accumulated_logging_time': 0.43695807456970215, 'global_step': 14164, 'preemption_count': 0}), (15925, {'train/ctc_loss': Array(0.35632157, dtype=float32), 'train/wer': 0.1272231059376812, 'validation/ctc_loss': Array(0.6625003, dtype=float32), 'validation/wer': 0.199088600751132, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42379397, dtype=float32), 'test/wer': 0.14075924684662727, 'test/num_examples': 2472, 'score': 13036.85973405838, 'total_duration': 14397.064236879349, 'accumulated_submission_time': 13036.85973405838, 'accumulated_eval_time': 1359.0066635608673, 'accumulated_logging_time': 0.4878711700439453, 'global_step': 15925, 'preemption_count': 0}), (17661, {'train/ctc_loss': Array(0.3570879, dtype=float32), 'train/wer': 0.12983530597907406, 'validation/ctc_loss': Array(0.65580726, dtype=float32), 'validation/wer': 0.1957577454454174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41565606, dtype=float32), 'test/wer': 0.13921556679462962, 'test/num_examples': 2472, 'score': 14477.183139562607, 'total_duration': 15967.536049604416, 'accumulated_submission_time': 14477.183139562607, 'accumulated_eval_time': 1488.9995353221893, 'accumulated_logging_time': 0.5632083415985107, 'global_step': 17661, 'preemption_count': 0}), (19420, {'train/ctc_loss': Array(0.35986096, dtype=float32), 'train/wer': 0.12371030434437716, 'validation/ctc_loss': Array(0.6383446, dtype=float32), 'validation/wer': 0.18857468356874596, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3996908, dtype=float32), 'test/wer': 0.13261430341437655, 'test/num_examples': 2472, 'score': 15917.56741642952, 'total_duration': 17542.244049310684, 'accumulated_submission_time': 15917.56741642952, 'accumulated_eval_time': 1623.1937549114227, 'accumulated_logging_time': 0.6120152473449707, 'global_step': 19420, 'preemption_count': 0}), (21179, {'train/ctc_loss': Array(0.35657898, dtype=float32), 'train/wer': 0.12375023082807925, 'validation/ctc_loss': Array(0.61579514, dtype=float32), 'validation/wer': 0.18322600577348253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3865894, dtype=float32), 'test/wer': 0.12877541486401398, 'test/num_examples': 2472, 'score': 17357.973737955093, 'total_duration': 19114.34250688553, 'accumulated_submission_time': 17357.973737955093, 'accumulated_eval_time': 1754.7336728572845, 'accumulated_logging_time': 0.6818695068359375, 'global_step': 21179, 'preemption_count': 0}), (22940, {'train/ctc_loss': Array(0.32963604, dtype=float32), 'train/wer': 0.11558802345920362, 'validation/ctc_loss': Array(0.59680986, dtype=float32), 'validation/wer': 0.17849522577406182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36326215, dtype=float32), 'test/wer': 0.12174760831149839, 'test/num_examples': 2472, 'score': 18798.168061494827, 'total_duration': 20688.558798074722, 'accumulated_submission_time': 18798.168061494827, 'accumulated_eval_time': 1888.6200063228607, 'accumulated_logging_time': 0.7375879287719727, 'global_step': 22940, 'preemption_count': 0}), (24661, {'train/ctc_loss': Array(0.30119097, dtype=float32), 'train/wer': 0.10811409991132132, 'validation/ctc_loss': Array(0.57746845, dtype=float32), 'validation/wer': 0.17099356034640895, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3587026, dtype=float32), 'test/wer': 0.12105701460402575, 'test/num_examples': 2472, 'score': 20238.383621692657, 'total_duration': 22261.172021389008, 'accumulated_submission_time': 20238.383621692657, 'accumulated_eval_time': 2020.8713102340698, 'accumulated_logging_time': 0.8044335842132568, 'global_step': 24661, 'preemption_count': 0}), (26396, {'train/ctc_loss': Array(0.28304073, dtype=float32), 'train/wer': 0.1015884608362242, 'validation/ctc_loss': Array(0.55854106, dtype=float32), 'validation/wer': 0.1680585458161561, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34688652, dtype=float32), 'test/wer': 0.11604005443503342, 'test/num_examples': 2472, 'score': 21678.261090755463, 'total_duration': 23834.59403681755, 'accumulated_submission_time': 21678.261090755463, 'accumulated_eval_time': 2154.2807199954987, 'accumulated_logging_time': 0.8591780662536621, 'global_step': 26396, 'preemption_count': 0}), (28138, {'train/ctc_loss': Array(0.27256718, dtype=float32), 'train/wer': 0.09634758773051313, 'validation/ctc_loss': Array(0.53650576, dtype=float32), 'validation/wer': 0.16111685026598568, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32934523, dtype=float32), 'test/wer': 0.11049499319562082, 'test/num_examples': 2472, 'score': 23118.897171020508, 'total_duration': 25409.191937446594, 'accumulated_submission_time': 23118.897171020508, 'accumulated_eval_time': 2288.1056954860687, 'accumulated_logging_time': 0.9155960083007812, 'global_step': 28138, 'preemption_count': 0}), (29849, {'train/ctc_loss': Array(0.2840189, dtype=float32), 'train/wer': 0.10035318079389842, 'validation/ctc_loss': Array(0.5314, dtype=float32), 'validation/wer': 0.15759290189907027, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.320143, dtype=float32), 'test/wer': 0.10720451729531005, 'test/num_examples': 2472, 'score': 24559.365059137344, 'total_duration': 26981.000098466873, 'accumulated_submission_time': 24559.365059137344, 'accumulated_eval_time': 2419.3094305992126, 'accumulated_logging_time': 0.9739279747009277, 'global_step': 29849, 'preemption_count': 0}), (31569, {'train/ctc_loss': Array(0.25978902, dtype=float32), 'train/wer': 0.09060277049794084, 'validation/ctc_loss': Array(0.518501, dtype=float32), 'validation/wer': 0.15248559043030788, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31301984, dtype=float32), 'test/wer': 0.10237036134300165, 'test/num_examples': 2472, 'score': 25999.80398464203, 'total_duration': 28554.238365650177, 'accumulated_submission_time': 25999.80398464203, 'accumulated_eval_time': 2551.974945783615, 'accumulated_logging_time': 1.0275635719299316, 'global_step': 31569, 'preemption_count': 0}), (33306, {'train/ctc_loss': Array(0.260205, dtype=float32), 'train/wer': 0.09063076298742995, 'validation/ctc_loss': Array(0.49199975, dtype=float32), 'validation/wer': 0.14521563667609605, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29075333, dtype=float32), 'test/wer': 0.09611439481648487, 'test/num_examples': 2472, 'score': 27440.208084106445, 'total_duration': 30128.411539316177, 'accumulated_submission_time': 27440.208084106445, 'accumulated_eval_time': 2685.604304790497, 'accumulated_logging_time': 1.0847833156585693, 'global_step': 33306, 'preemption_count': 0}), (35021, {'train/ctc_loss': Array(0.24406941, dtype=float32), 'train/wer': 0.08340526657928109, 'validation/ctc_loss': Array(0.4747575, dtype=float32), 'validation/wer': 0.14094828002355736, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2776102, dtype=float32), 'test/wer': 0.09272236101801637, 'test/num_examples': 2472, 'score': 28883.72544693947, 'total_duration': 31705.019716262817, 'accumulated_submission_time': 28883.72544693947, 'accumulated_eval_time': 2818.55673623085, 'accumulated_logging_time': 1.1437885761260986, 'global_step': 35021, 'preemption_count': 0}), (36769, {'train/ctc_loss': Array(0.19014135, dtype=float32), 'train/wer': 0.06823795906234946, 'validation/ctc_loss': Array(0.45848003, dtype=float32), 'validation/wer': 0.1356382208405341, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2668064, dtype=float32), 'test/wer': 0.08908658826396929, 'test/num_examples': 2472, 'score': 30324.13405394554, 'total_duration': 33280.008269548416, 'accumulated_submission_time': 30324.13405394554, 'accumulated_eval_time': 2952.996910095215, 'accumulated_logging_time': 1.2019007205963135, 'global_step': 36769, 'preemption_count': 0}), (38527, {'train/ctc_loss': Array(0.20307493, dtype=float32), 'train/wer': 0.07222656332495618, 'validation/ctc_loss': Array(0.4376409, dtype=float32), 'validation/wer': 0.13151568398389604, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25359207, dtype=float32), 'test/wer': 0.08411025125424004, 'test/num_examples': 2472, 'score': 31764.349419355392, 'total_duration': 34853.02194070816, 'accumulated_submission_time': 31764.349419355392, 'accumulated_eval_time': 3085.658665418625, 'accumulated_logging_time': 1.2569711208343506, 'global_step': 38527, 'preemption_count': 0}), (40234, {'train/ctc_loss': Array(0.24375516, dtype=float32), 'train/wer': 0.08508214771944479, 'validation/ctc_loss': Array(0.42571378, dtype=float32), 'validation/wer': 0.1254139432499493, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24401668, dtype=float32), 'test/wer': 0.08094164483171856, 'test/num_examples': 2472, 'score': 33204.359389066696, 'total_duration': 36425.31294822693, 'accumulated_submission_time': 33204.359389066696, 'accumulated_eval_time': 3217.803034543991, 'accumulated_logging_time': 1.3148212432861328, 'global_step': 40234, 'preemption_count': 0}), (41958, {'train/ctc_loss': Array(0.246588, dtype=float32), 'train/wer': 0.08537390985749735, 'validation/ctc_loss': Array(0.41846702, dtype=float32), 'validation/wer': 0.12228583565849561, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23728706, dtype=float32), 'test/wer': 0.07706213312209291, 'test/num_examples': 2472, 'score': 34644.56383180618, 'total_duration': 37997.54720067978, 'accumulated_submission_time': 34644.56383180618, 'accumulated_eval_time': 3349.6992213726044, 'accumulated_logging_time': 1.367805004119873, 'global_step': 41958, 'preemption_count': 0}), (43699, {'train/ctc_loss': Array(0.26955163, dtype=float32), 'train/wer': 0.09448906411778814, 'validation/ctc_loss': Array(0.3980731, dtype=float32), 'validation/wer': 0.11752609169989477, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22429788, dtype=float32), 'test/wer': 0.07338573720878273, 'test/num_examples': 2472, 'score': 36084.5872592926, 'total_duration': 39567.01443386078, 'accumulated_submission_time': 36084.5872592926, 'accumulated_eval_time': 3479.0051443576813, 'accumulated_logging_time': 1.4248120784759521, 'global_step': 43699, 'preemption_count': 0}), (45408, {'train/ctc_loss': Array(0.22383668, dtype=float32), 'train/wer': 0.07831662241167155, 'validation/ctc_loss': Array(0.3803963, dtype=float32), 'validation/wer': 0.11149193353736833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21352662, dtype=float32), 'test/wer': 0.07072492027704995, 'test/num_examples': 2472, 'score': 37524.958131313324, 'total_duration': 41137.80766797066, 'accumulated_submission_time': 37524.958131313324, 'accumulated_eval_time': 3609.2803947925568, 'accumulated_logging_time': 1.4917166233062744, 'global_step': 45408, 'preemption_count': 0}), (47130, {'train/ctc_loss': Array(0.19620135, dtype=float32), 'train/wer': 0.07103648692282855, 'validation/ctc_loss': Array(0.36219963, dtype=float32), 'validation/wer': 0.10645220464002626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20107274, dtype=float32), 'test/wer': 0.06698758962484512, 'test/num_examples': 2472, 'score': 38965.105144023895, 'total_duration': 42712.27090001106, 'accumulated_submission_time': 38965.105144023895, 'accumulated_eval_time': 3743.455114841461, 'accumulated_logging_time': 1.5524523258209229, 'global_step': 47130, 'preemption_count': 0}), (48870, {'train/ctc_loss': Array(0.1566978, dtype=float32), 'train/wer': 0.05633013953387204, 'validation/ctc_loss': Array(0.3515569, dtype=float32), 'validation/wer': 0.10123869198760342, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19438645, dtype=float32), 'test/wer': 0.06288465053927243, 'test/num_examples': 2472, 'score': 40405.32123088837, 'total_duration': 44285.95129823685, 'accumulated_submission_time': 40405.32123088837, 'accumulated_eval_time': 3876.783122062683, 'accumulated_logging_time': 1.610213041305542, 'global_step': 48870, 'preemption_count': 0}), (50579, {'train/ctc_loss': Array(0.1639799, dtype=float32), 'train/wer': 0.058291288078337236, 'validation/ctc_loss': Array(0.34078166, dtype=float32), 'validation/wer': 0.09763750639620765, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18385345, dtype=float32), 'test/wer': 0.059370747263014646, 'test/num_examples': 2472, 'score': 41845.57046985626, 'total_duration': 45858.63100862503, 'accumulated_submission_time': 41845.57046985626, 'accumulated_eval_time': 4009.082178592682, 'accumulated_logging_time': 1.663142442703247, 'global_step': 50579, 'preemption_count': 0}), (52293, {'train/ctc_loss': Array(0.13913022, dtype=float32), 'train/wer': 0.050565839202886105, 'validation/ctc_loss': Array(0.32421482, dtype=float32), 'validation/wer': 0.0936501346824102, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1766318, dtype=float32), 'test/wer': 0.057400524038754494, 'test/num_examples': 2472, 'score': 43285.585735082626, 'total_duration': 47431.84371328354, 'accumulated_submission_time': 43285.585735082626, 'accumulated_eval_time': 4142.147248268127, 'accumulated_logging_time': 1.7157704830169678, 'global_step': 52293, 'preemption_count': 0}), (54038, {'train/ctc_loss': Array(0.13397767, dtype=float32), 'train/wer': 0.04963169485457642, 'validation/ctc_loss': Array(0.32009247, dtype=float32), 'validation/wer': 0.09085993994805797, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1700292, dtype=float32), 'test/wer': 0.05447565657181159, 'test/num_examples': 2472, 'score': 44725.67624735832, 'total_duration': 49005.1785197258, 'accumulated_submission_time': 44725.67624735832, 'accumulated_eval_time': 4275.249014854431, 'accumulated_logging_time': 1.775087833404541, 'global_step': 54038, 'preemption_count': 0}), (55756, {'train/ctc_loss': Array(0.12478878, dtype=float32), 'train/wer': 0.04467632850241546, 'validation/ctc_loss': Array(0.3109721, dtype=float32), 'validation/wer': 0.08901590121358989, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16544126, dtype=float32), 'test/wer': 0.05299291125870859, 'test/num_examples': 2472, 'score': 46166.253903865814, 'total_duration': 50577.9007794857, 'accumulated_submission_time': 46166.253903865814, 'accumulated_eval_time': 4407.253207921982, 'accumulated_logging_time': 1.8347620964050293, 'global_step': 55756, 'preemption_count': 0}), (57459, {'train/ctc_loss': Array(0.12825522, dtype=float32), 'train/wer': 0.04683332761008665, 'validation/ctc_loss': Array(0.3062976, dtype=float32), 'validation/wer': 0.08708497060158144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16248833, dtype=float32), 'test/wer': 0.051632035423394874, 'test/num_examples': 2472, 'score': 47606.86358118057, 'total_duration': 52149.0624089241, 'accumulated_submission_time': 47606.86358118057, 'accumulated_eval_time': 4537.672585248947, 'accumulated_logging_time': 1.887894868850708, 'global_step': 57459, 'preemption_count': 0}), (59189, {'train/ctc_loss': Array(0.13686039, dtype=float32), 'train/wer': 0.04832333177229114, 'validation/ctc_loss': Array(0.30417097, dtype=float32), 'validation/wer': 0.0868242949689603, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16086234, dtype=float32), 'test/wer': 0.05116486909186928, 'test/num_examples': 2472, 'score': 49046.91881299019, 'total_duration': 53720.95758342743, 'accumulated_submission_time': 49046.91881299019, 'accumulated_eval_time': 4669.370657205582, 'accumulated_logging_time': 1.9503746032714844, 'global_step': 59189, 'preemption_count': 0}), (60000, {'train/ctc_loss': Array(0.11757164, dtype=float32), 'train/wer': 0.04299213127984397, 'validation/ctc_loss': Array(0.30437273, dtype=float32), 'validation/wer': 0.08692084149956071, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16084751, dtype=float32), 'test/wer': 0.05094144171592225, 'test/num_examples': 2472, 'score': 49712.73689985275, 'total_duration': 54517.20010280609, 'accumulated_submission_time': 49712.73689985275, 'accumulated_eval_time': 4799.699951648712, 'accumulated_logging_time': 2.0042941570281982, 'global_step': 60000, 'preemption_count': 0})], 'global_step': 60000}
I1012 13:21:53.418754 139633106130752 submission_runner.py:566] Timing: 49712.73689985275
I1012 13:21:53.418809 139633106130752 submission_runner.py:568] Total number of evals: 36
I1012 13:21:53.418853 139633106130752 submission_runner.py:569] ====================
I1012 13:21:53.423334 139633106130752 submission_runner.py:645] Final librispeech_conformer score: 49712.73689985275
