python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_conformer/adamw_run5 --overwrite=true --save_checkpoints=false --max_global_steps=60000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_10-20-2023-21-53-57.log
I1020 21:54:19.660449 140399020136256 logger_utils.py:61] Removing existing experiment directory /experiment_runs/targets_check_conformer/adamw_run5/librispeech_conformer_jax because --overwrite was set.
I1020 21:54:19.674723 140399020136256 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_conformer/adamw_run5/librispeech_conformer_jax.
I1020 21:54:20.774283 140399020136256 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1020 21:54:20.775111 140399020136256 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1020 21:54:20.775235 140399020136256 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1020 21:54:20.787121 140399020136256 submission_runner.py:525] Using RNG seed 982224644
I1020 21:54:26.526776 140399020136256 submission_runner.py:534] --- Tuning run 1/1 ---
I1020 21:54:26.526977 140399020136256 submission_runner.py:539] Creating tuning directory at /experiment_runs/targets_check_conformer/adamw_run5/librispeech_conformer_jax/trial_1.
I1020 21:54:26.527157 140399020136256 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_conformer/adamw_run5/librispeech_conformer_jax/trial_1/hparams.json.
I1020 21:54:26.709936 140399020136256 submission_runner.py:202] Initializing dataset.
I1020 21:54:26.710145 140399020136256 submission_runner.py:209] Initializing model.
I1020 21:54:31.627047 140399020136256 submission_runner.py:243] Initializing optimizer.
I1020 21:54:32.935917 140399020136256 submission_runner.py:250] Initializing metrics bundle.
I1020 21:54:32.936112 140399020136256 submission_runner.py:268] Initializing checkpoint and logger.
I1020 21:54:32.937296 140399020136256 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_conformer/adamw_run5/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I1020 21:54:32.937428 140399020136256 submission_runner.py:288] Saving meta data to /experiment_runs/targets_check_conformer/adamw_run5/librispeech_conformer_jax/trial_1/meta_data_0.json.
I1020 21:54:32.937615 140399020136256 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1020 21:54:32.937674 140399020136256 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1020 21:54:33.257431 140399020136256 logger_utils.py:220] Unable to record git information. Continuing without it.
I1020 21:54:33.551594 140399020136256 submission_runner.py:291] Saving flags to /experiment_runs/targets_check_conformer/adamw_run5/librispeech_conformer_jax/trial_1/flags_0.json.
I1020 21:54:33.567516 140399020136256 submission_runner.py:301] Starting training loop.
I1020 21:54:33.863035 140399020136256 input_pipeline.py:20] Loading split = train-clean-100
I1020 21:54:33.900996 140399020136256 input_pipeline.py:20] Loading split = train-clean-360
I1020 21:54:34.297228 140399020136256 input_pipeline.py:20] Loading split = train-other-500
2023-10-20 21:55:46.034612: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-20 21:55:48.903859: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1020 21:55:50.843609 140224726292224 logging_writer.py:48] [0] global_step=0, grad_norm=71.84624481201172, loss=31.529983520507812
I1020 21:55:50.882540 140399020136256 spec.py:321] Evaluating on the training split.
I1020 21:55:51.052944 140399020136256 input_pipeline.py:20] Loading split = train-clean-100
I1020 21:55:51.087779 140399020136256 input_pipeline.py:20] Loading split = train-clean-360
I1020 21:55:51.486916 140399020136256 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1020 21:57:13.222102 140399020136256 spec.py:333] Evaluating on the validation split.
I1020 21:57:13.339404 140399020136256 input_pipeline.py:20] Loading split = dev-clean
I1020 21:57:13.344868 140399020136256 input_pipeline.py:20] Loading split = dev-other
I1020 21:58:21.915773 140399020136256 spec.py:349] Evaluating on the test split.
I1020 21:58:22.036206 140399020136256 input_pipeline.py:20] Loading split = test-clean
I1020 21:59:01.410444 140399020136256 submission_runner.py:395] Time since start: 267.84s, 	Step: 1, 	{'train/ctc_loss': Array(31.07877, dtype=float32), 'train/wer': 1.5492940882493804, 'validation/ctc_loss': Array(30.239573, dtype=float32), 'validation/wer': 1.4522046400262607, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.312384, dtype=float32), 'test/wer': 1.4729348201409624, 'test/num_examples': 2472, 'score': 77.31493139266968, 'total_duration': 267.84015250205994, 'accumulated_submission_time': 77.31493139266968, 'accumulated_eval_time': 190.5251533985138, 'accumulated_logging_time': 0}
I1020 21:59:01.441167 140218468398848 logging_writer.py:48] [1] accumulated_eval_time=190.525153, accumulated_logging_time=0, accumulated_submission_time=77.314931, global_step=1, preemption_count=0, score=77.314931, test/ctc_loss=30.3123836517334, test/num_examples=2472, test/wer=1.472935, total_duration=267.840153, train/ctc_loss=31.07876968383789, train/wer=1.549294, validation/ctc_loss=30.239572525024414, validation/num_examples=5348, validation/wer=1.452205
I1020 21:59:24.351163 140226398459648 logging_writer.py:48] [1] global_step=1, grad_norm=73.78504180908203, loss=31.86408233642578
I1020 21:59:25.237113 140226406852352 logging_writer.py:48] [2] global_step=2, grad_norm=94.41590881347656, loss=31.22381019592285
I1020 21:59:26.110233 140226398459648 logging_writer.py:48] [3] global_step=3, grad_norm=134.10687255859375, loss=29.91098976135254
I1020 21:59:26.990523 140226406852352 logging_writer.py:48] [4] global_step=4, grad_norm=176.40365600585938, loss=29.028562545776367
I1020 21:59:27.872879 140226398459648 logging_writer.py:48] [5] global_step=5, grad_norm=185.6875762939453, loss=26.166728973388672
I1020 21:59:28.754050 140226406852352 logging_writer.py:48] [6] global_step=6, grad_norm=181.40235900878906, loss=22.69148826599121
I1020 21:59:29.633817 140226398459648 logging_writer.py:48] [7] global_step=7, grad_norm=175.9065399169922, loss=19.137676239013672
I1020 21:59:30.521562 140226406852352 logging_writer.py:48] [8] global_step=8, grad_norm=158.17449951171875, loss=15.343647956848145
I1020 21:59:31.400816 140226398459648 logging_writer.py:48] [9] global_step=9, grad_norm=117.66798400878906, loss=11.426590919494629
I1020 21:59:32.282012 140226406852352 logging_writer.py:48] [10] global_step=10, grad_norm=68.80738067626953, loss=8.756927490234375
I1020 21:59:33.160452 140226398459648 logging_writer.py:48] [11] global_step=11, grad_norm=24.54909324645996, loss=7.503876686096191
I1020 21:59:34.049709 140226406852352 logging_writer.py:48] [12] global_step=12, grad_norm=5.003727912902832, loss=7.2160725593566895
I1020 21:59:34.940349 140226398459648 logging_writer.py:48] [13] global_step=13, grad_norm=16.05533218383789, loss=7.413475036621094
I1020 21:59:35.824973 140226406852352 logging_writer.py:48] [14] global_step=14, grad_norm=20.679536819458008, loss=7.7010040283203125
I1020 21:59:36.700403 140226398459648 logging_writer.py:48] [15] global_step=15, grad_norm=22.873929977416992, loss=7.941904067993164
I1020 21:59:37.586783 140226406852352 logging_writer.py:48] [16] global_step=16, grad_norm=23.559154510498047, loss=8.053143501281738
I1020 21:59:38.480391 140226398459648 logging_writer.py:48] [17] global_step=17, grad_norm=23.481931686401367, loss=8.0208158493042
I1020 21:59:39.363110 140226406852352 logging_writer.py:48] [18] global_step=18, grad_norm=23.0540828704834, loss=7.909962177276611
I1020 21:59:40.246495 140226398459648 logging_writer.py:48] [19] global_step=19, grad_norm=21.370695114135742, loss=7.690743446350098
I1020 21:59:41.137573 140226406852352 logging_writer.py:48] [20] global_step=20, grad_norm=17.7219295501709, loss=7.399179458618164
I1020 21:59:42.026277 140226398459648 logging_writer.py:48] [21] global_step=21, grad_norm=12.126354217529297, loss=7.145046234130859
I1020 21:59:42.907334 140226406852352 logging_writer.py:48] [22] global_step=22, grad_norm=3.0874392986297607, loss=6.9943060874938965
I1020 21:59:43.788355 140226398459648 logging_writer.py:48] [23] global_step=23, grad_norm=12.779703140258789, loss=7.074086666107178
I1020 21:59:44.675503 140226406852352 logging_writer.py:48] [24] global_step=24, grad_norm=22.111154556274414, loss=7.192602634429932
I1020 21:59:45.559102 140226398459648 logging_writer.py:48] [25] global_step=25, grad_norm=22.73445701599121, loss=7.1922736167907715
I1020 21:59:46.433421 140226406852352 logging_writer.py:48] [26] global_step=26, grad_norm=15.257509231567383, loss=7.021297454833984
I1020 21:59:47.312110 140226398459648 logging_writer.py:48] [27] global_step=27, grad_norm=6.060824871063232, loss=6.8638081550598145
I1020 21:59:48.199709 140226406852352 logging_writer.py:48] [28] global_step=28, grad_norm=3.532470226287842, loss=6.827486515045166
I1020 21:59:49.077366 140226398459648 logging_writer.py:48] [29] global_step=29, grad_norm=8.141239166259766, loss=6.851689338684082
I1020 21:59:49.958387 140226406852352 logging_writer.py:48] [30] global_step=30, grad_norm=9.972911834716797, loss=6.819788932800293
I1020 21:59:50.838251 140226398459648 logging_writer.py:48] [31] global_step=31, grad_norm=9.60912036895752, loss=6.796028137207031
I1020 21:59:51.730354 140226406852352 logging_writer.py:48] [32] global_step=32, grad_norm=6.382361888885498, loss=6.713861465454102
I1020 21:59:52.611409 140226398459648 logging_writer.py:48] [33] global_step=33, grad_norm=1.9727489948272705, loss=6.6286797523498535
I1020 21:59:53.495385 140226406852352 logging_writer.py:48] [34] global_step=34, grad_norm=5.381663799285889, loss=6.600923538208008
I1020 21:59:54.377866 140226398459648 logging_writer.py:48] [35] global_step=35, grad_norm=8.426249504089355, loss=6.615165710449219
I1020 21:59:55.264262 140226406852352 logging_writer.py:48] [36] global_step=36, grad_norm=7.529530048370361, loss=6.55013370513916
I1020 21:59:56.148888 140226398459648 logging_writer.py:48] [37] global_step=37, grad_norm=3.8909170627593994, loss=6.495054244995117
I1020 21:59:57.034813 140226406852352 logging_writer.py:48] [38] global_step=38, grad_norm=1.46586012840271, loss=6.476155757904053
I1020 21:59:57.918042 140226398459648 logging_writer.py:48] [39] global_step=39, grad_norm=4.3190226554870605, loss=6.449679374694824
I1020 21:59:58.800765 140226406852352 logging_writer.py:48] [40] global_step=40, grad_norm=4.908133029937744, loss=6.414793491363525
I1020 21:59:59.674499 140226398459648 logging_writer.py:48] [41] global_step=41, grad_norm=2.4426941871643066, loss=6.366010665893555
I1020 22:00:00.555745 140226406852352 logging_writer.py:48] [42] global_step=42, grad_norm=3.3701066970825195, loss=6.361940860748291
I1020 22:00:01.441155 140226398459648 logging_writer.py:48] [43] global_step=43, grad_norm=4.482438087463379, loss=6.311036586761475
I1020 22:00:02.343481 140226406852352 logging_writer.py:48] [44] global_step=44, grad_norm=3.295436382293701, loss=6.273017406463623
I1020 22:00:03.229527 140226398459648 logging_writer.py:48] [45] global_step=45, grad_norm=1.1615349054336548, loss=6.253442764282227
I1020 22:00:04.116693 140226406852352 logging_writer.py:48] [46] global_step=46, grad_norm=3.259450674057007, loss=6.215806007385254
I1020 22:00:05.010031 140226398459648 logging_writer.py:48] [47] global_step=47, grad_norm=2.7366387844085693, loss=6.182610034942627
I1020 22:00:05.901752 140226406852352 logging_writer.py:48] [48] global_step=48, grad_norm=3.1729652881622314, loss=6.188877105712891
I1020 22:00:06.786561 140226398459648 logging_writer.py:48] [49] global_step=49, grad_norm=3.3624074459075928, loss=6.178645133972168
I1020 22:00:07.680940 140226406852352 logging_writer.py:48] [50] global_step=50, grad_norm=0.8776386976242065, loss=6.1299729347229
I1020 22:00:08.574362 140226398459648 logging_writer.py:48] [51] global_step=51, grad_norm=2.7302091121673584, loss=6.091001510620117
I1020 22:00:09.469321 140226406852352 logging_writer.py:48] [52] global_step=52, grad_norm=1.2544547319412231, loss=6.068541049957275
I1020 22:00:10.357886 140226398459648 logging_writer.py:48] [53] global_step=53, grad_norm=4.905755519866943, loss=6.086434841156006
I1020 22:00:11.243295 140226406852352 logging_writer.py:48] [54] global_step=54, grad_norm=1.0672967433929443, loss=6.058707237243652
I1020 22:00:12.136317 140226398459648 logging_writer.py:48] [55] global_step=55, grad_norm=2.946794271469116, loss=6.055575370788574
I1020 22:00:13.036091 140226406852352 logging_writer.py:48] [56] global_step=56, grad_norm=1.9752919673919678, loss=6.01290225982666
I1020 22:00:13.925362 140226398459648 logging_writer.py:48] [57] global_step=57, grad_norm=1.1416820287704468, loss=5.986503601074219
I1020 22:00:14.806192 140226406852352 logging_writer.py:48] [58] global_step=58, grad_norm=0.6217353343963623, loss=5.975804805755615
I1020 22:00:15.698483 140226398459648 logging_writer.py:48] [59] global_step=59, grad_norm=2.1426615715026855, loss=5.959508419036865
I1020 22:00:16.597018 140226406852352 logging_writer.py:48] [60] global_step=60, grad_norm=2.964550733566284, loss=5.9468584060668945
I1020 22:00:17.492309 140226398459648 logging_writer.py:48] [61] global_step=61, grad_norm=1.0333735942840576, loss=5.927186489105225
I1020 22:00:18.380866 140226406852352 logging_writer.py:48] [62] global_step=62, grad_norm=1.596503496170044, loss=5.918265342712402
I1020 22:00:19.271431 140226398459648 logging_writer.py:48] [63] global_step=63, grad_norm=2.88753604888916, loss=5.901729583740234
I1020 22:00:20.167226 140226406852352 logging_writer.py:48] [64] global_step=64, grad_norm=3.2149593830108643, loss=5.887948036193848
I1020 22:00:21.052552 140226398459648 logging_writer.py:48] [65] global_step=65, grad_norm=4.097466945648193, loss=5.886015892028809
I1020 22:00:21.944067 140226406852352 logging_writer.py:48] [66] global_step=66, grad_norm=4.062376499176025, loss=5.869814395904541
I1020 22:00:22.835197 140226398459648 logging_writer.py:48] [67] global_step=67, grad_norm=1.3733464479446411, loss=5.868885517120361
I1020 22:00:23.723822 140226406852352 logging_writer.py:48] [68] global_step=68, grad_norm=3.470175266265869, loss=5.8858747482299805
I1020 22:00:24.618841 140226398459648 logging_writer.py:48] [69] global_step=69, grad_norm=7.958998680114746, loss=5.90898323059082
I1020 22:00:25.509819 140226406852352 logging_writer.py:48] [70] global_step=70, grad_norm=8.238920211791992, loss=5.897810935974121
I1020 22:00:26.393853 140226398459648 logging_writer.py:48] [71] global_step=71, grad_norm=2.940711259841919, loss=5.861023902893066
I1020 22:00:27.285227 140226406852352 logging_writer.py:48] [72] global_step=72, grad_norm=4.02305793762207, loss=5.871519565582275
I1020 22:00:28.181923 140226398459648 logging_writer.py:48] [73] global_step=73, grad_norm=7.46594762802124, loss=5.90688419342041
I1020 22:00:29.075374 140226406852352 logging_writer.py:48] [74] global_step=74, grad_norm=4.06434440612793, loss=5.850093364715576
I1020 22:00:29.965474 140226398459648 logging_writer.py:48] [75] global_step=75, grad_norm=3.007357597351074, loss=5.846977710723877
I1020 22:00:30.861624 140226406852352 logging_writer.py:48] [76] global_step=76, grad_norm=6.31856107711792, loss=5.866827011108398
I1020 22:00:31.746072 140226398459648 logging_writer.py:48] [77] global_step=77, grad_norm=2.803697109222412, loss=5.836232662200928
I1020 22:00:32.634655 140226406852352 logging_writer.py:48] [78] global_step=78, grad_norm=2.536585569381714, loss=5.847885608673096
I1020 22:00:33.517314 140226398459648 logging_writer.py:48] [79] global_step=79, grad_norm=4.012081623077393, loss=5.828763008117676
I1020 22:00:34.404981 140226406852352 logging_writer.py:48] [80] global_step=80, grad_norm=1.692917823791504, loss=5.808587551116943
I1020 22:00:35.288899 140226398459648 logging_writer.py:48] [81] global_step=81, grad_norm=1.3384417295455933, loss=5.810763359069824
I1020 22:00:36.183588 140226406852352 logging_writer.py:48] [82] global_step=82, grad_norm=3.6438000202178955, loss=5.875662803649902
I1020 22:00:37.072649 140226398459648 logging_writer.py:48] [83] global_step=83, grad_norm=4.638082027435303, loss=5.839560031890869
I1020 22:00:37.960596 140226406852352 logging_writer.py:48] [84] global_step=84, grad_norm=3.559051990509033, loss=5.827897071838379
I1020 22:00:38.850081 140226398459648 logging_writer.py:48] [85] global_step=85, grad_norm=1.5931456089019775, loss=5.826920032501221
I1020 22:00:39.732949 140226406852352 logging_writer.py:48] [86] global_step=86, grad_norm=0.3947523236274719, loss=5.812983989715576
I1020 22:00:40.619676 140226398459648 logging_writer.py:48] [87] global_step=87, grad_norm=1.4231055974960327, loss=5.826526165008545
I1020 22:00:41.511917 140226406852352 logging_writer.py:48] [88] global_step=88, grad_norm=1.7264891862869263, loss=5.836958885192871
I1020 22:00:42.405854 140226398459648 logging_writer.py:48] [89] global_step=89, grad_norm=1.4327964782714844, loss=5.82724142074585
I1020 22:00:43.294659 140226406852352 logging_writer.py:48] [90] global_step=90, grad_norm=1.5388844013214111, loss=5.815577030181885
I1020 22:00:44.184409 140226398459648 logging_writer.py:48] [91] global_step=91, grad_norm=3.137878894805908, loss=5.823070526123047
I1020 22:00:45.070851 140226406852352 logging_writer.py:48] [92] global_step=92, grad_norm=5.45389986038208, loss=5.860340595245361
I1020 22:00:45.958175 140226398459648 logging_writer.py:48] [93] global_step=93, grad_norm=8.16844367980957, loss=5.897027015686035
I1020 22:00:46.839261 140226406852352 logging_writer.py:48] [94] global_step=94, grad_norm=7.987013816833496, loss=5.90332555770874
I1020 22:00:47.726760 140226398459648 logging_writer.py:48] [95] global_step=95, grad_norm=0.9151608347892761, loss=5.806933403015137
I1020 22:00:48.612725 140226406852352 logging_writer.py:48] [96] global_step=96, grad_norm=7.255191802978516, loss=5.858826160430908
I1020 22:00:49.502660 140226398459648 logging_writer.py:48] [97] global_step=97, grad_norm=6.007017135620117, loss=5.868372917175293
I1020 22:00:50.387179 140226406852352 logging_writer.py:48] [98] global_step=98, grad_norm=3.124572277069092, loss=5.835827350616455
I1020 22:00:51.280331 140226398459648 logging_writer.py:48] [99] global_step=99, grad_norm=8.419445037841797, loss=5.8598856925964355
I1020 22:00:52.170092 140226406852352 logging_writer.py:48] [100] global_step=100, grad_norm=3.207230806350708, loss=5.823116302490234
I1020 22:05:56.917455 140226398459648 logging_writer.py:48] [500] global_step=500, grad_norm=1.4658775329589844, loss=5.496442794799805
I1020 22:13:00.785398 140226406852352 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5768547058105469, loss=3.093064308166504
I1020 22:19:25.391721 140226473993984 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8365982174873352, loss=2.4600625038146973
I1020 22:23:01.419408 140399020136256 spec.py:321] Evaluating on the training split.
I1020 22:23:53.016343 140399020136256 spec.py:333] Evaluating on the validation split.
I1020 22:24:42.467119 140399020136256 spec.py:349] Evaluating on the test split.
I1020 22:25:08.035658 140399020136256 submission_runner.py:395] Time since start: 1834.46s, 	Step: 1757, 	{'train/ctc_loss': Array(2.489746, dtype=float32), 'train/wer': 0.5370126258985397, 'validation/ctc_loss': Array(3.0186522, dtype=float32), 'validation/wer': 0.5845506241733204, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.6696236, dtype=float32), 'test/wer': 0.5376475128470741, 'test/num_examples': 2472, 'score': 1517.2095663547516, 'total_duration': 1834.4627315998077, 'accumulated_submission_time': 1517.2095663547516, 'accumulated_eval_time': 317.1360650062561, 'accumulated_logging_time': 0.04747796058654785}
I1020 22:25:08.079685 140226473993984 logging_writer.py:48] [1757] accumulated_eval_time=317.136065, accumulated_logging_time=0.047478, accumulated_submission_time=1517.209566, global_step=1757, preemption_count=0, score=1517.209566, test/ctc_loss=2.669623613357544, test/num_examples=2472, test/wer=0.537648, total_duration=1834.462732, train/ctc_loss=2.48974609375, train/wer=0.537013, validation/ctc_loss=3.0186522006988525, validation/num_examples=5348, validation/wer=0.584551
I1020 22:28:13.316993 140226465601280 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6127709150314331, loss=2.17370867729187
I1020 22:34:36.584804 140226473993984 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7130406498908997, loss=2.084869861602783
I1020 22:41:43.194628 140226465601280 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6639528870582581, loss=1.981087327003479
I1020 22:48:16.803435 140226146313984 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8467963933944702, loss=1.928958773612976
I1020 22:49:08.354310 140399020136256 spec.py:321] Evaluating on the training split.
I1020 22:50:01.896617 140399020136256 spec.py:333] Evaluating on the validation split.
I1020 22:50:52.619328 140399020136256 spec.py:349] Evaluating on the test split.
I1020 22:51:18.895143 140399020136256 submission_runner.py:395] Time since start: 3405.32s, 	Step: 3565, 	{'train/ctc_loss': Array(0.7203328, dtype=float32), 'train/wer': 0.24164824334762805, 'validation/ctc_loss': Array(1.048355, dtype=float32), 'validation/wer': 0.3036678026975101, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.75977933, dtype=float32), 'test/wer': 0.24396238295452238, 'test/num_examples': 2472, 'score': 2957.3968431949615, 'total_duration': 3405.3215222358704, 'accumulated_submission_time': 2957.3968431949615, 'accumulated_eval_time': 447.67084193229675, 'accumulated_logging_time': 0.10671305656433105}
I1020 22:51:18.929168 140225854473984 logging_writer.py:48] [3565] accumulated_eval_time=447.670842, accumulated_logging_time=0.106713, accumulated_submission_time=2957.396843, global_step=3565, preemption_count=0, score=2957.396843, test/ctc_loss=0.7597793340682983, test/num_examples=2472, test/wer=0.243962, total_duration=3405.321522, train/ctc_loss=0.7203328013420105, train/wer=0.241648, validation/ctc_loss=1.048354983329773, validation/num_examples=5348, validation/wer=0.303668
I1020 22:57:03.669445 140225846081280 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6342659592628479, loss=1.8888849020004272
I1020 23:03:40.026194 140225854473984 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6257047653198242, loss=1.8489501476287842
I1020 23:10:46.610001 140225846081280 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5912865996360779, loss=1.8165711164474487
I1020 23:15:19.597286 140399020136256 spec.py:321] Evaluating on the training split.
I1020 23:16:13.507842 140399020136256 spec.py:333] Evaluating on the validation split.
I1020 23:17:05.636270 140399020136256 spec.py:349] Evaluating on the test split.
I1020 23:17:31.677668 140399020136256 submission_runner.py:395] Time since start: 4978.10s, 	Step: 5338, 	{'train/ctc_loss': Array(0.5150274, dtype=float32), 'train/wer': 0.1824617444428631, 'validation/ctc_loss': Array(0.85034025, dtype=float32), 'validation/wer': 0.2525367600915261, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58531016, dtype=float32), 'test/wer': 0.1946255560294924, 'test/num_examples': 2472, 'score': 4397.979216575623, 'total_duration': 4978.104779958725, 'accumulated_submission_time': 4397.979216575623, 'accumulated_eval_time': 579.7461776733398, 'accumulated_logging_time': 0.1551055908203125}
I1020 23:17:31.711403 140225854473984 logging_writer.py:48] [5338] accumulated_eval_time=579.746178, accumulated_logging_time=0.155106, accumulated_submission_time=4397.979217, global_step=5338, preemption_count=0, score=4397.979217, test/ctc_loss=0.585310161113739, test/num_examples=2472, test/wer=0.194626, total_duration=4978.104780, train/ctc_loss=0.5150274038314819, train/wer=0.182462, validation/ctc_loss=0.8503402471542358, validation/num_examples=5348, validation/wer=0.252537
I1020 23:19:35.425861 140225846081280 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6946902275085449, loss=1.7618540525436401
I1020 23:26:29.743306 140225854473984 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5686393976211548, loss=1.816026210784912
I1020 23:33:13.934307 140225854473984 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.688233494758606, loss=1.7637722492218018
I1020 23:40:15.280544 140225846081280 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8527147173881531, loss=1.7360646724700928
I1020 23:41:32.364479 140399020136256 spec.py:321] Evaluating on the training split.
I1020 23:42:27.293033 140399020136256 spec.py:333] Evaluating on the validation split.
I1020 23:43:19.496483 140399020136256 spec.py:349] Evaluating on the test split.
I1020 23:43:45.604477 140399020136256 submission_runner.py:395] Time since start: 6552.03s, 	Step: 7091, 	{'train/ctc_loss': Array(0.48449057, dtype=float32), 'train/wer': 0.17009812648577324, 'validation/ctc_loss': Array(0.7889404, dtype=float32), 'validation/wer': 0.2386533689911853, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.53206474, dtype=float32), 'test/wer': 0.178782523916885, 'test/num_examples': 2472, 'score': 5838.546237945557, 'total_duration': 6552.030452489853, 'accumulated_submission_time': 5838.546237945557, 'accumulated_eval_time': 712.9797339439392, 'accumulated_logging_time': 0.20348572731018066}
I1020 23:43:45.646291 140226473993984 logging_writer.py:48] [7091] accumulated_eval_time=712.979734, accumulated_logging_time=0.203486, accumulated_submission_time=5838.546238, global_step=7091, preemption_count=0, score=5838.546238, test/ctc_loss=0.5320647358894348, test/num_examples=2472, test/wer=0.178783, total_duration=6552.030452, train/ctc_loss=0.4844905734062195, train/wer=0.170098, validation/ctc_loss=0.7889404296875, validation/num_examples=5348, validation/wer=0.238653
I1020 23:48:59.763000 140223744820992 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.651409924030304, loss=1.7399038076400757
I1020 23:56:00.599151 140219206596352 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.0799942016601562, loss=1.8039811849594116
I1021 00:02:49.911349 140223744820992 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7283303141593933, loss=1.722182273864746
I1021 00:07:45.944104 140399020136256 spec.py:321] Evaluating on the training split.
I1021 00:08:39.387863 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 00:09:30.889786 140399020136256 spec.py:349] Evaluating on the test split.
I1021 00:09:57.253738 140399020136256 submission_runner.py:395] Time since start: 8123.68s, 	Step: 8862, 	{'train/ctc_loss': Array(0.46976593, dtype=float32), 'train/wer': 0.1622670638630977, 'validation/ctc_loss': Array(0.764583, dtype=float32), 'validation/wer': 0.22766637380885718, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5057142, dtype=float32), 'test/wer': 0.1684439298844271, 'test/num_examples': 2472, 'score': 7278.755240917206, 'total_duration': 8123.680272817612, 'accumulated_submission_time': 7278.755240917206, 'accumulated_eval_time': 844.2834892272949, 'accumulated_logging_time': 0.2622532844543457}
I1021 00:09:57.294050 140223744820992 logging_writer.py:48] [8862] accumulated_eval_time=844.283489, accumulated_logging_time=0.262253, accumulated_submission_time=7278.755241, global_step=8862, preemption_count=0, score=7278.755241, test/ctc_loss=0.5057141780853271, test/num_examples=2472, test/wer=0.168444, total_duration=8123.680273, train/ctc_loss=0.46976593136787415, train/wer=0.162267, validation/ctc_loss=0.7645829916000366, validation/num_examples=5348, validation/wer=0.227666
I1021 00:11:42.694425 140219206596352 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6379053592681885, loss=1.7322001457214355
I1021 00:18:24.354880 140226473993984 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5789690613746643, loss=1.652205228805542
I1021 00:25:24.279397 140226465601280 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8031253218650818, loss=1.7441726922988892
I1021 00:32:21.587140 140226473993984 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6266804933547974, loss=1.7135456800460815
I1021 00:33:57.848603 140399020136256 spec.py:321] Evaluating on the training split.
I1021 00:34:52.039509 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 00:35:43.997810 140399020136256 spec.py:349] Evaluating on the test split.
I1021 00:36:10.293270 140399020136256 submission_runner.py:395] Time since start: 9696.72s, 	Step: 10624, 	{'train/ctc_loss': Array(0.47079915, dtype=float32), 'train/wer': 0.16155155213895897, 'validation/ctc_loss': Array(0.7436257, dtype=float32), 'validation/wer': 0.21987506878940305, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48284253, dtype=float32), 'test/wer': 0.1600552474965978, 'test/num_examples': 2472, 'score': 8719.216614961624, 'total_duration': 9696.71895456314, 'accumulated_submission_time': 8719.216614961624, 'accumulated_eval_time': 976.7214727401733, 'accumulated_logging_time': 0.3232543468475342}
I1021 00:36:10.329227 140226473993984 logging_writer.py:48] [10624] accumulated_eval_time=976.721473, accumulated_logging_time=0.323254, accumulated_submission_time=8719.216615, global_step=10624, preemption_count=0, score=8719.216615, test/ctc_loss=0.4828425347805023, test/num_examples=2472, test/wer=0.160055, total_duration=9696.718955, train/ctc_loss=0.47079914808273315, train/wer=0.161552, validation/ctc_loss=0.7436257004737854, validation/num_examples=5348, validation/wer=0.219875
I1021 00:41:14.676146 140226465601280 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.1169941425323486, loss=1.7265613079071045
I1021 00:48:18.704440 140226146313984 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7249773740768433, loss=1.654162049293518
I1021 00:55:18.689437 140226137921280 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7297778129577637, loss=1.6460809707641602
I1021 01:00:10.825524 140399020136256 spec.py:321] Evaluating on the training split.
I1021 01:01:05.234727 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 01:01:56.493265 140399020136256 spec.py:349] Evaluating on the test split.
I1021 01:02:23.039135 140399020136256 submission_runner.py:395] Time since start: 11269.46s, 	Step: 12332, 	{'train/ctc_loss': Array(0.4325913, dtype=float32), 'train/wer': 0.15166549677748709, 'validation/ctc_loss': Array(0.7184128, dtype=float32), 'validation/wer': 0.2144781177288394, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46839824, dtype=float32), 'test/wer': 0.15605386630918286, 'test/num_examples': 2472, 'score': 10159.629392623901, 'total_duration': 11269.464804649353, 'accumulated_submission_time': 10159.629392623901, 'accumulated_eval_time': 1108.9283285140991, 'accumulated_logging_time': 0.37485384941101074}
I1021 01:02:23.076651 140226146313984 logging_writer.py:48] [12332] accumulated_eval_time=1108.928329, accumulated_logging_time=0.374854, accumulated_submission_time=10159.629393, global_step=12332, preemption_count=0, score=10159.629393, test/ctc_loss=0.46839824318885803, test/num_examples=2472, test/wer=0.156054, total_duration=11269.464805, train/ctc_loss=0.4325912892818451, train/wer=0.151665, validation/ctc_loss=0.7184128165245056, validation/num_examples=5348, validation/wer=0.214478
I1021 01:04:35.001179 140223744820992 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6394162178039551, loss=1.6857075691223145
I1021 01:11:36.174459 140219206596352 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6042983531951904, loss=1.6849766969680786
I1021 01:18:53.222176 140226473993984 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7877231240272522, loss=1.6232645511627197
I1021 01:25:45.002670 140226465601280 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7292338013648987, loss=1.6325790882110596
I1021 01:26:23.832109 140399020136256 spec.py:321] Evaluating on the training split.
I1021 01:27:18.805459 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 01:28:11.049556 140399020136256 spec.py:349] Evaluating on the test split.
I1021 01:28:37.542272 140399020136256 submission_runner.py:395] Time since start: 12843.97s, 	Step: 14045, 	{'train/ctc_loss': Array(0.39367908, dtype=float32), 'train/wer': 0.14158454457994227, 'validation/ctc_loss': Array(0.7046182, dtype=float32), 'validation/wer': 0.21032661691302124, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46340838, dtype=float32), 'test/wer': 0.15390083886823877, 'test/num_examples': 2472, 'score': 11600.30050110817, 'total_duration': 12843.967746257782, 'accumulated_submission_time': 11600.30050110817, 'accumulated_eval_time': 1242.6315286159515, 'accumulated_logging_time': 0.4273555278778076}
I1021 01:28:37.576183 140226473993984 logging_writer.py:48] [14045] accumulated_eval_time=1242.631529, accumulated_logging_time=0.427356, accumulated_submission_time=11600.300501, global_step=14045, preemption_count=0, score=11600.300501, test/ctc_loss=0.46340838074684143, test/num_examples=2472, test/wer=0.153901, total_duration=12843.967746, train/ctc_loss=0.39367908239364624, train/wer=0.141585, validation/ctc_loss=0.7046182155609131, validation/num_examples=5348, validation/wer=0.210327
I1021 01:34:43.769661 140226146313984 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6956730484962463, loss=1.6374118328094482
I1021 01:41:31.064983 140226137921280 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9483534693717957, loss=1.626509428024292
I1021 01:48:52.531345 140226146313984 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6872801184654236, loss=1.641912817955017
I1021 01:52:37.755939 140399020136256 spec.py:321] Evaluating on the training split.
I1021 01:53:32.435083 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 01:54:25.134627 140399020136256 spec.py:349] Evaluating on the test split.
I1021 01:54:51.736928 140399020136256 submission_runner.py:395] Time since start: 14418.16s, 	Step: 15796, 	{'train/ctc_loss': Array(0.37315938, dtype=float32), 'train/wer': 0.1305694024014598, 'validation/ctc_loss': Array(0.6939991, dtype=float32), 'validation/wer': 0.2064840649951244, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43945244, dtype=float32), 'test/wer': 0.14642617756382914, 'test/num_examples': 2472, 'score': 13040.39570426941, 'total_duration': 14418.164101362228, 'accumulated_submission_time': 13040.39570426941, 'accumulated_eval_time': 1376.607299566269, 'accumulated_logging_time': 0.475902795791626}
I1021 01:54:51.770190 140225854473984 logging_writer.py:48] [15796] accumulated_eval_time=1376.607300, accumulated_logging_time=0.475903, accumulated_submission_time=13040.395704, global_step=15796, preemption_count=0, score=13040.395704, test/ctc_loss=0.4394524395465851, test/num_examples=2472, test/wer=0.146426, total_duration=14418.164101, train/ctc_loss=0.37315937876701355, train/wer=0.130569, validation/ctc_loss=0.6939991116523743, validation/num_examples=5348, validation/wer=0.206484
I1021 01:57:27.159215 140225846081280 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7769522666931152, loss=1.6482183933258057
I1021 02:04:48.756490 140225854473984 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7307289242744446, loss=1.5587172508239746
I1021 02:11:27.618584 140225846081280 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5951817035675049, loss=1.6090768575668335
I1021 02:18:52.863376 140399020136256 spec.py:321] Evaluating on the training split.
I1021 02:19:46.530586 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 02:20:38.447414 140399020136256 spec.py:349] Evaluating on the test split.
I1021 02:21:04.509744 140399020136256 submission_runner.py:395] Time since start: 15990.94s, 	Step: 17499, 	{'train/ctc_loss': Array(0.3901441, dtype=float32), 'train/wer': 0.13791716496752776, 'validation/ctc_loss': Array(0.680422, dtype=float32), 'validation/wer': 0.2031532096894098, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44166613, dtype=float32), 'test/wer': 0.14593869965267198, 'test/num_examples': 2472, 'score': 14481.40464758873, 'total_duration': 15990.936472415924, 'accumulated_submission_time': 14481.40464758873, 'accumulated_eval_time': 1508.2479736804962, 'accumulated_logging_time': 0.5244870185852051}
I1021 02:21:04.550584 140225931273984 logging_writer.py:48] [17499] accumulated_eval_time=1508.247974, accumulated_logging_time=0.524487, accumulated_submission_time=14481.404648, global_step=17499, preemption_count=0, score=14481.404648, test/ctc_loss=0.4416661262512207, test/num_examples=2472, test/wer=0.145939, total_duration=15990.936472, train/ctc_loss=0.39014410972595215, train/wer=0.137917, validation/ctc_loss=0.6804220080375671, validation/num_examples=5348, validation/wer=0.203153
I1021 02:21:06.180206 140225922881280 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6945953369140625, loss=1.5815695524215698
I1021 02:27:49.052232 140225931273984 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6721830368041992, loss=1.5713964700698853
I1021 02:35:17.158773 140225922881280 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7242276072502136, loss=1.5660561323165894
I1021 02:42:01.075697 140225931273984 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6860032081604004, loss=1.5911040306091309
I1021 02:45:05.828378 140399020136256 spec.py:321] Evaluating on the training split.
I1021 02:46:00.305922 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 02:46:53.082607 140399020136256 spec.py:349] Evaluating on the test split.
I1021 02:47:19.472122 140399020136256 submission_runner.py:395] Time since start: 17565.90s, 	Step: 19212, 	{'train/ctc_loss': Array(0.37317973, dtype=float32), 'train/wer': 0.12934239403474443, 'validation/ctc_loss': Array(0.6545064, dtype=float32), 'validation/wer': 0.19357579385384785, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41649663, dtype=float32), 'test/wer': 0.13921556679462962, 'test/num_examples': 2472, 'score': 15922.596904039383, 'total_duration': 17565.898744106293, 'accumulated_submission_time': 15922.596904039383, 'accumulated_eval_time': 1641.8859112262726, 'accumulated_logging_time': 0.5815033912658691}
I1021 02:47:19.508793 140225931273984 logging_writer.py:48] [19212] accumulated_eval_time=1641.885911, accumulated_logging_time=0.581503, accumulated_submission_time=15922.596904, global_step=19212, preemption_count=0, score=15922.596904, test/ctc_loss=0.4164966344833374, test/num_examples=2472, test/wer=0.139216, total_duration=17565.898744, train/ctc_loss=0.37317973375320435, train/wer=0.129342, validation/ctc_loss=0.6545063853263855, validation/num_examples=5348, validation/wer=0.193576
I1021 02:51:02.304417 140225922881280 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6877967715263367, loss=1.6369521617889404
I1021 02:57:40.462490 140225931273984 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7612721920013428, loss=1.572650671005249
I1021 03:05:01.930787 140225922881280 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.8726103901863098, loss=1.591402292251587
I1021 03:11:19.468290 140399020136256 spec.py:321] Evaluating on the training split.
I1021 03:12:14.792273 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 03:13:07.322077 140399020136256 spec.py:349] Evaluating on the test split.
I1021 03:13:34.441903 140399020136256 submission_runner.py:395] Time since start: 19140.87s, 	Step: 20972, 	{'train/ctc_loss': Array(0.36757416, dtype=float32), 'train/wer': 0.12917403118157597, 'validation/ctc_loss': Array(0.6262302, dtype=float32), 'validation/wer': 0.18953049422169013, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39058656, dtype=float32), 'test/wer': 0.13149716653464139, 'test/num_examples': 2472, 'score': 17362.467200517654, 'total_duration': 19140.869252204895, 'accumulated_submission_time': 17362.467200517654, 'accumulated_eval_time': 1776.854728460312, 'accumulated_logging_time': 0.6335537433624268}
I1021 03:13:34.480506 140226146313984 logging_writer.py:48] [20972] accumulated_eval_time=1776.854728, accumulated_logging_time=0.633554, accumulated_submission_time=17362.467201, global_step=20972, preemption_count=0, score=17362.467201, test/ctc_loss=0.3905865550041199, test/num_examples=2472, test/wer=0.131497, total_duration=19140.869252, train/ctc_loss=0.36757415533065796, train/wer=0.129174, validation/ctc_loss=0.6262301802635193, validation/num_examples=5348, validation/wer=0.189530
I1021 03:13:56.501918 140226137921280 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6005942821502686, loss=1.515333652496338
I1021 03:20:49.357907 140226146313984 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8178460001945496, loss=1.4947054386138916
I1021 03:27:30.573427 140223744820992 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6509544849395752, loss=1.521203875541687
I1021 03:34:53.391996 140219206596352 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8526288270950317, loss=1.5795754194259644
I1021 03:37:34.509277 140399020136256 spec.py:321] Evaluating on the training split.
I1021 03:38:30.340775 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 03:39:22.731687 140399020136256 spec.py:349] Evaluating on the test split.
I1021 03:39:48.722815 140399020136256 submission_runner.py:395] Time since start: 20715.15s, 	Step: 22683, 	{'train/ctc_loss': Array(0.3454727, dtype=float32), 'train/wer': 0.12108756044860583, 'validation/ctc_loss': Array(0.6098198, dtype=float32), 'validation/wer': 0.1832646243857227, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37983215, dtype=float32), 'test/wer': 0.12615522109154428, 'test/num_examples': 2472, 'score': 18802.411761045456, 'total_duration': 20715.149357318878, 'accumulated_submission_time': 18802.411761045456, 'accumulated_eval_time': 1911.0628380775452, 'accumulated_logging_time': 0.6874861717224121}
I1021 03:39:48.759641 140226473993984 logging_writer.py:48] [22683] accumulated_eval_time=1911.062838, accumulated_logging_time=0.687486, accumulated_submission_time=18802.411761, global_step=22683, preemption_count=0, score=18802.411761, test/ctc_loss=0.3798321485519409, test/num_examples=2472, test/wer=0.126155, total_duration=20715.149357, train/ctc_loss=0.34547269344329834, train/wer=0.121088, validation/ctc_loss=0.6098198294639587, validation/num_examples=5348, validation/wer=0.183265
I1021 03:43:49.219919 140226465601280 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7245698571205139, loss=1.4706470966339111
I1021 03:50:56.873384 140226473993984 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6243489384651184, loss=1.514974594116211
I1021 03:57:43.828603 140226473993984 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7904882431030273, loss=1.5226340293884277
I1021 04:03:49.867349 140399020136256 spec.py:321] Evaluating on the training split.
I1021 04:04:44.623753 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 04:05:36.520648 140399020136256 spec.py:349] Evaluating on the test split.
I1021 04:06:02.544559 140399020136256 submission_runner.py:395] Time since start: 22288.97s, 	Step: 24418, 	{'train/ctc_loss': Array(0.31985536, dtype=float32), 'train/wer': 0.11234745154343144, 'validation/ctc_loss': Array(0.5861385, dtype=float32), 'validation/wer': 0.17557952054992904, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37367207, dtype=float32), 'test/wer': 0.12511933053033533, 'test/num_examples': 2472, 'score': 20243.434183120728, 'total_duration': 22288.971884012222, 'accumulated_submission_time': 20243.434183120728, 'accumulated_eval_time': 2043.7349452972412, 'accumulated_logging_time': 0.7391836643218994}
I1021 04:06:02.579195 140226043913984 logging_writer.py:48] [24418] accumulated_eval_time=2043.734945, accumulated_logging_time=0.739184, accumulated_submission_time=20243.434183, global_step=24418, preemption_count=0, score=20243.434183, test/ctc_loss=0.3736720681190491, test/num_examples=2472, test/wer=0.125119, total_duration=22288.971884, train/ctc_loss=0.31985536217689514, train/wer=0.112347, validation/ctc_loss=0.5861384868621826, validation/num_examples=5348, validation/wer=0.175580
I1021 04:07:05.512377 140226035521280 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8696625828742981, loss=1.4546549320220947
I1021 04:13:41.018918 140226043913984 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6561640501022339, loss=1.4723817110061646
I1021 04:20:57.583633 140226035521280 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.9855536222457886, loss=1.5054670572280884
I1021 04:27:55.543499 140226043913984 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6627660989761353, loss=1.3841595649719238
I1021 04:30:03.272836 140399020136256 spec.py:321] Evaluating on the training split.
I1021 04:30:58.094877 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 04:31:49.378629 140399020136256 spec.py:349] Evaluating on the test split.
I1021 04:32:16.103997 140399020136256 submission_runner.py:395] Time since start: 23862.53s, 	Step: 26160, 	{'train/ctc_loss': Array(0.29621673, dtype=float32), 'train/wer': 0.10711807819713608, 'validation/ctc_loss': Array(0.5780303, dtype=float32), 'validation/wer': 0.17229693850951466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35506412, dtype=float32), 'test/wer': 0.11949302297239656, 'test/num_examples': 2472, 'score': 21684.039636850357, 'total_duration': 23862.530698537827, 'accumulated_submission_time': 21684.039636850357, 'accumulated_eval_time': 2176.560405254364, 'accumulated_logging_time': 0.7891194820404053}
I1021 04:32:16.137247 140226473993984 logging_writer.py:48] [26160] accumulated_eval_time=2176.560405, accumulated_logging_time=0.789119, accumulated_submission_time=21684.039637, global_step=26160, preemption_count=0, score=21684.039637, test/ctc_loss=0.35506412386894226, test/num_examples=2472, test/wer=0.119493, total_duration=23862.530699, train/ctc_loss=0.2962167263031006, train/wer=0.107118, validation/ctc_loss=0.5780302882194519, validation/num_examples=5348, validation/wer=0.172297
I1021 04:36:47.152612 140226465601280 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7533676624298096, loss=1.5108063220977783
I1021 04:43:45.758692 140223744820992 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.7763481140136719, loss=1.465749740600586
I1021 04:50:51.059530 140219206596352 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7049205303192139, loss=1.4365730285644531
I1021 04:56:16.851617 140399020136256 spec.py:321] Evaluating on the training split.
I1021 04:57:12.568465 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 04:58:03.855608 140399020136256 spec.py:349] Evaluating on the test split.
I1021 04:58:30.003400 140399020136256 submission_runner.py:395] Time since start: 25436.43s, 	Step: 27876, 	{'train/ctc_loss': Array(0.285192, dtype=float32), 'train/wer': 0.10157920037638973, 'validation/ctc_loss': Array(0.557639, dtype=float32), 'validation/wer': 0.1659345221429468, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34469202, dtype=float32), 'test/wer': 0.1147604249182459, 'test/num_examples': 2472, 'score': 23124.66817355156, 'total_duration': 25436.430387735367, 'accumulated_submission_time': 23124.66817355156, 'accumulated_eval_time': 2309.706942796707, 'accumulated_logging_time': 0.8387844562530518}
I1021 04:58:30.039459 140223744820992 logging_writer.py:48] [27876] accumulated_eval_time=2309.706943, accumulated_logging_time=0.838784, accumulated_submission_time=23124.668174, global_step=27876, preemption_count=0, score=23124.668174, test/ctc_loss=0.34469202160835266, test/num_examples=2472, test/wer=0.114760, total_duration=25436.430388, train/ctc_loss=0.28519201278686523, train/wer=0.101579, validation/ctc_loss=0.5576390027999878, validation/num_examples=5348, validation/wer=0.165935
I1021 05:00:04.640479 140219206596352 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.7868125438690186, loss=1.4505314826965332
I1021 05:06:52.665904 140223744820992 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.9623817205429077, loss=1.4384307861328125
I1021 05:14:05.528980 140223744820992 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6617876291275024, loss=1.411993145942688
I1021 05:21:12.016861 140219206596352 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.8983023762702942, loss=1.4687236547470093
I1021 05:22:30.002446 140399020136256 spec.py:321] Evaluating on the training split.
I1021 05:23:27.035933 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 05:24:19.304407 140399020136256 spec.py:349] Evaluating on the test split.
I1021 05:24:45.898185 140399020136256 submission_runner.py:395] Time since start: 27012.32s, 	Step: 29590, 	{'train/ctc_loss': Array(0.29471737, dtype=float32), 'train/wer': 0.1031157912983186, 'validation/ctc_loss': Array(0.5430794, dtype=float32), 'validation/wer': 0.15910868242949688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32847142, dtype=float32), 'test/wer': 0.10738732151199398, 'test/num_examples': 2472, 'score': 24564.54708480835, 'total_duration': 27012.3249065876, 'accumulated_submission_time': 24564.54708480835, 'accumulated_eval_time': 2445.596962928772, 'accumulated_logging_time': 0.8901410102844238}
I1021 05:24:45.938074 140223744820992 logging_writer.py:48] [29590] accumulated_eval_time=2445.596963, accumulated_logging_time=0.890141, accumulated_submission_time=24564.547085, global_step=29590, preemption_count=0, score=24564.547085, test/ctc_loss=0.32847142219543457, test/num_examples=2472, test/wer=0.107387, total_duration=27012.324907, train/ctc_loss=0.29471737146377563, train/wer=0.103116, validation/ctc_loss=0.5430793762207031, validation/num_examples=5348, validation/wer=0.159109
I1021 05:30:05.997202 140223744820992 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6807070374488831, loss=1.3471673727035522
I1021 05:37:01.054320 140219206596352 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.79643315076828, loss=1.417166829109192
I1021 05:44:14.825762 140223744820992 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.7045010924339294, loss=1.4184281826019287
I1021 05:48:46.355429 140399020136256 spec.py:321] Evaluating on the training split.
I1021 05:49:39.944219 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 05:50:32.178011 140399020136256 spec.py:349] Evaluating on the test split.
I1021 05:50:59.248484 140399020136256 submission_runner.py:395] Time since start: 28585.68s, 	Step: 31344, 	{'train/ctc_loss': Array(0.26721093, dtype=float32), 'train/wer': 0.09198096571332455, 'validation/ctc_loss': Array(0.52418435, dtype=float32), 'validation/wer': 0.15458065014433706, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32299978, dtype=float32), 'test/wer': 0.10553896776552313, 'test/num_examples': 2472, 'score': 26004.87580347061, 'total_duration': 28585.675959825516, 'accumulated_submission_time': 26004.87580347061, 'accumulated_eval_time': 2578.48508810997, 'accumulated_logging_time': 0.9474437236785889}
I1021 05:50:59.294324 140226473993984 logging_writer.py:48] [31344] accumulated_eval_time=2578.485088, accumulated_logging_time=0.947444, accumulated_submission_time=26004.875803, global_step=31344, preemption_count=0, score=26004.875803, test/ctc_loss=0.3229997754096985, test/num_examples=2472, test/wer=0.105539, total_duration=28585.675960, train/ctc_loss=0.2672109305858612, train/wer=0.091981, validation/ctc_loss=0.5241843461990356, validation/num_examples=5348, validation/wer=0.154581
I1021 05:52:58.354475 140226465601280 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.7533060908317566, loss=1.3720182180404663
I1021 06:00:05.214989 140226473993984 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.8332254886627197, loss=1.3623093366622925
I1021 06:06:51.488050 140226465601280 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7066465020179749, loss=1.3854451179504395
I1021 06:14:21.696637 140226473993984 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7575526237487793, loss=1.3395464420318604
I1021 06:14:59.968695 140399020136256 spec.py:321] Evaluating on the training split.
I1021 06:15:56.015978 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 06:16:48.401366 140399020136256 spec.py:349] Evaluating on the test split.
I1021 06:17:14.569026 140399020136256 submission_runner.py:395] Time since start: 30160.99s, 	Step: 33052, 	{'train/ctc_loss': Array(0.27249825, dtype=float32), 'train/wer': 0.09456051040484104, 'validation/ctc_loss': Array(0.5073136, dtype=float32), 'validation/wer': 0.14830512565530957, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3023019, dtype=float32), 'test/wer': 0.09960798651311112, 'test/num_examples': 2472, 'score': 27445.45045208931, 'total_duration': 30160.994567632675, 'accumulated_submission_time': 27445.45045208931, 'accumulated_eval_time': 2713.0785553455353, 'accumulated_logging_time': 1.0223677158355713}
I1021 06:17:14.604962 140226473993984 logging_writer.py:48] [33052] accumulated_eval_time=2713.078555, accumulated_logging_time=1.022368, accumulated_submission_time=27445.450452, global_step=33052, preemption_count=0, score=27445.450452, test/ctc_loss=0.30230191349983215, test/num_examples=2472, test/wer=0.099608, total_duration=30160.994568, train/ctc_loss=0.2724982500076294, train/wer=0.094561, validation/ctc_loss=0.50731360912323, validation/num_examples=5348, validation/wer=0.148305
I1021 06:23:01.964837 140226465601280 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7523831129074097, loss=1.3717843294143677
I1021 06:30:27.221613 140226473993984 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.789385199546814, loss=1.3244240283966064
I1021 06:36:58.404031 140226465601280 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.813935399055481, loss=1.341361403465271
I1021 06:41:15.300461 140399020136256 spec.py:321] Evaluating on the training split.
I1021 06:42:11.244512 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 06:43:03.906804 140399020136256 spec.py:349] Evaluating on the test split.
I1021 06:43:30.150748 140399020136256 submission_runner.py:395] Time since start: 31736.58s, 	Step: 34794, 	{'train/ctc_loss': Array(0.2539235, dtype=float32), 'train/wer': 0.08543538263158455, 'validation/ctc_loss': Array(0.49405026, dtype=float32), 'validation/wer': 0.14556320418625757, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29258248, dtype=float32), 'test/wer': 0.096378445351695, 'test/num_examples': 2472, 'score': 28886.05903530121, 'total_duration': 31736.577694892883, 'accumulated_submission_time': 28886.05903530121, 'accumulated_eval_time': 2847.9233691692352, 'accumulated_logging_time': 1.0749807357788086}
I1021 06:43:30.188407 140226473993984 logging_writer.py:48] [34794] accumulated_eval_time=2847.923369, accumulated_logging_time=1.074981, accumulated_submission_time=28886.059035, global_step=34794, preemption_count=0, score=28886.059035, test/ctc_loss=0.2925824820995331, test/num_examples=2472, test/wer=0.096378, total_duration=31736.577695, train/ctc_loss=0.2539235055446625, train/wer=0.085435, validation/ctc_loss=0.4940502643585205, validation/num_examples=5348, validation/wer=0.145563
I1021 06:46:06.725907 140226465601280 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.8935003876686096, loss=1.3646608591079712
I1021 06:52:41.152330 140226473993984 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7636544704437256, loss=1.313492774963379
I1021 07:00:03.068681 140226465601280 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.8988569974899292, loss=1.3543771505355835
I1021 07:06:40.261580 140226473993984 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.8637542724609375, loss=1.365596890449524
I1021 07:07:31.200980 140399020136256 spec.py:321] Evaluating on the training split.
I1021 07:08:27.472217 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 07:09:20.025146 140399020136256 spec.py:349] Evaluating on the test split.
I1021 07:09:45.875775 140399020136256 submission_runner.py:395] Time since start: 33312.30s, 	Step: 36559, 	{'train/ctc_loss': Array(0.19989519, dtype=float32), 'train/wer': 0.07210725377448418, 'validation/ctc_loss': Array(0.47967562, dtype=float32), 'validation/wer': 0.14200063720710196, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2841733, dtype=float32), 'test/wer': 0.09564722848495928, 'test/num_examples': 2472, 'score': 30326.98366880417, 'total_duration': 33312.30351805687, 'accumulated_submission_time': 30326.98366880417, 'accumulated_eval_time': 2982.593507051468, 'accumulated_logging_time': 1.1281278133392334}
I1021 07:09:45.910297 140226473993984 logging_writer.py:48] [36559] accumulated_eval_time=2982.593507, accumulated_logging_time=1.128128, accumulated_submission_time=30326.983669, global_step=36559, preemption_count=0, score=30326.983669, test/ctc_loss=0.28417330980300903, test/num_examples=2472, test/wer=0.095647, total_duration=33312.303518, train/ctc_loss=0.19989518821239471, train/wer=0.072107, validation/ctc_loss=0.47967562079429626, validation/num_examples=5348, validation/wer=0.142001
I1021 07:15:41.728726 140226465601280 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.0440692901611328, loss=1.3102076053619385
I1021 07:22:17.283591 140226473993984 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.7788358330726624, loss=1.3265445232391357
I1021 07:29:31.682343 140226465601280 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.9075559973716736, loss=1.3137342929840088
I1021 07:33:46.621455 140399020136256 spec.py:321] Evaluating on the training split.
I1021 07:34:41.627284 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 07:35:34.964703 140399020136256 spec.py:349] Evaluating on the test split.
I1021 07:36:01.412782 140399020136256 submission_runner.py:395] Time since start: 34887.84s, 	Step: 38316, 	{'train/ctc_loss': Array(0.21843095, dtype=float32), 'train/wer': 0.07706278642478512, 'validation/ctc_loss': Array(0.46298662, dtype=float32), 'validation/wer': 0.13655541288123813, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26832625, dtype=float32), 'test/wer': 0.08823350191944428, 'test/num_examples': 2472, 'score': 31767.606841802597, 'total_duration': 34887.83992910385, 'accumulated_submission_time': 31767.606841802597, 'accumulated_eval_time': 3117.379721403122, 'accumulated_logging_time': 1.1782562732696533}
I1021 07:36:01.445398 140225890313984 logging_writer.py:48] [38316] accumulated_eval_time=3117.379721, accumulated_logging_time=1.178256, accumulated_submission_time=31767.606842, global_step=38316, preemption_count=0, score=31767.606842, test/ctc_loss=0.2683262526988983, test/num_examples=2472, test/wer=0.088234, total_duration=34887.839929, train/ctc_loss=0.21843095123767853, train/wer=0.077063, validation/ctc_loss=0.46298661828041077, validation/num_examples=5348, validation/wer=0.136555
I1021 07:38:21.828681 140225881921280 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.9219968318939209, loss=1.311084270477295
I1021 07:45:25.421386 140225890313984 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.8457157611846924, loss=1.2429404258728027
I1021 07:52:04.836413 140225890313984 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.8668372631072998, loss=1.2097556591033936
I1021 07:59:23.718772 140225881921280 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.8065541386604309, loss=1.2401061058044434
I1021 08:00:01.719718 140399020136256 spec.py:321] Evaluating on the training split.
I1021 08:00:56.062845 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 08:01:49.089048 140399020136256 spec.py:349] Evaluating on the test split.
I1021 08:02:15.488591 140399020136256 submission_runner.py:395] Time since start: 36461.91s, 	Step: 40046, 	{'train/ctc_loss': Array(0.25831828, dtype=float32), 'train/wer': 0.0909618524324295, 'validation/ctc_loss': Array(0.43524617, dtype=float32), 'validation/wer': 0.12950751614740724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25616488, dtype=float32), 'test/wer': 0.08571486604513233, 'test/num_examples': 2472, 'score': 33207.795552015305, 'total_duration': 36461.91294813156, 'accumulated_submission_time': 33207.795552015305, 'accumulated_eval_time': 3251.140545606613, 'accumulated_logging_time': 1.226668357849121}
I1021 08:02:15.525332 140225890313984 logging_writer.py:48] [40046] accumulated_eval_time=3251.140546, accumulated_logging_time=1.226668, accumulated_submission_time=33207.795552, global_step=40046, preemption_count=0, score=33207.795552, test/ctc_loss=0.25616487860679626, test/num_examples=2472, test/wer=0.085715, total_duration=36461.912948, train/ctc_loss=0.2583182752132416, train/wer=0.090962, validation/ctc_loss=0.43524616956710815, validation/num_examples=5348, validation/wer=0.129508
I1021 08:08:07.173407 140225890313984 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.8772463798522949, loss=1.2706670761108398
I1021 08:15:23.577673 140225881921280 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.8084779977798462, loss=1.2544512748718262
I1021 08:22:14.173943 140225890313984 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8747570514678955, loss=1.2646052837371826
I1021 08:26:15.837795 140399020136256 spec.py:321] Evaluating on the training split.
I1021 08:27:09.715860 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 08:28:02.329200 140399020136256 spec.py:349] Evaluating on the test split.
I1021 08:28:29.172693 140399020136256 submission_runner.py:395] Time since start: 38035.60s, 	Step: 41779, 	{'train/ctc_loss': Array(0.25485957, dtype=float32), 'train/wer': 0.0887158107440448, 'validation/ctc_loss': Array(0.4243696, dtype=float32), 'validation/wer': 0.12411056508684361, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24660671, dtype=float32), 'test/wer': 0.08234314382629537, 'test/num_examples': 2472, 'score': 34648.02050900459, 'total_duration': 38035.59652876854, 'accumulated_submission_time': 34648.02050900459, 'accumulated_eval_time': 3384.466889858246, 'accumulated_logging_time': 1.2789838314056396}
I1021 08:28:29.212314 140225890313984 logging_writer.py:48] [41779] accumulated_eval_time=3384.466890, accumulated_logging_time=1.278984, accumulated_submission_time=34648.020509, global_step=41779, preemption_count=0, score=34648.020509, test/ctc_loss=0.246606707572937, test/num_examples=2472, test/wer=0.082343, total_duration=38035.596529, train/ctc_loss=0.2548595666885376, train/wer=0.088716, validation/ctc_loss=0.42436960339546204, validation/num_examples=5348, validation/wer=0.124111
I1021 08:31:17.489685 140225881921280 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.7971321940422058, loss=1.2346303462982178
I1021 08:38:06.515685 140225890313984 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.0759538412094116, loss=1.2054224014282227
I1021 08:45:17.247497 140225881921280 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.9258918762207031, loss=1.2373309135437012
I1021 08:52:13.228356 140225890313984 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.9742806553840637, loss=1.1963095664978027
I1021 08:52:29.294059 140399020136256 spec.py:321] Evaluating on the training split.
I1021 08:53:22.128984 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 08:54:14.504405 140399020136256 spec.py:349] Evaluating on the test split.
I1021 08:54:40.925872 140399020136256 submission_runner.py:395] Time since start: 39607.35s, 	Step: 43522, 	{'train/ctc_loss': Array(0.2758223, dtype=float32), 'train/wer': 0.09571748629724307, 'validation/ctc_loss': Array(0.4042439, dtype=float32), 'validation/wer': 0.11801847900595692, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23432595, dtype=float32), 'test/wer': 0.07651372047204111, 'test/num_examples': 2472, 'score': 36088.01545572281, 'total_duration': 39607.353207826614, 'accumulated_submission_time': 36088.01545572281, 'accumulated_eval_time': 3516.093638896942, 'accumulated_logging_time': 1.3344237804412842}
I1021 08:54:40.963355 140225890313984 logging_writer.py:48] [43522] accumulated_eval_time=3516.093639, accumulated_logging_time=1.334424, accumulated_submission_time=36088.015456, global_step=43522, preemption_count=0, score=36088.015456, test/ctc_loss=0.23432594537734985, test/num_examples=2472, test/wer=0.076514, total_duration=39607.353208, train/ctc_loss=0.27582231163978577, train/wer=0.095717, validation/ctc_loss=0.4042438864707947, validation/num_examples=5348, validation/wer=0.118018
I1021 09:01:09.350739 140225881921280 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.9891669750213623, loss=1.2160999774932861
I1021 09:08:11.004784 140225890313984 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.9522784352302551, loss=1.1992453336715698
I1021 09:15:22.488284 140225881921280 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.2076953649520874, loss=1.1412428617477417
I1021 09:18:41.651673 140399020136256 spec.py:321] Evaluating on the training split.
I1021 09:19:36.887118 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 09:20:29.288806 140399020136256 spec.py:349] Evaluating on the test split.
I1021 09:20:56.331854 140399020136256 submission_runner.py:395] Time since start: 41182.76s, 	Step: 45228, 	{'train/ctc_loss': Array(0.23423436, dtype=float32), 'train/wer': 0.08067165541157145, 'validation/ctc_loss': Array(0.38965458, dtype=float32), 'validation/wer': 0.11413730847581992, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.218602, dtype=float32), 'test/wer': 0.07178112241789043, 'test/num_examples': 2472, 'score': 37528.61952996254, 'total_duration': 41182.75775170326, 'accumulated_submission_time': 37528.61952996254, 'accumulated_eval_time': 3650.7672851085663, 'accumulated_logging_time': 1.3876245021820068}
I1021 09:20:56.371696 140225890313984 logging_writer.py:48] [45228] accumulated_eval_time=3650.767285, accumulated_logging_time=1.387625, accumulated_submission_time=37528.619530, global_step=45228, preemption_count=0, score=37528.619530, test/ctc_loss=0.2186020016670227, test/num_examples=2472, test/wer=0.071781, total_duration=41182.757752, train/ctc_loss=0.23423436284065247, train/wer=0.080672, validation/ctc_loss=0.38965457677841187, validation/num_examples=5348, validation/wer=0.114137
I1021 09:24:26.739305 140225890313984 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.0396655797958374, loss=1.1782512664794922
I1021 09:31:27.072947 140225881921280 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.017609715461731, loss=1.1601860523223877
I1021 09:38:29.849711 140225890313984 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.066321611404419, loss=1.169450044631958
I1021 09:44:56.794304 140399020136256 spec.py:321] Evaluating on the training split.
I1021 09:45:50.555911 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 09:46:42.407740 140399020136256 spec.py:349] Evaluating on the test split.
I1021 09:47:09.509755 140399020136256 submission_runner.py:395] Time since start: 42755.94s, 	Step: 46969, 	{'train/ctc_loss': Array(0.20780899, dtype=float32), 'train/wer': 0.07400376337501252, 'validation/ctc_loss': Array(0.3748595, dtype=float32), 'validation/wer': 0.10946445639475945, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21163845, dtype=float32), 'test/wer': 0.07009526130847196, 'test/num_examples': 2472, 'score': 38968.956124305725, 'total_duration': 42755.936344861984, 'accumulated_submission_time': 38968.956124305725, 'accumulated_eval_time': 3783.4769291877747, 'accumulated_logging_time': 1.4426100254058838}
I1021 09:47:09.564370 140225890313984 logging_writer.py:48] [46969] accumulated_eval_time=3783.476929, accumulated_logging_time=1.442610, accumulated_submission_time=38968.956124, global_step=46969, preemption_count=0, score=38968.956124, test/ctc_loss=0.2116384506225586, test/num_examples=2472, test/wer=0.070095, total_duration=42755.936345, train/ctc_loss=0.2078089863061905, train/wer=0.074004, validation/ctc_loss=0.3748595118522644, validation/num_examples=5348, validation/wer=0.109464
I1021 09:47:33.834890 140225881921280 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.1261192560195923, loss=1.1232246160507202
I1021 09:54:16.805545 140225890313984 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.9630221128463745, loss=1.096572995185852
I1021 10:01:09.639688 140225881921280 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9946014285087585, loss=1.1223866939544678
I1021 10:08:27.245669 140225890313984 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.170982837677002, loss=1.118627905845642
I1021 10:11:09.826850 140399020136256 spec.py:321] Evaluating on the training split.
I1021 10:12:04.274887 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 10:12:55.490647 140399020136256 spec.py:349] Evaluating on the test split.
I1021 10:13:21.621845 140399020136256 submission_runner.py:395] Time since start: 44328.05s, 	Step: 48711, 	{'train/ctc_loss': Array(0.16579287, dtype=float32), 'train/wer': 0.060174662040913984, 'validation/ctc_loss': Array(0.36225975, dtype=float32), 'validation/wer': 0.10428956235457679, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20289198, dtype=float32), 'test/wer': 0.06544390957284749, 'test/num_examples': 2472, 'score': 40409.123242378235, 'total_duration': 44328.04894948006, 'accumulated_submission_time': 40409.123242378235, 'accumulated_eval_time': 3915.266623020172, 'accumulated_logging_time': 1.5204052925109863}
I1021 10:13:21.669386 140225890313984 logging_writer.py:48] [48711] accumulated_eval_time=3915.266623, accumulated_logging_time=1.520405, accumulated_submission_time=40409.123242, global_step=48711, preemption_count=0, score=40409.123242, test/ctc_loss=0.2028919756412506, test/num_examples=2472, test/wer=0.065444, total_duration=44328.048949, train/ctc_loss=0.16579286754131317, train/wer=0.060175, validation/ctc_loss=0.36225974559783936, validation/num_examples=5348, validation/wer=0.104290
I1021 10:17:06.545673 140225881921280 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.0039466619491577, loss=1.0576575994491577
I1021 10:24:26.540530 140225890313984 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.279427170753479, loss=1.0703537464141846
I1021 10:31:10.700275 140225881921280 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.9592227935791016, loss=1.0529937744140625
I1021 10:37:22.220486 140399020136256 spec.py:321] Evaluating on the training split.
I1021 10:38:16.496160 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 10:39:08.649791 140399020136256 spec.py:349] Evaluating on the test split.
I1021 10:39:35.246928 140399020136256 submission_runner.py:395] Time since start: 45901.67s, 	Step: 50413, 	{'train/ctc_loss': Array(0.17108391, dtype=float32), 'train/wer': 0.06250033943962591, 'validation/ctc_loss': Array(0.34546143, dtype=float32), 'validation/wer': 0.09917259623275437, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1897106, dtype=float32), 'test/wer': 0.06229561472995755, 'test/num_examples': 2472, 'score': 41849.58953285217, 'total_duration': 45901.67257618904, 'accumulated_submission_time': 41849.58953285217, 'accumulated_eval_time': 4048.286268234253, 'accumulated_logging_time': 1.5836589336395264}
I1021 10:39:35.288803 140226043913984 logging_writer.py:48] [50413] accumulated_eval_time=4048.286268, accumulated_logging_time=1.583659, accumulated_submission_time=41849.589533, global_step=50413, preemption_count=0, score=41849.589533, test/ctc_loss=0.1897106021642685, test/num_examples=2472, test/wer=0.062296, total_duration=45901.672576, train/ctc_loss=0.17108391225337982, train/wer=0.062500, validation/ctc_loss=0.3454614281654358, validation/num_examples=5348, validation/wer=0.099173
I1021 10:40:46.416697 140226043913984 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.0311915874481201, loss=1.0456374883651733
I1021 10:47:31.773744 140226035521280 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.0032551288604736, loss=1.0103572607040405
I1021 10:54:59.518518 140226043913984 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.269926905632019, loss=1.0180315971374512
I1021 11:01:30.062824 140226035521280 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.06288480758667, loss=1.0380144119262695
I1021 11:03:35.975473 140399020136256 spec.py:321] Evaluating on the training split.
I1021 11:04:31.145291 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 11:05:23.720631 140399020136256 spec.py:349] Evaluating on the test split.
I1021 11:05:49.985362 140399020136256 submission_runner.py:395] Time since start: 47476.41s, 	Step: 52141, 	{'train/ctc_loss': Array(0.14573947, dtype=float32), 'train/wer': 0.05297092423982134, 'validation/ctc_loss': Array(0.3293809, dtype=float32), 'validation/wer': 0.0953203896617975, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1823269, dtype=float32), 'test/wer': 0.060386326244592045, 'test/num_examples': 2472, 'score': 43290.18747854233, 'total_duration': 47476.41242003441, 'accumulated_submission_time': 43290.18747854233, 'accumulated_eval_time': 4182.29082608223, 'accumulated_logging_time': 1.6425158977508545}
I1021 11:05:50.020375 140226473993984 logging_writer.py:48] [52141] accumulated_eval_time=4182.290826, accumulated_logging_time=1.642516, accumulated_submission_time=43290.187479, global_step=52141, preemption_count=0, score=43290.187479, test/ctc_loss=0.18232689797878265, test/num_examples=2472, test/wer=0.060386, total_duration=47476.412420, train/ctc_loss=0.14573946595191956, train/wer=0.052971, validation/ctc_loss=0.32938089966773987, validation/num_examples=5348, validation/wer=0.095320
I1021 11:10:39.654280 140226465601280 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.3321176767349243, loss=1.0614607334136963
I1021 11:17:24.317143 140223744820992 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.1312302350997925, loss=1.0000462532043457
I1021 11:24:47.145828 140219206596352 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.2640976905822754, loss=1.040383219718933
I1021 11:29:50.483024 140399020136256 spec.py:321] Evaluating on the training split.
I1021 11:30:44.920272 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 11:31:37.552781 140399020136256 spec.py:349] Evaluating on the test split.
I1021 11:32:03.736831 140399020136256 submission_runner.py:395] Time since start: 49050.16s, 	Step: 53887, 	{'train/ctc_loss': Array(0.14309603, dtype=float32), 'train/wer': 0.0519424163855108, 'validation/ctc_loss': Array(0.32059798, dtype=float32), 'validation/wer': 0.09272328798864612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17777112, dtype=float32), 'test/wer': 0.057359900879491395, 'test/num_examples': 2472, 'score': 44730.56222438812, 'total_duration': 49050.16361618042, 'accumulated_submission_time': 44730.56222438812, 'accumulated_eval_time': 4315.539252758026, 'accumulated_logging_time': 1.6941368579864502}
I1021 11:32:03.775216 140226473993984 logging_writer.py:48] [53887] accumulated_eval_time=4315.539253, accumulated_logging_time=1.694137, accumulated_submission_time=44730.562224, global_step=53887, preemption_count=0, score=44730.562224, test/ctc_loss=0.17777112126350403, test/num_examples=2472, test/wer=0.057360, total_duration=49050.163616, train/ctc_loss=0.14309602975845337, train/wer=0.051942, validation/ctc_loss=0.32059797644615173, validation/num_examples=5348, validation/wer=0.092723
I1021 11:33:30.110535 140226465601280 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.1763176918029785, loss=1.041667103767395
I1021 11:40:39.684027 140226473993984 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.1047799587249756, loss=0.9997332692146301
I1021 11:47:20.655546 140226473993984 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.1546897888183594, loss=1.0099828243255615
I1021 11:54:47.947097 140226465601280 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.1892995834350586, loss=1.045809030532837
I1021 11:56:04.000767 140399020136256 spec.py:321] Evaluating on the training split.
I1021 11:56:57.264834 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 11:57:48.982151 140399020136256 spec.py:349] Evaluating on the test split.
I1021 11:58:15.158395 140399020136256 submission_runner.py:395] Time since start: 50621.58s, 	Step: 55589, 	{'train/ctc_loss': Array(0.13182299, dtype=float32), 'train/wer': 0.04740924775707384, 'validation/ctc_loss': Array(0.31506917, dtype=float32), 'validation/wer': 0.09083097598887784, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17118415, dtype=float32), 'test/wer': 0.05573497450896756, 'test/num_examples': 2472, 'score': 46170.69915294647, 'total_duration': 50621.58452963829, 'accumulated_submission_time': 46170.69915294647, 'accumulated_eval_time': 4446.690602779388, 'accumulated_logging_time': 1.7517480850219727}
I1021 11:58:15.194761 140225967113984 logging_writer.py:48] [55589] accumulated_eval_time=4446.690603, accumulated_logging_time=1.751748, accumulated_submission_time=46170.699153, global_step=55589, preemption_count=0, score=46170.699153, test/ctc_loss=0.17118415236473083, test/num_examples=2472, test/wer=0.055735, total_duration=50621.584530, train/ctc_loss=0.13182298839092255, train/wer=0.047409, validation/ctc_loss=0.31506916880607605, validation/num_examples=5348, validation/wer=0.090831
I1021 12:03:44.687228 140225967113984 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.101360559463501, loss=1.00922691822052
I1021 12:11:12.803258 140225958721280 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.1543885469436646, loss=0.9643803834915161
I1021 12:18:00.283845 140225967113984 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1244711875915527, loss=0.949612021446228
I1021 12:22:15.682087 140399020136256 spec.py:321] Evaluating on the training split.
I1021 12:23:08.416746 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 12:23:59.554924 140399020136256 spec.py:349] Evaluating on the test split.
I1021 12:24:26.463721 140399020136256 submission_runner.py:395] Time since start: 52192.89s, 	Step: 57295, 	{'train/ctc_loss': Array(0.13554887, dtype=float32), 'train/wer': 0.04970639744514268, 'validation/ctc_loss': Array(0.31007478, dtype=float32), 'validation/wer': 0.08895797329522964, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16756317, dtype=float32), 'test/wer': 0.054313163934759205, 'test/num_examples': 2472, 'score': 47611.10189247131, 'total_duration': 52192.89024591446, 'accumulated_submission_time': 47611.10189247131, 'accumulated_eval_time': 4577.466367483139, 'accumulated_logging_time': 1.8033475875854492}
I1021 12:24:26.508404 140226182153984 logging_writer.py:48] [57295] accumulated_eval_time=4577.466367, accumulated_logging_time=1.803348, accumulated_submission_time=47611.101892, global_step=57295, preemption_count=0, score=47611.101892, test/ctc_loss=0.16756317019462585, test/num_examples=2472, test/wer=0.054313, total_duration=52192.890246, train/ctc_loss=0.13554887473583221, train/wer=0.049706, validation/ctc_loss=0.3100747764110565, validation/num_examples=5348, validation/wer=0.088958
I1021 12:27:02.796365 140226173761280 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.1997431516647339, loss=0.9303559064865112
I1021 12:33:46.165069 140225854473984 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.1489731073379517, loss=0.9674493074417114
I1021 12:41:03.662437 140225846081280 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.260738730430603, loss=0.9651733040809631
I1021 12:47:53.637680 140225854473984 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.1276216506958008, loss=0.9582394361495972
I1021 12:48:27.016394 140399020136256 spec.py:321] Evaluating on the training split.
I1021 12:49:20.969021 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 12:50:13.757382 140399020136256 spec.py:349] Evaluating on the test split.
I1021 12:50:40.458506 140399020136256 submission_runner.py:395] Time since start: 53766.89s, 	Step: 59042, 	{'train/ctc_loss': Array(0.14432187, dtype=float32), 'train/wer': 0.0512203356603753, 'validation/ctc_loss': Array(0.307636, dtype=float32), 'validation/wer': 0.08860075113200808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16659053, dtype=float32), 'test/wer': 0.05411004813844373, 'test/num_examples': 2472, 'score': 49051.52210712433, 'total_duration': 53766.88582921028, 'accumulated_submission_time': 49051.52210712433, 'accumulated_eval_time': 4710.903360366821, 'accumulated_logging_time': 1.863328456878662}
I1021 12:50:40.493563 140225854473984 logging_writer.py:48] [59042] accumulated_eval_time=4710.903360, accumulated_logging_time=1.863328, accumulated_submission_time=49051.522107, global_step=59042, preemption_count=0, score=49051.522107, test/ctc_loss=0.16659052670001984, test/num_examples=2472, test/wer=0.054110, total_duration=53766.885829, train/ctc_loss=0.14432187378406525, train/wer=0.051220, validation/ctc_loss=0.30763599276542664, validation/num_examples=5348, validation/wer=0.088601
I1021 12:56:54.724992 140225846081280 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.0812705755233765, loss=0.9967526197433472
I1021 13:03:46.512892 140399020136256 spec.py:321] Evaluating on the training split.
I1021 13:04:39.235155 140399020136256 spec.py:333] Evaluating on the validation split.
I1021 13:05:32.256168 140399020136256 spec.py:349] Evaluating on the test split.
I1021 13:05:58.953176 140399020136256 submission_runner.py:395] Time since start: 54685.38s, 	Step: 60000, 	{'train/ctc_loss': Array(0.12471567, dtype=float32), 'train/wer': 0.045704710023090546, 'validation/ctc_loss': Array(0.30706546, dtype=float32), 'validation/wer': 0.0884269673769273, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16643563, dtype=float32), 'test/wer': 0.05402880181991754, 'test/num_examples': 2472, 'score': 49837.48560619354, 'total_duration': 54685.38202428818, 'accumulated_submission_time': 49837.48560619354, 'accumulated_eval_time': 4843.3401844501495, 'accumulated_logging_time': 1.9138898849487305}
I1021 13:05:58.996965 140226182153984 logging_writer.py:48] [60000] accumulated_eval_time=4843.340184, accumulated_logging_time=1.913890, accumulated_submission_time=49837.485606, global_step=60000, preemption_count=0, score=49837.485606, test/ctc_loss=0.1664356291294098, test/num_examples=2472, test/wer=0.054029, total_duration=54685.382024, train/ctc_loss=0.12471567094326019, train/wer=0.045705, validation/ctc_loss=0.3070654571056366, validation/num_examples=5348, validation/wer=0.088427
I1021 13:05:59.022583 140226173761280 logging_writer.py:48] [60000] global_step=60000, preemption_count=0, score=49837.485606
I1021 13:05:59.553057 140399020136256 checkpoints.py:490] Saving checkpoint at step: 60000
I1021 13:06:01.218757 140399020136256 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_conformer/adamw_run5/librispeech_conformer_jax/trial_1/checkpoint_60000
I1021 13:06:01.251878 140399020136256 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_conformer/adamw_run5/librispeech_conformer_jax/trial_1/checkpoint_60000.
I1021 13:06:02.749752 140399020136256 submission_runner.py:565] Tuning trial 1/1
I1021 13:06:02.750095 140399020136256 submission_runner.py:566] Hyperparameters: Hyperparameters(learning_rate=0.002106913873888147, beta1=0.8231189937738506, beta2=0.8774571227688758, warmup_steps=1199, weight_decay=0.27590534177690645)
I1021 13:06:02.775088 140399020136256 submission_runner.py:567] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.07877, dtype=float32), 'train/wer': 1.5492940882493804, 'validation/ctc_loss': Array(30.239573, dtype=float32), 'validation/wer': 1.4522046400262607, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.312384, dtype=float32), 'test/wer': 1.4729348201409624, 'test/num_examples': 2472, 'score': 77.31493139266968, 'total_duration': 267.84015250205994, 'accumulated_submission_time': 77.31493139266968, 'accumulated_eval_time': 190.5251533985138, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1757, {'train/ctc_loss': Array(2.489746, dtype=float32), 'train/wer': 0.5370126258985397, 'validation/ctc_loss': Array(3.0186522, dtype=float32), 'validation/wer': 0.5845506241733204, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.6696236, dtype=float32), 'test/wer': 0.5376475128470741, 'test/num_examples': 2472, 'score': 1517.2095663547516, 'total_duration': 1834.4627315998077, 'accumulated_submission_time': 1517.2095663547516, 'accumulated_eval_time': 317.1360650062561, 'accumulated_logging_time': 0.04747796058654785, 'global_step': 1757, 'preemption_count': 0}), (3565, {'train/ctc_loss': Array(0.7203328, dtype=float32), 'train/wer': 0.24164824334762805, 'validation/ctc_loss': Array(1.048355, dtype=float32), 'validation/wer': 0.3036678026975101, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.75977933, dtype=float32), 'test/wer': 0.24396238295452238, 'test/num_examples': 2472, 'score': 2957.3968431949615, 'total_duration': 3405.3215222358704, 'accumulated_submission_time': 2957.3968431949615, 'accumulated_eval_time': 447.67084193229675, 'accumulated_logging_time': 0.10671305656433105, 'global_step': 3565, 'preemption_count': 0}), (5338, {'train/ctc_loss': Array(0.5150274, dtype=float32), 'train/wer': 0.1824617444428631, 'validation/ctc_loss': Array(0.85034025, dtype=float32), 'validation/wer': 0.2525367600915261, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58531016, dtype=float32), 'test/wer': 0.1946255560294924, 'test/num_examples': 2472, 'score': 4397.979216575623, 'total_duration': 4978.104779958725, 'accumulated_submission_time': 4397.979216575623, 'accumulated_eval_time': 579.7461776733398, 'accumulated_logging_time': 0.1551055908203125, 'global_step': 5338, 'preemption_count': 0}), (7091, {'train/ctc_loss': Array(0.48449057, dtype=float32), 'train/wer': 0.17009812648577324, 'validation/ctc_loss': Array(0.7889404, dtype=float32), 'validation/wer': 0.2386533689911853, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.53206474, dtype=float32), 'test/wer': 0.178782523916885, 'test/num_examples': 2472, 'score': 5838.546237945557, 'total_duration': 6552.030452489853, 'accumulated_submission_time': 5838.546237945557, 'accumulated_eval_time': 712.9797339439392, 'accumulated_logging_time': 0.20348572731018066, 'global_step': 7091, 'preemption_count': 0}), (8862, {'train/ctc_loss': Array(0.46976593, dtype=float32), 'train/wer': 0.1622670638630977, 'validation/ctc_loss': Array(0.764583, dtype=float32), 'validation/wer': 0.22766637380885718, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5057142, dtype=float32), 'test/wer': 0.1684439298844271, 'test/num_examples': 2472, 'score': 7278.755240917206, 'total_duration': 8123.680272817612, 'accumulated_submission_time': 7278.755240917206, 'accumulated_eval_time': 844.2834892272949, 'accumulated_logging_time': 0.2622532844543457, 'global_step': 8862, 'preemption_count': 0}), (10624, {'train/ctc_loss': Array(0.47079915, dtype=float32), 'train/wer': 0.16155155213895897, 'validation/ctc_loss': Array(0.7436257, dtype=float32), 'validation/wer': 0.21987506878940305, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48284253, dtype=float32), 'test/wer': 0.1600552474965978, 'test/num_examples': 2472, 'score': 8719.216614961624, 'total_duration': 9696.71895456314, 'accumulated_submission_time': 8719.216614961624, 'accumulated_eval_time': 976.7214727401733, 'accumulated_logging_time': 0.3232543468475342, 'global_step': 10624, 'preemption_count': 0}), (12332, {'train/ctc_loss': Array(0.4325913, dtype=float32), 'train/wer': 0.15166549677748709, 'validation/ctc_loss': Array(0.7184128, dtype=float32), 'validation/wer': 0.2144781177288394, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46839824, dtype=float32), 'test/wer': 0.15605386630918286, 'test/num_examples': 2472, 'score': 10159.629392623901, 'total_duration': 11269.464804649353, 'accumulated_submission_time': 10159.629392623901, 'accumulated_eval_time': 1108.9283285140991, 'accumulated_logging_time': 0.37485384941101074, 'global_step': 12332, 'preemption_count': 0}), (14045, {'train/ctc_loss': Array(0.39367908, dtype=float32), 'train/wer': 0.14158454457994227, 'validation/ctc_loss': Array(0.7046182, dtype=float32), 'validation/wer': 0.21032661691302124, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46340838, dtype=float32), 'test/wer': 0.15390083886823877, 'test/num_examples': 2472, 'score': 11600.30050110817, 'total_duration': 12843.967746257782, 'accumulated_submission_time': 11600.30050110817, 'accumulated_eval_time': 1242.6315286159515, 'accumulated_logging_time': 0.4273555278778076, 'global_step': 14045, 'preemption_count': 0}), (15796, {'train/ctc_loss': Array(0.37315938, dtype=float32), 'train/wer': 0.1305694024014598, 'validation/ctc_loss': Array(0.6939991, dtype=float32), 'validation/wer': 0.2064840649951244, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43945244, dtype=float32), 'test/wer': 0.14642617756382914, 'test/num_examples': 2472, 'score': 13040.39570426941, 'total_duration': 14418.164101362228, 'accumulated_submission_time': 13040.39570426941, 'accumulated_eval_time': 1376.607299566269, 'accumulated_logging_time': 0.475902795791626, 'global_step': 15796, 'preemption_count': 0}), (17499, {'train/ctc_loss': Array(0.3901441, dtype=float32), 'train/wer': 0.13791716496752776, 'validation/ctc_loss': Array(0.680422, dtype=float32), 'validation/wer': 0.2031532096894098, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44166613, dtype=float32), 'test/wer': 0.14593869965267198, 'test/num_examples': 2472, 'score': 14481.40464758873, 'total_duration': 15990.936472415924, 'accumulated_submission_time': 14481.40464758873, 'accumulated_eval_time': 1508.2479736804962, 'accumulated_logging_time': 0.5244870185852051, 'global_step': 17499, 'preemption_count': 0}), (19212, {'train/ctc_loss': Array(0.37317973, dtype=float32), 'train/wer': 0.12934239403474443, 'validation/ctc_loss': Array(0.6545064, dtype=float32), 'validation/wer': 0.19357579385384785, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41649663, dtype=float32), 'test/wer': 0.13921556679462962, 'test/num_examples': 2472, 'score': 15922.596904039383, 'total_duration': 17565.898744106293, 'accumulated_submission_time': 15922.596904039383, 'accumulated_eval_time': 1641.8859112262726, 'accumulated_logging_time': 0.5815033912658691, 'global_step': 19212, 'preemption_count': 0}), (20972, {'train/ctc_loss': Array(0.36757416, dtype=float32), 'train/wer': 0.12917403118157597, 'validation/ctc_loss': Array(0.6262302, dtype=float32), 'validation/wer': 0.18953049422169013, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39058656, dtype=float32), 'test/wer': 0.13149716653464139, 'test/num_examples': 2472, 'score': 17362.467200517654, 'total_duration': 19140.869252204895, 'accumulated_submission_time': 17362.467200517654, 'accumulated_eval_time': 1776.854728460312, 'accumulated_logging_time': 0.6335537433624268, 'global_step': 20972, 'preemption_count': 0}), (22683, {'train/ctc_loss': Array(0.3454727, dtype=float32), 'train/wer': 0.12108756044860583, 'validation/ctc_loss': Array(0.6098198, dtype=float32), 'validation/wer': 0.1832646243857227, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37983215, dtype=float32), 'test/wer': 0.12615522109154428, 'test/num_examples': 2472, 'score': 18802.411761045456, 'total_duration': 20715.149357318878, 'accumulated_submission_time': 18802.411761045456, 'accumulated_eval_time': 1911.0628380775452, 'accumulated_logging_time': 0.6874861717224121, 'global_step': 22683, 'preemption_count': 0}), (24418, {'train/ctc_loss': Array(0.31985536, dtype=float32), 'train/wer': 0.11234745154343144, 'validation/ctc_loss': Array(0.5861385, dtype=float32), 'validation/wer': 0.17557952054992904, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37367207, dtype=float32), 'test/wer': 0.12511933053033533, 'test/num_examples': 2472, 'score': 20243.434183120728, 'total_duration': 22288.971884012222, 'accumulated_submission_time': 20243.434183120728, 'accumulated_eval_time': 2043.7349452972412, 'accumulated_logging_time': 0.7391836643218994, 'global_step': 24418, 'preemption_count': 0}), (26160, {'train/ctc_loss': Array(0.29621673, dtype=float32), 'train/wer': 0.10711807819713608, 'validation/ctc_loss': Array(0.5780303, dtype=float32), 'validation/wer': 0.17229693850951466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35506412, dtype=float32), 'test/wer': 0.11949302297239656, 'test/num_examples': 2472, 'score': 21684.039636850357, 'total_duration': 23862.530698537827, 'accumulated_submission_time': 21684.039636850357, 'accumulated_eval_time': 2176.560405254364, 'accumulated_logging_time': 0.7891194820404053, 'global_step': 26160, 'preemption_count': 0}), (27876, {'train/ctc_loss': Array(0.285192, dtype=float32), 'train/wer': 0.10157920037638973, 'validation/ctc_loss': Array(0.557639, dtype=float32), 'validation/wer': 0.1659345221429468, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34469202, dtype=float32), 'test/wer': 0.1147604249182459, 'test/num_examples': 2472, 'score': 23124.66817355156, 'total_duration': 25436.430387735367, 'accumulated_submission_time': 23124.66817355156, 'accumulated_eval_time': 2309.706942796707, 'accumulated_logging_time': 0.8387844562530518, 'global_step': 27876, 'preemption_count': 0}), (29590, {'train/ctc_loss': Array(0.29471737, dtype=float32), 'train/wer': 0.1031157912983186, 'validation/ctc_loss': Array(0.5430794, dtype=float32), 'validation/wer': 0.15910868242949688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32847142, dtype=float32), 'test/wer': 0.10738732151199398, 'test/num_examples': 2472, 'score': 24564.54708480835, 'total_duration': 27012.3249065876, 'accumulated_submission_time': 24564.54708480835, 'accumulated_eval_time': 2445.596962928772, 'accumulated_logging_time': 0.8901410102844238, 'global_step': 29590, 'preemption_count': 0}), (31344, {'train/ctc_loss': Array(0.26721093, dtype=float32), 'train/wer': 0.09198096571332455, 'validation/ctc_loss': Array(0.52418435, dtype=float32), 'validation/wer': 0.15458065014433706, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32299978, dtype=float32), 'test/wer': 0.10553896776552313, 'test/num_examples': 2472, 'score': 26004.87580347061, 'total_duration': 28585.675959825516, 'accumulated_submission_time': 26004.87580347061, 'accumulated_eval_time': 2578.48508810997, 'accumulated_logging_time': 0.9474437236785889, 'global_step': 31344, 'preemption_count': 0}), (33052, {'train/ctc_loss': Array(0.27249825, dtype=float32), 'train/wer': 0.09456051040484104, 'validation/ctc_loss': Array(0.5073136, dtype=float32), 'validation/wer': 0.14830512565530957, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3023019, dtype=float32), 'test/wer': 0.09960798651311112, 'test/num_examples': 2472, 'score': 27445.45045208931, 'total_duration': 30160.994567632675, 'accumulated_submission_time': 27445.45045208931, 'accumulated_eval_time': 2713.0785553455353, 'accumulated_logging_time': 1.0223677158355713, 'global_step': 33052, 'preemption_count': 0}), (34794, {'train/ctc_loss': Array(0.2539235, dtype=float32), 'train/wer': 0.08543538263158455, 'validation/ctc_loss': Array(0.49405026, dtype=float32), 'validation/wer': 0.14556320418625757, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29258248, dtype=float32), 'test/wer': 0.096378445351695, 'test/num_examples': 2472, 'score': 28886.05903530121, 'total_duration': 31736.577694892883, 'accumulated_submission_time': 28886.05903530121, 'accumulated_eval_time': 2847.9233691692352, 'accumulated_logging_time': 1.0749807357788086, 'global_step': 34794, 'preemption_count': 0}), (36559, {'train/ctc_loss': Array(0.19989519, dtype=float32), 'train/wer': 0.07210725377448418, 'validation/ctc_loss': Array(0.47967562, dtype=float32), 'validation/wer': 0.14200063720710196, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2841733, dtype=float32), 'test/wer': 0.09564722848495928, 'test/num_examples': 2472, 'score': 30326.98366880417, 'total_duration': 33312.30351805687, 'accumulated_submission_time': 30326.98366880417, 'accumulated_eval_time': 2982.593507051468, 'accumulated_logging_time': 1.1281278133392334, 'global_step': 36559, 'preemption_count': 0}), (38316, {'train/ctc_loss': Array(0.21843095, dtype=float32), 'train/wer': 0.07706278642478512, 'validation/ctc_loss': Array(0.46298662, dtype=float32), 'validation/wer': 0.13655541288123813, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26832625, dtype=float32), 'test/wer': 0.08823350191944428, 'test/num_examples': 2472, 'score': 31767.606841802597, 'total_duration': 34887.83992910385, 'accumulated_submission_time': 31767.606841802597, 'accumulated_eval_time': 3117.379721403122, 'accumulated_logging_time': 1.1782562732696533, 'global_step': 38316, 'preemption_count': 0}), (40046, {'train/ctc_loss': Array(0.25831828, dtype=float32), 'train/wer': 0.0909618524324295, 'validation/ctc_loss': Array(0.43524617, dtype=float32), 'validation/wer': 0.12950751614740724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25616488, dtype=float32), 'test/wer': 0.08571486604513233, 'test/num_examples': 2472, 'score': 33207.795552015305, 'total_duration': 36461.91294813156, 'accumulated_submission_time': 33207.795552015305, 'accumulated_eval_time': 3251.140545606613, 'accumulated_logging_time': 1.226668357849121, 'global_step': 40046, 'preemption_count': 0}), (41779, {'train/ctc_loss': Array(0.25485957, dtype=float32), 'train/wer': 0.0887158107440448, 'validation/ctc_loss': Array(0.4243696, dtype=float32), 'validation/wer': 0.12411056508684361, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24660671, dtype=float32), 'test/wer': 0.08234314382629537, 'test/num_examples': 2472, 'score': 34648.02050900459, 'total_duration': 38035.59652876854, 'accumulated_submission_time': 34648.02050900459, 'accumulated_eval_time': 3384.466889858246, 'accumulated_logging_time': 1.2789838314056396, 'global_step': 41779, 'preemption_count': 0}), (43522, {'train/ctc_loss': Array(0.2758223, dtype=float32), 'train/wer': 0.09571748629724307, 'validation/ctc_loss': Array(0.4042439, dtype=float32), 'validation/wer': 0.11801847900595692, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23432595, dtype=float32), 'test/wer': 0.07651372047204111, 'test/num_examples': 2472, 'score': 36088.01545572281, 'total_duration': 39607.353207826614, 'accumulated_submission_time': 36088.01545572281, 'accumulated_eval_time': 3516.093638896942, 'accumulated_logging_time': 1.3344237804412842, 'global_step': 43522, 'preemption_count': 0}), (45228, {'train/ctc_loss': Array(0.23423436, dtype=float32), 'train/wer': 0.08067165541157145, 'validation/ctc_loss': Array(0.38965458, dtype=float32), 'validation/wer': 0.11413730847581992, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.218602, dtype=float32), 'test/wer': 0.07178112241789043, 'test/num_examples': 2472, 'score': 37528.61952996254, 'total_duration': 41182.75775170326, 'accumulated_submission_time': 37528.61952996254, 'accumulated_eval_time': 3650.7672851085663, 'accumulated_logging_time': 1.3876245021820068, 'global_step': 45228, 'preemption_count': 0}), (46969, {'train/ctc_loss': Array(0.20780899, dtype=float32), 'train/wer': 0.07400376337501252, 'validation/ctc_loss': Array(0.3748595, dtype=float32), 'validation/wer': 0.10946445639475945, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21163845, dtype=float32), 'test/wer': 0.07009526130847196, 'test/num_examples': 2472, 'score': 38968.956124305725, 'total_duration': 42755.936344861984, 'accumulated_submission_time': 38968.956124305725, 'accumulated_eval_time': 3783.4769291877747, 'accumulated_logging_time': 1.4426100254058838, 'global_step': 46969, 'preemption_count': 0}), (48711, {'train/ctc_loss': Array(0.16579287, dtype=float32), 'train/wer': 0.060174662040913984, 'validation/ctc_loss': Array(0.36225975, dtype=float32), 'validation/wer': 0.10428956235457679, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20289198, dtype=float32), 'test/wer': 0.06544390957284749, 'test/num_examples': 2472, 'score': 40409.123242378235, 'total_duration': 44328.04894948006, 'accumulated_submission_time': 40409.123242378235, 'accumulated_eval_time': 3915.266623020172, 'accumulated_logging_time': 1.5204052925109863, 'global_step': 48711, 'preemption_count': 0}), (50413, {'train/ctc_loss': Array(0.17108391, dtype=float32), 'train/wer': 0.06250033943962591, 'validation/ctc_loss': Array(0.34546143, dtype=float32), 'validation/wer': 0.09917259623275437, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1897106, dtype=float32), 'test/wer': 0.06229561472995755, 'test/num_examples': 2472, 'score': 41849.58953285217, 'total_duration': 45901.67257618904, 'accumulated_submission_time': 41849.58953285217, 'accumulated_eval_time': 4048.286268234253, 'accumulated_logging_time': 1.5836589336395264, 'global_step': 50413, 'preemption_count': 0}), (52141, {'train/ctc_loss': Array(0.14573947, dtype=float32), 'train/wer': 0.05297092423982134, 'validation/ctc_loss': Array(0.3293809, dtype=float32), 'validation/wer': 0.0953203896617975, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1823269, dtype=float32), 'test/wer': 0.060386326244592045, 'test/num_examples': 2472, 'score': 43290.18747854233, 'total_duration': 47476.41242003441, 'accumulated_submission_time': 43290.18747854233, 'accumulated_eval_time': 4182.29082608223, 'accumulated_logging_time': 1.6425158977508545, 'global_step': 52141, 'preemption_count': 0}), (53887, {'train/ctc_loss': Array(0.14309603, dtype=float32), 'train/wer': 0.0519424163855108, 'validation/ctc_loss': Array(0.32059798, dtype=float32), 'validation/wer': 0.09272328798864612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17777112, dtype=float32), 'test/wer': 0.057359900879491395, 'test/num_examples': 2472, 'score': 44730.56222438812, 'total_duration': 49050.16361618042, 'accumulated_submission_time': 44730.56222438812, 'accumulated_eval_time': 4315.539252758026, 'accumulated_logging_time': 1.6941368579864502, 'global_step': 53887, 'preemption_count': 0}), (55589, {'train/ctc_loss': Array(0.13182299, dtype=float32), 'train/wer': 0.04740924775707384, 'validation/ctc_loss': Array(0.31506917, dtype=float32), 'validation/wer': 0.09083097598887784, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17118415, dtype=float32), 'test/wer': 0.05573497450896756, 'test/num_examples': 2472, 'score': 46170.69915294647, 'total_duration': 50621.58452963829, 'accumulated_submission_time': 46170.69915294647, 'accumulated_eval_time': 4446.690602779388, 'accumulated_logging_time': 1.7517480850219727, 'global_step': 55589, 'preemption_count': 0}), (57295, {'train/ctc_loss': Array(0.13554887, dtype=float32), 'train/wer': 0.04970639744514268, 'validation/ctc_loss': Array(0.31007478, dtype=float32), 'validation/wer': 0.08895797329522964, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16756317, dtype=float32), 'test/wer': 0.054313163934759205, 'test/num_examples': 2472, 'score': 47611.10189247131, 'total_duration': 52192.89024591446, 'accumulated_submission_time': 47611.10189247131, 'accumulated_eval_time': 4577.466367483139, 'accumulated_logging_time': 1.8033475875854492, 'global_step': 57295, 'preemption_count': 0}), (59042, {'train/ctc_loss': Array(0.14432187, dtype=float32), 'train/wer': 0.0512203356603753, 'validation/ctc_loss': Array(0.307636, dtype=float32), 'validation/wer': 0.08860075113200808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16659053, dtype=float32), 'test/wer': 0.05411004813844373, 'test/num_examples': 2472, 'score': 49051.52210712433, 'total_duration': 53766.88582921028, 'accumulated_submission_time': 49051.52210712433, 'accumulated_eval_time': 4710.903360366821, 'accumulated_logging_time': 1.863328456878662, 'global_step': 59042, 'preemption_count': 0}), (60000, {'train/ctc_loss': Array(0.12471567, dtype=float32), 'train/wer': 0.045704710023090546, 'validation/ctc_loss': Array(0.30706546, dtype=float32), 'validation/wer': 0.0884269673769273, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16643563, dtype=float32), 'test/wer': 0.05402880181991754, 'test/num_examples': 2472, 'score': 49837.48560619354, 'total_duration': 54685.38202428818, 'accumulated_submission_time': 49837.48560619354, 'accumulated_eval_time': 4843.3401844501495, 'accumulated_logging_time': 1.9138898849487305, 'global_step': 60000, 'preemption_count': 0})], 'global_step': 60000}
I1021 13:06:02.775410 140399020136256 submission_runner.py:568] Timing: 49837.48560619354
I1021 13:06:02.775493 140399020136256 submission_runner.py:570] Total number of evals: 36
I1021 13:06:02.775547 140399020136256 submission_runner.py:571] ====================
I1021 13:06:02.780703 140399020136256 submission_runner.py:647] Final librispeech_conformer score: 49837.48560619354
