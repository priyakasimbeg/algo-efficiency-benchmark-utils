python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_conformer/adamw_run0 --overwrite=true --save_checkpoints=false --max_global_steps=60000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_10-13-2023-02-16-42.log
2023-10-13 02:16:48.050760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1013 02:17:06.836568 139660230973248 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_conformer/adamw_run0/librispeech_conformer_jax.
I1013 02:17:07.832589 139660230973248 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1013 02:17:07.833587 139660230973248 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1013 02:17:07.833761 139660230973248 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1013 02:17:07.839177 139660230973248 submission_runner.py:507] Using RNG seed 795326643
I1013 02:17:14.489578 139660230973248 submission_runner.py:516] --- Tuning run 1/1 ---
I1013 02:17:14.489782 139660230973248 submission_runner.py:521] Creating tuning directory at /experiment_runs/targets_check_conformer/adamw_run0/librispeech_conformer_jax/trial_1.
I1013 02:17:14.489952 139660230973248 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_conformer/adamw_run0/librispeech_conformer_jax/trial_1/hparams.json.
I1013 02:17:14.672546 139660230973248 submission_runner.py:191] Initializing dataset.
I1013 02:17:14.672767 139660230973248 submission_runner.py:198] Initializing model.
I1013 02:17:19.412669 139660230973248 submission_runner.py:232] Initializing optimizer.
I1013 02:17:20.659856 139660230973248 submission_runner.py:239] Initializing metrics bundle.
I1013 02:17:20.660083 139660230973248 submission_runner.py:257] Initializing checkpoint and logger.
I1013 02:17:20.661272 139660230973248 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_conformer/adamw_run0/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I1013 02:17:20.661429 139660230973248 submission_runner.py:277] Saving meta data to /experiment_runs/targets_check_conformer/adamw_run0/librispeech_conformer_jax/trial_1/meta_data_0.json.
I1013 02:17:20.661661 139660230973248 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1013 02:17:20.661748 139660230973248 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1013 02:17:21.011914 139660230973248 logger_utils.py:220] Unable to record git information. Continuing without it.
I1013 02:17:21.338501 139660230973248 submission_runner.py:280] Saving flags to /experiment_runs/targets_check_conformer/adamw_run0/librispeech_conformer_jax/trial_1/flags_0.json.
I1013 02:17:21.354551 139660230973248 submission_runner.py:290] Starting training loop.
I1013 02:17:21.652431 139660230973248 input_pipeline.py:20] Loading split = train-clean-100
I1013 02:17:21.712972 139660230973248 input_pipeline.py:20] Loading split = train-clean-360
I1013 02:17:22.133432 139660230973248 input_pipeline.py:20] Loading split = train-other-500
2023-10-13 02:18:35.023988: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-13 02:18:37.930481: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1013 02:18:39.938612 139483458565888 logging_writer.py:48] [0] global_step=0, grad_norm=52.14987564086914, loss=31.865938186645508
I1013 02:18:39.981528 139660230973248 spec.py:321] Evaluating on the training split.
I1013 02:18:40.158525 139660230973248 input_pipeline.py:20] Loading split = train-clean-100
I1013 02:18:40.194388 139660230973248 input_pipeline.py:20] Loading split = train-clean-360
I1013 02:18:40.594670 139660230973248 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1013 02:19:34.880888 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 02:19:34.997621 139660230973248 input_pipeline.py:20] Loading split = dev-clean
I1013 02:19:35.002957 139660230973248 input_pipeline.py:20] Loading split = dev-other
I1013 02:20:23.448785 139660230973248 spec.py:349] Evaluating on the test split.
I1013 02:20:23.568209 139660230973248 input_pipeline.py:20] Loading split = test-clean
I1013 02:20:45.840483 139660230973248 submission_runner.py:381] Time since start: 204.48s, 	Step: 1, 	{'train/ctc_loss': Array(31.282171, dtype=float32), 'train/wer': 0.9550475813672615, 'validation/ctc_loss': Array(30.486477, dtype=float32), 'validation/wer': 1.0443709056527317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.603224, dtype=float32), 'test/wer': 1.0379416934359658, 'test/num_examples': 2472, 'score': 78.62688779830933, 'total_duration': 204.48333477973938, 'accumulated_submission_time': 78.62688779830933, 'accumulated_eval_time': 125.85637426376343, 'accumulated_logging_time': 0}
I1013 02:20:45.867170 139483349509888 logging_writer.py:48] [1] accumulated_eval_time=125.856374, accumulated_logging_time=0, accumulated_submission_time=78.626888, global_step=1, preemption_count=0, score=78.626888, test/ctc_loss=30.60322380065918, test/num_examples=2472, test/wer=1.037942, total_duration=204.483335, train/ctc_loss=31.28217124938965, train/wer=0.955048, validation/ctc_loss=30.48647689819336, validation/num_examples=5348, validation/wer=1.044371
I1013 02:21:07.996421 139485404391168 logging_writer.py:48] [1] global_step=1, grad_norm=53.00121307373047, loss=32.03447723388672
I1013 02:21:08.817345 139485412783872 logging_writer.py:48] [2] global_step=2, grad_norm=55.355712890625, loss=31.920522689819336
I1013 02:21:09.637816 139485404391168 logging_writer.py:48] [3] global_step=3, grad_norm=57.02499771118164, loss=32.28528594970703
I1013 02:21:10.460342 139485412783872 logging_writer.py:48] [4] global_step=4, grad_norm=62.293766021728516, loss=31.820056915283203
I1013 02:21:11.350378 139485404391168 logging_writer.py:48] [5] global_step=5, grad_norm=55.589088439941406, loss=31.639333724975586
I1013 02:21:12.231086 139485412783872 logging_writer.py:48] [6] global_step=6, grad_norm=53.26023483276367, loss=31.91278076171875
I1013 02:21:13.108618 139485404391168 logging_writer.py:48] [7] global_step=7, grad_norm=61.76700210571289, loss=31.65776252746582
I1013 02:21:13.996178 139485412783872 logging_writer.py:48] [8] global_step=8, grad_norm=64.24337768554688, loss=31.87213897705078
I1013 02:21:14.877386 139485404391168 logging_writer.py:48] [9] global_step=9, grad_norm=70.77041625976562, loss=31.023773193359375
I1013 02:21:15.764960 139485412783872 logging_writer.py:48] [10] global_step=10, grad_norm=82.4976577758789, loss=30.77826499938965
I1013 02:21:16.649553 139485404391168 logging_writer.py:48] [11] global_step=11, grad_norm=84.56330108642578, loss=30.8603572845459
I1013 02:21:17.534397 139485412783872 logging_writer.py:48] [12] global_step=12, grad_norm=90.62464141845703, loss=30.942058563232422
I1013 02:21:18.421413 139485404391168 logging_writer.py:48] [13] global_step=13, grad_norm=97.42666625976562, loss=30.106380462646484
I1013 02:21:19.298398 139485412783872 logging_writer.py:48] [14] global_step=14, grad_norm=102.28355407714844, loss=30.79413414001465
I1013 02:21:20.175965 139485404391168 logging_writer.py:48] [15] global_step=15, grad_norm=101.48656463623047, loss=29.60109519958496
I1013 02:21:21.067585 139485412783872 logging_writer.py:48] [16] global_step=16, grad_norm=98.45671844482422, loss=28.943159103393555
I1013 02:21:21.954854 139485404391168 logging_writer.py:48] [17] global_step=17, grad_norm=101.83007049560547, loss=29.250967025756836
I1013 02:21:22.833874 139485412783872 logging_writer.py:48] [18] global_step=18, grad_norm=108.23612213134766, loss=28.2921199798584
I1013 02:21:23.714340 139485404391168 logging_writer.py:48] [19] global_step=19, grad_norm=107.01395416259766, loss=27.80585479736328
I1013 02:21:24.607151 139485412783872 logging_writer.py:48] [20] global_step=20, grad_norm=113.91431427001953, loss=27.374618530273438
I1013 02:21:25.496932 139485404391168 logging_writer.py:48] [21] global_step=21, grad_norm=108.47921752929688, loss=26.909706115722656
I1013 02:21:26.375031 139485412783872 logging_writer.py:48] [22] global_step=22, grad_norm=102.87333679199219, loss=26.293899536132812
I1013 02:21:27.256605 139485404391168 logging_writer.py:48] [23] global_step=23, grad_norm=105.1792984008789, loss=25.311573028564453
I1013 02:21:28.153593 139485412783872 logging_writer.py:48] [24] global_step=24, grad_norm=102.13799285888672, loss=25.55489158630371
I1013 02:21:29.052678 139485404391168 logging_writer.py:48] [25] global_step=25, grad_norm=105.15876007080078, loss=24.784854888916016
I1013 02:21:29.958843 139485412783872 logging_writer.py:48] [26] global_step=26, grad_norm=102.99360656738281, loss=23.368263244628906
I1013 02:21:30.846971 139485404391168 logging_writer.py:48] [27] global_step=27, grad_norm=101.63920593261719, loss=23.48638916015625
I1013 02:21:31.739767 139485412783872 logging_writer.py:48] [28] global_step=28, grad_norm=99.89716339111328, loss=22.228843688964844
I1013 02:21:32.625826 139485404391168 logging_writer.py:48] [29] global_step=29, grad_norm=92.71818542480469, loss=22.04340171813965
I1013 02:21:33.525128 139485412783872 logging_writer.py:48] [30] global_step=30, grad_norm=90.59589385986328, loss=21.795930862426758
I1013 02:21:34.409711 139485404391168 logging_writer.py:48] [31] global_step=31, grad_norm=87.28421020507812, loss=20.244548797607422
I1013 02:21:35.308540 139485412783872 logging_writer.py:48] [32] global_step=32, grad_norm=82.60811614990234, loss=19.233362197875977
I1013 02:21:36.195888 139485404391168 logging_writer.py:48] [33] global_step=33, grad_norm=74.09664916992188, loss=19.833383560180664
I1013 02:21:37.080479 139485412783872 logging_writer.py:48] [34] global_step=34, grad_norm=69.82099914550781, loss=19.220428466796875
I1013 02:21:37.964565 139485404391168 logging_writer.py:48] [35] global_step=35, grad_norm=68.08008575439453, loss=18.536664962768555
I1013 02:21:38.854416 139485412783872 logging_writer.py:48] [36] global_step=36, grad_norm=64.21832275390625, loss=17.83809471130371
I1013 02:21:39.739741 139485404391168 logging_writer.py:48] [37] global_step=37, grad_norm=56.68415451049805, loss=16.579397201538086
I1013 02:21:40.632153 139485412783872 logging_writer.py:48] [38] global_step=38, grad_norm=59.273193359375, loss=16.940649032592773
I1013 02:21:41.512612 139485404391168 logging_writer.py:48] [39] global_step=39, grad_norm=72.26107788085938, loss=17.671737670898438
I1013 02:21:42.393946 139485412783872 logging_writer.py:48] [40] global_step=40, grad_norm=65.04542541503906, loss=16.616411209106445
I1013 02:21:43.269279 139485404391168 logging_writer.py:48] [41] global_step=41, grad_norm=65.76837921142578, loss=16.330787658691406
I1013 02:21:44.158665 139485412783872 logging_writer.py:48] [42] global_step=42, grad_norm=72.56970977783203, loss=16.38740348815918
I1013 02:21:45.038259 139485404391168 logging_writer.py:48] [43] global_step=43, grad_norm=80.93641662597656, loss=16.661157608032227
I1013 02:21:45.917771 139485412783872 logging_writer.py:48] [44] global_step=44, grad_norm=67.63188171386719, loss=15.293364524841309
I1013 02:21:46.808201 139485404391168 logging_writer.py:48] [45] global_step=45, grad_norm=86.1064453125, loss=16.134593963623047
I1013 02:21:47.696248 139485412783872 logging_writer.py:48] [46] global_step=46, grad_norm=65.5656967163086, loss=14.554421424865723
I1013 02:21:48.581406 139485404391168 logging_writer.py:48] [47] global_step=47, grad_norm=60.81437683105469, loss=14.070540428161621
I1013 02:21:49.465089 139485412783872 logging_writer.py:48] [48] global_step=48, grad_norm=62.32337951660156, loss=13.767937660217285
I1013 02:21:50.347337 139485404391168 logging_writer.py:48] [49] global_step=49, grad_norm=61.25960922241211, loss=13.586915016174316
I1013 02:21:51.245361 139485412783872 logging_writer.py:48] [50] global_step=50, grad_norm=54.76224899291992, loss=13.102139472961426
I1013 02:21:52.125707 139485404391168 logging_writer.py:48] [51] global_step=51, grad_norm=47.23716735839844, loss=12.61220645904541
I1013 02:21:53.015737 139485412783872 logging_writer.py:48] [52] global_step=52, grad_norm=50.89237594604492, loss=12.66128921508789
I1013 02:21:53.895275 139485404391168 logging_writer.py:48] [53] global_step=53, grad_norm=49.551944732666016, loss=12.323901176452637
I1013 02:21:54.778813 139485412783872 logging_writer.py:48] [54] global_step=54, grad_norm=36.022666931152344, loss=11.671950340270996
I1013 02:21:55.666041 139485404391168 logging_writer.py:48] [55] global_step=55, grad_norm=51.242706298828125, loss=11.85708999633789
I1013 02:21:56.553365 139485412783872 logging_writer.py:48] [56] global_step=56, grad_norm=43.882774353027344, loss=11.285021781921387
I1013 02:21:57.441403 139485404391168 logging_writer.py:48] [57] global_step=57, grad_norm=59.69696044921875, loss=10.904610633850098
I1013 02:21:58.331472 139485412783872 logging_writer.py:48] [58] global_step=58, grad_norm=85.1696548461914, loss=10.443967819213867
I1013 02:21:59.211794 139485404391168 logging_writer.py:48] [59] global_step=59, grad_norm=95.82470703125, loss=9.698005676269531
I1013 02:22:00.089067 139485412783872 logging_writer.py:48] [60] global_step=60, grad_norm=81.85759735107422, loss=9.108440399169922
I1013 02:22:00.974362 139485404391168 logging_writer.py:48] [61] global_step=61, grad_norm=63.19145202636719, loss=8.486560821533203
I1013 02:22:01.858555 139485412783872 logging_writer.py:48] [62] global_step=62, grad_norm=47.98600769042969, loss=8.053583145141602
I1013 02:22:02.749448 139485404391168 logging_writer.py:48] [63] global_step=63, grad_norm=33.43387985229492, loss=7.716328144073486
I1013 02:22:03.632283 139485412783872 logging_writer.py:48] [64] global_step=64, grad_norm=22.660165786743164, loss=7.516454219818115
I1013 02:22:04.523729 139485404391168 logging_writer.py:48] [65] global_step=65, grad_norm=13.861356735229492, loss=7.376754283905029
I1013 02:22:05.403671 139485412783872 logging_writer.py:48] [66] global_step=66, grad_norm=7.656798362731934, loss=7.306697368621826
I1013 02:22:06.283230 139485404391168 logging_writer.py:48] [67] global_step=67, grad_norm=6.930742263793945, loss=7.262562274932861
I1013 02:22:07.166594 139485412783872 logging_writer.py:48] [68] global_step=68, grad_norm=9.344078063964844, loss=7.286017417907715
I1013 02:22:08.052803 139485404391168 logging_writer.py:48] [69] global_step=69, grad_norm=12.213820457458496, loss=7.346686363220215
I1013 02:22:08.937377 139485412783872 logging_writer.py:48] [70] global_step=70, grad_norm=14.208324432373047, loss=7.393259525299072
I1013 02:22:09.823917 139485404391168 logging_writer.py:48] [71] global_step=71, grad_norm=15.886149406433105, loss=7.443117618560791
I1013 02:22:10.704185 139485412783872 logging_writer.py:48] [72] global_step=72, grad_norm=17.477333068847656, loss=7.48768949508667
I1013 02:22:11.584601 139485404391168 logging_writer.py:48] [73] global_step=73, grad_norm=18.178552627563477, loss=7.541357517242432
I1013 02:22:12.464498 139485412783872 logging_writer.py:48] [74] global_step=74, grad_norm=19.470375061035156, loss=7.632547378540039
I1013 02:22:13.349155 139485404391168 logging_writer.py:48] [75] global_step=75, grad_norm=19.866432189941406, loss=7.655900001525879
I1013 02:22:14.231221 139485412783872 logging_writer.py:48] [76] global_step=76, grad_norm=20.363367080688477, loss=7.727081775665283
I1013 02:22:15.113191 139485404391168 logging_writer.py:48] [77] global_step=77, grad_norm=20.787919998168945, loss=7.751461029052734
I1013 02:22:15.998869 139485412783872 logging_writer.py:48] [78] global_step=78, grad_norm=21.079463958740234, loss=7.792288303375244
I1013 02:22:16.880883 139485404391168 logging_writer.py:48] [79] global_step=79, grad_norm=21.311227798461914, loss=7.81937313079834
I1013 02:22:17.764727 139485412783872 logging_writer.py:48] [80] global_step=80, grad_norm=21.520723342895508, loss=7.85877799987793
I1013 02:22:18.638944 139485404391168 logging_writer.py:48] [81] global_step=81, grad_norm=21.621768951416016, loss=7.8548455238342285
I1013 02:22:19.521775 139485412783872 logging_writer.py:48] [82] global_step=82, grad_norm=21.71426773071289, loss=7.839752674102783
I1013 02:22:20.408217 139485404391168 logging_writer.py:48] [83] global_step=83, grad_norm=21.64475440979004, loss=7.854837894439697
I1013 02:22:21.295186 139485412783872 logging_writer.py:48] [84] global_step=84, grad_norm=21.796417236328125, loss=7.8544135093688965
I1013 02:22:22.182753 139485404391168 logging_writer.py:48] [85] global_step=85, grad_norm=21.757234573364258, loss=7.86018180847168
I1013 02:22:23.068592 139485412783872 logging_writer.py:48] [86] global_step=86, grad_norm=21.54853057861328, loss=7.834375381469727
I1013 02:22:23.950670 139485404391168 logging_writer.py:48] [87] global_step=87, grad_norm=21.32761001586914, loss=7.783128261566162
I1013 02:22:24.829260 139485412783872 logging_writer.py:48] [88] global_step=88, grad_norm=21.354219436645508, loss=7.769698619842529
I1013 02:22:25.707373 139485404391168 logging_writer.py:48] [89] global_step=89, grad_norm=20.974430084228516, loss=7.729644775390625
I1013 02:22:26.591003 139485412783872 logging_writer.py:48] [90] global_step=90, grad_norm=20.67357635498047, loss=7.6993632316589355
I1013 02:22:27.477822 139485404391168 logging_writer.py:48] [91] global_step=91, grad_norm=20.316638946533203, loss=7.660858154296875
I1013 02:22:28.365427 139485412783872 logging_writer.py:48] [92] global_step=92, grad_norm=19.963191986083984, loss=7.6034746170043945
I1013 02:22:29.255956 139485404391168 logging_writer.py:48] [93] global_step=93, grad_norm=19.504581451416016, loss=7.556240558624268
I1013 02:22:30.136067 139485412783872 logging_writer.py:48] [94] global_step=94, grad_norm=19.1480712890625, loss=7.509180545806885
I1013 02:22:31.018641 139485404391168 logging_writer.py:48] [95] global_step=95, grad_norm=18.400829315185547, loss=7.447378635406494
I1013 02:22:31.908159 139485412783872 logging_writer.py:48] [96] global_step=96, grad_norm=17.661001205444336, loss=7.38303279876709
I1013 02:22:32.790421 139485404391168 logging_writer.py:48] [97] global_step=97, grad_norm=16.785123825073242, loss=7.314447402954102
I1013 02:22:33.672455 139485412783872 logging_writer.py:48] [98] global_step=98, grad_norm=15.672940254211426, loss=7.253636360168457
I1013 02:22:34.562068 139485404391168 logging_writer.py:48] [99] global_step=99, grad_norm=14.881830215454102, loss=7.206811904907227
I1013 02:22:35.453177 139485412783872 logging_writer.py:48] [100] global_step=100, grad_norm=13.433318138122559, loss=7.15138053894043
I1013 02:27:40.511423 139485404391168 logging_writer.py:48] [500] global_step=500, grad_norm=0.8751013278961182, loss=5.8264055252075195
I1013 02:34:10.671338 139485412783872 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1392693519592285, loss=5.802319526672363
I1013 02:40:38.052997 139487471183616 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.4769193232059479, loss=5.812132835388184
I1013 02:44:45.872089 139660230973248 spec.py:321] Evaluating on the training split.
I1013 02:45:21.572572 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 02:46:05.923314 139660230973248 spec.py:349] Evaluating on the test split.
I1013 02:46:29.272966 139660230973248 submission_runner.py:381] Time since start: 1747.91s, 	Step: 1825, 	{'train/ctc_loss': Array(6.392737, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.480569, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.4471064, dtype=float32), 'test/wer': 0.8947815019092066, 'test/num_examples': 2472, 'score': 1518.5892317295074, 'total_duration': 1747.9132425785065, 'accumulated_submission_time': 1518.5892317295074, 'accumulated_eval_time': 229.25212001800537, 'accumulated_logging_time': 0.04096341133117676}
I1013 02:46:29.306982 139487179343616 logging_writer.py:48] [1825] accumulated_eval_time=229.252120, accumulated_logging_time=0.040963, accumulated_submission_time=1518.589232, global_step=1825, preemption_count=0, score=1518.589232, test/ctc_loss=6.44710636138916, test/num_examples=2472, test/wer=0.894782, total_duration=1747.913243, train/ctc_loss=6.392736911773682, train/wer=0.944636, validation/ctc_loss=6.480568885803223, validation/num_examples=5348, validation/wer=0.895995
I1013 02:48:43.682926 139487170950912 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.6732970476150513, loss=5.399288177490234
I1013 02:55:08.806685 139487179343616 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8300632834434509, loss=3.6693904399871826
I1013 03:01:43.775533 139487170950912 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0635145902633667, loss=3.1153910160064697
I1013 03:08:12.681674 139486523983616 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8115134835243225, loss=2.74635910987854
I1013 03:10:30.375271 139660230973248 spec.py:321] Evaluating on the training split.
I1013 03:11:24.254461 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 03:12:15.784126 139660230973248 spec.py:349] Evaluating on the test split.
I1013 03:12:42.548375 139660230973248 submission_runner.py:381] Time since start: 3321.19s, 	Step: 3679, 	{'train/ctc_loss': Array(2.0757494, dtype=float32), 'train/wer': 0.5652664944197321, 'validation/ctc_loss': Array(2.3676524, dtype=float32), 'validation/wer': 0.5774778338430665, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.0936549, dtype=float32), 'test/wer': 0.5396892740974201, 'test/num_examples': 2472, 'score': 2959.6147639751434, 'total_duration': 3321.188063621521, 'accumulated_submission_time': 2959.6147639751434, 'accumulated_eval_time': 361.41952252388, 'accumulated_logging_time': 0.08943367004394531}
I1013 03:12:42.582227 139485802063616 logging_writer.py:48] [3679] accumulated_eval_time=361.419523, accumulated_logging_time=0.089434, accumulated_submission_time=2959.614764, global_step=3679, preemption_count=0, score=2959.614764, test/ctc_loss=2.0936548709869385, test/num_examples=2472, test/wer=0.539689, total_duration=3321.188064, train/ctc_loss=2.075749397277832, train/wer=0.565266, validation/ctc_loss=2.367652416229248, validation/num_examples=5348, validation/wer=0.577478
I1013 03:16:48.050147 139485793670912 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.069348692893982, loss=2.5113847255706787
I1013 03:23:11.953243 139485802063616 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8287769556045532, loss=2.311201572418213
I1013 03:29:47.015995 139485793670912 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8678953647613525, loss=2.0901200771331787
I1013 03:36:20.215642 139487179343616 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8073504567146301, loss=2.018216609954834
I1013 03:36:42.727810 139660230973248 spec.py:321] Evaluating on the training split.
I1013 03:37:35.598321 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 03:38:25.713469 139660230973248 spec.py:349] Evaluating on the test split.
I1013 03:38:51.808837 139660230973248 submission_runner.py:381] Time since start: 4890.45s, 	Step: 5531, 	{'train/ctc_loss': Array(0.7355601, dtype=float32), 'train/wer': 0.2515773993073675, 'validation/ctc_loss': Array(1.0863374, dtype=float32), 'validation/wer': 0.31641405126918737, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.79588956, dtype=float32), 'test/wer': 0.25601551609188433, 'test/num_examples': 2472, 'score': 4399.716722488403, 'total_duration': 4890.447804689407, 'accumulated_submission_time': 4399.716722488403, 'accumulated_eval_time': 490.494099855423, 'accumulated_logging_time': 0.13850045204162598}
I1013 03:38:51.842205 139487179343616 logging_writer.py:48] [5531] accumulated_eval_time=490.494100, accumulated_logging_time=0.138500, accumulated_submission_time=4399.716722, global_step=5531, preemption_count=0, score=4399.716722, test/ctc_loss=0.7958895564079285, test/num_examples=2472, test/wer=0.256016, total_duration=4890.447805, train/ctc_loss=0.7355601191520691, train/wer=0.251577, validation/ctc_loss=1.0863374471664429, validation/num_examples=5348, validation/wer=0.316414
I1013 03:44:49.392994 139487170950912 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7430769801139832, loss=1.959873914718628
I1013 03:51:23.378496 139486851663616 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.925195574760437, loss=1.8602925539016724
I1013 03:58:00.133797 139486843270912 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6654837727546692, loss=1.804774522781372
I1013 04:02:51.807752 139660230973248 spec.py:321] Evaluating on the training split.
I1013 04:03:45.235669 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 04:04:35.764738 139660230973248 spec.py:349] Evaluating on the test split.
I1013 04:05:02.417486 139660230973248 submission_runner.py:381] Time since start: 6461.06s, 	Step: 7361, 	{'train/ctc_loss': Array(0.5353512, dtype=float32), 'train/wer': 0.18389552723350333, 'validation/ctc_loss': Array(0.86038196, dtype=float32), 'validation/wer': 0.2568090381962199, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5859844, dtype=float32), 'test/wer': 0.19255712467422267, 'test/num_examples': 2472, 'score': 5839.638425111771, 'total_duration': 6461.057765007019, 'accumulated_submission_time': 5839.638425111771, 'accumulated_eval_time': 621.09885430336, 'accumulated_logging_time': 0.18726372718811035}
I1013 04:05:02.455486 139486559823616 logging_writer.py:48] [7361] accumulated_eval_time=621.098854, accumulated_logging_time=0.187264, accumulated_submission_time=5839.638425, global_step=7361, preemption_count=0, score=5839.638425, test/ctc_loss=0.5859844088554382, test/num_examples=2472, test/wer=0.192557, total_duration=6461.057765, train/ctc_loss=0.5353512167930603, train/wer=0.183896, validation/ctc_loss=0.8603819608688354, validation/num_examples=5348, validation/wer=0.256809
I1013 04:06:48.704767 139486551430912 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7589336037635803, loss=1.8365899324417114
I1013 04:13:33.458376 139486559823616 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7139081358909607, loss=1.8416645526885986
I1013 04:20:22.765673 139486559823616 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6406741142272949, loss=1.7925336360931396
I1013 04:27:11.732457 139486551430912 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8786659836769104, loss=1.786069393157959
I1013 04:29:02.956640 139660230973248 spec.py:321] Evaluating on the training split.
I1013 04:29:55.477888 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 04:30:46.351154 139660230973248 spec.py:349] Evaluating on the test split.
I1013 04:31:12.543434 139660230973248 submission_runner.py:381] Time since start: 8031.18s, 	Step: 9132, 	{'train/ctc_loss': Array(0.47408965, dtype=float32), 'train/wer': 0.16640935875970342, 'validation/ctc_loss': Array(0.77574897, dtype=float32), 'validation/wer': 0.23329699273509633, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.522608, dtype=float32), 'test/wer': 0.17207103460815806, 'test/num_examples': 2472, 'score': 7280.09526014328, 'total_duration': 8031.183102846146, 'accumulated_submission_time': 7280.09526014328, 'accumulated_eval_time': 750.6799328327179, 'accumulated_logging_time': 0.24143624305725098}
I1013 04:31:12.578140 139486559823616 logging_writer.py:48] [9132] accumulated_eval_time=750.679933, accumulated_logging_time=0.241436, accumulated_submission_time=7280.095260, global_step=9132, preemption_count=0, score=7280.095260, test/ctc_loss=0.5226079821586609, test/num_examples=2472, test/wer=0.172071, total_duration=8031.183103, train/ctc_loss=0.474089652299881, train/wer=0.166409, validation/ctc_loss=0.7757489681243896, validation/num_examples=5348, validation/wer=0.233297
I1013 04:35:55.739825 139487179343616 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.777877688407898, loss=1.7117092609405518
I1013 04:42:44.402337 139487170950912 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6787063479423523, loss=1.7724827527999878
I1013 04:49:40.337566 139486559823616 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6994976997375488, loss=1.848302960395813
I1013 04:55:12.883104 139660230973248 spec.py:321] Evaluating on the training split.
I1013 04:56:06.196112 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 04:56:56.682742 139660230973248 spec.py:349] Evaluating on the test split.
I1013 04:57:23.188280 139660230973248 submission_runner.py:381] Time since start: 9601.83s, 	Step: 10919, 	{'train/ctc_loss': Array(0.45372334, dtype=float32), 'train/wer': 0.1583827240185435, 'validation/ctc_loss': Array(0.72464544, dtype=float32), 'validation/wer': 0.2177252071896497, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46669164, dtype=float32), 'test/wer': 0.16055518516273715, 'test/num_examples': 2472, 'score': 8720.35819721222, 'total_duration': 9601.827582836151, 'accumulated_submission_time': 8720.35819721222, 'accumulated_eval_time': 880.9789929389954, 'accumulated_logging_time': 0.29038214683532715}
I1013 04:57:23.224455 139486636623616 logging_writer.py:48] [10919] accumulated_eval_time=880.978993, accumulated_logging_time=0.290382, accumulated_submission_time=8720.358197, global_step=10919, preemption_count=0, score=8720.358197, test/ctc_loss=0.46669164299964905, test/num_examples=2472, test/wer=0.160555, total_duration=9601.827583, train/ctc_loss=0.45372334122657776, train/wer=0.158383, validation/ctc_loss=0.7246454358100891, validation/num_examples=5348, validation/wer=0.217725
I1013 04:58:25.323591 139486628230912 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6168273091316223, loss=1.6464039087295532
I1013 05:04:51.922742 139486636623616 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6303461790084839, loss=1.6762007474899292
I1013 05:11:25.741147 139486628230912 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.633404016494751, loss=1.6105059385299683
I1013 05:18:21.913478 139487179343616 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6421586871147156, loss=1.625802993774414
I1013 05:21:23.370333 139660230973248 spec.py:321] Evaluating on the training split.
I1013 05:22:16.696403 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 05:23:06.905122 139660230973248 spec.py:349] Evaluating on the test split.
I1013 05:23:33.224116 139660230973248 submission_runner.py:381] Time since start: 11171.86s, 	Step: 12741, 	{'train/ctc_loss': Array(0.3876675, dtype=float32), 'train/wer': 0.1373449896837045, 'validation/ctc_loss': Array(0.68030334, dtype=float32), 'validation/wer': 0.2039189958417351, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43548384, dtype=float32), 'test/wer': 0.14629169444612805, 'test/num_examples': 2472, 'score': 10160.457416057587, 'total_duration': 11171.864776134491, 'accumulated_submission_time': 10160.457416057587, 'accumulated_eval_time': 1010.8280313014984, 'accumulated_logging_time': 0.34416723251342773}
I1013 05:23:33.258165 139486887503616 logging_writer.py:48] [12741] accumulated_eval_time=1010.828031, accumulated_logging_time=0.344167, accumulated_submission_time=10160.457416, global_step=12741, preemption_count=0, score=10160.457416, test/ctc_loss=0.43548384308815, test/num_examples=2472, test/wer=0.146292, total_duration=11171.864776, train/ctc_loss=0.3876675069332123, train/wer=0.137345, validation/ctc_loss=0.6803033351898193, validation/num_examples=5348, validation/wer=0.203919
I1013 05:26:50.356090 139486879110912 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5983283519744873, loss=1.6031928062438965
I1013 05:33:37.115996 139486887503616 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6439819931983948, loss=1.5339514017105103
I1013 05:40:10.234994 139486879110912 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6407451033592224, loss=1.6168519258499146
I1013 05:47:12.802931 139486887503616 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.8361098766326904, loss=1.5337892770767212
I1013 05:47:33.706396 139660230973248 spec.py:321] Evaluating on the training split.
I1013 05:48:27.536908 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 05:49:17.894612 139660230973248 spec.py:349] Evaluating on the test split.
I1013 05:49:44.077126 139660230973248 submission_runner.py:381] Time since start: 12742.72s, 	Step: 14529, 	{'train/ctc_loss': Array(0.32761952, dtype=float32), 'train/wer': 0.1211783977742531, 'validation/ctc_loss': Array(0.6463526, dtype=float32), 'validation/wer': 0.1942131617285261, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4045969, dtype=float32), 'test/wer': 0.13659413701840514, 'test/num_examples': 2472, 'score': 11600.862089395523, 'total_duration': 12742.716598272324, 'accumulated_submission_time': 11600.862089395523, 'accumulated_eval_time': 1141.1928191184998, 'accumulated_logging_time': 0.39309096336364746}
I1013 05:49:44.116900 139486887503616 logging_writer.py:48] [14529] accumulated_eval_time=1141.192819, accumulated_logging_time=0.393091, accumulated_submission_time=11600.862089, global_step=14529, preemption_count=0, score=11600.862089, test/ctc_loss=0.40459689497947693, test/num_examples=2472, test/wer=0.136594, total_duration=12742.716598, train/ctc_loss=0.3276195228099823, train/wer=0.121178, validation/ctc_loss=0.6463525891304016, validation/num_examples=5348, validation/wer=0.194213
I1013 05:55:42.169629 139486879110912 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8218300342559814, loss=1.6273326873779297
I1013 06:02:40.423280 139486887503616 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6570593118667603, loss=1.5302436351776123
I1013 06:09:07.166253 139486879110912 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5649638175964355, loss=1.5898295640945435
I1013 06:13:44.664531 139660230973248 spec.py:321] Evaluating on the training split.
I1013 06:14:38.672972 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 06:15:29.404932 139660230973248 spec.py:349] Evaluating on the test split.
I1013 06:15:55.912246 139660230973248 submission_runner.py:381] Time since start: 14314.55s, 	Step: 16325, 	{'train/ctc_loss': Array(0.3095974, dtype=float32), 'train/wer': 0.11256643382685443, 'validation/ctc_loss': Array(0.6278412, dtype=float32), 'validation/wer': 0.18638867717006435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38101736, dtype=float32), 'test/wer': 0.1274016607067095, 'test/num_examples': 2472, 'score': 13041.365608215332, 'total_duration': 14314.551568984985, 'accumulated_submission_time': 13041.365608215332, 'accumulated_eval_time': 1272.434442281723, 'accumulated_logging_time': 0.4481072425842285}
I1013 06:15:55.953101 139486457423616 logging_writer.py:48] [16325] accumulated_eval_time=1272.434442, accumulated_logging_time=0.448107, accumulated_submission_time=13041.365608, global_step=16325, preemption_count=0, score=13041.365608, test/ctc_loss=0.3810173571109772, test/num_examples=2472, test/wer=0.127402, total_duration=14314.551569, train/ctc_loss=0.3095974028110504, train/wer=0.112566, validation/ctc_loss=0.6278411746025085, validation/num_examples=5348, validation/wer=0.186389
I1013 06:18:12.736848 139486457423616 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7062687873840332, loss=1.5316801071166992
I1013 06:24:37.068500 139486449030912 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6155937910079956, loss=1.5148677825927734
I1013 06:31:43.522466 139486457423616 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6576269268989563, loss=1.484513759613037
I1013 06:38:07.069849 139486457423616 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6137845516204834, loss=1.5476274490356445
I1013 06:39:56.156380 139660230973248 spec.py:321] Evaluating on the training split.
I1013 06:40:49.084351 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 06:41:39.943810 139660230973248 spec.py:349] Evaluating on the test split.
I1013 06:42:06.429583 139660230973248 submission_runner.py:381] Time since start: 15885.07s, 	Step: 18128, 	{'train/ctc_loss': Array(0.30740917, dtype=float32), 'train/wer': 0.1153126290442813, 'validation/ctc_loss': Array(0.61150557, dtype=float32), 'validation/wer': 0.18498972493704716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37650645, dtype=float32), 'test/wer': 0.12715922177101643, 'test/num_examples': 2472, 'score': 14481.523082494736, 'total_duration': 15885.069373607635, 'accumulated_submission_time': 14481.523082494736, 'accumulated_eval_time': 1402.7020227909088, 'accumulated_logging_time': 0.5057213306427002}
I1013 06:42:06.464137 139486887503616 logging_writer.py:48] [18128] accumulated_eval_time=1402.702023, accumulated_logging_time=0.505721, accumulated_submission_time=14481.523082, global_step=18128, preemption_count=0, score=14481.523082, test/ctc_loss=0.3765064477920532, test/num_examples=2472, test/wer=0.127159, total_duration=15885.069374, train/ctc_loss=0.3074091672897339, train/wer=0.115313, validation/ctc_loss=0.6115055680274963, validation/num_examples=5348, validation/wer=0.184990
I1013 06:46:51.646140 139486879110912 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6258141398429871, loss=1.562441349029541
I1013 06:53:17.412417 139486887503616 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5618141293525696, loss=1.4741171598434448
I1013 07:00:24.791533 139486879110912 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.8173914551734924, loss=1.5218286514282227
I1013 07:06:06.977869 139660230973248 spec.py:321] Evaluating on the training split.
I1013 07:07:00.696026 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 07:07:50.949984 139660230973248 spec.py:349] Evaluating on the test split.
I1013 07:08:17.564105 139660230973248 submission_runner.py:381] Time since start: 17456.20s, 	Step: 19940, 	{'train/ctc_loss': Array(0.31152633, dtype=float32), 'train/wer': 0.11269996562574368, 'validation/ctc_loss': Array(0.5942979, dtype=float32), 'validation/wer': 0.17818792270065317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.365154, dtype=float32), 'test/wer': 0.12307816635351637, 'test/num_examples': 2472, 'score': 15921.992024421692, 'total_duration': 17456.20342564583, 'accumulated_submission_time': 15921.992024421692, 'accumulated_eval_time': 1533.2822642326355, 'accumulated_logging_time': 0.5559425354003906}
I1013 07:08:17.603094 139487041103616 logging_writer.py:48] [19940] accumulated_eval_time=1533.282264, accumulated_logging_time=0.555943, accumulated_submission_time=15921.992024, global_step=19940, preemption_count=0, score=15921.992024, test/ctc_loss=0.3651539981365204, test/num_examples=2472, test/wer=0.123078, total_duration=17456.203426, train/ctc_loss=0.3115263283252716, train/wer=0.112700, validation/ctc_loss=0.5942978858947754, validation/num_examples=5348, validation/wer=0.178188
I1013 07:09:03.739986 139487032710912 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6360064744949341, loss=1.4810413122177124
I1013 07:15:44.876006 139487041103616 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6814793944358826, loss=1.519087791442871
I1013 07:22:17.086898 139487041103616 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6324357986450195, loss=1.4754992723464966
I1013 07:29:15.697888 139487032710912 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6235476732254028, loss=1.50072181224823
I1013 07:32:18.088413 139660230973248 spec.py:321] Evaluating on the training split.
I1013 07:33:11.907081 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 07:34:02.812980 139660230973248 spec.py:349] Evaluating on the test split.
I1013 07:34:29.511609 139660230973248 submission_runner.py:381] Time since start: 19028.15s, 	Step: 21722, 	{'train/ctc_loss': Array(0.3033637, dtype=float32), 'train/wer': 0.10925158941620282, 'validation/ctc_loss': Array(0.57038325, dtype=float32), 'validation/wer': 0.17055639707088346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34617653, dtype=float32), 'test/wer': 0.11570398205951876, 'test/num_examples': 2472, 'score': 17362.43122434616, 'total_duration': 19028.152482509613, 'accumulated_submission_time': 17362.43122434616, 'accumulated_eval_time': 1664.7011005878448, 'accumulated_logging_time': 0.6112995147705078}
I1013 07:34:29.545379 139487471183616 logging_writer.py:48] [21722] accumulated_eval_time=1664.701101, accumulated_logging_time=0.611300, accumulated_submission_time=17362.431224, global_step=21722, preemption_count=0, score=17362.431224, test/ctc_loss=0.34617653489112854, test/num_examples=2472, test/wer=0.115704, total_duration=19028.152483, train/ctc_loss=0.30336371064186096, train/wer=0.109252, validation/ctc_loss=0.5703832507133484, validation/num_examples=5348, validation/wer=0.170556
I1013 07:38:00.631712 139487462790912 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.5668948292732239, loss=1.3832470178604126
I1013 07:44:46.889616 139487471183616 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8480413556098938, loss=1.4805972576141357
I1013 07:51:26.067265 139487471183616 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.648732602596283, loss=1.4154067039489746
I1013 07:58:22.276695 139487462790912 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6661996245384216, loss=1.4777474403381348
I1013 07:58:29.662959 139660230973248 spec.py:321] Evaluating on the training split.
I1013 07:59:24.564180 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 08:00:15.781174 139660230973248 spec.py:349] Evaluating on the test split.
I1013 08:00:42.307530 139660230973248 submission_runner.py:381] Time since start: 20600.95s, 	Step: 23510, 	{'train/ctc_loss': Array(0.2867725, dtype=float32), 'train/wer': 0.10359090441403436, 'validation/ctc_loss': Array(0.56134623, dtype=float32), 'validation/wer': 0.1679900433192795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33919716, dtype=float32), 'test/wer': 0.11144109744024891, 'test/num_examples': 2472, 'score': 18802.503789901733, 'total_duration': 20600.94754266739, 'accumulated_submission_time': 18802.503789901733, 'accumulated_eval_time': 1797.3402605056763, 'accumulated_logging_time': 0.66159987449646}
I1013 08:00:42.348802 139487471183616 logging_writer.py:48] [23510] accumulated_eval_time=1797.340261, accumulated_logging_time=0.661600, accumulated_submission_time=18802.503790, global_step=23510, preemption_count=0, score=18802.503790, test/ctc_loss=0.33919715881347656, test/num_examples=2472, test/wer=0.111441, total_duration=20600.947543, train/ctc_loss=0.2867724895477295, train/wer=0.103591, validation/ctc_loss=0.5613462328910828, validation/num_examples=5348, validation/wer=0.167990
I1013 08:06:57.964646 139487471183616 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6446349024772644, loss=1.4240237474441528
I1013 08:13:57.484802 139487462790912 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6498456597328186, loss=1.3554356098175049
I1013 08:20:45.076911 139487471183616 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6722506284713745, loss=1.424538493156433
I1013 08:24:42.365707 139660230973248 spec.py:321] Evaluating on the training split.
I1013 08:25:36.893053 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 08:26:27.876538 139660230973248 spec.py:349] Evaluating on the test split.
I1013 08:26:54.569637 139660230973248 submission_runner.py:381] Time since start: 22173.21s, 	Step: 25297, 	{'train/ctc_loss': Array(0.26029703, dtype=float32), 'train/wer': 0.0952398547358642, 'validation/ctc_loss': Array(0.53788465, dtype=float32), 'validation/wer': 0.1611014095649741, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32242474, dtype=float32), 'test/wer': 0.10855203345657312, 'test/num_examples': 2472, 'score': 20242.476589679718, 'total_duration': 22173.208388328552, 'accumulated_submission_time': 20242.476589679718, 'accumulated_eval_time': 1929.5375201702118, 'accumulated_logging_time': 0.7176668643951416}
I1013 08:26:54.612566 139487041103616 logging_writer.py:48] [25297] accumulated_eval_time=1929.537520, accumulated_logging_time=0.717667, accumulated_submission_time=20242.476590, global_step=25297, preemption_count=0, score=20242.476590, test/ctc_loss=0.3224247395992279, test/num_examples=2472, test/wer=0.108552, total_duration=22173.208388, train/ctc_loss=0.260297030210495, train/wer=0.095240, validation/ctc_loss=0.5378846526145935, validation/num_examples=5348, validation/wer=0.161101
I1013 08:29:29.303382 139487032710912 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.7532109022140503, loss=1.474370002746582
I1013 08:36:09.729463 139487041103616 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.9321318864822388, loss=1.3663437366485596
I1013 08:43:01.874916 139487032710912 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.8323405385017395, loss=1.4330567121505737
I1013 08:49:52.647217 139487041103616 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.679852306842804, loss=1.4142377376556396
I1013 08:50:55.230837 139660230973248 spec.py:321] Evaluating on the training split.
I1013 08:51:49.826710 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 08:52:40.638112 139660230973248 spec.py:349] Evaluating on the test split.
I1013 08:53:07.380860 139660230973248 submission_runner.py:381] Time since start: 23746.02s, 	Step: 27084, 	{'train/ctc_loss': Array(0.23703115, dtype=float32), 'train/wer': 0.0877070346122747, 'validation/ctc_loss': Array(0.5147594, dtype=float32), 'validation/wer': 0.15338305241729297, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30386835, dtype=float32), 'test/wer': 0.10273349899993939, 'test/num_examples': 2472, 'score': 21683.049283742905, 'total_duration': 23746.02119398117, 'accumulated_submission_time': 21683.049283742905, 'accumulated_eval_time': 2061.6824510097504, 'accumulated_logging_time': 0.7768373489379883}
I1013 08:53:07.418841 139487471183616 logging_writer.py:48] [27084] accumulated_eval_time=2061.682451, accumulated_logging_time=0.776837, accumulated_submission_time=21683.049284, global_step=27084, preemption_count=0, score=21683.049284, test/ctc_loss=0.3038683533668518, test/num_examples=2472, test/wer=0.102733, total_duration=23746.021194, train/ctc_loss=0.23703114688396454, train/wer=0.087707, validation/ctc_loss=0.5147594213485718, validation/num_examples=5348, validation/wer=0.153383
I1013 08:58:32.037334 139487462790912 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7093545794487, loss=1.411744475364685
I1013 09:05:25.765085 139487471183616 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6998460292816162, loss=1.3856512308120728
I1013 09:12:10.272766 139487462790912 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6596274971961975, loss=1.3585896492004395
I1013 09:17:07.428809 139660230973248 spec.py:321] Evaluating on the training split.
I1013 09:18:02.986835 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 09:18:54.211550 139660230973248 spec.py:349] Evaluating on the test split.
I1013 09:19:20.919220 139660230973248 submission_runner.py:381] Time since start: 25319.56s, 	Step: 28841, 	{'train/ctc_loss': Array(0.23476799, dtype=float32), 'train/wer': 0.08488713422180401, 'validation/ctc_loss': Array(0.50734174, dtype=float32), 'validation/wer': 0.15225424268444462, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30331925, dtype=float32), 'test/wer': 0.10164252378932057, 'test/num_examples': 2472, 'score': 23123.011386871338, 'total_duration': 25319.558921575546, 'accumulated_submission_time': 23123.011386871338, 'accumulated_eval_time': 2195.1673657894135, 'accumulated_logging_time': 0.8329625129699707}
I1013 09:19:20.955325 139487041103616 logging_writer.py:48] [28841] accumulated_eval_time=2195.167366, accumulated_logging_time=0.832963, accumulated_submission_time=23123.011387, global_step=28841, preemption_count=0, score=23123.011387, test/ctc_loss=0.3033192455768585, test/num_examples=2472, test/wer=0.101643, total_duration=25319.558922, train/ctc_loss=0.23476798832416534, train/wer=0.084887, validation/ctc_loss=0.507341742515564, validation/num_examples=5348, validation/wer=0.152254
I1013 09:21:22.083090 139487032710912 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.0851954221725464, loss=1.384168267250061
I1013 09:27:48.205239 139487041103616 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.5504814982414246, loss=1.3442254066467285
I1013 09:34:49.057260 139487041103616 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.8503648042678833, loss=1.3570890426635742
I1013 09:41:22.434187 139487032710912 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6705419421195984, loss=1.3899253606796265
I1013 09:43:21.023843 139660230973248 spec.py:321] Evaluating on the training split.
I1013 09:44:14.658832 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 09:45:05.590365 139660230973248 spec.py:349] Evaluating on the test split.
I1013 09:45:32.310980 139660230973248 submission_runner.py:381] Time since start: 26890.95s, 	Step: 30641, 	{'train/ctc_loss': Array(0.24080803, dtype=float32), 'train/wer': 0.08729307505633559, 'validation/ctc_loss': Array(0.49346223, dtype=float32), 'validation/wer': 0.14717942285984428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28759897, dtype=float32), 'test/wer': 0.0962482574701497, 'test/num_examples': 2472, 'score': 24563.034650802612, 'total_duration': 26890.950448989868, 'accumulated_submission_time': 24563.034650802612, 'accumulated_eval_time': 2326.448561191559, 'accumulated_logging_time': 0.8856112957000732}
I1013 09:45:32.352813 139487041103616 logging_writer.py:48] [30641] accumulated_eval_time=2326.448561, accumulated_logging_time=0.885611, accumulated_submission_time=24563.034651, global_step=30641, preemption_count=0, score=24563.034651, test/ctc_loss=0.28759896755218506, test/num_examples=2472, test/wer=0.096248, total_duration=26890.950449, train/ctc_loss=0.24080802500247955, train/wer=0.087293, validation/ctc_loss=0.4934622347354889, validation/num_examples=5348, validation/wer=0.147179
I1013 09:50:08.653634 139487041103616 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.8452942967414856, loss=1.3791989088058472
I1013 09:56:38.927592 139487032710912 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6738353371620178, loss=1.340259313583374
I1013 10:03:39.727923 139487041103616 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6855045557022095, loss=1.3225880861282349
I1013 10:09:32.947284 139660230973248 spec.py:321] Evaluating on the training split.
I1013 10:10:26.330975 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 10:11:18.137143 139660230973248 spec.py:349] Evaluating on the test split.
I1013 10:11:44.701970 139660230973248 submission_runner.py:381] Time since start: 28463.34s, 	Step: 32464, 	{'train/ctc_loss': Array(0.21947882, dtype=float32), 'train/wer': 0.07833466269485999, 'validation/ctc_loss': Array(0.48267198, dtype=float32), 'validation/wer': 0.14350355526826114, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27600875, dtype=float32), 'test/wer': 0.09196516960623877, 'test/num_examples': 2472, 'score': 26003.58583521843, 'total_duration': 28463.34000825882, 'accumulated_submission_time': 26003.58583521843, 'accumulated_eval_time': 2458.195885658264, 'accumulated_logging_time': 0.9412505626678467}
I1013 10:11:44.750679 139487471183616 logging_writer.py:48] [32464] accumulated_eval_time=2458.195886, accumulated_logging_time=0.941251, accumulated_submission_time=26003.585835, global_step=32464, preemption_count=0, score=26003.585835, test/ctc_loss=0.2760087549686432, test/num_examples=2472, test/wer=0.091965, total_duration=28463.340008, train/ctc_loss=0.2194788157939911, train/wer=0.078335, validation/ctc_loss=0.48267197608947754, validation/num_examples=5348, validation/wer=0.143504
I1013 10:12:12.924712 139487462790912 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.6508094072341919, loss=1.3598161935806274
I1013 10:18:50.593001 139487471183616 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7338114380836487, loss=1.3319671154022217
I1013 10:25:18.154181 139487462790912 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7617030143737793, loss=1.3003312349319458
I1013 10:32:32.024703 139487471183616 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7101126313209534, loss=1.2901291847229004
I1013 10:35:45.075714 139660230973248 spec.py:321] Evaluating on the training split.
I1013 10:36:39.723830 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 10:37:30.285761 139660230973248 spec.py:349] Evaluating on the test split.
I1013 10:37:56.627450 139660230973248 submission_runner.py:381] Time since start: 30035.27s, 	Step: 34256, 	{'train/ctc_loss': Array(0.22801153, dtype=float32), 'train/wer': 0.0817233355079648, 'validation/ctc_loss': Array(0.4684329, dtype=float32), 'validation/wer': 0.1390462040154753, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27156022, dtype=float32), 'test/wer': 0.0905509424813625, 'test/num_examples': 2472, 'score': 27443.854074001312, 'total_duration': 30035.26726746559, 'accumulated_submission_time': 27443.854074001312, 'accumulated_eval_time': 2589.7420234680176, 'accumulated_logging_time': 1.0170419216156006}
I1013 10:37:56.660786 139487041103616 logging_writer.py:48] [34256] accumulated_eval_time=2589.742023, accumulated_logging_time=1.017042, accumulated_submission_time=27443.854074, global_step=34256, preemption_count=0, score=27443.854074, test/ctc_loss=0.2715602219104767, test/num_examples=2472, test/wer=0.090551, total_duration=30035.267267, train/ctc_loss=0.22801153361797333, train/wer=0.081723, validation/ctc_loss=0.4684329032897949, validation/num_examples=5348, validation/wer=0.139046
I1013 10:41:02.141640 139487032710912 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.8300617337226868, loss=1.2831400632858276
I1013 10:48:04.040102 139487041103616 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.8885362148284912, loss=1.3034672737121582
I1013 10:54:28.458513 139487041103616 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.8591276407241821, loss=1.3228211402893066
I1013 11:01:35.155339 139487032710912 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7440558075904846, loss=1.2661476135253906
I1013 11:01:57.076518 139660230973248 spec.py:321] Evaluating on the training split.
I1013 11:02:50.692224 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 11:03:42.204857 139660230973248 spec.py:349] Evaluating on the test split.
I1013 11:04:08.642672 139660230973248 submission_runner.py:381] Time since start: 31607.28s, 	Step: 36028, 	{'train/ctc_loss': Array(0.2100577, dtype=float32), 'train/wer': 0.07396336200006394, 'validation/ctc_loss': Array(0.44706374, dtype=float32), 'validation/wer': 0.13241806481490415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25340295, dtype=float32), 'test/wer': 0.0848536274925753, 'test/num_examples': 2472, 'score': 28884.225048542023, 'total_duration': 31607.28241801262, 'accumulated_submission_time': 28884.225048542023, 'accumulated_eval_time': 2721.302494764328, 'accumulated_logging_time': 1.0658469200134277}
I1013 11:04:08.677536 139487471183616 logging_writer.py:48] [36028] accumulated_eval_time=2721.302495, accumulated_logging_time=1.065847, accumulated_submission_time=28884.225049, global_step=36028, preemption_count=0, score=28884.225049, test/ctc_loss=0.2534029483795166, test/num_examples=2472, test/wer=0.084854, total_duration=31607.282418, train/ctc_loss=0.21005770564079285, train/wer=0.073963, validation/ctc_loss=0.44706374406814575, validation/num_examples=5348, validation/wer=0.132418
I1013 11:10:13.346003 139487471183616 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.8160769939422607, loss=1.2651997804641724
I1013 11:17:21.182492 139487462790912 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.9445805549621582, loss=1.2839573621749878
I1013 11:23:51.517486 139487471183616 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.8651811480522156, loss=1.2604668140411377
I1013 11:28:08.647414 139660230973248 spec.py:321] Evaluating on the training split.
I1013 11:29:04.138592 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 11:29:55.364108 139660230973248 spec.py:349] Evaluating on the test split.
I1013 11:30:22.158905 139660230973248 submission_runner.py:381] Time since start: 33180.80s, 	Step: 37805, 	{'train/ctc_loss': Array(0.15897675, dtype=float32), 'train/wer': 0.05832128983323596, 'validation/ctc_loss': Array(0.43366468, dtype=float32), 'validation/wer': 0.12898339588418606, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2495774, dtype=float32), 'test/wer': 0.08430813988726589, 'test/num_examples': 2472, 'score': 30324.14963078499, 'total_duration': 33180.79685497284, 'accumulated_submission_time': 30324.14963078499, 'accumulated_eval_time': 2854.8065383434296, 'accumulated_logging_time': 1.115863561630249}
I1013 11:30:22.198472 139487041103616 logging_writer.py:48] [37805] accumulated_eval_time=2854.806538, accumulated_logging_time=1.115864, accumulated_submission_time=30324.149631, global_step=37805, preemption_count=0, score=30324.149631, test/ctc_loss=0.24957740306854248, test/num_examples=2472, test/wer=0.084308, total_duration=33180.796855, train/ctc_loss=0.158976748585701, train/wer=0.058321, validation/ctc_loss=0.4336646795272827, validation/num_examples=5348, validation/wer=0.128983
I1013 11:32:50.524488 139487032710912 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.8549069166183472, loss=1.249142050743103
I1013 11:39:13.684743 139487041103616 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7861719727516174, loss=1.2202937602996826
I1013 11:46:21.837234 139487032710912 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.6176162958145142, loss=1.2102807760238647
I1013 11:52:58.935293 139487041103616 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.7810534238815308, loss=1.1872618198394775
I1013 11:54:22.389376 139660230973248 spec.py:321] Evaluating on the training split.
I1013 11:55:17.064210 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 11:56:08.884029 139660230973248 spec.py:349] Evaluating on the test split.
I1013 11:56:35.962044 139660230973248 submission_runner.py:381] Time since start: 34754.60s, 	Step: 39609, 	{'train/ctc_loss': Array(0.17318016, dtype=float32), 'train/wer': 0.0620472640493337, 'validation/ctc_loss': Array(0.4152827, dtype=float32), 'validation/wer': 0.12227807311213808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23602532, dtype=float32), 'test/wer': 0.07873204436632524, 'test/num_examples': 2472, 'score': 31764.295923948288, 'total_duration': 34754.60251188278, 'accumulated_submission_time': 31764.295923948288, 'accumulated_eval_time': 2988.3742775917053, 'accumulated_logging_time': 1.1702988147735596}
I1013 11:56:36.004697 139487471183616 logging_writer.py:48] [39609] accumulated_eval_time=2988.374278, accumulated_logging_time=1.170299, accumulated_submission_time=31764.295924, global_step=39609, preemption_count=0, score=31764.295924, test/ctc_loss=0.23602531850337982, test/num_examples=2472, test/wer=0.078732, total_duration=34754.602512, train/ctc_loss=0.17318016290664673, train/wer=0.062047, validation/ctc_loss=0.4152826964855194, validation/num_examples=5348, validation/wer=0.122278
I1013 12:01:41.996770 139487462790912 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.8068749904632568, loss=1.2225465774536133
I1013 12:08:22.663408 139487471183616 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6986751556396484, loss=1.183501958847046
I1013 12:15:28.525283 139487462790912 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.697361409664154, loss=1.200828194618225
I1013 12:20:36.703896 139660230973248 spec.py:321] Evaluating on the training split.
I1013 12:21:30.315346 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 12:22:21.614507 139660230973248 spec.py:349] Evaluating on the test split.
I1013 12:22:48.495042 139660230973248 submission_runner.py:381] Time since start: 36327.13s, 	Step: 41375, 	{'train/ctc_loss': Array(0.20501196, dtype=float32), 'train/wer': 0.0734312199302463, 'validation/ctc_loss': Array(0.40115854, dtype=float32), 'validation/wer': 0.11899777132437361, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22964244, dtype=float32), 'test/wer': 0.07592379336121381, 'test/num_examples': 2472, 'score': 33204.95049738884, 'total_duration': 36327.13359904289, 'accumulated_submission_time': 33204.95049738884, 'accumulated_eval_time': 3120.1587953567505, 'accumulated_logging_time': 1.2276079654693604}
I1013 12:22:48.532076 139487471183616 logging_writer.py:48] [41375] accumulated_eval_time=3120.158795, accumulated_logging_time=1.227608, accumulated_submission_time=33204.950497, global_step=41375, preemption_count=0, score=33204.950497, test/ctc_loss=0.22964243590831757, test/num_examples=2472, test/wer=0.075924, total_duration=36327.133599, train/ctc_loss=0.20501196384429932, train/wer=0.073431, validation/ctc_loss=0.40115854144096375, validation/num_examples=5348, validation/wer=0.118998
I1013 12:24:23.920204 139487462790912 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8473681807518005, loss=1.1609641313552856
I1013 12:31:07.405383 139487471183616 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.5716931819915771, loss=1.152536392211914
I1013 12:37:54.371336 139487471183616 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.8382416367530823, loss=1.204166293144226
I1013 12:44:46.660272 139487462790912 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6929397583007812, loss=1.1425834894180298
I1013 12:46:49.217797 139660230973248 spec.py:321] Evaluating on the training split.
I1013 12:47:42.524984 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 12:48:34.142934 139660230973248 spec.py:349] Evaluating on the test split.
I1013 12:49:00.840414 139660230973248 submission_runner.py:381] Time since start: 37899.48s, 	Step: 43145, 	{'train/ctc_loss': Array(0.19973153, dtype=float32), 'train/wer': 0.07101261817394539, 'validation/ctc_loss': Array(0.3852075, dtype=float32), 'validation/wer': 0.11362386516030062, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21550332, dtype=float32), 'test/wer': 0.07143867305089198, 'test/num_examples': 2472, 'score': 34645.59170675278, 'total_duration': 37899.478531360626, 'accumulated_submission_time': 34645.59170675278, 'accumulated_eval_time': 3251.7741045951843, 'accumulated_logging_time': 1.278609275817871}
I1013 12:49:00.879292 139487471183616 logging_writer.py:48] [43145] accumulated_eval_time=3251.774105, accumulated_logging_time=1.278609, accumulated_submission_time=34645.591707, global_step=43145, preemption_count=0, score=34645.591707, test/ctc_loss=0.21550332009792328, test/num_examples=2472, test/wer=0.071439, total_duration=37899.478531, train/ctc_loss=0.19973152875900269, train/wer=0.071013, validation/ctc_loss=0.38520750403404236, validation/num_examples=5348, validation/wer=0.113624
I1013 12:53:34.322314 139487471183616 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.783557116985321, loss=1.1989574432373047
I1013 13:00:26.502087 139487462790912 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.7797800302505493, loss=1.1406073570251465
I1013 13:07:19.548889 139487471183616 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.2663249969482422, loss=1.1110848188400269
I1013 13:13:01.971498 139660230973248 spec.py:321] Evaluating on the training split.
I1013 13:13:53.355548 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 13:14:45.469937 139660230973248 spec.py:349] Evaluating on the test split.
I1013 13:15:12.143829 139660230973248 submission_runner.py:381] Time since start: 39470.78s, 	Step: 44924, 	{'train/ctc_loss': Array(0.22303014, dtype=float32), 'train/wer': 0.08178951863399454, 'validation/ctc_loss': Array(0.37065703, dtype=float32), 'validation/wer': 0.10893496319308435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21001306, dtype=float32), 'test/wer': 0.06927692587429542, 'test/num_examples': 2472, 'score': 36086.64011669159, 'total_duration': 39470.7828605175, 'accumulated_submission_time': 36086.64011669159, 'accumulated_eval_time': 3381.940108537674, 'accumulated_logging_time': 1.33180570602417}
I1013 13:15:12.182120 139487471183616 logging_writer.py:48] [44924] accumulated_eval_time=3381.940109, accumulated_logging_time=1.331806, accumulated_submission_time=36086.640117, global_step=44924, preemption_count=0, score=36086.640117, test/ctc_loss=0.21001306176185608, test/num_examples=2472, test/wer=0.069277, total_duration=39470.782861, train/ctc_loss=0.22303013503551483, train/wer=0.081790, validation/ctc_loss=0.3706570267677307, validation/num_examples=5348, validation/wer=0.108935
I1013 13:16:10.578817 139487462790912 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.8276440501213074, loss=1.1469786167144775
I1013 13:22:41.501819 139487471183616 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.027902364730835, loss=1.0860878229141235
I1013 13:29:25.045398 139487462790912 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.8955410718917847, loss=1.101762294769287
I1013 13:36:23.757954 139487471183616 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.8169127106666565, loss=1.0665174722671509
I1013 13:39:12.186944 139660230973248 spec.py:321] Evaluating on the training split.
I1013 13:40:03.263377 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 13:40:54.142588 139660230973248 spec.py:349] Evaluating on the test split.
I1013 13:41:21.232639 139660230973248 submission_runner.py:381] Time since start: 41039.87s, 	Step: 46724, 	{'train/ctc_loss': Array(0.18576692, dtype=float32), 'train/wer': 0.06542870431971928, 'validation/ctc_loss': Array(0.3589722, dtype=float32), 'validation/wer': 0.10608881899487695, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19952978, dtype=float32), 'test/wer': 0.06553932561569388, 'test/num_examples': 2472, 'score': 37526.59911942482, 'total_duration': 41039.873332977295, 'accumulated_submission_time': 37526.59911942482, 'accumulated_eval_time': 3510.981105566025, 'accumulated_logging_time': 1.386012077331543}
I1013 13:41:21.273075 139487471183616 logging_writer.py:48] [46724] accumulated_eval_time=3510.981106, accumulated_logging_time=1.386012, accumulated_submission_time=37526.599119, global_step=46724, preemption_count=0, score=37526.599119, test/ctc_loss=0.19952978193759918, test/num_examples=2472, test/wer=0.065539, total_duration=41039.873333, train/ctc_loss=0.18576692044734955, train/wer=0.065429, validation/ctc_loss=0.3589721918106079, validation/num_examples=5348, validation/wer=0.106089
I1013 13:44:50.934306 139487462790912 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.8212813138961792, loss=1.1301155090332031
I1013 13:51:52.674535 139487471183616 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.8321386575698853, loss=1.1014647483825684
I1013 13:58:27.514975 139487462790912 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.7898265719413757, loss=1.1074128150939941
I1013 14:05:21.887032 139660230973248 spec.py:321] Evaluating on the training split.
I1013 14:06:14.492403 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 14:07:05.698548 139660230973248 spec.py:349] Evaluating on the test split.
I1013 14:07:32.368237 139660230973248 submission_runner.py:381] Time since start: 42611.01s, 	Step: 48486, 	{'train/ctc_loss': Array(0.16211706, dtype=float32), 'train/wer': 0.06009709062162493, 'validation/ctc_loss': Array(0.35218254, dtype=float32), 'validation/wer': 0.1016990033671333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19315596, dtype=float32), 'test/wer': 0.06444835040507506, 'test/num_examples': 2472, 'score': 38967.16755080223, 'total_duration': 42611.00805640221, 'accumulated_submission_time': 38967.16755080223, 'accumulated_eval_time': 3641.4568905830383, 'accumulated_logging_time': 1.4418017864227295}
I1013 14:07:32.413481 139487471183616 logging_writer.py:48] [48486] accumulated_eval_time=3641.456891, accumulated_logging_time=1.441802, accumulated_submission_time=38967.167551, global_step=48486, preemption_count=0, score=38967.167551, test/ctc_loss=0.19315595924854279, test/num_examples=2472, test/wer=0.064448, total_duration=42611.008056, train/ctc_loss=0.16211706399917603, train/wer=0.060097, validation/ctc_loss=0.352182537317276, validation/num_examples=5348, validation/wer=0.101699
I1013 14:07:43.843032 139487462790912 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0714071989059448, loss=1.0260838270187378
I1013 14:14:05.725645 139487471183616 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.7971009612083435, loss=1.041334629058838
I1013 14:21:14.644937 139487471183616 logging_writer.py:48] [49500] global_step=49500, grad_norm=2.249093532562256, loss=1.091539978981018
I1013 14:27:41.795604 139487462790912 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.7078208327293396, loss=1.0679490566253662
I1013 14:31:32.898731 139660230973248 spec.py:321] Evaluating on the training split.
I1013 14:32:26.725432 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 14:33:18.472023 139660230973248 spec.py:349] Evaluating on the test split.
I1013 14:33:45.376915 139660230973248 submission_runner.py:381] Time since start: 44184.02s, 	Step: 50267, 	{'train/ctc_loss': Array(0.13102518, dtype=float32), 'train/wer': 0.04839095585596363, 'validation/ctc_loss': Array(0.33665368, dtype=float32), 'validation/wer': 0.0975793302395585, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18713242, dtype=float32), 'test/wer': 0.06248863567488939, 'test/num_examples': 2472, 'score': 40407.606982946396, 'total_duration': 44184.01608181, 'accumulated_submission_time': 40407.606982946396, 'accumulated_eval_time': 3773.9288313388824, 'accumulated_logging_time': 1.5033035278320312}
I1013 14:33:45.419175 139487041103616 logging_writer.py:48] [50267] accumulated_eval_time=3773.928831, accumulated_logging_time=1.503304, accumulated_submission_time=40407.606983, global_step=50267, preemption_count=0, score=40407.606983, test/ctc_loss=0.18713241815567017, test/num_examples=2472, test/wer=0.062489, total_duration=44184.016082, train/ctc_loss=0.13102518022060394, train/wer=0.048391, validation/ctc_loss=0.3366536796092987, validation/num_examples=5348, validation/wer=0.097579
I1013 14:36:46.409646 139487041103616 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.209912657737732, loss=1.0718555450439453
I1013 14:43:13.862637 139487032710912 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.8882470726966858, loss=1.0185692310333252
I1013 14:50:26.978932 139487041103616 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.1208044290542603, loss=1.0343104600906372
I1013 14:56:52.098548 139487032710912 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.2135241031646729, loss=1.0271438360214233
I1013 14:57:45.776442 139660230973248 spec.py:321] Evaluating on the training split.
I1013 14:58:39.395881 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 14:59:30.937173 139660230973248 spec.py:349] Evaluating on the test split.
I1013 14:59:57.586438 139660230973248 submission_runner.py:381] Time since start: 45756.22s, 	Step: 52064, 	{'train/ctc_loss': Array(0.13984925, dtype=float32), 'train/wer': 0.052034736893557164, 'validation/ctc_loss': Array(0.3301808, dtype=float32), 'validation/wer': 0.09555326148829221, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17835274, dtype=float32), 'test/wer': 0.058468189991312605, 'test/num_examples': 2472, 'score': 41847.9191968441, 'total_duration': 45756.22497534752, 'accumulated_submission_time': 41847.9191968441, 'accumulated_eval_time': 3905.7319672107697, 'accumulated_logging_time': 1.5603466033935547}
I1013 14:59:57.629749 139487471183616 logging_writer.py:48] [52064] accumulated_eval_time=3905.731967, accumulated_logging_time=1.560347, accumulated_submission_time=41847.919197, global_step=52064, preemption_count=0, score=41847.919197, test/ctc_loss=0.1783527433872223, test/num_examples=2472, test/wer=0.058468, total_duration=45756.224975, train/ctc_loss=0.1398492455482483, train/wer=0.052035, validation/ctc_loss=0.3301807940006256, validation/num_examples=5348, validation/wer=0.095553
I1013 15:05:41.692191 139487462790912 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.040110468864441, loss=1.0177277326583862
I1013 15:12:09.777314 139487471183616 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.9990237355232239, loss=1.0148588418960571
I1013 15:19:19.375869 139487462790912 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.2090498208999634, loss=0.9489331841468811
I1013 15:23:57.636674 139660230973248 spec.py:321] Evaluating on the training split.
I1013 15:24:51.564800 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 15:25:42.794039 139660230973248 spec.py:349] Evaluating on the test split.
I1013 15:26:09.566304 139660230973248 submission_runner.py:381] Time since start: 47328.21s, 	Step: 53857, 	{'train/ctc_loss': Array(0.12152492, dtype=float32), 'train/wer': 0.044537021130389966, 'validation/ctc_loss': Array(0.3229883, dtype=float32), 'validation/wer': 0.09346930505841831, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17643908, dtype=float32), 'test/wer': 0.05798331211992646, 'test/num_examples': 2472, 'score': 43287.88098239899, 'total_duration': 47328.20605754852, 'accumulated_submission_time': 43287.88098239899, 'accumulated_eval_time': 4037.6561448574066, 'accumulated_logging_time': 1.6179511547088623}
I1013 15:26:09.606400 139487471183616 logging_writer.py:48] [53857] accumulated_eval_time=4037.656145, accumulated_logging_time=1.617951, accumulated_submission_time=43287.880982, global_step=53857, preemption_count=0, score=43287.880982, test/ctc_loss=0.1764390766620636, test/num_examples=2472, test/wer=0.057983, total_duration=47328.206058, train/ctc_loss=0.12152491509914398, train/wer=0.044537, validation/ctc_loss=0.3229883015155792, validation/num_examples=5348, validation/wer=0.093469
I1013 15:27:58.634571 139487462790912 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.8626266717910767, loss=1.027967095375061
I1013 15:34:47.216919 139487471183616 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.7713797688484192, loss=0.9830743074417114
I1013 15:41:17.986579 139487471183616 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.3453831672668457, loss=0.9783636331558228
I1013 15:48:21.919320 139487462790912 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.1105672121047974, loss=0.981683611869812
I1013 15:50:10.029996 139660230973248 spec.py:321] Evaluating on the training split.
I1013 15:51:03.656160 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 15:51:55.128790 139660230973248 spec.py:349] Evaluating on the test split.
I1013 15:52:22.024690 139660230973248 submission_runner.py:381] Time since start: 48900.66s, 	Step: 55625, 	{'train/ctc_loss': Array(0.12235944, dtype=float32), 'train/wer': 0.045498432397834646, 'validation/ctc_loss': Array(0.3146154, dtype=float32), 'validation/wer': 0.09073893621742612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17075971, dtype=float32), 'test/wer': 0.056064003879022974, 'test/num_examples': 2472, 'score': 44728.258295059204, 'total_duration': 48900.66444969177, 'accumulated_submission_time': 44728.258295059204, 'accumulated_eval_time': 4169.645361185074, 'accumulated_logging_time': 1.6745784282684326}
I1013 15:52:22.062072 139487041103616 logging_writer.py:48] [55625] accumulated_eval_time=4169.645361, accumulated_logging_time=1.674578, accumulated_submission_time=44728.258295, global_step=55625, preemption_count=0, score=44728.258295, test/ctc_loss=0.17075970768928528, test/num_examples=2472, test/wer=0.056064, total_duration=48900.664450, train/ctc_loss=0.12235943973064423, train/wer=0.045498, validation/ctc_loss=0.314615398645401, validation/num_examples=5348, validation/wer=0.090739
I1013 15:57:06.814922 139487032710912 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.7927950620651245, loss=1.0065041780471802
I1013 16:03:57.363550 139487041103616 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.5870122909545898, loss=1.0039706230163574
I1013 16:10:34.457376 139487041103616 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1328463554382324, loss=0.9788295030593872
I1013 16:16:22.051745 139660230973248 spec.py:321] Evaluating on the training split.
I1013 16:17:15.261920 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 16:18:06.707662 139660230973248 spec.py:349] Evaluating on the test split.
I1013 16:18:33.545757 139660230973248 submission_runner.py:381] Time since start: 50472.19s, 	Step: 57420, 	{'train/ctc_loss': Array(0.11635064, dtype=float32), 'train/wer': 0.04343409247757074, 'validation/ctc_loss': Array(0.31374025, dtype=float32), 'validation/wer': 0.089967100502658, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16906595, dtype=float32), 'test/wer': 0.055316483827302664, 'test/num_examples': 2472, 'score': 46168.2030479908, 'total_duration': 50472.18540120125, 'accumulated_submission_time': 46168.2030479908, 'accumulated_eval_time': 4301.133611679077, 'accumulated_logging_time': 1.7258071899414062}
I1013 16:18:33.590958 139487471183616 logging_writer.py:48] [57420] accumulated_eval_time=4301.133612, accumulated_logging_time=1.725807, accumulated_submission_time=46168.203048, global_step=57420, preemption_count=0, score=46168.203048, test/ctc_loss=0.1690659523010254, test/num_examples=2472, test/wer=0.055316, total_duration=50472.185401, train/ctc_loss=0.11635063588619232, train/wer=0.043434, validation/ctc_loss=0.31374025344848633, validation/num_examples=5348, validation/wer=0.089967
I1013 16:19:35.017873 139487462790912 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.7474393844604492, loss=0.9601288437843323
I1013 16:25:57.674852 139487471183616 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.035680890083313, loss=0.9590335488319397
I1013 16:32:54.666495 139487462790912 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.8000935912132263, loss=0.9538116455078125
I1013 16:39:36.861894 139487471183616 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.8146976232528687, loss=0.9624207615852356
I1013 16:42:34.267955 139660230973248 spec.py:321] Evaluating on the training split.
I1013 16:43:26.647663 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 16:44:18.455887 139660230973248 spec.py:349] Evaluating on the test split.
I1013 16:44:45.479330 139660230973248 submission_runner.py:381] Time since start: 52044.12s, 	Step: 59229, 	{'train/ctc_loss': Array(0.12494056, dtype=float32), 'train/wer': 0.046106475281297576, 'validation/ctc_loss': Array(0.31217274, dtype=float32), 'validation/wer': 0.08951364702023175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16805044, dtype=float32), 'test/wer': 0.05471038648806999, 'test/num_examples': 2472, 'score': 47608.83378863335, 'total_duration': 52044.11736369133, 'accumulated_submission_time': 47608.83378863335, 'accumulated_eval_time': 4432.337597131729, 'accumulated_logging_time': 1.786052942276001}
I1013 16:44:45.520985 139487041103616 logging_writer.py:48] [59229] accumulated_eval_time=4432.337597, accumulated_logging_time=1.786053, accumulated_submission_time=47608.833789, global_step=59229, preemption_count=0, score=47608.833789, test/ctc_loss=0.16805043816566467, test/num_examples=2472, test/wer=0.054710, total_duration=52044.117364, train/ctc_loss=0.12494055926799774, train/wer=0.046106, validation/ctc_loss=0.3121727406978607, validation/num_examples=5348, validation/wer=0.089514
I1013 16:48:12.049347 139487032710912 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.9057866334915161, loss=0.9717590808868408
I1013 16:54:51.709614 139660230973248 spec.py:321] Evaluating on the training split.
I1013 16:55:44.950413 139660230973248 spec.py:333] Evaluating on the validation split.
I1013 16:56:36.504598 139660230973248 spec.py:349] Evaluating on the test split.
I1013 16:57:03.156168 139660230973248 submission_runner.py:381] Time since start: 52781.80s, 	Step: 60000, 	{'train/ctc_loss': Array(0.13582408, dtype=float32), 'train/wer': 0.04959213234733552, 'validation/ctc_loss': Array(0.3120159, dtype=float32), 'validation/wer': 0.08961012648457775, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16800222, dtype=float32), 'test/wer': 0.05458916702022345, 'test/num_examples': 2472, 'score': 48214.99224925041, 'total_duration': 52781.79877066612, 'accumulated_submission_time': 48214.99224925041, 'accumulated_eval_time': 4563.781610965729, 'accumulated_logging_time': 1.84389328956604}
I1013 16:57:03.189922 139487471183616 logging_writer.py:48] [60000] accumulated_eval_time=4563.781611, accumulated_logging_time=1.843893, accumulated_submission_time=48214.992249, global_step=60000, preemption_count=0, score=48214.992249, test/ctc_loss=0.16800221800804138, test/num_examples=2472, test/wer=0.054589, total_duration=52781.798771, train/ctc_loss=0.1358240842819214, train/wer=0.049592, validation/ctc_loss=0.3120158910751343, validation/num_examples=5348, validation/wer=0.089610
I1013 16:57:03.213084 139487462790912 logging_writer.py:48] [60000] global_step=60000, preemption_count=0, score=48214.992249
I1013 16:57:03.670581 139660230973248 checkpoints.py:490] Saving checkpoint at step: 60000
I1013 16:57:05.188916 139660230973248 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_conformer/adamw_run0/librispeech_conformer_jax/trial_1/checkpoint_60000
I1013 16:57:05.223634 139660230973248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_conformer/adamw_run0/librispeech_conformer_jax/trial_1/checkpoint_60000.
I1013 16:57:06.606485 139660230973248 submission_runner.py:549] Tuning trial 1/1
I1013 16:57:06.606751 139660230973248 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, beta1=0.9731333693827139, beta2=0.9981232922116359, warmup_steps=9999, weight_decay=0.16375311233774334)
I1013 16:57:06.626548 139660230973248 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.282171, dtype=float32), 'train/wer': 0.9550475813672615, 'validation/ctc_loss': Array(30.486477, dtype=float32), 'validation/wer': 1.0443709056527317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.603224, dtype=float32), 'test/wer': 1.0379416934359658, 'test/num_examples': 2472, 'score': 78.62688779830933, 'total_duration': 204.48333477973938, 'accumulated_submission_time': 78.62688779830933, 'accumulated_eval_time': 125.85637426376343, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1825, {'train/ctc_loss': Array(6.392737, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.480569, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.4471064, dtype=float32), 'test/wer': 0.8947815019092066, 'test/num_examples': 2472, 'score': 1518.5892317295074, 'total_duration': 1747.9132425785065, 'accumulated_submission_time': 1518.5892317295074, 'accumulated_eval_time': 229.25212001800537, 'accumulated_logging_time': 0.04096341133117676, 'global_step': 1825, 'preemption_count': 0}), (3679, {'train/ctc_loss': Array(2.0757494, dtype=float32), 'train/wer': 0.5652664944197321, 'validation/ctc_loss': Array(2.3676524, dtype=float32), 'validation/wer': 0.5774778338430665, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.0936549, dtype=float32), 'test/wer': 0.5396892740974201, 'test/num_examples': 2472, 'score': 2959.6147639751434, 'total_duration': 3321.188063621521, 'accumulated_submission_time': 2959.6147639751434, 'accumulated_eval_time': 361.41952252388, 'accumulated_logging_time': 0.08943367004394531, 'global_step': 3679, 'preemption_count': 0}), (5531, {'train/ctc_loss': Array(0.7355601, dtype=float32), 'train/wer': 0.2515773993073675, 'validation/ctc_loss': Array(1.0863374, dtype=float32), 'validation/wer': 0.31641405126918737, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.79588956, dtype=float32), 'test/wer': 0.25601551609188433, 'test/num_examples': 2472, 'score': 4399.716722488403, 'total_duration': 4890.447804689407, 'accumulated_submission_time': 4399.716722488403, 'accumulated_eval_time': 490.494099855423, 'accumulated_logging_time': 0.13850045204162598, 'global_step': 5531, 'preemption_count': 0}), (7361, {'train/ctc_loss': Array(0.5353512, dtype=float32), 'train/wer': 0.18389552723350333, 'validation/ctc_loss': Array(0.86038196, dtype=float32), 'validation/wer': 0.2568090381962199, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5859844, dtype=float32), 'test/wer': 0.19255712467422267, 'test/num_examples': 2472, 'score': 5839.638425111771, 'total_duration': 6461.057765007019, 'accumulated_submission_time': 5839.638425111771, 'accumulated_eval_time': 621.09885430336, 'accumulated_logging_time': 0.18726372718811035, 'global_step': 7361, 'preemption_count': 0}), (9132, {'train/ctc_loss': Array(0.47408965, dtype=float32), 'train/wer': 0.16640935875970342, 'validation/ctc_loss': Array(0.77574897, dtype=float32), 'validation/wer': 0.23329699273509633, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.522608, dtype=float32), 'test/wer': 0.17207103460815806, 'test/num_examples': 2472, 'score': 7280.09526014328, 'total_duration': 8031.183102846146, 'accumulated_submission_time': 7280.09526014328, 'accumulated_eval_time': 750.6799328327179, 'accumulated_logging_time': 0.24143624305725098, 'global_step': 9132, 'preemption_count': 0}), (10919, {'train/ctc_loss': Array(0.45372334, dtype=float32), 'train/wer': 0.1583827240185435, 'validation/ctc_loss': Array(0.72464544, dtype=float32), 'validation/wer': 0.2177252071896497, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46669164, dtype=float32), 'test/wer': 0.16055518516273715, 'test/num_examples': 2472, 'score': 8720.35819721222, 'total_duration': 9601.827582836151, 'accumulated_submission_time': 8720.35819721222, 'accumulated_eval_time': 880.9789929389954, 'accumulated_logging_time': 0.29038214683532715, 'global_step': 10919, 'preemption_count': 0}), (12741, {'train/ctc_loss': Array(0.3876675, dtype=float32), 'train/wer': 0.1373449896837045, 'validation/ctc_loss': Array(0.68030334, dtype=float32), 'validation/wer': 0.2039189958417351, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43548384, dtype=float32), 'test/wer': 0.14629169444612805, 'test/num_examples': 2472, 'score': 10160.457416057587, 'total_duration': 11171.864776134491, 'accumulated_submission_time': 10160.457416057587, 'accumulated_eval_time': 1010.8280313014984, 'accumulated_logging_time': 0.34416723251342773, 'global_step': 12741, 'preemption_count': 0}), (14529, {'train/ctc_loss': Array(0.32761952, dtype=float32), 'train/wer': 0.1211783977742531, 'validation/ctc_loss': Array(0.6463526, dtype=float32), 'validation/wer': 0.1942131617285261, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4045969, dtype=float32), 'test/wer': 0.13659413701840514, 'test/num_examples': 2472, 'score': 11600.862089395523, 'total_duration': 12742.716598272324, 'accumulated_submission_time': 11600.862089395523, 'accumulated_eval_time': 1141.1928191184998, 'accumulated_logging_time': 0.39309096336364746, 'global_step': 14529, 'preemption_count': 0}), (16325, {'train/ctc_loss': Array(0.3095974, dtype=float32), 'train/wer': 0.11256643382685443, 'validation/ctc_loss': Array(0.6278412, dtype=float32), 'validation/wer': 0.18638867717006435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38101736, dtype=float32), 'test/wer': 0.1274016607067095, 'test/num_examples': 2472, 'score': 13041.365608215332, 'total_duration': 14314.551568984985, 'accumulated_submission_time': 13041.365608215332, 'accumulated_eval_time': 1272.434442281723, 'accumulated_logging_time': 0.4481072425842285, 'global_step': 16325, 'preemption_count': 0}), (18128, {'train/ctc_loss': Array(0.30740917, dtype=float32), 'train/wer': 0.1153126290442813, 'validation/ctc_loss': Array(0.61150557, dtype=float32), 'validation/wer': 0.18498972493704716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37650645, dtype=float32), 'test/wer': 0.12715922177101643, 'test/num_examples': 2472, 'score': 14481.523082494736, 'total_duration': 15885.069373607635, 'accumulated_submission_time': 14481.523082494736, 'accumulated_eval_time': 1402.7020227909088, 'accumulated_logging_time': 0.5057213306427002, 'global_step': 18128, 'preemption_count': 0}), (19940, {'train/ctc_loss': Array(0.31152633, dtype=float32), 'train/wer': 0.11269996562574368, 'validation/ctc_loss': Array(0.5942979, dtype=float32), 'validation/wer': 0.17818792270065317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.365154, dtype=float32), 'test/wer': 0.12307816635351637, 'test/num_examples': 2472, 'score': 15921.992024421692, 'total_duration': 17456.20342564583, 'accumulated_submission_time': 15921.992024421692, 'accumulated_eval_time': 1533.2822642326355, 'accumulated_logging_time': 0.5559425354003906, 'global_step': 19940, 'preemption_count': 0}), (21722, {'train/ctc_loss': Array(0.3033637, dtype=float32), 'train/wer': 0.10925158941620282, 'validation/ctc_loss': Array(0.57038325, dtype=float32), 'validation/wer': 0.17055639707088346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34617653, dtype=float32), 'test/wer': 0.11570398205951876, 'test/num_examples': 2472, 'score': 17362.43122434616, 'total_duration': 19028.152482509613, 'accumulated_submission_time': 17362.43122434616, 'accumulated_eval_time': 1664.7011005878448, 'accumulated_logging_time': 0.6112995147705078, 'global_step': 21722, 'preemption_count': 0}), (23510, {'train/ctc_loss': Array(0.2867725, dtype=float32), 'train/wer': 0.10359090441403436, 'validation/ctc_loss': Array(0.56134623, dtype=float32), 'validation/wer': 0.1679900433192795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33919716, dtype=float32), 'test/wer': 0.11144109744024891, 'test/num_examples': 2472, 'score': 18802.503789901733, 'total_duration': 20600.94754266739, 'accumulated_submission_time': 18802.503789901733, 'accumulated_eval_time': 1797.3402605056763, 'accumulated_logging_time': 0.66159987449646, 'global_step': 23510, 'preemption_count': 0}), (25297, {'train/ctc_loss': Array(0.26029703, dtype=float32), 'train/wer': 0.0952398547358642, 'validation/ctc_loss': Array(0.53788465, dtype=float32), 'validation/wer': 0.1611014095649741, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32242474, dtype=float32), 'test/wer': 0.10855203345657312, 'test/num_examples': 2472, 'score': 20242.476589679718, 'total_duration': 22173.208388328552, 'accumulated_submission_time': 20242.476589679718, 'accumulated_eval_time': 1929.5375201702118, 'accumulated_logging_time': 0.7176668643951416, 'global_step': 25297, 'preemption_count': 0}), (27084, {'train/ctc_loss': Array(0.23703115, dtype=float32), 'train/wer': 0.0877070346122747, 'validation/ctc_loss': Array(0.5147594, dtype=float32), 'validation/wer': 0.15338305241729297, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30386835, dtype=float32), 'test/wer': 0.10273349899993939, 'test/num_examples': 2472, 'score': 21683.049283742905, 'total_duration': 23746.02119398117, 'accumulated_submission_time': 21683.049283742905, 'accumulated_eval_time': 2061.6824510097504, 'accumulated_logging_time': 0.7768373489379883, 'global_step': 27084, 'preemption_count': 0}), (28841, {'train/ctc_loss': Array(0.23476799, dtype=float32), 'train/wer': 0.08488713422180401, 'validation/ctc_loss': Array(0.50734174, dtype=float32), 'validation/wer': 0.15225424268444462, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30331925, dtype=float32), 'test/wer': 0.10164252378932057, 'test/num_examples': 2472, 'score': 23123.011386871338, 'total_duration': 25319.558921575546, 'accumulated_submission_time': 23123.011386871338, 'accumulated_eval_time': 2195.1673657894135, 'accumulated_logging_time': 0.8329625129699707, 'global_step': 28841, 'preemption_count': 0}), (30641, {'train/ctc_loss': Array(0.24080803, dtype=float32), 'train/wer': 0.08729307505633559, 'validation/ctc_loss': Array(0.49346223, dtype=float32), 'validation/wer': 0.14717942285984428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28759897, dtype=float32), 'test/wer': 0.0962482574701497, 'test/num_examples': 2472, 'score': 24563.034650802612, 'total_duration': 26890.950448989868, 'accumulated_submission_time': 24563.034650802612, 'accumulated_eval_time': 2326.448561191559, 'accumulated_logging_time': 0.8856112957000732, 'global_step': 30641, 'preemption_count': 0}), (32464, {'train/ctc_loss': Array(0.21947882, dtype=float32), 'train/wer': 0.07833466269485999, 'validation/ctc_loss': Array(0.48267198, dtype=float32), 'validation/wer': 0.14350355526826114, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27600875, dtype=float32), 'test/wer': 0.09196516960623877, 'test/num_examples': 2472, 'score': 26003.58583521843, 'total_duration': 28463.34000825882, 'accumulated_submission_time': 26003.58583521843, 'accumulated_eval_time': 2458.195885658264, 'accumulated_logging_time': 0.9412505626678467, 'global_step': 32464, 'preemption_count': 0}), (34256, {'train/ctc_loss': Array(0.22801153, dtype=float32), 'train/wer': 0.0817233355079648, 'validation/ctc_loss': Array(0.4684329, dtype=float32), 'validation/wer': 0.1390462040154753, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27156022, dtype=float32), 'test/wer': 0.0905509424813625, 'test/num_examples': 2472, 'score': 27443.854074001312, 'total_duration': 30035.26726746559, 'accumulated_submission_time': 27443.854074001312, 'accumulated_eval_time': 2589.7420234680176, 'accumulated_logging_time': 1.0170419216156006, 'global_step': 34256, 'preemption_count': 0}), (36028, {'train/ctc_loss': Array(0.2100577, dtype=float32), 'train/wer': 0.07396336200006394, 'validation/ctc_loss': Array(0.44706374, dtype=float32), 'validation/wer': 0.13241806481490415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25340295, dtype=float32), 'test/wer': 0.0848536274925753, 'test/num_examples': 2472, 'score': 28884.225048542023, 'total_duration': 31607.28241801262, 'accumulated_submission_time': 28884.225048542023, 'accumulated_eval_time': 2721.302494764328, 'accumulated_logging_time': 1.0658469200134277, 'global_step': 36028, 'preemption_count': 0}), (37805, {'train/ctc_loss': Array(0.15897675, dtype=float32), 'train/wer': 0.05832128983323596, 'validation/ctc_loss': Array(0.43366468, dtype=float32), 'validation/wer': 0.12898339588418606, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2495774, dtype=float32), 'test/wer': 0.08430813988726589, 'test/num_examples': 2472, 'score': 30324.14963078499, 'total_duration': 33180.79685497284, 'accumulated_submission_time': 30324.14963078499, 'accumulated_eval_time': 2854.8065383434296, 'accumulated_logging_time': 1.115863561630249, 'global_step': 37805, 'preemption_count': 0}), (39609, {'train/ctc_loss': Array(0.17318016, dtype=float32), 'train/wer': 0.0620472640493337, 'validation/ctc_loss': Array(0.4152827, dtype=float32), 'validation/wer': 0.12227807311213808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23602532, dtype=float32), 'test/wer': 0.07873204436632524, 'test/num_examples': 2472, 'score': 31764.295923948288, 'total_duration': 34754.60251188278, 'accumulated_submission_time': 31764.295923948288, 'accumulated_eval_time': 2988.3742775917053, 'accumulated_logging_time': 1.1702988147735596, 'global_step': 39609, 'preemption_count': 0}), (41375, {'train/ctc_loss': Array(0.20501196, dtype=float32), 'train/wer': 0.0734312199302463, 'validation/ctc_loss': Array(0.40115854, dtype=float32), 'validation/wer': 0.11899777132437361, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22964244, dtype=float32), 'test/wer': 0.07592379336121381, 'test/num_examples': 2472, 'score': 33204.95049738884, 'total_duration': 36327.13359904289, 'accumulated_submission_time': 33204.95049738884, 'accumulated_eval_time': 3120.1587953567505, 'accumulated_logging_time': 1.2276079654693604, 'global_step': 41375, 'preemption_count': 0}), (43145, {'train/ctc_loss': Array(0.19973153, dtype=float32), 'train/wer': 0.07101261817394539, 'validation/ctc_loss': Array(0.3852075, dtype=float32), 'validation/wer': 0.11362386516030062, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21550332, dtype=float32), 'test/wer': 0.07143867305089198, 'test/num_examples': 2472, 'score': 34645.59170675278, 'total_duration': 37899.478531360626, 'accumulated_submission_time': 34645.59170675278, 'accumulated_eval_time': 3251.7741045951843, 'accumulated_logging_time': 1.278609275817871, 'global_step': 43145, 'preemption_count': 0}), (44924, {'train/ctc_loss': Array(0.22303014, dtype=float32), 'train/wer': 0.08178951863399454, 'validation/ctc_loss': Array(0.37065703, dtype=float32), 'validation/wer': 0.10893496319308435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21001306, dtype=float32), 'test/wer': 0.06927692587429542, 'test/num_examples': 2472, 'score': 36086.64011669159, 'total_duration': 39470.7828605175, 'accumulated_submission_time': 36086.64011669159, 'accumulated_eval_time': 3381.940108537674, 'accumulated_logging_time': 1.33180570602417, 'global_step': 44924, 'preemption_count': 0}), (46724, {'train/ctc_loss': Array(0.18576692, dtype=float32), 'train/wer': 0.06542870431971928, 'validation/ctc_loss': Array(0.3589722, dtype=float32), 'validation/wer': 0.10608881899487695, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19952978, dtype=float32), 'test/wer': 0.06553932561569388, 'test/num_examples': 2472, 'score': 37526.59911942482, 'total_duration': 41039.873332977295, 'accumulated_submission_time': 37526.59911942482, 'accumulated_eval_time': 3510.981105566025, 'accumulated_logging_time': 1.386012077331543, 'global_step': 46724, 'preemption_count': 0}), (48486, {'train/ctc_loss': Array(0.16211706, dtype=float32), 'train/wer': 0.06009709062162493, 'validation/ctc_loss': Array(0.35218254, dtype=float32), 'validation/wer': 0.1016990033671333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19315596, dtype=float32), 'test/wer': 0.06444835040507506, 'test/num_examples': 2472, 'score': 38967.16755080223, 'total_duration': 42611.00805640221, 'accumulated_submission_time': 38967.16755080223, 'accumulated_eval_time': 3641.4568905830383, 'accumulated_logging_time': 1.4418017864227295, 'global_step': 48486, 'preemption_count': 0}), (50267, {'train/ctc_loss': Array(0.13102518, dtype=float32), 'train/wer': 0.04839095585596363, 'validation/ctc_loss': Array(0.33665368, dtype=float32), 'validation/wer': 0.0975793302395585, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18713242, dtype=float32), 'test/wer': 0.06248863567488939, 'test/num_examples': 2472, 'score': 40407.606982946396, 'total_duration': 44184.01608181, 'accumulated_submission_time': 40407.606982946396, 'accumulated_eval_time': 3773.9288313388824, 'accumulated_logging_time': 1.5033035278320312, 'global_step': 50267, 'preemption_count': 0}), (52064, {'train/ctc_loss': Array(0.13984925, dtype=float32), 'train/wer': 0.052034736893557164, 'validation/ctc_loss': Array(0.3301808, dtype=float32), 'validation/wer': 0.09555326148829221, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17835274, dtype=float32), 'test/wer': 0.058468189991312605, 'test/num_examples': 2472, 'score': 41847.9191968441, 'total_duration': 45756.22497534752, 'accumulated_submission_time': 41847.9191968441, 'accumulated_eval_time': 3905.7319672107697, 'accumulated_logging_time': 1.5603466033935547, 'global_step': 52064, 'preemption_count': 0}), (53857, {'train/ctc_loss': Array(0.12152492, dtype=float32), 'train/wer': 0.044537021130389966, 'validation/ctc_loss': Array(0.3229883, dtype=float32), 'validation/wer': 0.09346930505841831, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17643908, dtype=float32), 'test/wer': 0.05798331211992646, 'test/num_examples': 2472, 'score': 43287.88098239899, 'total_duration': 47328.20605754852, 'accumulated_submission_time': 43287.88098239899, 'accumulated_eval_time': 4037.6561448574066, 'accumulated_logging_time': 1.6179511547088623, 'global_step': 53857, 'preemption_count': 0}), (55625, {'train/ctc_loss': Array(0.12235944, dtype=float32), 'train/wer': 0.045498432397834646, 'validation/ctc_loss': Array(0.3146154, dtype=float32), 'validation/wer': 0.09073893621742612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17075971, dtype=float32), 'test/wer': 0.056064003879022974, 'test/num_examples': 2472, 'score': 44728.258295059204, 'total_duration': 48900.66444969177, 'accumulated_submission_time': 44728.258295059204, 'accumulated_eval_time': 4169.645361185074, 'accumulated_logging_time': 1.6745784282684326, 'global_step': 55625, 'preemption_count': 0}), (57420, {'train/ctc_loss': Array(0.11635064, dtype=float32), 'train/wer': 0.04343409247757074, 'validation/ctc_loss': Array(0.31374025, dtype=float32), 'validation/wer': 0.089967100502658, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16906595, dtype=float32), 'test/wer': 0.055316483827302664, 'test/num_examples': 2472, 'score': 46168.2030479908, 'total_duration': 50472.18540120125, 'accumulated_submission_time': 46168.2030479908, 'accumulated_eval_time': 4301.133611679077, 'accumulated_logging_time': 1.7258071899414062, 'global_step': 57420, 'preemption_count': 0}), (59229, {'train/ctc_loss': Array(0.12494056, dtype=float32), 'train/wer': 0.046106475281297576, 'validation/ctc_loss': Array(0.31217274, dtype=float32), 'validation/wer': 0.08951364702023175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16805044, dtype=float32), 'test/wer': 0.05471038648806999, 'test/num_examples': 2472, 'score': 47608.83378863335, 'total_duration': 52044.11736369133, 'accumulated_submission_time': 47608.83378863335, 'accumulated_eval_time': 4432.337597131729, 'accumulated_logging_time': 1.786052942276001, 'global_step': 59229, 'preemption_count': 0}), (60000, {'train/ctc_loss': Array(0.13582408, dtype=float32), 'train/wer': 0.04959213234733552, 'validation/ctc_loss': Array(0.3120159, dtype=float32), 'validation/wer': 0.08961012648457775, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16800222, dtype=float32), 'test/wer': 0.05458916702022345, 'test/num_examples': 2472, 'score': 48214.99224925041, 'total_duration': 52781.79877066612, 'accumulated_submission_time': 48214.99224925041, 'accumulated_eval_time': 4563.781610965729, 'accumulated_logging_time': 1.84389328956604, 'global_step': 60000, 'preemption_count': 0})], 'global_step': 60000}
I1013 16:57:06.626779 139660230973248 submission_runner.py:552] Timing: 48214.99224925041
I1013 16:57:06.626847 139660230973248 submission_runner.py:554] Total number of evals: 35
I1013 16:57:06.626899 139660230973248 submission_runner.py:555] ====================
I1013 16:57:06.647828 139660230973248 submission_runner.py:625] Final librispeech_conformer score: 48214.99224925041
