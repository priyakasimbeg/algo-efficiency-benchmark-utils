python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_conformer/adamw_run8 --overwrite=true --save_checkpoints=false --max_global_steps=60000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_10-13-2023-02-17-01.log
2023-10-13 02:17:07.316908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1013 02:17:25.719588 140528462927680 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_conformer/adamw_run8/librispeech_conformer_jax.
I1013 02:17:26.705691 140528462927680 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I1013 02:17:26.706383 140528462927680 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1013 02:17:26.706537 140528462927680 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1013 02:17:26.714933 140528462927680 submission_runner.py:523] Using RNG seed 3794297714
I1013 02:17:32.808781 140528462927680 submission_runner.py:532] --- Tuning run 1/1 ---
I1013 02:17:32.808976 140528462927680 submission_runner.py:537] Creating tuning directory at /experiment_runs/targets_check_conformer/adamw_run8/librispeech_conformer_jax/trial_1.
I1013 02:17:32.809177 140528462927680 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_conformer/adamw_run8/librispeech_conformer_jax/trial_1/hparams.json.
I1013 02:17:32.989499 140528462927680 submission_runner.py:200] Initializing dataset.
I1013 02:17:32.989735 140528462927680 submission_runner.py:207] Initializing model.
I1013 02:17:37.688756 140528462927680 submission_runner.py:241] Initializing optimizer.
I1013 02:17:38.903132 140528462927680 submission_runner.py:248] Initializing metrics bundle.
I1013 02:17:38.903328 140528462927680 submission_runner.py:266] Initializing checkpoint and logger.
I1013 02:17:38.904384 140528462927680 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_conformer/adamw_run8/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I1013 02:17:38.904531 140528462927680 submission_runner.py:286] Saving meta data to /experiment_runs/targets_check_conformer/adamw_run8/librispeech_conformer_jax/trial_1/meta_data_0.json.
I1013 02:17:38.904719 140528462927680 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1013 02:17:38.904779 140528462927680 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1013 02:17:39.274850 140528462927680 logger_utils.py:220] Unable to record git information. Continuing without it.
I1013 02:17:39.620907 140528462927680 submission_runner.py:289] Saving flags to /experiment_runs/targets_check_conformer/adamw_run8/librispeech_conformer_jax/trial_1/flags_0.json.
I1013 02:17:39.635179 140528462927680 submission_runner.py:299] Starting training loop.
I1013 02:17:39.928673 140528462927680 input_pipeline.py:20] Loading split = train-clean-100
I1013 02:17:39.986730 140528462927680 input_pipeline.py:20] Loading split = train-clean-360
I1013 02:17:40.410448 140528462927680 input_pipeline.py:20] Loading split = train-other-500
2023-10-13 02:18:50.916354: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-13 02:18:53.730220: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1013 02:18:55.735750 140353441093376 logging_writer.py:48] [0] global_step=0, grad_norm=60.6518440246582, loss=32.59967803955078
I1013 02:18:55.772189 140528462927680 spec.py:321] Evaluating on the training split.
I1013 02:18:55.938175 140528462927680 input_pipeline.py:20] Loading split = train-clean-100
I1013 02:18:55.972875 140528462927680 input_pipeline.py:20] Loading split = train-clean-360
I1013 02:18:56.370336 140528462927680 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1013 02:20:22.390696 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 02:20:22.506556 140528462927680 input_pipeline.py:20] Loading split = dev-clean
I1013 02:20:22.511579 140528462927680 input_pipeline.py:20] Loading split = dev-other
I1013 02:21:28.277733 140528462927680 spec.py:349] Evaluating on the test split.
I1013 02:21:28.398381 140528462927680 input_pipeline.py:20] Loading split = test-clean
I1013 02:22:07.444428 140528462927680 submission_runner.py:393] Time since start: 267.81s, 	Step: 1, 	{'train/ctc_loss': Array(30.823114, dtype=float32), 'train/wer': 1.7632853007933698, 'validation/ctc_loss': Array(29.720152, dtype=float32), 'validation/wer': 1.3843807022794636, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.881086, dtype=float32), 'test/wer': 1.471289582190807, 'test/num_examples': 2472, 'score': 76.13695120811462, 'total_duration': 267.806923866272, 'accumulated_submission_time': 76.13695120811462, 'accumulated_eval_time': 191.66992235183716, 'accumulated_logging_time': 0}
I1013 02:22:07.475598 140347208361728 logging_writer.py:48] [1] accumulated_eval_time=191.669922, accumulated_logging_time=0, accumulated_submission_time=76.136951, global_step=1, preemption_count=0, score=76.136951, test/ctc_loss=29.881086349487305, test/num_examples=2472, test/wer=1.471290, total_duration=267.806924, train/ctc_loss=30.8231143951416, train/wer=1.763285, validation/ctc_loss=29.720151901245117, validation/num_examples=5348, validation/wer=1.384381
I1013 02:22:30.083610 140355053487872 logging_writer.py:48] [1] global_step=1, grad_norm=59.031227111816406, loss=31.405126571655273
I1013 02:22:30.974807 140355061880576 logging_writer.py:48] [2] global_step=2, grad_norm=71.0946044921875, loss=31.27874755859375
I1013 02:22:31.801477 140355053487872 logging_writer.py:48] [3] global_step=3, grad_norm=92.72341918945312, loss=30.685091018676758
I1013 02:22:32.695739 140355061880576 logging_writer.py:48] [4] global_step=4, grad_norm=141.41494750976562, loss=29.40772247314453
I1013 02:22:33.592607 140355053487872 logging_writer.py:48] [5] global_step=5, grad_norm=182.88296508789062, loss=27.62799644470215
I1013 02:22:34.480916 140355061880576 logging_writer.py:48] [6] global_step=6, grad_norm=190.46585083007812, loss=24.328187942504883
I1013 02:22:35.360982 140355053487872 logging_writer.py:48] [7] global_step=7, grad_norm=176.1442413330078, loss=20.111425399780273
I1013 02:22:36.250982 140355061880576 logging_writer.py:48] [8] global_step=8, grad_norm=164.59793090820312, loss=16.36696434020996
I1013 02:22:37.130363 140355053487872 logging_writer.py:48] [9] global_step=9, grad_norm=129.16226196289062, loss=12.397334098815918
I1013 02:22:38.011880 140355061880576 logging_writer.py:48] [10] global_step=10, grad_norm=84.07386779785156, loss=9.510960578918457
I1013 02:22:38.903433 140355053487872 logging_writer.py:48] [11] global_step=11, grad_norm=33.758609771728516, loss=7.776369571685791
I1013 02:22:39.803876 140355061880576 logging_writer.py:48] [12] global_step=12, grad_norm=4.206180572509766, loss=7.285386085510254
I1013 02:22:40.699513 140355053487872 logging_writer.py:48] [13] global_step=13, grad_norm=11.979933738708496, loss=7.381763935089111
I1013 02:22:41.584532 140355061880576 logging_writer.py:48] [14] global_step=14, grad_norm=18.26526641845703, loss=7.6468892097473145
I1013 02:22:42.461285 140355053487872 logging_writer.py:48] [15] global_step=15, grad_norm=21.08738136291504, loss=7.862880229949951
I1013 02:22:43.348949 140355061880576 logging_writer.py:48] [16] global_step=16, grad_norm=21.996780395507812, loss=7.977934837341309
I1013 02:22:44.237343 140355053487872 logging_writer.py:48] [17] global_step=17, grad_norm=22.16638946533203, loss=7.967485427856445
I1013 02:22:45.132523 140355061880576 logging_writer.py:48] [18] global_step=18, grad_norm=22.073884963989258, loss=7.886059761047363
I1013 02:22:46.022771 140355053487872 logging_writer.py:48] [19] global_step=19, grad_norm=20.433340072631836, loss=7.689330101013184
I1013 02:22:46.922483 140355061880576 logging_writer.py:48] [20] global_step=20, grad_norm=17.189598083496094, loss=7.427083969116211
I1013 02:22:47.804535 140355053487872 logging_writer.py:48] [21] global_step=21, grad_norm=10.568976402282715, loss=7.194606781005859
I1013 02:22:48.685892 140355061880576 logging_writer.py:48] [22] global_step=22, grad_norm=2.8190438747406006, loss=7.089526176452637
I1013 02:22:49.560677 140355053487872 logging_writer.py:48] [23] global_step=23, grad_norm=14.721478462219238, loss=7.174688816070557
I1013 02:22:50.452369 140355061880576 logging_writer.py:48] [24] global_step=24, grad_norm=21.459674835205078, loss=7.247330188751221
I1013 02:22:51.332901 140355053487872 logging_writer.py:48] [25] global_step=25, grad_norm=19.868196487426758, loss=7.196482181549072
I1013 02:22:52.225138 140355061880576 logging_writer.py:48] [26] global_step=26, grad_norm=12.103171348571777, loss=7.0403971672058105
I1013 02:22:53.108193 140355053487872 logging_writer.py:48] [27] global_step=27, grad_norm=3.083958148956299, loss=6.938595294952393
I1013 02:22:54.013724 140355061880576 logging_writer.py:48] [28] global_step=28, grad_norm=5.660976409912109, loss=6.915743350982666
I1013 02:22:54.898042 140355053487872 logging_writer.py:48] [29] global_step=29, grad_norm=9.32616138458252, loss=6.93166971206665
I1013 02:22:55.778235 140355061880576 logging_writer.py:48] [30] global_step=30, grad_norm=9.671133995056152, loss=6.904515743255615
I1013 02:22:56.651264 140355053487872 logging_writer.py:48] [31] global_step=31, grad_norm=8.048491477966309, loss=6.815601348876953
I1013 02:22:57.544738 140355061880576 logging_writer.py:48] [32] global_step=32, grad_norm=3.69954514503479, loss=6.738208770751953
I1013 02:22:58.439824 140355053487872 logging_writer.py:48] [33] global_step=33, grad_norm=3.3917856216430664, loss=6.690659999847412
I1013 02:22:59.330771 140355061880576 logging_writer.py:48] [34] global_step=34, grad_norm=9.905620574951172, loss=6.717809677124023
I1013 02:23:00.224996 140355053487872 logging_writer.py:48] [35] global_step=35, grad_norm=9.321910858154297, loss=6.665290355682373
I1013 02:23:01.114339 140355061880576 logging_writer.py:48] [36] global_step=36, grad_norm=4.597956657409668, loss=6.583609104156494
I1013 02:23:02.081229 140355053487872 logging_writer.py:48] [37] global_step=37, grad_norm=2.5037760734558105, loss=6.544168949127197
I1013 02:23:02.975657 140355061880576 logging_writer.py:48] [38] global_step=38, grad_norm=5.570356369018555, loss=6.520286560058594
I1013 02:23:03.860541 140355053487872 logging_writer.py:48] [39] global_step=39, grad_norm=4.971263885498047, loss=6.4552154541015625
I1013 02:23:04.752928 140355061880576 logging_writer.py:48] [40] global_step=40, grad_norm=1.7475987672805786, loss=6.424993991851807
I1013 02:23:05.635876 140355053487872 logging_writer.py:48] [41] global_step=41, grad_norm=5.84037971496582, loss=6.416783809661865
I1013 02:23:06.520462 140355061880576 logging_writer.py:48] [42] global_step=42, grad_norm=5.517841339111328, loss=6.386558532714844
I1013 02:23:07.420020 140355053487872 logging_writer.py:48] [43] global_step=43, grad_norm=1.2244027853012085, loss=6.313882350921631
I1013 02:23:08.304206 140355061880576 logging_writer.py:48] [44] global_step=44, grad_norm=2.868131399154663, loss=6.306762218475342
I1013 02:23:09.189791 140355053487872 logging_writer.py:48] [45] global_step=45, grad_norm=2.6270577907562256, loss=6.256462097167969
I1013 02:23:10.077270 140355061880576 logging_writer.py:48] [46] global_step=46, grad_norm=1.7203792333602905, loss=6.241166591644287
I1013 02:23:10.976569 140355053487872 logging_writer.py:48] [47] global_step=47, grad_norm=3.797912359237671, loss=6.208065032958984
I1013 02:23:11.866224 140355061880576 logging_writer.py:48] [48] global_step=48, grad_norm=0.9460170269012451, loss=6.179615497589111
I1013 02:23:12.768525 140355053487872 logging_writer.py:48] [49] global_step=49, grad_norm=2.5220370292663574, loss=6.150888442993164
I1013 02:23:13.668965 140355061880576 logging_writer.py:48] [50] global_step=50, grad_norm=0.9001132249832153, loss=6.1201171875
I1013 02:23:14.563181 140355053487872 logging_writer.py:48] [51] global_step=51, grad_norm=2.5132365226745605, loss=6.1040425300598145
I1013 02:23:15.448552 140355061880576 logging_writer.py:48] [52] global_step=52, grad_norm=0.8564764261245728, loss=6.086636066436768
I1013 02:23:16.329293 140355053487872 logging_writer.py:48] [53] global_step=53, grad_norm=1.4343827962875366, loss=6.070474147796631
I1013 02:23:17.214703 140355061880576 logging_writer.py:48] [54] global_step=54, grad_norm=1.2088924646377563, loss=6.0563530921936035
I1013 02:23:18.095476 140355053487872 logging_writer.py:48] [55] global_step=55, grad_norm=0.7174556851387024, loss=6.008722305297852
I1013 02:23:18.981858 140355061880576 logging_writer.py:48] [56] global_step=56, grad_norm=0.7444440722465515, loss=5.975090503692627
I1013 02:23:19.874626 140355053487872 logging_writer.py:48] [57] global_step=57, grad_norm=1.56669020652771, loss=5.982481479644775
I1013 02:23:20.767609 140355061880576 logging_writer.py:48] [58] global_step=58, grad_norm=2.743084669113159, loss=5.975739002227783
I1013 02:23:21.660694 140355053487872 logging_writer.py:48] [59] global_step=59, grad_norm=4.258749961853027, loss=5.957167625427246
I1013 02:23:22.543953 140355061880576 logging_writer.py:48] [60] global_step=60, grad_norm=5.119749546051025, loss=5.945534706115723
I1013 02:23:23.422678 140355053487872 logging_writer.py:48] [61] global_step=61, grad_norm=5.581885814666748, loss=5.9512786865234375
I1013 02:23:24.308633 140355061880576 logging_writer.py:48] [62] global_step=62, grad_norm=4.320209980010986, loss=5.942849159240723
I1013 02:23:25.194661 140355053487872 logging_writer.py:48] [63] global_step=63, grad_norm=0.616037905216217, loss=5.900052070617676
I1013 02:23:26.084232 140355061880576 logging_writer.py:48] [64] global_step=64, grad_norm=5.024406909942627, loss=5.918530464172363
I1013 02:23:26.971763 140355053487872 logging_writer.py:48] [65] global_step=65, grad_norm=7.483720302581787, loss=5.927283763885498
I1013 02:23:27.860529 140355061880576 logging_writer.py:48] [66] global_step=66, grad_norm=3.697011709213257, loss=5.876466274261475
I1013 02:23:28.744200 140355053487872 logging_writer.py:48] [67] global_step=67, grad_norm=2.763538360595703, loss=5.881828784942627
I1013 02:23:29.626065 140355061880576 logging_writer.py:48] [68] global_step=68, grad_norm=5.61683988571167, loss=5.903205871582031
I1013 02:23:30.508453 140355053487872 logging_writer.py:48] [69] global_step=69, grad_norm=4.595611572265625, loss=5.876806735992432
I1013 02:23:31.397668 140355061880576 logging_writer.py:48] [70] global_step=70, grad_norm=0.6705284714698792, loss=5.857838153839111
I1013 02:23:32.284986 140355053487872 logging_writer.py:48] [71] global_step=71, grad_norm=3.4847187995910645, loss=5.869355201721191
I1013 02:23:33.168371 140355061880576 logging_writer.py:48] [72] global_step=72, grad_norm=6.102992057800293, loss=5.880586624145508
I1013 02:23:34.056480 140355053487872 logging_writer.py:48] [73] global_step=73, grad_norm=5.7049102783203125, loss=5.887538909912109
I1013 02:23:34.942958 140355061880576 logging_writer.py:48] [74] global_step=74, grad_norm=0.7769572138786316, loss=5.826934337615967
I1013 02:23:35.828911 140355053487872 logging_writer.py:48] [75] global_step=75, grad_norm=5.390964508056641, loss=5.857789039611816
I1013 02:23:36.711418 140355061880576 logging_writer.py:48] [76] global_step=76, grad_norm=6.8331708908081055, loss=5.876186847686768
I1013 02:23:37.607562 140355053487872 logging_writer.py:48] [77] global_step=77, grad_norm=2.0192806720733643, loss=5.849228858947754
I1013 02:23:38.497815 140355061880576 logging_writer.py:48] [78] global_step=78, grad_norm=3.864337682723999, loss=5.831053733825684
I1013 02:23:39.382003 140355053487872 logging_writer.py:48] [79] global_step=79, grad_norm=5.259383678436279, loss=5.848038673400879
I1013 02:23:40.272786 140355061880576 logging_writer.py:48] [80] global_step=80, grad_norm=1.7454943656921387, loss=5.833584785461426
I1013 02:23:41.165833 140355053487872 logging_writer.py:48] [81] global_step=81, grad_norm=3.032397747039795, loss=5.829919338226318
I1013 02:23:42.049866 140355061880576 logging_writer.py:48] [82] global_step=82, grad_norm=4.467556476593018, loss=5.842633247375488
I1013 02:23:42.935761 140355053487872 logging_writer.py:48] [83] global_step=83, grad_norm=1.617567539215088, loss=5.837526798248291
I1013 02:23:43.826831 140355061880576 logging_writer.py:48] [84] global_step=84, grad_norm=3.018070697784424, loss=5.8287553787231445
I1013 02:23:44.713325 140355053487872 logging_writer.py:48] [85] global_step=85, grad_norm=5.457849502563477, loss=5.854903697967529
I1013 02:23:45.598910 140355061880576 logging_writer.py:48] [86] global_step=86, grad_norm=3.328005313873291, loss=5.826963901519775
I1013 02:23:46.488571 140355053487872 logging_writer.py:48] [87] global_step=87, grad_norm=1.083056926727295, loss=5.802922248840332
I1013 02:23:47.379502 140355061880576 logging_writer.py:48] [88] global_step=88, grad_norm=3.6888375282287598, loss=5.8229498863220215
I1013 02:23:48.275812 140355053487872 logging_writer.py:48] [89] global_step=89, grad_norm=3.9661483764648438, loss=5.84529972076416
I1013 02:23:49.157036 140355061880576 logging_writer.py:48] [90] global_step=90, grad_norm=3.351315498352051, loss=5.827075958251953
I1013 02:23:50.044120 140355053487872 logging_writer.py:48] [91] global_step=91, grad_norm=2.5724074840545654, loss=5.833681106567383
I1013 02:23:50.935864 140355061880576 logging_writer.py:48] [92] global_step=92, grad_norm=1.524161696434021, loss=5.809943199157715
I1013 02:23:51.818256 140355053487872 logging_writer.py:48] [93] global_step=93, grad_norm=0.3412703275680542, loss=5.808932304382324
I1013 02:23:52.707691 140355061880576 logging_writer.py:48] [94] global_step=94, grad_norm=1.4952226877212524, loss=5.830535411834717
I1013 02:23:53.600484 140355053487872 logging_writer.py:48] [95] global_step=95, grad_norm=2.941746234893799, loss=5.834974765777588
I1013 02:23:54.489861 140355061880576 logging_writer.py:48] [96] global_step=96, grad_norm=4.757950305938721, loss=5.831575870513916
I1013 02:23:55.375012 140355053487872 logging_writer.py:48] [97] global_step=97, grad_norm=6.450178623199463, loss=5.889652252197266
I1013 02:23:56.259510 140355061880576 logging_writer.py:48] [98] global_step=98, grad_norm=5.70418643951416, loss=5.8471598625183105
I1013 02:23:57.146899 140355053487872 logging_writer.py:48] [99] global_step=99, grad_norm=2.968018054962158, loss=5.8349714279174805
I1013 02:23:58.035839 140355061880576 logging_writer.py:48] [100] global_step=100, grad_norm=0.4696475863456726, loss=5.823634147644043
I1013 02:29:03.986228 140355053487872 logging_writer.py:48] [500] global_step=500, grad_norm=2.3174970149993896, loss=5.514542102813721
I1013 02:35:25.919579 140355061880576 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6497565507888794, loss=3.4176807403564453
I1013 02:41:50.765419 140355137414912 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.3674012422561646, loss=2.7359249591827393
I1013 02:46:07.747528 140528462927680 spec.py:321] Evaluating on the training split.
I1013 02:46:58.374180 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 02:47:46.582603 140528462927680 spec.py:349] Evaluating on the test split.
I1013 02:48:11.201665 140528462927680 submission_runner.py:393] Time since start: 1831.56s, 	Step: 1838, 	{'train/ctc_loss': Array(2.761366, dtype=float32), 'train/wer': 0.5540668575312558, 'validation/ctc_loss': Array(3.1618004, dtype=float32), 'validation/wer': 0.5959141508249901, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.8121145, dtype=float32), 'test/wer': 0.5452034204700099, 'test/num_examples': 2472, 'score': 1516.3296041488647, 'total_duration': 1831.561188697815, 'accumulated_submission_time': 1516.3296041488647, 'accumulated_eval_time': 315.11881017684937, 'accumulated_logging_time': 0.045053958892822266}
I1013 02:48:11.236275 140355792774912 logging_writer.py:48] [1838] accumulated_eval_time=315.118810, accumulated_logging_time=0.045054, accumulated_submission_time=1516.329604, global_step=1838, preemption_count=0, score=1516.329604, test/ctc_loss=2.8121144771575928, test/num_examples=2472, test/wer=0.545203, total_duration=1831.561189, train/ctc_loss=2.7613658905029297, train/wer=0.554067, validation/ctc_loss=3.1618003845214844, validation/num_examples=5348, validation/wer=0.595914
I1013 02:50:15.228981 140355784382208 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9650749564170837, loss=2.4522440433502197
I1013 02:56:39.827314 140355792774912 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.806600034236908, loss=2.2384517192840576
I1013 03:03:01.903982 140355784382208 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9080978035926819, loss=2.214923858642578
I1013 03:09:30.921986 140355792774912 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.1275771856307983, loss=2.1429402828216553
I1013 03:12:11.842563 140528462927680 spec.py:321] Evaluating on the training split.
I1013 03:13:03.947308 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 03:13:53.781049 140528462927680 spec.py:349] Evaluating on the test split.
I1013 03:14:19.059809 140528462927680 submission_runner.py:393] Time since start: 3399.42s, 	Step: 3712, 	{'train/ctc_loss': Array(0.8107718, dtype=float32), 'train/wer': 0.26824707668540076, 'validation/ctc_loss': Array(1.1465917, dtype=float32), 'validation/wer': 0.3259990152253879, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8400212, dtype=float32), 'test/wer': 0.26721914163264476, 'test/num_examples': 2472, 'score': 2956.850061416626, 'total_duration': 3399.418752193451, 'accumulated_submission_time': 2956.850061416626, 'accumulated_eval_time': 442.330219745636, 'accumulated_logging_time': 0.09564995765686035}
I1013 03:14:19.093987 140355792774912 logging_writer.py:48] [3712] accumulated_eval_time=442.330220, accumulated_logging_time=0.095650, accumulated_submission_time=2956.850061, global_step=3712, preemption_count=0, score=2956.850061, test/ctc_loss=0.8400211930274963, test/num_examples=2472, test/wer=0.267219, total_duration=3399.418752, train/ctc_loss=0.8107718229293823, train/wer=0.268247, validation/ctc_loss=1.1465916633605957, validation/num_examples=5348, validation/wer=0.325999
I1013 03:17:59.067770 140355784382208 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8551717400550842, loss=2.0329978466033936
I1013 03:24:23.660017 140355792774912 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0359771251678467, loss=2.0218868255615234
I1013 03:30:50.506292 140355784382208 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5924692749977112, loss=1.9725242853164673
I1013 03:37:23.274510 140355792774912 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.0561981201171875, loss=1.984236717224121
I1013 03:38:19.528108 140528462927680 spec.py:321] Evaluating on the training split.
I1013 03:39:12.748534 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 03:40:02.918227 140528462927680 spec.py:349] Evaluating on the test split.
I1013 03:40:28.243088 140528462927680 submission_runner.py:393] Time since start: 4968.60s, 	Step: 5575, 	{'train/ctc_loss': Array(0.6195338, dtype=float32), 'train/wer': 0.21718824949528492, 'validation/ctc_loss': Array(0.98032075, dtype=float32), 'validation/wer': 0.2860480608629329, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.68563527, dtype=float32), 'test/wer': 0.22399610017671073, 'test/num_examples': 2472, 'score': 4397.20131111145, 'total_duration': 4968.601372718811, 'accumulated_submission_time': 4397.20131111145, 'accumulated_eval_time': 571.0387015342712, 'accumulated_logging_time': 0.14401650428771973}
I1013 03:40:28.285719 140354615166720 logging_writer.py:48] [5575] accumulated_eval_time=571.038702, accumulated_logging_time=0.144017, accumulated_submission_time=4397.201311, global_step=5575, preemption_count=0, score=4397.201311, test/ctc_loss=0.6856352686882019, test/num_examples=2472, test/wer=0.223996, total_duration=4968.601373, train/ctc_loss=0.6195337772369385, train/wer=0.217188, validation/ctc_loss=0.9803207516670227, validation/num_examples=5348, validation/wer=0.286048
I1013 03:45:52.590242 140354606774016 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7996086478233337, loss=1.8818011283874512
I1013 03:52:26.645188 140354615166720 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8017681837081909, loss=1.9169633388519287
I1013 03:58:57.225974 140354606774016 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.698397696018219, loss=1.8947826623916626
I1013 04:04:28.613213 140528462927680 spec.py:321] Evaluating on the training split.
I1013 04:05:22.745124 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 04:06:13.511369 140528462927680 spec.py:349] Evaluating on the test split.
I1013 04:06:38.491045 140528462927680 submission_runner.py:393] Time since start: 6538.85s, 	Step: 7412, 	{'train/ctc_loss': Array(0.5800135, dtype=float32), 'train/wer': 0.2003319754107803, 'validation/ctc_loss': Array(0.8981867, dtype=float32), 'validation/wer': 0.26745319906929144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6163951, dtype=float32), 'test/wer': 0.20396888266000446, 'test/num_examples': 2472, 'score': 5837.44485616684, 'total_duration': 6538.850746393204, 'accumulated_submission_time': 5837.44485616684, 'accumulated_eval_time': 700.9116535186768, 'accumulated_logging_time': 0.20094585418701172}
I1013 04:06:38.535063 140355792774912 logging_writer.py:48] [7412] accumulated_eval_time=700.911654, accumulated_logging_time=0.200946, accumulated_submission_time=5837.444856, global_step=7412, preemption_count=0, score=5837.444856, test/ctc_loss=0.616395115852356, test/num_examples=2472, test/wer=0.203969, total_duration=6538.850746, train/ctc_loss=0.5800135135650635, train/wer=0.200332, validation/ctc_loss=0.8981866836547852, validation/num_examples=5348, validation/wer=0.267453
I1013 04:07:46.080026 140355784382208 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7864949703216553, loss=1.8842895030975342
I1013 04:14:22.472176 140355792774912 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8411476016044617, loss=1.842265248298645
I1013 04:21:08.175820 140354553734912 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7945444583892822, loss=1.8742239475250244
I1013 04:27:47.362200 140354545342208 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8291935324668884, loss=1.8366913795471191
I1013 04:30:38.982954 140528462927680 spec.py:321] Evaluating on the training split.
I1013 04:31:31.998955 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 04:32:22.379806 140528462927680 spec.py:349] Evaluating on the test split.
I1013 04:32:47.567303 140528462927680 submission_runner.py:393] Time since start: 8107.93s, 	Step: 9207, 	{'train/ctc_loss': Array(0.5400141, dtype=float32), 'train/wer': 0.18638152601709104, 'validation/ctc_loss': Array(0.8547003, dtype=float32), 'validation/wer': 0.25209264605076415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5706699, dtype=float32), 'test/wer': 0.19062417484207747, 'test/num_examples': 2472, 'score': 7277.80882358551, 'total_duration': 8107.925623416901, 'accumulated_submission_time': 7277.80882358551, 'accumulated_eval_time': 829.4895484447479, 'accumulated_logging_time': 0.26154112815856934}
I1013 04:32:47.603615 140354707334912 logging_writer.py:48] [9207] accumulated_eval_time=829.489548, accumulated_logging_time=0.261541, accumulated_submission_time=7277.808824, global_step=9207, preemption_count=0, score=7277.808824, test/ctc_loss=0.5706698894500732, test/num_examples=2472, test/wer=0.190624, total_duration=8107.925623, train/ctc_loss=0.5400140881538391, train/wer=0.186382, validation/ctc_loss=0.8547003269195557, validation/num_examples=5348, validation/wer=0.252093
I1013 04:36:34.381280 140354707334912 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6006962060928345, loss=1.8123196363449097
I1013 04:43:13.271744 140354698942208 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7835163474082947, loss=1.798349380493164
I1013 04:50:03.642198 140354707334912 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6915792226791382, loss=1.8313883543014526
I1013 04:56:36.893143 140354698942208 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7102006077766418, loss=1.7647455930709839
I1013 04:56:48.045075 140528462927680 spec.py:321] Evaluating on the training split.
I1013 04:57:41.346146 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 04:58:31.767516 140528462927680 spec.py:349] Evaluating on the test split.
I1013 04:58:56.941208 140528462927680 submission_runner.py:393] Time since start: 9677.30s, 	Step: 11015, 	{'train/ctc_loss': Array(0.5553886, dtype=float32), 'train/wer': 0.191095272897984, 'validation/ctc_loss': Array(0.8514675, dtype=float32), 'validation/wer': 0.2508085771937785, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5711759, dtype=float32), 'test/wer': 0.1898320232364471, 'test/num_examples': 2472, 'score': 8718.165016412735, 'total_duration': 9677.299626350403, 'accumulated_submission_time': 8718.165016412735, 'accumulated_eval_time': 958.379335641861, 'accumulated_logging_time': 0.3135843276977539}
I1013 04:58:56.983876 140354707334912 logging_writer.py:48] [11015] accumulated_eval_time=958.379336, accumulated_logging_time=0.313584, accumulated_submission_time=8718.165016, global_step=11015, preemption_count=0, score=8718.165016, test/ctc_loss=0.5711758732795715, test/num_examples=2472, test/wer=0.189832, total_duration=9677.299626, train/ctc_loss=0.5553886294364929, train/wer=0.191095, validation/ctc_loss=0.851467490196228, validation/num_examples=5348, validation/wer=0.250809
I1013 05:05:09.787702 140354707334912 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6761962175369263, loss=1.7856831550598145
I1013 05:11:35.031507 140354698942208 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6294034123420715, loss=1.7787461280822754
I1013 05:18:26.145286 140355792774912 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6570107936859131, loss=1.7557015419006348
I1013 05:22:57.218600 140528462927680 spec.py:321] Evaluating on the training split.
I1013 05:23:51.022885 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 05:24:41.062847 140528462927680 spec.py:349] Evaluating on the test split.
I1013 05:25:06.860305 140528462927680 submission_runner.py:393] Time since start: 11247.22s, 	Step: 12857, 	{'train/ctc_loss': Array(0.49079964, dtype=float32), 'train/wer': 0.16994767404760386, 'validation/ctc_loss': Array(0.786208, dtype=float32), 'validation/wer': 0.2355928439711519, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5218694, dtype=float32), 'test/wer': 0.17476083114983854, 'test/num_examples': 2472, 'score': 10158.31107378006, 'total_duration': 11247.22007727623, 'accumulated_submission_time': 10158.31107378006, 'accumulated_eval_time': 1088.0160675048828, 'accumulated_logging_time': 0.373049259185791}
I1013 05:25:06.902816 140355792774912 logging_writer.py:48] [12857] accumulated_eval_time=1088.016068, accumulated_logging_time=0.373049, accumulated_submission_time=10158.311074, global_step=12857, preemption_count=0, score=10158.311074, test/ctc_loss=0.521869421005249, test/num_examples=2472, test/wer=0.174761, total_duration=11247.220077, train/ctc_loss=0.4907996356487274, train/wer=0.169948, validation/ctc_loss=0.7862079739570618, validation/num_examples=5348, validation/wer=0.235593
I1013 05:26:56.545898 140355784382208 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7578032612800598, loss=1.7205896377563477
I1013 05:33:23.427960 140355792774912 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.9748840928077698, loss=1.8019130229949951
I1013 05:39:50.445271 140355784382208 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7065091729164124, loss=1.7982319593429565
I1013 05:46:43.597048 140355465094912 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7675610780715942, loss=1.7272847890853882
I1013 05:49:07.392152 140528462927680 spec.py:321] Evaluating on the training split.
I1013 05:50:01.277181 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 05:50:51.535779 140528462927680 spec.py:349] Evaluating on the test split.
I1013 05:51:16.885230 140528462927680 submission_runner.py:393] Time since start: 12817.24s, 	Step: 14690, 	{'train/ctc_loss': Array(0.4396304, dtype=float32), 'train/wer': 0.15606750045503004, 'validation/ctc_loss': Array(0.76096594, dtype=float32), 'validation/wer': 0.22763740984967706, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50188094, dtype=float32), 'test/wer': 0.16592529401011516, 'test/num_examples': 2472, 'score': 11598.714666128159, 'total_duration': 12817.244627475739, 'accumulated_submission_time': 11598.714666128159, 'accumulated_eval_time': 1217.5038132667542, 'accumulated_logging_time': 0.4322206974029541}
I1013 05:51:16.923126 140355173254912 logging_writer.py:48] [14690] accumulated_eval_time=1217.503813, accumulated_logging_time=0.432221, accumulated_submission_time=11598.714666, global_step=14690, preemption_count=0, score=11598.714666, test/ctc_loss=0.501880943775177, test/num_examples=2472, test/wer=0.165925, total_duration=12817.244627, train/ctc_loss=0.439630389213562, train/wer=0.156068, validation/ctc_loss=0.7609659433364868, validation/num_examples=5348, validation/wer=0.227637
I1013 05:55:13.628688 140355164862208 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.63823002576828, loss=1.6962600946426392
I1013 06:01:56.610905 140355792774912 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7323006391525269, loss=1.7528541088104248
I1013 06:08:18.011134 140355784382208 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9918139576911926, loss=1.7564125061035156
I1013 06:15:17.022002 140528462927680 spec.py:321] Evaluating on the training split.
I1013 06:16:10.622553 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 06:17:01.020493 140528462927680 spec.py:349] Evaluating on the test split.
I1013 06:17:26.776697 140528462927680 submission_runner.py:393] Time since start: 14387.13s, 	Step: 16497, 	{'train/ctc_loss': Array(0.42868078, dtype=float32), 'train/wer': 0.15134942463916923, 'validation/ctc_loss': Array(0.7672707, dtype=float32), 'validation/wer': 0.22576440715602886, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50134116, dtype=float32), 'test/wer': 0.16748928564174437, 'test/num_examples': 2472, 'score': 13038.728790521622, 'total_duration': 14387.134721279144, 'accumulated_submission_time': 13038.728790521622, 'accumulated_eval_time': 1347.2519481182098, 'accumulated_logging_time': 0.48560333251953125}
I1013 06:17:26.817839 140354845574912 logging_writer.py:48] [16497] accumulated_eval_time=1347.251948, accumulated_logging_time=0.485603, accumulated_submission_time=13038.728791, global_step=16497, preemption_count=0, score=13038.728791, test/ctc_loss=0.5013411641120911, test/num_examples=2472, test/wer=0.167489, total_duration=14387.134721, train/ctc_loss=0.42868077754974365, train/wer=0.151349, validation/ctc_loss=0.7672706842422485, validation/num_examples=5348, validation/wer=0.225764
I1013 06:17:29.944961 140354837182208 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6970678567886353, loss=1.7342135906219482
I1013 06:23:49.710895 140354845574912 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.841924786567688, loss=1.6834535598754883
I1013 06:30:32.586189 140354837182208 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8511722087860107, loss=1.6472501754760742
I1013 06:36:57.053909 140355792774912 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.0885487794876099, loss=1.7538179159164429
I1013 06:41:27.394079 140528462927680 spec.py:321] Evaluating on the training split.
I1013 06:42:21.342961 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 06:43:11.728024 140528462927680 spec.py:349] Evaluating on the test split.
I1013 06:43:37.324084 140528462927680 submission_runner.py:393] Time since start: 15957.68s, 	Step: 18329, 	{'train/ctc_loss': Array(0.4167879, dtype=float32), 'train/wer': 0.14965650758579266, 'validation/ctc_loss': Array(0.72004986, dtype=float32), 'validation/wer': 0.21597458895314597, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47295943, dtype=float32), 'test/wer': 0.15739443056486502, 'test/num_examples': 2472, 'score': 14479.21630859375, 'total_duration': 15957.682151556015, 'accumulated_submission_time': 14479.21630859375, 'accumulated_eval_time': 1477.175256729126, 'accumulated_logging_time': 0.5452520847320557}
I1013 06:43:37.356687 140355209094912 logging_writer.py:48] [18329] accumulated_eval_time=1477.175257, accumulated_logging_time=0.545252, accumulated_submission_time=14479.216309, global_step=18329, preemption_count=0, score=14479.216309, test/ctc_loss=0.47295942902565, test/num_examples=2472, test/wer=0.157394, total_duration=15957.682152, train/ctc_loss=0.41678789258003235, train/wer=0.149657, validation/ctc_loss=0.7200498580932617, validation/num_examples=5348, validation/wer=0.215975
I1013 06:45:48.026241 140355200702208 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.9225265979766846, loss=1.7155654430389404
I1013 06:52:12.289030 140354881414912 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.3321025371551514, loss=1.6430922746658325
I1013 06:59:01.757719 140354873022208 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7475751638412476, loss=1.65803062915802
I1013 07:05:29.829874 140355209094912 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9968827366828918, loss=1.660699486732483
I1013 07:07:37.521446 140528462927680 spec.py:321] Evaluating on the training split.
I1013 07:08:32.155169 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 07:09:23.443993 140528462927680 spec.py:349] Evaluating on the test split.
I1013 07:09:49.429353 140528462927680 submission_runner.py:393] Time since start: 17529.79s, 	Step: 20165, 	{'train/ctc_loss': Array(0.4170413, dtype=float32), 'train/wer': 0.14343055078135328, 'validation/ctc_loss': Array(0.7015538, dtype=float32), 'validation/wer': 0.20741091168888845, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45137963, dtype=float32), 'test/wer': 0.14815266183251072, 'test/num_examples': 2472, 'score': 15919.290590047836, 'total_duration': 17529.786338329315, 'accumulated_submission_time': 15919.290590047836, 'accumulated_eval_time': 1609.0753996372223, 'accumulated_logging_time': 0.5965089797973633}
I1013 07:09:49.462764 140355209094912 logging_writer.py:48] [20165] accumulated_eval_time=1609.075400, accumulated_logging_time=0.596509, accumulated_submission_time=15919.290590, global_step=20165, preemption_count=0, score=15919.290590, test/ctc_loss=0.4513796269893646, test/num_examples=2472, test/wer=0.148153, total_duration=17529.786338, train/ctc_loss=0.4170413017272949, train/wer=0.143431, validation/ctc_loss=0.7015538215637207, validation/num_examples=5348, validation/wer=0.207411
I1013 07:14:05.281109 140355200702208 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.8278096914291382, loss=1.6287364959716797
I1013 07:20:31.202351 140354553734912 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.9540613889694214, loss=1.5993152856826782
I1013 07:27:13.774142 140354545342208 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7877712845802307, loss=1.6243476867675781
I1013 07:33:46.027032 140355209094912 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6582568883895874, loss=1.5972265005111694
I1013 07:33:49.564470 140528462927680 spec.py:321] Evaluating on the training split.
I1013 07:34:44.162559 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 07:35:34.760437 140528462927680 spec.py:349] Evaluating on the test split.
I1013 07:36:00.494435 140528462927680 submission_runner.py:393] Time since start: 19100.85s, 	Step: 22006, 	{'train/ctc_loss': Array(0.40213054, dtype=float32), 'train/wer': 0.13534703352942729, 'validation/ctc_loss': Array(0.6727824, dtype=float32), 'validation/wer': 0.19780453189414637, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42915112, dtype=float32), 'test/wer': 0.14114516685962666, 'test/num_examples': 2472, 'score': 17359.304765701294, 'total_duration': 19100.8536298275, 'accumulated_submission_time': 17359.304765701294, 'accumulated_eval_time': 1739.999810218811, 'accumulated_logging_time': 0.6459355354309082}
I1013 07:36:00.528819 140355209094912 logging_writer.py:48] [22006] accumulated_eval_time=1739.999810, accumulated_logging_time=0.645936, accumulated_submission_time=17359.304766, global_step=22006, preemption_count=0, score=17359.304766, test/ctc_loss=0.42915111780166626, test/num_examples=2472, test/wer=0.141145, total_duration=19100.853630, train/ctc_loss=0.4021305441856384, train/wer=0.135347, validation/ctc_loss=0.6727824211120605, validation/num_examples=5348, validation/wer=0.197805
I1013 07:42:21.768233 140355200702208 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.9212480783462524, loss=1.720049262046814
I1013 07:48:59.480833 140354553734912 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7976234555244446, loss=1.592248558998108
I1013 07:55:41.751160 140354545342208 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.7658420205116272, loss=1.6146045923233032
I1013 08:00:00.738646 140528462927680 spec.py:321] Evaluating on the training split.
I1013 08:00:55.672053 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 08:01:46.548167 140528462927680 spec.py:349] Evaluating on the test split.
I1013 08:02:12.354275 140528462927680 submission_runner.py:393] Time since start: 20672.71s, 	Step: 23818, 	{'train/ctc_loss': Array(0.38689435, dtype=float32), 'train/wer': 0.13650581335528347, 'validation/ctc_loss': Array(0.66221136, dtype=float32), 'validation/wer': 0.19869275997567026, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42067477, dtype=float32), 'test/wer': 0.14224199215973027, 'test/num_examples': 2472, 'score': 18799.42692923546, 'total_duration': 20672.712401151657, 'accumulated_submission_time': 18799.42692923546, 'accumulated_eval_time': 1871.6090109348297, 'accumulated_logging_time': 0.6956217288970947}
I1013 08:02:12.387182 140355362694912 logging_writer.py:48] [23818] accumulated_eval_time=1871.609011, accumulated_logging_time=0.695622, accumulated_submission_time=18799.426929, global_step=23818, preemption_count=0, score=18799.426929, test/ctc_loss=0.42067477107048035, test/num_examples=2472, test/wer=0.142242, total_duration=20672.712401, train/ctc_loss=0.3868943452835083, train/wer=0.136506, validation/ctc_loss=0.6622113585472107, validation/num_examples=5348, validation/wer=0.198693
I1013 08:04:31.439161 140355354302208 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.9361630082130432, loss=1.656238079071045
I1013 08:11:07.539690 140355362694912 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.7908187508583069, loss=1.5800210237503052
I1013 08:17:48.741008 140355362694912 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.772088885307312, loss=1.5202034711837769
I1013 08:24:27.051421 140355354302208 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.9564769268035889, loss=1.562117576599121
I1013 08:26:12.551573 140528462927680 spec.py:321] Evaluating on the training split.
I1013 08:27:07.141355 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 08:27:57.295261 140528462927680 spec.py:349] Evaluating on the test split.
I1013 08:28:22.968227 140528462927680 submission_runner.py:393] Time since start: 22243.33s, 	Step: 25628, 	{'train/ctc_loss': Array(0.36858398, dtype=float32), 'train/wer': 0.1305265824922934, 'validation/ctc_loss': Array(0.661228, dtype=float32), 'validation/wer': 0.1961632408739392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4166255, dtype=float32), 'test/wer': 0.13917494363536653, 'test/num_examples': 2472, 'score': 20239.503421783447, 'total_duration': 22243.326597213745, 'accumulated_submission_time': 20239.503421783447, 'accumulated_eval_time': 2002.0192692279816, 'accumulated_logging_time': 0.7444384098052979}
I1013 08:28:23.004683 140355362694912 logging_writer.py:48] [25628] accumulated_eval_time=2002.019269, accumulated_logging_time=0.744438, accumulated_submission_time=20239.503422, global_step=25628, preemption_count=0, score=20239.503422, test/ctc_loss=0.4166254997253418, test/num_examples=2472, test/wer=0.139175, total_duration=22243.326597, train/ctc_loss=0.3685839772224426, train/wer=0.130527, validation/ctc_loss=0.6612280011177063, validation/num_examples=5348, validation/wer=0.196163
I1013 08:33:10.016027 140355362694912 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.9461421370506287, loss=1.553466796875
I1013 08:39:50.379036 140355354302208 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.9741325974464417, loss=1.546184778213501
I1013 08:46:38.972412 140355362694912 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.1030057668685913, loss=1.5828745365142822
I1013 08:52:23.672311 140528462927680 spec.py:321] Evaluating on the training split.
I1013 08:53:18.817005 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 08:54:09.481840 140528462927680 spec.py:349] Evaluating on the test split.
I1013 08:54:34.993202 140528462927680 submission_runner.py:393] Time since start: 23815.35s, 	Step: 27440, 	{'train/ctc_loss': Array(0.3263501, dtype=float32), 'train/wer': 0.1158767834320144, 'validation/ctc_loss': Array(0.615209, dtype=float32), 'validation/wer': 0.18446180136516793, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38853222, dtype=float32), 'test/wer': 0.12867385696585623, 'test/num_examples': 2472, 'score': 21680.08334159851, 'total_duration': 23815.351699590683, 'accumulated_submission_time': 21680.08334159851, 'accumulated_eval_time': 2133.3339269161224, 'accumulated_logging_time': 0.7971978187561035}
I1013 08:54:35.035923 140355792774912 logging_writer.py:48] [27440] accumulated_eval_time=2133.333927, accumulated_logging_time=0.797198, accumulated_submission_time=21680.083342, global_step=27440, preemption_count=0, score=21680.083342, test/ctc_loss=0.38853222131729126, test/num_examples=2472, test/wer=0.128674, total_duration=23815.351700, train/ctc_loss=0.3263500928878784, train/wer=0.115877, validation/ctc_loss=0.6152089834213257, validation/num_examples=5348, validation/wer=0.184462
I1013 08:55:21.399593 140355784382208 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.0159040689468384, loss=1.5123196840286255
I1013 09:01:45.446627 140355792774912 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.9501017928123474, loss=1.603981852531433
I1013 09:08:21.531209 140355784382208 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7282379865646362, loss=1.5044898986816406
I1013 09:15:12.629445 140355465094912 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6361372470855713, loss=1.4822978973388672
I1013 09:18:35.645403 140528462927680 spec.py:321] Evaluating on the training split.
I1013 09:19:30.977670 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 09:20:21.626179 140528462927680 spec.py:349] Evaluating on the test split.
I1013 09:20:47.267311 140528462927680 submission_runner.py:393] Time since start: 25387.63s, 	Step: 29268, 	{'train/ctc_loss': Array(0.3176866, dtype=float32), 'train/wer': 0.11241574700064436, 'validation/ctc_loss': Array(0.59929836, dtype=float32), 'validation/wer': 0.1794317271208859, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37526193, dtype=float32), 'test/wer': 0.12593179371559726, 'test/num_examples': 2472, 'score': 23120.60368990898, 'total_duration': 25387.625465154648, 'accumulated_submission_time': 23120.60368990898, 'accumulated_eval_time': 2264.9492416381836, 'accumulated_logging_time': 0.8556094169616699}
I1013 09:20:47.305404 140355500934912 logging_writer.py:48] [29268] accumulated_eval_time=2264.949242, accumulated_logging_time=0.855609, accumulated_submission_time=23120.603690, global_step=29268, preemption_count=0, score=23120.603690, test/ctc_loss=0.37526193261146545, test/num_examples=2472, test/wer=0.125932, total_duration=25387.625465, train/ctc_loss=0.3176865875720978, train/wer=0.112416, validation/ctc_loss=0.599298357963562, validation/num_examples=5348, validation/wer=0.179432
I1013 09:23:44.630208 140355492542208 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.763331413269043, loss=1.517296552658081
I1013 09:30:24.645911 140355500934912 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7841510772705078, loss=1.4810049533843994
I1013 09:36:55.498231 140355492542208 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.9480592608451843, loss=1.4455633163452148
I1013 09:43:49.994965 140355500934912 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.8516393899917603, loss=1.4202938079833984
I1013 09:44:47.464353 140528462927680 spec.py:321] Evaluating on the training split.
I1013 09:45:40.691869 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 09:46:31.975111 140528462927680 spec.py:349] Evaluating on the test split.
I1013 09:46:57.729751 140528462927680 submission_runner.py:393] Time since start: 26958.09s, 	Step: 31077, 	{'train/ctc_loss': Array(0.32796368, dtype=float32), 'train/wer': 0.11549878661813139, 'validation/ctc_loss': Array(0.5954125, dtype=float32), 'validation/wer': 0.1763905114069726, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3714614, dtype=float32), 'test/wer': 0.12272256413381269, 'test/num_examples': 2472, 'score': 24560.674696683884, 'total_duration': 26958.088918447495, 'accumulated_submission_time': 24560.674696683884, 'accumulated_eval_time': 2395.2090513706207, 'accumulated_logging_time': 0.9091525077819824}
I1013 09:46:57.766575 140355209094912 logging_writer.py:48] [31077] accumulated_eval_time=2395.209051, accumulated_logging_time=0.909153, accumulated_submission_time=24560.674697, global_step=31077, preemption_count=0, score=24560.674697, test/ctc_loss=0.3714613914489746, test/num_examples=2472, test/wer=0.122723, total_duration=26958.088918, train/ctc_loss=0.3279636800289154, train/wer=0.115499, validation/ctc_loss=0.5954124927520752, validation/num_examples=5348, validation/wer=0.176391
I1013 09:52:20.482969 140355200702208 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.9851430058479309, loss=1.4178422689437866
I1013 09:59:05.932181 140354553734912 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.1431398391723633, loss=1.423342227935791
I1013 10:05:27.008595 140354545342208 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.1281191110610962, loss=1.492112636566162
I1013 10:10:57.914047 140528462927680 spec.py:321] Evaluating on the training split.
I1013 10:11:52.146842 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 10:12:42.851968 140528462927680 spec.py:349] Evaluating on the test split.
I1013 10:13:09.012071 140528462927680 submission_runner.py:393] Time since start: 28529.37s, 	Step: 32900, 	{'train/ctc_loss': Array(0.29728517, dtype=float32), 'train/wer': 0.10219263262415966, 'validation/ctc_loss': Array(0.56708217, dtype=float32), 'validation/wer': 0.16756615851009393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35162997, dtype=float32), 'test/wer': 0.11872118294639775, 'test/num_examples': 2472, 'score': 26000.73390674591, 'total_duration': 28529.370970726013, 'accumulated_submission_time': 26000.73390674591, 'accumulated_eval_time': 2526.301216840744, 'accumulated_logging_time': 0.9616312980651855}
I1013 10:13:09.051482 140355362694912 logging_writer.py:48] [32900] accumulated_eval_time=2526.301217, accumulated_logging_time=0.961631, accumulated_submission_time=26000.733907, global_step=32900, preemption_count=0, score=26000.733907, test/ctc_loss=0.35162997245788574, test/num_examples=2472, test/wer=0.118721, total_duration=28529.370971, train/ctc_loss=0.29728516936302185, train/wer=0.102193, validation/ctc_loss=0.5670821666717529, validation/num_examples=5348, validation/wer=0.167566
I1013 10:14:29.118088 140355362694912 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.8449006676673889, loss=1.478101134300232
I1013 10:20:49.927380 140355354302208 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.088921070098877, loss=1.390776515007019
I1013 10:27:49.745604 140355362694912 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.855129599571228, loss=1.4615132808685303
I1013 10:34:10.142957 140355354302208 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.6975349187850952, loss=1.4300175905227661
I1013 10:37:10.032754 140528462927680 spec.py:321] Evaluating on the training split.
I1013 10:38:05.662542 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 10:38:56.972564 140528462927680 spec.py:349] Evaluating on the test split.
I1013 10:39:22.997733 140528462927680 submission_runner.py:393] Time since start: 30103.36s, 	Step: 34724, 	{'train/ctc_loss': Array(0.2909537, dtype=float32), 'train/wer': 0.10144912646594989, 'validation/ctc_loss': Array(0.5382805, dtype=float32), 'validation/wer': 0.16125201540882628, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3259944, dtype=float32), 'test/wer': 0.11094184794751488, 'test/num_examples': 2472, 'score': 27441.624930858612, 'total_duration': 30103.356734752655, 'accumulated_submission_time': 27441.624930858612, 'accumulated_eval_time': 2659.260458946228, 'accumulated_logging_time': 1.0185678005218506}
I1013 10:39:23.037916 140355792774912 logging_writer.py:48] [34724] accumulated_eval_time=2659.260459, accumulated_logging_time=1.018568, accumulated_submission_time=27441.624931, global_step=34724, preemption_count=0, score=27441.624931, test/ctc_loss=0.3259944021701813, test/num_examples=2472, test/wer=0.110942, total_duration=30103.356735, train/ctc_loss=0.29095369577407837, train/wer=0.101449, validation/ctc_loss=0.5382804870605469, validation/num_examples=5348, validation/wer=0.161252
I1013 10:42:54.183006 140355784382208 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7161647081375122, loss=1.4677568674087524
I1013 10:49:19.250743 140355465094912 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7918859124183655, loss=1.4121736288070679
I1013 10:56:12.692604 140355456702208 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.1067988872528076, loss=1.3852647542953491
I1013 11:02:40.896326 140355465094912 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.864850640296936, loss=1.3636473417282104
I1013 11:03:23.387229 140528462927680 spec.py:321] Evaluating on the training split.
I1013 11:04:17.928203 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 11:05:08.998847 140528462927680 spec.py:349] Evaluating on the test split.
I1013 11:05:34.730118 140528462927680 submission_runner.py:393] Time since start: 31675.09s, 	Step: 36557, 	{'train/ctc_loss': Array(0.27816463, dtype=float32), 'train/wer': 0.09440838901499408, 'validation/ctc_loss': Array(0.52119833, dtype=float32), 'validation/wer': 0.154570995491277, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3078582, dtype=float32), 'test/wer': 0.10167976763552901, 'test/num_examples': 2472, 'score': 28881.882888317108, 'total_duration': 31675.08744454384, 'accumulated_submission_time': 28881.882888317108, 'accumulated_eval_time': 2790.595918416977, 'accumulated_logging_time': 1.0748741626739502}
I1013 11:05:34.772484 140354881414912 logging_writer.py:48] [36557] accumulated_eval_time=2790.595918, accumulated_logging_time=1.074874, accumulated_submission_time=28881.882888, global_step=36557, preemption_count=0, score=28881.882888, test/ctc_loss=0.3078581988811493, test/num_examples=2472, test/wer=0.101680, total_duration=31675.087445, train/ctc_loss=0.2781646251678467, train/wer=0.094408, validation/ctc_loss=0.5211983323097229, validation/num_examples=5348, validation/wer=0.154571
I1013 11:11:17.134393 140354873022208 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7843052744865417, loss=1.3989728689193726
I1013 11:17:47.577282 140354553734912 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.9664681553840637, loss=1.4256484508514404
I1013 11:24:40.849910 140354545342208 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.9584954380989075, loss=1.3685612678527832
I1013 11:29:35.363717 140528462927680 spec.py:321] Evaluating on the training split.
I1013 11:30:30.649056 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 11:31:21.892086 140528462927680 spec.py:349] Evaluating on the test split.
I1013 11:31:47.668806 140528462927680 submission_runner.py:393] Time since start: 33248.03s, 	Step: 38372, 	{'train/ctc_loss': Array(0.21717745, dtype=float32), 'train/wer': 0.07710402509147936, 'validation/ctc_loss': Array(0.5061055, dtype=float32), 'validation/wer': 0.14884578622667194, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30427524, dtype=float32), 'test/wer': 0.10080636971137245, 'test/num_examples': 2472, 'score': 30322.385301828384, 'total_duration': 33248.02740955353, 'accumulated_submission_time': 30322.385301828384, 'accumulated_eval_time': 2922.894958257675, 'accumulated_logging_time': 1.1327526569366455}
I1013 11:31:47.713586 140354553734912 logging_writer.py:48] [38372] accumulated_eval_time=2922.894958, accumulated_logging_time=1.132753, accumulated_submission_time=30322.385302, global_step=38372, preemption_count=0, score=30322.385302, test/ctc_loss=0.304275244474411, test/num_examples=2472, test/wer=0.100806, total_duration=33248.027410, train/ctc_loss=0.21717745065689087, train/wer=0.077104, validation/ctc_loss=0.5061054825782776, validation/num_examples=5348, validation/wer=0.148846
I1013 11:33:25.780262 140354545342208 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.8567405939102173, loss=1.3288897275924683
I1013 11:40:04.878128 140354553734912 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.1913198232650757, loss=1.3547072410583496
I1013 11:46:41.320721 140355792774912 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.810169517993927, loss=1.348851203918457
I1013 11:53:30.684360 140355784382208 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.8731157183647156, loss=1.3734036684036255
I1013 11:55:47.971460 140528462927680 spec.py:321] Evaluating on the training split.
I1013 11:56:41.986757 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 11:57:32.945480 140528462927680 spec.py:349] Evaluating on the test split.
I1013 11:57:58.650917 140528462927680 submission_runner.py:393] Time since start: 34819.01s, 	Step: 40164, 	{'train/ctc_loss': Array(0.23430294, dtype=float32), 'train/wer': 0.08162446410846656, 'validation/ctc_loss': Array(0.4788264, dtype=float32), 'validation/wer': 0.14221303957442288, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2849079, dtype=float32), 'test/wer': 0.09323015050880507, 'test/num_examples': 2472, 'score': 31762.552495718002, 'total_duration': 34819.008442640305, 'accumulated_submission_time': 31762.552495718002, 'accumulated_eval_time': 3053.5671892166138, 'accumulated_logging_time': 1.1970024108886719}
I1013 11:57:58.692825 140354912130816 logging_writer.py:48] [40164] accumulated_eval_time=3053.567189, accumulated_logging_time=1.197002, accumulated_submission_time=31762.552496, global_step=40164, preemption_count=0, score=31762.552496, test/ctc_loss=0.28490790724754333, test/num_examples=2472, test/wer=0.093230, total_duration=34819.008443, train/ctc_loss=0.23430293798446655, train/wer=0.081624, validation/ctc_loss=0.4788264036178589, validation/num_examples=5348, validation/wer=0.142213
I1013 12:02:18.818385 140354912130816 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.0502760410308838, loss=1.3537907600402832
I1013 12:09:12.630004 140354903738112 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.0224578380584717, loss=1.2723915576934814
I1013 12:15:57.748103 140354584450816 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.9789161682128906, loss=1.32963228225708
I1013 12:21:59.414806 140528462927680 spec.py:321] Evaluating on the training split.
I1013 12:22:53.230364 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 12:23:44.212855 140528462927680 spec.py:349] Evaluating on the test split.
I1013 12:24:10.216212 140528462927680 submission_runner.py:393] Time since start: 36390.57s, 	Step: 41948, 	{'train/ctc_loss': Array(0.27890697, dtype=float32), 'train/wer': 0.09769313799406605, 'validation/ctc_loss': Array(0.45374945, dtype=float32), 'validation/wer': 0.13564787549359414, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26860154, dtype=float32), 'test/wer': 0.08896471878618, 'test/num_examples': 2472, 'score': 33203.18786692619, 'total_duration': 36390.5732922554, 'accumulated_submission_time': 33203.18786692619, 'accumulated_eval_time': 3184.3609116077423, 'accumulated_logging_time': 1.2530229091644287}
I1013 12:24:10.258077 140355362694912 logging_writer.py:48] [41948] accumulated_eval_time=3184.360912, accumulated_logging_time=1.253023, accumulated_submission_time=33203.187867, global_step=41948, preemption_count=0, score=33203.187867, test/ctc_loss=0.26860153675079346, test/num_examples=2472, test/wer=0.088965, total_duration=36390.573292, train/ctc_loss=0.2789069712162018, train/wer=0.097693, validation/ctc_loss=0.4537494480609894, validation/num_examples=5348, validation/wer=0.135648
I1013 12:24:50.443730 140355354302208 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.0932800769805908, loss=1.272399663925171
I1013 12:31:15.163047 140355362694912 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.9650406241416931, loss=1.3027915954589844
I1013 12:37:57.616480 140355354302208 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.9777812361717224, loss=1.3081098794937134
I1013 12:44:43.510185 140354707334912 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.0754328966140747, loss=1.2678780555725098
I1013 12:48:10.218357 140528462927680 spec.py:321] Evaluating on the training split.
I1013 12:49:03.499826 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 12:49:54.690635 140528462927680 spec.py:349] Evaluating on the test split.
I1013 12:50:20.672746 140528462927680 submission_runner.py:393] Time since start: 37961.03s, 	Step: 43773, 	{'train/ctc_loss': Array(0.273045, dtype=float32), 'train/wer': 0.09574934633084818, 'validation/ctc_loss': Array(0.44358933, dtype=float32), 'validation/wer': 0.13138051884105545, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25235382, dtype=float32), 'test/wer': 0.08473991022281803, 'test/num_examples': 2472, 'score': 34643.059309482574, 'total_duration': 37961.0304582119, 'accumulated_submission_time': 34643.059309482574, 'accumulated_eval_time': 3314.808256626129, 'accumulated_logging_time': 1.3121247291564941}
I1013 12:50:20.716809 140355792774912 logging_writer.py:48] [43773] accumulated_eval_time=3314.808257, accumulated_logging_time=1.312125, accumulated_submission_time=34643.059309, global_step=43773, preemption_count=0, score=34643.059309, test/ctc_loss=0.25235381722450256, test/num_examples=2472, test/wer=0.084740, total_duration=37961.030458, train/ctc_loss=0.27304500341415405, train/wer=0.095749, validation/ctc_loss=0.44358932971954346, validation/num_examples=5348, validation/wer=0.131381
I1013 12:53:14.418069 140355784382208 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.934420645236969, loss=1.1951115131378174
I1013 12:59:46.242134 140355792774912 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.4628984928131104, loss=1.2953592538833618
I1013 13:06:21.389214 140355784382208 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.4425960779190063, loss=1.239399790763855
I1013 13:13:14.242000 140355792774912 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.4812406301498413, loss=1.2443585395812988
I1013 13:14:21.140642 140528462927680 spec.py:321] Evaluating on the training split.
I1013 13:15:12.471156 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 13:16:03.413552 140528462927680 spec.py:349] Evaluating on the test split.
I1013 13:16:29.438879 140528462927680 submission_runner.py:393] Time since start: 39529.80s, 	Step: 45589, 	{'train/ctc_loss': Array(0.2973748, dtype=float32), 'train/wer': 0.10500084819626676, 'validation/ctc_loss': Array(0.4245359, dtype=float32), 'validation/wer': 0.12656284696409434, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24460913, dtype=float32), 'test/wer': 0.08236345540592692, 'test/num_examples': 2472, 'score': 36083.39624404907, 'total_duration': 39529.795149326324, 'accumulated_submission_time': 36083.39624404907, 'accumulated_eval_time': 3443.097991466522, 'accumulated_logging_time': 1.371171474456787}
I1013 13:16:29.477871 140355792774912 logging_writer.py:48] [45589] accumulated_eval_time=3443.097991, accumulated_logging_time=1.371171, accumulated_submission_time=36083.396244, global_step=45589, preemption_count=0, score=36083.396244, test/ctc_loss=0.24460913240909576, test/num_examples=2472, test/wer=0.082363, total_duration=39529.795149, train/ctc_loss=0.29737481474876404, train/wer=0.105001, validation/ctc_loss=0.4245359003543854, validation/num_examples=5348, validation/wer=0.126563
I1013 13:21:42.898361 140355784382208 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.066414475440979, loss=1.231249213218689
I1013 13:28:35.566851 140355792774912 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.0798298120498657, loss=1.1535269021987915
I1013 13:35:03.006510 140355784382208 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.0725387334823608, loss=1.1822693347930908
I1013 13:40:30.014835 140528462927680 spec.py:321] Evaluating on the training split.
I1013 13:41:21.526671 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 13:42:12.793157 140528462927680 spec.py:349] Evaluating on the test split.
I1013 13:42:38.747007 140528462927680 submission_runner.py:393] Time since start: 41099.11s, 	Step: 47386, 	{'train/ctc_loss': Array(0.25112334, dtype=float32), 'train/wer': 0.086400273183828, 'validation/ctc_loss': Array(0.40379113, dtype=float32), 'validation/wer': 0.1185398302711992, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23312776, dtype=float32), 'test/wer': 0.07588406150346312, 'test/num_examples': 2472, 'score': 37523.8457300663, 'total_duration': 41099.10501933098, 'accumulated_submission_time': 37523.8457300663, 'accumulated_eval_time': 3571.823614358902, 'accumulated_logging_time': 1.4255969524383545}
I1013 13:42:38.798476 140354912130816 logging_writer.py:48] [47386] accumulated_eval_time=3571.823614, accumulated_logging_time=1.425597, accumulated_submission_time=37523.845730, global_step=47386, preemption_count=0, score=37523.845730, test/ctc_loss=0.23312775790691376, test/num_examples=2472, test/wer=0.075884, total_duration=41099.105019, train/ctc_loss=0.2511233389377594, train/wer=0.086400, validation/ctc_loss=0.4037911295890808, validation/num_examples=5348, validation/wer=0.118540
I1013 13:44:06.274936 140354903738112 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.0118155479431152, loss=1.1492618322372437
I1013 13:50:27.778251 140354912130816 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.4724525213241577, loss=1.192171335220337
I1013 13:57:27.322395 140354912130816 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.9771971702575684, loss=1.1277403831481934
I1013 14:03:53.568757 140354903738112 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.163621187210083, loss=1.1696220636367798
I1013 14:06:38.881486 140528462927680 spec.py:321] Evaluating on the training split.
I1013 14:07:31.754918 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 14:08:22.758913 140528462927680 spec.py:349] Evaluating on the test split.
I1013 14:08:48.592505 140528462927680 submission_runner.py:393] Time since start: 42668.95s, 	Step: 49198, 	{'train/ctc_loss': Array(0.21648009, dtype=float32), 'train/wer': 0.07842405887789072, 'validation/ctc_loss': Array(0.38508216, dtype=float32), 'validation/wer': 0.11409868986357975, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21575205, dtype=float32), 'test/wer': 0.07147644872341723, 'test/num_examples': 2472, 'score': 38963.83891916275, 'total_duration': 42668.95094180107, 'accumulated_submission_time': 38963.83891916275, 'accumulated_eval_time': 3701.5283229351044, 'accumulated_logging_time': 1.4950025081634521}
I1013 14:08:48.630431 140354912130816 logging_writer.py:48] [49198] accumulated_eval_time=3701.528323, accumulated_logging_time=1.495003, accumulated_submission_time=38963.838919, global_step=49198, preemption_count=0, score=38963.838919, test/ctc_loss=0.21575205028057098, test/num_examples=2472, test/wer=0.071476, total_duration=42668.950942, train/ctc_loss=0.21648009121418, train/wer=0.078424, validation/ctc_loss=0.3850821554660797, validation/num_examples=5348, validation/wer=0.114099
I1013 14:12:42.985576 140354912130816 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1163115501403809, loss=1.1909445524215698
I1013 14:19:07.333444 140354903738112 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.143211007118225, loss=1.1559594869613647
I1013 14:26:05.441977 140354912130816 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.3887630701065063, loss=1.1132206916809082
I1013 14:32:28.176600 140354903738112 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.1746244430541992, loss=1.1173598766326904
I1013 14:32:48.963204 140528462927680 spec.py:321] Evaluating on the training split.
I1013 14:33:42.678606 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 14:34:33.702577 140528462927680 spec.py:349] Evaluating on the test split.
I1013 14:34:59.437296 140528462927680 submission_runner.py:393] Time since start: 44239.79s, 	Step: 51026, 	{'train/ctc_loss': Array(0.17452212, dtype=float32), 'train/wer': 0.06286637157554731, 'validation/ctc_loss': Array(0.3732639, dtype=float32), 'validation/wer': 0.10960927619066009, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2089306, dtype=float32), 'test/wer': 0.06775942965084394, 'test/num_examples': 2472, 'score': 40404.082528829575, 'total_duration': 44239.79353737831, 'accumulated_submission_time': 40404.082528829575, 'accumulated_eval_time': 3831.99388051033, 'accumulated_logging_time': 1.5474884510040283}
I1013 14:34:59.473583 140354912130816 logging_writer.py:48] [51026] accumulated_eval_time=3831.993881, accumulated_logging_time=1.547488, accumulated_submission_time=40404.082529, global_step=51026, preemption_count=0, score=40404.082529, test/ctc_loss=0.2089305967092514, test/num_examples=2472, test/wer=0.067759, total_duration=44239.793537, train/ctc_loss=0.17452211678028107, train/wer=0.062866, validation/ctc_loss=0.3732638955116272, validation/num_examples=5348, validation/wer=0.109609
I1013 14:41:11.597910 140354912130816 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.9904763698577881, loss=1.1482954025268555
I1013 14:47:33.425644 140354903738112 logging_writer.py:48] [52000] global_step=52000, grad_norm=2.0161283016204834, loss=1.1372791528701782
I1013 14:54:31.407884 140354912130816 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.2046866416931152, loss=1.0394937992095947
I1013 14:58:59.465453 140528462927680 spec.py:321] Evaluating on the training split.
I1013 14:59:53.303494 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 15:00:44.020327 140528462927680 spec.py:349] Evaluating on the test split.
I1013 15:01:09.856889 140528462927680 submission_runner.py:393] Time since start: 45810.22s, 	Step: 52845, 	{'train/ctc_loss': Array(0.18791229, dtype=float32), 'train/wer': 0.0680508562025124, 'validation/ctc_loss': Array(0.35731253, dtype=float32), 'validation/wer': 0.10542881141566178, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20236841, dtype=float32), 'test/wer': 0.06593138748400464, 'test/num_examples': 2472, 'score': 41843.98575305939, 'total_duration': 45810.21587514877, 'accumulated_submission_time': 41843.98575305939, 'accumulated_eval_time': 3962.37974858284, 'accumulated_logging_time': 1.6000745296478271}
I1013 15:01:09.898658 140354779014912 logging_writer.py:48] [52845] accumulated_eval_time=3962.379749, accumulated_logging_time=1.600075, accumulated_submission_time=41843.985753, global_step=52845, preemption_count=0, score=41843.985753, test/ctc_loss=0.20236840844154358, test/num_examples=2472, test/wer=0.065931, total_duration=45810.215875, train/ctc_loss=0.18791228532791138, train/wer=0.068051, validation/ctc_loss=0.35731253027915955, validation/num_examples=5348, validation/wer=0.105429
I1013 15:03:09.041471 140354770622208 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.8902472257614136, loss=1.0810739994049072
I1013 15:09:50.297399 140354779014912 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.1830320358276367, loss=1.0866665840148926
I1013 15:16:19.562581 140354779014912 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.8285821676254272, loss=1.0503053665161133
I1013 15:23:10.463129 140354770622208 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.9430389404296875, loss=1.095196008682251
I1013 15:25:10.097058 140528462927680 spec.py:321] Evaluating on the training split.
I1013 15:26:04.512123 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 15:26:55.178550 140528462927680 spec.py:349] Evaluating on the test split.
I1013 15:27:21.048075 140528462927680 submission_runner.py:393] Time since start: 47381.41s, 	Step: 54646, 	{'train/ctc_loss': Array(0.16191684, dtype=float32), 'train/wer': 0.05788846418141213, 'validation/ctc_loss': Array(0.34708014, dtype=float32), 'validation/wer': 0.10124834664066347, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19277844, dtype=float32), 'test/wer': 0.06243779578737838, 'test/num_examples': 2472, 'score': 43284.094190597534, 'total_duration': 47381.40700984001, 'accumulated_submission_time': 43284.094190597534, 'accumulated_eval_time': 4093.3252091407776, 'accumulated_logging_time': 1.6582086086273193}
I1013 15:27:21.085861 140354779014912 logging_writer.py:48] [54646] accumulated_eval_time=4093.325209, accumulated_logging_time=1.658209, accumulated_submission_time=43284.094191, global_step=54646, preemption_count=0, score=43284.094191, test/ctc_loss=0.19277843832969666, test/num_examples=2472, test/wer=0.062438, total_duration=47381.407010, train/ctc_loss=0.1619168370962143, train/wer=0.057888, validation/ctc_loss=0.34708014130592346, validation/num_examples=5348, validation/wer=0.101248
I1013 15:31:50.906634 140354770622208 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.052777647972107, loss=1.0536282062530518
I1013 15:38:27.946826 140354779014912 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.2115398645401, loss=1.0669044256210327
I1013 15:45:01.167562 140354779014912 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.4023877382278442, loss=1.0952892303466797
I1013 15:51:21.097634 140528462927680 spec.py:321] Evaluating on the training split.
I1013 15:52:15.123527 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 15:53:06.512087 140528462927680 spec.py:349] Evaluating on the test split.
I1013 15:53:32.201358 140528462927680 submission_runner.py:393] Time since start: 48952.56s, 	Step: 56474, 	{'train/ctc_loss': Array(0.16085176, dtype=float32), 'train/wer': 0.05753479642868766, 'validation/ctc_loss': Array(0.3371203, dtype=float32), 'validation/wer': 0.09872848219199243, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18818602, dtype=float32), 'test/wer': 0.061442528385432536, 'test/num_examples': 2472, 'score': 44724.01770877838, 'total_duration': 48952.56027507782, 'accumulated_submission_time': 44724.01770877838, 'accumulated_eval_time': 4224.423105239868, 'accumulated_logging_time': 1.7109320163726807}
I1013 15:53:32.242229 140354779014912 logging_writer.py:48] [56474] accumulated_eval_time=4224.423105, accumulated_logging_time=1.710932, accumulated_submission_time=44724.017709, global_step=56474, preemption_count=0, score=44724.017709, test/ctc_loss=0.18818601965904236, test/num_examples=2472, test/wer=0.061443, total_duration=48952.560275, train/ctc_loss=0.16085176169872284, train/wer=0.057535, validation/ctc_loss=0.33712029457092285, validation/num_examples=5348, validation/wer=0.098728
I1013 15:53:52.833984 140354770622208 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.2241950035095215, loss=1.0804554224014282
I1013 16:00:17.171308 140354779014912 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.3275706768035889, loss=1.0690025091171265
I1013 16:07:03.471015 140354770622208 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.116086483001709, loss=0.9839972853660583
I1013 16:13:42.744222 140354779014912 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.525079607963562, loss=1.0347328186035156
I1013 16:17:32.432082 140528462927680 spec.py:321] Evaluating on the training split.
I1013 16:18:26.336012 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 16:19:17.485702 140528462927680 spec.py:349] Evaluating on the test split.
I1013 16:19:43.385688 140528462927680 submission_runner.py:393] Time since start: 50523.74s, 	Step: 58301, 	{'train/ctc_loss': Array(0.15483034, dtype=float32), 'train/wer': 0.05452035886818495, 'validation/ctc_loss': Array(0.33326855, dtype=float32), 'validation/wer': 0.09792714598800892, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18528402, dtype=float32), 'test/wer': 0.060041029390855725, 'test/num_examples': 2472, 'score': 46164.11700630188, 'total_duration': 50523.74375748634, 'accumulated_submission_time': 46164.11700630188, 'accumulated_eval_time': 4355.3700251579285, 'accumulated_logging_time': 1.7688863277435303}
I1013 16:19:43.436602 140355209094912 logging_writer.py:48] [58301] accumulated_eval_time=4355.370025, accumulated_logging_time=1.768886, accumulated_submission_time=46164.117006, global_step=58301, preemption_count=0, score=46164.117006, test/ctc_loss=0.18528401851654053, test/num_examples=2472, test/wer=0.060041, total_duration=50523.743757, train/ctc_loss=0.15483033657073975, train/wer=0.054520, validation/ctc_loss=0.33326855301856995, validation/num_examples=5348, validation/wer=0.097927
I1013 16:22:15.774142 140355200702208 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.6722290515899658, loss=0.9951809644699097
I1013 16:28:40.327384 140354553734912 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.4160265922546387, loss=1.0242424011230469
I1013 16:35:17.647611 140354545342208 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.4204423427581787, loss=1.0155563354492188
I1013 16:41:56.957141 140528462927680 spec.py:321] Evaluating on the training split.
I1013 16:42:49.272952 140528462927680 spec.py:333] Evaluating on the validation split.
I1013 16:43:40.232210 140528462927680 spec.py:349] Evaluating on the test split.
I1013 16:44:06.295665 140528462927680 submission_runner.py:393] Time since start: 51986.65s, 	Step: 60000, 	{'train/ctc_loss': Array(0.16338377, dtype=float32), 'train/wer': 0.059355791353318914, 'validation/ctc_loss': Array(0.33251595, dtype=float32), 'validation/wer': 0.09758923313090744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18519112, dtype=float32), 'test/wer': 0.06018321044827656, 'test/num_examples': 2472, 'score': 47497.551612615585, 'total_duration': 51986.65382814407, 'accumulated_submission_time': 47497.551612615585, 'accumulated_eval_time': 4484.702233552933, 'accumulated_logging_time': 1.8369083404541016}
I1013 16:44:06.337482 140354553734912 logging_writer.py:48] [60000] accumulated_eval_time=4484.702234, accumulated_logging_time=1.836908, accumulated_submission_time=47497.551613, global_step=60000, preemption_count=0, score=47497.551613, test/ctc_loss=0.18519112467765808, test/num_examples=2472, test/wer=0.060183, total_duration=51986.653828, train/ctc_loss=0.16338376700878143, train/wer=0.059356, validation/ctc_loss=0.3325159549713135, validation/num_examples=5348, validation/wer=0.097589
I1013 16:44:06.365500 140354545342208 logging_writer.py:48] [60000] global_step=60000, preemption_count=0, score=47497.551613
I1013 16:44:06.829713 140528462927680 checkpoints.py:490] Saving checkpoint at step: 60000
I1013 16:44:08.356420 140528462927680 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_conformer/adamw_run8/librispeech_conformer_jax/trial_1/checkpoint_60000
I1013 16:44:08.390727 140528462927680 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_conformer/adamw_run8/librispeech_conformer_jax/trial_1/checkpoint_60000.
I1013 16:44:09.618060 140528462927680 submission_runner.py:563] Tuning trial 1/1
I1013 16:44:09.618319 140528462927680 submission_runner.py:564] Hyperparameters: Hyperparameters(learning_rate=0.002106913873888147, beta1=0.8231189937738506, beta2=0.8774571227688758, warmup_steps=1199, weight_decay=0.27590534177690645)
I1013 16:44:09.637463 140528462927680 submission_runner.py:565] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.823114, dtype=float32), 'train/wer': 1.7632853007933698, 'validation/ctc_loss': Array(29.720152, dtype=float32), 'validation/wer': 1.3843807022794636, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.881086, dtype=float32), 'test/wer': 1.471289582190807, 'test/num_examples': 2472, 'score': 76.13695120811462, 'total_duration': 267.806923866272, 'accumulated_submission_time': 76.13695120811462, 'accumulated_eval_time': 191.66992235183716, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1838, {'train/ctc_loss': Array(2.761366, dtype=float32), 'train/wer': 0.5540668575312558, 'validation/ctc_loss': Array(3.1618004, dtype=float32), 'validation/wer': 0.5959141508249901, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.8121145, dtype=float32), 'test/wer': 0.5452034204700099, 'test/num_examples': 2472, 'score': 1516.3296041488647, 'total_duration': 1831.561188697815, 'accumulated_submission_time': 1516.3296041488647, 'accumulated_eval_time': 315.11881017684937, 'accumulated_logging_time': 0.045053958892822266, 'global_step': 1838, 'preemption_count': 0}), (3712, {'train/ctc_loss': Array(0.8107718, dtype=float32), 'train/wer': 0.26824707668540076, 'validation/ctc_loss': Array(1.1465917, dtype=float32), 'validation/wer': 0.3259990152253879, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8400212, dtype=float32), 'test/wer': 0.26721914163264476, 'test/num_examples': 2472, 'score': 2956.850061416626, 'total_duration': 3399.418752193451, 'accumulated_submission_time': 2956.850061416626, 'accumulated_eval_time': 442.330219745636, 'accumulated_logging_time': 0.09564995765686035, 'global_step': 3712, 'preemption_count': 0}), (5575, {'train/ctc_loss': Array(0.6195338, dtype=float32), 'train/wer': 0.21718824949528492, 'validation/ctc_loss': Array(0.98032075, dtype=float32), 'validation/wer': 0.2860480608629329, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.68563527, dtype=float32), 'test/wer': 0.22399610017671073, 'test/num_examples': 2472, 'score': 4397.20131111145, 'total_duration': 4968.601372718811, 'accumulated_submission_time': 4397.20131111145, 'accumulated_eval_time': 571.0387015342712, 'accumulated_logging_time': 0.14401650428771973, 'global_step': 5575, 'preemption_count': 0}), (7412, {'train/ctc_loss': Array(0.5800135, dtype=float32), 'train/wer': 0.2003319754107803, 'validation/ctc_loss': Array(0.8981867, dtype=float32), 'validation/wer': 0.26745319906929144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6163951, dtype=float32), 'test/wer': 0.20396888266000446, 'test/num_examples': 2472, 'score': 5837.44485616684, 'total_duration': 6538.850746393204, 'accumulated_submission_time': 5837.44485616684, 'accumulated_eval_time': 700.9116535186768, 'accumulated_logging_time': 0.20094585418701172, 'global_step': 7412, 'preemption_count': 0}), (9207, {'train/ctc_loss': Array(0.5400141, dtype=float32), 'train/wer': 0.18638152601709104, 'validation/ctc_loss': Array(0.8547003, dtype=float32), 'validation/wer': 0.25209264605076415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5706699, dtype=float32), 'test/wer': 0.19062417484207747, 'test/num_examples': 2472, 'score': 7277.80882358551, 'total_duration': 8107.925623416901, 'accumulated_submission_time': 7277.80882358551, 'accumulated_eval_time': 829.4895484447479, 'accumulated_logging_time': 0.26154112815856934, 'global_step': 9207, 'preemption_count': 0}), (11015, {'train/ctc_loss': Array(0.5553886, dtype=float32), 'train/wer': 0.191095272897984, 'validation/ctc_loss': Array(0.8514675, dtype=float32), 'validation/wer': 0.2508085771937785, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5711759, dtype=float32), 'test/wer': 0.1898320232364471, 'test/num_examples': 2472, 'score': 8718.165016412735, 'total_duration': 9677.299626350403, 'accumulated_submission_time': 8718.165016412735, 'accumulated_eval_time': 958.379335641861, 'accumulated_logging_time': 0.3135843276977539, 'global_step': 11015, 'preemption_count': 0}), (12857, {'train/ctc_loss': Array(0.49079964, dtype=float32), 'train/wer': 0.16994767404760386, 'validation/ctc_loss': Array(0.786208, dtype=float32), 'validation/wer': 0.2355928439711519, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5218694, dtype=float32), 'test/wer': 0.17476083114983854, 'test/num_examples': 2472, 'score': 10158.31107378006, 'total_duration': 11247.22007727623, 'accumulated_submission_time': 10158.31107378006, 'accumulated_eval_time': 1088.0160675048828, 'accumulated_logging_time': 0.373049259185791, 'global_step': 12857, 'preemption_count': 0}), (14690, {'train/ctc_loss': Array(0.4396304, dtype=float32), 'train/wer': 0.15606750045503004, 'validation/ctc_loss': Array(0.76096594, dtype=float32), 'validation/wer': 0.22763740984967706, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50188094, dtype=float32), 'test/wer': 0.16592529401011516, 'test/num_examples': 2472, 'score': 11598.714666128159, 'total_duration': 12817.244627475739, 'accumulated_submission_time': 11598.714666128159, 'accumulated_eval_time': 1217.5038132667542, 'accumulated_logging_time': 0.4322206974029541, 'global_step': 14690, 'preemption_count': 0}), (16497, {'train/ctc_loss': Array(0.42868078, dtype=float32), 'train/wer': 0.15134942463916923, 'validation/ctc_loss': Array(0.7672707, dtype=float32), 'validation/wer': 0.22576440715602886, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50134116, dtype=float32), 'test/wer': 0.16748928564174437, 'test/num_examples': 2472, 'score': 13038.728790521622, 'total_duration': 14387.134721279144, 'accumulated_submission_time': 13038.728790521622, 'accumulated_eval_time': 1347.2519481182098, 'accumulated_logging_time': 0.48560333251953125, 'global_step': 16497, 'preemption_count': 0}), (18329, {'train/ctc_loss': Array(0.4167879, dtype=float32), 'train/wer': 0.14965650758579266, 'validation/ctc_loss': Array(0.72004986, dtype=float32), 'validation/wer': 0.21597458895314597, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47295943, dtype=float32), 'test/wer': 0.15739443056486502, 'test/num_examples': 2472, 'score': 14479.21630859375, 'total_duration': 15957.682151556015, 'accumulated_submission_time': 14479.21630859375, 'accumulated_eval_time': 1477.175256729126, 'accumulated_logging_time': 0.5452520847320557, 'global_step': 18329, 'preemption_count': 0}), (20165, {'train/ctc_loss': Array(0.4170413, dtype=float32), 'train/wer': 0.14343055078135328, 'validation/ctc_loss': Array(0.7015538, dtype=float32), 'validation/wer': 0.20741091168888845, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45137963, dtype=float32), 'test/wer': 0.14815266183251072, 'test/num_examples': 2472, 'score': 15919.290590047836, 'total_duration': 17529.786338329315, 'accumulated_submission_time': 15919.290590047836, 'accumulated_eval_time': 1609.0753996372223, 'accumulated_logging_time': 0.5965089797973633, 'global_step': 20165, 'preemption_count': 0}), (22006, {'train/ctc_loss': Array(0.40213054, dtype=float32), 'train/wer': 0.13534703352942729, 'validation/ctc_loss': Array(0.6727824, dtype=float32), 'validation/wer': 0.19780453189414637, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42915112, dtype=float32), 'test/wer': 0.14114516685962666, 'test/num_examples': 2472, 'score': 17359.304765701294, 'total_duration': 19100.8536298275, 'accumulated_submission_time': 17359.304765701294, 'accumulated_eval_time': 1739.999810218811, 'accumulated_logging_time': 0.6459355354309082, 'global_step': 22006, 'preemption_count': 0}), (23818, {'train/ctc_loss': Array(0.38689435, dtype=float32), 'train/wer': 0.13650581335528347, 'validation/ctc_loss': Array(0.66221136, dtype=float32), 'validation/wer': 0.19869275997567026, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42067477, dtype=float32), 'test/wer': 0.14224199215973027, 'test/num_examples': 2472, 'score': 18799.42692923546, 'total_duration': 20672.712401151657, 'accumulated_submission_time': 18799.42692923546, 'accumulated_eval_time': 1871.6090109348297, 'accumulated_logging_time': 0.6956217288970947, 'global_step': 23818, 'preemption_count': 0}), (25628, {'train/ctc_loss': Array(0.36858398, dtype=float32), 'train/wer': 0.1305265824922934, 'validation/ctc_loss': Array(0.661228, dtype=float32), 'validation/wer': 0.1961632408739392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4166255, dtype=float32), 'test/wer': 0.13917494363536653, 'test/num_examples': 2472, 'score': 20239.503421783447, 'total_duration': 22243.326597213745, 'accumulated_submission_time': 20239.503421783447, 'accumulated_eval_time': 2002.0192692279816, 'accumulated_logging_time': 0.7444384098052979, 'global_step': 25628, 'preemption_count': 0}), (27440, {'train/ctc_loss': Array(0.3263501, dtype=float32), 'train/wer': 0.1158767834320144, 'validation/ctc_loss': Array(0.615209, dtype=float32), 'validation/wer': 0.18446180136516793, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38853222, dtype=float32), 'test/wer': 0.12867385696585623, 'test/num_examples': 2472, 'score': 21680.08334159851, 'total_duration': 23815.351699590683, 'accumulated_submission_time': 21680.08334159851, 'accumulated_eval_time': 2133.3339269161224, 'accumulated_logging_time': 0.7971978187561035, 'global_step': 27440, 'preemption_count': 0}), (29268, {'train/ctc_loss': Array(0.3176866, dtype=float32), 'train/wer': 0.11241574700064436, 'validation/ctc_loss': Array(0.59929836, dtype=float32), 'validation/wer': 0.1794317271208859, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37526193, dtype=float32), 'test/wer': 0.12593179371559726, 'test/num_examples': 2472, 'score': 23120.60368990898, 'total_duration': 25387.625465154648, 'accumulated_submission_time': 23120.60368990898, 'accumulated_eval_time': 2264.9492416381836, 'accumulated_logging_time': 0.8556094169616699, 'global_step': 29268, 'preemption_count': 0}), (31077, {'train/ctc_loss': Array(0.32796368, dtype=float32), 'train/wer': 0.11549878661813139, 'validation/ctc_loss': Array(0.5954125, dtype=float32), 'validation/wer': 0.1763905114069726, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3714614, dtype=float32), 'test/wer': 0.12272256413381269, 'test/num_examples': 2472, 'score': 24560.674696683884, 'total_duration': 26958.088918447495, 'accumulated_submission_time': 24560.674696683884, 'accumulated_eval_time': 2395.2090513706207, 'accumulated_logging_time': 0.9091525077819824, 'global_step': 31077, 'preemption_count': 0}), (32900, {'train/ctc_loss': Array(0.29728517, dtype=float32), 'train/wer': 0.10219263262415966, 'validation/ctc_loss': Array(0.56708217, dtype=float32), 'validation/wer': 0.16756615851009393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35162997, dtype=float32), 'test/wer': 0.11872118294639775, 'test/num_examples': 2472, 'score': 26000.73390674591, 'total_duration': 28529.370970726013, 'accumulated_submission_time': 26000.73390674591, 'accumulated_eval_time': 2526.301216840744, 'accumulated_logging_time': 0.9616312980651855, 'global_step': 32900, 'preemption_count': 0}), (34724, {'train/ctc_loss': Array(0.2909537, dtype=float32), 'train/wer': 0.10144912646594989, 'validation/ctc_loss': Array(0.5382805, dtype=float32), 'validation/wer': 0.16125201540882628, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3259944, dtype=float32), 'test/wer': 0.11094184794751488, 'test/num_examples': 2472, 'score': 27441.624930858612, 'total_duration': 30103.356734752655, 'accumulated_submission_time': 27441.624930858612, 'accumulated_eval_time': 2659.260458946228, 'accumulated_logging_time': 1.0185678005218506, 'global_step': 34724, 'preemption_count': 0}), (36557, {'train/ctc_loss': Array(0.27816463, dtype=float32), 'train/wer': 0.09440838901499408, 'validation/ctc_loss': Array(0.52119833, dtype=float32), 'validation/wer': 0.154570995491277, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3078582, dtype=float32), 'test/wer': 0.10167976763552901, 'test/num_examples': 2472, 'score': 28881.882888317108, 'total_duration': 31675.08744454384, 'accumulated_submission_time': 28881.882888317108, 'accumulated_eval_time': 2790.595918416977, 'accumulated_logging_time': 1.0748741626739502, 'global_step': 36557, 'preemption_count': 0}), (38372, {'train/ctc_loss': Array(0.21717745, dtype=float32), 'train/wer': 0.07710402509147936, 'validation/ctc_loss': Array(0.5061055, dtype=float32), 'validation/wer': 0.14884578622667194, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30427524, dtype=float32), 'test/wer': 0.10080636971137245, 'test/num_examples': 2472, 'score': 30322.385301828384, 'total_duration': 33248.02740955353, 'accumulated_submission_time': 30322.385301828384, 'accumulated_eval_time': 2922.894958257675, 'accumulated_logging_time': 1.1327526569366455, 'global_step': 38372, 'preemption_count': 0}), (40164, {'train/ctc_loss': Array(0.23430294, dtype=float32), 'train/wer': 0.08162446410846656, 'validation/ctc_loss': Array(0.4788264, dtype=float32), 'validation/wer': 0.14221303957442288, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2849079, dtype=float32), 'test/wer': 0.09323015050880507, 'test/num_examples': 2472, 'score': 31762.552495718002, 'total_duration': 34819.008442640305, 'accumulated_submission_time': 31762.552495718002, 'accumulated_eval_time': 3053.5671892166138, 'accumulated_logging_time': 1.1970024108886719, 'global_step': 40164, 'preemption_count': 0}), (41948, {'train/ctc_loss': Array(0.27890697, dtype=float32), 'train/wer': 0.09769313799406605, 'validation/ctc_loss': Array(0.45374945, dtype=float32), 'validation/wer': 0.13564787549359414, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26860154, dtype=float32), 'test/wer': 0.08896471878618, 'test/num_examples': 2472, 'score': 33203.18786692619, 'total_duration': 36390.5732922554, 'accumulated_submission_time': 33203.18786692619, 'accumulated_eval_time': 3184.3609116077423, 'accumulated_logging_time': 1.2530229091644287, 'global_step': 41948, 'preemption_count': 0}), (43773, {'train/ctc_loss': Array(0.273045, dtype=float32), 'train/wer': 0.09574934633084818, 'validation/ctc_loss': Array(0.44358933, dtype=float32), 'validation/wer': 0.13138051884105545, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25235382, dtype=float32), 'test/wer': 0.08473991022281803, 'test/num_examples': 2472, 'score': 34643.059309482574, 'total_duration': 37961.0304582119, 'accumulated_submission_time': 34643.059309482574, 'accumulated_eval_time': 3314.808256626129, 'accumulated_logging_time': 1.3121247291564941, 'global_step': 43773, 'preemption_count': 0}), (45589, {'train/ctc_loss': Array(0.2973748, dtype=float32), 'train/wer': 0.10500084819626676, 'validation/ctc_loss': Array(0.4245359, dtype=float32), 'validation/wer': 0.12656284696409434, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24460913, dtype=float32), 'test/wer': 0.08236345540592692, 'test/num_examples': 2472, 'score': 36083.39624404907, 'total_duration': 39529.795149326324, 'accumulated_submission_time': 36083.39624404907, 'accumulated_eval_time': 3443.097991466522, 'accumulated_logging_time': 1.371171474456787, 'global_step': 45589, 'preemption_count': 0}), (47386, {'train/ctc_loss': Array(0.25112334, dtype=float32), 'train/wer': 0.086400273183828, 'validation/ctc_loss': Array(0.40379113, dtype=float32), 'validation/wer': 0.1185398302711992, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23312776, dtype=float32), 'test/wer': 0.07588406150346312, 'test/num_examples': 2472, 'score': 37523.8457300663, 'total_duration': 41099.10501933098, 'accumulated_submission_time': 37523.8457300663, 'accumulated_eval_time': 3571.823614358902, 'accumulated_logging_time': 1.4255969524383545, 'global_step': 47386, 'preemption_count': 0}), (49198, {'train/ctc_loss': Array(0.21648009, dtype=float32), 'train/wer': 0.07842405887789072, 'validation/ctc_loss': Array(0.38508216, dtype=float32), 'validation/wer': 0.11409868986357975, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21575205, dtype=float32), 'test/wer': 0.07147644872341723, 'test/num_examples': 2472, 'score': 38963.83891916275, 'total_duration': 42668.95094180107, 'accumulated_submission_time': 38963.83891916275, 'accumulated_eval_time': 3701.5283229351044, 'accumulated_logging_time': 1.4950025081634521, 'global_step': 49198, 'preemption_count': 0}), (51026, {'train/ctc_loss': Array(0.17452212, dtype=float32), 'train/wer': 0.06286637157554731, 'validation/ctc_loss': Array(0.3732639, dtype=float32), 'validation/wer': 0.10960927619066009, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2089306, dtype=float32), 'test/wer': 0.06775942965084394, 'test/num_examples': 2472, 'score': 40404.082528829575, 'total_duration': 44239.79353737831, 'accumulated_submission_time': 40404.082528829575, 'accumulated_eval_time': 3831.99388051033, 'accumulated_logging_time': 1.5474884510040283, 'global_step': 51026, 'preemption_count': 0}), (52845, {'train/ctc_loss': Array(0.18791229, dtype=float32), 'train/wer': 0.0680508562025124, 'validation/ctc_loss': Array(0.35731253, dtype=float32), 'validation/wer': 0.10542881141566178, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20236841, dtype=float32), 'test/wer': 0.06593138748400464, 'test/num_examples': 2472, 'score': 41843.98575305939, 'total_duration': 45810.21587514877, 'accumulated_submission_time': 41843.98575305939, 'accumulated_eval_time': 3962.37974858284, 'accumulated_logging_time': 1.6000745296478271, 'global_step': 52845, 'preemption_count': 0}), (54646, {'train/ctc_loss': Array(0.16191684, dtype=float32), 'train/wer': 0.05788846418141213, 'validation/ctc_loss': Array(0.34708014, dtype=float32), 'validation/wer': 0.10124834664066347, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19277844, dtype=float32), 'test/wer': 0.06243779578737838, 'test/num_examples': 2472, 'score': 43284.094190597534, 'total_duration': 47381.40700984001, 'accumulated_submission_time': 43284.094190597534, 'accumulated_eval_time': 4093.3252091407776, 'accumulated_logging_time': 1.6582086086273193, 'global_step': 54646, 'preemption_count': 0}), (56474, {'train/ctc_loss': Array(0.16085176, dtype=float32), 'train/wer': 0.05753479642868766, 'validation/ctc_loss': Array(0.3371203, dtype=float32), 'validation/wer': 0.09872848219199243, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18818602, dtype=float32), 'test/wer': 0.061442528385432536, 'test/num_examples': 2472, 'score': 44724.01770877838, 'total_duration': 48952.56027507782, 'accumulated_submission_time': 44724.01770877838, 'accumulated_eval_time': 4224.423105239868, 'accumulated_logging_time': 1.7109320163726807, 'global_step': 56474, 'preemption_count': 0}), (58301, {'train/ctc_loss': Array(0.15483034, dtype=float32), 'train/wer': 0.05452035886818495, 'validation/ctc_loss': Array(0.33326855, dtype=float32), 'validation/wer': 0.09792714598800892, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18528402, dtype=float32), 'test/wer': 0.060041029390855725, 'test/num_examples': 2472, 'score': 46164.11700630188, 'total_duration': 50523.74375748634, 'accumulated_submission_time': 46164.11700630188, 'accumulated_eval_time': 4355.3700251579285, 'accumulated_logging_time': 1.7688863277435303, 'global_step': 58301, 'preemption_count': 0}), (60000, {'train/ctc_loss': Array(0.16338377, dtype=float32), 'train/wer': 0.059355791353318914, 'validation/ctc_loss': Array(0.33251595, dtype=float32), 'validation/wer': 0.09758923313090744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18519112, dtype=float32), 'test/wer': 0.06018321044827656, 'test/num_examples': 2472, 'score': 47497.551612615585, 'total_duration': 51986.65382814407, 'accumulated_submission_time': 47497.551612615585, 'accumulated_eval_time': 4484.702233552933, 'accumulated_logging_time': 1.8369083404541016, 'global_step': 60000, 'preemption_count': 0})], 'global_step': 60000}
I1013 16:44:09.637697 140528462927680 submission_runner.py:566] Timing: 47497.551612615585
I1013 16:44:09.637765 140528462927680 submission_runner.py:568] Total number of evals: 34
I1013 16:44:09.637818 140528462927680 submission_runner.py:569] ====================
I1013 16:44:09.642118 140528462927680 submission_runner.py:645] Final librispeech_conformer score: 47497.551612615585
