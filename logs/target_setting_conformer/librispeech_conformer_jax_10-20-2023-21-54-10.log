python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_conformer/adamw_run9 --overwrite=true --save_checkpoints=false --max_global_steps=60000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_10-20-2023-21-54-10.log
I1020 21:54:32.028290 139830899640128 logger_utils.py:61] Removing existing experiment directory /experiment_runs/targets_check_conformer/adamw_run9/librispeech_conformer_jax because --overwrite was set.
I1020 21:54:32.041981 139830899640128 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_conformer/adamw_run9/librispeech_conformer_jax.
I1020 21:54:33.003576 139830899640128 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I1020 21:54:33.004373 139830899640128 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1020 21:54:33.004755 139830899640128 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1020 21:54:33.011198 139830899640128 submission_runner.py:525] Using RNG seed 4169562629
I1020 21:54:38.544490 139830899640128 submission_runner.py:534] --- Tuning run 1/1 ---
I1020 21:54:38.544723 139830899640128 submission_runner.py:539] Creating tuning directory at /experiment_runs/targets_check_conformer/adamw_run9/librispeech_conformer_jax/trial_1.
I1020 21:54:38.544945 139830899640128 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_conformer/adamw_run9/librispeech_conformer_jax/trial_1/hparams.json.
I1020 21:54:38.729587 139830899640128 submission_runner.py:202] Initializing dataset.
I1020 21:54:38.729962 139830899640128 submission_runner.py:209] Initializing model.
I1020 21:54:43.615150 139830899640128 submission_runner.py:243] Initializing optimizer.
I1020 21:54:44.905807 139830899640128 submission_runner.py:250] Initializing metrics bundle.
I1020 21:54:44.906007 139830899640128 submission_runner.py:268] Initializing checkpoint and logger.
I1020 21:54:44.907247 139830899640128 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_conformer/adamw_run9/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I1020 21:54:44.907399 139830899640128 submission_runner.py:288] Saving meta data to /experiment_runs/targets_check_conformer/adamw_run9/librispeech_conformer_jax/trial_1/meta_data_0.json.
I1020 21:54:44.907657 139830899640128 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1020 21:54:44.907723 139830899640128 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I1020 21:54:45.257508 139830899640128 logger_utils.py:220] Unable to record git information. Continuing without it.
I1020 21:54:45.574628 139830899640128 submission_runner.py:291] Saving flags to /experiment_runs/targets_check_conformer/adamw_run9/librispeech_conformer_jax/trial_1/flags_0.json.
I1020 21:54:45.589951 139830899640128 submission_runner.py:301] Starting training loop.
I1020 21:54:45.886358 139830899640128 input_pipeline.py:20] Loading split = train-clean-100
I1020 21:54:45.927777 139830899640128 input_pipeline.py:20] Loading split = train-clean-360
I1020 21:54:46.335383 139830899640128 input_pipeline.py:20] Loading split = train-other-500
2023-10-20 21:55:57.357062: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-20 21:56:00.286056: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1020 21:56:02.424348 139656725260032 logging_writer.py:48] [0] global_step=0, grad_norm=56.403812408447266, loss=31.686445236206055
I1020 21:56:02.460846 139830899640128 spec.py:321] Evaluating on the training split.
I1020 21:56:02.630995 139830899640128 input_pipeline.py:20] Loading split = train-clean-100
I1020 21:56:02.665566 139830899640128 input_pipeline.py:20] Loading split = train-clean-360
I1020 21:56:03.066024 139830899640128 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1020 21:57:16.355866 139830899640128 spec.py:333] Evaluating on the validation split.
I1020 21:57:16.472948 139830899640128 input_pipeline.py:20] Loading split = dev-clean
I1020 21:57:16.478359 139830899640128 input_pipeline.py:20] Loading split = dev-other
I1020 21:58:20.732369 139830899640128 spec.py:349] Evaluating on the test split.
I1020 21:58:20.850045 139830899640128 input_pipeline.py:20] Loading split = test-clean
I1020 21:58:58.103484 139830899640128 submission_runner.py:395] Time since start: 252.51s, 	Step: 1, 	{'train/ctc_loss': Array(31.450764, dtype=float32), 'train/wer': 1.0936166003789851, 'validation/ctc_loss': Array(29.804022, dtype=float32), 'validation/wer': 1.23516803923651, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.961452, dtype=float32), 'test/wer': 1.2324457173034347, 'test/num_examples': 2472, 'score': 76.87082004547119, 'total_duration': 252.51129627227783, 'accumulated_submission_time': 76.87082004547119, 'accumulated_eval_time': 175.64041686058044, 'accumulated_logging_time': 0}
I1020 21:58:58.128360 139650492528384 logging_writer.py:48] [1] accumulated_eval_time=175.640417, accumulated_logging_time=0, accumulated_submission_time=76.870820, global_step=1, preemption_count=0, score=76.870820, test/ctc_loss=29.96145248413086, test/num_examples=2472, test/wer=1.232446, total_duration=252.511296, train/ctc_loss=31.450763702392578, train/wer=1.093617, validation/ctc_loss=29.80402183532715, validation/num_examples=5348, validation/wer=1.235168
I1020 21:59:21.082824 139658120599296 logging_writer.py:48] [1] global_step=1, grad_norm=61.0703125, loss=31.677852630615234
I1020 21:59:21.966398 139658128992000 logging_writer.py:48] [2] global_step=2, grad_norm=76.12238311767578, loss=31.755189895629883
I1020 21:59:22.848010 139658120599296 logging_writer.py:48] [3] global_step=3, grad_norm=101.34638977050781, loss=31.732799530029297
I1020 21:59:23.722694 139658128992000 logging_writer.py:48] [4] global_step=4, grad_norm=148.1741180419922, loss=29.76701545715332
I1020 21:59:24.600938 139658120599296 logging_writer.py:48] [5] global_step=5, grad_norm=175.9189453125, loss=27.35076332092285
I1020 21:59:25.488977 139658128992000 logging_writer.py:48] [6] global_step=6, grad_norm=176.74708557128906, loss=24.39698028564453
I1020 21:59:26.384889 139658120599296 logging_writer.py:48] [7] global_step=7, grad_norm=168.84112548828125, loss=20.658058166503906
I1020 21:59:27.264544 139658128992000 logging_writer.py:48] [8] global_step=8, grad_norm=153.93292236328125, loss=16.996723175048828
I1020 21:59:28.146236 139658120599296 logging_writer.py:48] [9] global_step=9, grad_norm=126.73207092285156, loss=13.226149559020996
I1020 21:59:29.029437 139658128992000 logging_writer.py:48] [10] global_step=10, grad_norm=88.43970489501953, loss=10.245780944824219
I1020 21:59:29.907142 139658120599296 logging_writer.py:48] [11] global_step=11, grad_norm=46.84336853027344, loss=8.304072380065918
I1020 21:59:30.787381 139658128992000 logging_writer.py:48] [12] global_step=12, grad_norm=15.929766654968262, loss=7.486072540283203
I1020 21:59:31.667355 139658120599296 logging_writer.py:48] [13] global_step=13, grad_norm=4.4994940757751465, loss=7.3169355392456055
I1020 21:59:32.549874 139658128992000 logging_writer.py:48] [14] global_step=14, grad_norm=12.292662620544434, loss=7.44499397277832
I1020 21:59:33.425605 139658120599296 logging_writer.py:48] [15] global_step=15, grad_norm=16.683740615844727, loss=7.634188175201416
I1020 21:59:34.312716 139658128992000 logging_writer.py:48] [16] global_step=16, grad_norm=18.789382934570312, loss=7.77117919921875
I1020 21:59:35.194161 139658120599296 logging_writer.py:48] [17] global_step=17, grad_norm=19.881956100463867, loss=7.828553199768066
I1020 21:59:36.077664 139658128992000 logging_writer.py:48] [18] global_step=18, grad_norm=19.768449783325195, loss=7.768596649169922
I1020 21:59:36.961669 139658120599296 logging_writer.py:48] [19] global_step=19, grad_norm=19.131298065185547, loss=7.64921236038208
I1020 21:59:37.844575 139658128992000 logging_writer.py:48] [20] global_step=20, grad_norm=16.942962646484375, loss=7.451330184936523
I1020 21:59:38.726814 139658120599296 logging_writer.py:48] [21] global_step=21, grad_norm=11.686712265014648, loss=7.2140374183654785
I1020 21:59:39.611599 139658128992000 logging_writer.py:48] [22] global_step=22, grad_norm=3.5633366107940674, loss=7.064136981964111
I1020 21:59:40.489310 139658120599296 logging_writer.py:48] [23] global_step=23, grad_norm=11.024184226989746, loss=7.086530685424805
I1020 21:59:41.371839 139658128992000 logging_writer.py:48] [24] global_step=24, grad_norm=21.901798248291016, loss=7.22829532623291
I1020 21:59:42.250382 139658120599296 logging_writer.py:48] [25] global_step=25, grad_norm=19.945178985595703, loss=7.172642230987549
I1020 21:59:43.128076 139658128992000 logging_writer.py:48] [26] global_step=26, grad_norm=12.566086769104004, loss=7.026022911071777
I1020 21:59:44.019069 139658120599296 logging_writer.py:48] [27] global_step=27, grad_norm=2.5891504287719727, loss=6.903536319732666
I1020 21:59:44.917258 139658128992000 logging_writer.py:48] [28] global_step=28, grad_norm=6.188517093658447, loss=6.907861709594727
I1020 21:59:45.797072 139658120599296 logging_writer.py:48] [29] global_step=29, grad_norm=9.044537544250488, loss=6.924062728881836
I1020 21:59:46.675726 139658128992000 logging_writer.py:48] [30] global_step=30, grad_norm=9.45952320098877, loss=6.893932342529297
I1020 21:59:47.554607 139658120599296 logging_writer.py:48] [31] global_step=31, grad_norm=7.448592662811279, loss=6.797194957733154
I1020 21:59:48.437297 139658128992000 logging_writer.py:48] [32] global_step=32, grad_norm=2.6802337169647217, loss=6.720095634460449
I1020 21:59:49.321196 139658120599296 logging_writer.py:48] [33] global_step=33, grad_norm=4.97496223449707, loss=6.68979549407959
I1020 21:59:50.218543 139658128992000 logging_writer.py:48] [34] global_step=34, grad_norm=9.431349754333496, loss=6.693227291107178
I1020 21:59:51.111552 139658120599296 logging_writer.py:48] [35] global_step=35, grad_norm=8.790763854980469, loss=6.6518025398254395
I1020 21:59:52.014576 139658128992000 logging_writer.py:48] [36] global_step=36, grad_norm=2.997161388397217, loss=6.567673206329346
I1020 21:59:52.914748 139658120599296 logging_writer.py:48] [37] global_step=37, grad_norm=2.9314799308776855, loss=6.553051948547363
I1020 21:59:53.796989 139658128992000 logging_writer.py:48] [38] global_step=38, grad_norm=5.430605888366699, loss=6.508947849273682
I1020 21:59:54.678550 139658120599296 logging_writer.py:48] [39] global_step=39, grad_norm=4.4990668296813965, loss=6.476768970489502
I1020 21:59:55.563647 139658128992000 logging_writer.py:48] [40] global_step=40, grad_norm=2.026278257369995, loss=6.439136981964111
I1020 21:59:56.446020 139658120599296 logging_writer.py:48] [41] global_step=41, grad_norm=5.2862091064453125, loss=6.4108476638793945
I1020 21:59:57.338652 139658128992000 logging_writer.py:48] [42] global_step=42, grad_norm=5.142151832580566, loss=6.372208595275879
I1020 21:59:58.232539 139658120599296 logging_writer.py:48] [43] global_step=43, grad_norm=1.4226717948913574, loss=6.332021713256836
I1020 21:59:59.130449 139658128992000 logging_writer.py:48] [44] global_step=44, grad_norm=4.149363994598389, loss=6.288619518280029
I1020 22:00:00.017749 139658120599296 logging_writer.py:48] [45] global_step=45, grad_norm=2.0269317626953125, loss=6.267897129058838
I1020 22:00:00.905193 139658128992000 logging_writer.py:48] [46] global_step=46, grad_norm=3.385493516921997, loss=6.227771759033203
I1020 22:00:01.792026 139658120599296 logging_writer.py:48] [47] global_step=47, grad_norm=4.249624729156494, loss=6.2124762535095215
I1020 22:00:02.769646 139658128992000 logging_writer.py:48] [48] global_step=48, grad_norm=1.9073251485824585, loss=6.174783706665039
I1020 22:00:03.659289 139658120599296 logging_writer.py:48] [49] global_step=49, grad_norm=2.5781290531158447, loss=6.157304763793945
I1020 22:00:04.552334 139658128992000 logging_writer.py:48] [50] global_step=50, grad_norm=1.1712398529052734, loss=6.1186299324035645
I1020 22:00:05.433357 139658120599296 logging_writer.py:48] [51] global_step=51, grad_norm=3.7039265632629395, loss=6.110504627227783
I1020 22:00:06.324220 139658128992000 logging_writer.py:48] [52] global_step=52, grad_norm=1.7436431646347046, loss=6.088746547698975
I1020 22:00:07.205836 139658120599296 logging_writer.py:48] [53] global_step=53, grad_norm=1.4160789251327515, loss=6.054951190948486
I1020 22:00:08.095696 139658128992000 logging_writer.py:48] [54] global_step=54, grad_norm=1.6843650341033936, loss=6.050074100494385
I1020 22:00:08.979546 139658120599296 logging_writer.py:48] [55] global_step=55, grad_norm=1.6234142780303955, loss=6.018388748168945
I1020 22:00:09.867801 139658128992000 logging_writer.py:48] [56] global_step=56, grad_norm=2.2785098552703857, loss=6.001371383666992
I1020 22:00:10.754045 139658120599296 logging_writer.py:48] [57] global_step=57, grad_norm=1.3598895072937012, loss=5.981725692749023
I1020 22:00:11.635977 139658128992000 logging_writer.py:48] [58] global_step=58, grad_norm=1.0633938312530518, loss=5.958245277404785
I1020 22:00:12.515771 139658120599296 logging_writer.py:48] [59] global_step=59, grad_norm=1.816078782081604, loss=5.946683406829834
I1020 22:00:13.401313 139658128992000 logging_writer.py:48] [60] global_step=60, grad_norm=2.4593873023986816, loss=5.934665203094482
I1020 22:00:14.282398 139658120599296 logging_writer.py:48] [61] global_step=61, grad_norm=3.0443403720855713, loss=5.904455661773682
I1020 22:00:15.167454 139658128992000 logging_writer.py:48] [62] global_step=62, grad_norm=6.053829193115234, loss=5.942098140716553
I1020 22:00:16.048583 139658120599296 logging_writer.py:48] [63] global_step=63, grad_norm=8.294750213623047, loss=5.961886882781982
I1020 22:00:16.931488 139658128992000 logging_writer.py:48] [64] global_step=64, grad_norm=3.319676637649536, loss=5.892983913421631
I1020 22:00:17.830143 139658120599296 logging_writer.py:48] [65] global_step=65, grad_norm=4.7676520347595215, loss=5.891961574554443
I1020 22:00:18.722153 139658128992000 logging_writer.py:48] [66] global_step=66, grad_norm=6.198122024536133, loss=5.910904884338379
I1020 22:00:19.621407 139658120599296 logging_writer.py:48] [67] global_step=67, grad_norm=0.38539183139801025, loss=5.869155406951904
I1020 22:00:20.504102 139658128992000 logging_writer.py:48] [68] global_step=68, grad_norm=7.454155921936035, loss=5.894880771636963
I1020 22:00:21.381057 139658120599296 logging_writer.py:48] [69] global_step=69, grad_norm=6.936915874481201, loss=5.913623809814453
I1020 22:00:22.264442 139658128992000 logging_writer.py:48] [70] global_step=70, grad_norm=2.0323824882507324, loss=5.833079814910889
I1020 22:00:23.144927 139658120599296 logging_writer.py:48] [71] global_step=71, grad_norm=9.40247631072998, loss=5.906391620635986
I1020 22:00:24.034935 139658128992000 logging_writer.py:48] [72] global_step=72, grad_norm=4.797308921813965, loss=5.8652825355529785
I1020 22:00:24.923799 139658120599296 logging_writer.py:48] [73] global_step=73, grad_norm=5.816492557525635, loss=5.870995044708252
I1020 22:00:25.809711 139658128992000 logging_writer.py:48] [74] global_step=74, grad_norm=6.620607376098633, loss=5.871015548706055
I1020 22:00:26.698847 139658120599296 logging_writer.py:48] [75] global_step=75, grad_norm=2.0351266860961914, loss=5.850539684295654
I1020 22:00:27.578852 139658128992000 logging_writer.py:48] [76] global_step=76, grad_norm=5.107519149780273, loss=5.852298736572266
I1020 22:00:28.456532 139658120599296 logging_writer.py:48] [77] global_step=77, grad_norm=0.9200481176376343, loss=5.8314056396484375
I1020 22:00:29.340377 139658128992000 logging_writer.py:48] [78] global_step=78, grad_norm=4.982913494110107, loss=5.8391337394714355
I1020 22:00:30.222908 139658120599296 logging_writer.py:48] [79] global_step=79, grad_norm=1.0875920057296753, loss=5.829412937164307
I1020 22:00:31.108297 139658128992000 logging_writer.py:48] [80] global_step=80, grad_norm=3.2706706523895264, loss=5.848376750946045
I1020 22:00:31.990536 139658120599296 logging_writer.py:48] [81] global_step=81, grad_norm=1.8359646797180176, loss=5.846986770629883
I1020 22:00:32.874882 139658128992000 logging_writer.py:48] [82] global_step=82, grad_norm=2.2461109161376953, loss=5.8300933837890625
I1020 22:00:33.765689 139658120599296 logging_writer.py:48] [83] global_step=83, grad_norm=3.5192384719848633, loss=5.850278377532959
I1020 22:00:34.649157 139658128992000 logging_writer.py:48] [84] global_step=84, grad_norm=0.4558769464492798, loss=5.7957682609558105
I1020 22:00:35.530791 139658120599296 logging_writer.py:48] [85] global_step=85, grad_norm=4.460272789001465, loss=5.838881492614746
I1020 22:00:36.420210 139658128992000 logging_writer.py:48] [86] global_step=86, grad_norm=5.568408012390137, loss=5.87177038192749
I1020 22:00:37.314911 139658120599296 logging_writer.py:48] [87] global_step=87, grad_norm=0.8684503436088562, loss=5.828580856323242
I1020 22:00:38.203359 139658128992000 logging_writer.py:48] [88] global_step=88, grad_norm=4.650230884552002, loss=5.83029317855835
I1020 22:00:39.095675 139658120599296 logging_writer.py:48] [89] global_step=89, grad_norm=5.047555446624756, loss=5.847435474395752
I1020 22:00:39.985368 139658128992000 logging_writer.py:48] [90] global_step=90, grad_norm=0.29006239771842957, loss=5.830074787139893
I1020 22:00:40.870446 139658120599296 logging_writer.py:48] [91] global_step=91, grad_norm=5.213242053985596, loss=5.860695838928223
I1020 22:00:41.755371 139658128992000 logging_writer.py:48] [92] global_step=92, grad_norm=5.473117828369141, loss=5.856407165527344
I1020 22:00:42.646839 139658120599296 logging_writer.py:48] [93] global_step=93, grad_norm=0.6901134848594666, loss=5.827743053436279
I1020 22:00:43.532414 139658128992000 logging_writer.py:48] [94] global_step=94, grad_norm=3.545788049697876, loss=5.816986083984375
I1020 22:00:44.423705 139658120599296 logging_writer.py:48] [95] global_step=95, grad_norm=3.1468470096588135, loss=5.813462734222412
I1020 22:00:45.314477 139658128992000 logging_writer.py:48] [96] global_step=96, grad_norm=0.5159809589385986, loss=5.816898345947266
I1020 22:00:46.207388 139658120599296 logging_writer.py:48] [97] global_step=97, grad_norm=3.477343797683716, loss=5.835052967071533
I1020 22:00:47.092892 139658128992000 logging_writer.py:48] [98] global_step=98, grad_norm=3.015413761138916, loss=5.820653915405273
I1020 22:00:47.973776 139658120599296 logging_writer.py:48] [99] global_step=99, grad_norm=0.6234981417655945, loss=5.808557510375977
I1020 22:00:48.858844 139658128992000 logging_writer.py:48] [100] global_step=100, grad_norm=3.806366205215454, loss=5.84045934677124
I1020 22:05:54.617838 139658120599296 logging_writer.py:48] [500] global_step=500, grad_norm=2.846771240234375, loss=5.495685577392578
I1020 22:12:17.098129 139658128992000 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1969949007034302, loss=3.0887844562530518
I1020 22:18:42.253844 139658196133632 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.4898792505264282, loss=2.406297445297241
I1020 22:22:58.144822 139830899640128 spec.py:321] Evaluating on the training split.
I1020 22:23:50.259673 139830899640128 spec.py:333] Evaluating on the validation split.
I1020 22:24:39.146869 139830899640128 spec.py:349] Evaluating on the test split.
I1020 22:25:04.228806 139830899640128 submission_runner.py:395] Time since start: 1818.63s, 	Step: 1836, 	{'train/ctc_loss': Array(2.1721225, dtype=float32), 'train/wer': 0.5044231838685094, 'validation/ctc_loss': Array(2.673906, dtype=float32), 'validation/wer': 0.5500738580959094, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.2805436, dtype=float32), 'test/wer': 0.5014319663640241, 'test/num_examples': 2472, 'score': 1516.8055872917175, 'total_duration': 1818.633487701416, 'accumulated_submission_time': 1516.8055872917175, 'accumulated_eval_time': 301.71912598609924, 'accumulated_logging_time': 0.03926873207092285}
I1020 22:25:04.260858 139658196133632 logging_writer.py:48] [1836] accumulated_eval_time=301.719126, accumulated_logging_time=0.039269, accumulated_submission_time=1516.805587, global_step=1836, preemption_count=0, score=1516.805587, test/ctc_loss=2.280543565750122, test/num_examples=2472, test/wer=0.501432, total_duration=1818.633488, train/ctc_loss=2.1721224784851074, train/wer=0.504423, validation/ctc_loss=2.673906087875366, validation/num_examples=5348, validation/wer=0.550074
I1020 22:27:10.134874 139658187740928 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8762319087982178, loss=2.2124407291412354
I1020 22:33:35.246388 139658196133632 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7833642363548279, loss=2.0459909439086914
I1020 22:39:57.345491 139658187740928 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5947437882423401, loss=2.0118942260742188
I1020 22:46:22.029746 139658196133632 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9442961812019348, loss=1.8941612243652344
I1020 22:49:04.918833 139830899640128 spec.py:321] Evaluating on the training split.
I1020 22:49:56.907964 139830899640128 spec.py:333] Evaluating on the validation split.
I1020 22:50:47.025914 139830899640128 spec.py:349] Evaluating on the test split.
I1020 22:51:12.282700 139830899640128 submission_runner.py:395] Time since start: 3386.69s, 	Step: 3715, 	{'train/ctc_loss': Array(0.6602704, dtype=float32), 'train/wer': 0.22555469728045174, 'validation/ctc_loss': Array(0.98302835, dtype=float32), 'validation/wer': 0.28588393176091215, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.69698733, dtype=float32), 'test/wer': 0.22679909816586435, 'test/num_examples': 2472, 'score': 2957.3760089874268, 'total_duration': 3386.6866416931152, 'accumulated_submission_time': 2957.3760089874268, 'accumulated_eval_time': 429.0769395828247, 'accumulated_logging_time': 0.08616781234741211}
I1020 22:51:12.320314 139658196133632 logging_writer.py:48] [3715] accumulated_eval_time=429.076940, accumulated_logging_time=0.086168, accumulated_submission_time=2957.376009, global_step=3715, preemption_count=0, score=2957.376009, test/ctc_loss=0.6969873309135437, test/num_examples=2472, test/wer=0.226799, total_duration=3386.686642, train/ctc_loss=0.6602703928947449, train/wer=0.225555, validation/ctc_loss=0.9830283522605896, validation/num_examples=5348, validation/wer=0.285884
I1020 22:54:50.083596 139658187740928 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7996541857719421, loss=1.9685572385787964
I1020 23:01:14.692304 139658196133632 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6386853456497192, loss=1.810665488243103
I1020 23:07:35.505562 139658187740928 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7709345817565918, loss=1.8225833177566528
I1020 23:13:59.523878 139658196133632 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7038217782974243, loss=1.8560993671417236
I1020 23:15:12.692269 139830899640128 spec.py:321] Evaluating on the training split.
I1020 23:16:05.988121 139830899640128 spec.py:333] Evaluating on the validation split.
I1020 23:16:56.428352 139830899640128 spec.py:349] Evaluating on the test split.
I1020 23:17:21.899028 139830899640128 submission_runner.py:395] Time since start: 4956.30s, 	Step: 5597, 	{'train/ctc_loss': Array(0.50195926, dtype=float32), 'train/wer': 0.1773539752571016, 'validation/ctc_loss': Array(0.83409363, dtype=float32), 'validation/wer': 0.24700464388812188, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.567773, dtype=float32), 'test/wer': 0.18798366948997625, 'test/num_examples': 2472, 'score': 4397.660763263702, 'total_duration': 4956.30383849144, 'accumulated_submission_time': 4397.660763263702, 'accumulated_eval_time': 558.2785234451294, 'accumulated_logging_time': 0.13854622840881348}
I1020 23:17:21.933592 139657612453632 logging_writer.py:48] [5597] accumulated_eval_time=558.278523, accumulated_logging_time=0.138546, accumulated_submission_time=4397.660763, global_step=5597, preemption_count=0, score=4397.660763, test/ctc_loss=0.5677729845046997, test/num_examples=2472, test/wer=0.187984, total_duration=4956.303838, train/ctc_loss=0.5019592642784119, train/wer=0.177354, validation/ctc_loss=0.8340936303138733, validation/num_examples=5348, validation/wer=0.247005
I1020 23:22:29.834361 139657604060928 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7532539963722229, loss=1.8319460153579712
I1020 23:28:53.936183 139657612453632 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8450420498847961, loss=1.745487093925476
I1020 23:35:14.904011 139657604060928 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8724391460418701, loss=1.7457194328308105
I1020 23:41:22.610215 139830899640128 spec.py:321] Evaluating on the training split.
I1020 23:42:16.392901 139830899640128 spec.py:333] Evaluating on the validation split.
I1020 23:43:06.916273 139830899640128 spec.py:349] Evaluating on the test split.
I1020 23:43:32.149371 139830899640128 submission_runner.py:395] Time since start: 6526.55s, 	Step: 7468, 	{'train/ctc_loss': Array(0.48403767, dtype=float32), 'train/wer': 0.16476766957450598, 'validation/ctc_loss': Array(0.78869116, dtype=float32), 'validation/wer': 0.23267713874701912, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5266797, dtype=float32), 'test/wer': 0.17195783316068491, 'test/num_examples': 2472, 'score': 5838.247625112534, 'total_duration': 6526.554671525955, 'accumulated_submission_time': 5838.247625112534, 'accumulated_eval_time': 687.8131964206696, 'accumulated_logging_time': 0.1898651123046875}
I1020 23:43:32.185252 139657612453632 logging_writer.py:48] [7468] accumulated_eval_time=687.813196, accumulated_logging_time=0.189865, accumulated_submission_time=5838.247625, global_step=7468, preemption_count=0, score=5838.247625, test/ctc_loss=0.5266796946525574, test/num_examples=2472, test/wer=0.171958, total_duration=6526.554672, train/ctc_loss=0.4840376675128937, train/wer=0.164768, validation/ctc_loss=0.7886911630630493, validation/num_examples=5348, validation/wer=0.232677
I1020 23:43:57.304898 139657604060928 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6579928994178772, loss=1.8286914825439453
I1020 23:50:19.376031 139657612453632 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9123945236206055, loss=1.7390543222427368
I1020 23:56:43.810272 139657612453632 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.1076672077178955, loss=1.7496418952941895
I1021 00:03:04.997907 139657604060928 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.945641815662384, loss=1.7464566230773926
I1021 00:07:32.158213 139830899640128 spec.py:321] Evaluating on the training split.
I1021 00:08:25.479552 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 00:09:15.815948 139830899640128 spec.py:349] Evaluating on the test split.
I1021 00:09:41.274783 139830899640128 submission_runner.py:395] Time since start: 8095.68s, 	Step: 9330, 	{'train/ctc_loss': Array(0.45282903, dtype=float32), 'train/wer': 0.15905977516362607, 'validation/ctc_loss': Array(0.7491323, dtype=float32), 'validation/wer': 0.22434517315620264, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4959548, dtype=float32), 'test/wer': 0.16541750451932646, 'test/num_examples': 2472, 'score': 7278.132206201553, 'total_duration': 8095.679551124573, 'accumulated_submission_time': 7278.132206201553, 'accumulated_eval_time': 816.9247591495514, 'accumulated_logging_time': 0.24134063720703125}
I1021 00:09:41.310239 139657612453632 logging_writer.py:48] [9330] accumulated_eval_time=816.924759, accumulated_logging_time=0.241341, accumulated_submission_time=7278.132206, global_step=9330, preemption_count=0, score=7278.132206, test/ctc_loss=0.49595481157302856, test/num_examples=2472, test/wer=0.165418, total_duration=8095.679551, train/ctc_loss=0.4528290331363678, train/wer=0.159060, validation/ctc_loss=0.7491322755813599, validation/num_examples=5348, validation/wer=0.224345
I1021 00:11:51.101356 139657604060928 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5777615308761597, loss=1.6964092254638672
I1021 00:18:11.554066 139657612453632 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8855355978012085, loss=1.7172995805740356
I1021 00:24:52.450347 139657612453632 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.8839949369430542, loss=1.682808518409729
I1021 00:31:16.609157 139657604060928 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7221930027008057, loss=1.6865782737731934
I1021 00:33:41.546900 139830899640128 spec.py:321] Evaluating on the training split.
I1021 00:34:35.897706 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 00:35:26.286437 139830899640128 spec.py:349] Evaluating on the test split.
I1021 00:35:51.745734 139830899640128 submission_runner.py:395] Time since start: 9666.15s, 	Step: 11174, 	{'train/ctc_loss': Array(0.47206983, dtype=float32), 'train/wer': 0.16344538039274265, 'validation/ctc_loss': Array(0.7439973, dtype=float32), 'validation/wer': 0.22111086438108846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4895346, dtype=float32), 'test/wer': 0.16261450653017284, 'test/num_examples': 2472, 'score': 8718.283967733383, 'total_duration': 9666.149890422821, 'accumulated_submission_time': 8718.283967733383, 'accumulated_eval_time': 947.117752790451, 'accumulated_logging_time': 0.29043126106262207}
I1021 00:35:51.785604 139657612453632 logging_writer.py:48] [11174] accumulated_eval_time=947.117753, accumulated_logging_time=0.290431, accumulated_submission_time=8718.283968, global_step=11174, preemption_count=0, score=8718.283968, test/ctc_loss=0.4895345866680145, test/num_examples=2472, test/wer=0.162615, total_duration=9666.149890, train/ctc_loss=0.4720698297023773, train/wer=0.163445, validation/ctc_loss=0.7439972758293152, validation/num_examples=5348, validation/wer=0.221111
I1021 00:40:03.964468 139657612453632 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7388692498207092, loss=1.667736530303955
I1021 00:46:25.783241 139657604060928 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6158586740493774, loss=1.663917064666748
I1021 00:53:12.963217 139657612453632 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.7096879482269287, loss=1.6130439043045044
I1021 00:59:33.892353 139657604060928 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6542647480964661, loss=1.6428453922271729
I1021 00:59:51.781277 139830899640128 spec.py:321] Evaluating on the training split.
I1021 01:00:45.921673 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 01:01:36.629712 139830899640128 spec.py:349] Evaluating on the test split.
I1021 01:02:02.594409 139830899640128 submission_runner.py:395] Time since start: 11237.00s, 	Step: 13024, 	{'train/ctc_loss': Array(0.4283777, dtype=float32), 'train/wer': 0.14997447514517262, 'validation/ctc_loss': Array(0.73161554, dtype=float32), 'validation/wer': 0.21483533989206097, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4661154, dtype=float32), 'test/wer': 0.1547133020535007, 'test/num_examples': 2472, 'score': 10158.19237780571, 'total_duration': 11236.997790336609, 'accumulated_submission_time': 10158.19237780571, 'accumulated_eval_time': 1077.9242832660675, 'accumulated_logging_time': 0.34496259689331055}
I1021 01:02:02.628393 139657612453632 logging_writer.py:48] [13024] accumulated_eval_time=1077.924283, accumulated_logging_time=0.344963, accumulated_submission_time=10158.192378, global_step=13024, preemption_count=0, score=10158.192378, test/ctc_loss=0.46611538529396057, test/num_examples=2472, test/wer=0.154713, total_duration=11236.997790, train/ctc_loss=0.4283776879310608, train/wer=0.149974, validation/ctc_loss=0.7316155433654785, validation/num_examples=5348, validation/wer=0.214835
I1021 01:08:09.223343 139657612453632 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6516456604003906, loss=1.6794098615646362
I1021 01:14:31.848235 139657604060928 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.5972372889518738, loss=1.6141343116760254
I1021 01:21:28.554723 139657612453632 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7437458634376526, loss=1.6553542613983154
I1021 01:26:03.104174 139830899640128 spec.py:321] Evaluating on the training split.
I1021 01:26:57.685237 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 01:27:48.293890 139830899640128 spec.py:349] Evaluating on the test split.
I1021 01:28:14.370337 139830899640128 submission_runner.py:395] Time since start: 12808.78s, 	Step: 14862, 	{'train/ctc_loss': Array(0.38148648, dtype=float32), 'train/wer': 0.13793390363764008, 'validation/ctc_loss': Array(0.6888415, dtype=float32), 'validation/wer': 0.20587582185234174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4464068, dtype=float32), 'test/wer': 0.14770580708061665, 'test/num_examples': 2472, 'score': 11598.581330299377, 'total_duration': 12808.775510311127, 'accumulated_submission_time': 11598.581330299377, 'accumulated_eval_time': 1209.1856474876404, 'accumulated_logging_time': 0.3936469554901123}
I1021 01:28:14.417635 139657766053632 logging_writer.py:48] [14862] accumulated_eval_time=1209.185647, accumulated_logging_time=0.393647, accumulated_submission_time=11598.581330, global_step=14862, preemption_count=0, score=11598.581330, test/ctc_loss=0.4464068114757538, test/num_examples=2472, test/wer=0.147706, total_duration=12808.775510, train/ctc_loss=0.3814864754676819, train/wer=0.137934, validation/ctc_loss=0.6888415217399597, validation/num_examples=5348, validation/wer=0.205876
I1021 01:30:00.226594 139657757660928 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6501844525337219, loss=1.6061091423034668
I1021 01:36:34.980275 139657766053632 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6577451229095459, loss=1.547595739364624
I1021 01:42:55.609897 139657757660928 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6653808355331421, loss=1.6781119108200073
I1021 01:49:50.086953 139657766053632 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6705889105796814, loss=1.6000101566314697
I1021 01:52:14.616452 139830899640128 spec.py:321] Evaluating on the training split.
I1021 01:53:08.142247 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 01:53:58.677956 139830899640128 spec.py:349] Evaluating on the test split.
I1021 01:54:24.359171 139830899640128 submission_runner.py:395] Time since start: 14378.76s, 	Step: 16691, 	{'train/ctc_loss': Array(0.35427904, dtype=float32), 'train/wer': 0.12452585266719512, 'validation/ctc_loss': Array(0.65696126, dtype=float32), 'validation/wer': 0.19508191973121447, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42384827, dtype=float32), 'test/wer': 0.14033270367436476, 'test/num_examples': 2472, 'score': 13038.67775440216, 'total_duration': 14378.763293743134, 'accumulated_submission_time': 13038.67775440216, 'accumulated_eval_time': 1338.9225084781647, 'accumulated_logging_time': 0.47103238105773926}
I1021 01:54:24.392475 139658196133632 logging_writer.py:48] [16691] accumulated_eval_time=1338.922508, accumulated_logging_time=0.471032, accumulated_submission_time=13038.677754, global_step=16691, preemption_count=0, score=13038.677754, test/ctc_loss=0.4238482713699341, test/num_examples=2472, test/wer=0.140333, total_duration=14378.763294, train/ctc_loss=0.3542790412902832, train/wer=0.124526, validation/ctc_loss=0.6569612622261047, validation/num_examples=5348, validation/wer=0.195082
I1021 01:58:20.415936 139658187740928 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6452201008796692, loss=1.618266224861145
I1021 02:04:57.018024 139658196133632 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6663728356361389, loss=1.534853219985962
I1021 02:11:22.142179 139658196133632 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7552487254142761, loss=1.5554282665252686
I1021 02:18:10.705587 139658187740928 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.0049484968185425, loss=1.6220515966415405
I1021 02:18:25.309282 139830899640128 spec.py:321] Evaluating on the training split.
I1021 02:19:18.623201 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 02:20:09.205075 139830899640128 spec.py:349] Evaluating on the test split.
I1021 02:20:34.824716 139830899640128 submission_runner.py:395] Time since start: 15949.23s, 	Step: 18519, 	{'train/ctc_loss': Array(0.3512982, dtype=float32), 'train/wer': 0.1288002702891128, 'validation/ctc_loss': Array(0.6411016, dtype=float32), 'validation/wer': 0.19166417254795948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41200873, dtype=float32), 'test/wer': 0.1364531919647391, 'test/num_examples': 2472, 'score': 14479.509255170822, 'total_duration': 15949.22879743576, 'accumulated_submission_time': 14479.509255170822, 'accumulated_eval_time': 1468.4320187568665, 'accumulated_logging_time': 0.5177080631256104}
I1021 02:20:34.861124 139658196133632 logging_writer.py:48] [18519] accumulated_eval_time=1468.432019, accumulated_logging_time=0.517708, accumulated_submission_time=14479.509255, global_step=18519, preemption_count=0, score=14479.509255, test/ctc_loss=0.41200873255729675, test/num_examples=2472, test/wer=0.136453, total_duration=15949.228797, train/ctc_loss=0.3512982130050659, train/wer=0.128800, validation/ctc_loss=0.641101598739624, validation/num_examples=5348, validation/wer=0.191664
I1021 02:26:44.684795 139657868453632 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6775313019752502, loss=1.603726863861084
I1021 02:33:34.196804 139657860060928 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.5739792585372925, loss=1.5292242765426636
I1021 02:40:03.066210 139658196133632 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6796281337738037, loss=1.5944424867630005
I1021 02:44:34.821817 139830899640128 spec.py:321] Evaluating on the training split.
I1021 02:45:29.212991 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 02:46:19.910835 139830899640128 spec.py:349] Evaluating on the test split.
I1021 02:46:45.638114 139830899640128 submission_runner.py:395] Time since start: 17520.04s, 	Step: 20346, 	{'train/ctc_loss': Array(0.35971203, dtype=float32), 'train/wer': 0.12398529839498665, 'validation/ctc_loss': Array(0.6287478, dtype=float32), 'validation/wer': 0.1873002693648204, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39301708, dtype=float32), 'test/wer': 0.1320862023439563, 'test/num_examples': 2472, 'score': 15919.383308410645, 'total_duration': 17520.042124271393, 'accumulated_submission_time': 15919.383308410645, 'accumulated_eval_time': 1599.24236702919, 'accumulated_logging_time': 0.5689959526062012}
I1021 02:46:45.677869 139657689253632 logging_writer.py:48] [20346] accumulated_eval_time=1599.242367, accumulated_logging_time=0.568996, accumulated_submission_time=15919.383308, global_step=20346, preemption_count=0, score=15919.383308, test/ctc_loss=0.39301708340644836, test/num_examples=2472, test/wer=0.132086, total_duration=17520.042124, train/ctc_loss=0.35971203446388245, train/wer=0.123985, validation/ctc_loss=0.628747820854187, validation/num_examples=5348, validation/wer=0.187300
I1021 02:48:43.709491 139657680860928 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9953561425209045, loss=1.576196312904358
I1021 02:55:07.829733 139657689253632 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6966367959976196, loss=1.5683914422988892
I1021 03:01:41.551322 139657680860928 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6386383771896362, loss=1.5309672355651855
I1021 03:08:14.862248 139657689253632 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7743217945098877, loss=1.5522974729537964
I1021 03:10:45.986664 139830899640128 spec.py:321] Evaluating on the training split.
I1021 03:11:40.974463 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 03:12:31.771805 139830899640128 spec.py:349] Evaluating on the test split.
I1021 03:12:57.413112 139830899640128 submission_runner.py:395] Time since start: 19091.82s, 	Step: 22200, 	{'train/ctc_loss': Array(0.36246973, dtype=float32), 'train/wer': 0.12645685493444483, 'validation/ctc_loss': Array(0.61231947, dtype=float32), 'validation/wer': 0.18367977446730452, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38885424, dtype=float32), 'test/wer': 0.1295066317307497, 'test/num_examples': 2472, 'score': 17359.603481531143, 'total_duration': 19091.81662297249, 'accumulated_submission_time': 17359.603481531143, 'accumulated_eval_time': 1730.6623520851135, 'accumulated_logging_time': 0.6245684623718262}
I1021 03:12:57.448192 139657904293632 logging_writer.py:48] [22200] accumulated_eval_time=1730.662352, accumulated_logging_time=0.624568, accumulated_submission_time=17359.603482, global_step=22200, preemption_count=0, score=17359.603482, test/ctc_loss=0.3888542354106903, test/num_examples=2472, test/wer=0.129507, total_duration=19091.816623, train/ctc_loss=0.36246973276138306, train/wer=0.126457, validation/ctc_loss=0.6123194694519043, validation/num_examples=5348, validation/wer=0.183680
I1021 03:16:46.704452 139657895900928 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7855523824691772, loss=1.5034388303756714
I1021 03:23:10.688216 139657904293632 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6964711546897888, loss=1.5006184577941895
I1021 03:29:42.188401 139657895900928 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8687851428985596, loss=1.4812407493591309
I1021 03:36:17.849714 139657904293632 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7316374778747559, loss=1.483672022819519
I1021 03:36:57.828969 139830899640128 spec.py:321] Evaluating on the training split.
I1021 03:37:52.764133 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 03:38:43.307096 139830899640128 spec.py:349] Evaluating on the test split.
I1021 03:39:09.212233 139830899640128 submission_runner.py:395] Time since start: 20663.62s, 	Step: 24054, 	{'train/ctc_loss': Array(0.33017087, dtype=float32), 'train/wer': 0.11635456322666941, 'validation/ctc_loss': Array(0.59012485, dtype=float32), 'validation/wer': 0.1760525985498711, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3647378, dtype=float32), 'test/wer': 0.12296630308939126, 'test/num_examples': 2472, 'score': 18799.894825220108, 'total_duration': 20663.61720943451, 'accumulated_submission_time': 18799.894825220108, 'accumulated_eval_time': 1862.040631055832, 'accumulated_logging_time': 0.6753864288330078}
I1021 03:39:09.249663 139657904293632 logging_writer.py:48] [24054] accumulated_eval_time=1862.040631, accumulated_logging_time=0.675386, accumulated_submission_time=18799.894825, global_step=24054, preemption_count=0, score=18799.894825, test/ctc_loss=0.3647378087043762, test/num_examples=2472, test/wer=0.122966, total_duration=20663.617209, train/ctc_loss=0.3301708698272705, train/wer=0.116355, validation/ctc_loss=0.5901248455047607, validation/num_examples=5348, validation/wer=0.176053
I1021 03:44:49.678758 139657895900928 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6942061185836792, loss=1.5260459184646606
I1021 03:51:21.968697 139657904293632 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7186389565467834, loss=1.5248290300369263
I1021 03:57:48.121501 139657895900928 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8179460167884827, loss=1.4567835330963135
I1021 04:03:09.232744 139830899640128 spec.py:321] Evaluating on the training split.
I1021 04:04:03.555681 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 04:04:53.495532 139830899640128 spec.py:349] Evaluating on the test split.
I1021 04:05:19.304677 139830899640128 submission_runner.py:395] Time since start: 22233.71s, 	Step: 25899, 	{'train/ctc_loss': Array(0.30398074, dtype=float32), 'train/wer': 0.10732232591529074, 'validation/ctc_loss': Array(0.5778364, dtype=float32), 'validation/wer': 0.1718238605095726, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.363951, dtype=float32), 'test/wer': 0.12034610931692158, 'test/num_examples': 2472, 'score': 20239.790155649185, 'total_duration': 22233.708899497986, 'accumulated_submission_time': 20239.790155649185, 'accumulated_eval_time': 1992.1069824695587, 'accumulated_logging_time': 0.7268610000610352}
I1021 04:05:19.338746 139657612453632 logging_writer.py:48] [25899] accumulated_eval_time=1992.106982, accumulated_logging_time=0.726861, accumulated_submission_time=20239.790156, global_step=25899, preemption_count=0, score=20239.790156, test/ctc_loss=0.3639509975910187, test/num_examples=2472, test/wer=0.120346, total_duration=22233.708899, train/ctc_loss=0.3039807379245758, train/wer=0.107322, validation/ctc_loss=0.5778363943099976, validation/num_examples=5348, validation/wer=0.171824
I1021 04:06:36.806770 139657604060928 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8209163546562195, loss=1.5087825059890747
I1021 04:12:58.737790 139657612453632 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.9643985629081726, loss=1.5166676044464111
I1021 04:19:40.649159 139657612453632 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.786426842212677, loss=1.4778962135314941
I1021 04:26:04.148015 139657604060928 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8016027808189392, loss=1.4978382587432861
I1021 04:29:19.317155 139830899640128 spec.py:321] Evaluating on the training split.
I1021 04:30:14.392361 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 04:31:05.061480 139830899640128 spec.py:349] Evaluating on the test split.
I1021 04:31:31.031760 139830899640128 submission_runner.py:395] Time since start: 23805.44s, 	Step: 27741, 	{'train/ctc_loss': Array(0.29009196, dtype=float32), 'train/wer': 0.10419155429197423, 'validation/ctc_loss': Array(0.56793153, dtype=float32), 'validation/wer': 0.16989292989756413, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3519648, dtype=float32), 'test/wer': 0.11636503970913818, 'test/num_examples': 2472, 'score': 21679.679236888885, 'total_duration': 23805.436156749725, 'accumulated_submission_time': 21679.679236888885, 'accumulated_eval_time': 2123.8160145282745, 'accumulated_logging_time': 0.7764804363250732}
I1021 04:31:31.070502 139657766053632 logging_writer.py:48] [27741] accumulated_eval_time=2123.816015, accumulated_logging_time=0.776480, accumulated_submission_time=21679.679237, global_step=27741, preemption_count=0, score=21679.679237, test/ctc_loss=0.3519648015499115, test/num_examples=2472, test/wer=0.116365, total_duration=23805.436157, train/ctc_loss=0.29009196162223816, train/wer=0.104192, validation/ctc_loss=0.5679315328598022, validation/num_examples=5348, validation/wer=0.169893
I1021 04:34:52.227055 139657766053632 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.7207515835762024, loss=1.4944614171981812
I1021 04:41:15.234456 139657757660928 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6426889300346375, loss=1.4203011989593506
I1021 04:48:01.486132 139657766053632 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.7413584589958191, loss=1.4323983192443848
I1021 04:54:22.921182 139657757660928 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.8193295001983643, loss=1.4816797971725464
I1021 04:55:31.589792 139830899640128 spec.py:321] Evaluating on the training split.
I1021 04:56:26.917632 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 04:57:17.611475 139830899640128 spec.py:349] Evaluating on the test split.
I1021 04:57:43.802435 139830899640128 submission_runner.py:395] Time since start: 25378.21s, 	Step: 29588, 	{'train/ctc_loss': Array(0.2731936, dtype=float32), 'train/wer': 0.09538615744955048, 'validation/ctc_loss': Array(0.5399274, dtype=float32), 'validation/wer': 0.16005483842938104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3304064, dtype=float32), 'test/wer': 0.10964190685109582, 'test/num_examples': 2472, 'score': 23120.109338998795, 'total_duration': 25378.206876039505, 'accumulated_submission_time': 23120.109338998795, 'accumulated_eval_time': 2256.0231161117554, 'accumulated_logging_time': 0.8302571773529053}
I1021 04:57:43.836655 139658196133632 logging_writer.py:48] [29588] accumulated_eval_time=2256.023116, accumulated_logging_time=0.830257, accumulated_submission_time=23120.109339, global_step=29588, preemption_count=0, score=23120.109339, test/ctc_loss=0.33040639758110046, test/num_examples=2472, test/wer=0.109642, total_duration=25378.206876, train/ctc_loss=0.2731935977935791, train/wer=0.095386, validation/ctc_loss=0.5399274230003357, validation/num_examples=5348, validation/wer=0.160055
I1021 05:03:01.354491 139658196133632 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7589107155799866, loss=1.3515912294387817
I1021 05:09:22.402849 139658187740928 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.7398096323013306, loss=1.4431980848312378
I1021 05:16:14.922499 139658196133632 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.9955129027366638, loss=1.3639150857925415
I1021 05:21:44.201360 139830899640128 spec.py:321] Evaluating on the training split.
I1021 05:22:38.549214 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 05:23:29.568682 139830899640128 spec.py:349] Evaluating on the test split.
I1021 05:23:55.205357 139830899640128 submission_runner.py:395] Time since start: 26949.61s, 	Step: 31434, 	{'train/ctc_loss': Array(0.27667153, dtype=float32), 'train/wer': 0.09658844687120818, 'validation/ctc_loss': Array(0.5234666, dtype=float32), 'validation/wer': 0.15526613051160007, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32105663, dtype=float32), 'test/wer': 0.10501086669510287, 'test/num_examples': 2472, 'score': 24560.38601064682, 'total_duration': 26949.607805252075, 'accumulated_submission_time': 24560.38601064682, 'accumulated_eval_time': 2387.0195891857147, 'accumulated_logging_time': 0.8789505958557129}
I1021 05:23:55.252805 139657612453632 logging_writer.py:48] [31434] accumulated_eval_time=2387.019589, accumulated_logging_time=0.878951, accumulated_submission_time=24560.386011, global_step=31434, preemption_count=0, score=24560.386011, test/ctc_loss=0.32105663418769836, test/num_examples=2472, test/wer=0.105011, total_duration=26949.607805, train/ctc_loss=0.27667152881622314, train/wer=0.096588, validation/ctc_loss=0.5234665870666504, validation/num_examples=5348, validation/wer=0.155266
I1021 05:24:46.176668 139657604060928 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.7816879749298096, loss=1.4868444204330444
I1021 05:31:10.805811 139657612453632 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.7833091020584106, loss=1.3518309593200684
I1021 05:37:32.373549 139657604060928 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7333030700683594, loss=1.359204649925232
I1021 05:44:23.042251 139657612453632 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7684952020645142, loss=1.384324312210083
I1021 05:47:55.637214 139830899640128 spec.py:321] Evaluating on the training split.
I1021 05:48:48.870076 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 05:49:40.058990 139830899640128 spec.py:349] Evaluating on the test split.
I1021 05:50:05.881977 139830899640128 submission_runner.py:395] Time since start: 28520.29s, 	Step: 33280, 	{'train/ctc_loss': Array(0.25150132, dtype=float32), 'train/wer': 0.08752082214227966, 'validation/ctc_loss': Array(0.49624133, dtype=float32), 'validation/wer': 0.14733966034930535, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30043656, dtype=float32), 'test/wer': 0.10007515284463672, 'test/num_examples': 2472, 'score': 26000.67474770546, 'total_duration': 28520.286294460297, 'accumulated_submission_time': 26000.67474770546, 'accumulated_eval_time': 2517.2587049007416, 'accumulated_logging_time': 0.9479522705078125}
I1021 05:50:05.919568 139657612453632 logging_writer.py:48] [33280] accumulated_eval_time=2517.258705, accumulated_logging_time=0.947952, accumulated_submission_time=26000.674748, global_step=33280, preemption_count=0, score=26000.674748, test/ctc_loss=0.3004365563392639, test/num_examples=2472, test/wer=0.100075, total_duration=28520.286294, train/ctc_loss=0.25150132179260254, train/wer=0.087521, validation/ctc_loss=0.49624133110046387, validation/num_examples=5348, validation/wer=0.147340
I1021 05:52:54.266493 139657604060928 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7452828288078308, loss=1.3368374109268188
I1021 05:59:31.904112 139657612453632 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.8277333974838257, loss=1.2880432605743408
I1021 06:05:52.405976 139657604060928 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.851184070110321, loss=1.378429651260376
I1021 06:12:49.874899 139657612453632 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.8306050896644592, loss=1.3301695585250854
I1021 06:14:06.578694 139830899640128 spec.py:321] Evaluating on the training split.
I1021 06:15:01.645040 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 06:15:52.637294 139830899640128 spec.py:349] Evaluating on the test split.
I1021 06:16:18.609687 139830899640128 submission_runner.py:395] Time since start: 30093.02s, 	Step: 35096, 	{'train/ctc_loss': Array(0.252029, dtype=float32), 'train/wer': 0.08698868335482281, 'validation/ctc_loss': Array(0.48229554, dtype=float32), 'validation/wer': 0.14255095243152438, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2871261, dtype=float32), 'test/wer': 0.09477383056080271, 'test/num_examples': 2472, 'score': 27441.245020389557, 'total_duration': 30093.015188694, 'accumulated_submission_time': 27441.245020389557, 'accumulated_eval_time': 2649.285391807556, 'accumulated_logging_time': 1.0013396739959717}
I1021 06:16:18.642000 139657612453632 logging_writer.py:48] [35096] accumulated_eval_time=2649.285392, accumulated_logging_time=1.001340, accumulated_submission_time=27441.245020, global_step=35096, preemption_count=0, score=27441.245020, test/ctc_loss=0.2871260941028595, test/num_examples=2472, test/wer=0.094774, total_duration=30093.015189, train/ctc_loss=0.2520290017127991, train/wer=0.086989, validation/ctc_loss=0.48229554295539856, validation/num_examples=5348, validation/wer=0.142551
I1021 06:21:27.201053 139657604060928 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.8529449701309204, loss=1.3179739713668823
I1021 06:27:58.925510 139657612453632 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7596977949142456, loss=1.299695611000061
I1021 06:34:24.852695 139657612453632 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6948021650314331, loss=1.3257218599319458
I1021 06:40:19.418407 139830899640128 spec.py:321] Evaluating on the training split.
I1021 06:41:13.607069 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 06:42:04.536781 139830899640128 spec.py:349] Evaluating on the test split.
I1021 06:42:30.139619 139830899640128 submission_runner.py:395] Time since start: 31664.54s, 	Step: 36952, 	{'train/ctc_loss': Array(0.23590183, dtype=float32), 'train/wer': 0.08117800014919488, 'validation/ctc_loss': Array(0.4629277, dtype=float32), 'validation/wer': 0.13676781524855905, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27535346, dtype=float32), 'test/wer': 0.09278329575691102, 'test/num_examples': 2472, 'score': 28881.935079813004, 'total_duration': 31664.54434323311, 'accumulated_submission_time': 28881.935079813004, 'accumulated_eval_time': 2780.001361846924, 'accumulated_logging_time': 1.047724723815918}
I1021 06:42:30.179870 139657766053632 logging_writer.py:48] [36952] accumulated_eval_time=2780.001362, accumulated_logging_time=1.047725, accumulated_submission_time=28881.935080, global_step=36952, preemption_count=0, score=28881.935080, test/ctc_loss=0.27535346150398254, test/num_examples=2472, test/wer=0.092783, total_duration=31664.544343, train/ctc_loss=0.2359018325805664, train/wer=0.081178, validation/ctc_loss=0.4629276990890503, validation/num_examples=5348, validation/wer=0.136768
I1021 06:43:07.586091 139657757660928 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7997455596923828, loss=1.3087360858917236
I1021 06:49:33.155164 139657766053632 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.8179126381874084, loss=1.3864465951919556
I1021 06:56:10.806020 139657757660928 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.7771577835083008, loss=1.2800798416137695
I1021 07:02:40.977844 139657766053632 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.9499267339706421, loss=1.263305425643921
I1021 07:06:30.556742 139830899640128 spec.py:321] Evaluating on the training split.
I1021 07:07:26.559257 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 07:08:17.414772 139830899640128 spec.py:349] Evaluating on the test split.
I1021 07:08:43.613976 139830899640128 submission_runner.py:395] Time since start: 33238.02s, 	Step: 38798, 	{'train/ctc_loss': Array(0.17703904, dtype=float32), 'train/wer': 0.06486577903508502, 'validation/ctc_loss': Array(0.44492716, dtype=float32), 'validation/wer': 0.13220116435115903, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25913087, dtype=float32), 'test/wer': 0.08695387240265676, 'test/num_examples': 2472, 'score': 30322.219118833542, 'total_duration': 33238.01707649231, 'accumulated_submission_time': 30322.219118833542, 'accumulated_eval_time': 2913.0517041683197, 'accumulated_logging_time': 1.1081364154815674}
I1021 07:08:43.650070 139658196133632 logging_writer.py:48] [38798] accumulated_eval_time=2913.051704, accumulated_logging_time=1.108136, accumulated_submission_time=30322.219119, global_step=38798, preemption_count=0, score=30322.219119, test/ctc_loss=0.2591308653354645, test/num_examples=2472, test/wer=0.086954, total_duration=33238.017076, train/ctc_loss=0.1770390421152115, train/wer=0.064866, validation/ctc_loss=0.4449271559715271, validation/num_examples=5348, validation/wer=0.132201
I1021 07:11:18.410121 139658187740928 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.8844572305679321, loss=1.3165112733840942
I1021 07:17:42.777567 139658196133632 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.8208104372024536, loss=1.2903971672058105
I1021 07:24:21.208106 139658187740928 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.7708737850189209, loss=1.2917331457138062
I1021 07:30:56.173651 139658196133632 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7529528141021729, loss=1.187540054321289
I1021 07:32:44.082827 139830899640128 spec.py:321] Evaluating on the training split.
I1021 07:33:38.716546 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 07:34:30.136005 139830899640128 spec.py:349] Evaluating on the test split.
I1021 07:34:55.886990 139830899640128 submission_runner.py:395] Time since start: 34810.29s, 	Step: 40643, 	{'train/ctc_loss': Array(0.19533722, dtype=float32), 'train/wer': 0.06910624907604908, 'validation/ctc_loss': Array(0.43243784, dtype=float32), 'validation/wer': 0.1275572762292787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25135544, dtype=float32), 'test/wer': 0.08329778806897813, 'test/num_examples': 2472, 'score': 31762.563623905182, 'total_duration': 34810.29227042198, 'accumulated_submission_time': 31762.563623905182, 'accumulated_eval_time': 3044.8511548042297, 'accumulated_logging_time': 1.1602110862731934}
I1021 07:34:55.921781 139658196133632 logging_writer.py:48] [40643] accumulated_eval_time=3044.851155, accumulated_logging_time=1.160211, accumulated_submission_time=31762.563624, global_step=40643, preemption_count=0, score=31762.563624, test/ctc_loss=0.25135543942451477, test/num_examples=2472, test/wer=0.083298, total_duration=34810.292270, train/ctc_loss=0.1953372210264206, train/wer=0.069106, validation/ctc_loss=0.43243783712387085, validation/num_examples=5348, validation/wer=0.127557
I1021 07:39:29.730612 139658187740928 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.952433168888092, loss=1.2569255828857422
I1021 07:45:55.537137 139658196133632 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.9742898344993591, loss=1.2385811805725098
I1021 07:52:21.111508 139658187740928 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.0300993919372559, loss=1.2434196472167969
I1021 07:58:56.337853 139830899640128 spec.py:321] Evaluating on the training split.
I1021 07:59:49.045495 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 08:00:39.989092 139830899640128 spec.py:349] Evaluating on the test split.
I1021 08:01:05.952267 139830899640128 submission_runner.py:395] Time since start: 36380.36s, 	Step: 42497, 	{'train/ctc_loss': Array(0.22531338, dtype=float32), 'train/wer': 0.07920786708829863, 'validation/ctc_loss': Array(0.41273975, dtype=float32), 'validation/wer': 0.12290373345433832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23794486, dtype=float32), 'test/wer': 0.07850425527593281, 'test/num_examples': 2472, 'score': 33202.88981986046, 'total_duration': 36380.35505485535, 'accumulated_submission_time': 33202.88981986046, 'accumulated_eval_time': 3174.4584741592407, 'accumulated_logging_time': 1.2100152969360352}
I1021 08:01:05.993723 139657612453632 logging_writer.py:48] [42497] accumulated_eval_time=3174.458474, accumulated_logging_time=1.210015, accumulated_submission_time=33202.889820, global_step=42497, preemption_count=0, score=33202.889820, test/ctc_loss=0.2379448562860489, test/num_examples=2472, test/wer=0.078504, total_duration=36380.355055, train/ctc_loss=0.22531338036060333, train/wer=0.079208, validation/ctc_loss=0.41273975372314453, validation/num_examples=5348, validation/wer=0.122904
I1021 08:01:09.125875 139657604060928 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.7990306615829468, loss=1.2117666006088257
I1021 08:07:30.539875 139657612453632 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.8118590712547302, loss=1.2190608978271484
I1021 08:14:12.982127 139657612453632 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.9039438962936401, loss=1.1467148065567017
I1021 08:20:41.098052 139657604060928 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.0243477821350098, loss=1.121530532836914
I1021 08:25:06.131610 139830899640128 spec.py:321] Evaluating on the training split.
I1021 08:25:58.287431 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 08:26:49.385143 139830899640128 spec.py:349] Evaluating on the test split.
I1021 08:27:15.260506 139830899640128 submission_runner.py:395] Time since start: 37949.67s, 	Step: 44314, 	{'train/ctc_loss': Array(0.22217162, dtype=float32), 'train/wer': 0.07790181917096432, 'validation/ctc_loss': Array(0.39360687, dtype=float32), 'validation/wer': 0.11560481574094635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22694327, dtype=float32), 'test/wer': 0.07344667194767737, 'test/num_examples': 2472, 'score': 34642.93722462654, 'total_duration': 37949.66504859924, 'accumulated_submission_time': 34642.93722462654, 'accumulated_eval_time': 3303.5820989608765, 'accumulated_logging_time': 1.268972396850586}
I1021 08:27:15.297289 139657612453632 logging_writer.py:48] [44314] accumulated_eval_time=3303.582099, accumulated_logging_time=1.268972, accumulated_submission_time=34642.937225, global_step=44314, preemption_count=0, score=34642.937225, test/ctc_loss=0.2269432693719864, test/num_examples=2472, test/wer=0.073447, total_duration=37949.665049, train/ctc_loss=0.2221716195344925, train/wer=0.077902, validation/ctc_loss=0.39360687136650085, validation/num_examples=5348, validation/wer=0.115605
I1021 08:29:37.735320 139657604060928 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.809394359588623, loss=1.1457490921020508
I1021 08:35:58.549637 139657612453632 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.8835654854774475, loss=1.1420282125473022
I1021 08:42:45.747733 139657612453632 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.9278901219367981, loss=1.1785906553268433
I1021 08:49:13.654742 139657604060928 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9430031180381775, loss=1.1374907493591309
I1021 08:51:15.889911 139830899640128 spec.py:321] Evaluating on the training split.
I1021 08:52:07.914772 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 08:52:59.142530 139830899640128 spec.py:349] Evaluating on the test split.
I1021 08:53:24.970583 139830899640128 submission_runner.py:395] Time since start: 39519.38s, 	Step: 46147, 	{'train/ctc_loss': Array(0.23933567, dtype=float32), 'train/wer': 0.08507701037139297, 'validation/ctc_loss': Array(0.3777713, dtype=float32), 'validation/wer': 0.11108643810884655, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21491286, dtype=float32), 'test/wer': 0.07017650762699815, 'test/num_examples': 2472, 'score': 36083.43827319145, 'total_duration': 39519.37506532669, 'accumulated_submission_time': 36083.43827319145, 'accumulated_eval_time': 3432.6572666168213, 'accumulated_logging_time': 1.3231289386749268}
I1021 08:53:25.007711 139657612453632 logging_writer.py:48] [46147] accumulated_eval_time=3432.657267, accumulated_logging_time=1.323129, accumulated_submission_time=36083.438273, global_step=46147, preemption_count=0, score=36083.438273, test/ctc_loss=0.21491286158561707, test/num_examples=2472, test/wer=0.070177, total_duration=39519.375065, train/ctc_loss=0.23933567106723785, train/wer=0.085077, validation/ctc_loss=0.3777712881565094, validation/num_examples=5348, validation/wer=0.111086
I1021 08:57:58.515725 139657612453632 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.9845150709152222, loss=1.148813247680664
I1021 09:04:21.942236 139657604060928 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.991301417350769, loss=1.1140440702438354
I1021 09:11:17.037901 139657612453632 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.075727105140686, loss=1.1133291721343994
I1021 09:17:25.724398 139830899640128 spec.py:321] Evaluating on the training split.
I1021 09:18:17.313721 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 09:19:08.041150 139830899640128 spec.py:349] Evaluating on the test split.
I1021 09:19:33.556875 139830899640128 submission_runner.py:395] Time since start: 41087.96s, 	Step: 47984, 	{'train/ctc_loss': Array(0.19812894, dtype=float32), 'train/wer': 0.06833717007459567, 'validation/ctc_loss': Array(0.36412218, dtype=float32), 'validation/wer': 0.1058439614972436, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20263518, dtype=float32), 'test/wer': 0.06688603172668739, 'test/num_examples': 2472, 'score': 37523.990773916245, 'total_duration': 41087.96130132675, 'accumulated_submission_time': 37523.990773916245, 'accumulated_eval_time': 3560.484179496765, 'accumulated_logging_time': 1.450742244720459}
I1021 09:19:33.609612 139657612453632 logging_writer.py:48] [47984] accumulated_eval_time=3560.484179, accumulated_logging_time=1.450742, accumulated_submission_time=37523.990774, global_step=47984, preemption_count=0, score=37523.990774, test/ctc_loss=0.20263518393039703, test/num_examples=2472, test/wer=0.066886, total_duration=41087.961301, train/ctc_loss=0.19812893867492676, train/wer=0.068337, validation/ctc_loss=0.3641221821308136, validation/num_examples=5348, validation/wer=0.105844
I1021 09:19:46.690618 139657604060928 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0547834634780884, loss=1.1100153923034668
I1021 09:26:11.436114 139657612453632 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.9554014205932617, loss=1.0587126016616821
I1021 09:32:32.849235 139657604060928 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.028931736946106, loss=1.1172789335250854
I1021 09:39:23.389428 139657612453632 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1116188764572144, loss=1.0789438486099243
I1021 09:43:33.791754 139830899640128 spec.py:321] Evaluating on the training split.
I1021 09:44:26.440469 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 09:45:17.348826 139830899640128 spec.py:349] Evaluating on the test split.
I1021 09:45:42.814684 139830899640128 submission_runner.py:395] Time since start: 42657.22s, 	Step: 49830, 	{'train/ctc_loss': Array(0.17208968, dtype=float32), 'train/wer': 0.062040016478683485, 'validation/ctc_loss': Array(0.3454565, dtype=float32), 'validation/wer': 0.09973256611023683, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19459349, dtype=float32), 'test/wer': 0.06320963581337721, 'test/num_examples': 2472, 'score': 38964.00204706192, 'total_duration': 42657.21874332428, 'accumulated_submission_time': 38964.00204706192, 'accumulated_eval_time': 3689.5011858940125, 'accumulated_logging_time': 1.6001155376434326}
I1021 09:45:42.855268 139658196133632 logging_writer.py:48] [49830] accumulated_eval_time=3689.501186, accumulated_logging_time=1.600116, accumulated_submission_time=38964.002047, global_step=49830, preemption_count=0, score=38964.002047, test/ctc_loss=0.19459348917007446, test/num_examples=2472, test/wer=0.063210, total_duration=42657.218743, train/ctc_loss=0.17208968102931976, train/wer=0.062040, validation/ctc_loss=0.3454565107822418, validation/num_examples=5348, validation/wer=0.099733
I1021 09:47:53.122787 139658187740928 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.158813714981079, loss=1.0812785625457764
I1021 09:54:32.085856 139658196133632 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.025162696838379, loss=1.0637304782867432
I1021 10:00:53.067321 139658187740928 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.1052087545394897, loss=1.0140959024429321
I1021 10:07:49.739806 139658196133632 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.5686266422271729, loss=1.0738922357559204
I1021 10:09:43.116667 139830899640128 spec.py:321] Evaluating on the training split.
I1021 10:10:36.207142 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 10:11:27.427258 139830899640128 spec.py:349] Evaluating on the test split.
I1021 10:11:53.661365 139830899640128 submission_runner.py:395] Time since start: 44228.07s, 	Step: 51650, 	{'train/ctc_loss': Array(0.13969707, dtype=float32), 'train/wer': 0.05028874702280612, 'validation/ctc_loss': Array(0.33363423, dtype=float32), 'validation/wer': 0.09593828745764021, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18793625, dtype=float32), 'test/wer': 0.06048788414274978, 'test/num_examples': 2472, 'score': 40404.10059261322, 'total_duration': 44228.066487789154, 'accumulated_submission_time': 40404.10059261322, 'accumulated_eval_time': 3820.0410339832306, 'accumulated_logging_time': 1.7318615913391113}
I1021 10:11:53.700712 139657766053632 logging_writer.py:48] [51650] accumulated_eval_time=3820.041034, accumulated_logging_time=1.731862, accumulated_submission_time=40404.100593, global_step=51650, preemption_count=0, score=40404.100593, test/ctc_loss=0.18793624639511108, test/num_examples=2472, test/wer=0.060488, total_duration=44228.066488, train/ctc_loss=0.13969707489013672, train/wer=0.050289, validation/ctc_loss=0.33363422751426697, validation/num_examples=5348, validation/wer=0.095938
I1021 10:16:21.522160 139657757660928 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.1375596523284912, loss=1.0942089557647705
I1021 10:23:02.625617 139657766053632 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.279140591621399, loss=1.020876407623291
I1021 10:29:28.914309 139657766053632 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.0997034311294556, loss=0.9965105652809143
I1021 10:35:53.798604 139830899640128 spec.py:321] Evaluating on the training split.
I1021 10:36:47.372291 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 10:37:38.604080 139830899640128 spec.py:349] Evaluating on the test split.
I1021 10:38:04.439675 139830899640128 submission_runner.py:395] Time since start: 45798.84s, 	Step: 53470, 	{'train/ctc_loss': Array(0.14543146, dtype=float32), 'train/wer': 0.0531480988665432, 'validation/ctc_loss': Array(0.3237435, dtype=float32), 'validation/wer': 0.09346669627426939, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17842638, dtype=float32), 'test/wer': 0.05792862510917474, 'test/num_examples': 2472, 'score': 41844.03465628624, 'total_duration': 45798.84410381317, 'accumulated_submission_time': 41844.03465628624, 'accumulated_eval_time': 3950.6765501499176, 'accumulated_logging_time': 1.861433982849121}
I1021 10:38:04.476820 139658196133632 logging_writer.py:48] [53470] accumulated_eval_time=3950.676550, accumulated_logging_time=1.861434, accumulated_submission_time=41844.034656, global_step=53470, preemption_count=0, score=41844.034656, test/ctc_loss=0.17842638492584229, test/num_examples=2472, test/wer=0.057929, total_duration=45798.844104, train/ctc_loss=0.14543145895004272, train/wer=0.053148, validation/ctc_loss=0.3237434923648834, validation/num_examples=5348, validation/wer=0.093467
I1021 10:38:28.049997 139658187740928 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.129703402519226, loss=0.972954273223877
I1021 10:44:52.413928 139658196133632 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.122864007949829, loss=0.9990955591201782
I1021 10:51:40.132472 139658187740928 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.141044020652771, loss=1.0236274003982544
I1021 10:58:11.342553 139656347768576 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2024322748184204, loss=1.0026625394821167
I1021 11:02:05.053534 139830899640128 spec.py:321] Evaluating on the training split.
I1021 11:02:59.017629 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 11:03:49.314990 139830899640128 spec.py:349] Evaluating on the test split.
I1021 11:04:15.070771 139830899640128 submission_runner.py:395] Time since start: 47369.48s, 	Step: 55298, 	{'train/ctc_loss': Array(0.12695841, dtype=float32), 'train/wer': 0.045825459543033846, 'validation/ctc_loss': Array(0.31194285, dtype=float32), 'validation/wer': 0.08887108141768925, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17104146, dtype=float32), 'test/wer': 0.05579590924786221, 'test/num_examples': 2472, 'score': 43284.44806480408, 'total_duration': 47369.47530460358, 'accumulated_submission_time': 43284.44806480408, 'accumulated_eval_time': 4080.6883358955383, 'accumulated_logging_time': 1.9894473552703857}
I1021 11:04:15.107919 139658196133632 logging_writer.py:48] [55298] accumulated_eval_time=4080.688336, accumulated_logging_time=1.989447, accumulated_submission_time=43284.448065, global_step=55298, preemption_count=0, score=43284.448065, test/ctc_loss=0.17104145884513855, test/num_examples=2472, test/wer=0.055796, total_duration=47369.475305, train/ctc_loss=0.12695841491222382, train/wer=0.045825, validation/ctc_loss=0.31194284558296204, validation/num_examples=5348, validation/wer=0.088871
I1021 11:06:49.775538 139658187740928 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.2101012468338013, loss=1.0165454149246216
I1021 11:13:14.304158 139658196133632 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.2002421617507935, loss=0.9823418855667114
I1021 11:19:58.548711 139658187740928 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.29267156124115, loss=0.9526285529136658
I1021 11:26:33.540833 139658196133632 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.2199561595916748, loss=0.994080662727356
I1021 11:28:15.794580 139830899640128 spec.py:321] Evaluating on the training split.
I1021 11:29:09.031419 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 11:29:59.930312 139830899640128 spec.py:349] Evaluating on the test split.
I1021 11:30:25.584129 139830899640128 submission_runner.py:395] Time since start: 48939.99s, 	Step: 57135, 	{'train/ctc_loss': Array(0.12656689, dtype=float32), 'train/wer': 0.04617646101606657, 'validation/ctc_loss': Array(0.30616346, dtype=float32), 'validation/wer': 0.08708497060158144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16804637, dtype=float32), 'test/wer': 0.0543944102532854, 'test/num_examples': 2472, 'score': 44724.971220731735, 'total_duration': 48939.98755145073, 'accumulated_submission_time': 44724.971220731735, 'accumulated_eval_time': 4210.471300125122, 'accumulated_logging_time': 2.116922378540039}
I1021 11:30:25.623299 139658196133632 logging_writer.py:48] [57135] accumulated_eval_time=4210.471300, accumulated_logging_time=2.116922, accumulated_submission_time=44724.971221, global_step=57135, preemption_count=0, score=44724.971221, test/ctc_loss=0.16804637014865875, test/num_examples=2472, test/wer=0.054394, total_duration=48939.987551, train/ctc_loss=0.12656688690185547, train/wer=0.046176, validation/ctc_loss=0.3061634600162506, validation/num_examples=5348, validation/wer=0.087085
I1021 11:35:04.577856 139658187740928 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.121024489402771, loss=1.0077849626541138
I1021 11:41:35.829764 139658196133632 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.1395293474197388, loss=1.041028618812561
I1021 11:48:05.776885 139658187740928 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.1289355754852295, loss=1.0102267265319824
I1021 11:54:25.907485 139830899640128 spec.py:321] Evaluating on the training split.
I1021 11:55:19.040006 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 11:56:09.693308 139830899640128 spec.py:349] Evaluating on the test split.
I1021 11:56:35.718664 139830899640128 submission_runner.py:395] Time since start: 50510.12s, 	Step: 58977, 	{'train/ctc_loss': Array(0.12126162, dtype=float32), 'train/wer': 0.04361628709454796, 'validation/ctc_loss': Array(0.3061722, dtype=float32), 'validation/wer': 0.08662154725469941, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16704951, dtype=float32), 'test/wer': 0.05360225864765503, 'test/num_examples': 2472, 'score': 46165.09203457832, 'total_duration': 50510.12358522415, 'accumulated_submission_time': 46165.09203457832, 'accumulated_eval_time': 4340.277631282806, 'accumulated_logging_time': 2.2467944622039795}
I1021 11:56:35.757689 139657612453632 logging_writer.py:48] [58977] accumulated_eval_time=4340.277631, accumulated_logging_time=2.246794, accumulated_submission_time=46165.092035, global_step=58977, preemption_count=0, score=46165.092035, test/ctc_loss=0.16704951226711273, test/num_examples=2472, test/wer=0.053602, total_duration=50510.123585, train/ctc_loss=0.12126161903142929, train/wer=0.043616, validation/ctc_loss=0.3061721920967102, validation/num_examples=5348, validation/wer=0.086622
I1021 11:56:54.105199 139657604060928 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.2666057348251343, loss=0.943365752696991
I1021 12:03:21.431787 139657612453632 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.1358535289764404, loss=1.0182645320892334
I1021 12:10:03.194933 139830899640128 spec.py:321] Evaluating on the training split.
I1021 12:10:54.037757 139830899640128 spec.py:333] Evaluating on the validation split.
I1021 12:11:44.792263 139830899640128 spec.py:349] Evaluating on the test split.
I1021 12:12:10.682194 139830899640128 submission_runner.py:395] Time since start: 51445.09s, 	Step: 60000, 	{'train/ctc_loss': Array(0.12971856, dtype=float32), 'train/wer': 0.04682188111671989, 'validation/ctc_loss': Array(0.30558762, dtype=float32), 'validation/wer': 0.08660223794857931, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16683209, dtype=float32), 'test/wer': 0.053642881806918126, 'test/num_examples': 2472, 'score': 46972.39722585678, 'total_duration': 51445.089549064636, 'accumulated_submission_time': 46972.39722585678, 'accumulated_eval_time': 4467.762434720993, 'accumulated_logging_time': 2.376314163208008}
I1021 12:12:10.716526 139657766053632 logging_writer.py:48] [60000] accumulated_eval_time=4467.762435, accumulated_logging_time=2.376314, accumulated_submission_time=46972.397226, global_step=60000, preemption_count=0, score=46972.397226, test/ctc_loss=0.1668320894241333, test/num_examples=2472, test/wer=0.053643, total_duration=51445.089549, train/ctc_loss=0.12971855700016022, train/wer=0.046822, validation/ctc_loss=0.30558761954307556, validation/num_examples=5348, validation/wer=0.086602
I1021 12:12:10.740646 139657757660928 logging_writer.py:48] [60000] global_step=60000, preemption_count=0, score=46972.397226
I1021 12:12:11.212872 139830899640128 checkpoints.py:490] Saving checkpoint at step: 60000
I1021 12:12:12.665489 139830899640128 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_conformer/adamw_run9/librispeech_conformer_jax/trial_1/checkpoint_60000
I1021 12:12:12.699721 139830899640128 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_conformer/adamw_run9/librispeech_conformer_jax/trial_1/checkpoint_60000.
I1021 12:12:14.115318 139830899640128 submission_runner.py:565] Tuning trial 1/1
I1021 12:12:14.115633 139830899640128 submission_runner.py:566] Hyperparameters: Hyperparameters(learning_rate=0.002106913873888147, beta1=0.8231189937738506, beta2=0.8774571227688758, warmup_steps=1199, weight_decay=0.27590534177690645)
I1021 12:12:14.134541 139830899640128 submission_runner.py:567] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.450764, dtype=float32), 'train/wer': 1.0936166003789851, 'validation/ctc_loss': Array(29.804022, dtype=float32), 'validation/wer': 1.23516803923651, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.961452, dtype=float32), 'test/wer': 1.2324457173034347, 'test/num_examples': 2472, 'score': 76.87082004547119, 'total_duration': 252.51129627227783, 'accumulated_submission_time': 76.87082004547119, 'accumulated_eval_time': 175.64041686058044, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1836, {'train/ctc_loss': Array(2.1721225, dtype=float32), 'train/wer': 0.5044231838685094, 'validation/ctc_loss': Array(2.673906, dtype=float32), 'validation/wer': 0.5500738580959094, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.2805436, dtype=float32), 'test/wer': 0.5014319663640241, 'test/num_examples': 2472, 'score': 1516.8055872917175, 'total_duration': 1818.633487701416, 'accumulated_submission_time': 1516.8055872917175, 'accumulated_eval_time': 301.71912598609924, 'accumulated_logging_time': 0.03926873207092285, 'global_step': 1836, 'preemption_count': 0}), (3715, {'train/ctc_loss': Array(0.6602704, dtype=float32), 'train/wer': 0.22555469728045174, 'validation/ctc_loss': Array(0.98302835, dtype=float32), 'validation/wer': 0.28588393176091215, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.69698733, dtype=float32), 'test/wer': 0.22679909816586435, 'test/num_examples': 2472, 'score': 2957.3760089874268, 'total_duration': 3386.6866416931152, 'accumulated_submission_time': 2957.3760089874268, 'accumulated_eval_time': 429.0769395828247, 'accumulated_logging_time': 0.08616781234741211, 'global_step': 3715, 'preemption_count': 0}), (5597, {'train/ctc_loss': Array(0.50195926, dtype=float32), 'train/wer': 0.1773539752571016, 'validation/ctc_loss': Array(0.83409363, dtype=float32), 'validation/wer': 0.24700464388812188, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.567773, dtype=float32), 'test/wer': 0.18798366948997625, 'test/num_examples': 2472, 'score': 4397.660763263702, 'total_duration': 4956.30383849144, 'accumulated_submission_time': 4397.660763263702, 'accumulated_eval_time': 558.2785234451294, 'accumulated_logging_time': 0.13854622840881348, 'global_step': 5597, 'preemption_count': 0}), (7468, {'train/ctc_loss': Array(0.48403767, dtype=float32), 'train/wer': 0.16476766957450598, 'validation/ctc_loss': Array(0.78869116, dtype=float32), 'validation/wer': 0.23267713874701912, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5266797, dtype=float32), 'test/wer': 0.17195783316068491, 'test/num_examples': 2472, 'score': 5838.247625112534, 'total_duration': 6526.554671525955, 'accumulated_submission_time': 5838.247625112534, 'accumulated_eval_time': 687.8131964206696, 'accumulated_logging_time': 0.1898651123046875, 'global_step': 7468, 'preemption_count': 0}), (9330, {'train/ctc_loss': Array(0.45282903, dtype=float32), 'train/wer': 0.15905977516362607, 'validation/ctc_loss': Array(0.7491323, dtype=float32), 'validation/wer': 0.22434517315620264, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4959548, dtype=float32), 'test/wer': 0.16541750451932646, 'test/num_examples': 2472, 'score': 7278.132206201553, 'total_duration': 8095.679551124573, 'accumulated_submission_time': 7278.132206201553, 'accumulated_eval_time': 816.9247591495514, 'accumulated_logging_time': 0.24134063720703125, 'global_step': 9330, 'preemption_count': 0}), (11174, {'train/ctc_loss': Array(0.47206983, dtype=float32), 'train/wer': 0.16344538039274265, 'validation/ctc_loss': Array(0.7439973, dtype=float32), 'validation/wer': 0.22111086438108846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4895346, dtype=float32), 'test/wer': 0.16261450653017284, 'test/num_examples': 2472, 'score': 8718.283967733383, 'total_duration': 9666.149890422821, 'accumulated_submission_time': 8718.283967733383, 'accumulated_eval_time': 947.117752790451, 'accumulated_logging_time': 0.29043126106262207, 'global_step': 11174, 'preemption_count': 0}), (13024, {'train/ctc_loss': Array(0.4283777, dtype=float32), 'train/wer': 0.14997447514517262, 'validation/ctc_loss': Array(0.73161554, dtype=float32), 'validation/wer': 0.21483533989206097, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4661154, dtype=float32), 'test/wer': 0.1547133020535007, 'test/num_examples': 2472, 'score': 10158.19237780571, 'total_duration': 11236.997790336609, 'accumulated_submission_time': 10158.19237780571, 'accumulated_eval_time': 1077.9242832660675, 'accumulated_logging_time': 0.34496259689331055, 'global_step': 13024, 'preemption_count': 0}), (14862, {'train/ctc_loss': Array(0.38148648, dtype=float32), 'train/wer': 0.13793390363764008, 'validation/ctc_loss': Array(0.6888415, dtype=float32), 'validation/wer': 0.20587582185234174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4464068, dtype=float32), 'test/wer': 0.14770580708061665, 'test/num_examples': 2472, 'score': 11598.581330299377, 'total_duration': 12808.775510311127, 'accumulated_submission_time': 11598.581330299377, 'accumulated_eval_time': 1209.1856474876404, 'accumulated_logging_time': 0.3936469554901123, 'global_step': 14862, 'preemption_count': 0}), (16691, {'train/ctc_loss': Array(0.35427904, dtype=float32), 'train/wer': 0.12452585266719512, 'validation/ctc_loss': Array(0.65696126, dtype=float32), 'validation/wer': 0.19508191973121447, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42384827, dtype=float32), 'test/wer': 0.14033270367436476, 'test/num_examples': 2472, 'score': 13038.67775440216, 'total_duration': 14378.763293743134, 'accumulated_submission_time': 13038.67775440216, 'accumulated_eval_time': 1338.9225084781647, 'accumulated_logging_time': 0.47103238105773926, 'global_step': 16691, 'preemption_count': 0}), (18519, {'train/ctc_loss': Array(0.3512982, dtype=float32), 'train/wer': 0.1288002702891128, 'validation/ctc_loss': Array(0.6411016, dtype=float32), 'validation/wer': 0.19166417254795948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41200873, dtype=float32), 'test/wer': 0.1364531919647391, 'test/num_examples': 2472, 'score': 14479.509255170822, 'total_duration': 15949.22879743576, 'accumulated_submission_time': 14479.509255170822, 'accumulated_eval_time': 1468.4320187568665, 'accumulated_logging_time': 0.5177080631256104, 'global_step': 18519, 'preemption_count': 0}), (20346, {'train/ctc_loss': Array(0.35971203, dtype=float32), 'train/wer': 0.12398529839498665, 'validation/ctc_loss': Array(0.6287478, dtype=float32), 'validation/wer': 0.1873002693648204, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39301708, dtype=float32), 'test/wer': 0.1320862023439563, 'test/num_examples': 2472, 'score': 15919.383308410645, 'total_duration': 17520.042124271393, 'accumulated_submission_time': 15919.383308410645, 'accumulated_eval_time': 1599.24236702919, 'accumulated_logging_time': 0.5689959526062012, 'global_step': 20346, 'preemption_count': 0}), (22200, {'train/ctc_loss': Array(0.36246973, dtype=float32), 'train/wer': 0.12645685493444483, 'validation/ctc_loss': Array(0.61231947, dtype=float32), 'validation/wer': 0.18367977446730452, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38885424, dtype=float32), 'test/wer': 0.1295066317307497, 'test/num_examples': 2472, 'score': 17359.603481531143, 'total_duration': 19091.81662297249, 'accumulated_submission_time': 17359.603481531143, 'accumulated_eval_time': 1730.6623520851135, 'accumulated_logging_time': 0.6245684623718262, 'global_step': 22200, 'preemption_count': 0}), (24054, {'train/ctc_loss': Array(0.33017087, dtype=float32), 'train/wer': 0.11635456322666941, 'validation/ctc_loss': Array(0.59012485, dtype=float32), 'validation/wer': 0.1760525985498711, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3647378, dtype=float32), 'test/wer': 0.12296630308939126, 'test/num_examples': 2472, 'score': 18799.894825220108, 'total_duration': 20663.61720943451, 'accumulated_submission_time': 18799.894825220108, 'accumulated_eval_time': 1862.040631055832, 'accumulated_logging_time': 0.6753864288330078, 'global_step': 24054, 'preemption_count': 0}), (25899, {'train/ctc_loss': Array(0.30398074, dtype=float32), 'train/wer': 0.10732232591529074, 'validation/ctc_loss': Array(0.5778364, dtype=float32), 'validation/wer': 0.1718238605095726, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.363951, dtype=float32), 'test/wer': 0.12034610931692158, 'test/num_examples': 2472, 'score': 20239.790155649185, 'total_duration': 22233.708899497986, 'accumulated_submission_time': 20239.790155649185, 'accumulated_eval_time': 1992.1069824695587, 'accumulated_logging_time': 0.7268610000610352, 'global_step': 25899, 'preemption_count': 0}), (27741, {'train/ctc_loss': Array(0.29009196, dtype=float32), 'train/wer': 0.10419155429197423, 'validation/ctc_loss': Array(0.56793153, dtype=float32), 'validation/wer': 0.16989292989756413, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3519648, dtype=float32), 'test/wer': 0.11636503970913818, 'test/num_examples': 2472, 'score': 21679.679236888885, 'total_duration': 23805.436156749725, 'accumulated_submission_time': 21679.679236888885, 'accumulated_eval_time': 2123.8160145282745, 'accumulated_logging_time': 0.7764804363250732, 'global_step': 27741, 'preemption_count': 0}), (29588, {'train/ctc_loss': Array(0.2731936, dtype=float32), 'train/wer': 0.09538615744955048, 'validation/ctc_loss': Array(0.5399274, dtype=float32), 'validation/wer': 0.16005483842938104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3304064, dtype=float32), 'test/wer': 0.10964190685109582, 'test/num_examples': 2472, 'score': 23120.109338998795, 'total_duration': 25378.206876039505, 'accumulated_submission_time': 23120.109338998795, 'accumulated_eval_time': 2256.0231161117554, 'accumulated_logging_time': 0.8302571773529053, 'global_step': 29588, 'preemption_count': 0}), (31434, {'train/ctc_loss': Array(0.27667153, dtype=float32), 'train/wer': 0.09658844687120818, 'validation/ctc_loss': Array(0.5234666, dtype=float32), 'validation/wer': 0.15526613051160007, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32105663, dtype=float32), 'test/wer': 0.10501086669510287, 'test/num_examples': 2472, 'score': 24560.38601064682, 'total_duration': 26949.607805252075, 'accumulated_submission_time': 24560.38601064682, 'accumulated_eval_time': 2387.0195891857147, 'accumulated_logging_time': 0.8789505958557129, 'global_step': 31434, 'preemption_count': 0}), (33280, {'train/ctc_loss': Array(0.25150132, dtype=float32), 'train/wer': 0.08752082214227966, 'validation/ctc_loss': Array(0.49624133, dtype=float32), 'validation/wer': 0.14733966034930535, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30043656, dtype=float32), 'test/wer': 0.10007515284463672, 'test/num_examples': 2472, 'score': 26000.67474770546, 'total_duration': 28520.286294460297, 'accumulated_submission_time': 26000.67474770546, 'accumulated_eval_time': 2517.2587049007416, 'accumulated_logging_time': 0.9479522705078125, 'global_step': 33280, 'preemption_count': 0}), (35096, {'train/ctc_loss': Array(0.252029, dtype=float32), 'train/wer': 0.08698868335482281, 'validation/ctc_loss': Array(0.48229554, dtype=float32), 'validation/wer': 0.14255095243152438, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2871261, dtype=float32), 'test/wer': 0.09477383056080271, 'test/num_examples': 2472, 'score': 27441.245020389557, 'total_duration': 30093.015188694, 'accumulated_submission_time': 27441.245020389557, 'accumulated_eval_time': 2649.285391807556, 'accumulated_logging_time': 1.0013396739959717, 'global_step': 35096, 'preemption_count': 0}), (36952, {'train/ctc_loss': Array(0.23590183, dtype=float32), 'train/wer': 0.08117800014919488, 'validation/ctc_loss': Array(0.4629277, dtype=float32), 'validation/wer': 0.13676781524855905, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27535346, dtype=float32), 'test/wer': 0.09278329575691102, 'test/num_examples': 2472, 'score': 28881.935079813004, 'total_duration': 31664.54434323311, 'accumulated_submission_time': 28881.935079813004, 'accumulated_eval_time': 2780.001361846924, 'accumulated_logging_time': 1.047724723815918, 'global_step': 36952, 'preemption_count': 0}), (38798, {'train/ctc_loss': Array(0.17703904, dtype=float32), 'train/wer': 0.06486577903508502, 'validation/ctc_loss': Array(0.44492716, dtype=float32), 'validation/wer': 0.13220116435115903, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25913087, dtype=float32), 'test/wer': 0.08695387240265676, 'test/num_examples': 2472, 'score': 30322.219118833542, 'total_duration': 33238.01707649231, 'accumulated_submission_time': 30322.219118833542, 'accumulated_eval_time': 2913.0517041683197, 'accumulated_logging_time': 1.1081364154815674, 'global_step': 38798, 'preemption_count': 0}), (40643, {'train/ctc_loss': Array(0.19533722, dtype=float32), 'train/wer': 0.06910624907604908, 'validation/ctc_loss': Array(0.43243784, dtype=float32), 'validation/wer': 0.1275572762292787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25135544, dtype=float32), 'test/wer': 0.08329778806897813, 'test/num_examples': 2472, 'score': 31762.563623905182, 'total_duration': 34810.29227042198, 'accumulated_submission_time': 31762.563623905182, 'accumulated_eval_time': 3044.8511548042297, 'accumulated_logging_time': 1.1602110862731934, 'global_step': 40643, 'preemption_count': 0}), (42497, {'train/ctc_loss': Array(0.22531338, dtype=float32), 'train/wer': 0.07920786708829863, 'validation/ctc_loss': Array(0.41273975, dtype=float32), 'validation/wer': 0.12290373345433832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23794486, dtype=float32), 'test/wer': 0.07850425527593281, 'test/num_examples': 2472, 'score': 33202.88981986046, 'total_duration': 36380.35505485535, 'accumulated_submission_time': 33202.88981986046, 'accumulated_eval_time': 3174.4584741592407, 'accumulated_logging_time': 1.2100152969360352, 'global_step': 42497, 'preemption_count': 0}), (44314, {'train/ctc_loss': Array(0.22217162, dtype=float32), 'train/wer': 0.07790181917096432, 'validation/ctc_loss': Array(0.39360687, dtype=float32), 'validation/wer': 0.11560481574094635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22694327, dtype=float32), 'test/wer': 0.07344667194767737, 'test/num_examples': 2472, 'score': 34642.93722462654, 'total_duration': 37949.66504859924, 'accumulated_submission_time': 34642.93722462654, 'accumulated_eval_time': 3303.5820989608765, 'accumulated_logging_time': 1.268972396850586, 'global_step': 44314, 'preemption_count': 0}), (46147, {'train/ctc_loss': Array(0.23933567, dtype=float32), 'train/wer': 0.08507701037139297, 'validation/ctc_loss': Array(0.3777713, dtype=float32), 'validation/wer': 0.11108643810884655, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21491286, dtype=float32), 'test/wer': 0.07017650762699815, 'test/num_examples': 2472, 'score': 36083.43827319145, 'total_duration': 39519.37506532669, 'accumulated_submission_time': 36083.43827319145, 'accumulated_eval_time': 3432.6572666168213, 'accumulated_logging_time': 1.3231289386749268, 'global_step': 46147, 'preemption_count': 0}), (47984, {'train/ctc_loss': Array(0.19812894, dtype=float32), 'train/wer': 0.06833717007459567, 'validation/ctc_loss': Array(0.36412218, dtype=float32), 'validation/wer': 0.1058439614972436, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20263518, dtype=float32), 'test/wer': 0.06688603172668739, 'test/num_examples': 2472, 'score': 37523.990773916245, 'total_duration': 41087.96130132675, 'accumulated_submission_time': 37523.990773916245, 'accumulated_eval_time': 3560.484179496765, 'accumulated_logging_time': 1.450742244720459, 'global_step': 47984, 'preemption_count': 0}), (49830, {'train/ctc_loss': Array(0.17208968, dtype=float32), 'train/wer': 0.062040016478683485, 'validation/ctc_loss': Array(0.3454565, dtype=float32), 'validation/wer': 0.09973256611023683, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19459349, dtype=float32), 'test/wer': 0.06320963581337721, 'test/num_examples': 2472, 'score': 38964.00204706192, 'total_duration': 42657.21874332428, 'accumulated_submission_time': 38964.00204706192, 'accumulated_eval_time': 3689.5011858940125, 'accumulated_logging_time': 1.6001155376434326, 'global_step': 49830, 'preemption_count': 0}), (51650, {'train/ctc_loss': Array(0.13969707, dtype=float32), 'train/wer': 0.05028874702280612, 'validation/ctc_loss': Array(0.33363423, dtype=float32), 'validation/wer': 0.09593828745764021, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18793625, dtype=float32), 'test/wer': 0.06048788414274978, 'test/num_examples': 2472, 'score': 40404.10059261322, 'total_duration': 44228.066487789154, 'accumulated_submission_time': 40404.10059261322, 'accumulated_eval_time': 3820.0410339832306, 'accumulated_logging_time': 1.7318615913391113, 'global_step': 51650, 'preemption_count': 0}), (53470, {'train/ctc_loss': Array(0.14543146, dtype=float32), 'train/wer': 0.0531480988665432, 'validation/ctc_loss': Array(0.3237435, dtype=float32), 'validation/wer': 0.09346669627426939, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17842638, dtype=float32), 'test/wer': 0.05792862510917474, 'test/num_examples': 2472, 'score': 41844.03465628624, 'total_duration': 45798.84410381317, 'accumulated_submission_time': 41844.03465628624, 'accumulated_eval_time': 3950.6765501499176, 'accumulated_logging_time': 1.861433982849121, 'global_step': 53470, 'preemption_count': 0}), (55298, {'train/ctc_loss': Array(0.12695841, dtype=float32), 'train/wer': 0.045825459543033846, 'validation/ctc_loss': Array(0.31194285, dtype=float32), 'validation/wer': 0.08887108141768925, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17104146, dtype=float32), 'test/wer': 0.05579590924786221, 'test/num_examples': 2472, 'score': 43284.44806480408, 'total_duration': 47369.47530460358, 'accumulated_submission_time': 43284.44806480408, 'accumulated_eval_time': 4080.6883358955383, 'accumulated_logging_time': 1.9894473552703857, 'global_step': 55298, 'preemption_count': 0}), (57135, {'train/ctc_loss': Array(0.12656689, dtype=float32), 'train/wer': 0.04617646101606657, 'validation/ctc_loss': Array(0.30616346, dtype=float32), 'validation/wer': 0.08708497060158144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16804637, dtype=float32), 'test/wer': 0.0543944102532854, 'test/num_examples': 2472, 'score': 44724.971220731735, 'total_duration': 48939.98755145073, 'accumulated_submission_time': 44724.971220731735, 'accumulated_eval_time': 4210.471300125122, 'accumulated_logging_time': 2.116922378540039, 'global_step': 57135, 'preemption_count': 0}), (58977, {'train/ctc_loss': Array(0.12126162, dtype=float32), 'train/wer': 0.04361628709454796, 'validation/ctc_loss': Array(0.3061722, dtype=float32), 'validation/wer': 0.08662154725469941, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16704951, dtype=float32), 'test/wer': 0.05360225864765503, 'test/num_examples': 2472, 'score': 46165.09203457832, 'total_duration': 50510.12358522415, 'accumulated_submission_time': 46165.09203457832, 'accumulated_eval_time': 4340.277631282806, 'accumulated_logging_time': 2.2467944622039795, 'global_step': 58977, 'preemption_count': 0}), (60000, {'train/ctc_loss': Array(0.12971856, dtype=float32), 'train/wer': 0.04682188111671989, 'validation/ctc_loss': Array(0.30558762, dtype=float32), 'validation/wer': 0.08660223794857931, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16683209, dtype=float32), 'test/wer': 0.053642881806918126, 'test/num_examples': 2472, 'score': 46972.39722585678, 'total_duration': 51445.089549064636, 'accumulated_submission_time': 46972.39722585678, 'accumulated_eval_time': 4467.762434720993, 'accumulated_logging_time': 2.376314163208008, 'global_step': 60000, 'preemption_count': 0})], 'global_step': 60000}
I1021 12:12:14.134785 139830899640128 submission_runner.py:568] Timing: 46972.39722585678
I1021 12:12:14.134904 139830899640128 submission_runner.py:570] Total number of evals: 34
I1021 12:12:14.134996 139830899640128 submission_runner.py:571] ====================
I1021 12:12:14.139152 139830899640128 submission_runner.py:647] Final librispeech_conformer score: 46972.39722585678
