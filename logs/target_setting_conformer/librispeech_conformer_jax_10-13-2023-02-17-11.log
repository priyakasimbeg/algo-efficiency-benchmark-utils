python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_conformer/adamw_run12 --overwrite=true --save_checkpoints=false --max_global_steps=60000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_10-13-2023-02-17-11.log
2023-10-13 02:17:16.515493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1013 02:17:35.689544 140673456789312 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_conformer/adamw_run12/librispeech_conformer_jax.
I1013 02:17:36.628971 140673456789312 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I1013 02:17:36.629647 140673456789312 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1013 02:17:36.629795 140673456789312 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1013 02:17:36.635937 140673456789312 submission_runner.py:523] Using RNG seed 1146653876
I1013 02:17:42.758644 140673456789312 submission_runner.py:532] --- Tuning run 1/1 ---
I1013 02:17:42.758867 140673456789312 submission_runner.py:537] Creating tuning directory at /experiment_runs/targets_check_conformer/adamw_run12/librispeech_conformer_jax/trial_1.
I1013 02:17:42.759200 140673456789312 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_conformer/adamw_run12/librispeech_conformer_jax/trial_1/hparams.json.
I1013 02:17:42.942722 140673456789312 submission_runner.py:200] Initializing dataset.
I1013 02:17:42.942925 140673456789312 submission_runner.py:207] Initializing model.
I1013 02:17:47.672294 140673456789312 submission_runner.py:241] Initializing optimizer.
I1013 02:17:48.903367 140673456789312 submission_runner.py:248] Initializing metrics bundle.
I1013 02:17:48.903568 140673456789312 submission_runner.py:266] Initializing checkpoint and logger.
I1013 02:17:48.904695 140673456789312 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_conformer/adamw_run12/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I1013 02:17:48.904829 140673456789312 submission_runner.py:286] Saving meta data to /experiment_runs/targets_check_conformer/adamw_run12/librispeech_conformer_jax/trial_1/meta_data_0.json.
I1013 02:17:48.905005 140673456789312 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1013 02:17:48.905063 140673456789312 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1013 02:17:49.735482 140673456789312 submission_runner.py:289] Saving flags to /experiment_runs/targets_check_conformer/adamw_run12/librispeech_conformer_jax/trial_1/flags_0.json.
I1013 02:17:49.749461 140673456789312 submission_runner.py:299] Starting training loop.
I1013 02:17:50.043477 140673456789312 input_pipeline.py:20] Loading split = train-clean-100
I1013 02:17:50.086771 140673456789312 input_pipeline.py:20] Loading split = train-clean-360
I1013 02:17:50.486718 140673456789312 input_pipeline.py:20] Loading split = train-other-500
2023-10-13 02:19:01.423277: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-13 02:19:04.269914: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1013 02:19:06.308236 140497800652544 logging_writer.py:48] [0] global_step=0, grad_norm=41.40178298950195, loss=32.313846588134766
I1013 02:19:06.346370 140673456789312 spec.py:321] Evaluating on the training split.
I1013 02:19:06.514428 140673456789312 input_pipeline.py:20] Loading split = train-clean-100
I1013 02:19:06.549881 140673456789312 input_pipeline.py:20] Loading split = train-clean-360
I1013 02:19:06.947338 140673456789312 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1013 02:20:19.223456 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 02:20:19.337945 140673456789312 input_pipeline.py:20] Loading split = dev-clean
I1013 02:20:19.342975 140673456789312 input_pipeline.py:20] Loading split = dev-other
I1013 02:21:20.450541 140673456789312 spec.py:349] Evaluating on the test split.
I1013 02:21:20.565864 140673456789312 input_pipeline.py:20] Loading split = test-clean
I1013 02:21:56.939800 140673456789312 submission_runner.py:393] Time since start: 247.19s, 	Step: 1, 	{'train/ctc_loss': Array(31.26119, dtype=float32), 'train/wer': 1.1754107406867542, 'validation/ctc_loss': Array(30.911684, dtype=float32), 'validation/wer': 1.2526043426629465, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.850784, dtype=float32), 'test/wer': 1.2688034448439056, 'test/num_examples': 2472, 'score': 76.59682488441467, 'total_duration': 247.18804740905762, 'accumulated_submission_time': 76.59682488441467, 'accumulated_eval_time': 170.59115362167358, 'accumulated_logging_time': 0}
I1013 02:21:56.967645 140491962181376 logging_writer.py:48] [1] accumulated_eval_time=170.591154, accumulated_logging_time=0, accumulated_submission_time=76.596825, global_step=1, preemption_count=0, score=76.596825, test/ctc_loss=30.850784301757812, test/num_examples=2472, test/wer=1.268803, total_duration=247.188047, train/ctc_loss=31.26119041442871, train/wer=1.175411, validation/ctc_loss=30.911684036254883, validation/num_examples=5348, validation/wer=1.252604
I1013 02:22:18.734202 140499370059520 logging_writer.py:48] [1] global_step=1, grad_norm=42.02229690551758, loss=32.31010437011719
I1013 02:22:19.612928 140499378452224 logging_writer.py:48] [2] global_step=2, grad_norm=44.766510009765625, loss=31.457828521728516
I1013 02:22:20.419892 140499370059520 logging_writer.py:48] [3] global_step=3, grad_norm=42.27846908569336, loss=31.607458114624023
I1013 02:22:21.231538 140499378452224 logging_writer.py:48] [4] global_step=4, grad_norm=50.76101303100586, loss=31.11288070678711
I1013 02:22:22.105278 140499370059520 logging_writer.py:48] [5] global_step=5, grad_norm=47.48152542114258, loss=30.6267147064209
I1013 02:22:22.997527 140499378452224 logging_writer.py:48] [6] global_step=6, grad_norm=42.534690856933594, loss=29.598995208740234
I1013 02:22:23.867084 140499370059520 logging_writer.py:48] [7] global_step=7, grad_norm=42.50325393676758, loss=29.386106491088867
I1013 02:22:24.739190 140499378452224 logging_writer.py:48] [8] global_step=8, grad_norm=39.56875228881836, loss=27.763080596923828
I1013 02:22:25.614188 140499370059520 logging_writer.py:48] [9] global_step=9, grad_norm=40.167564392089844, loss=27.39317512512207
I1013 02:22:26.494272 140499378452224 logging_writer.py:48] [10] global_step=10, grad_norm=45.30274200439453, loss=27.11333465576172
I1013 02:22:27.372394 140499370059520 logging_writer.py:48] [11] global_step=11, grad_norm=40.2135009765625, loss=24.85491371154785
I1013 02:22:28.245531 140499378452224 logging_writer.py:48] [12] global_step=12, grad_norm=35.50452423095703, loss=25.235877990722656
I1013 02:22:29.135109 140499370059520 logging_writer.py:48] [13] global_step=13, grad_norm=38.18304443359375, loss=24.79793930053711
I1013 02:22:30.013556 140499378452224 logging_writer.py:48] [14] global_step=14, grad_norm=29.935678482055664, loss=23.765666961669922
I1013 02:22:30.887823 140499370059520 logging_writer.py:48] [15] global_step=15, grad_norm=32.08018112182617, loss=22.427562713623047
I1013 02:22:31.760961 140499378452224 logging_writer.py:48] [16] global_step=16, grad_norm=29.18520736694336, loss=21.556306838989258
I1013 02:22:32.640266 140499370059520 logging_writer.py:48] [17] global_step=17, grad_norm=35.23054122924805, loss=21.156349182128906
I1013 02:22:33.527443 140499378452224 logging_writer.py:48] [18] global_step=18, grad_norm=52.17269515991211, loss=19.24856948852539
I1013 02:22:34.406572 140499370059520 logging_writer.py:48] [19] global_step=19, grad_norm=52.60915756225586, loss=18.578386306762695
I1013 02:22:35.288726 140499378452224 logging_writer.py:48] [20] global_step=20, grad_norm=31.371719360351562, loss=17.700843811035156
I1013 02:22:36.166322 140499370059520 logging_writer.py:48] [21] global_step=21, grad_norm=34.27324676513672, loss=16.025390625
I1013 02:22:37.046212 140499378452224 logging_writer.py:48] [22] global_step=22, grad_norm=41.59488296508789, loss=14.994205474853516
I1013 02:22:37.919555 140499370059520 logging_writer.py:48] [23] global_step=23, grad_norm=23.302093505859375, loss=12.853856086730957
I1013 02:22:38.789808 140499378452224 logging_writer.py:48] [24] global_step=24, grad_norm=24.06871795654297, loss=12.870381355285645
I1013 02:22:39.670670 140499370059520 logging_writer.py:48] [25] global_step=25, grad_norm=83.50885772705078, loss=11.65304183959961
I1013 02:22:40.550075 140499378452224 logging_writer.py:48] [26] global_step=26, grad_norm=60.04518127441406, loss=8.63079833984375
I1013 02:22:41.433108 140499370059520 logging_writer.py:48] [27] global_step=27, grad_norm=8.596192359924316, loss=7.449828147888184
I1013 02:22:42.300427 140499378452224 logging_writer.py:48] [28] global_step=28, grad_norm=18.1157169342041, loss=7.6772332191467285
I1013 02:22:43.184058 140499370059520 logging_writer.py:48] [29] global_step=29, grad_norm=21.969158172607422, loss=8.027304649353027
I1013 02:22:44.069171 140499378452224 logging_writer.py:48] [30] global_step=30, grad_norm=22.735368728637695, loss=8.115500450134277
I1013 02:22:44.941280 140499370059520 logging_writer.py:48] [31] global_step=31, grad_norm=22.382612228393555, loss=8.006331443786621
I1013 02:22:45.820690 140499378452224 logging_writer.py:48] [32] global_step=32, grad_norm=20.244638442993164, loss=7.651935577392578
I1013 02:22:46.709157 140499370059520 logging_writer.py:48] [33] global_step=33, grad_norm=15.026824951171875, loss=7.237816333770752
I1013 02:22:47.593371 140499378452224 logging_writer.py:48] [34] global_step=34, grad_norm=3.0714879035949707, loss=6.975523471832275
I1013 02:22:48.474955 140499370059520 logging_writer.py:48] [35] global_step=35, grad_norm=17.78605842590332, loss=7.148240089416504
I1013 02:22:49.352001 140499378452224 logging_writer.py:48] [36] global_step=36, grad_norm=26.814050674438477, loss=7.300283432006836
I1013 02:22:50.234879 140499370059520 logging_writer.py:48] [37] global_step=37, grad_norm=18.52500343322754, loss=7.072397232055664
I1013 02:22:51.113298 140499378452224 logging_writer.py:48] [38] global_step=38, grad_norm=5.291647434234619, loss=6.85448694229126
I1013 02:22:51.992218 140499370059520 logging_writer.py:48] [39] global_step=39, grad_norm=5.381949424743652, loss=6.82460355758667
I1013 02:22:52.868824 140499378452224 logging_writer.py:48] [40] global_step=40, grad_norm=10.16444206237793, loss=6.866337776184082
I1013 02:22:53.750742 140499370059520 logging_writer.py:48] [41] global_step=41, grad_norm=10.938482284545898, loss=6.852321624755859
I1013 02:22:54.625812 140499378452224 logging_writer.py:48] [42] global_step=42, grad_norm=9.466038703918457, loss=6.750646591186523
I1013 02:22:55.501447 140499370059520 logging_writer.py:48] [43] global_step=43, grad_norm=5.802647590637207, loss=6.641071319580078
I1013 02:22:56.381181 140499378452224 logging_writer.py:48] [44] global_step=44, grad_norm=1.9745591878890991, loss=6.573012828826904
I1013 02:22:57.264811 140499370059520 logging_writer.py:48] [45] global_step=45, grad_norm=8.688907623291016, loss=6.590483665466309
I1013 02:22:58.139503 140499378452224 logging_writer.py:48] [46] global_step=46, grad_norm=9.794042587280273, loss=6.5590925216674805
I1013 02:22:59.019999 140499370059520 logging_writer.py:48] [47] global_step=47, grad_norm=7.478456974029541, loss=6.485563278198242
I1013 02:22:59.910491 140499378452224 logging_writer.py:48] [48] global_step=48, grad_norm=2.3249099254608154, loss=6.410501003265381
I1013 02:23:00.792494 140499370059520 logging_writer.py:48] [49] global_step=49, grad_norm=3.105792284011841, loss=6.391866683959961
I1013 02:23:01.669294 140499378452224 logging_writer.py:48] [50] global_step=50, grad_norm=5.223236560821533, loss=6.385242462158203
I1013 02:23:02.603356 140499370059520 logging_writer.py:48] [51] global_step=51, grad_norm=4.651615142822266, loss=6.315053462982178
I1013 02:23:03.479874 140499378452224 logging_writer.py:48] [52] global_step=52, grad_norm=1.3004292249679565, loss=6.267529010772705
I1013 02:23:04.357289 140499370059520 logging_writer.py:48] [53] global_step=53, grad_norm=4.37558126449585, loss=6.26411247253418
I1013 02:23:05.236850 140499378452224 logging_writer.py:48] [54] global_step=54, grad_norm=5.514388084411621, loss=6.244194030761719
I1013 02:23:06.123707 140499370059520 logging_writer.py:48] [55] global_step=55, grad_norm=2.26278018951416, loss=6.181306838989258
I1013 02:23:07.003368 140499378452224 logging_writer.py:48] [56] global_step=56, grad_norm=2.2283568382263184, loss=6.158157825469971
I1013 02:23:07.874686 140499370059520 logging_writer.py:48] [57] global_step=57, grad_norm=2.9280450344085693, loss=6.133838653564453
I1013 02:23:08.753011 140499378452224 logging_writer.py:48] [58] global_step=58, grad_norm=1.0367236137390137, loss=6.119275093078613
I1013 02:23:09.629578 140499370059520 logging_writer.py:48] [59] global_step=59, grad_norm=2.3301753997802734, loss=6.086455821990967
I1013 02:23:10.509898 140499378452224 logging_writer.py:48] [60] global_step=60, grad_norm=2.560457944869995, loss=6.079959869384766
I1013 02:23:11.397783 140499370059520 logging_writer.py:48] [61] global_step=61, grad_norm=0.6706086993217468, loss=6.0433030128479
I1013 02:23:12.276690 140499378452224 logging_writer.py:48] [62] global_step=62, grad_norm=1.8277864456176758, loss=6.040234565734863
I1013 02:23:13.155137 140499370059520 logging_writer.py:48] [63] global_step=63, grad_norm=0.7567006349563599, loss=6.016827583312988
I1013 02:23:14.031890 140499378452224 logging_writer.py:48] [64] global_step=64, grad_norm=1.7704468965530396, loss=6.007334232330322
I1013 02:23:14.904803 140499370059520 logging_writer.py:48] [65] global_step=65, grad_norm=0.893929123878479, loss=5.986946105957031
I1013 02:23:15.781434 140499378452224 logging_writer.py:48] [66] global_step=66, grad_norm=1.4653621912002563, loss=5.976729393005371
I1013 02:23:16.650655 140499370059520 logging_writer.py:48] [67] global_step=67, grad_norm=0.5685520768165588, loss=5.942347526550293
I1013 02:23:17.532273 140499378452224 logging_writer.py:48] [68] global_step=68, grad_norm=1.413508415222168, loss=5.963184356689453
I1013 02:23:18.415118 140499370059520 logging_writer.py:48] [69] global_step=69, grad_norm=0.6924107074737549, loss=5.927145481109619
I1013 02:23:19.295049 140499378452224 logging_writer.py:48] [70] global_step=70, grad_norm=1.852013349533081, loss=5.907293796539307
I1013 02:23:20.165002 140499370059520 logging_writer.py:48] [71] global_step=71, grad_norm=3.1577532291412354, loss=5.905645370483398
I1013 02:23:21.041165 140499378452224 logging_writer.py:48] [72] global_step=72, grad_norm=0.804318368434906, loss=5.899625301361084
I1013 02:23:21.912775 140499370059520 logging_writer.py:48] [73] global_step=73, grad_norm=3.6472373008728027, loss=5.890336513519287
I1013 02:23:22.792815 140499378452224 logging_writer.py:48] [74] global_step=74, grad_norm=6.581387996673584, loss=5.895428657531738
I1013 02:23:23.676244 140499370059520 logging_writer.py:48] [75] global_step=75, grad_norm=4.162412643432617, loss=5.893196105957031
I1013 02:23:24.555636 140499378452224 logging_writer.py:48] [76] global_step=76, grad_norm=2.038715124130249, loss=5.841463565826416
I1013 02:23:25.432991 140499370059520 logging_writer.py:48] [77] global_step=77, grad_norm=5.619339942932129, loss=5.867969036102295
I1013 02:23:26.307570 140499378452224 logging_writer.py:48] [78] global_step=78, grad_norm=2.8736181259155273, loss=5.873164176940918
I1013 02:23:27.176659 140499370059520 logging_writer.py:48] [79] global_step=79, grad_norm=3.152463912963867, loss=5.860579967498779
I1013 02:23:28.055014 140499378452224 logging_writer.py:48] [80] global_step=80, grad_norm=6.691169261932373, loss=5.885600566864014
I1013 02:23:28.929930 140499370059520 logging_writer.py:48] [81] global_step=81, grad_norm=4.555557727813721, loss=5.843717575073242
I1013 02:23:29.821872 140499378452224 logging_writer.py:48] [82] global_step=82, grad_norm=1.6495693922042847, loss=5.847328186035156
I1013 02:23:30.714320 140499370059520 logging_writer.py:48] [83] global_step=83, grad_norm=6.0522990226745605, loss=5.866812705993652
I1013 02:23:31.585416 140499378452224 logging_writer.py:48] [84] global_step=84, grad_norm=3.903898239135742, loss=5.849438190460205
I1013 02:23:32.462695 140499370059520 logging_writer.py:48] [85] global_step=85, grad_norm=1.491225004196167, loss=5.836171627044678
I1013 02:23:33.342426 140499378452224 logging_writer.py:48] [86] global_step=86, grad_norm=3.594545364379883, loss=5.859848976135254
I1013 02:23:34.218209 140499370059520 logging_writer.py:48] [87] global_step=87, grad_norm=1.6084277629852295, loss=5.827003002166748
I1013 02:23:35.095321 140499378452224 logging_writer.py:48] [88] global_step=88, grad_norm=1.5249196290969849, loss=5.823178291320801
I1013 02:23:35.984526 140499370059520 logging_writer.py:48] [89] global_step=89, grad_norm=3.812370538711548, loss=5.836938381195068
I1013 02:23:36.863929 140499378452224 logging_writer.py:48] [90] global_step=90, grad_norm=4.029290199279785, loss=5.844444751739502
I1013 02:23:37.746475 140499370059520 logging_writer.py:48] [91] global_step=91, grad_norm=1.5097347497940063, loss=5.842737197875977
I1013 02:23:38.622796 140499378452224 logging_writer.py:48] [92] global_step=92, grad_norm=1.4297044277191162, loss=5.812060356140137
I1013 02:23:39.502306 140499370059520 logging_writer.py:48] [93] global_step=93, grad_norm=1.9727104902267456, loss=5.815707206726074
I1013 02:23:40.380119 140499378452224 logging_writer.py:48] [94] global_step=94, grad_norm=0.8218197822570801, loss=5.821255207061768
I1013 02:23:41.253309 140499370059520 logging_writer.py:48] [95] global_step=95, grad_norm=1.0170375108718872, loss=5.836075782775879
I1013 02:23:42.136446 140499378452224 logging_writer.py:48] [96] global_step=96, grad_norm=2.8899126052856445, loss=5.826525688171387
I1013 02:23:43.027046 140499370059520 logging_writer.py:48] [97] global_step=97, grad_norm=4.883115768432617, loss=5.830665588378906
I1013 02:23:43.909487 140499378452224 logging_writer.py:48] [98] global_step=98, grad_norm=5.282373428344727, loss=5.863128662109375
I1013 02:23:44.779865 140499370059520 logging_writer.py:48] [99] global_step=99, grad_norm=2.1484434604644775, loss=5.827138423919678
I1013 02:23:45.648878 140499378452224 logging_writer.py:48] [100] global_step=100, grad_norm=2.1356122493743896, loss=5.828464031219482
I1013 02:28:48.342963 140499370059520 logging_writer.py:48] [500] global_step=500, grad_norm=2.059166431427002, loss=4.1119842529296875
I1013 02:35:06.544000 140499378452224 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6281174421310425, loss=2.80876088142395
I1013 02:41:27.380522 140500092561152 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8101271986961365, loss=2.333270788192749
I1013 02:45:57.577373 140673456789312 spec.py:321] Evaluating on the training split.
I1013 02:46:47.766471 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 02:47:34.406251 140673456789312 spec.py:349] Evaluating on the test split.
I1013 02:47:57.824800 140673456789312 submission_runner.py:393] Time since start: 1808.07s, 	Step: 1859, 	{'train/ctc_loss': Array(1.823575, dtype=float32), 'train/wer': 0.4623746163827728, 'validation/ctc_loss': Array(2.333378, dtype=float32), 'validation/wer': 0.5039921990403274, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.9621372, dtype=float32), 'test/wer': 0.4548981374281478, 'test/num_examples': 2472, 'score': 1517.1216580867767, 'total_duration': 1808.0695090293884, 'accumulated_submission_time': 1517.1216580867767, 'accumulated_eval_time': 290.83280849456787, 'accumulated_logging_time': 0.04293513298034668}
I1013 02:47:57.859680 140500092561152 logging_writer.py:48] [1859] accumulated_eval_time=290.832808, accumulated_logging_time=0.042935, accumulated_submission_time=1517.121658, global_step=1859, preemption_count=0, score=1517.121658, test/ctc_loss=1.962137222290039, test/num_examples=2472, test/wer=0.454898, total_duration=1808.069509, train/ctc_loss=1.8235750198364258, train/wer=0.462375, validation/ctc_loss=2.3333780765533447, validation/num_examples=5348, validation/wer=0.503992
I1013 02:49:44.908896 140500084168448 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6827175617218018, loss=2.081801176071167
I1013 02:56:05.513844 140500092561152 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6392225027084351, loss=1.9972838163375854
I1013 03:02:23.290429 140500084168448 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.085196852684021, loss=2.0130867958068848
I1013 03:08:43.778568 140500747921152 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5750457048416138, loss=1.8996464014053345
I1013 03:11:58.080355 140673456789312 spec.py:321] Evaluating on the training split.
I1013 03:12:49.339840 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 03:13:37.460956 140673456789312 spec.py:349] Evaluating on the test split.
I1013 03:14:01.816673 140673456789312 submission_runner.py:393] Time since start: 3372.06s, 	Step: 3759, 	{'train/ctc_loss': Array(0.6583183, dtype=float32), 'train/wer': 0.22827158191939909, 'validation/ctc_loss': Array(0.9888188, dtype=float32), 'validation/wer': 0.2907402222501134, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6888168, dtype=float32), 'test/wer': 0.2310035951495948, 'test/num_examples': 2472, 'score': 2957.25461769104, 'total_duration': 3372.0592234134674, 'accumulated_submission_time': 2957.25461769104, 'accumulated_eval_time': 414.56118083000183, 'accumulated_logging_time': 0.09320473670959473}
I1013 03:14:01.866131 140500747921152 logging_writer.py:48] [3759] accumulated_eval_time=414.561181, accumulated_logging_time=0.093205, accumulated_submission_time=2957.254618, global_step=3759, preemption_count=0, score=2957.254618, test/ctc_loss=0.6888167858123779, test/num_examples=2472, test/wer=0.231004, total_duration=3372.059223, train/ctc_loss=0.658318281173706, train/wer=0.228272, validation/ctc_loss=0.9888188242912292, validation/num_examples=5348, validation/wer=0.290740
I1013 03:17:04.084949 140500739528448 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6151741147041321, loss=1.841366171836853
I1013 03:23:24.301191 140500747921152 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6519114375114441, loss=1.8409147262573242
I1013 03:29:41.083039 140500739528448 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7549759745597839, loss=1.8353654146194458
I1013 03:36:00.892666 140500747921152 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7095142006874084, loss=1.7992023229599
I1013 03:38:02.111912 140673456789312 spec.py:321] Evaluating on the training split.
I1013 03:38:54.042840 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 03:39:42.216766 140673456789312 spec.py:349] Evaluating on the test split.
I1013 03:40:06.557321 140673456789312 submission_runner.py:393] Time since start: 4936.80s, 	Step: 5662, 	{'train/ctc_loss': Array(0.5001109, dtype=float32), 'train/wer': 0.17819736335794972, 'validation/ctc_loss': Array(0.83646595, dtype=float32), 'validation/wer': 0.24894522915319037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.56430906, dtype=float32), 'test/wer': 0.18607438100461074, 'test/num_examples': 2472, 'score': 4397.406131744385, 'total_duration': 4936.801326990128, 'accumulated_submission_time': 4397.406131744385, 'accumulated_eval_time': 539.0001018047333, 'accumulated_logging_time': 0.1638789176940918}
I1013 03:40:06.591741 140500164241152 logging_writer.py:48] [5662] accumulated_eval_time=539.000102, accumulated_logging_time=0.163879, accumulated_submission_time=4397.406132, global_step=5662, preemption_count=0, score=4397.406132, test/ctc_loss=0.5643090605735779, test/num_examples=2472, test/wer=0.186074, total_duration=4936.801327, train/ctc_loss=0.500110924243927, train/wer=0.178197, validation/ctc_loss=0.8364659547805786, validation/num_examples=5348, validation/wer=0.248945
I1013 03:44:21.852787 140500155848448 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7316122651100159, loss=1.7571423053741455
I1013 03:50:41.528273 140500164241152 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7562639713287354, loss=1.7455843687057495
I1013 03:56:58.285369 140500155848448 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6398801803588867, loss=1.7420482635498047
I1013 04:03:18.465925 140500164241152 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6443536281585693, loss=1.7111250162124634
I1013 04:04:07.250661 140673456789312 spec.py:321] Evaluating on the training split.
I1013 04:04:59.645591 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 04:05:48.789459 140673456789312 spec.py:349] Evaluating on the test split.
I1013 04:06:13.875537 140673456789312 submission_runner.py:393] Time since start: 6504.12s, 	Step: 7566, 	{'train/ctc_loss': Array(0.46576366, dtype=float32), 'train/wer': 0.16242184963713097, 'validation/ctc_loss': Array(0.7752064, dtype=float32), 'validation/wer': 0.23147030711451383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50696874, dtype=float32), 'test/wer': 0.16913452359189973, 'test/num_examples': 2472, 'score': 5837.976397037506, 'total_duration': 6504.11940073967, 'accumulated_submission_time': 5837.976397037506, 'accumulated_eval_time': 665.6183609962463, 'accumulated_logging_time': 0.2127223014831543}
I1013 04:06:13.907769 140499652237056 logging_writer.py:48] [7566] accumulated_eval_time=665.618361, accumulated_logging_time=0.212722, accumulated_submission_time=5837.976397, global_step=7566, preemption_count=0, score=5837.976397, test/ctc_loss=0.5069687366485596, test/num_examples=2472, test/wer=0.169135, total_duration=6504.119401, train/ctc_loss=0.465763658285141, train/wer=0.162422, validation/ctc_loss=0.7752063870429993, validation/num_examples=5348, validation/wer=0.231470
I1013 04:11:41.705023 140499643844352 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6406005620956421, loss=1.7012509107589722
I1013 04:18:16.523010 140499652237056 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7696448564529419, loss=1.7091017961502075
I1013 04:24:41.932730 140499643844352 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6661743521690369, loss=1.679140567779541
I1013 04:30:14.448051 140673456789312 spec.py:321] Evaluating on the training split.
I1013 04:31:06.617234 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 04:31:55.125630 140673456789312 spec.py:349] Evaluating on the test split.
I1013 04:32:19.917761 140673456789312 submission_runner.py:393] Time since start: 8070.16s, 	Step: 9416, 	{'train/ctc_loss': Array(0.44879016, dtype=float32), 'train/wer': 0.1574235143186414, 'validation/ctc_loss': Array(0.7413137, dtype=float32), 'validation/wer': 0.22210529364627282, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48560467, dtype=float32), 'test/wer': 0.16393475920622347, 'test/num_examples': 2472, 'score': 7278.4315865039825, 'total_duration': 8070.162228822708, 'accumulated_submission_time': 7278.4315865039825, 'accumulated_eval_time': 791.0822207927704, 'accumulated_logging_time': 0.25879669189453125}
I1013 04:32:19.950124 140499867277056 logging_writer.py:48] [9416] accumulated_eval_time=791.082221, accumulated_logging_time=0.258797, accumulated_submission_time=7278.431587, global_step=9416, preemption_count=0, score=7278.431587, test/ctc_loss=0.4856046736240387, test/num_examples=2472, test/wer=0.163935, total_duration=8070.162229, train/ctc_loss=0.44879016280174255, train/wer=0.157424, validation/ctc_loss=0.7413136959075928, validation/num_examples=5348, validation/wer=0.222105
I1013 04:33:23.892219 140499858884352 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.1510478258132935, loss=1.7281920909881592
I1013 04:39:41.153702 140499867277056 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5695053339004517, loss=1.7273536920547485
I1013 04:46:17.342424 140499867277056 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9670456647872925, loss=1.6614055633544922
I1013 04:52:38.580087 140499858884352 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7960590720176697, loss=1.7650747299194336
I1013 04:56:20.038836 140673456789312 spec.py:321] Evaluating on the training split.
I1013 04:57:12.975115 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 04:58:01.773526 140673456789312 spec.py:349] Evaluating on the test split.
I1013 04:58:26.554641 140673456789312 submission_runner.py:393] Time since start: 9636.80s, 	Step: 11273, 	{'train/ctc_loss': Array(0.451032, dtype=float32), 'train/wer': 0.15410960731071052, 'validation/ctc_loss': Array(0.7257357, dtype=float32), 'validation/wer': 0.2137443640962762, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46346053, dtype=float32), 'test/wer': 0.15408364308492273, 'test/num_examples': 2472, 'score': 8718.429273843765, 'total_duration': 9636.798786640167, 'accumulated_submission_time': 8718.429273843765, 'accumulated_eval_time': 917.5916821956635, 'accumulated_logging_time': 0.3092501163482666}
I1013 04:58:26.587959 140500747921152 logging_writer.py:48] [11273] accumulated_eval_time=917.591682, accumulated_logging_time=0.309250, accumulated_submission_time=8718.429274, global_step=11273, preemption_count=0, score=8718.429274, test/ctc_loss=0.4634605348110199, test/num_examples=2472, test/wer=0.154084, total_duration=9636.798787, train/ctc_loss=0.45103201270103455, train/wer=0.154110, validation/ctc_loss=0.7257357239723206, validation/num_examples=5348, validation/wer=0.213744
I1013 05:01:21.408589 140499877521152 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7251985669136047, loss=1.6512531042099
I1013 05:07:38.580144 140499869128448 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7747734189033508, loss=1.714248776435852
I1013 05:14:10.408011 140499877521152 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.719656229019165, loss=1.6266577243804932
I1013 05:20:27.813282 140499869128448 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6631949543952942, loss=1.62504243850708
I1013 05:22:26.745111 140673456789312 spec.py:321] Evaluating on the training split.
I1013 05:23:19.475118 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 05:24:08.298227 140673456789312 spec.py:349] Evaluating on the test split.
I1013 05:24:32.935946 140673456789312 submission_runner.py:393] Time since start: 11203.18s, 	Step: 13159, 	{'train/ctc_loss': Array(0.40789133, dtype=float32), 'train/wer': 0.14170016803862762, 'validation/ctc_loss': Array(0.6824526, dtype=float32), 'validation/wer': 0.2025546211996872, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4475893, dtype=float32), 'test/wer': 0.14845733552698392, 'test/num_examples': 2472, 'score': 10158.496913194656, 'total_duration': 11203.180178165436, 'accumulated_submission_time': 10158.496913194656, 'accumulated_eval_time': 1043.7762591838837, 'accumulated_logging_time': 0.35781049728393555}
I1013 05:24:32.967920 140499877521152 logging_writer.py:48] [13159] accumulated_eval_time=1043.776259, accumulated_logging_time=0.357810, accumulated_submission_time=10158.496913, global_step=13159, preemption_count=0, score=10158.496913, test/ctc_loss=0.44758930802345276, test/num_examples=2472, test/wer=0.148457, total_duration=11203.180178, train/ctc_loss=0.40789133310317993, train/wer=0.141700, validation/ctc_loss=0.6824526190757751, validation/num_examples=5348, validation/wer=0.202555
I1013 05:28:53.668219 140500747921152 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5899582505226135, loss=1.6700439453125
I1013 05:35:11.062793 140500739528448 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.8071114420890808, loss=1.5989396572113037
I1013 05:41:46.729985 140499877521152 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6715312600135803, loss=1.6363391876220703
I1013 05:48:03.183994 140499869128448 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.106603741645813, loss=1.5885014533996582
I1013 05:48:33.002471 140673456789312 spec.py:321] Evaluating on the training split.
I1013 05:49:26.205568 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 05:50:14.655158 140673456789312 spec.py:349] Evaluating on the test split.
I1013 05:50:39.405202 140673456789312 submission_runner.py:393] Time since start: 12769.65s, 	Step: 15041, 	{'train/ctc_loss': Array(0.36828592, dtype=float32), 'train/wer': 0.13233833433006578, 'validation/ctc_loss': Array(0.6705605, dtype=float32), 'validation/wer': 0.20003475675101615, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43684083, dtype=float32), 'test/wer': 0.14614181544898747, 'test/num_examples': 2472, 'score': 11598.443712949753, 'total_duration': 12769.64871764183, 'accumulated_submission_time': 11598.443712949753, 'accumulated_eval_time': 1170.1720054149628, 'accumulated_logging_time': 0.4033846855163574}
I1013 05:50:39.441235 140499662481152 logging_writer.py:48] [15041] accumulated_eval_time=1170.172005, accumulated_logging_time=0.403385, accumulated_submission_time=11598.443713, global_step=15041, preemption_count=0, score=11598.443713, test/ctc_loss=0.43684083223342896, test/num_examples=2472, test/wer=0.146142, total_duration=12769.648718, train/ctc_loss=0.3682859241962433, train/wer=0.132338, validation/ctc_loss=0.6705604791641235, validation/num_examples=5348, validation/wer=0.200035
I1013 05:56:28.583035 140500747921152 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7630288004875183, loss=1.597916603088379
I1013 06:02:45.073221 140500739528448 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5968822240829468, loss=1.5462146997451782
I1013 06:09:21.304064 140499662481152 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6969853043556213, loss=1.581013798713684
I1013 06:14:39.425627 140673456789312 spec.py:321] Evaluating on the training split.
I1013 06:15:32.021524 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 06:16:20.985123 140673456789312 spec.py:349] Evaluating on the test split.
I1013 06:16:45.809011 140673456789312 submission_runner.py:393] Time since start: 14336.05s, 	Step: 16924, 	{'train/ctc_loss': Array(0.34190282, dtype=float32), 'train/wer': 0.11814182125776058, 'validation/ctc_loss': Array(0.65478075, dtype=float32), 'validation/wer': 0.1915965899765392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4151378, dtype=float32), 'test/wer': 0.1373469014685272, 'test/num_examples': 2472, 'score': 13038.340107679367, 'total_duration': 14336.052446603775, 'accumulated_submission_time': 13038.340107679367, 'accumulated_eval_time': 1296.548362493515, 'accumulated_logging_time': 0.4521961212158203}
I1013 06:16:45.857069 140499949201152 logging_writer.py:48] [16924] accumulated_eval_time=1296.548362, accumulated_logging_time=0.452196, accumulated_submission_time=13038.340108, global_step=16924, preemption_count=0, score=13038.340108, test/ctc_loss=0.41513779759407043, test/num_examples=2472, test/wer=0.137347, total_duration=14336.052447, train/ctc_loss=0.34190282225608826, train/wer=0.118142, validation/ctc_loss=0.6547807455062866, validation/num_examples=5348, validation/wer=0.191597
I1013 06:17:43.778631 140499940808448 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7369380593299866, loss=1.618430256843567
I1013 06:24:02.841998 140499949201152 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7634440660476685, loss=1.5653109550476074
I1013 06:30:22.914048 140499949201152 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6706106066703796, loss=1.5779716968536377
I1013 06:36:54.819080 140499940808448 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6332652568817139, loss=1.5383144617080688
I1013 06:40:46.103393 140673456789312 spec.py:321] Evaluating on the training split.
I1013 06:41:38.980491 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 06:42:27.809309 140673456789312 spec.py:349] Evaluating on the test split.
I1013 06:42:52.388280 140673456789312 submission_runner.py:393] Time since start: 15902.63s, 	Step: 18800, 	{'train/ctc_loss': Array(0.35107782, dtype=float32), 'train/wer': 0.1257702435283455, 'validation/ctc_loss': Array(0.6287276, dtype=float32), 'validation/wer': 0.18711683095667958, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39819273, dtype=float32), 'test/wer': 0.13421891820526882, 'test/num_examples': 2472, 'score': 14478.491617918015, 'total_duration': 15902.633050203323, 'accumulated_submission_time': 14478.491617918015, 'accumulated_eval_time': 1422.827733039856, 'accumulated_logging_time': 0.520374059677124}
I1013 06:42:52.421893 140499949201152 logging_writer.py:48] [18800] accumulated_eval_time=1422.827733, accumulated_logging_time=0.520374, accumulated_submission_time=14478.491618, global_step=18800, preemption_count=0, score=14478.491618, test/ctc_loss=0.39819273352622986, test/num_examples=2472, test/wer=0.134219, total_duration=15902.633050, train/ctc_loss=0.3510778248310089, train/wer=0.125770, validation/ctc_loss=0.6287276148796082, validation/num_examples=5348, validation/wer=0.187117
I1013 06:45:23.634531 140499940808448 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7910780310630798, loss=1.5814425945281982
I1013 06:51:45.137922 140499949201152 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.8182827234268188, loss=1.57815682888031
I1013 06:58:09.160823 140499949201152 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7904201745986938, loss=1.5438730716705322
I1013 07:04:33.574942 140499940808448 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6747559309005737, loss=1.529921531677246
I1013 07:06:53.075994 140673456789312 spec.py:321] Evaluating on the training split.
I1013 07:07:46.665049 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 07:08:36.185257 140673456789312 spec.py:349] Evaluating on the test split.
I1013 07:09:00.972461 140673456789312 submission_runner.py:393] Time since start: 17471.22s, 	Step: 20674, 	{'train/ctc_loss': Array(0.35100877, dtype=float32), 'train/wer': 0.1229593590523282, 'validation/ctc_loss': Array(0.62338454, dtype=float32), 'validation/wer': 0.1874643984668411, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.390619, dtype=float32), 'test/wer': 0.13167997075132534, 'test/num_examples': 2472, 'score': 15919.052374124527, 'total_duration': 17471.21564912796, 'accumulated_submission_time': 15919.052374124527, 'accumulated_eval_time': 1550.7171084880829, 'accumulated_logging_time': 0.569277286529541}
I1013 07:09:01.008845 140500532881152 logging_writer.py:48] [20674] accumulated_eval_time=1550.717108, accumulated_logging_time=0.569277, accumulated_submission_time=15919.052374, global_step=20674, preemption_count=0, score=15919.052374, test/ctc_loss=0.3906190097332001, test/num_examples=2472, test/wer=0.131680, total_duration=17471.215649, train/ctc_loss=0.3510087728500366, train/wer=0.122959, validation/ctc_loss=0.6233845353126526, validation/num_examples=5348, validation/wer=0.187464
I1013 07:13:07.174699 140500524488448 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8572317361831665, loss=1.4469448328018188
I1013 07:19:27.202337 140500532881152 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7427913546562195, loss=1.5788371562957764
I1013 07:25:54.001449 140500205201152 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6329600214958191, loss=1.4881867170333862
I1013 07:32:17.899592 140500196808448 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6724984645843506, loss=1.5238863229751587
I1013 07:33:01.404238 140673456789312 spec.py:321] Evaluating on the training split.
I1013 07:33:54.986854 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 07:34:44.287545 140673456789312 spec.py:349] Evaluating on the test split.
I1013 07:35:09.310326 140673456789312 submission_runner.py:393] Time since start: 19039.55s, 	Step: 22556, 	{'train/ctc_loss': Array(0.3452498, dtype=float32), 'train/wer': 0.1197615216186984, 'validation/ctc_loss': Array(0.60689515, dtype=float32), 'validation/wer': 0.17908415961072438, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37766132, dtype=float32), 'test/wer': 0.12617553267117584, 'test/num_examples': 2472, 'score': 17359.355275392532, 'total_duration': 19039.554783821106, 'accumulated_submission_time': 17359.355275392532, 'accumulated_eval_time': 1678.617175102234, 'accumulated_logging_time': 0.6200308799743652}
I1013 07:35:09.345175 140500205201152 logging_writer.py:48] [22556] accumulated_eval_time=1678.617175, accumulated_logging_time=0.620031, accumulated_submission_time=17359.355275, global_step=22556, preemption_count=0, score=17359.355275, test/ctc_loss=0.3776613175868988, test/num_examples=2472, test/wer=0.126176, total_duration=19039.554784, train/ctc_loss=0.34524980187416077, train/wer=0.119762, validation/ctc_loss=0.6068951487541199, validation/num_examples=5348, validation/wer=0.179084
I1013 07:40:47.439092 140499293841152 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.8004098534584045, loss=1.5149681568145752
I1013 07:47:15.339238 140499285448448 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8395155072212219, loss=1.4463218450546265
I1013 07:53:44.651298 140499293841152 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6118087768554688, loss=1.4893081188201904
I1013 07:59:09.642671 140673456789312 spec.py:321] Evaluating on the training split.
I1013 08:00:06.764649 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 08:00:55.875563 140673456789312 spec.py:349] Evaluating on the test split.
I1013 08:01:21.004222 140673456789312 submission_runner.py:393] Time since start: 20611.25s, 	Step: 24432, 	{'train/ctc_loss': Array(0.3137263, dtype=float32), 'train/wer': 0.10891552628871283, 'validation/ctc_loss': Array(0.56719977, dtype=float32), 'validation/wer': 0.16800061789779586, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35258034, dtype=float32), 'test/wer': 0.11549164178498161, 'test/num_examples': 2472, 'score': 18799.559829235077, 'total_duration': 20611.24811911583, 'accumulated_submission_time': 18799.559829235077, 'accumulated_eval_time': 1809.9721534252167, 'accumulated_logging_time': 0.6716907024383545}
I1013 08:01:21.044528 140500532881152 logging_writer.py:48] [24432] accumulated_eval_time=1809.972153, accumulated_logging_time=0.671691, accumulated_submission_time=18799.559829, global_step=24432, preemption_count=0, score=18799.559829, test/ctc_loss=0.35258033871650696, test/num_examples=2472, test/wer=0.115492, total_duration=20611.248119, train/ctc_loss=0.3137263059616089, train/wer=0.108916, validation/ctc_loss=0.5671997666358948, validation/num_examples=5348, validation/wer=0.168001
I1013 08:02:13.299127 140500524488448 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.7656955122947693, loss=1.4856549501419067
I1013 08:08:33.254384 140500532881152 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7519541382789612, loss=1.483593225479126
I1013 08:15:01.737760 140500524488448 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6771957278251648, loss=1.4170286655426025
I1013 08:21:34.949120 140500532881152 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.692046046257019, loss=1.4844340085983276
I1013 08:25:21.331167 140673456789312 spec.py:321] Evaluating on the training split.
I1013 08:26:14.654712 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 08:27:03.952650 140673456789312 spec.py:349] Evaluating on the test split.
I1013 08:27:28.724126 140673456789312 submission_runner.py:393] Time since start: 22178.97s, 	Step: 26302, 	{'train/ctc_loss': Array(0.2906842, dtype=float32), 'train/wer': 0.10294645496389511, 'validation/ctc_loss': Array(0.56886786, dtype=float32), 'validation/wer': 0.16756615851009393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34685555, dtype=float32), 'test/wer': 0.11388702699408933, 'test/num_examples': 2472, 'score': 20239.755044221878, 'total_duration': 22178.966100931168, 'accumulated_submission_time': 20239.755044221878, 'accumulated_eval_time': 1937.356656074524, 'accumulated_logging_time': 0.7264351844787598}
I1013 08:27:28.759835 140500241041152 logging_writer.py:48] [26302] accumulated_eval_time=1937.356656, accumulated_logging_time=0.726435, accumulated_submission_time=20239.755044, global_step=26302, preemption_count=0, score=20239.755044, test/ctc_loss=0.3468555510044098, test/num_examples=2472, test/wer=0.113887, total_duration=22178.966101, train/ctc_loss=0.29068419337272644, train/wer=0.102946, validation/ctc_loss=0.5688678622245789, validation/num_examples=5348, validation/wer=0.167566
I1013 08:29:58.616582 140500232648448 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7578094005584717, loss=1.4379302263259888
I1013 08:36:19.133663 140500241041152 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.7950414419174194, loss=1.4938257932662964
I1013 08:42:39.311459 140500232648448 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8681051731109619, loss=1.4379240274429321
I1013 08:49:17.666508 140500241041152 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.7536419034004211, loss=1.4306284189224243
I1013 08:51:29.322027 140673456789312 spec.py:321] Evaluating on the training split.
I1013 08:52:22.970071 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 08:53:12.171480 140673456789312 spec.py:349] Evaluating on the test split.
I1013 08:53:37.119857 140673456789312 submission_runner.py:393] Time since start: 23747.36s, 	Step: 28176, 	{'train/ctc_loss': Array(0.27847216, dtype=float32), 'train/wer': 0.09994000886825426, 'validation/ctc_loss': Array(0.55278903, dtype=float32), 'validation/wer': 0.1636560240207768, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3358176, dtype=float32), 'test/wer': 0.11155119533646132, 'test/num_examples': 2472, 'score': 21680.22256922722, 'total_duration': 23747.363839149475, 'accumulated_submission_time': 21680.22256922722, 'accumulated_eval_time': 2065.1480174064636, 'accumulated_logging_time': 0.780937910079956}
I1013 08:53:37.158983 140500317841152 logging_writer.py:48] [28176] accumulated_eval_time=2065.148017, accumulated_logging_time=0.780938, accumulated_submission_time=21680.222569, global_step=28176, preemption_count=0, score=21680.222569, test/ctc_loss=0.3358176052570343, test/num_examples=2472, test/wer=0.111551, total_duration=23747.363839, train/ctc_loss=0.2784721553325653, train/wer=0.099940, validation/ctc_loss=0.552789032459259, validation/num_examples=5348, validation/wer=0.163656
I1013 08:57:41.560200 140500309448448 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.8276407718658447, loss=1.3604580163955688
I1013 09:04:07.158674 140499990161152 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.8898130655288696, loss=1.4016931056976318
I1013 09:10:23.746364 140499981768448 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.6434245109558105, loss=1.4161800146102905
I1013 09:17:03.423469 140500317841152 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7306488752365112, loss=1.3663311004638672
I1013 09:17:37.731298 140673456789312 spec.py:321] Evaluating on the training split.
I1013 09:18:31.737554 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 09:19:21.000415 140673456789312 spec.py:349] Evaluating on the test split.
I1013 09:19:45.914476 140673456789312 submission_runner.py:393] Time since start: 25316.16s, 	Step: 30047, 	{'train/ctc_loss': Array(0.2547219, dtype=float32), 'train/wer': 0.09049206820018206, 'validation/ctc_loss': Array(0.52692395, dtype=float32), 'validation/wer': 0.15583575504214256, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3147888, dtype=float32), 'test/wer': 0.1048889972173136, 'test/num_examples': 2472, 'score': 23120.70231986046, 'total_duration': 25316.15892100334, 'accumulated_submission_time': 23120.70231986046, 'accumulated_eval_time': 2193.3251678943634, 'accumulated_logging_time': 0.8369204998016357}
I1013 09:19:45.947350 140500747921152 logging_writer.py:48] [30047] accumulated_eval_time=2193.325168, accumulated_logging_time=0.836920, accumulated_submission_time=23120.702320, global_step=30047, preemption_count=0, score=23120.702320, test/ctc_loss=0.3147887885570526, test/num_examples=2472, test/wer=0.104889, total_duration=25316.158921, train/ctc_loss=0.25472190976142883, train/wer=0.090492, validation/ctc_loss=0.5269239544868469, validation/num_examples=5348, validation/wer=0.155836
I1013 09:25:27.813864 140500739528448 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.7240685820579529, loss=1.3733912706375122
I1013 09:32:01.560371 140500747921152 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6659550070762634, loss=1.3215043544769287
I1013 09:38:18.423871 140500739528448 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6762005686759949, loss=1.4395511150360107
I1013 09:43:46.615888 140673456789312 spec.py:321] Evaluating on the training split.
I1013 09:44:39.280314 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 09:45:28.323146 140673456789312 spec.py:349] Evaluating on the test split.
I1013 09:45:53.202085 140673456789312 submission_runner.py:393] Time since start: 26883.45s, 	Step: 31910, 	{'train/ctc_loss': Array(0.26242533, dtype=float32), 'train/wer': 0.09455169873461605, 'validation/ctc_loss': Array(0.5106409, dtype=float32), 'validation/wer': 0.15240835320582755, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30022863, dtype=float32), 'test/wer': 0.10060325391505698, 'test/num_examples': 2472, 'score': 24561.27951812744, 'total_duration': 26883.44611763954, 'accumulated_submission_time': 24561.27951812744, 'accumulated_eval_time': 2319.904929161072, 'accumulated_logging_time': 0.8843526840209961}
I1013 09:45:53.243258 140500747921152 logging_writer.py:48] [31910] accumulated_eval_time=2319.904929, accumulated_logging_time=0.884353, accumulated_submission_time=24561.279518, global_step=31910, preemption_count=0, score=24561.279518, test/ctc_loss=0.30022862553596497, test/num_examples=2472, test/wer=0.100603, total_duration=26883.446118, train/ctc_loss=0.26242533326148987, train/wer=0.094552, validation/ctc_loss=0.5106409192085266, validation/num_examples=5348, validation/wer=0.152408
I1013 09:47:04.984090 140500747921152 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.764194667339325, loss=1.3538919687271118
I1013 09:53:21.923991 140500739528448 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7221313714981079, loss=1.3838881254196167
I1013 09:59:56.620993 140500420241152 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7313528060913086, loss=1.3331669569015503
I1013 10:06:13.031701 140500411848448 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7540580630302429, loss=1.335571050643921
I1013 10:09:53.526615 140673456789312 spec.py:321] Evaluating on the training split.
I1013 10:10:46.394714 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 10:11:35.812283 140673456789312 spec.py:349] Evaluating on the test split.
I1013 10:12:00.788742 140673456789312 submission_runner.py:393] Time since start: 28451.03s, 	Step: 33787, 	{'train/ctc_loss': Array(0.24002926, dtype=float32), 'train/wer': 0.08474272784985268, 'validation/ctc_loss': Array(0.49889132, dtype=float32), 'validation/wer': 0.1487878583083117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29228598, dtype=float32), 'test/wer': 0.09767838644811407, 'test/num_examples': 2472, 'score': 26001.471496343613, 'total_duration': 28451.03255057335, 'accumulated_submission_time': 26001.471496343613, 'accumulated_eval_time': 2447.160432100296, 'accumulated_logging_time': 0.9408268928527832}
I1013 10:12:00.824919 140499437197056 logging_writer.py:48] [33787] accumulated_eval_time=2447.160432, accumulated_logging_time=0.940827, accumulated_submission_time=26001.471496, global_step=33787, preemption_count=0, score=26001.471496, test/ctc_loss=0.2922859787940979, test/num_examples=2472, test/wer=0.097678, total_duration=28451.032551, train/ctc_loss=0.2400292605161667, train/wer=0.084743, validation/ctc_loss=0.49889132380485535, validation/num_examples=5348, validation/wer=0.148788
I1013 10:14:45.561207 140499437197056 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.9433817267417908, loss=1.3083398342132568
I1013 10:21:02.368193 140499428804352 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7588703036308289, loss=1.3807870149612427
I1013 10:27:38.784197 140499437197056 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6889110207557678, loss=1.3515225648880005
I1013 10:34:00.412122 140499109517056 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7907474040985107, loss=1.2813999652862549
I1013 10:36:00.857593 140673456789312 spec.py:321] Evaluating on the training split.
I1013 10:36:55.396124 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 10:37:44.475879 140673456789312 spec.py:349] Evaluating on the test split.
I1013 10:38:09.406838 140673456789312 submission_runner.py:393] Time since start: 30019.65s, 	Step: 35661, 	{'train/ctc_loss': Array(0.24271333, dtype=float32), 'train/wer': 0.08625924004335564, 'validation/ctc_loss': Array(0.47414345, dtype=float32), 'validation/wer': 0.14084207883989688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27732986, dtype=float32), 'test/wer': 0.0929051652347003, 'test/num_examples': 2472, 'score': 27441.413339614868, 'total_duration': 30019.650795936584, 'accumulated_submission_time': 27441.413339614868, 'accumulated_eval_time': 2575.7031786441803, 'accumulated_logging_time': 0.9906232357025146}
I1013 10:38:09.442399 140500317841152 logging_writer.py:48] [35661] accumulated_eval_time=2575.703179, accumulated_logging_time=0.990623, accumulated_submission_time=27441.413340, global_step=35661, preemption_count=0, score=27441.413340, test/ctc_loss=0.27732986211776733, test/num_examples=2472, test/wer=0.092905, total_duration=30019.650796, train/ctc_loss=0.2427133321762085, train/wer=0.086259, validation/ctc_loss=0.47414344549179077, validation/num_examples=5348, validation/wer=0.140842
I1013 10:42:25.567750 140500309448448 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.68705153465271, loss=1.2987421751022339
I1013 10:48:45.906664 140500317841152 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6960759162902832, loss=1.3158183097839355
I1013 10:55:19.621374 140500309448448 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.8159753084182739, loss=1.2750988006591797
I1013 11:01:44.064640 140499662481152 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.7195723056793213, loss=1.2689257860183716
I1013 11:02:10.144992 140673456789312 spec.py:321] Evaluating on the training split.
I1013 11:03:03.688506 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 11:03:52.515837 140673456789312 spec.py:349] Evaluating on the test split.
I1013 11:04:17.658884 140673456789312 submission_runner.py:393] Time since start: 31587.90s, 	Step: 37536, 	{'train/ctc_loss': Array(0.22554836, dtype=float32), 'train/wer': 0.07781578694971067, 'validation/ctc_loss': Array(0.4519426, dtype=float32), 'validation/wer': 0.133562470432625, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2644897, dtype=float32), 'test/wer': 0.08953344301586334, 'test/num_examples': 2472, 'score': 28882.021570444107, 'total_duration': 31587.90259885788, 'accumulated_submission_time': 28882.021570444107, 'accumulated_eval_time': 2703.2102982997894, 'accumulated_logging_time': 1.041644811630249}
I1013 11:04:17.695668 140499662481152 logging_writer.py:48] [37536] accumulated_eval_time=2703.210298, accumulated_logging_time=1.041645, accumulated_submission_time=28882.021570, global_step=37536, preemption_count=0, score=28882.021570, test/ctc_loss=0.26448971033096313, test/num_examples=2472, test/wer=0.089533, total_duration=31587.902599, train/ctc_loss=0.22554835677146912, train/wer=0.077816, validation/ctc_loss=0.4519425928592682, validation/num_examples=5348, validation/wer=0.133562
I1013 11:10:07.874423 140499654088448 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.7688803672790527, loss=1.2830162048339844
I1013 11:16:29.572360 140499662481152 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.0254572629928589, loss=1.2240755558013916
I1013 11:23:02.042975 140499654088448 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7629812955856323, loss=1.2173997163772583
I1013 11:28:17.735938 140673456789312 spec.py:321] Evaluating on the training split.
I1013 11:29:11.929936 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 11:30:00.848511 140673456789312 spec.py:349] Evaluating on the test split.
I1013 11:30:25.961436 140673456789312 submission_runner.py:393] Time since start: 33156.21s, 	Step: 39407, 	{'train/ctc_loss': Array(0.16926204, dtype=float32), 'train/wer': 0.061350101985383806, 'validation/ctc_loss': Array(0.440538, dtype=float32), 'validation/wer': 0.1305695279840119, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25084078, dtype=float32), 'test/wer': 0.08382588913939837, 'test/num_examples': 2472, 'score': 30321.969761371613, 'total_duration': 33156.20608329773, 'accumulated_submission_time': 30321.969761371613, 'accumulated_eval_time': 2831.430045604706, 'accumulated_logging_time': 1.0932466983795166}
I1013 11:30:26.001998 140500747921152 logging_writer.py:48] [39407] accumulated_eval_time=2831.430046, accumulated_logging_time=1.093247, accumulated_submission_time=30321.969761, global_step=39407, preemption_count=0, score=30321.969761, test/ctc_loss=0.25084078311920166, test/num_examples=2472, test/wer=0.083826, total_duration=33156.206083, train/ctc_loss=0.16926203668117523, train/wer=0.061350, validation/ctc_loss=0.4405379891395569, validation/num_examples=5348, validation/wer=0.130570
I1013 11:31:36.781381 140500739528448 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.781311571598053, loss=1.2328952550888062
I1013 11:37:59.584513 140500747921152 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.7691066861152649, loss=1.2449510097503662
I1013 11:44:29.656684 140499764881152 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.8666285872459412, loss=1.2160518169403076
I1013 11:50:55.356717 140499756488448 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.8544402122497559, loss=1.2473174333572388
I1013 11:54:26.470875 140673456789312 spec.py:321] Evaluating on the training split.
I1013 11:55:20.645372 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 11:56:10.443169 140673456789312 spec.py:349] Evaluating on the test split.
I1013 11:56:35.429345 140673456789312 submission_runner.py:393] Time since start: 34725.67s, 	Step: 41259, 	{'train/ctc_loss': Array(0.18502629, dtype=float32), 'train/wer': 0.06609152921796795, 'validation/ctc_loss': Array(0.4219626, dtype=float32), 'validation/wer': 0.123058207903299, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23841603, dtype=float32), 'test/wer': 0.08023073954461439, 'test/num_examples': 2472, 'score': 31762.34598684311, 'total_duration': 34725.67310214043, 'accumulated_submission_time': 31762.34598684311, 'accumulated_eval_time': 2960.381998062134, 'accumulated_logging_time': 1.14918851852417}
I1013 11:56:35.466420 140498884237056 logging_writer.py:48] [41259] accumulated_eval_time=2960.381998, accumulated_logging_time=1.149189, accumulated_submission_time=31762.345987, global_step=41259, preemption_count=0, score=31762.345987, test/ctc_loss=0.23841603100299835, test/num_examples=2472, test/wer=0.080231, total_duration=34725.673102, train/ctc_loss=0.18502628803253174, train/wer=0.066092, validation/ctc_loss=0.42196258902549744, validation/num_examples=5348, validation/wer=0.123058
I1013 11:59:38.069361 140498875844352 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.7845453023910522, loss=1.2494149208068848
I1013 12:05:59.017348 140498884237056 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.8017008304595947, loss=1.194441795349121
I1013 12:12:35.141499 140498884237056 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.7868089079856873, loss=1.1842765808105469
I1013 12:19:02.452356 140498875844352 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.8762543201446533, loss=1.2109229564666748
I1013 12:20:36.341321 140673456789312 spec.py:321] Evaluating on the training split.
I1013 12:21:29.998952 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 12:22:19.957759 140673456789312 spec.py:349] Evaluating on the test split.
I1013 12:22:44.947805 140673456789312 submission_runner.py:393] Time since start: 36295.19s, 	Step: 43117, 	{'train/ctc_loss': Array(0.21619506, dtype=float32), 'train/wer': 0.07624089432261354, 'validation/ctc_loss': Array(0.39728707, dtype=float32), 'validation/wer': 0.11843362908753874, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22376235, dtype=float32), 'test/wer': 0.07509190989783275, 'test/num_examples': 2472, 'score': 33203.12744092941, 'total_duration': 36295.1917412281, 'accumulated_submission_time': 33203.12744092941, 'accumulated_eval_time': 3088.981956243515, 'accumulated_logging_time': 1.2015783786773682}
I1013 12:22:44.987203 140499334801152 logging_writer.py:48] [43117] accumulated_eval_time=3088.981956, accumulated_logging_time=1.201578, accumulated_submission_time=33203.127441, global_step=43117, preemption_count=0, score=33203.127441, test/ctc_loss=0.22376234829425812, test/num_examples=2472, test/wer=0.075092, total_duration=36295.191741, train/ctc_loss=0.21619506180286407, train/wer=0.076241, validation/ctc_loss=0.3972870707511902, validation/num_examples=5348, validation/wer=0.118434
I1013 12:27:37.987999 140499334801152 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.8301423192024231, loss=1.1769622564315796
I1013 12:34:01.841499 140499326408448 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.8199711441993713, loss=1.1448935270309448
I1013 12:40:38.490911 140500092561152 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.8641474843025208, loss=1.1830507516860962
I1013 12:46:44.982069 140673456789312 spec.py:321] Evaluating on the training split.
I1013 12:47:37.497563 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 12:48:26.782716 140673456789312 spec.py:349] Evaluating on the test split.
I1013 12:48:51.811069 140673456789312 submission_runner.py:393] Time since start: 37862.06s, 	Step: 44983, 	{'train/ctc_loss': Array(0.20942391, dtype=float32), 'train/wer': 0.07494851141075959, 'validation/ctc_loss': Array(0.3889882, dtype=float32), 'validation/wer': 0.1151607017001844, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21613249, dtype=float32), 'test/wer': 0.07088741291410233, 'test/num_examples': 2472, 'score': 34643.02970933914, 'total_duration': 37862.0554959774, 'accumulated_submission_time': 34643.02970933914, 'accumulated_eval_time': 3215.804906845093, 'accumulated_logging_time': 1.2567241191864014}
I1013 12:48:51.854187 140499877521152 logging_writer.py:48] [44983] accumulated_eval_time=3215.804907, accumulated_logging_time=1.256724, accumulated_submission_time=34643.029709, global_step=44983, preemption_count=0, score=34643.029709, test/ctc_loss=0.2161324918270111, test/num_examples=2472, test/wer=0.070887, total_duration=37862.055496, train/ctc_loss=0.20942391455173492, train/wer=0.074949, validation/ctc_loss=0.388988196849823, validation/num_examples=5348, validation/wer=0.115161
I1013 12:49:05.503266 140499869128448 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.8968855738639832, loss=1.1519594192504883
I1013 12:55:25.593473 140499549841152 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.1827704906463623, loss=1.1145269870758057
I1013 13:01:46.994699 140499541448448 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9135645031929016, loss=1.0909740924835205
I1013 13:08:29.706589 140499549841152 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.904990553855896, loss=1.138499140739441
I1013 13:12:52.313697 140673456789312 spec.py:321] Evaluating on the training split.
I1013 13:13:43.133292 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 13:14:32.979348 140673456789312 spec.py:349] Evaluating on the test split.
I1013 13:14:57.862548 140673456789312 submission_runner.py:393] Time since start: 39428.11s, 	Step: 46849, 	{'train/ctc_loss': Array(0.22263704, dtype=float32), 'train/wer': 0.08061959274879936, 'validation/ctc_loss': Array(0.37133935, dtype=float32), 'validation/wer': 0.10822866080307404, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20644823, dtype=float32), 'test/wer': 0.06905937074726301, 'test/num_examples': 2472, 'score': 36083.39652848244, 'total_duration': 39428.10625362396, 'accumulated_submission_time': 36083.39652848244, 'accumulated_eval_time': 3341.3469812870026, 'accumulated_logging_time': 1.3147220611572266}
I1013 13:14:57.905196 140499549841152 logging_writer.py:48] [46849] accumulated_eval_time=3341.346981, accumulated_logging_time=1.314722, accumulated_submission_time=36083.396528, global_step=46849, preemption_count=0, score=36083.396528, test/ctc_loss=0.20644822716712952, test/num_examples=2472, test/wer=0.069059, total_duration=39428.106254, train/ctc_loss=0.22263704240322113, train/wer=0.080620, validation/ctc_loss=0.371339350938797, validation/num_examples=5348, validation/wer=0.108229
I1013 13:16:52.410926 140499541448448 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.982223629951477, loss=1.132885217666626
I1013 13:23:13.951277 140499222161152 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.9428374171257019, loss=1.0510647296905518
I1013 13:29:31.978403 140499213768448 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9299927353858948, loss=1.1042180061340332
I1013 13:36:16.602272 140499877521152 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.9629291296005249, loss=1.1200352907180786
I1013 13:38:57.934381 140673456789312 spec.py:321] Evaluating on the training split.
I1013 13:39:48.630028 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 13:40:38.158334 140673456789312 spec.py:349] Evaluating on the test split.
I1013 13:41:03.492461 140673456789312 submission_runner.py:393] Time since start: 40993.74s, 	Step: 48715, 	{'train/ctc_loss': Array(0.1854418, dtype=float32), 'train/wer': 0.06379195638478884, 'validation/ctc_loss': Array(0.3513036, dtype=float32), 'validation/wer': 0.10312134933431168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19208613, dtype=float32), 'test/wer': 0.063758048463429, 'test/num_examples': 2472, 'score': 37523.32898855209, 'total_duration': 40993.73630857468, 'accumulated_submission_time': 37523.32898855209, 'accumulated_eval_time': 3466.8984401226044, 'accumulated_logging_time': 1.3741002082824707}
I1013 13:41:03.530858 140499949201152 logging_writer.py:48] [48715] accumulated_eval_time=3466.898440, accumulated_logging_time=1.374100, accumulated_submission_time=37523.328989, global_step=48715, preemption_count=0, score=37523.328989, test/ctc_loss=0.1920861303806305, test/num_examples=2472, test/wer=0.063758, total_duration=40993.736309, train/ctc_loss=0.18544180691242218, train/wer=0.063792, validation/ctc_loss=0.3513036072254181, validation/num_examples=5348, validation/wer=0.103121
I1013 13:44:39.159902 140499940808448 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9710189700126648, loss=1.0945262908935547
I1013 13:51:16.882976 140499293841152 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.0010347366333008, loss=1.086350917816162
I1013 13:57:34.341631 140499285448448 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.9551774263381958, loss=1.079897403717041
I1013 14:04:16.563443 140498966161152 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.0376906394958496, loss=1.0364830493927002
I1013 14:05:03.712054 140673456789312 spec.py:321] Evaluating on the training split.
I1013 14:05:56.032513 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 14:06:46.010107 140673456789312 spec.py:349] Evaluating on the test split.
I1013 14:07:11.538122 140673456789312 submission_runner.py:393] Time since start: 42561.78s, 	Step: 50564, 	{'train/ctc_loss': Array(0.15914606, dtype=float32), 'train/wer': 0.05808179216817164, 'validation/ctc_loss': Array(0.3352296, dtype=float32), 'validation/wer': 0.09659480386572308, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18569238, dtype=float32), 'test/wer': 0.06020352202790811, 'test/num_examples': 2472, 'score': 38963.41722846031, 'total_duration': 42561.78264927864, 'accumulated_submission_time': 38963.41722846031, 'accumulated_eval_time': 3594.718602180481, 'accumulated_logging_time': 1.4291489124298096}
I1013 14:07:11.573615 140499949201152 logging_writer.py:48] [50564] accumulated_eval_time=3594.718602, accumulated_logging_time=1.429149, accumulated_submission_time=38963.417228, global_step=50564, preemption_count=0, score=38963.417228, test/ctc_loss=0.18569238483905792, test/num_examples=2472, test/wer=0.060204, total_duration=42561.782649, train/ctc_loss=0.15914605557918549, train/wer=0.058082, validation/ctc_loss=0.3352296054363251, validation/num_examples=5348, validation/wer=0.096595
I1013 14:12:41.297912 140499940808448 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.9925174117088318, loss=0.9839208126068115
I1013 14:19:20.253859 140499949201152 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.000293493270874, loss=0.9855801463127136
I1013 14:25:37.502201 140499940808448 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.0503853559494019, loss=1.0916088819503784
I1013 14:31:11.714956 140673456789312 spec.py:321] Evaluating on the training split.
I1013 14:32:05.188101 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 14:32:54.862246 140673456789312 spec.py:349] Evaluating on the test split.
I1013 14:33:20.051774 140673456789312 submission_runner.py:393] Time since start: 44130.30s, 	Step: 52424, 	{'train/ctc_loss': Array(0.12840603, dtype=float32), 'train/wer': 0.04693906404637353, 'validation/ctc_loss': Array(0.32378438, dtype=float32), 'validation/wer': 0.09370806260077044, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17719635, dtype=float32), 'test/wer': 0.05945199358154084, 'test/num_examples': 2472, 'score': 40403.46730303764, 'total_duration': 44130.29601240158, 'accumulated_submission_time': 40403.46730303764, 'accumulated_eval_time': 3723.0491814613342, 'accumulated_logging_time': 1.4793915748596191}
I1013 14:33:20.090163 140499355272960 logging_writer.py:48] [52424] accumulated_eval_time=3723.049181, accumulated_logging_time=1.479392, accumulated_submission_time=40403.467303, global_step=52424, preemption_count=0, score=40403.467303, test/ctc_loss=0.17719635367393494, test/num_examples=2472, test/wer=0.059452, total_duration=44130.296012, train/ctc_loss=0.12840603291988373, train/wer=0.046939, validation/ctc_loss=0.32378438115119934, validation/num_examples=5348, validation/wer=0.093708
I1013 14:34:18.052783 140499346880256 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.9253617525100708, loss=1.0105794668197632
I1013 14:40:38.699859 140499355272960 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.0142383575439453, loss=1.011048436164856
I1013 14:47:15.482177 140499346880256 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.0159012079238892, loss=0.9999992251396179
I1013 14:53:39.788772 140499027592960 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.9564645886421204, loss=0.9815405607223511
I1013 14:57:20.546691 140673456789312 spec.py:321] Evaluating on the training split.
I1013 14:58:14.521447 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 14:59:04.051194 140673456789312 spec.py:349] Evaluating on the test split.
I1013 14:59:28.997452 140673456789312 submission_runner.py:393] Time since start: 45699.24s, 	Step: 54287, 	{'train/ctc_loss': Array(0.13690329, dtype=float32), 'train/wer': 0.050280512906852336, 'validation/ctc_loss': Array(0.31513014, dtype=float32), 'validation/wer': 0.09003929443795437, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16948064, dtype=float32), 'test/wer': 0.054617837629232426, 'test/num_examples': 2472, 'score': 41843.82804989815, 'total_duration': 45699.24114871025, 'accumulated_submission_time': 41843.82804989815, 'accumulated_eval_time': 3851.4934508800507, 'accumulated_logging_time': 1.535499095916748}
I1013 14:59:29.035990 140500747921152 logging_writer.py:48] [54287] accumulated_eval_time=3851.493451, accumulated_logging_time=1.535499, accumulated_submission_time=41843.828050, global_step=54287, preemption_count=0, score=41843.828050, test/ctc_loss=0.16948063671588898, test/num_examples=2472, test/wer=0.054618, total_duration=45699.241149, train/ctc_loss=0.1369032859802246, train/wer=0.050281, validation/ctc_loss=0.3151301443576813, validation/num_examples=5348, validation/wer=0.090039
I1013 15:02:10.413616 140500739528448 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.0743988752365112, loss=0.982031524181366
I1013 15:08:31.290128 140500747921152 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.1922991275787354, loss=0.9694363474845886
I1013 15:15:07.107146 140500739528448 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.2571943998336792, loss=0.9684764742851257
I1013 15:21:34.503471 140499877521152 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.0060614347457886, loss=1.0067675113677979
I1013 15:23:29.095745 140673456789312 spec.py:321] Evaluating on the training split.
I1013 15:24:22.536222 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 15:25:12.076651 140673456789312 spec.py:349] Evaluating on the test split.
I1013 15:25:36.835233 140673456789312 submission_runner.py:393] Time since start: 47267.08s, 	Step: 56153, 	{'train/ctc_loss': Array(0.1199391, dtype=float32), 'train/wer': 0.04340426902594056, 'validation/ctc_loss': Array(0.3065793, dtype=float32), 'validation/wer': 0.08796354403004528, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16503379, dtype=float32), 'test/wer': 0.054008490240285985, 'test/num_examples': 2472, 'score': 43283.79488635063, 'total_duration': 47267.07876253128, 'accumulated_submission_time': 43283.79488635063, 'accumulated_eval_time': 3979.2260196208954, 'accumulated_logging_time': 1.5886797904968262}
I1013 15:25:36.875154 140499078801152 logging_writer.py:48] [56153] accumulated_eval_time=3979.226020, accumulated_logging_time=1.588680, accumulated_submission_time=43283.794886, global_step=56153, preemption_count=0, score=43283.794886, test/ctc_loss=0.16503378748893738, test/num_examples=2472, test/wer=0.054008, total_duration=47267.078763, train/ctc_loss=0.11993909627199173, train/wer=0.043404, validation/ctc_loss=0.3065792918205261, validation/num_examples=5348, validation/wer=0.087964
I1013 15:29:59.266651 140499070408448 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.0518702268600464, loss=0.9929829835891724
I1013 15:36:19.888670 140500747921152 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.0825999975204468, loss=0.9714378714561462
I1013 15:42:44.321937 140500739528448 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.3382712602615356, loss=1.0199038982391357
I1013 15:49:15.573658 140499078801152 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.0725780725479126, loss=0.9876230359077454
I1013 15:49:37.149717 140673456789312 spec.py:321] Evaluating on the training split.
I1013 15:50:30.285418 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 15:51:19.718023 140673456789312 spec.py:349] Evaluating on the test split.
I1013 15:51:44.832176 140673456789312 submission_runner.py:393] Time since start: 48835.08s, 	Step: 58030, 	{'train/ctc_loss': Array(0.12203111, dtype=float32), 'train/wer': 0.044532919645472395, 'validation/ctc_loss': Array(0.3029603, dtype=float32), 'validation/wer': 0.08659258329551928, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16364382, dtype=float32), 'test/wer': 0.053216338634655615, 'test/num_examples': 2472, 'score': 44723.974480867386, 'total_duration': 48835.0761179924, 'accumulated_submission_time': 44723.974480867386, 'accumulated_eval_time': 4106.901967287064, 'accumulated_logging_time': 1.6432688236236572}
I1013 15:51:44.871089 140499662481152 logging_writer.py:48] [58030] accumulated_eval_time=4106.901967, accumulated_logging_time=1.643269, accumulated_submission_time=44723.974481, global_step=58030, preemption_count=0, score=44723.974481, test/ctc_loss=0.16364382207393646, test/num_examples=2472, test/wer=0.053216, total_duration=48835.076118, train/ctc_loss=0.12203110754489899, train/wer=0.044533, validation/ctc_loss=0.3029603064060211, validation/num_examples=5348, validation/wer=0.086593
I1013 15:57:40.165136 140499654088448 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.0909284353256226, loss=1.0204883813858032
I1013 16:04:04.833301 140500747921152 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.2721455097198486, loss=0.9733697772026062
I1013 16:10:29.127235 140500739528448 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.0489418506622314, loss=0.9612600207328796
I1013 16:15:44.840635 140673456789312 spec.py:321] Evaluating on the training split.
I1013 16:16:37.677958 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 16:17:26.974643 140673456789312 spec.py:349] Evaluating on the test split.
I1013 16:17:52.062585 140673456789312 submission_runner.py:393] Time since start: 50402.31s, 	Step: 59900, 	{'train/ctc_loss': Array(0.1180026, dtype=float32), 'train/wer': 0.04261145617667357, 'validation/ctc_loss': Array(0.3019864, dtype=float32), 'validation/wer': 0.08641879954043852, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16307755, dtype=float32), 'test/wer': 0.05268823756423537, 'test/num_examples': 2472, 'score': 46163.84957194328, 'total_duration': 50402.306914806366, 'accumulated_submission_time': 46163.84957194328, 'accumulated_eval_time': 4234.117990493774, 'accumulated_logging_time': 1.6976354122161865}
I1013 16:17:52.104220 140499078801152 logging_writer.py:48] [59900] accumulated_eval_time=4234.117990, accumulated_logging_time=1.697635, accumulated_submission_time=46163.849572, global_step=59900, preemption_count=0, score=46163.849572, test/ctc_loss=0.16307754814624786, test/num_examples=2472, test/wer=0.052688, total_duration=50402.306915, train/ctc_loss=0.11800260096788406, train/wer=0.042611, validation/ctc_loss=0.3019863963127136, validation/num_examples=5348, validation/wer=0.086419
I1013 16:19:06.896928 140673456789312 spec.py:321] Evaluating on the training split.
I1013 16:19:57.375527 140673456789312 spec.py:333] Evaluating on the validation split.
I1013 16:20:42.351038 140673456789312 spec.py:349] Evaluating on the test split.
I1013 16:21:04.811283 140673456789312 submission_runner.py:393] Time since start: 50595.06s, 	Step: 60000, 	{'train/ctc_loss': Array(0.12727168, dtype=float32), 'train/wer': 0.046409807355516634, 'validation/ctc_loss': Array(0.30198509, dtype=float32), 'validation/wer': 0.08641879954043852, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16307914, dtype=float32), 'test/wer': 0.05270854914386692, 'test/num_examples': 2472, 'score': 46238.62197804451, 'total_duration': 50595.05905365944, 'accumulated_submission_time': 46238.62197804451, 'accumulated_eval_time': 4352.029641866684, 'accumulated_logging_time': 1.754603385925293}
I1013 16:21:04.840022 140499078801152 logging_writer.py:48] [60000] accumulated_eval_time=4352.029642, accumulated_logging_time=1.754603, accumulated_submission_time=46238.621978, global_step=60000, preemption_count=0, score=46238.621978, test/ctc_loss=0.1630791425704956, test/num_examples=2472, test/wer=0.052709, total_duration=50595.059054, train/ctc_loss=0.12727168202400208, train/wer=0.046410, validation/ctc_loss=0.30198508501052856, validation/num_examples=5348, validation/wer=0.086419
I1013 16:21:04.861042 140499070408448 logging_writer.py:48] [60000] global_step=60000, preemption_count=0, score=46238.621978
I1013 16:21:05.336242 140673456789312 checkpoints.py:490] Saving checkpoint at step: 60000
I1013 16:21:06.784089 140673456789312 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_conformer/adamw_run12/librispeech_conformer_jax/trial_1/checkpoint_60000
I1013 16:21:06.818747 140673456789312 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_conformer/adamw_run12/librispeech_conformer_jax/trial_1/checkpoint_60000.
I1013 16:21:08.312084 140673456789312 submission_runner.py:563] Tuning trial 1/1
I1013 16:21:08.312334 140673456789312 submission_runner.py:564] Hyperparameters: Hyperparameters(learning_rate=0.002106913873888147, beta1=0.8231189937738506, beta2=0.8774571227688758, warmup_steps=1199, weight_decay=0.27590534177690645)
I1013 16:21:08.332248 140673456789312 submission_runner.py:565] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.26119, dtype=float32), 'train/wer': 1.1754107406867542, 'validation/ctc_loss': Array(30.911684, dtype=float32), 'validation/wer': 1.2526043426629465, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.850784, dtype=float32), 'test/wer': 1.2688034448439056, 'test/num_examples': 2472, 'score': 76.59682488441467, 'total_duration': 247.18804740905762, 'accumulated_submission_time': 76.59682488441467, 'accumulated_eval_time': 170.59115362167358, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1859, {'train/ctc_loss': Array(1.823575, dtype=float32), 'train/wer': 0.4623746163827728, 'validation/ctc_loss': Array(2.333378, dtype=float32), 'validation/wer': 0.5039921990403274, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.9621372, dtype=float32), 'test/wer': 0.4548981374281478, 'test/num_examples': 2472, 'score': 1517.1216580867767, 'total_duration': 1808.0695090293884, 'accumulated_submission_time': 1517.1216580867767, 'accumulated_eval_time': 290.83280849456787, 'accumulated_logging_time': 0.04293513298034668, 'global_step': 1859, 'preemption_count': 0}), (3759, {'train/ctc_loss': Array(0.6583183, dtype=float32), 'train/wer': 0.22827158191939909, 'validation/ctc_loss': Array(0.9888188, dtype=float32), 'validation/wer': 0.2907402222501134, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6888168, dtype=float32), 'test/wer': 0.2310035951495948, 'test/num_examples': 2472, 'score': 2957.25461769104, 'total_duration': 3372.0592234134674, 'accumulated_submission_time': 2957.25461769104, 'accumulated_eval_time': 414.56118083000183, 'accumulated_logging_time': 0.09320473670959473, 'global_step': 3759, 'preemption_count': 0}), (5662, {'train/ctc_loss': Array(0.5001109, dtype=float32), 'train/wer': 0.17819736335794972, 'validation/ctc_loss': Array(0.83646595, dtype=float32), 'validation/wer': 0.24894522915319037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.56430906, dtype=float32), 'test/wer': 0.18607438100461074, 'test/num_examples': 2472, 'score': 4397.406131744385, 'total_duration': 4936.801326990128, 'accumulated_submission_time': 4397.406131744385, 'accumulated_eval_time': 539.0001018047333, 'accumulated_logging_time': 0.1638789176940918, 'global_step': 5662, 'preemption_count': 0}), (7566, {'train/ctc_loss': Array(0.46576366, dtype=float32), 'train/wer': 0.16242184963713097, 'validation/ctc_loss': Array(0.7752064, dtype=float32), 'validation/wer': 0.23147030711451383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50696874, dtype=float32), 'test/wer': 0.16913452359189973, 'test/num_examples': 2472, 'score': 5837.976397037506, 'total_duration': 6504.11940073967, 'accumulated_submission_time': 5837.976397037506, 'accumulated_eval_time': 665.6183609962463, 'accumulated_logging_time': 0.2127223014831543, 'global_step': 7566, 'preemption_count': 0}), (9416, {'train/ctc_loss': Array(0.44879016, dtype=float32), 'train/wer': 0.1574235143186414, 'validation/ctc_loss': Array(0.7413137, dtype=float32), 'validation/wer': 0.22210529364627282, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48560467, dtype=float32), 'test/wer': 0.16393475920622347, 'test/num_examples': 2472, 'score': 7278.4315865039825, 'total_duration': 8070.162228822708, 'accumulated_submission_time': 7278.4315865039825, 'accumulated_eval_time': 791.0822207927704, 'accumulated_logging_time': 0.25879669189453125, 'global_step': 9416, 'preemption_count': 0}), (11273, {'train/ctc_loss': Array(0.451032, dtype=float32), 'train/wer': 0.15410960731071052, 'validation/ctc_loss': Array(0.7257357, dtype=float32), 'validation/wer': 0.2137443640962762, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46346053, dtype=float32), 'test/wer': 0.15408364308492273, 'test/num_examples': 2472, 'score': 8718.429273843765, 'total_duration': 9636.798786640167, 'accumulated_submission_time': 8718.429273843765, 'accumulated_eval_time': 917.5916821956635, 'accumulated_logging_time': 0.3092501163482666, 'global_step': 11273, 'preemption_count': 0}), (13159, {'train/ctc_loss': Array(0.40789133, dtype=float32), 'train/wer': 0.14170016803862762, 'validation/ctc_loss': Array(0.6824526, dtype=float32), 'validation/wer': 0.2025546211996872, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4475893, dtype=float32), 'test/wer': 0.14845733552698392, 'test/num_examples': 2472, 'score': 10158.496913194656, 'total_duration': 11203.180178165436, 'accumulated_submission_time': 10158.496913194656, 'accumulated_eval_time': 1043.7762591838837, 'accumulated_logging_time': 0.35781049728393555, 'global_step': 13159, 'preemption_count': 0}), (15041, {'train/ctc_loss': Array(0.36828592, dtype=float32), 'train/wer': 0.13233833433006578, 'validation/ctc_loss': Array(0.6705605, dtype=float32), 'validation/wer': 0.20003475675101615, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43684083, dtype=float32), 'test/wer': 0.14614181544898747, 'test/num_examples': 2472, 'score': 11598.443712949753, 'total_duration': 12769.64871764183, 'accumulated_submission_time': 11598.443712949753, 'accumulated_eval_time': 1170.1720054149628, 'accumulated_logging_time': 0.4033846855163574, 'global_step': 15041, 'preemption_count': 0}), (16924, {'train/ctc_loss': Array(0.34190282, dtype=float32), 'train/wer': 0.11814182125776058, 'validation/ctc_loss': Array(0.65478075, dtype=float32), 'validation/wer': 0.1915965899765392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4151378, dtype=float32), 'test/wer': 0.1373469014685272, 'test/num_examples': 2472, 'score': 13038.340107679367, 'total_duration': 14336.052446603775, 'accumulated_submission_time': 13038.340107679367, 'accumulated_eval_time': 1296.548362493515, 'accumulated_logging_time': 0.4521961212158203, 'global_step': 16924, 'preemption_count': 0}), (18800, {'train/ctc_loss': Array(0.35107782, dtype=float32), 'train/wer': 0.1257702435283455, 'validation/ctc_loss': Array(0.6287276, dtype=float32), 'validation/wer': 0.18711683095667958, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39819273, dtype=float32), 'test/wer': 0.13421891820526882, 'test/num_examples': 2472, 'score': 14478.491617918015, 'total_duration': 15902.633050203323, 'accumulated_submission_time': 14478.491617918015, 'accumulated_eval_time': 1422.827733039856, 'accumulated_logging_time': 0.520374059677124, 'global_step': 18800, 'preemption_count': 0}), (20674, {'train/ctc_loss': Array(0.35100877, dtype=float32), 'train/wer': 0.1229593590523282, 'validation/ctc_loss': Array(0.62338454, dtype=float32), 'validation/wer': 0.1874643984668411, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.390619, dtype=float32), 'test/wer': 0.13167997075132534, 'test/num_examples': 2472, 'score': 15919.052374124527, 'total_duration': 17471.21564912796, 'accumulated_submission_time': 15919.052374124527, 'accumulated_eval_time': 1550.7171084880829, 'accumulated_logging_time': 0.569277286529541, 'global_step': 20674, 'preemption_count': 0}), (22556, {'train/ctc_loss': Array(0.3452498, dtype=float32), 'train/wer': 0.1197615216186984, 'validation/ctc_loss': Array(0.60689515, dtype=float32), 'validation/wer': 0.17908415961072438, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37766132, dtype=float32), 'test/wer': 0.12617553267117584, 'test/num_examples': 2472, 'score': 17359.355275392532, 'total_duration': 19039.554783821106, 'accumulated_submission_time': 17359.355275392532, 'accumulated_eval_time': 1678.617175102234, 'accumulated_logging_time': 0.6200308799743652, 'global_step': 22556, 'preemption_count': 0}), (24432, {'train/ctc_loss': Array(0.3137263, dtype=float32), 'train/wer': 0.10891552628871283, 'validation/ctc_loss': Array(0.56719977, dtype=float32), 'validation/wer': 0.16800061789779586, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35258034, dtype=float32), 'test/wer': 0.11549164178498161, 'test/num_examples': 2472, 'score': 18799.559829235077, 'total_duration': 20611.24811911583, 'accumulated_submission_time': 18799.559829235077, 'accumulated_eval_time': 1809.9721534252167, 'accumulated_logging_time': 0.6716907024383545, 'global_step': 24432, 'preemption_count': 0}), (26302, {'train/ctc_loss': Array(0.2906842, dtype=float32), 'train/wer': 0.10294645496389511, 'validation/ctc_loss': Array(0.56886786, dtype=float32), 'validation/wer': 0.16756615851009393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34685555, dtype=float32), 'test/wer': 0.11388702699408933, 'test/num_examples': 2472, 'score': 20239.755044221878, 'total_duration': 22178.966100931168, 'accumulated_submission_time': 20239.755044221878, 'accumulated_eval_time': 1937.356656074524, 'accumulated_logging_time': 0.7264351844787598, 'global_step': 26302, 'preemption_count': 0}), (28176, {'train/ctc_loss': Array(0.27847216, dtype=float32), 'train/wer': 0.09994000886825426, 'validation/ctc_loss': Array(0.55278903, dtype=float32), 'validation/wer': 0.1636560240207768, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3358176, dtype=float32), 'test/wer': 0.11155119533646132, 'test/num_examples': 2472, 'score': 21680.22256922722, 'total_duration': 23747.363839149475, 'accumulated_submission_time': 21680.22256922722, 'accumulated_eval_time': 2065.1480174064636, 'accumulated_logging_time': 0.780937910079956, 'global_step': 28176, 'preemption_count': 0}), (30047, {'train/ctc_loss': Array(0.2547219, dtype=float32), 'train/wer': 0.09049206820018206, 'validation/ctc_loss': Array(0.52692395, dtype=float32), 'validation/wer': 0.15583575504214256, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3147888, dtype=float32), 'test/wer': 0.1048889972173136, 'test/num_examples': 2472, 'score': 23120.70231986046, 'total_duration': 25316.15892100334, 'accumulated_submission_time': 23120.70231986046, 'accumulated_eval_time': 2193.3251678943634, 'accumulated_logging_time': 0.8369204998016357, 'global_step': 30047, 'preemption_count': 0}), (31910, {'train/ctc_loss': Array(0.26242533, dtype=float32), 'train/wer': 0.09455169873461605, 'validation/ctc_loss': Array(0.5106409, dtype=float32), 'validation/wer': 0.15240835320582755, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30022863, dtype=float32), 'test/wer': 0.10060325391505698, 'test/num_examples': 2472, 'score': 24561.27951812744, 'total_duration': 26883.44611763954, 'accumulated_submission_time': 24561.27951812744, 'accumulated_eval_time': 2319.904929161072, 'accumulated_logging_time': 0.8843526840209961, 'global_step': 31910, 'preemption_count': 0}), (33787, {'train/ctc_loss': Array(0.24002926, dtype=float32), 'train/wer': 0.08474272784985268, 'validation/ctc_loss': Array(0.49889132, dtype=float32), 'validation/wer': 0.1487878583083117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29228598, dtype=float32), 'test/wer': 0.09767838644811407, 'test/num_examples': 2472, 'score': 26001.471496343613, 'total_duration': 28451.03255057335, 'accumulated_submission_time': 26001.471496343613, 'accumulated_eval_time': 2447.160432100296, 'accumulated_logging_time': 0.9408268928527832, 'global_step': 33787, 'preemption_count': 0}), (35661, {'train/ctc_loss': Array(0.24271333, dtype=float32), 'train/wer': 0.08625924004335564, 'validation/ctc_loss': Array(0.47414345, dtype=float32), 'validation/wer': 0.14084207883989688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27732986, dtype=float32), 'test/wer': 0.0929051652347003, 'test/num_examples': 2472, 'score': 27441.413339614868, 'total_duration': 30019.650795936584, 'accumulated_submission_time': 27441.413339614868, 'accumulated_eval_time': 2575.7031786441803, 'accumulated_logging_time': 0.9906232357025146, 'global_step': 35661, 'preemption_count': 0}), (37536, {'train/ctc_loss': Array(0.22554836, dtype=float32), 'train/wer': 0.07781578694971067, 'validation/ctc_loss': Array(0.4519426, dtype=float32), 'validation/wer': 0.133562470432625, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2644897, dtype=float32), 'test/wer': 0.08953344301586334, 'test/num_examples': 2472, 'score': 28882.021570444107, 'total_duration': 31587.90259885788, 'accumulated_submission_time': 28882.021570444107, 'accumulated_eval_time': 2703.2102982997894, 'accumulated_logging_time': 1.041644811630249, 'global_step': 37536, 'preemption_count': 0}), (39407, {'train/ctc_loss': Array(0.16926204, dtype=float32), 'train/wer': 0.061350101985383806, 'validation/ctc_loss': Array(0.440538, dtype=float32), 'validation/wer': 0.1305695279840119, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25084078, dtype=float32), 'test/wer': 0.08382588913939837, 'test/num_examples': 2472, 'score': 30321.969761371613, 'total_duration': 33156.20608329773, 'accumulated_submission_time': 30321.969761371613, 'accumulated_eval_time': 2831.430045604706, 'accumulated_logging_time': 1.0932466983795166, 'global_step': 39407, 'preemption_count': 0}), (41259, {'train/ctc_loss': Array(0.18502629, dtype=float32), 'train/wer': 0.06609152921796795, 'validation/ctc_loss': Array(0.4219626, dtype=float32), 'validation/wer': 0.123058207903299, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23841603, dtype=float32), 'test/wer': 0.08023073954461439, 'test/num_examples': 2472, 'score': 31762.34598684311, 'total_duration': 34725.67310214043, 'accumulated_submission_time': 31762.34598684311, 'accumulated_eval_time': 2960.381998062134, 'accumulated_logging_time': 1.14918851852417, 'global_step': 41259, 'preemption_count': 0}), (43117, {'train/ctc_loss': Array(0.21619506, dtype=float32), 'train/wer': 0.07624089432261354, 'validation/ctc_loss': Array(0.39728707, dtype=float32), 'validation/wer': 0.11843362908753874, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22376235, dtype=float32), 'test/wer': 0.07509190989783275, 'test/num_examples': 2472, 'score': 33203.12744092941, 'total_duration': 36295.1917412281, 'accumulated_submission_time': 33203.12744092941, 'accumulated_eval_time': 3088.981956243515, 'accumulated_logging_time': 1.2015783786773682, 'global_step': 43117, 'preemption_count': 0}), (44983, {'train/ctc_loss': Array(0.20942391, dtype=float32), 'train/wer': 0.07494851141075959, 'validation/ctc_loss': Array(0.3889882, dtype=float32), 'validation/wer': 0.1151607017001844, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21613249, dtype=float32), 'test/wer': 0.07088741291410233, 'test/num_examples': 2472, 'score': 34643.02970933914, 'total_duration': 37862.0554959774, 'accumulated_submission_time': 34643.02970933914, 'accumulated_eval_time': 3215.804906845093, 'accumulated_logging_time': 1.2567241191864014, 'global_step': 44983, 'preemption_count': 0}), (46849, {'train/ctc_loss': Array(0.22263704, dtype=float32), 'train/wer': 0.08061959274879936, 'validation/ctc_loss': Array(0.37133935, dtype=float32), 'validation/wer': 0.10822866080307404, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20644823, dtype=float32), 'test/wer': 0.06905937074726301, 'test/num_examples': 2472, 'score': 36083.39652848244, 'total_duration': 39428.10625362396, 'accumulated_submission_time': 36083.39652848244, 'accumulated_eval_time': 3341.3469812870026, 'accumulated_logging_time': 1.3147220611572266, 'global_step': 46849, 'preemption_count': 0}), (48715, {'train/ctc_loss': Array(0.1854418, dtype=float32), 'train/wer': 0.06379195638478884, 'validation/ctc_loss': Array(0.3513036, dtype=float32), 'validation/wer': 0.10312134933431168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19208613, dtype=float32), 'test/wer': 0.063758048463429, 'test/num_examples': 2472, 'score': 37523.32898855209, 'total_duration': 40993.73630857468, 'accumulated_submission_time': 37523.32898855209, 'accumulated_eval_time': 3466.8984401226044, 'accumulated_logging_time': 1.3741002082824707, 'global_step': 48715, 'preemption_count': 0}), (50564, {'train/ctc_loss': Array(0.15914606, dtype=float32), 'train/wer': 0.05808179216817164, 'validation/ctc_loss': Array(0.3352296, dtype=float32), 'validation/wer': 0.09659480386572308, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18569238, dtype=float32), 'test/wer': 0.06020352202790811, 'test/num_examples': 2472, 'score': 38963.41722846031, 'total_duration': 42561.78264927864, 'accumulated_submission_time': 38963.41722846031, 'accumulated_eval_time': 3594.718602180481, 'accumulated_logging_time': 1.4291489124298096, 'global_step': 50564, 'preemption_count': 0}), (52424, {'train/ctc_loss': Array(0.12840603, dtype=float32), 'train/wer': 0.04693906404637353, 'validation/ctc_loss': Array(0.32378438, dtype=float32), 'validation/wer': 0.09370806260077044, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17719635, dtype=float32), 'test/wer': 0.05945199358154084, 'test/num_examples': 2472, 'score': 40403.46730303764, 'total_duration': 44130.29601240158, 'accumulated_submission_time': 40403.46730303764, 'accumulated_eval_time': 3723.0491814613342, 'accumulated_logging_time': 1.4793915748596191, 'global_step': 52424, 'preemption_count': 0}), (54287, {'train/ctc_loss': Array(0.13690329, dtype=float32), 'train/wer': 0.050280512906852336, 'validation/ctc_loss': Array(0.31513014, dtype=float32), 'validation/wer': 0.09003929443795437, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16948064, dtype=float32), 'test/wer': 0.054617837629232426, 'test/num_examples': 2472, 'score': 41843.82804989815, 'total_duration': 45699.24114871025, 'accumulated_submission_time': 41843.82804989815, 'accumulated_eval_time': 3851.4934508800507, 'accumulated_logging_time': 1.535499095916748, 'global_step': 54287, 'preemption_count': 0}), (56153, {'train/ctc_loss': Array(0.1199391, dtype=float32), 'train/wer': 0.04340426902594056, 'validation/ctc_loss': Array(0.3065793, dtype=float32), 'validation/wer': 0.08796354403004528, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16503379, dtype=float32), 'test/wer': 0.054008490240285985, 'test/num_examples': 2472, 'score': 43283.79488635063, 'total_duration': 47267.07876253128, 'accumulated_submission_time': 43283.79488635063, 'accumulated_eval_time': 3979.2260196208954, 'accumulated_logging_time': 1.5886797904968262, 'global_step': 56153, 'preemption_count': 0}), (58030, {'train/ctc_loss': Array(0.12203111, dtype=float32), 'train/wer': 0.044532919645472395, 'validation/ctc_loss': Array(0.3029603, dtype=float32), 'validation/wer': 0.08659258329551928, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16364382, dtype=float32), 'test/wer': 0.053216338634655615, 'test/num_examples': 2472, 'score': 44723.974480867386, 'total_duration': 48835.0761179924, 'accumulated_submission_time': 44723.974480867386, 'accumulated_eval_time': 4106.901967287064, 'accumulated_logging_time': 1.6432688236236572, 'global_step': 58030, 'preemption_count': 0}), (59900, {'train/ctc_loss': Array(0.1180026, dtype=float32), 'train/wer': 0.04261145617667357, 'validation/ctc_loss': Array(0.3019864, dtype=float32), 'validation/wer': 0.08641879954043852, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16307755, dtype=float32), 'test/wer': 0.05268823756423537, 'test/num_examples': 2472, 'score': 46163.84957194328, 'total_duration': 50402.306914806366, 'accumulated_submission_time': 46163.84957194328, 'accumulated_eval_time': 4234.117990493774, 'accumulated_logging_time': 1.6976354122161865, 'global_step': 59900, 'preemption_count': 0}), (60000, {'train/ctc_loss': Array(0.12727168, dtype=float32), 'train/wer': 0.046409807355516634, 'validation/ctc_loss': Array(0.30198509, dtype=float32), 'validation/wer': 0.08641879954043852, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16307914, dtype=float32), 'test/wer': 0.05270854914386692, 'test/num_examples': 2472, 'score': 46238.62197804451, 'total_duration': 50595.05905365944, 'accumulated_submission_time': 46238.62197804451, 'accumulated_eval_time': 4352.029641866684, 'accumulated_logging_time': 1.754603385925293, 'global_step': 60000, 'preemption_count': 0})], 'global_step': 60000}
I1013 16:21:08.332537 140673456789312 submission_runner.py:566] Timing: 46238.62197804451
I1013 16:21:08.332622 140673456789312 submission_runner.py:568] Total number of evals: 34
I1013 16:21:08.332673 140673456789312 submission_runner.py:569] ====================
I1013 16:21:08.336422 140673456789312 submission_runner.py:645] Final librispeech_conformer score: 46238.62197804451
