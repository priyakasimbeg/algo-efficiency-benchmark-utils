python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=targets_check_conformer/adamw_run13 --overwrite=true --save_checkpoints=false --max_global_steps=60000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_10-20-2023-21-54-24.log
I1020 21:54:47.289810 140198087026496 logger_utils.py:61] Removing existing experiment directory /experiment_runs/targets_check_conformer/adamw_run13/librispeech_conformer_jax because --overwrite was set.
I1020 21:54:47.311114 140198087026496 logger_utils.py:76] Creating experiment directory at /experiment_runs/targets_check_conformer/adamw_run13/librispeech_conformer_jax.
I1020 21:54:48.367652 140198087026496 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I1020 21:54:48.368290 140198087026496 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1020 21:54:48.368427 140198087026496 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I1020 21:54:48.383627 140198087026496 submission_runner.py:525] Using RNG seed 972042339
I1020 21:54:54.083237 140198087026496 submission_runner.py:534] --- Tuning run 1/1 ---
I1020 21:54:54.083443 140198087026496 submission_runner.py:539] Creating tuning directory at /experiment_runs/targets_check_conformer/adamw_run13/librispeech_conformer_jax/trial_1.
I1020 21:54:54.083669 140198087026496 logger_utils.py:92] Saving hparams to /experiment_runs/targets_check_conformer/adamw_run13/librispeech_conformer_jax/trial_1/hparams.json.
I1020 21:54:54.267084 140198087026496 submission_runner.py:202] Initializing dataset.
I1020 21:54:54.267298 140198087026496 submission_runner.py:209] Initializing model.
I1020 21:54:59.209709 140198087026496 submission_runner.py:243] Initializing optimizer.
I1020 21:55:00.473855 140198087026496 submission_runner.py:250] Initializing metrics bundle.
I1020 21:55:00.474083 140198087026496 submission_runner.py:268] Initializing checkpoint and logger.
I1020 21:55:00.475392 140198087026496 checkpoints.py:915] Found no checkpoint files in /experiment_runs/targets_check_conformer/adamw_run13/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I1020 21:55:00.475528 140198087026496 submission_runner.py:288] Saving meta data to /experiment_runs/targets_check_conformer/adamw_run13/librispeech_conformer_jax/trial_1/meta_data_0.json.
I1020 21:55:00.475718 140198087026496 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I1020 21:55:00.475799 140198087026496 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I1020 21:55:01.293493 140198087026496 submission_runner.py:291] Saving flags to /experiment_runs/targets_check_conformer/adamw_run13/librispeech_conformer_jax/trial_1/flags_0.json.
I1020 21:55:01.308179 140198087026496 submission_runner.py:301] Starting training loop.
I1020 21:55:01.604143 140198087026496 input_pipeline.py:20] Loading split = train-clean-100
I1020 21:55:01.643434 140198087026496 input_pipeline.py:20] Loading split = train-clean-360
I1020 21:55:02.085708 140198087026496 input_pipeline.py:20] Loading split = train-other-500
2023-10-20 21:56:11.900921: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-10-20 21:56:15.011505: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I1020 21:56:16.979787 140020958619392 logging_writer.py:48] [0] global_step=0, grad_norm=34.18340301513672, loss=32.30278778076172
I1020 21:56:17.022234 140198087026496 spec.py:321] Evaluating on the training split.
I1020 21:56:17.192414 140198087026496 input_pipeline.py:20] Loading split = train-clean-100
I1020 21:56:17.227677 140198087026496 input_pipeline.py:20] Loading split = train-clean-360
I1020 21:56:17.634227 140198087026496 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I1020 21:57:32.170670 140198087026496 spec.py:333] Evaluating on the validation split.
I1020 21:57:32.289875 140198087026496 input_pipeline.py:20] Loading split = dev-clean
I1020 21:57:32.295162 140198087026496 input_pipeline.py:20] Loading split = dev-other
I1020 21:58:35.697040 140198087026496 spec.py:349] Evaluating on the test split.
I1020 21:58:35.817289 140198087026496 input_pipeline.py:20] Loading split = test-clean
I1020 21:59:13.583726 140198087026496 submission_runner.py:395] Time since start: 252.27s, 	Step: 1, 	{'train/ctc_loss': Array(31.425308, dtype=float32), 'train/wer': 1.0707994086167045, 'validation/ctc_loss': Array(30.578106, dtype=float32), 'validation/wer': 1.0319472469756799, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.617601, dtype=float32), 'test/wer': 1.043365222513355, 'test/num_examples': 2472, 'score': 75.71397948265076, 'total_duration': 252.2725167274475, 'accumulated_submission_time': 75.71397948265076, 'accumulated_eval_time': 176.55847454071045, 'accumulated_logging_time': 0}
I1020 21:59:13.620666 140015757682432 logging_writer.py:48] [1] accumulated_eval_time=176.558475, accumulated_logging_time=0, accumulated_submission_time=75.713979, global_step=1, preemption_count=0, score=75.713979, test/ctc_loss=30.61760139465332, test/num_examples=2472, test/wer=1.043365, total_duration=252.272517, train/ctc_loss=31.425308227539062, train/wer=1.070799, validation/ctc_loss=30.578105926513672, validation/num_examples=5348, validation/wer=1.031947
I1020 21:59:38.897119 140023483279104 logging_writer.py:48] [1] global_step=1, grad_norm=34.63112258911133, loss=32.070125579833984
I1020 21:59:39.732193 140023491671808 logging_writer.py:48] [2] global_step=2, grad_norm=40.09004211425781, loss=31.36461639404297
I1020 21:59:40.551163 140023483279104 logging_writer.py:48] [3] global_step=3, grad_norm=40.49196243286133, loss=31.031938552856445
I1020 21:59:41.377915 140023491671808 logging_writer.py:48] [4] global_step=4, grad_norm=45.50988006591797, loss=31.174108505249023
I1020 21:59:42.279394 140023483279104 logging_writer.py:48] [5] global_step=5, grad_norm=41.30473709106445, loss=30.637174606323242
I1020 21:59:43.170097 140023491671808 logging_writer.py:48] [6] global_step=6, grad_norm=41.318878173828125, loss=29.669498443603516
I1020 21:59:44.066519 140023483279104 logging_writer.py:48] [7] global_step=7, grad_norm=39.54635238647461, loss=29.785938262939453
I1020 21:59:44.975021 140023491671808 logging_writer.py:48] [8] global_step=8, grad_norm=42.959720611572266, loss=28.66201400756836
I1020 21:59:45.871958 140023483279104 logging_writer.py:48] [9] global_step=9, grad_norm=54.46258544921875, loss=27.682985305786133
I1020 21:59:46.756618 140023491671808 logging_writer.py:48] [10] global_step=10, grad_norm=85.49012756347656, loss=27.327465057373047
I1020 21:59:47.648983 140023483279104 logging_writer.py:48] [11] global_step=11, grad_norm=91.66244506835938, loss=23.76844596862793
I1020 21:59:48.542865 140023491671808 logging_writer.py:48] [12] global_step=12, grad_norm=96.09396362304688, loss=21.176828384399414
I1020 21:59:49.437826 140023483279104 logging_writer.py:48] [13] global_step=13, grad_norm=105.52237701416016, loss=18.341888427734375
I1020 21:59:50.321793 140023491671808 logging_writer.py:48] [14] global_step=14, grad_norm=143.79505920410156, loss=14.172826766967773
I1020 21:59:51.209485 140023483279104 logging_writer.py:48] [15] global_step=15, grad_norm=91.01752471923828, loss=9.901986122131348
I1020 21:59:52.107031 140023491671808 logging_writer.py:48] [16] global_step=16, grad_norm=32.84461212158203, loss=7.673954963684082
I1020 21:59:52.997131 140023483279104 logging_writer.py:48] [17] global_step=17, grad_norm=3.8568451404571533, loss=7.1973395347595215
I1020 21:59:53.879080 140023491671808 logging_writer.py:48] [18] global_step=18, grad_norm=15.027223587036133, loss=7.402409076690674
I1020 21:59:54.768354 140023483279104 logging_writer.py:48] [19] global_step=19, grad_norm=19.675100326538086, loss=7.707673072814941
I1020 21:59:55.663944 140023491671808 logging_writer.py:48] [20] global_step=20, grad_norm=21.170433044433594, loss=7.924647331237793
I1020 21:59:56.551057 140023483279104 logging_writer.py:48] [21] global_step=21, grad_norm=21.634729385375977, loss=7.959529876708984
I1020 21:59:57.434255 140023491671808 logging_writer.py:48] [22] global_step=22, grad_norm=21.401552200317383, loss=7.893836975097656
I1020 21:59:58.325354 140023483279104 logging_writer.py:48] [23] global_step=23, grad_norm=20.048601150512695, loss=7.706181526184082
I1020 21:59:59.221598 140023491671808 logging_writer.py:48] [24] global_step=24, grad_norm=17.33611488342285, loss=7.453180313110352
I1020 22:00:00.111759 140023483279104 logging_writer.py:48] [25] global_step=25, grad_norm=12.08920955657959, loss=7.188755035400391
I1020 22:00:01.006231 140023491671808 logging_writer.py:48] [26] global_step=26, grad_norm=2.6391804218292236, loss=7.043387413024902
I1020 22:00:01.923108 140023483279104 logging_writer.py:48] [27] global_step=27, grad_norm=13.428110122680664, loss=7.120095729827881
I1020 22:00:02.824587 140023491671808 logging_writer.py:48] [28] global_step=28, grad_norm=22.07063865661621, loss=7.2552900314331055
I1020 22:00:03.719337 140023483279104 logging_writer.py:48] [29] global_step=29, grad_norm=19.93694496154785, loss=7.1987714767456055
I1020 22:00:04.630673 140023491671808 logging_writer.py:48] [30] global_step=30, grad_norm=9.82608413696289, loss=6.990880966186523
I1020 22:00:05.530677 140023483279104 logging_writer.py:48] [31] global_step=31, grad_norm=2.2970991134643555, loss=6.886231899261475
I1020 22:00:06.433741 140023491671808 logging_writer.py:48] [32] global_step=32, grad_norm=6.5624613761901855, loss=6.8828816413879395
I1020 22:00:07.329363 140023483279104 logging_writer.py:48] [33] global_step=33, grad_norm=8.724026679992676, loss=6.863595485687256
I1020 22:00:08.219073 140023491671808 logging_writer.py:48] [34] global_step=34, grad_norm=9.20134162902832, loss=6.859122276306152
I1020 22:00:09.106845 140023483279104 logging_writer.py:48] [35] global_step=35, grad_norm=7.93311882019043, loss=6.764463901519775
I1020 22:00:10.003682 140023491671808 logging_writer.py:48] [36] global_step=36, grad_norm=4.010095119476318, loss=6.6818132400512695
I1020 22:00:10.903699 140023483279104 logging_writer.py:48] [37] global_step=37, grad_norm=2.9184067249298096, loss=6.64161491394043
I1020 22:00:11.795253 140023491671808 logging_writer.py:48] [38] global_step=38, grad_norm=7.800067901611328, loss=6.6486711502075195
I1020 22:00:12.679968 140023483279104 logging_writer.py:48] [39] global_step=39, grad_norm=8.346601486206055, loss=6.597320556640625
I1020 22:00:13.573211 140023491671808 logging_writer.py:48] [40] global_step=40, grad_norm=5.724184513092041, loss=6.545501232147217
I1020 22:00:14.464167 140023483279104 logging_writer.py:48] [41] global_step=41, grad_norm=1.636404275894165, loss=6.479489326477051
I1020 22:00:15.361089 140023491671808 logging_writer.py:48] [42] global_step=42, grad_norm=3.887241840362549, loss=6.4587836265563965
I1020 22:00:16.262010 140023483279104 logging_writer.py:48] [43] global_step=43, grad_norm=4.700568199157715, loss=6.435420036315918
I1020 22:00:17.154472 140023491671808 logging_writer.py:48] [44] global_step=44, grad_norm=2.96838116645813, loss=6.3794779777526855
I1020 22:00:18.053091 140023483279104 logging_writer.py:48] [45] global_step=45, grad_norm=2.214916229248047, loss=6.341150760650635
I1020 22:00:18.940714 140023491671808 logging_writer.py:48] [46] global_step=46, grad_norm=5.001911163330078, loss=6.3507914543151855
I1020 22:00:19.839609 140023483279104 logging_writer.py:48] [47] global_step=47, grad_norm=3.696503162384033, loss=6.281836986541748
I1020 22:00:20.733740 140023491671808 logging_writer.py:48] [48] global_step=48, grad_norm=1.1830527782440186, loss=6.259160995483398
I1020 22:00:21.629402 140023483279104 logging_writer.py:48] [49] global_step=49, grad_norm=2.9525251388549805, loss=6.229088306427002
I1020 22:00:22.522008 140023491671808 logging_writer.py:48] [50] global_step=50, grad_norm=2.242661237716675, loss=6.204019069671631
I1020 22:00:23.422590 140023483279104 logging_writer.py:48] [51] global_step=51, grad_norm=1.6122764348983765, loss=6.167461395263672
I1020 22:00:24.319994 140023491671808 logging_writer.py:48] [52] global_step=52, grad_norm=3.971242666244507, loss=6.173884868621826
I1020 22:00:25.219255 140023483279104 logging_writer.py:48] [53] global_step=53, grad_norm=0.8256471753120422, loss=6.128248691558838
I1020 22:00:26.106275 140023491671808 logging_writer.py:48] [54] global_step=54, grad_norm=2.817639112472534, loss=6.099496841430664
I1020 22:00:27.002488 140023483279104 logging_writer.py:48] [55] global_step=55, grad_norm=0.9653855562210083, loss=6.0682053565979
I1020 22:00:27.904021 140023491671808 logging_writer.py:48] [56] global_step=56, grad_norm=3.0128791332244873, loss=6.077550411224365
I1020 22:00:28.805776 140023483279104 logging_writer.py:48] [57] global_step=57, grad_norm=0.6608229279518127, loss=6.046165943145752
I1020 22:00:29.698175 140023491671808 logging_writer.py:48] [58] global_step=58, grad_norm=2.1777825355529785, loss=6.029109477996826
I1020 22:00:30.592337 140023483279104 logging_writer.py:48] [59] global_step=59, grad_norm=1.2073103189468384, loss=6.022900581359863
I1020 22:00:31.485010 140023491671808 logging_writer.py:48] [60] global_step=60, grad_norm=2.1282670497894287, loss=6.0004563331604
I1020 22:00:32.383984 140023483279104 logging_writer.py:48] [61] global_step=61, grad_norm=2.7462821006774902, loss=5.9772047996521
I1020 22:00:33.278413 140023491671808 logging_writer.py:48] [62] global_step=62, grad_norm=0.5306000113487244, loss=5.9620490074157715
I1020 22:00:34.173236 140023483279104 logging_writer.py:48] [63] global_step=63, grad_norm=2.8718347549438477, loss=5.953521251678467
I1020 22:00:35.067348 140023491671808 logging_writer.py:48] [64] global_step=64, grad_norm=2.054697275161743, loss=5.956791877746582
I1020 22:00:35.973258 140023483279104 logging_writer.py:48] [65] global_step=65, grad_norm=1.3328311443328857, loss=5.91449499130249
I1020 22:00:36.869878 140023491671808 logging_writer.py:48] [66] global_step=66, grad_norm=5.044370174407959, loss=5.920438289642334
I1020 22:00:37.761202 140023483279104 logging_writer.py:48] [67] global_step=67, grad_norm=5.610871315002441, loss=5.933282852172852
I1020 22:00:38.659843 140023491671808 logging_writer.py:48] [68] global_step=68, grad_norm=1.0489662885665894, loss=5.899042129516602
I1020 22:00:39.555900 140023483279104 logging_writer.py:48] [69] global_step=69, grad_norm=5.014947891235352, loss=5.915654182434082
I1020 22:00:40.446720 140023491671808 logging_writer.py:48] [70] global_step=70, grad_norm=6.016120910644531, loss=5.917578220367432
I1020 22:00:41.337767 140023483279104 logging_writer.py:48] [71] global_step=71, grad_norm=0.4102917015552521, loss=5.859676837921143
I1020 22:00:42.234069 140023491671808 logging_writer.py:48] [72] global_step=72, grad_norm=7.7428107261657715, loss=5.891017913818359
I1020 22:00:43.131469 140023483279104 logging_writer.py:48] [73] global_step=73, grad_norm=6.235257625579834, loss=5.893691062927246
I1020 22:00:44.021766 140023491671808 logging_writer.py:48] [74] global_step=74, grad_norm=3.572563409805298, loss=5.874323844909668
I1020 22:00:44.914998 140023483279104 logging_writer.py:48] [75] global_step=75, grad_norm=7.9039106369018555, loss=5.888672828674316
I1020 22:00:45.810840 140023491671808 logging_writer.py:48] [76] global_step=76, grad_norm=1.9090992212295532, loss=5.828106880187988
I1020 22:00:46.707805 140023483279104 logging_writer.py:48] [77] global_step=77, grad_norm=4.505623817443848, loss=5.861199855804443
I1020 22:00:47.594683 140023491671808 logging_writer.py:48] [78] global_step=78, grad_norm=2.9605376720428467, loss=5.8341779708862305
I1020 22:00:48.479746 140023483279104 logging_writer.py:48] [79] global_step=79, grad_norm=2.8534207344055176, loss=5.859068870544434
I1020 22:00:49.375844 140023491671808 logging_writer.py:48] [80] global_step=80, grad_norm=3.9015917778015137, loss=5.859179496765137
I1020 22:00:50.272018 140023483279104 logging_writer.py:48] [81] global_step=81, grad_norm=0.5270567536354065, loss=5.830728054046631
I1020 22:00:51.164331 140023491671808 logging_writer.py:48] [82] global_step=82, grad_norm=3.5327138900756836, loss=5.849173545837402
I1020 22:00:52.060396 140023483279104 logging_writer.py:48] [83] global_step=83, grad_norm=1.4779750108718872, loss=5.831887722015381
I1020 22:00:52.948721 140023491671808 logging_writer.py:48] [84] global_step=84, grad_norm=2.483452796936035, loss=5.819234371185303
I1020 22:00:53.838713 140023483279104 logging_writer.py:48] [85] global_step=85, grad_norm=3.49619197845459, loss=5.824854373931885
I1020 22:00:54.735032 140023491671808 logging_writer.py:48] [86] global_step=86, grad_norm=0.8580846190452576, loss=5.808700084686279
I1020 22:00:55.625492 140023483279104 logging_writer.py:48] [87] global_step=87, grad_norm=1.690561294555664, loss=5.8437628746032715
I1020 22:00:56.515161 140023491671808 logging_writer.py:48] [88] global_step=88, grad_norm=2.408299446105957, loss=5.860200881958008
I1020 22:00:57.402156 140023483279104 logging_writer.py:48] [89] global_step=89, grad_norm=2.5025339126586914, loss=5.836832046508789
I1020 22:00:58.301061 140023491671808 logging_writer.py:48] [90] global_step=90, grad_norm=1.8292559385299683, loss=5.825008392333984
I1020 22:00:59.191388 140023483279104 logging_writer.py:48] [91] global_step=91, grad_norm=0.3064000606536865, loss=5.845145225524902
I1020 22:01:00.082818 140023491671808 logging_writer.py:48] [92] global_step=92, grad_norm=2.2304093837738037, loss=5.828293323516846
I1020 22:01:00.965879 140023483279104 logging_writer.py:48] [93] global_step=93, grad_norm=5.3627543449401855, loss=5.845363616943359
I1020 22:01:01.861843 140023491671808 logging_writer.py:48] [94] global_step=94, grad_norm=6.990790843963623, loss=5.880906581878662
I1020 22:01:02.774706 140023483279104 logging_writer.py:48] [95] global_step=95, grad_norm=2.4359257221221924, loss=5.833540439605713
I1020 22:01:03.659645 140023491671808 logging_writer.py:48] [96] global_step=96, grad_norm=5.248764514923096, loss=5.853601455688477
I1020 22:01:04.544401 140023483279104 logging_writer.py:48] [97] global_step=97, grad_norm=7.287499904632568, loss=5.874724388122559
I1020 22:01:05.432352 140023491671808 logging_writer.py:48] [98] global_step=98, grad_norm=0.5003166794776917, loss=5.819962501525879
I1020 22:01:06.321966 140023483279104 logging_writer.py:48] [99] global_step=99, grad_norm=9.267555236816406, loss=5.8933329582214355
I1020 22:01:07.209434 140023491671808 logging_writer.py:48] [100] global_step=100, grad_norm=5.37648868560791, loss=5.849436283111572
I1020 22:06:11.686798 140023483279104 logging_writer.py:48] [500] global_step=500, grad_norm=0.8225256204605103, loss=4.784429550170898
I1020 22:12:41.424141 140023491671808 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8246164321899414, loss=3.0048460960388184
I1020 22:19:05.741322 140025516500736 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.1405531167984009, loss=2.474395751953125
I1020 22:23:13.701825 140198087026496 spec.py:321] Evaluating on the training split.
I1020 22:24:07.508743 140198087026496 spec.py:333] Evaluating on the validation split.
I1020 22:24:57.360139 140198087026496 spec.py:349] Evaluating on the test split.
I1020 22:25:22.589739 140198087026496 submission_runner.py:395] Time since start: 1821.28s, 	Step: 1820, 	{'train/ctc_loss': Array(2.193813, dtype=float32), 'train/wer': 0.49749747687998186, 'validation/ctc_loss': Array(2.696645, dtype=float32), 'validation/wer': 0.5468974772391554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.3071384, dtype=float32), 'test/wer': 0.49781650518960857, 'test/num_examples': 2472, 'score': 1515.7065501213074, 'total_duration': 1821.2753655910492, 'accumulated_submission_time': 1515.7065501213074, 'accumulated_eval_time': 305.4402585029602, 'accumulated_logging_time': 0.05211830139160156}
I1020 22:25:22.630291 140025516500736 logging_writer.py:48] [1820] accumulated_eval_time=305.440259, accumulated_logging_time=0.052118, accumulated_submission_time=1515.706550, global_step=1820, preemption_count=0, score=1515.706550, test/ctc_loss=2.307138442993164, test/num_examples=2472, test/wer=0.497817, total_duration=1821.275366, train/ctc_loss=2.1938130855560303, train/wer=0.497497, validation/ctc_loss=2.6966450214385986, validation/num_examples=5348, validation/wer=0.546897
I1020 22:27:40.075261 140025508108032 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7194327116012573, loss=2.2921180725097656
I1020 22:34:03.535066 140024205780736 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.3937568664550781, loss=2.102524757385254
I1020 22:40:42.218973 140024197388032 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0166966915130615, loss=2.051645040512085
I1020 22:47:11.681674 140023550420736 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9280899167060852, loss=2.0882277488708496
I1020 22:49:22.714486 140198087026496 spec.py:321] Evaluating on the training split.
I1020 22:50:15.338755 140198087026496 spec.py:333] Evaluating on the validation split.
I1020 22:51:05.722459 140198087026496 spec.py:349] Evaluating on the test split.
I1020 22:51:32.048160 140198087026496 submission_runner.py:395] Time since start: 3390.73s, 	Step: 3671, 	{'train/ctc_loss': Array(0.716506, dtype=float32), 'train/wer': 0.24138188210851558, 'validation/ctc_loss': Array(1.0385997, dtype=float32), 'validation/wer': 0.30102242775905846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.749395, dtype=float32), 'test/wer': 0.2435155282026283, 'test/num_examples': 2472, 'score': 2955.695737838745, 'total_duration': 3390.7333834171295, 'accumulated_submission_time': 2955.695737838745, 'accumulated_eval_time': 434.7673707008362, 'accumulated_logging_time': 0.10907244682312012}
I1020 22:51:32.105515 140024861140736 logging_writer.py:48] [3671] accumulated_eval_time=434.767371, accumulated_logging_time=0.109072, accumulated_submission_time=2955.695738, global_step=3671, preemption_count=0, score=2955.695738, test/ctc_loss=0.7493950128555298, test/num_examples=2472, test/wer=0.243516, total_duration=3390.733383, train/ctc_loss=0.7165060043334961, train/wer=0.241382, validation/ctc_loss=1.0385997295379639, validation/num_examples=5348, validation/wer=0.301022
I1020 22:55:42.873250 140024852748032 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6409481167793274, loss=1.9826515913009644
I1020 23:02:06.783766 140025516500736 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7600875496864319, loss=1.950865387916565
I1020 23:08:48.876574 140025508108032 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8606100678443909, loss=1.9664807319641113
I1020 23:15:21.159752 140024861140736 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6186837553977966, loss=1.8576157093048096
I1020 23:15:32.459746 140198087026496 spec.py:321] Evaluating on the training split.
I1020 23:16:26.452508 140198087026496 spec.py:333] Evaluating on the validation split.
I1020 23:17:18.089944 140198087026496 spec.py:349] Evaluating on the test split.
I1020 23:17:44.469606 140198087026496 submission_runner.py:395] Time since start: 4963.15s, 	Step: 5516, 	{'train/ctc_loss': Array(0.57312626, dtype=float32), 'train/wer': 0.2016277390346369, 'validation/ctc_loss': Array(0.9277797, dtype=float32), 'validation/wer': 0.2725798198441739, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64360434, dtype=float32), 'test/wer': 0.2105092113013629, 'test/num_examples': 2472, 'score': 4395.957211256027, 'total_duration': 4963.1545214653015, 'accumulated_submission_time': 4395.957211256027, 'accumulated_eval_time': 566.7703504562378, 'accumulated_logging_time': 0.1822798252105713}
I1020 23:17:44.509034 140024277460736 logging_writer.py:48] [5516] accumulated_eval_time=566.770350, accumulated_logging_time=0.182280, accumulated_submission_time=4395.957211, global_step=5516, preemption_count=0, score=4395.957211, test/ctc_loss=0.6436043381690979, test/num_examples=2472, test/wer=0.210509, total_duration=4963.154521, train/ctc_loss=0.5731262564659119, train/wer=0.201628, validation/ctc_loss=0.9277796745300293, validation/num_examples=5348, validation/wer=0.272580
I1020 23:24:00.866801 140024269068032 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9450565576553345, loss=1.8856008052825928
I1020 23:30:35.875000 140023622100736 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.0331932306289673, loss=1.9019118547439575
I1020 23:37:22.255388 140023613708032 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.9150375723838806, loss=1.8476166725158691
I1020 23:41:44.831875 140198087026496 spec.py:321] Evaluating on the training split.
I1020 23:42:39.721927 140198087026496 spec.py:333] Evaluating on the validation split.
I1020 23:43:30.306298 140198087026496 spec.py:349] Evaluating on the test split.
I1020 23:43:55.661107 140198087026496 submission_runner.py:395] Time since start: 6534.35s, 	Step: 7321, 	{'train/ctc_loss': Array(0.5306109, dtype=float32), 'train/wer': 0.18209950884395062, 'validation/ctc_loss': Array(0.8476886, dtype=float32), 'validation/wer': 0.2511175260916999, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5728257, dtype=float32), 'test/wer': 0.18934454532528994, 'test/num_examples': 2472, 'score': 5836.187634706497, 'total_duration': 6534.346150159836, 'accumulated_submission_time': 5836.187634706497, 'accumulated_eval_time': 697.5930202007294, 'accumulated_logging_time': 0.23746156692504883}
I1020 23:43:55.700680 140022997456640 logging_writer.py:48] [7321] accumulated_eval_time=697.593020, accumulated_logging_time=0.237462, accumulated_submission_time=5836.187635, global_step=7321, preemption_count=0, score=5836.187635, test/ctc_loss=0.5728256702423096, test/num_examples=2472, test/wer=0.189345, total_duration=6534.346150, train/ctc_loss=0.5306109189987183, train/wer=0.182100, validation/ctc_loss=0.847688615322113, validation/num_examples=5348, validation/wer=0.251118
I1020 23:46:12.183385 140022989063936 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.2416279315948486, loss=1.867924451828003
I1020 23:52:42.864731 140022997456640 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6842373609542847, loss=1.8613675832748413
I1020 23:59:24.714514 140024277460736 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6763829588890076, loss=1.83686101436615
I1021 00:06:06.729652 140024269068032 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5709251761436462, loss=1.7752389907836914
I1021 00:07:56.262371 140198087026496 spec.py:321] Evaluating on the training split.
I1021 00:08:50.707803 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 00:09:42.236125 140198087026496 spec.py:349] Evaluating on the test split.
I1021 00:10:07.987950 140198087026496 submission_runner.py:395] Time since start: 8106.67s, 	Step: 9133, 	{'train/ctc_loss': Array(0.49094442, dtype=float32), 'train/wer': 0.17156820109156537, 'validation/ctc_loss': Array(0.79310954, dtype=float32), 'validation/wer': 0.23508114735896965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52749735, dtype=float32), 'test/wer': 0.17380618690715577, 'test/num_examples': 2472, 'score': 7276.659263849258, 'total_duration': 8106.6732313632965, 'accumulated_submission_time': 7276.659263849258, 'accumulated_eval_time': 829.3120892047882, 'accumulated_logging_time': 0.2909657955169678}
I1021 00:10:08.024734 140024431060736 logging_writer.py:48] [9133] accumulated_eval_time=829.312089, accumulated_logging_time=0.290966, accumulated_submission_time=7276.659264, global_step=9133, preemption_count=0, score=7276.659264, test/ctc_loss=0.5274973511695862, test/num_examples=2472, test/wer=0.173806, total_duration=8106.673231, train/ctc_loss=0.49094441533088684, train/wer=0.171568, validation/ctc_loss=0.7931095361709595, validation/num_examples=5348, validation/wer=0.235081
I1021 00:14:50.506958 140025516500736 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7661826014518738, loss=1.7852782011032104
I1021 00:21:41.097051 140025508108032 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8300260305404663, loss=1.862588882446289
I1021 00:28:27.523842 140023775700736 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.8622180223464966, loss=1.7048271894454956
I1021 00:34:08.343101 140198087026496 spec.py:321] Evaluating on the training split.
I1021 00:35:03.001976 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 00:35:53.608855 140198087026496 spec.py:349] Evaluating on the test split.
I1021 00:36:19.631106 140198087026496 submission_runner.py:395] Time since start: 9678.32s, 	Step: 10925, 	{'train/ctc_loss': Array(0.5014538, dtype=float32), 'train/wer': 0.17137812014873222, 'validation/ctc_loss': Array(0.7821775, dtype=float32), 'validation/wer': 0.23025382082894852, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.515465, dtype=float32), 'test/wer': 0.17262811528852598, 'test/num_examples': 2472, 'score': 8716.886959075928, 'total_duration': 9678.316082954407, 'accumulated_submission_time': 8716.886959075928, 'accumulated_eval_time': 960.593284368515, 'accumulated_logging_time': 0.3422517776489258}
I1021 00:36:19.672724 140025086420736 logging_writer.py:48] [10925] accumulated_eval_time=960.593284, accumulated_logging_time=0.342252, accumulated_submission_time=8716.886959, global_step=10925, preemption_count=0, score=8716.886959, test/ctc_loss=0.5154650211334229, test/num_examples=2472, test/wer=0.172628, total_duration=9678.316083, train/ctc_loss=0.5014538168907166, train/wer=0.171378, validation/ctc_loss=0.7821775078773499, validation/num_examples=5348, validation/wer=0.230254
I1021 00:37:17.048757 140025078028032 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7493717670440674, loss=1.77288019657135
I1021 00:43:49.604202 140023775700736 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.8231913447380066, loss=1.7600032091140747
I1021 00:50:30.117699 140023767308032 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.0464612245559692, loss=1.7753478288650513
I1021 00:57:28.288862 140024431060736 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6358756422996521, loss=1.6855350732803345
I1021 01:00:20.030879 140198087026496 spec.py:321] Evaluating on the training split.
I1021 01:01:14.887434 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 01:02:06.868561 140198087026496 spec.py:349] Evaluating on the test split.
I1021 01:02:32.605068 140198087026496 submission_runner.py:395] Time since start: 11251.29s, 	Step: 12728, 	{'train/ctc_loss': Array(0.4776491, dtype=float32), 'train/wer': 0.1640663220811265, 'validation/ctc_loss': Array(0.7791487, dtype=float32), 'validation/wer': 0.23111308495129226, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50749695, dtype=float32), 'test/wer': 0.16836268356590092, 'test/num_examples': 2472, 'score': 10157.152910709381, 'total_duration': 11251.290914058685, 'accumulated_submission_time': 10157.152910709381, 'accumulated_eval_time': 1093.1615800857544, 'accumulated_logging_time': 0.39956164360046387}
I1021 01:02:32.645798 140024431060736 logging_writer.py:48] [12728] accumulated_eval_time=1093.161580, accumulated_logging_time=0.399562, accumulated_submission_time=10157.152911, global_step=12728, preemption_count=0, score=10157.152911, test/ctc_loss=0.5074969530105591, test/num_examples=2472, test/wer=0.168363, total_duration=11251.290914, train/ctc_loss=0.47764909267425537, train/wer=0.164066, validation/ctc_loss=0.7791486978530884, validation/num_examples=5348, validation/wer=0.231113
I1021 01:05:59.704981 140024422668032 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8004088997840881, loss=1.7371405363082886
I1021 01:13:04.007704 140025086420736 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7784684896469116, loss=1.765836238861084
I1021 01:19:42.622457 140025078028032 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7697479724884033, loss=1.72951340675354
I1021 01:26:32.601711 140198087026496 spec.py:321] Evaluating on the training split.
I1021 01:27:27.954908 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 01:28:19.407362 140198087026496 spec.py:349] Evaluating on the test split.
I1021 01:28:45.392724 140198087026496 submission_runner.py:395] Time since start: 12824.08s, 	Step: 14477, 	{'train/ctc_loss': Array(0.42572266, dtype=float32), 'train/wer': 0.1530252996697782, 'validation/ctc_loss': Array(0.75034535, dtype=float32), 'validation/wer': 0.22285835658495612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48662755, dtype=float32), 'test/wer': 0.16373164340990798, 'test/num_examples': 2472, 'score': 11597.019608259201, 'total_duration': 12824.077835083008, 'accumulated_submission_time': 11597.019608259201, 'accumulated_eval_time': 1225.9461002349854, 'accumulated_logging_time': 0.4544496536254883}
I1021 01:28:45.432347 140024431060736 logging_writer.py:48] [14477] accumulated_eval_time=1225.946100, accumulated_logging_time=0.454450, accumulated_submission_time=11597.019608, global_step=14477, preemption_count=0, score=11597.019608, test/ctc_loss=0.4866275489330292, test/num_examples=2472, test/wer=0.163732, total_duration=12824.077835, train/ctc_loss=0.4257226586341858, train/wer=0.153025, validation/ctc_loss=0.7503453493118286, validation/num_examples=5348, validation/wer=0.222858
I1021 01:29:03.625987 140024422668032 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6388207077980042, loss=1.6381655931472778
I1021 01:35:28.232551 140024431060736 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8339934945106506, loss=1.7412207126617432
I1021 01:42:32.644751 140024431060736 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.8434757590293884, loss=1.6277387142181396
I1021 01:49:01.941340 140024422668032 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9405294060707092, loss=1.6992136240005493
I1021 01:52:45.797626 140198087026496 spec.py:321] Evaluating on the training split.
I1021 01:53:40.230733 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 01:54:31.555364 140198087026496 spec.py:349] Evaluating on the test split.
I1021 01:54:57.465844 140198087026496 submission_runner.py:395] Time since start: 14396.15s, 	Step: 16260, 	{'train/ctc_loss': Array(0.403061, dtype=float32), 'train/wer': 0.14105517399145603, 'validation/ctc_loss': Array(0.72263044, dtype=float32), 'validation/wer': 0.21480637593288085, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47058672, dtype=float32), 'test/wer': 0.1561148010480775, 'test/num_examples': 2472, 'score': 13037.290833950043, 'total_duration': 14396.150520086288, 'accumulated_submission_time': 13037.290833950043, 'accumulated_eval_time': 1357.6072144508362, 'accumulated_logging_time': 0.5123186111450195}
I1021 01:54:57.506797 140025516500736 logging_writer.py:48] [16260] accumulated_eval_time=1357.607214, accumulated_logging_time=0.512319, accumulated_submission_time=13037.290834, global_step=16260, preemption_count=0, score=13037.290834, test/ctc_loss=0.47058671712875366, test/num_examples=2472, test/wer=0.156115, total_duration=14396.150520, train/ctc_loss=0.40306100249290466, train/wer=0.141055, validation/ctc_loss=0.7226304411888123, validation/num_examples=5348, validation/wer=0.214806
I1021 01:58:03.830768 140025188820736 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.9492850303649902, loss=1.6450142860412598
I1021 02:04:38.621969 140025180428032 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6535489559173584, loss=1.637863278388977
I1021 02:11:52.975113 140025188820736 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.2240188121795654, loss=1.6871881484985352
I1021 02:18:24.506346 140025516500736 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.470857858657837, loss=1.6812771558761597
I1021 02:18:58.029741 140198087026496 spec.py:321] Evaluating on the training split.
I1021 02:19:52.644722 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 02:20:43.850986 140198087026496 spec.py:349] Evaluating on the test split.
I1021 02:21:09.908924 140198087026496 submission_runner.py:395] Time since start: 15968.59s, 	Step: 18041, 	{'train/ctc_loss': Array(0.40040925, dtype=float32), 'train/wer': 0.14585958909619395, 'validation/ctc_loss': Array(0.70875674, dtype=float32), 'validation/wer': 0.2101142145457003, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4586529, dtype=float32), 'test/wer': 0.15211341986066257, 'test/num_examples': 2472, 'score': 14477.724244832993, 'total_duration': 15968.594814777374, 'accumulated_submission_time': 14477.724244832993, 'accumulated_eval_time': 1489.4804847240448, 'accumulated_logging_time': 0.5687074661254883}
I1021 02:21:09.952314 140025301460736 logging_writer.py:48] [18041] accumulated_eval_time=1489.480485, accumulated_logging_time=0.568707, accumulated_submission_time=14477.724245, global_step=18041, preemption_count=0, score=14477.724245, test/ctc_loss=0.45865291357040405, test/num_examples=2472, test/wer=0.152113, total_duration=15968.594815, train/ctc_loss=0.4004092514514923, train/wer=0.145860, validation/ctc_loss=0.7087567448616028, validation/num_examples=5348, validation/wer=0.210114
I1021 02:27:12.201260 140025293068032 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7937086820602417, loss=1.770508885383606
I1021 02:33:43.963962 140025301460736 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.168588399887085, loss=1.6687287092208862
I1021 02:40:50.597248 140025293068032 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.9272390604019165, loss=1.6776201725006104
I1021 02:45:10.511153 140198087026496 spec.py:321] Evaluating on the training split.
I1021 02:46:05.389881 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 02:46:57.151342 140198087026496 spec.py:349] Evaluating on the test split.
I1021 02:47:23.154903 140198087026496 submission_runner.py:395] Time since start: 17541.84s, 	Step: 19832, 	{'train/ctc_loss': Array(0.41879952, dtype=float32), 'train/wer': 0.14645019699093048, 'validation/ctc_loss': Array(0.7252199, dtype=float32), 'validation/wer': 0.2155497842185041, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46211413, dtype=float32), 'test/wer': 0.15658196737960312, 'test/num_examples': 2472, 'score': 15918.192623853683, 'total_duration': 17541.840811014175, 'accumulated_submission_time': 15918.192623853683, 'accumulated_eval_time': 1622.1185681819916, 'accumulated_logging_time': 0.6266741752624512}
I1021 02:47:23.197066 140025301460736 logging_writer.py:48] [19832] accumulated_eval_time=1622.118568, accumulated_logging_time=0.626674, accumulated_submission_time=15918.192624, global_step=19832, preemption_count=0, score=15918.192624, test/ctc_loss=0.4621141254901886, test/num_examples=2472, test/wer=0.156582, total_duration=17541.840811, train/ctc_loss=0.4187995195388794, train/wer=0.146450, validation/ctc_loss=0.7252199053764343, validation/num_examples=5348, validation/wer=0.215550
I1021 02:49:31.528119 140025293068032 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.17911958694458, loss=1.6341774463653564
I1021 02:56:13.838119 140025301460736 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.2019819021224976, loss=1.681640386581421
I1021 03:02:45.135206 140025301460736 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8328693509101868, loss=1.6099356412887573
I1021 03:09:41.985709 140025293068032 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7678055167198181, loss=1.6414923667907715
I1021 03:11:23.325732 140198087026496 spec.py:321] Evaluating on the training split.
I1021 03:12:17.805403 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 03:13:08.960674 140198087026496 spec.py:349] Evaluating on the test split.
I1021 03:13:35.720888 140198087026496 submission_runner.py:395] Time since start: 19114.41s, 	Step: 21620, 	{'train/ctc_loss': Array(0.41190353, dtype=float32), 'train/wer': 0.1419948822117287, 'validation/ctc_loss': Array(0.68248945, dtype=float32), 'validation/wer': 0.2026704770364077, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43280604, dtype=float32), 'test/wer': 0.14555277963967259, 'test/num_examples': 2472, 'score': 17358.230726480484, 'total_duration': 19114.40650701523, 'accumulated_submission_time': 17358.230726480484, 'accumulated_eval_time': 1754.5075569152832, 'accumulated_logging_time': 0.6849470138549805}
I1021 03:13:35.767525 140025086420736 logging_writer.py:48] [21620] accumulated_eval_time=1754.507557, accumulated_logging_time=0.684947, accumulated_submission_time=17358.230726, global_step=21620, preemption_count=0, score=17358.230726, test/ctc_loss=0.4328060448169708, test/num_examples=2472, test/wer=0.145553, total_duration=19114.406507, train/ctc_loss=0.4119035303592682, train/wer=0.141995, validation/ctc_loss=0.6824894547462463, validation/num_examples=5348, validation/wer=0.202670
I1021 03:18:28.151460 140024431060736 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.5575648546218872, loss=1.6712725162506104
I1021 03:25:25.787435 140024422668032 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8143424391746521, loss=1.6322101354599
I1021 03:32:02.585782 140023448020736 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.9090852737426758, loss=1.5882954597473145
I1021 03:37:36.075068 140198087026496 spec.py:321] Evaluating on the training split.
I1021 03:38:31.719547 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 03:39:23.830415 140198087026496 spec.py:349] Evaluating on the test split.
I1021 03:39:49.933336 140198087026496 submission_runner.py:395] Time since start: 20688.62s, 	Step: 23404, 	{'train/ctc_loss': Array(0.38955504, dtype=float32), 'train/wer': 0.13493672188496758, 'validation/ctc_loss': Array(0.6696636, dtype=float32), 'validation/wer': 0.19759212952682545, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41805887, dtype=float32), 'test/wer': 0.13972335628541832, 'test/num_examples': 2472, 'score': 18798.447294950485, 'total_duration': 20688.61761546135, 'accumulated_submission_time': 18798.447294950485, 'accumulated_eval_time': 1888.358320236206, 'accumulated_logging_time': 0.747185230255127}
I1021 03:39:49.982051 140025516500736 logging_writer.py:48] [23404] accumulated_eval_time=1888.358320, accumulated_logging_time=0.747185, accumulated_submission_time=18798.447295, global_step=23404, preemption_count=0, score=18798.447295, test/ctc_loss=0.4180588722229004, test/num_examples=2472, test/wer=0.139723, total_duration=20688.617615, train/ctc_loss=0.38955503702163696, train/wer=0.134937, validation/ctc_loss=0.6696636080741882, validation/num_examples=5348, validation/wer=0.197592
I1021 03:41:03.675013 140025508108032 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.7172480821609497, loss=1.5827680826187134
I1021 03:47:26.378564 140025516500736 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.9111993312835693, loss=1.6354920864105225
I1021 03:54:22.309271 140025508108032 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.788577675819397, loss=1.5738047361373901
I1021 04:01:01.323118 140025188820736 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.9190051555633545, loss=1.544405221939087
I1021 04:03:50.058114 140198087026496 spec.py:321] Evaluating on the training split.
I1021 04:04:45.300106 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 04:05:37.025850 140198087026496 spec.py:349] Evaluating on the test split.
I1021 04:06:02.920368 140198087026496 submission_runner.py:395] Time since start: 22261.61s, 	Step: 25213, 	{'train/ctc_loss': Array(0.36059958, dtype=float32), 'train/wer': 0.12565981166335882, 'validation/ctc_loss': Array(0.64565474, dtype=float32), 'validation/wer': 0.19088214565009606, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4041364, dtype=float32), 'test/wer': 0.13572197509800338, 'test/num_examples': 2472, 'score': 20238.43037390709, 'total_duration': 22261.60532474518, 'accumulated_submission_time': 20238.43037390709, 'accumulated_eval_time': 2021.213785648346, 'accumulated_logging_time': 0.8119029998779297}
I1021 04:06:02.965154 140024605140736 logging_writer.py:48] [25213] accumulated_eval_time=2021.213786, accumulated_logging_time=0.811903, accumulated_submission_time=20238.430374, global_step=25213, preemption_count=0, score=20238.430374, test/ctc_loss=0.40413638949394226, test/num_examples=2472, test/wer=0.135722, total_duration=22261.605325, train/ctc_loss=0.3605995774269104, train/wer=0.125660, validation/ctc_loss=0.6456547379493713, validation/num_examples=5348, validation/wer=0.190882
I1021 04:09:41.608154 140024596748032 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8824496865272522, loss=1.644195556640625
I1021 04:16:26.357442 140025516500736 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.903300940990448, loss=1.5556739568710327
I1021 04:23:09.480215 140025508108032 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.9980712532997131, loss=1.540472149848938
I1021 04:29:52.777910 140025516500736 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.0943121910095215, loss=1.5527158975601196
I1021 04:30:03.188105 140198087026496 spec.py:321] Evaluating on the training split.
I1021 04:30:58.039186 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 04:31:48.865543 140198087026496 spec.py:349] Evaluating on the test split.
I1021 04:32:14.968806 140198087026496 submission_runner.py:395] Time since start: 23833.65s, 	Step: 27015, 	{'train/ctc_loss': Array(0.32621488, dtype=float32), 'train/wer': 0.11689402436161611, 'validation/ctc_loss': Array(0.6163341, dtype=float32), 'validation/wer': 0.18436525483456753, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.385615, dtype=float32), 'test/wer': 0.1300144212215384, 'test/num_examples': 2472, 'score': 21678.559955358505, 'total_duration': 23833.65296936035, 'accumulated_submission_time': 21678.559955358505, 'accumulated_eval_time': 2152.986874818802, 'accumulated_logging_time': 0.8726892471313477}
I1021 04:32:15.010359 140025009620736 logging_writer.py:48] [27015] accumulated_eval_time=2152.986875, accumulated_logging_time=0.872689, accumulated_submission_time=21678.559955, global_step=27015, preemption_count=0, score=21678.559955, test/ctc_loss=0.3856149911880493, test/num_examples=2472, test/wer=0.130014, total_duration=23833.652969, train/ctc_loss=0.32621487975120544, train/wer=0.116894, validation/ctc_loss=0.616334080696106, validation/num_examples=5348, validation/wer=0.184365
I1021 04:38:40.435007 140025001228032 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.126114010810852, loss=1.6369614601135254
I1021 04:45:30.005779 140025009620736 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.7325340509414673, loss=1.4790924787521362
I1021 04:52:18.531612 140025001228032 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.8663647174835205, loss=1.4685478210449219
I1021 04:56:15.517441 140198087026496 spec.py:321] Evaluating on the training split.
I1021 04:57:11.542857 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 04:58:03.426007 140198087026496 spec.py:349] Evaluating on the test split.
I1021 04:58:29.544402 140198087026496 submission_runner.py:395] Time since start: 25408.23s, 	Step: 28783, 	{'train/ctc_loss': Array(0.30786785, dtype=float32), 'train/wer': 0.108743901565904, 'validation/ctc_loss': Array(0.5970837, dtype=float32), 'validation/wer': 0.17703737316199541, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3635187, dtype=float32), 'test/wer': 0.12170698515223528, 'test/num_examples': 2472, 'score': 23118.97879600525, 'total_duration': 25408.23015165329, 'accumulated_submission_time': 23118.97879600525, 'accumulated_eval_time': 2287.0078358650208, 'accumulated_logging_time': 0.9285540580749512}
I1021 04:58:29.590417 140024502740736 logging_writer.py:48] [28783] accumulated_eval_time=2287.007836, accumulated_logging_time=0.928554, accumulated_submission_time=23118.978796, global_step=28783, preemption_count=0, score=23118.978796, test/ctc_loss=0.36351868510246277, test/num_examples=2472, test/wer=0.121707, total_duration=25408.230152, train/ctc_loss=0.3078678548336029, train/wer=0.108744, validation/ctc_loss=0.5970836877822876, validation/num_examples=5348, validation/wer=0.177037
I1021 05:01:18.005202 140024502740736 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.084276556968689, loss=1.5089887380599976
I1021 05:08:06.194288 140024494348032 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.7468230724334717, loss=1.5346834659576416
I1021 05:15:08.839217 140024502740736 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.9046794772148132, loss=1.4949102401733398
I1021 05:21:52.166318 140024494348032 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6310905814170837, loss=1.4828946590423584
I1021 05:22:29.555017 140198087026496 spec.py:321] Evaluating on the training split.
I1021 05:23:23.735494 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 05:24:15.495411 140198087026496 spec.py:349] Evaluating on the test split.
I1021 05:24:41.719796 140198087026496 submission_runner.py:395] Time since start: 26980.40s, 	Step: 30545, 	{'train/ctc_loss': Array(0.3210939, dtype=float32), 'train/wer': 0.1139874761657133, 'validation/ctc_loss': Array(0.58573115, dtype=float32), 'validation/wer': 0.1738127190399413, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35593492, dtype=float32), 'test/wer': 0.11898523348160786, 'test/num_examples': 2472, 'score': 24558.852508544922, 'total_duration': 26980.40470767021, 'accumulated_submission_time': 24558.852508544922, 'accumulated_eval_time': 2419.1657495498657, 'accumulated_logging_time': 0.9904017448425293}
I1021 05:24:41.761630 140024502740736 logging_writer.py:48] [30545] accumulated_eval_time=2419.165750, accumulated_logging_time=0.990402, accumulated_submission_time=24558.852509, global_step=30545, preemption_count=0, score=24558.852509, test/ctc_loss=0.35593491792678833, test/num_examples=2472, test/wer=0.118985, total_duration=26980.404708, train/ctc_loss=0.321093887090683, train/wer=0.113987, validation/ctc_loss=0.5857311487197876, validation/num_examples=5348, validation/wer=0.173813
I1021 05:30:31.903056 140023847380736 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.4446935653686523, loss=1.504361629486084
I1021 05:37:13.731769 140023838988032 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.4568438529968262, loss=1.4145331382751465
I1021 05:44:19.416717 140024502740736 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.7141768336296082, loss=1.4563164710998535
I1021 05:48:41.965586 140198087026496 spec.py:321] Evaluating on the training split.
I1021 05:49:37.128043 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 05:50:28.625060 140198087026496 spec.py:349] Evaluating on the test split.
I1021 05:50:54.891708 140198087026496 submission_runner.py:395] Time since start: 28553.58s, 	Step: 32345, 	{'train/ctc_loss': Array(0.2966427, dtype=float32), 'train/wer': 0.10325612184547936, 'validation/ctc_loss': Array(0.57263756, dtype=float32), 'validation/wer': 0.16969983683636328, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34842104, dtype=float32), 'test/wer': 0.11437450490524648, 'test/num_examples': 2472, 'score': 25998.96476507187, 'total_duration': 28553.576659440994, 'accumulated_submission_time': 25998.96476507187, 'accumulated_eval_time': 2552.0850961208344, 'accumulated_logging_time': 1.0472321510314941}
I1021 05:50:54.932748 140024502740736 logging_writer.py:48] [32345] accumulated_eval_time=2552.085096, accumulated_logging_time=1.047232, accumulated_submission_time=25998.964765, global_step=32345, preemption_count=0, score=25998.964765, test/ctc_loss=0.34842103719711304, test/num_examples=2472, test/wer=0.114375, total_duration=28553.576659, train/ctc_loss=0.2966426908969879, train/wer=0.103256, validation/ctc_loss=0.5726375579833984, validation/num_examples=5348, validation/wer=0.169700
I1021 05:52:53.038851 140024494348032 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.8304684162139893, loss=1.4038267135620117
I1021 05:59:43.041044 140023192020736 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.0970960855484009, loss=1.4488439559936523
I1021 06:06:15.873792 140023183628032 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.263159155845642, loss=1.4763275384902954
I1021 06:13:33.461635 140023847380736 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.8128545880317688, loss=1.4430391788482666
I1021 06:14:55.432705 140198087026496 spec.py:321] Evaluating on the training split.
I1021 06:15:50.988578 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 06:16:42.612514 140198087026496 spec.py:349] Evaluating on the test split.
I1021 06:17:08.936025 140198087026496 submission_runner.py:395] Time since start: 30127.62s, 	Step: 34110, 	{'train/ctc_loss': Array(0.31054127, dtype=float32), 'train/wer': 0.10908773353744047, 'validation/ctc_loss': Array(0.56212187, dtype=float32), 'validation/wer': 0.16798130859167576, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33788675, dtype=float32), 'test/wer': 0.11404951963114171, 'test/num_examples': 2472, 'score': 27439.37448334694, 'total_duration': 30127.621012210846, 'accumulated_submission_time': 27439.37448334694, 'accumulated_eval_time': 2685.5816509723663, 'accumulated_logging_time': 1.103471279144287}
I1021 06:17:08.981292 140023908812544 logging_writer.py:48] [34110] accumulated_eval_time=2685.581651, accumulated_logging_time=1.103471, accumulated_submission_time=27439.374483, global_step=34110, preemption_count=0, score=27439.374483, test/ctc_loss=0.3378867506980896, test/num_examples=2472, test/wer=0.114050, total_duration=30127.621012, train/ctc_loss=0.3105412721633911, train/wer=0.109088, validation/ctc_loss=0.5621218681335449, validation/num_examples=5348, validation/wer=0.167981
I1021 06:22:05.263892 140023900419840 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.8836162090301514, loss=1.4859483242034912
I1021 06:29:05.586920 140023908812544 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7500361204147339, loss=1.4111775159835815
I1021 06:35:32.258233 140022925772544 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.8601773977279663, loss=1.3896523714065552
I1021 06:41:09.588836 140198087026496 spec.py:321] Evaluating on the training split.
I1021 06:42:04.128683 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 06:42:55.624354 140198087026496 spec.py:349] Evaluating on the test split.
I1021 06:43:21.634407 140198087026496 submission_runner.py:395] Time since start: 31700.32s, 	Step: 35903, 	{'train/ctc_loss': Array(0.28951797, dtype=float32), 'train/wer': 0.09782388610036553, 'validation/ctc_loss': Array(0.5307529, dtype=float32), 'validation/wer': 0.15763152051131044, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31606698, dtype=float32), 'test/wer': 0.10600613409704873, 'test/num_examples': 2472, 'score': 28879.892230033875, 'total_duration': 31700.31954073906, 'accumulated_submission_time': 28879.892230033875, 'accumulated_eval_time': 2817.620602607727, 'accumulated_logging_time': 1.1625759601593018}
I1021 06:43:21.677801 140022925772544 logging_writer.py:48] [35903] accumulated_eval_time=2817.620603, accumulated_logging_time=1.162576, accumulated_submission_time=28879.892230, global_step=35903, preemption_count=0, score=28879.892230, test/ctc_loss=0.3160669803619385, test/num_examples=2472, test/wer=0.106006, total_duration=31700.319541, train/ctc_loss=0.28951796889305115, train/wer=0.097824, validation/ctc_loss=0.5307528972625732, validation/num_examples=5348, validation/wer=0.157632
I1021 06:44:36.208767 140022917379840 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.9442681074142456, loss=1.4446901082992554
I1021 06:51:01.682277 140022925772544 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.8287880420684814, loss=1.3908601999282837
I1021 06:58:05.709590 140022917379840 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7488260269165039, loss=1.4013599157333374
I1021 07:04:35.920115 140023908812544 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.856404185295105, loss=1.391528606414795
I1021 07:07:21.792058 140198087026496 spec.py:321] Evaluating on the training split.
I1021 07:08:18.081212 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 07:09:09.222785 140198087026496 spec.py:349] Evaluating on the test split.
I1021 07:09:35.529816 140198087026496 submission_runner.py:395] Time since start: 33274.22s, 	Step: 37700, 	{'train/ctc_loss': Array(0.2151113, dtype=float32), 'train/wer': 0.0776062646700081, 'validation/ctc_loss': Array(0.500329, dtype=float32), 'validation/wer': 0.15020709230813792, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29686123, dtype=float32), 'test/wer': 0.09910019702232242, 'test/num_examples': 2472, 'score': 30319.913461446762, 'total_duration': 33274.215215206146, 'accumulated_submission_time': 30319.913461446762, 'accumulated_eval_time': 2951.3520109653473, 'accumulated_logging_time': 1.223691463470459}
I1021 07:09:35.569617 140023908812544 logging_writer.py:48] [37700] accumulated_eval_time=2951.352011, accumulated_logging_time=1.223691, accumulated_submission_time=30319.913461, global_step=37700, preemption_count=0, score=30319.913461, test/ctc_loss=0.2968612313270569, test/num_examples=2472, test/wer=0.099100, total_duration=33274.215215, train/ctc_loss=0.21511130034923553, train/wer=0.077606, validation/ctc_loss=0.5003290176391602, validation/num_examples=5348, validation/wer=0.150207
I1021 07:13:24.086697 140023900419840 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.8396633267402649, loss=1.4029165506362915
I1021 07:19:54.447548 140023908812544 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.8115087151527405, loss=1.3448230028152466
I1021 07:26:55.222462 140023900419840 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.1889159679412842, loss=1.3420177698135376
I1021 07:33:30.336375 140023908812544 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.8972030282020569, loss=1.2946438789367676
I1021 07:33:36.115930 140198087026496 spec.py:321] Evaluating on the training split.
I1021 07:34:31.268040 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 07:35:22.357081 140198087026496 spec.py:349] Evaluating on the test split.
I1021 07:35:48.476583 140198087026496 submission_runner.py:395] Time since start: 34847.16s, 	Step: 39509, 	{'train/ctc_loss': Array(0.23098134, dtype=float32), 'train/wer': 0.08047348524846361, 'validation/ctc_loss': Array(0.48058802, dtype=float32), 'validation/wer': 0.14288886528862585, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28094158, dtype=float32), 'test/wer': 0.09418479475148782, 'test/num_examples': 2472, 'score': 31760.36798143387, 'total_duration': 34847.16221642494, 'accumulated_submission_time': 31760.36798143387, 'accumulated_eval_time': 3083.706532716751, 'accumulated_logging_time': 1.2791519165039062}
I1021 07:35:48.521238 140024205776640 logging_writer.py:48] [39509] accumulated_eval_time=3083.706533, accumulated_logging_time=1.279152, accumulated_submission_time=31760.367981, global_step=39509, preemption_count=0, score=31760.367981, test/ctc_loss=0.28094157576560974, test/num_examples=2472, test/wer=0.094185, total_duration=34847.162216, train/ctc_loss=0.23098133504390717, train/wer=0.080473, validation/ctc_loss=0.48058801889419556, validation/num_examples=5348, validation/wer=0.142889
I1021 07:42:16.485531 140024197383936 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9094491004943848, loss=1.340995192527771
I1021 07:48:52.645989 140023878096640 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.139898657798767, loss=1.3311049938201904
I1021 07:55:47.082664 140023869703936 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.8895829916000366, loss=1.3003829717636108
I1021 07:59:48.561896 140198087026496 spec.py:321] Evaluating on the training split.
I1021 08:00:42.495542 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 08:01:34.000298 140198087026496 spec.py:349] Evaluating on the test split.
I1021 08:01:59.616707 140198087026496 submission_runner.py:395] Time since start: 36418.30s, 	Step: 41292, 	{'train/ctc_loss': Array(0.2832291, dtype=float32), 'train/wer': 0.09786128453106101, 'validation/ctc_loss': Array(0.46717575, dtype=float32), 'validation/wer': 0.13716365602402078, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27615497, dtype=float32), 'test/wer': 0.09268173785875328, 'test/num_examples': 2472, 'score': 33200.31581401825, 'total_duration': 36418.3018321991, 'accumulated_submission_time': 33200.31581401825, 'accumulated_eval_time': 3214.754864692688, 'accumulated_logging_time': 1.3403780460357666}
I1021 08:01:59.662168 140024205776640 logging_writer.py:48] [41292] accumulated_eval_time=3214.754865, accumulated_logging_time=1.340378, accumulated_submission_time=33200.315814, global_step=41292, preemption_count=0, score=33200.315814, test/ctc_loss=0.2761549651622772, test/num_examples=2472, test/wer=0.092682, total_duration=36418.301832, train/ctc_loss=0.28322911262512207, train/wer=0.097861, validation/ctc_loss=0.46717575192451477, validation/num_examples=5348, validation/wer=0.137164
I1021 08:04:37.829343 140024197383936 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8208381533622742, loss=1.2986363172531128
I1021 08:11:22.937704 140024205776640 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.9878079891204834, loss=1.2180763483047485
I1021 08:18:07.459021 140023878096640 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.241163730621338, loss=1.2836028337478638
I1021 08:25:09.955075 140023869703936 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.0107676982879639, loss=1.331160068511963
I1021 08:26:00.024803 140198087026496 spec.py:321] Evaluating on the training split.
I1021 08:26:53.191883 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 08:27:44.300042 140198087026496 spec.py:349] Evaluating on the test split.
I1021 08:28:10.614561 140198087026496 submission_runner.py:395] Time since start: 37989.30s, 	Step: 43058, 	{'train/ctc_loss': Array(0.27360976, dtype=float32), 'train/wer': 0.09577710298272982, 'validation/ctc_loss': Array(0.43741032, dtype=float32), 'validation/wer': 0.1310715699431341, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2558681, dtype=float32), 'test/wer': 0.08601953973960555, 'test/num_examples': 2472, 'score': 34640.58737039566, 'total_duration': 37989.29826402664, 'accumulated_submission_time': 34640.58737039566, 'accumulated_eval_time': 3345.3365590572357, 'accumulated_logging_time': 1.400590181350708}
I1021 08:28:10.659123 140024502740736 logging_writer.py:48] [43058] accumulated_eval_time=3345.336559, accumulated_logging_time=1.400590, accumulated_submission_time=34640.587370, global_step=43058, preemption_count=0, score=34640.587370, test/ctc_loss=0.2558681070804596, test/num_examples=2472, test/wer=0.086020, total_duration=37989.298264, train/ctc_loss=0.2736097574234009, train/wer=0.095777, validation/ctc_loss=0.4374103248119354, validation/num_examples=5348, validation/wer=0.131072
I1021 08:33:51.028601 140022925772544 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.0863381624221802, loss=1.2437963485717773
I1021 08:40:42.841803 140022917379840 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.2777177095413208, loss=1.2701820135116577
I1021 08:47:35.416272 140024502740736 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.98857843875885, loss=1.2517691850662231
I1021 08:52:10.842722 140198087026496 spec.py:321] Evaluating on the training split.
I1021 08:53:02.978935 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 08:53:54.010448 140198087026496 spec.py:349] Evaluating on the test split.
I1021 08:54:20.087892 140198087026496 submission_runner.py:395] Time since start: 39558.77s, 	Step: 44837, 	{'train/ctc_loss': Array(0.30323243, dtype=float32), 'train/wer': 0.10670309035922575, 'validation/ctc_loss': Array(0.42683902, dtype=float32), 'validation/wer': 0.12672697606611508, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24299584, dtype=float32), 'test/wer': 0.08165255011882275, 'test/num_examples': 2472, 'score': 36080.67891216278, 'total_duration': 39558.77188563347, 'accumulated_submission_time': 36080.67891216278, 'accumulated_eval_time': 3474.5739798545837, 'accumulated_logging_time': 1.4610958099365234}
I1021 08:54:20.134494 140024502740736 logging_writer.py:48] [44837] accumulated_eval_time=3474.573980, accumulated_logging_time=1.461096, accumulated_submission_time=36080.678912, global_step=44837, preemption_count=0, score=36080.678912, test/ctc_loss=0.24299584329128265, test/num_examples=2472, test/wer=0.081653, total_duration=39558.771886, train/ctc_loss=0.30323243141174316, train/wer=0.106703, validation/ctc_loss=0.42683902382850647, validation/num_examples=5348, validation/wer=0.126727
I1021 08:56:24.862756 140024494348032 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.1353285312652588, loss=1.2339931726455688
I1021 09:03:04.110362 140023192020736 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.340232491493225, loss=1.2441487312316895
I1021 09:09:58.308656 140023183628032 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9491603374481201, loss=1.2132530212402344
I1021 09:16:56.714499 140023847380736 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1787357330322266, loss=1.2184884548187256
I1021 09:18:20.789438 140198087026496 spec.py:321] Evaluating on the training split.
I1021 09:19:13.082099 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 09:20:04.959515 140198087026496 spec.py:349] Evaluating on the test split.
I1021 09:20:31.706809 140198087026496 submission_runner.py:395] Time since start: 41130.39s, 	Step: 46612, 	{'train/ctc_loss': Array(0.25414667, dtype=float32), 'train/wer': 0.08765432825627469, 'validation/ctc_loss': Array(0.40933293, dtype=float32), 'validation/wer': 0.1209824574953899, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22970492, dtype=float32), 'test/wer': 0.07763085735177624, 'test/num_examples': 2472, 'score': 37521.242700099945, 'total_duration': 41130.3929233551, 'accumulated_submission_time': 37521.242700099945, 'accumulated_eval_time': 3605.485710144043, 'accumulated_logging_time': 1.5238821506500244}
I1021 09:20:31.751217 140023847380736 logging_writer.py:48] [46612] accumulated_eval_time=3605.485710, accumulated_logging_time=1.523882, accumulated_submission_time=37521.242700, global_step=46612, preemption_count=0, score=37521.242700, test/ctc_loss=0.22970491647720337, test/num_examples=2472, test/wer=0.077631, total_duration=41130.392923, train/ctc_loss=0.25414666533470154, train/wer=0.087654, validation/ctc_loss=0.40933293104171753, validation/num_examples=5348, validation/wer=0.120982
I1021 09:25:33.001467 140023838988032 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.132554292678833, loss=1.2076181173324585
I1021 09:32:31.388433 140024502740736 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.0399558544158936, loss=1.1985647678375244
I1021 09:39:09.798645 140024494348032 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9558466076850891, loss=1.116953730583191
I1021 09:44:31.916553 140198087026496 spec.py:321] Evaluating on the training split.
I1021 09:45:25.200597 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 09:46:16.556326 140198087026496 spec.py:349] Evaluating on the test split.
I1021 09:46:43.218939 140198087026496 submission_runner.py:395] Time since start: 42701.90s, 	Step: 48371, 	{'train/ctc_loss': Array(0.21986894, dtype=float32), 'train/wer': 0.07861334105307695, 'validation/ctc_loss': Array(0.38523635, dtype=float32), 'validation/wer': 0.11406972590439962, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21609479, dtype=float32), 'test/wer': 0.06958747181768327, 'test/num_examples': 2472, 'score': 38961.31883120537, 'total_duration': 42701.90442609787, 'accumulated_submission_time': 38961.31883120537, 'accumulated_eval_time': 3736.781819343567, 'accumulated_logging_time': 1.5830357074737549}
I1021 09:46:43.267692 140024502740736 logging_writer.py:48] [48371] accumulated_eval_time=3736.781819, accumulated_logging_time=1.583036, accumulated_submission_time=38961.318831, global_step=48371, preemption_count=0, score=38961.318831, test/ctc_loss=0.21609479188919067, test/num_examples=2472, test/wer=0.069587, total_duration=42701.904426, train/ctc_loss=0.21986894309520721, train/wer=0.078613, validation/ctc_loss=0.38523635268211365, validation/num_examples=5348, validation/wer=0.114070
I1021 09:48:25.104458 140023847380736 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.1672179698944092, loss=1.1211738586425781
I1021 09:55:06.933175 140023838988032 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.1518456935882568, loss=1.1338627338409424
I1021 10:02:13.462419 140023847380736 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.71604323387146, loss=1.1237952709197998
I1021 10:08:47.507131 140023838988032 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.4149470329284668, loss=1.1511961221694946
I1021 10:10:43.634536 140198087026496 spec.py:321] Evaluating on the training split.
I1021 10:11:37.698390 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 10:12:29.290847 140198087026496 spec.py:349] Evaluating on the test split.
I1021 10:12:54.985354 140198087026496 submission_runner.py:395] Time since start: 44273.67s, 	Step: 50136, 	{'train/ctc_loss': Array(0.1773363, dtype=float32), 'train/wer': 0.06374185689893311, 'validation/ctc_loss': Array(0.37084493, dtype=float32), 'validation/wer': 0.10877897602749645, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20711939, dtype=float32), 'test/wer': 0.0691812402250523, 'test/num_examples': 2472, 'score': 40401.594277620316, 'total_duration': 44273.67018413544, 'accumulated_submission_time': 40401.594277620316, 'accumulated_eval_time': 3868.125723838806, 'accumulated_logging_time': 1.648780345916748}
I1021 10:12:55.023707 140023253452544 logging_writer.py:48] [50136] accumulated_eval_time=3868.125724, accumulated_logging_time=1.648780, accumulated_submission_time=40401.594278, global_step=50136, preemption_count=0, score=40401.594278, test/ctc_loss=0.2071193903684616, test/num_examples=2472, test/wer=0.069181, total_duration=44273.670184, train/ctc_loss=0.17733630537986755, train/wer=0.063742, validation/ctc_loss=0.37084493041038513, validation/num_examples=5348, validation/wer=0.108779
I1021 10:17:37.393182 140023253452544 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.163184642791748, loss=1.0398041009902954
I1021 10:24:13.838886 140023245059840 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.1232526302337646, loss=1.111714243888855
I1021 10:31:23.490847 140024502740736 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.8174164295196533, loss=1.135236144065857
I1021 10:36:55.652127 140198087026496 spec.py:321] Evaluating on the training split.
I1021 10:37:50.846384 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 10:38:42.589010 140198087026496 spec.py:349] Evaluating on the test split.
I1021 10:39:09.169840 140198087026496 submission_runner.py:395] Time since start: 45847.86s, 	Step: 51935, 	{'train/ctc_loss': Array(0.18663979, dtype=float32), 'train/wer': 0.06758921831127428, 'validation/ctc_loss': Array(0.3541452, dtype=float32), 'validation/wer': 0.1046564391708584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19410646, dtype=float32), 'test/wer': 0.06463144638758556, 'test/num_examples': 2472, 'score': 41842.131314992905, 'total_duration': 45847.85550451279, 'accumulated_submission_time': 41842.131314992905, 'accumulated_eval_time': 4001.6373660564423, 'accumulated_logging_time': 1.700425386428833}
I1021 10:39:09.222456 140024502740736 logging_writer.py:48] [51935] accumulated_eval_time=4001.637366, accumulated_logging_time=1.700425, accumulated_submission_time=41842.131315, global_step=51935, preemption_count=0, score=41842.131315, test/ctc_loss=0.19410645961761475, test/num_examples=2472, test/wer=0.064631, total_duration=45847.855505, train/ctc_loss=0.18663978576660156, train/wer=0.067589, validation/ctc_loss=0.35414519906044006, validation/num_examples=5348, validation/wer=0.104656
I1021 10:39:59.387344 140024494348032 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.14767324924469, loss=1.1010252237319946
I1021 10:46:45.297721 140024502740736 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.1751089096069336, loss=1.071136236190796
I1021 10:53:13.913797 140024175060736 logging_writer.py:48] [53000] global_step=53000, grad_norm=2.037522792816162, loss=1.0629032850265503
I1021 11:00:21.054571 140024166668032 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.3994522094726562, loss=1.107040524482727
I1021 11:03:09.207883 140198087026496 spec.py:321] Evaluating on the training split.
I1021 11:04:03.313107 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 11:04:55.013551 140198087026496 spec.py:349] Evaluating on the test split.
I1021 11:05:21.023207 140198087026496 submission_runner.py:395] Time since start: 47419.71s, 	Step: 53712, 	{'train/ctc_loss': Array(0.15992081, dtype=float32), 'train/wer': 0.05713687510736987, 'validation/ctc_loss': Array(0.34490946, dtype=float32), 'validation/wer': 0.10122903733454339, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18961944, dtype=float32), 'test/wer': 0.06318932423374565, 'test/num_examples': 2472, 'score': 43282.01273679733, 'total_duration': 47419.70767736435, 'accumulated_submission_time': 43282.01273679733, 'accumulated_eval_time': 4133.445558786392, 'accumulated_logging_time': 1.7801804542541504}
I1021 11:05:21.075703 140023908812544 logging_writer.py:48] [53712] accumulated_eval_time=4133.445559, accumulated_logging_time=1.780180, accumulated_submission_time=43282.012737, global_step=53712, preemption_count=0, score=43282.012737, test/ctc_loss=0.18961943686008453, test/num_examples=2472, test/wer=0.063189, total_duration=47419.707677, train/ctc_loss=0.1599208116531372, train/wer=0.057137, validation/ctc_loss=0.3449094593524933, validation/num_examples=5348, validation/wer=0.101229
I1021 11:09:00.464757 140023900419840 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.8666592836380005, loss=1.0832768678665161
I1021 11:16:00.895659 140023908812544 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.7265043258666992, loss=1.088452696800232
I1021 11:22:33.447528 140022925772544 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.294419288635254, loss=1.0561671257019043
I1021 11:29:21.143566 140198087026496 spec.py:321] Evaluating on the training split.
I1021 11:30:15.389780 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 11:31:06.240098 140198087026496 spec.py:349] Evaluating on the test split.
I1021 11:31:32.347405 140198087026496 submission_runner.py:395] Time since start: 48991.03s, 	Step: 55492, 	{'train/ctc_loss': Array(0.15798569, dtype=float32), 'train/wer': 0.05686761626834745, 'validation/ctc_loss': Array(0.33512616, dtype=float32), 'validation/wer': 0.09811058439614972, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18356822, dtype=float32), 'test/wer': 0.05987853675380334, 'test/num_examples': 2472, 'score': 44721.989095926285, 'total_duration': 48991.03154373169, 'accumulated_submission_time': 44721.989095926285, 'accumulated_eval_time': 4264.641788482666, 'accumulated_logging_time': 1.8483967781066895}
I1021 11:31:32.395637 140024502740736 logging_writer.py:48] [55492] accumulated_eval_time=4264.641788, accumulated_logging_time=1.848397, accumulated_submission_time=44721.989096, global_step=55492, preemption_count=0, score=44721.989096, test/ctc_loss=0.18356822431087494, test/num_examples=2472, test/wer=0.059879, total_duration=48991.031544, train/ctc_loss=0.15798568725585938, train/wer=0.056868, validation/ctc_loss=0.3351261615753174, validation/num_examples=5348, validation/wer=0.098111
I1021 11:31:39.307031 140024494348032 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.2543127536773682, loss=1.0267994403839111
I1021 11:38:04.434028 140023847380736 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.7976659536361694, loss=1.0522425174713135
I1021 11:45:04.434702 140023838988032 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.8490052223205566, loss=1.04849112033844
I1021 11:51:39.996397 140024502740736 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.5291038751602173, loss=1.0607165098190308
I1021 11:55:32.878186 140198087026496 spec.py:321] Evaluating on the training split.
I1021 11:56:27.273096 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 11:57:18.578521 140198087026496 spec.py:349] Evaluating on the test split.
I1021 11:57:44.415290 140198087026496 submission_runner.py:395] Time since start: 50563.10s, 	Step: 57279, 	{'train/ctc_loss': Array(0.14897352, dtype=float32), 'train/wer': 0.052902691511387163, 'validation/ctc_loss': Array(0.32784665, dtype=float32), 'validation/wer': 0.0953879722332178, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18057045, dtype=float32), 'test/wer': 0.059370747263014646, 'test/num_examples': 2472, 'score': 46162.37911748886, 'total_duration': 50563.09929227829, 'accumulated_submission_time': 46162.37911748886, 'accumulated_eval_time': 4396.17117357254, 'accumulated_logging_time': 1.913705587387085}
I1021 11:57:44.460324 140024502740736 logging_writer.py:48] [57279] accumulated_eval_time=4396.171174, accumulated_logging_time=1.913706, accumulated_submission_time=46162.379117, global_step=57279, preemption_count=0, score=46162.379117, test/ctc_loss=0.18057045340538025, test/num_examples=2472, test/wer=0.059371, total_duration=50563.099292, train/ctc_loss=0.1489735245704651, train/wer=0.052903, validation/ctc_loss=0.3278466463088989, validation/num_examples=5348, validation/wer=0.095388
I1021 12:00:32.985099 140024494348032 logging_writer.py:48] [57500] global_step=57500, grad_norm=2.4026763439178467, loss=1.0099976062774658
I1021 12:07:09.947035 140024502740736 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.3013663291931152, loss=0.9771495461463928
I1021 12:14:20.318192 140024494348032 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.776716947555542, loss=1.0277947187423706
I1021 12:21:06.563968 140024502740736 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.6984683275222778, loss=1.0068246126174927
I1021 12:21:44.435761 140198087026496 spec.py:321] Evaluating on the training split.
I1021 12:22:37.356846 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 12:23:28.073917 140198087026496 spec.py:349] Evaluating on the test split.
I1021 12:23:54.819693 140198087026496 submission_runner.py:395] Time since start: 52133.51s, 	Step: 59051, 	{'train/ctc_loss': Array(0.15792759, dtype=float32), 'train/wer': 0.05693485800624978, 'validation/ctc_loss': Array(0.3268275, dtype=float32), 'validation/wer': 0.09518522451895692, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17944317, dtype=float32), 'test/wer': 0.05855828407775273, 'test/num_examples': 2472, 'score': 47602.2635307312, 'total_duration': 52133.505554676056, 'accumulated_submission_time': 47602.2635307312, 'accumulated_eval_time': 4526.549210309982, 'accumulated_logging_time': 1.9743337631225586}
I1021 12:23:54.863093 140024717780736 logging_writer.py:48] [59051] accumulated_eval_time=4526.549210, accumulated_logging_time=1.974334, accumulated_submission_time=47602.263531, global_step=59051, preemption_count=0, score=47602.263531, test/ctc_loss=0.17944316565990448, test/num_examples=2472, test/wer=0.058558, total_duration=52133.505555, train/ctc_loss=0.15792758762836456, train/wer=0.056935, validation/ctc_loss=0.3268274962902069, validation/num_examples=5348, validation/wer=0.095185
I1021 12:29:51.108194 140024709388032 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.5769898891448975, loss=1.0266789197921753
I1021 12:36:42.091277 140198087026496 spec.py:321] Evaluating on the training split.
I1021 12:37:35.937442 140198087026496 spec.py:333] Evaluating on the validation split.
I1021 12:38:27.389453 140198087026496 spec.py:349] Evaluating on the test split.
I1021 12:38:53.583040 140198087026496 submission_runner.py:395] Time since start: 53032.27s, 	Step: 60000, 	{'train/ctc_loss': Array(0.16787261, dtype=float32), 'train/wer': 0.05832126248379965, 'validation/ctc_loss': Array(0.32642904, dtype=float32), 'validation/wer': 0.09506936868223641, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17920987, dtype=float32), 'test/wer': 0.05861921881664737, 'test/num_examples': 2472, 'score': 48369.43556380272, 'total_duration': 53032.27194094658, 'accumulated_submission_time': 48369.43556380272, 'accumulated_eval_time': 4658.038312196732, 'accumulated_logging_time': 2.0325448513031006}
I1021 12:38:53.619795 140023908812544 logging_writer.py:48] [60000] accumulated_eval_time=4658.038312, accumulated_logging_time=2.032545, accumulated_submission_time=48369.435564, global_step=60000, preemption_count=0, score=48369.435564, test/ctc_loss=0.1792098730802536, test/num_examples=2472, test/wer=0.058619, total_duration=53032.271941, train/ctc_loss=0.1678726077079773, train/wer=0.058321, validation/ctc_loss=0.3264290392398834, validation/num_examples=5348, validation/wer=0.095069
I1021 12:38:53.643483 140023900419840 logging_writer.py:48] [60000] global_step=60000, preemption_count=0, score=48369.435564
I1021 12:38:54.123188 140198087026496 checkpoints.py:490] Saving checkpoint at step: 60000
I1021 12:38:55.598701 140198087026496 checkpoints.py:422] Saved checkpoint at /experiment_runs/targets_check_conformer/adamw_run13/librispeech_conformer_jax/trial_1/checkpoint_60000
I1021 12:38:55.633538 140198087026496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/targets_check_conformer/adamw_run13/librispeech_conformer_jax/trial_1/checkpoint_60000.
I1021 12:38:57.056196 140198087026496 submission_runner.py:565] Tuning trial 1/1
I1021 12:38:57.056497 140198087026496 submission_runner.py:566] Hyperparameters: Hyperparameters(learning_rate=0.002106913873888147, beta1=0.8231189937738506, beta2=0.8774571227688758, warmup_steps=1199, weight_decay=0.27590534177690645)
I1021 12:38:57.076689 140198087026496 submission_runner.py:567] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.425308, dtype=float32), 'train/wer': 1.0707994086167045, 'validation/ctc_loss': Array(30.578106, dtype=float32), 'validation/wer': 1.0319472469756799, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.617601, dtype=float32), 'test/wer': 1.043365222513355, 'test/num_examples': 2472, 'score': 75.71397948265076, 'total_duration': 252.2725167274475, 'accumulated_submission_time': 75.71397948265076, 'accumulated_eval_time': 176.55847454071045, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1820, {'train/ctc_loss': Array(2.193813, dtype=float32), 'train/wer': 0.49749747687998186, 'validation/ctc_loss': Array(2.696645, dtype=float32), 'validation/wer': 0.5468974772391554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.3071384, dtype=float32), 'test/wer': 0.49781650518960857, 'test/num_examples': 2472, 'score': 1515.7065501213074, 'total_duration': 1821.2753655910492, 'accumulated_submission_time': 1515.7065501213074, 'accumulated_eval_time': 305.4402585029602, 'accumulated_logging_time': 0.05211830139160156, 'global_step': 1820, 'preemption_count': 0}), (3671, {'train/ctc_loss': Array(0.716506, dtype=float32), 'train/wer': 0.24138188210851558, 'validation/ctc_loss': Array(1.0385997, dtype=float32), 'validation/wer': 0.30102242775905846, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.749395, dtype=float32), 'test/wer': 0.2435155282026283, 'test/num_examples': 2472, 'score': 2955.695737838745, 'total_duration': 3390.7333834171295, 'accumulated_submission_time': 2955.695737838745, 'accumulated_eval_time': 434.7673707008362, 'accumulated_logging_time': 0.10907244682312012, 'global_step': 3671, 'preemption_count': 0}), (5516, {'train/ctc_loss': Array(0.57312626, dtype=float32), 'train/wer': 0.2016277390346369, 'validation/ctc_loss': Array(0.9277797, dtype=float32), 'validation/wer': 0.2725798198441739, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64360434, dtype=float32), 'test/wer': 0.2105092113013629, 'test/num_examples': 2472, 'score': 4395.957211256027, 'total_duration': 4963.1545214653015, 'accumulated_submission_time': 4395.957211256027, 'accumulated_eval_time': 566.7703504562378, 'accumulated_logging_time': 0.1822798252105713, 'global_step': 5516, 'preemption_count': 0}), (7321, {'train/ctc_loss': Array(0.5306109, dtype=float32), 'train/wer': 0.18209950884395062, 'validation/ctc_loss': Array(0.8476886, dtype=float32), 'validation/wer': 0.2511175260916999, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5728257, dtype=float32), 'test/wer': 0.18934454532528994, 'test/num_examples': 2472, 'score': 5836.187634706497, 'total_duration': 6534.346150159836, 'accumulated_submission_time': 5836.187634706497, 'accumulated_eval_time': 697.5930202007294, 'accumulated_logging_time': 0.23746156692504883, 'global_step': 7321, 'preemption_count': 0}), (9133, {'train/ctc_loss': Array(0.49094442, dtype=float32), 'train/wer': 0.17156820109156537, 'validation/ctc_loss': Array(0.79310954, dtype=float32), 'validation/wer': 0.23508114735896965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52749735, dtype=float32), 'test/wer': 0.17380618690715577, 'test/num_examples': 2472, 'score': 7276.659263849258, 'total_duration': 8106.6732313632965, 'accumulated_submission_time': 7276.659263849258, 'accumulated_eval_time': 829.3120892047882, 'accumulated_logging_time': 0.2909657955169678, 'global_step': 9133, 'preemption_count': 0}), (10925, {'train/ctc_loss': Array(0.5014538, dtype=float32), 'train/wer': 0.17137812014873222, 'validation/ctc_loss': Array(0.7821775, dtype=float32), 'validation/wer': 0.23025382082894852, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.515465, dtype=float32), 'test/wer': 0.17262811528852598, 'test/num_examples': 2472, 'score': 8716.886959075928, 'total_duration': 9678.316082954407, 'accumulated_submission_time': 8716.886959075928, 'accumulated_eval_time': 960.593284368515, 'accumulated_logging_time': 0.3422517776489258, 'global_step': 10925, 'preemption_count': 0}), (12728, {'train/ctc_loss': Array(0.4776491, dtype=float32), 'train/wer': 0.1640663220811265, 'validation/ctc_loss': Array(0.7791487, dtype=float32), 'validation/wer': 0.23111308495129226, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50749695, dtype=float32), 'test/wer': 0.16836268356590092, 'test/num_examples': 2472, 'score': 10157.152910709381, 'total_duration': 11251.290914058685, 'accumulated_submission_time': 10157.152910709381, 'accumulated_eval_time': 1093.1615800857544, 'accumulated_logging_time': 0.39956164360046387, 'global_step': 12728, 'preemption_count': 0}), (14477, {'train/ctc_loss': Array(0.42572266, dtype=float32), 'train/wer': 0.1530252996697782, 'validation/ctc_loss': Array(0.75034535, dtype=float32), 'validation/wer': 0.22285835658495612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48662755, dtype=float32), 'test/wer': 0.16373164340990798, 'test/num_examples': 2472, 'score': 11597.019608259201, 'total_duration': 12824.077835083008, 'accumulated_submission_time': 11597.019608259201, 'accumulated_eval_time': 1225.9461002349854, 'accumulated_logging_time': 0.4544496536254883, 'global_step': 14477, 'preemption_count': 0}), (16260, {'train/ctc_loss': Array(0.403061, dtype=float32), 'train/wer': 0.14105517399145603, 'validation/ctc_loss': Array(0.72263044, dtype=float32), 'validation/wer': 0.21480637593288085, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47058672, dtype=float32), 'test/wer': 0.1561148010480775, 'test/num_examples': 2472, 'score': 13037.290833950043, 'total_duration': 14396.150520086288, 'accumulated_submission_time': 13037.290833950043, 'accumulated_eval_time': 1357.6072144508362, 'accumulated_logging_time': 0.5123186111450195, 'global_step': 16260, 'preemption_count': 0}), (18041, {'train/ctc_loss': Array(0.40040925, dtype=float32), 'train/wer': 0.14585958909619395, 'validation/ctc_loss': Array(0.70875674, dtype=float32), 'validation/wer': 0.2101142145457003, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4586529, dtype=float32), 'test/wer': 0.15211341986066257, 'test/num_examples': 2472, 'score': 14477.724244832993, 'total_duration': 15968.594814777374, 'accumulated_submission_time': 14477.724244832993, 'accumulated_eval_time': 1489.4804847240448, 'accumulated_logging_time': 0.5687074661254883, 'global_step': 18041, 'preemption_count': 0}), (19832, {'train/ctc_loss': Array(0.41879952, dtype=float32), 'train/wer': 0.14645019699093048, 'validation/ctc_loss': Array(0.7252199, dtype=float32), 'validation/wer': 0.2155497842185041, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46211413, dtype=float32), 'test/wer': 0.15658196737960312, 'test/num_examples': 2472, 'score': 15918.192623853683, 'total_duration': 17541.840811014175, 'accumulated_submission_time': 15918.192623853683, 'accumulated_eval_time': 1622.1185681819916, 'accumulated_logging_time': 0.6266741752624512, 'global_step': 19832, 'preemption_count': 0}), (21620, {'train/ctc_loss': Array(0.41190353, dtype=float32), 'train/wer': 0.1419948822117287, 'validation/ctc_loss': Array(0.68248945, dtype=float32), 'validation/wer': 0.2026704770364077, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43280604, dtype=float32), 'test/wer': 0.14555277963967259, 'test/num_examples': 2472, 'score': 17358.230726480484, 'total_duration': 19114.40650701523, 'accumulated_submission_time': 17358.230726480484, 'accumulated_eval_time': 1754.5075569152832, 'accumulated_logging_time': 0.6849470138549805, 'global_step': 21620, 'preemption_count': 0}), (23404, {'train/ctc_loss': Array(0.38955504, dtype=float32), 'train/wer': 0.13493672188496758, 'validation/ctc_loss': Array(0.6696636, dtype=float32), 'validation/wer': 0.19759212952682545, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41805887, dtype=float32), 'test/wer': 0.13972335628541832, 'test/num_examples': 2472, 'score': 18798.447294950485, 'total_duration': 20688.61761546135, 'accumulated_submission_time': 18798.447294950485, 'accumulated_eval_time': 1888.358320236206, 'accumulated_logging_time': 0.747185230255127, 'global_step': 23404, 'preemption_count': 0}), (25213, {'train/ctc_loss': Array(0.36059958, dtype=float32), 'train/wer': 0.12565981166335882, 'validation/ctc_loss': Array(0.64565474, dtype=float32), 'validation/wer': 0.19088214565009606, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4041364, dtype=float32), 'test/wer': 0.13572197509800338, 'test/num_examples': 2472, 'score': 20238.43037390709, 'total_duration': 22261.60532474518, 'accumulated_submission_time': 20238.43037390709, 'accumulated_eval_time': 2021.213785648346, 'accumulated_logging_time': 0.8119029998779297, 'global_step': 25213, 'preemption_count': 0}), (27015, {'train/ctc_loss': Array(0.32621488, dtype=float32), 'train/wer': 0.11689402436161611, 'validation/ctc_loss': Array(0.6163341, dtype=float32), 'validation/wer': 0.18436525483456753, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.385615, dtype=float32), 'test/wer': 0.1300144212215384, 'test/num_examples': 2472, 'score': 21678.559955358505, 'total_duration': 23833.65296936035, 'accumulated_submission_time': 21678.559955358505, 'accumulated_eval_time': 2152.986874818802, 'accumulated_logging_time': 0.8726892471313477, 'global_step': 27015, 'preemption_count': 0}), (28783, {'train/ctc_loss': Array(0.30786785, dtype=float32), 'train/wer': 0.108743901565904, 'validation/ctc_loss': Array(0.5970837, dtype=float32), 'validation/wer': 0.17703737316199541, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3635187, dtype=float32), 'test/wer': 0.12170698515223528, 'test/num_examples': 2472, 'score': 23118.97879600525, 'total_duration': 25408.23015165329, 'accumulated_submission_time': 23118.97879600525, 'accumulated_eval_time': 2287.0078358650208, 'accumulated_logging_time': 0.9285540580749512, 'global_step': 28783, 'preemption_count': 0}), (30545, {'train/ctc_loss': Array(0.3210939, dtype=float32), 'train/wer': 0.1139874761657133, 'validation/ctc_loss': Array(0.58573115, dtype=float32), 'validation/wer': 0.1738127190399413, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35593492, dtype=float32), 'test/wer': 0.11898523348160786, 'test/num_examples': 2472, 'score': 24558.852508544922, 'total_duration': 26980.40470767021, 'accumulated_submission_time': 24558.852508544922, 'accumulated_eval_time': 2419.1657495498657, 'accumulated_logging_time': 0.9904017448425293, 'global_step': 30545, 'preemption_count': 0}), (32345, {'train/ctc_loss': Array(0.2966427, dtype=float32), 'train/wer': 0.10325612184547936, 'validation/ctc_loss': Array(0.57263756, dtype=float32), 'validation/wer': 0.16969983683636328, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34842104, dtype=float32), 'test/wer': 0.11437450490524648, 'test/num_examples': 2472, 'score': 25998.96476507187, 'total_duration': 28553.576659440994, 'accumulated_submission_time': 25998.96476507187, 'accumulated_eval_time': 2552.0850961208344, 'accumulated_logging_time': 1.0472321510314941, 'global_step': 32345, 'preemption_count': 0}), (34110, {'train/ctc_loss': Array(0.31054127, dtype=float32), 'train/wer': 0.10908773353744047, 'validation/ctc_loss': Array(0.56212187, dtype=float32), 'validation/wer': 0.16798130859167576, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33788675, dtype=float32), 'test/wer': 0.11404951963114171, 'test/num_examples': 2472, 'score': 27439.37448334694, 'total_duration': 30127.621012210846, 'accumulated_submission_time': 27439.37448334694, 'accumulated_eval_time': 2685.5816509723663, 'accumulated_logging_time': 1.103471279144287, 'global_step': 34110, 'preemption_count': 0}), (35903, {'train/ctc_loss': Array(0.28951797, dtype=float32), 'train/wer': 0.09782388610036553, 'validation/ctc_loss': Array(0.5307529, dtype=float32), 'validation/wer': 0.15763152051131044, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31606698, dtype=float32), 'test/wer': 0.10600613409704873, 'test/num_examples': 2472, 'score': 28879.892230033875, 'total_duration': 31700.31954073906, 'accumulated_submission_time': 28879.892230033875, 'accumulated_eval_time': 2817.620602607727, 'accumulated_logging_time': 1.1625759601593018, 'global_step': 35903, 'preemption_count': 0}), (37700, {'train/ctc_loss': Array(0.2151113, dtype=float32), 'train/wer': 0.0776062646700081, 'validation/ctc_loss': Array(0.500329, dtype=float32), 'validation/wer': 0.15020709230813792, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29686123, dtype=float32), 'test/wer': 0.09910019702232242, 'test/num_examples': 2472, 'score': 30319.913461446762, 'total_duration': 33274.215215206146, 'accumulated_submission_time': 30319.913461446762, 'accumulated_eval_time': 2951.3520109653473, 'accumulated_logging_time': 1.223691463470459, 'global_step': 37700, 'preemption_count': 0}), (39509, {'train/ctc_loss': Array(0.23098134, dtype=float32), 'train/wer': 0.08047348524846361, 'validation/ctc_loss': Array(0.48058802, dtype=float32), 'validation/wer': 0.14288886528862585, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28094158, dtype=float32), 'test/wer': 0.09418479475148782, 'test/num_examples': 2472, 'score': 31760.36798143387, 'total_duration': 34847.16221642494, 'accumulated_submission_time': 31760.36798143387, 'accumulated_eval_time': 3083.706532716751, 'accumulated_logging_time': 1.2791519165039062, 'global_step': 39509, 'preemption_count': 0}), (41292, {'train/ctc_loss': Array(0.2832291, dtype=float32), 'train/wer': 0.09786128453106101, 'validation/ctc_loss': Array(0.46717575, dtype=float32), 'validation/wer': 0.13716365602402078, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27615497, dtype=float32), 'test/wer': 0.09268173785875328, 'test/num_examples': 2472, 'score': 33200.31581401825, 'total_duration': 36418.3018321991, 'accumulated_submission_time': 33200.31581401825, 'accumulated_eval_time': 3214.754864692688, 'accumulated_logging_time': 1.3403780460357666, 'global_step': 41292, 'preemption_count': 0}), (43058, {'train/ctc_loss': Array(0.27360976, dtype=float32), 'train/wer': 0.09577710298272982, 'validation/ctc_loss': Array(0.43741032, dtype=float32), 'validation/wer': 0.1310715699431341, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2558681, dtype=float32), 'test/wer': 0.08601953973960555, 'test/num_examples': 2472, 'score': 34640.58737039566, 'total_duration': 37989.29826402664, 'accumulated_submission_time': 34640.58737039566, 'accumulated_eval_time': 3345.3365590572357, 'accumulated_logging_time': 1.400590181350708, 'global_step': 43058, 'preemption_count': 0}), (44837, {'train/ctc_loss': Array(0.30323243, dtype=float32), 'train/wer': 0.10670309035922575, 'validation/ctc_loss': Array(0.42683902, dtype=float32), 'validation/wer': 0.12672697606611508, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24299584, dtype=float32), 'test/wer': 0.08165255011882275, 'test/num_examples': 2472, 'score': 36080.67891216278, 'total_duration': 39558.77188563347, 'accumulated_submission_time': 36080.67891216278, 'accumulated_eval_time': 3474.5739798545837, 'accumulated_logging_time': 1.4610958099365234, 'global_step': 44837, 'preemption_count': 0}), (46612, {'train/ctc_loss': Array(0.25414667, dtype=float32), 'train/wer': 0.08765432825627469, 'validation/ctc_loss': Array(0.40933293, dtype=float32), 'validation/wer': 0.1209824574953899, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22970492, dtype=float32), 'test/wer': 0.07763085735177624, 'test/num_examples': 2472, 'score': 37521.242700099945, 'total_duration': 41130.3929233551, 'accumulated_submission_time': 37521.242700099945, 'accumulated_eval_time': 3605.485710144043, 'accumulated_logging_time': 1.5238821506500244, 'global_step': 46612, 'preemption_count': 0}), (48371, {'train/ctc_loss': Array(0.21986894, dtype=float32), 'train/wer': 0.07861334105307695, 'validation/ctc_loss': Array(0.38523635, dtype=float32), 'validation/wer': 0.11406972590439962, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21609479, dtype=float32), 'test/wer': 0.06958747181768327, 'test/num_examples': 2472, 'score': 38961.31883120537, 'total_duration': 42701.90442609787, 'accumulated_submission_time': 38961.31883120537, 'accumulated_eval_time': 3736.781819343567, 'accumulated_logging_time': 1.5830357074737549, 'global_step': 48371, 'preemption_count': 0}), (50136, {'train/ctc_loss': Array(0.1773363, dtype=float32), 'train/wer': 0.06374185689893311, 'validation/ctc_loss': Array(0.37084493, dtype=float32), 'validation/wer': 0.10877897602749645, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20711939, dtype=float32), 'test/wer': 0.0691812402250523, 'test/num_examples': 2472, 'score': 40401.594277620316, 'total_duration': 44273.67018413544, 'accumulated_submission_time': 40401.594277620316, 'accumulated_eval_time': 3868.125723838806, 'accumulated_logging_time': 1.648780345916748, 'global_step': 50136, 'preemption_count': 0}), (51935, {'train/ctc_loss': Array(0.18663979, dtype=float32), 'train/wer': 0.06758921831127428, 'validation/ctc_loss': Array(0.3541452, dtype=float32), 'validation/wer': 0.1046564391708584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19410646, dtype=float32), 'test/wer': 0.06463144638758556, 'test/num_examples': 2472, 'score': 41842.131314992905, 'total_duration': 45847.85550451279, 'accumulated_submission_time': 41842.131314992905, 'accumulated_eval_time': 4001.6373660564423, 'accumulated_logging_time': 1.700425386428833, 'global_step': 51935, 'preemption_count': 0}), (53712, {'train/ctc_loss': Array(0.15992081, dtype=float32), 'train/wer': 0.05713687510736987, 'validation/ctc_loss': Array(0.34490946, dtype=float32), 'validation/wer': 0.10122903733454339, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18961944, dtype=float32), 'test/wer': 0.06318932423374565, 'test/num_examples': 2472, 'score': 43282.01273679733, 'total_duration': 47419.70767736435, 'accumulated_submission_time': 43282.01273679733, 'accumulated_eval_time': 4133.445558786392, 'accumulated_logging_time': 1.7801804542541504, 'global_step': 53712, 'preemption_count': 0}), (55492, {'train/ctc_loss': Array(0.15798569, dtype=float32), 'train/wer': 0.05686761626834745, 'validation/ctc_loss': Array(0.33512616, dtype=float32), 'validation/wer': 0.09811058439614972, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18356822, dtype=float32), 'test/wer': 0.05987853675380334, 'test/num_examples': 2472, 'score': 44721.989095926285, 'total_duration': 48991.03154373169, 'accumulated_submission_time': 44721.989095926285, 'accumulated_eval_time': 4264.641788482666, 'accumulated_logging_time': 1.8483967781066895, 'global_step': 55492, 'preemption_count': 0}), (57279, {'train/ctc_loss': Array(0.14897352, dtype=float32), 'train/wer': 0.052902691511387163, 'validation/ctc_loss': Array(0.32784665, dtype=float32), 'validation/wer': 0.0953879722332178, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18057045, dtype=float32), 'test/wer': 0.059370747263014646, 'test/num_examples': 2472, 'score': 46162.37911748886, 'total_duration': 50563.09929227829, 'accumulated_submission_time': 46162.37911748886, 'accumulated_eval_time': 4396.17117357254, 'accumulated_logging_time': 1.913705587387085, 'global_step': 57279, 'preemption_count': 0}), (59051, {'train/ctc_loss': Array(0.15792759, dtype=float32), 'train/wer': 0.05693485800624978, 'validation/ctc_loss': Array(0.3268275, dtype=float32), 'validation/wer': 0.09518522451895692, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17944317, dtype=float32), 'test/wer': 0.05855828407775273, 'test/num_examples': 2472, 'score': 47602.2635307312, 'total_duration': 52133.505554676056, 'accumulated_submission_time': 47602.2635307312, 'accumulated_eval_time': 4526.549210309982, 'accumulated_logging_time': 1.9743337631225586, 'global_step': 59051, 'preemption_count': 0}), (60000, {'train/ctc_loss': Array(0.16787261, dtype=float32), 'train/wer': 0.05832126248379965, 'validation/ctc_loss': Array(0.32642904, dtype=float32), 'validation/wer': 0.09506936868223641, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17920987, dtype=float32), 'test/wer': 0.05861921881664737, 'test/num_examples': 2472, 'score': 48369.43556380272, 'total_duration': 53032.27194094658, 'accumulated_submission_time': 48369.43556380272, 'accumulated_eval_time': 4658.038312196732, 'accumulated_logging_time': 2.0325448513031006, 'global_step': 60000, 'preemption_count': 0})], 'global_step': 60000}
I1021 12:38:57.077022 140198087026496 submission_runner.py:568] Timing: 48369.43556380272
I1021 12:38:57.077109 140198087026496 submission_runner.py:570] Total number of evals: 35
I1021 12:38:57.077182 140198087026496 submission_runner.py:571] ====================
I1021 12:38:57.081820 140198087026496 submission_runner.py:647] Final librispeech_conformer score: 48369.43556380272
