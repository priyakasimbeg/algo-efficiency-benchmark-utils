torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_conformer --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_pytorch_06-09-2023-16-57-30.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 16:57:53.796107 139718178785088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 16:57:53.796208 140195547076416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 16:57:53.796250 140506113992512 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 16:57:53.796285 139817453094720 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 16:57:53.796854 139660606883648 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 16:57:53.797059 139812263409472 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 16:57:54.763309 140586526959424 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 16:57:54.765050 139857213830976 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 16:57:54.765364 139857213830976 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 16:57:54.774233 140586526959424 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 16:57:54.774367 140195547076416 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 16:57:54.774386 140506113992512 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 16:57:54.774410 139817453094720 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 16:57:54.774419 139718178785088 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 16:57:54.774438 139660606883648 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 16:57:54.774463 139812263409472 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 16:57:55.127294 139857213830976 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/momentum/librispeech_conformer_pytorch.
W0609 16:57:55.447372 140195547076416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 16:57:55.447837 140506113992512 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 16:57:55.448113 139817453094720 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 16:57:55.449125 139660606883648 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 16:57:55.450464 139857213830976 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 16:57:55.453166 140586526959424 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 16:57:55.454112 139718178785088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 16:57:55.456052 139857213830976 submission_runner.py:541] Using RNG seed 1089755124
I0609 16:57:55.457454 139857213830976 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 16:57:55.457567 139857213830976 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/momentum/librispeech_conformer_pytorch/trial_1.
W0609 16:57:55.457683 139812263409472 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 16:57:55.457782 139857213830976 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/momentum/librispeech_conformer_pytorch/trial_1/hparams.json.
I0609 16:57:55.458770 139857213830976 submission_runner.py:255] Initializing dataset.
I0609 16:57:55.458901 139857213830976 input_pipeline.py:20] Loading split = train-clean-100
I0609 16:57:55.494133 139857213830976 input_pipeline.py:20] Loading split = train-clean-360
I0609 16:57:55.838458 139857213830976 input_pipeline.py:20] Loading split = train-other-500
I0609 16:57:56.289813 139857213830976 submission_runner.py:262] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0609 16:58:03.164372 139857213830976 submission_runner.py:272] Initializing optimizer.
I0609 16:58:03.650605 139857213830976 submission_runner.py:279] Initializing metrics bundle.
I0609 16:58:03.650825 139857213830976 submission_runner.py:297] Initializing checkpoint and logger.
I0609 16:58:03.652518 139857213830976 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 16:58:03.652642 139857213830976 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 16:58:04.222251 139857213830976 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/momentum/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0609 16:58:04.223205 139857213830976 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/momentum/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0609 16:58:04.230870 139857213830976 submission_runner.py:332] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0609 16:58:12.230128 139830594164480 logging_writer.py:48] [0] global_step=0, grad_norm=43.684967, loss=32.573437
I0609 16:58:12.253603 139857213830976 spec.py:298] Evaluating on the training split.
I0609 16:58:12.254783 139857213830976 input_pipeline.py:20] Loading split = train-clean-100
I0609 16:58:12.291476 139857213830976 input_pipeline.py:20] Loading split = train-clean-360
I0609 16:58:12.725042 139857213830976 input_pipeline.py:20] Loading split = train-other-500
I0609 16:58:29.673746 139857213830976 spec.py:310] Evaluating on the validation split.
I0609 16:58:29.675099 139857213830976 input_pipeline.py:20] Loading split = dev-clean
I0609 16:58:29.678858 139857213830976 input_pipeline.py:20] Loading split = dev-other
I0609 16:58:40.863417 139857213830976 spec.py:326] Evaluating on the test split.
I0609 16:58:40.864794 139857213830976 input_pipeline.py:20] Loading split = test-clean
I0609 16:58:46.756570 139857213830976 submission_runner.py:419] Time since start: 42.53s, 	Step: 1, 	{'train/ctc_loss': 31.407430119065868, 'train/wer': 2.2530743105556312, 'validation/ctc_loss': 30.186661769138034, 'validation/wer': 2.129319750881089, 'validation/num_examples': 5348, 'test/ctc_loss': 30.25707464997742, 'test/wer': 2.168850161477058, 'test/num_examples': 2472, 'score': 8.022817134857178, 'total_duration': 42.52596831321716, 'accumulated_submission_time': 8.022817134857178, 'accumulated_eval_time': 34.50271821022034, 'accumulated_logging_time': 0}
I0609 16:58:46.783450 139828934997760 logging_writer.py:48] [1] accumulated_eval_time=34.502718, accumulated_logging_time=0, accumulated_submission_time=8.022817, global_step=1, preemption_count=0, score=8.022817, test/ctc_loss=30.257075, test/num_examples=2472, test/wer=2.168850, total_duration=42.525968, train/ctc_loss=31.407430, train/wer=2.253074, validation/ctc_loss=30.186662, validation/num_examples=5348, validation/wer=2.129320
I0609 16:58:46.828539 139812263409472 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 16:58:46.828904 139660606883648 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 16:58:46.829256 139857213830976 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 16:58:46.828970 139718178785088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 16:58:46.829043 140506113992512 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 16:58:46.829103 140195547076416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 16:58:46.829094 139817453094720 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 16:58:46.829171 140586526959424 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 16:58:47.886649 139828926605056 logging_writer.py:48] [1] global_step=1, grad_norm=45.633823, loss=31.817129
I0609 16:58:48.745398 139828934997760 logging_writer.py:48] [2] global_step=2, grad_norm=46.300510, loss=32.159142
I0609 16:58:49.703811 139828926605056 logging_writer.py:48] [3] global_step=3, grad_norm=58.335751, loss=31.400772
I0609 16:58:50.494230 139828934997760 logging_writer.py:48] [4] global_step=4, grad_norm=75.156975, loss=28.550058
I0609 16:58:51.287840 139828926605056 logging_writer.py:48] [5] global_step=5, grad_norm=75.839096, loss=24.813837
I0609 16:58:52.080147 139828934997760 logging_writer.py:48] [6] global_step=6, grad_norm=58.773781, loss=20.130325
I0609 16:58:52.870563 139828926605056 logging_writer.py:48] [7] global_step=7, grad_norm=74.380913, loss=20.750448
I0609 16:58:53.665147 139828934997760 logging_writer.py:48] [8] global_step=8, grad_norm=80.237190, loss=19.573288
I0609 16:58:54.458293 139828926605056 logging_writer.py:48] [9] global_step=9, grad_norm=60.040432, loss=16.913984
I0609 16:58:55.250547 139828934997760 logging_writer.py:48] [10] global_step=10, grad_norm=45.525696, loss=16.137617
I0609 16:58:56.041389 139828926605056 logging_writer.py:48] [11] global_step=11, grad_norm=61.731407, loss=16.614016
I0609 16:58:56.834653 139828934997760 logging_writer.py:48] [12] global_step=12, grad_norm=81.957756, loss=16.399071
I0609 16:58:57.628772 139828926605056 logging_writer.py:48] [13] global_step=13, grad_norm=88.932228, loss=10.823925
I0609 16:58:58.421241 139828934997760 logging_writer.py:48] [14] global_step=14, grad_norm=26.247923, loss=10.000045
I0609 16:58:59.212103 139828926605056 logging_writer.py:48] [15] global_step=15, grad_norm=27.093500, loss=12.242976
I0609 16:59:00.005377 139828934997760 logging_writer.py:48] [16] global_step=16, grad_norm=27.393057, loss=13.298686
I0609 16:59:00.799853 139828926605056 logging_writer.py:48] [17] global_step=17, grad_norm=28.276299, loss=13.187438
I0609 16:59:01.590887 139828934997760 logging_writer.py:48] [18] global_step=18, grad_norm=29.630024, loss=11.843588
I0609 16:59:02.384856 139828926605056 logging_writer.py:48] [19] global_step=19, grad_norm=29.689823, loss=9.314704
I0609 16:59:03.179808 139828934997760 logging_writer.py:48] [20] global_step=20, grad_norm=38.826130, loss=7.944707
I0609 16:59:03.974196 139828926605056 logging_writer.py:48] [21] global_step=21, grad_norm=131.120789, loss=13.971446
I0609 16:59:04.767159 139828934997760 logging_writer.py:48] [22] global_step=22, grad_norm=10.720267, loss=7.431375
I0609 16:59:05.561354 139828926605056 logging_writer.py:48] [23] global_step=23, grad_norm=28.082932, loss=9.611181
I0609 16:59:06.353988 139828934997760 logging_writer.py:48] [24] global_step=24, grad_norm=27.721304, loss=10.880141
I0609 16:59:07.144892 139828926605056 logging_writer.py:48] [25] global_step=25, grad_norm=27.059174, loss=10.729190
I0609 16:59:07.936458 139828934997760 logging_writer.py:48] [26] global_step=26, grad_norm=25.847969, loss=9.444254
I0609 16:59:08.731348 139828926605056 logging_writer.py:48] [27] global_step=27, grad_norm=13.899667, loss=7.433406
I0609 16:59:09.524084 139828934997760 logging_writer.py:48] [28] global_step=28, grad_norm=70.859970, loss=9.827390
I0609 16:59:10.315359 139828926605056 logging_writer.py:48] [29] global_step=29, grad_norm=47.116165, loss=8.398492
I0609 16:59:11.109486 139828934997760 logging_writer.py:48] [30] global_step=30, grad_norm=18.206726, loss=7.717862
I0609 16:59:11.903439 139828926605056 logging_writer.py:48] [31] global_step=31, grad_norm=24.059284, loss=9.147561
I0609 16:59:12.698046 139828934997760 logging_writer.py:48] [32] global_step=32, grad_norm=24.235680, loss=9.511697
I0609 16:59:13.490616 139828926605056 logging_writer.py:48] [33] global_step=33, grad_norm=22.574598, loss=8.630113
I0609 16:59:14.285130 139828934997760 logging_writer.py:48] [34] global_step=34, grad_norm=6.932014, loss=7.199267
I0609 16:59:15.078773 139828926605056 logging_writer.py:48] [35] global_step=35, grad_norm=62.686234, loss=9.451968
I0609 16:59:15.869506 139828934997760 logging_writer.py:48] [36] global_step=36, grad_norm=18.903368, loss=7.243213
I0609 16:59:16.663712 139828926605056 logging_writer.py:48] [37] global_step=37, grad_norm=18.418552, loss=7.696053
I0609 16:59:17.458962 139828934997760 logging_writer.py:48] [38] global_step=38, grad_norm=22.074593, loss=8.514271
I0609 16:59:18.254053 139828926605056 logging_writer.py:48] [39] global_step=39, grad_norm=20.974047, loss=8.189761
I0609 16:59:19.049219 139828934997760 logging_writer.py:48] [40] global_step=40, grad_norm=8.744940, loss=7.096390
I0609 16:59:19.843964 139828926605056 logging_writer.py:48] [41] global_step=41, grad_norm=45.626389, loss=8.343674
I0609 16:59:20.636624 139828934997760 logging_writer.py:48] [42] global_step=42, grad_norm=16.550652, loss=7.211422
I0609 16:59:21.426767 139828926605056 logging_writer.py:48] [43] global_step=43, grad_norm=16.266432, loss=7.455433
I0609 16:59:22.221735 139828934997760 logging_writer.py:48] [44] global_step=44, grad_norm=20.070864, loss=8.012191
I0609 16:59:23.013087 139828926605056 logging_writer.py:48] [45] global_step=45, grad_norm=16.707592, loss=7.414826
I0609 16:59:23.806245 139828934997760 logging_writer.py:48] [46] global_step=46, grad_norm=9.754874, loss=6.987115
I0609 16:59:24.598251 139828926605056 logging_writer.py:48] [47] global_step=47, grad_norm=40.301830, loss=8.017429
I0609 16:59:25.391816 139828934997760 logging_writer.py:48] [48] global_step=48, grad_norm=7.771156, loss=6.966832
I0609 16:59:26.186228 139828926605056 logging_writer.py:48] [49] global_step=49, grad_norm=18.695055, loss=7.642997
I0609 16:59:26.979268 139828934997760 logging_writer.py:48] [50] global_step=50, grad_norm=17.694965, loss=7.513499
I0609 16:59:27.772566 139828926605056 logging_writer.py:48] [51] global_step=51, grad_norm=1.122534, loss=6.736840
I0609 16:59:28.565038 139828934997760 logging_writer.py:48] [52] global_step=52, grad_norm=39.227833, loss=7.865334
I0609 16:59:29.358840 139828926605056 logging_writer.py:48] [53] global_step=53, grad_norm=4.211484, loss=6.729360
I0609 16:59:30.153161 139828934997760 logging_writer.py:48] [54] global_step=54, grad_norm=17.155813, loss=7.385209
I0609 16:59:30.949276 139828926605056 logging_writer.py:48] [55] global_step=55, grad_norm=15.746491, loss=7.163313
I0609 16:59:31.744533 139828934997760 logging_writer.py:48] [56] global_step=56, grad_norm=4.893809, loss=6.660881
I0609 16:59:32.537355 139828926605056 logging_writer.py:48] [57] global_step=57, grad_norm=33.695896, loss=7.453756
I0609 16:59:33.331302 139828934997760 logging_writer.py:48] [58] global_step=58, grad_norm=10.342069, loss=6.742522
I0609 16:59:34.126903 139828926605056 logging_writer.py:48] [59] global_step=59, grad_norm=17.590048, loss=7.327280
I0609 16:59:34.921713 139828934997760 logging_writer.py:48] [60] global_step=60, grad_norm=12.519566, loss=6.836085
I0609 16:59:35.716198 139828926605056 logging_writer.py:48] [61] global_step=61, grad_norm=21.478121, loss=6.959296
I0609 16:59:36.507748 139828934997760 logging_writer.py:48] [62] global_step=62, grad_norm=14.109299, loss=6.694376
I0609 16:59:37.300052 139828926605056 logging_writer.py:48] [63] global_step=63, grad_norm=12.991504, loss=6.785648
I0609 16:59:38.094603 139828934997760 logging_writer.py:48] [64] global_step=64, grad_norm=15.122705, loss=6.935183
I0609 16:59:38.890508 139828926605056 logging_writer.py:48] [65] global_step=65, grad_norm=0.798211, loss=6.463819
I0609 16:59:39.683727 139828934997760 logging_writer.py:48] [66] global_step=66, grad_norm=28.376228, loss=7.023257
I0609 16:59:40.480823 139828926605056 logging_writer.py:48] [67] global_step=67, grad_norm=9.366294, loss=6.570862
I0609 16:59:41.275165 139828934997760 logging_writer.py:48] [68] global_step=68, grad_norm=15.698647, loss=6.909877
I0609 16:59:42.070272 139828926605056 logging_writer.py:48] [69] global_step=69, grad_norm=5.512452, loss=6.464553
I0609 16:59:42.867794 139828934997760 logging_writer.py:48] [70] global_step=70, grad_norm=27.275415, loss=6.913870
I0609 16:59:43.667423 139828926605056 logging_writer.py:48] [71] global_step=71, grad_norm=6.114949, loss=6.385503
I0609 16:59:44.461757 139828934997760 logging_writer.py:48] [72] global_step=72, grad_norm=14.428126, loss=6.719500
I0609 16:59:45.259590 139828926605056 logging_writer.py:48] [73] global_step=73, grad_norm=4.926939, loss=6.277377
I0609 16:59:46.054867 139828934997760 logging_writer.py:48] [74] global_step=74, grad_norm=25.085339, loss=6.810087
I0609 16:59:46.850056 139828926605056 logging_writer.py:48] [75] global_step=75, grad_norm=7.345551, loss=6.321312
I0609 16:59:47.644937 139828934997760 logging_writer.py:48] [76] global_step=76, grad_norm=13.784506, loss=6.612494
I0609 16:59:48.438925 139828926605056 logging_writer.py:48] [77] global_step=77, grad_norm=1.606135, loss=6.205888
I0609 16:59:49.235743 139828934997760 logging_writer.py:48] [78] global_step=78, grad_norm=25.075413, loss=6.754576
I0609 16:59:50.033574 139828926605056 logging_writer.py:48] [79] global_step=79, grad_norm=11.721842, loss=6.460152
I0609 16:59:50.826119 139828934997760 logging_writer.py:48] [80] global_step=80, grad_norm=14.867456, loss=6.648649
I0609 16:59:51.621545 139828926605056 logging_writer.py:48] [81] global_step=81, grad_norm=3.397552, loss=6.191353
I0609 16:59:52.415828 139828934997760 logging_writer.py:48] [82] global_step=82, grad_norm=26.595411, loss=6.711172
I0609 16:59:53.214068 139828926605056 logging_writer.py:48] [83] global_step=83, grad_norm=15.008883, loss=6.633614
I0609 16:59:54.007315 139828934997760 logging_writer.py:48] [84] global_step=84, grad_norm=17.211981, loss=6.879517
I0609 16:59:54.801787 139828926605056 logging_writer.py:48] [85] global_step=85, grad_norm=3.656469, loss=6.085152
I0609 16:59:55.595967 139828934997760 logging_writer.py:48] [86] global_step=86, grad_norm=35.558205, loss=7.101058
I0609 16:59:56.386412 139828926605056 logging_writer.py:48] [87] global_step=87, grad_norm=18.517826, loss=7.062346
I0609 16:59:57.183801 139828934997760 logging_writer.py:48] [88] global_step=88, grad_norm=20.431971, loss=7.725963
I0609 16:59:57.985893 139828926605056 logging_writer.py:48] [89] global_step=89, grad_norm=10.722166, loss=6.265904
I0609 16:59:58.783027 139828934997760 logging_writer.py:48] [90] global_step=90, grad_norm=64.164589, loss=9.364512
I0609 16:59:59.580312 139828926605056 logging_writer.py:48] [91] global_step=91, grad_norm=20.842112, loss=7.991394
I0609 17:00:00.372428 139828934997760 logging_writer.py:48] [92] global_step=92, grad_norm=21.979116, loss=10.241479
I0609 17:00:01.172855 139828926605056 logging_writer.py:48] [93] global_step=93, grad_norm=21.786207, loss=9.479435
I0609 17:00:01.970405 139828934997760 logging_writer.py:48] [94] global_step=94, grad_norm=12.288667, loss=6.280357
I0609 17:00:02.765431 139828926605056 logging_writer.py:48] [95] global_step=95, grad_norm=88.042923, loss=15.542352
I0609 17:00:03.560116 139828934997760 logging_writer.py:48] [96] global_step=96, grad_norm=20.750463, loss=8.055780
I0609 17:00:04.358246 139828926605056 logging_writer.py:48] [97] global_step=97, grad_norm=21.808239, loss=11.946280
I0609 17:00:05.152462 139828934997760 logging_writer.py:48] [98] global_step=98, grad_norm=21.766281, loss=12.580307
I0609 17:00:05.947134 139828926605056 logging_writer.py:48] [99] global_step=99, grad_norm=21.477470, loss=10.134138
I0609 17:00:06.743683 139828934997760 logging_writer.py:48] [100] global_step=100, grad_norm=4.640697, loss=6.081886
I0609 17:05:21.473827 139828926605056 logging_writer.py:48] [500] global_step=500, grad_norm=3.978880, loss=6.010100
I0609 17:11:55.004143 139828934997760 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.298100, loss=5.812075
I0609 17:18:29.472664 139828934997760 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.267749, loss=5.778262
I0609 17:25:03.872202 139828926605056 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.185419, loss=5.787518
I0609 17:31:38.466673 139828926605056 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.099974, loss=5.792159
I0609 17:38:12.638522 139828918212352 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.166814, loss=5.800239
I0609 17:38:47.307096 139857213830976 spec.py:298] Evaluating on the training split.
I0609 17:38:57.562624 139857213830976 spec.py:310] Evaluating on the validation split.
I0609 17:39:07.069595 139857213830976 spec.py:326] Evaluating on the test split.
I0609 17:39:12.461591 139857213830976 submission_runner.py:419] Time since start: 2468.23s, 	Step: 3045, 	{'train/ctc_loss': 5.924086991329824, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': 5.957203919278682, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': 5.945292438221821, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2406.2797107696533, 'total_duration': 2468.2309124469757, 'accumulated_submission_time': 2406.2797107696533, 'accumulated_eval_time': 59.657020568847656, 'accumulated_logging_time': 0.03559732437133789}
I0609 17:39:12.480833 139828926605056 logging_writer.py:48] [3045] accumulated_eval_time=59.657021, accumulated_logging_time=0.035597, accumulated_submission_time=2406.279711, global_step=3045, preemption_count=0, score=2406.279711, test/ctc_loss=5.945292, test/num_examples=2472, test/wer=0.899580, total_duration=2468.230912, train/ctc_loss=5.924087, train/wer=0.941573, validation/ctc_loss=5.957204, validation/num_examples=5348, validation/wer=0.896722
I0609 17:45:12.404767 139828918212352 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.196339, loss=5.764128
I0609 17:51:46.929152 139828029028096 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.196665, loss=5.801624
I0609 17:58:16.088555 139828029028096 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0609 18:04:32.361991 139828020635392 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0609 18:10:49.097382 139828029028096 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0609 18:17:05.319336 139828020635392 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0609 18:19:13.203176 139857213830976 spec.py:298] Evaluating on the training split.
I0609 18:19:23.172513 139857213830976 spec.py:310] Evaluating on the validation split.
I0609 18:19:32.557858 139857213830976 spec.py:326] Evaluating on the test split.
I0609 18:19:37.735687 139857213830976 submission_runner.py:419] Time since start: 4893.51s, 	Step: 6171, 	{'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4804.394938468933, 'total_duration': 4893.505056858063, 'accumulated_submission_time': 4804.394938468933, 'accumulated_eval_time': 84.18939018249512, 'accumulated_logging_time': 0.06410527229309082}
I0609 18:19:37.756284 139828029028096 logging_writer.py:48] [6171] accumulated_eval_time=84.189390, accumulated_logging_time=0.064105, accumulated_submission_time=4804.394938, global_step=6171, preemption_count=0, score=4804.394938, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=4893.505057, train/ctc_loss=nan, train/wer=0.941573, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0609 18:23:46.730022 139828029028096 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0609 18:30:02.761301 139828020635392 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0609 18:36:19.859648 139828029028096 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0609 18:42:35.759015 139828020635392 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0609 18:48:52.762212 139828029028096 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0609 18:55:08.787560 139828020635392 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0609 18:59:37.894707 139857213830976 spec.py:298] Evaluating on the training split.
I0609 18:59:47.894439 139857213830976 spec.py:310] Evaluating on the validation split.
I0609 18:59:57.369921 139857213830976 spec.py:326] Evaluating on the test split.
I0609 19:00:02.835896 139857213830976 submission_runner.py:419] Time since start: 7318.61s, 	Step: 9357, 	{'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7201.72297668457, 'total_duration': 7318.605179548264, 'accumulated_submission_time': 7201.72297668457, 'accumulated_eval_time': 109.13051128387451, 'accumulated_logging_time': 0.0950925350189209}
I0609 19:00:02.856712 139828020635392 logging_writer.py:48] [9357] accumulated_eval_time=109.130511, accumulated_logging_time=0.095093, accumulated_submission_time=7201.722977, global_step=9357, preemption_count=0, score=7201.722977, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7318.605180, train/ctc_loss=nan, train/wer=0.941573, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0609 19:01:50.939758 139828012242688 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0609 19:08:06.863712 139828020635392 logging_writer.py:48] [10000] global_step=10000, grad_norm=nan, loss=nan
I0609 19:14:24.081814 139828020635392 logging_writer.py:48] [10500] global_step=10500, grad_norm=nan, loss=nan
I0609 19:20:39.872010 139828012242688 logging_writer.py:48] [11000] global_step=11000, grad_norm=nan, loss=nan
I0609 19:26:57.103034 139828020635392 logging_writer.py:48] [11500] global_step=11500, grad_norm=nan, loss=nan
I0609 19:33:12.855853 139828012242688 logging_writer.py:48] [12000] global_step=12000, grad_norm=nan, loss=nan
I0609 19:39:30.328146 139828020635392 logging_writer.py:48] [12500] global_step=12500, grad_norm=nan, loss=nan
I0609 19:40:03.306245 139857213830976 spec.py:298] Evaluating on the training split.
I0609 19:40:13.300394 139857213830976 spec.py:310] Evaluating on the validation split.
I0609 19:40:22.860622 139857213830976 spec.py:326] Evaluating on the test split.
I0609 19:40:28.109833 139857213830976 submission_runner.py:419] Time since start: 9743.88s, 	Step: 12545, 	{'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9599.307266950607, 'total_duration': 9743.879188299179, 'accumulated_submission_time': 9599.307266950607, 'accumulated_eval_time': 133.93377780914307, 'accumulated_logging_time': 0.12560319900512695}
I0609 19:40:28.128596 139828020635392 logging_writer.py:48] [12545] accumulated_eval_time=133.933778, accumulated_logging_time=0.125603, accumulated_submission_time=9599.307267, global_step=12545, preemption_count=0, score=9599.307267, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=9743.879188, train/ctc_loss=nan, train/wer=0.941573, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0609 19:46:10.918059 139828012242688 logging_writer.py:48] [13000] global_step=13000, grad_norm=nan, loss=nan
I0609 19:52:28.354520 139828020635392 logging_writer.py:48] [13500] global_step=13500, grad_norm=nan, loss=nan
I0609 19:58:43.969199 139828012242688 logging_writer.py:48] [14000] global_step=14000, grad_norm=nan, loss=nan
I0609 20:05:01.378822 139828020635392 logging_writer.py:48] [14500] global_step=14500, grad_norm=nan, loss=nan
I0609 20:11:16.844736 139828012242688 logging_writer.py:48] [15000] global_step=15000, grad_norm=nan, loss=nan
I0609 20:17:34.395058 139828020635392 logging_writer.py:48] [15500] global_step=15500, grad_norm=nan, loss=nan
I0609 20:20:28.398952 139857213830976 spec.py:298] Evaluating on the training split.
I0609 20:20:38.253899 139857213830976 spec.py:310] Evaluating on the validation split.
I0609 20:20:47.742959 139857213830976 spec.py:326] Evaluating on the test split.
I0609 20:20:52.892375 139857213830976 submission_runner.py:419] Time since start: 12168.66s, 	Step: 15733, 	{'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11996.573089838028, 'total_duration': 12168.661793708801, 'accumulated_submission_time': 11996.573089838028, 'accumulated_eval_time': 158.4290418624878, 'accumulated_logging_time': 0.15509581565856934}
I0609 20:20:52.912797 139828020635392 logging_writer.py:48] [15733] accumulated_eval_time=158.429042, accumulated_logging_time=0.155096, accumulated_submission_time=11996.573090, global_step=15733, preemption_count=0, score=11996.573090, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=12168.661794, train/ctc_loss=nan, train/wer=0.941573, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0609 20:24:14.448873 139828012242688 logging_writer.py:48] [16000] global_step=16000, grad_norm=nan, loss=nan
I0609 20:30:32.401979 139828020635392 logging_writer.py:48] [16500] global_step=16500, grad_norm=nan, loss=nan
I0609 20:36:47.819981 139828012242688 logging_writer.py:48] [17000] global_step=17000, grad_norm=nan, loss=nan
I0609 20:43:04.118343 139828020635392 logging_writer.py:48] [17500] global_step=17500, grad_norm=nan, loss=nan
I0609 20:49:21.080645 139828020635392 logging_writer.py:48] [18000] global_step=18000, grad_norm=nan, loss=nan
I0609 20:55:37.250377 139828012242688 logging_writer.py:48] [18500] global_step=18500, grad_norm=nan, loss=nan
I0609 21:00:53.258393 139857213830976 spec.py:298] Evaluating on the training split.
I0609 21:01:03.201957 139857213830976 spec.py:310] Evaluating on the validation split.
I0609 21:01:12.708712 139857213830976 spec.py:326] Evaluating on the test split.
I0609 21:01:17.971704 139857213830976 submission_runner.py:419] Time since start: 14593.74s, 	Step: 18920, 	{'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14393.978370428085, 'total_duration': 14593.741079568863, 'accumulated_submission_time': 14393.978370428085, 'accumulated_eval_time': 183.14243340492249, 'accumulated_logging_time': 0.18502330780029297}
I0609 21:01:17.992074 139828020635392 logging_writer.py:48] [18920] accumulated_eval_time=183.142433, accumulated_logging_time=0.185023, accumulated_submission_time=14393.978370, global_step=18920, preemption_count=0, score=14393.978370, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=14593.741080, train/ctc_loss=nan, train/wer=0.941573, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0609 21:02:19.005362 139828012242688 logging_writer.py:48] [19000] global_step=19000, grad_norm=nan, loss=nan
I0609 21:08:35.237955 139828020635392 logging_writer.py:48] [19500] global_step=19500, grad_norm=nan, loss=nan
I0609 21:14:51.356134 139857213830976 spec.py:298] Evaluating on the training split.
I0609 21:15:01.074867 139857213830976 spec.py:310] Evaluating on the validation split.
I0609 21:15:10.540717 139857213830976 spec.py:326] Evaluating on the test split.
I0609 21:15:16.563891 139857213830976 submission_runner.py:419] Time since start: 15432.33s, 	Step: 20000, 	{'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15206.341142177582, 'total_duration': 15432.333207130432, 'accumulated_submission_time': 15206.341142177582, 'accumulated_eval_time': 208.35035109519958, 'accumulated_logging_time': 0.21596479415893555}
I0609 21:15:16.585396 139828020635392 logging_writer.py:48] [20000] accumulated_eval_time=208.350351, accumulated_logging_time=0.215965, accumulated_submission_time=15206.341142, global_step=20000, preemption_count=0, score=15206.341142, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=15432.333207, train/ctc_loss=nan, train/wer=0.941573, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0609 21:15:16.609736 139828012242688 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15206.341142
I0609 21:15:17.069797 139857213830976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/momentum/librispeech_conformer_pytorch/trial_1/checkpoint_20000.
I0609 21:15:17.178330 139857213830976 submission_runner.py:581] Tuning trial 1/1
I0609 21:15:17.178699 139857213830976 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0609 21:15:17.179378 139857213830976 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.407430119065868, 'train/wer': 2.2530743105556312, 'validation/ctc_loss': 30.186661769138034, 'validation/wer': 2.129319750881089, 'validation/num_examples': 5348, 'test/ctc_loss': 30.25707464997742, 'test/wer': 2.168850161477058, 'test/num_examples': 2472, 'score': 8.022817134857178, 'total_duration': 42.52596831321716, 'accumulated_submission_time': 8.022817134857178, 'accumulated_eval_time': 34.50271821022034, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3045, {'train/ctc_loss': 5.924086991329824, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': 5.957203919278682, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': 5.945292438221821, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2406.2797107696533, 'total_duration': 2468.2309124469757, 'accumulated_submission_time': 2406.2797107696533, 'accumulated_eval_time': 59.657020568847656, 'accumulated_logging_time': 0.03559732437133789, 'global_step': 3045, 'preemption_count': 0}), (6171, {'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4804.394938468933, 'total_duration': 4893.505056858063, 'accumulated_submission_time': 4804.394938468933, 'accumulated_eval_time': 84.18939018249512, 'accumulated_logging_time': 0.06410527229309082, 'global_step': 6171, 'preemption_count': 0}), (9357, {'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7201.72297668457, 'total_duration': 7318.605179548264, 'accumulated_submission_time': 7201.72297668457, 'accumulated_eval_time': 109.13051128387451, 'accumulated_logging_time': 0.0950925350189209, 'global_step': 9357, 'preemption_count': 0}), (12545, {'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9599.307266950607, 'total_duration': 9743.879188299179, 'accumulated_submission_time': 9599.307266950607, 'accumulated_eval_time': 133.93377780914307, 'accumulated_logging_time': 0.12560319900512695, 'global_step': 12545, 'preemption_count': 0}), (15733, {'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11996.573089838028, 'total_duration': 12168.661793708801, 'accumulated_submission_time': 11996.573089838028, 'accumulated_eval_time': 158.4290418624878, 'accumulated_logging_time': 0.15509581565856934, 'global_step': 15733, 'preemption_count': 0}), (18920, {'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14393.978370428085, 'total_duration': 14593.741079568863, 'accumulated_submission_time': 14393.978370428085, 'accumulated_eval_time': 183.14243340492249, 'accumulated_logging_time': 0.18502330780029297, 'global_step': 18920, 'preemption_count': 0}), (20000, {'train/ctc_loss': nan, 'train/wer': 0.9415731558212199, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15206.341142177582, 'total_duration': 15432.333207130432, 'accumulated_submission_time': 15206.341142177582, 'accumulated_eval_time': 208.35035109519958, 'accumulated_logging_time': 0.21596479415893555, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0609 21:15:17.179552 139857213830976 submission_runner.py:584] Timing: 15206.341142177582
I0609 21:15:17.179660 139857213830976 submission_runner.py:586] Total number of evals: 8
I0609 21:15:17.179786 139857213830976 submission_runner.py:587] ====================
I0609 21:15:17.180062 139857213830976 submission_runner.py:655] Final librispeech_conformer score: 15206.341142177582
