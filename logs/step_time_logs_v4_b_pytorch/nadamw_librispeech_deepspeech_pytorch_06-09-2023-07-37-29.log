torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_deepspeech --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/nadamw --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_pytorch_06-09-2023-07-37-29.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 07:37:52.750431 140701209761600 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 07:37:52.750460 140022227928896 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 07:37:52.750480 140449409529664 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 07:37:52.750704 140368621983552 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 07:37:52.751219 140323281590080 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 07:37:52.751435 139732594587456 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 07:37:52.752067 140682684479296 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 07:37:52.752162 140141309380416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 07:37:52.752392 140682684479296 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 07:37:52.752549 140141309380416 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 07:37:52.761142 140022227928896 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 07:37:52.761170 140701209761600 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 07:37:52.761198 140449409529664 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 07:37:52.761394 140368621983552 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 07:37:52.761829 140323281590080 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 07:37:52.762057 139732594587456 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 07:37:53.126007 140682684479296 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/nadamw/librispeech_deepspeech_pytorch.
W0609 07:37:53.371290 140368621983552 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 07:37:53.371728 140141309380416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 07:37:53.373263 140323281590080 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 07:37:53.374695 140701209761600 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 07:37:53.375295 139732594587456 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 07:37:53.379522 140682684479296 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 07:37:53.384560 140682684479296 submission_runner.py:541] Using RNG seed 2362789173
W0609 07:37:53.385119 140022227928896 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 07:37:53.386015 140682684479296 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 07:37:53.386138 140682684479296 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/nadamw/librispeech_deepspeech_pytorch/trial_1.
I0609 07:37:53.386405 140682684479296 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/nadamw/librispeech_deepspeech_pytorch/trial_1/hparams.json.
W0609 07:37:53.387205 140449409529664 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 07:37:53.387387 140682684479296 submission_runner.py:255] Initializing dataset.
I0609 07:37:53.387518 140682684479296 input_pipeline.py:20] Loading split = train-clean-100
I0609 07:37:53.422308 140682684479296 input_pipeline.py:20] Loading split = train-clean-360
I0609 07:37:53.766495 140682684479296 input_pipeline.py:20] Loading split = train-other-500
I0609 07:37:54.220697 140682684479296 submission_runner.py:262] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0609 07:38:02.365816 140682684479296 submission_runner.py:272] Initializing optimizer.
I0609 07:38:02.366941 140682684479296 submission_runner.py:279] Initializing metrics bundle.
I0609 07:38:02.367076 140682684479296 submission_runner.py:297] Initializing checkpoint and logger.
I0609 07:38:02.368083 140682684479296 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 07:38:02.368187 140682684479296 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 07:38:03.068371 140682684479296 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/nadamw/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I0609 07:38:03.069266 140682684479296 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/nadamw/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I0609 07:38:03.076425 140682684479296 submission_runner.py:332] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0609 07:38:12.593079 140655951083264 logging_writer.py:48] [0] global_step=0, grad_norm=20.632086, loss=33.480728
I0609 07:38:12.612461 140682684479296 submission.py:296] 0) loss = 33.481, grad_norm = 20.632
I0609 07:38:12.613974 140682684479296 spec.py:298] Evaluating on the training split.
I0609 07:38:12.615164 140682684479296 input_pipeline.py:20] Loading split = train-clean-100
I0609 07:38:12.650396 140682684479296 input_pipeline.py:20] Loading split = train-clean-360
I0609 07:38:13.091405 140682684479296 input_pipeline.py:20] Loading split = train-other-500
I0609 07:38:33.121176 140682684479296 spec.py:310] Evaluating on the validation split.
I0609 07:38:33.122465 140682684479296 input_pipeline.py:20] Loading split = dev-clean
I0609 07:38:33.126258 140682684479296 input_pipeline.py:20] Loading split = dev-other
I0609 07:38:46.179400 140682684479296 spec.py:326] Evaluating on the test split.
I0609 07:38:46.180847 140682684479296 input_pipeline.py:20] Loading split = test-clean
I0609 07:38:53.909332 140682684479296 submission_runner.py:419] Time since start: 50.83s, 	Step: 1, 	{'train/ctc_loss': 32.7659976193408, 'train/wer': 3.4854645119642877, 'validation/ctc_loss': 31.576049829214387, 'validation/wer': 3.1147781586443295, 'validation/num_examples': 5348, 'test/ctc_loss': 31.678717981805278, 'test/wer': 3.3555338898706153, 'test/num_examples': 2472, 'score': 9.537626504898071, 'total_duration': 50.83308291435242, 'accumulated_submission_time': 9.537626504898071, 'accumulated_eval_time': 41.2950279712677, 'accumulated_logging_time': 0}
I0609 07:38:53.930725 140651714393856 logging_writer.py:48] [1] accumulated_eval_time=41.295028, accumulated_logging_time=0, accumulated_submission_time=9.537627, global_step=1, preemption_count=0, score=9.537627, test/ctc_loss=31.678718, test/num_examples=2472, test/wer=3.355534, total_duration=50.833083, train/ctc_loss=32.765998, train/wer=3.485465, validation/ctc_loss=31.576050, validation/num_examples=5348, validation/wer=3.114778
I0609 07:38:53.971556 140682684479296 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 07:38:53.973027 140323281590080 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 07:38:53.973016 140368621983552 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 07:38:53.973073 139732594587456 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 07:38:53.973069 140141309380416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 07:38:53.973114 140449409529664 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 07:38:53.973227 140701209761600 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 07:38:53.973393 140022227928896 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 07:38:55.249729 140651706001152 logging_writer.py:48] [1] global_step=1, grad_norm=20.689976, loss=32.891472
I0609 07:38:55.253017 140682684479296 submission.py:296] 1) loss = 32.891, grad_norm = 20.690
I0609 07:38:56.347155 140651714393856 logging_writer.py:48] [2] global_step=2, grad_norm=21.204563, loss=33.354588
I0609 07:38:56.350436 140682684479296 submission.py:296] 2) loss = 33.355, grad_norm = 21.205
I0609 07:38:57.299249 140651706001152 logging_writer.py:48] [3] global_step=3, grad_norm=22.831745, loss=33.431347
I0609 07:38:57.302797 140682684479296 submission.py:296] 3) loss = 33.431, grad_norm = 22.832
I0609 07:38:58.246279 140651714393856 logging_writer.py:48] [4] global_step=4, grad_norm=22.241693, loss=32.929783
I0609 07:38:58.249578 140682684479296 submission.py:296] 4) loss = 32.930, grad_norm = 22.242
I0609 07:38:59.201318 140651706001152 logging_writer.py:48] [5] global_step=5, grad_norm=23.015285, loss=33.204659
I0609 07:38:59.204658 140682684479296 submission.py:296] 5) loss = 33.205, grad_norm = 23.015
I0609 07:39:00.162509 140651714393856 logging_writer.py:48] [6] global_step=6, grad_norm=25.602417, loss=33.349678
I0609 07:39:00.166125 140682684479296 submission.py:296] 6) loss = 33.350, grad_norm = 25.602
I0609 07:39:01.105755 140651706001152 logging_writer.py:48] [7] global_step=7, grad_norm=24.098898, loss=32.268322
I0609 07:39:01.108996 140682684479296 submission.py:296] 7) loss = 32.268, grad_norm = 24.099
I0609 07:39:02.059945 140651714393856 logging_writer.py:48] [8] global_step=8, grad_norm=26.723526, loss=32.737858
I0609 07:39:02.063415 140682684479296 submission.py:296] 8) loss = 32.738, grad_norm = 26.724
I0609 07:39:03.001170 140651706001152 logging_writer.py:48] [9] global_step=9, grad_norm=28.049059, loss=32.354401
I0609 07:39:03.004776 140682684479296 submission.py:296] 9) loss = 32.354, grad_norm = 28.049
I0609 07:39:03.958173 140651714393856 logging_writer.py:48] [10] global_step=10, grad_norm=30.661835, loss=32.198902
I0609 07:39:03.961849 140682684479296 submission.py:296] 10) loss = 32.199, grad_norm = 30.662
I0609 07:39:04.909180 140651706001152 logging_writer.py:48] [11] global_step=11, grad_norm=31.673201, loss=32.349442
I0609 07:39:04.912875 140682684479296 submission.py:296] 11) loss = 32.349, grad_norm = 31.673
I0609 07:39:05.873217 140651714393856 logging_writer.py:48] [12] global_step=12, grad_norm=32.272491, loss=32.479740
I0609 07:39:05.876939 140682684479296 submission.py:296] 12) loss = 32.480, grad_norm = 32.272
I0609 07:39:06.844066 140651706001152 logging_writer.py:48] [13] global_step=13, grad_norm=34.404884, loss=32.024067
I0609 07:39:06.847675 140682684479296 submission.py:296] 13) loss = 32.024, grad_norm = 34.405
I0609 07:39:07.797478 140651714393856 logging_writer.py:48] [14] global_step=14, grad_norm=34.268650, loss=31.849829
I0609 07:39:07.801669 140682684479296 submission.py:296] 14) loss = 31.850, grad_norm = 34.269
I0609 07:39:08.739163 140651706001152 logging_writer.py:48] [15] global_step=15, grad_norm=34.331860, loss=30.875561
I0609 07:39:08.742800 140682684479296 submission.py:296] 15) loss = 30.876, grad_norm = 34.332
I0609 07:39:09.698839 140651714393856 logging_writer.py:48] [16] global_step=16, grad_norm=35.982010, loss=31.360428
I0609 07:39:09.702406 140682684479296 submission.py:296] 16) loss = 31.360, grad_norm = 35.982
I0609 07:39:10.650835 140651706001152 logging_writer.py:48] [17] global_step=17, grad_norm=35.978634, loss=30.950508
I0609 07:39:10.654353 140682684479296 submission.py:296] 17) loss = 30.951, grad_norm = 35.979
I0609 07:39:11.614975 140651714393856 logging_writer.py:48] [18] global_step=18, grad_norm=37.931328, loss=30.692095
I0609 07:39:11.618877 140682684479296 submission.py:296] 18) loss = 30.692, grad_norm = 37.931
I0609 07:39:12.570585 140651706001152 logging_writer.py:48] [19] global_step=19, grad_norm=37.685162, loss=29.965103
I0609 07:39:12.573926 140682684479296 submission.py:296] 19) loss = 29.965, grad_norm = 37.685
I0609 07:39:13.519701 140651714393856 logging_writer.py:48] [20] global_step=20, grad_norm=38.208263, loss=29.319323
I0609 07:39:13.523128 140682684479296 submission.py:296] 20) loss = 29.319, grad_norm = 38.208
I0609 07:39:14.471305 140651706001152 logging_writer.py:48] [21] global_step=21, grad_norm=38.350399, loss=29.325951
I0609 07:39:14.474594 140682684479296 submission.py:296] 21) loss = 29.326, grad_norm = 38.350
I0609 07:39:15.427196 140651714393856 logging_writer.py:48] [22] global_step=22, grad_norm=39.418610, loss=29.426493
I0609 07:39:15.430855 140682684479296 submission.py:296] 22) loss = 29.426, grad_norm = 39.419
I0609 07:39:16.405254 140651706001152 logging_writer.py:48] [23] global_step=23, grad_norm=38.247528, loss=28.402699
I0609 07:39:16.408967 140682684479296 submission.py:296] 23) loss = 28.403, grad_norm = 38.248
I0609 07:39:17.377829 140651714393856 logging_writer.py:48] [24] global_step=24, grad_norm=39.653839, loss=28.457684
I0609 07:39:17.381179 140682684479296 submission.py:296] 24) loss = 28.458, grad_norm = 39.654
I0609 07:39:18.334616 140651706001152 logging_writer.py:48] [25] global_step=25, grad_norm=39.218586, loss=28.105982
I0609 07:39:18.338204 140682684479296 submission.py:296] 25) loss = 28.106, grad_norm = 39.219
I0609 07:39:19.295616 140651714393856 logging_writer.py:48] [26] global_step=26, grad_norm=40.008724, loss=27.675522
I0609 07:39:19.299403 140682684479296 submission.py:296] 26) loss = 27.676, grad_norm = 40.009
I0609 07:39:20.263341 140651706001152 logging_writer.py:48] [27] global_step=27, grad_norm=40.815456, loss=27.396086
I0609 07:39:20.267006 140682684479296 submission.py:296] 27) loss = 27.396, grad_norm = 40.815
I0609 07:39:21.219675 140651714393856 logging_writer.py:48] [28] global_step=28, grad_norm=38.844444, loss=26.633381
I0609 07:39:21.223330 140682684479296 submission.py:296] 28) loss = 26.633, grad_norm = 38.844
I0609 07:39:22.172350 140651706001152 logging_writer.py:48] [29] global_step=29, grad_norm=38.757118, loss=26.048374
I0609 07:39:22.175845 140682684479296 submission.py:296] 29) loss = 26.048, grad_norm = 38.757
I0609 07:39:23.128657 140651714393856 logging_writer.py:48] [30] global_step=30, grad_norm=39.576473, loss=25.440807
I0609 07:39:23.132260 140682684479296 submission.py:296] 30) loss = 25.441, grad_norm = 39.576
I0609 07:39:24.080027 140651706001152 logging_writer.py:48] [31] global_step=31, grad_norm=38.411327, loss=25.333059
I0609 07:39:24.083904 140682684479296 submission.py:296] 31) loss = 25.333, grad_norm = 38.411
I0609 07:39:25.033739 140651714393856 logging_writer.py:48] [32] global_step=32, grad_norm=37.681862, loss=24.916374
I0609 07:39:25.037395 140682684479296 submission.py:296] 32) loss = 24.916, grad_norm = 37.682
I0609 07:39:25.986176 140651706001152 logging_writer.py:48] [33] global_step=33, grad_norm=38.046844, loss=24.588545
I0609 07:39:25.989422 140682684479296 submission.py:296] 33) loss = 24.589, grad_norm = 38.047
I0609 07:39:26.935650 140651714393856 logging_writer.py:48] [34] global_step=34, grad_norm=36.867588, loss=23.985491
I0609 07:39:26.939171 140682684479296 submission.py:296] 34) loss = 23.985, grad_norm = 36.868
I0609 07:39:27.888141 140651706001152 logging_writer.py:48] [35] global_step=35, grad_norm=36.701561, loss=23.501602
I0609 07:39:27.891550 140682684479296 submission.py:296] 35) loss = 23.502, grad_norm = 36.702
I0609 07:39:28.842195 140651714393856 logging_writer.py:48] [36] global_step=36, grad_norm=35.137356, loss=23.051380
I0609 07:39:28.845491 140682684479296 submission.py:296] 36) loss = 23.051, grad_norm = 35.137
I0609 07:39:29.799198 140651706001152 logging_writer.py:48] [37] global_step=37, grad_norm=34.724335, loss=22.315712
I0609 07:39:29.802405 140682684479296 submission.py:296] 37) loss = 22.316, grad_norm = 34.724
I0609 07:39:30.756823 140651714393856 logging_writer.py:48] [38] global_step=38, grad_norm=34.214207, loss=21.980728
I0609 07:39:30.760215 140682684479296 submission.py:296] 38) loss = 21.981, grad_norm = 34.214
I0609 07:39:31.705719 140651706001152 logging_writer.py:48] [39] global_step=39, grad_norm=34.101814, loss=21.835222
I0609 07:39:31.709166 140682684479296 submission.py:296] 39) loss = 21.835, grad_norm = 34.102
I0609 07:39:32.653753 140651714393856 logging_writer.py:48] [40] global_step=40, grad_norm=32.560318, loss=21.306217
I0609 07:39:32.657106 140682684479296 submission.py:296] 40) loss = 21.306, grad_norm = 32.560
I0609 07:39:33.605204 140651706001152 logging_writer.py:48] [41] global_step=41, grad_norm=31.983852, loss=20.569920
I0609 07:39:33.608951 140682684479296 submission.py:296] 41) loss = 20.570, grad_norm = 31.984
I0609 07:39:34.557205 140651714393856 logging_writer.py:48] [42] global_step=42, grad_norm=31.163198, loss=19.913620
I0609 07:39:34.560819 140682684479296 submission.py:296] 42) loss = 19.914, grad_norm = 31.163
I0609 07:39:35.508495 140651706001152 logging_writer.py:48] [43] global_step=43, grad_norm=30.026606, loss=19.632853
I0609 07:39:35.512069 140682684479296 submission.py:296] 43) loss = 19.633, grad_norm = 30.027
I0609 07:39:36.460398 140651714393856 logging_writer.py:48] [44] global_step=44, grad_norm=30.015703, loss=19.075319
I0609 07:39:36.463773 140682684479296 submission.py:296] 44) loss = 19.075, grad_norm = 30.016
I0609 07:39:37.409105 140651706001152 logging_writer.py:48] [45] global_step=45, grad_norm=28.264818, loss=18.839754
I0609 07:39:37.413223 140682684479296 submission.py:296] 45) loss = 18.840, grad_norm = 28.265
I0609 07:39:38.369067 140651714393856 logging_writer.py:48] [46] global_step=46, grad_norm=28.099712, loss=17.964472
I0609 07:39:38.372560 140682684479296 submission.py:296] 46) loss = 17.964, grad_norm = 28.100
I0609 07:39:39.315699 140651706001152 logging_writer.py:48] [47] global_step=47, grad_norm=26.748363, loss=17.587029
I0609 07:39:39.319034 140682684479296 submission.py:296] 47) loss = 17.587, grad_norm = 26.748
I0609 07:39:40.293374 140651714393856 logging_writer.py:48] [48] global_step=48, grad_norm=27.226681, loss=17.886141
I0609 07:39:40.296779 140682684479296 submission.py:296] 48) loss = 17.886, grad_norm = 27.227
I0609 07:39:41.242575 140651706001152 logging_writer.py:48] [49] global_step=49, grad_norm=26.143476, loss=17.189064
I0609 07:39:41.245942 140682684479296 submission.py:296] 49) loss = 17.189, grad_norm = 26.143
I0609 07:39:42.188207 140651714393856 logging_writer.py:48] [50] global_step=50, grad_norm=24.647635, loss=16.792847
I0609 07:39:42.191910 140682684479296 submission.py:296] 50) loss = 16.793, grad_norm = 24.648
I0609 07:39:43.150899 140651706001152 logging_writer.py:48] [51] global_step=51, grad_norm=23.251221, loss=16.178709
I0609 07:39:43.154085 140682684479296 submission.py:296] 51) loss = 16.179, grad_norm = 23.251
I0609 07:39:44.101205 140651714393856 logging_writer.py:48] [52] global_step=52, grad_norm=22.609306, loss=15.445648
I0609 07:39:44.104439 140682684479296 submission.py:296] 52) loss = 15.446, grad_norm = 22.609
I0609 07:39:45.046673 140651706001152 logging_writer.py:48] [53] global_step=53, grad_norm=22.095280, loss=15.536580
I0609 07:39:45.049985 140682684479296 submission.py:296] 53) loss = 15.537, grad_norm = 22.095
I0609 07:39:46.008666 140651714393856 logging_writer.py:48] [54] global_step=54, grad_norm=22.892241, loss=15.459575
I0609 07:39:46.012092 140682684479296 submission.py:296] 54) loss = 15.460, grad_norm = 22.892
I0609 07:39:46.950950 140651706001152 logging_writer.py:48] [55] global_step=55, grad_norm=21.963140, loss=15.028907
I0609 07:39:46.954439 140682684479296 submission.py:296] 55) loss = 15.029, grad_norm = 21.963
I0609 07:39:47.893742 140651714393856 logging_writer.py:48] [56] global_step=56, grad_norm=21.142149, loss=14.393023
I0609 07:39:47.897292 140682684479296 submission.py:296] 56) loss = 14.393, grad_norm = 21.142
I0609 07:39:48.839131 140651706001152 logging_writer.py:48] [57] global_step=57, grad_norm=20.179558, loss=14.370807
I0609 07:39:48.842801 140682684479296 submission.py:296] 57) loss = 14.371, grad_norm = 20.180
I0609 07:39:49.784838 140651714393856 logging_writer.py:48] [58] global_step=58, grad_norm=19.562637, loss=13.583606
I0609 07:39:49.788482 140682684479296 submission.py:296] 58) loss = 13.584, grad_norm = 19.563
I0609 07:39:50.729125 140651706001152 logging_writer.py:48] [59] global_step=59, grad_norm=18.302916, loss=13.678612
I0609 07:39:50.733187 140682684479296 submission.py:296] 59) loss = 13.679, grad_norm = 18.303
I0609 07:39:51.670481 140651714393856 logging_writer.py:48] [60] global_step=60, grad_norm=18.210514, loss=13.460426
I0609 07:39:51.673762 140682684479296 submission.py:296] 60) loss = 13.460, grad_norm = 18.211
I0609 07:39:52.618695 140651706001152 logging_writer.py:48] [61] global_step=61, grad_norm=16.879349, loss=12.819641
I0609 07:39:52.622197 140682684479296 submission.py:296] 61) loss = 12.820, grad_norm = 16.879
I0609 07:39:53.566258 140651714393856 logging_writer.py:48] [62] global_step=62, grad_norm=17.218298, loss=12.664696
I0609 07:39:53.569657 140682684479296 submission.py:296] 62) loss = 12.665, grad_norm = 17.218
I0609 07:39:54.510285 140651706001152 logging_writer.py:48] [63] global_step=63, grad_norm=15.446688, loss=12.652970
I0609 07:39:54.513925 140682684479296 submission.py:296] 63) loss = 12.653, grad_norm = 15.447
I0609 07:39:55.461916 140651714393856 logging_writer.py:48] [64] global_step=64, grad_norm=14.388573, loss=12.270021
I0609 07:39:55.465250 140682684479296 submission.py:296] 64) loss = 12.270, grad_norm = 14.389
I0609 07:39:56.409839 140651706001152 logging_writer.py:48] [65] global_step=65, grad_norm=14.016338, loss=11.753489
I0609 07:39:56.413368 140682684479296 submission.py:296] 65) loss = 11.753, grad_norm = 14.016
I0609 07:39:57.355642 140651714393856 logging_writer.py:48] [66] global_step=66, grad_norm=13.146471, loss=11.485993
I0609 07:39:57.358932 140682684479296 submission.py:296] 66) loss = 11.486, grad_norm = 13.146
I0609 07:39:58.300817 140651706001152 logging_writer.py:48] [67] global_step=67, grad_norm=11.477744, loss=11.252713
I0609 07:39:58.304293 140682684479296 submission.py:296] 67) loss = 11.253, grad_norm = 11.478
I0609 07:39:59.250327 140651714393856 logging_writer.py:48] [68] global_step=68, grad_norm=11.398113, loss=11.046841
I0609 07:39:59.253547 140682684479296 submission.py:296] 68) loss = 11.047, grad_norm = 11.398
I0609 07:40:00.196103 140651706001152 logging_writer.py:48] [69] global_step=69, grad_norm=11.617400, loss=11.605989
I0609 07:40:00.199390 140682684479296 submission.py:296] 69) loss = 11.606, grad_norm = 11.617
I0609 07:40:01.138403 140651714393856 logging_writer.py:48] [70] global_step=70, grad_norm=10.689054, loss=10.802067
I0609 07:40:01.142065 140682684479296 submission.py:296] 70) loss = 10.802, grad_norm = 10.689
I0609 07:40:02.117044 140651706001152 logging_writer.py:48] [71] global_step=71, grad_norm=10.505600, loss=10.945722
I0609 07:40:02.120500 140682684479296 submission.py:296] 71) loss = 10.946, grad_norm = 10.506
I0609 07:40:03.070897 140651714393856 logging_writer.py:48] [72] global_step=72, grad_norm=9.775331, loss=10.631035
I0609 07:40:03.074101 140682684479296 submission.py:296] 72) loss = 10.631, grad_norm = 9.775
I0609 07:40:04.017206 140651706001152 logging_writer.py:48] [73] global_step=73, grad_norm=9.198727, loss=10.490666
I0609 07:40:04.020916 140682684479296 submission.py:296] 73) loss = 10.491, grad_norm = 9.199
I0609 07:40:04.968251 140651714393856 logging_writer.py:48] [74] global_step=74, grad_norm=9.242260, loss=10.434641
I0609 07:40:04.971677 140682684479296 submission.py:296] 74) loss = 10.435, grad_norm = 9.242
I0609 07:40:05.917063 140651706001152 logging_writer.py:48] [75] global_step=75, grad_norm=8.283636, loss=10.079873
I0609 07:40:05.920439 140682684479296 submission.py:296] 75) loss = 10.080, grad_norm = 8.284
I0609 07:40:06.885890 140651714393856 logging_writer.py:48] [76] global_step=76, grad_norm=7.170891, loss=9.873416
I0609 07:40:06.889941 140682684479296 submission.py:296] 76) loss = 9.873, grad_norm = 7.171
I0609 07:40:07.834712 140651706001152 logging_writer.py:48] [77] global_step=77, grad_norm=7.109545, loss=9.863050
I0609 07:40:07.838593 140682684479296 submission.py:296] 77) loss = 9.863, grad_norm = 7.110
I0609 07:40:08.782734 140651714393856 logging_writer.py:48] [78] global_step=78, grad_norm=7.446301, loss=9.890152
I0609 07:40:08.786569 140682684479296 submission.py:296] 78) loss = 9.890, grad_norm = 7.446
I0609 07:40:09.729079 140651706001152 logging_writer.py:48] [79] global_step=79, grad_norm=6.678512, loss=9.754662
I0609 07:40:09.732758 140682684479296 submission.py:296] 79) loss = 9.755, grad_norm = 6.679
I0609 07:40:10.676380 140651714393856 logging_writer.py:48] [80] global_step=80, grad_norm=6.879683, loss=9.774524
I0609 07:40:10.680543 140682684479296 submission.py:296] 80) loss = 9.775, grad_norm = 6.880
I0609 07:40:11.623675 140651706001152 logging_writer.py:48] [81] global_step=81, grad_norm=6.202285, loss=9.603070
I0609 07:40:11.627809 140682684479296 submission.py:296] 81) loss = 9.603, grad_norm = 6.202
I0609 07:40:12.574129 140651714393856 logging_writer.py:48] [82] global_step=82, grad_norm=6.463530, loss=9.681921
I0609 07:40:12.577663 140682684479296 submission.py:296] 82) loss = 9.682, grad_norm = 6.464
I0609 07:40:13.529031 140651706001152 logging_writer.py:48] [83] global_step=83, grad_norm=7.097739, loss=9.628377
I0609 07:40:13.533194 140682684479296 submission.py:296] 83) loss = 9.628, grad_norm = 7.098
I0609 07:40:14.474212 140651714393856 logging_writer.py:48] [84] global_step=84, grad_norm=5.861813, loss=9.370123
I0609 07:40:14.477811 140682684479296 submission.py:296] 84) loss = 9.370, grad_norm = 5.862
I0609 07:40:15.420079 140651706001152 logging_writer.py:48] [85] global_step=85, grad_norm=5.544822, loss=9.267260
I0609 07:40:15.423745 140682684479296 submission.py:296] 85) loss = 9.267, grad_norm = 5.545
I0609 07:40:16.369940 140651714393856 logging_writer.py:48] [86] global_step=86, grad_norm=5.837928, loss=9.040585
I0609 07:40:16.373327 140682684479296 submission.py:296] 86) loss = 9.041, grad_norm = 5.838
I0609 07:40:17.309708 140651706001152 logging_writer.py:48] [87] global_step=87, grad_norm=5.549317, loss=9.155594
I0609 07:40:17.313764 140682684479296 submission.py:296] 87) loss = 9.156, grad_norm = 5.549
I0609 07:40:18.254414 140651714393856 logging_writer.py:48] [88] global_step=88, grad_norm=5.817428, loss=9.223139
I0609 07:40:18.258059 140682684479296 submission.py:296] 88) loss = 9.223, grad_norm = 5.817
I0609 07:40:19.196474 140651706001152 logging_writer.py:48] [89] global_step=89, grad_norm=6.036403, loss=9.247155
I0609 07:40:19.199844 140682684479296 submission.py:296] 89) loss = 9.247, grad_norm = 6.036
I0609 07:40:20.143877 140651714393856 logging_writer.py:48] [90] global_step=90, grad_norm=5.398397, loss=8.969066
I0609 07:40:20.147552 140682684479296 submission.py:296] 90) loss = 8.969, grad_norm = 5.398
I0609 07:40:21.086205 140651706001152 logging_writer.py:48] [91] global_step=91, grad_norm=5.626774, loss=9.080100
I0609 07:40:21.089644 140682684479296 submission.py:296] 91) loss = 9.080, grad_norm = 5.627
I0609 07:40:22.039851 140651714393856 logging_writer.py:48] [92] global_step=92, grad_norm=4.598824, loss=8.899964
I0609 07:40:22.043701 140682684479296 submission.py:296] 92) loss = 8.900, grad_norm = 4.599
I0609 07:40:23.006593 140651706001152 logging_writer.py:48] [93] global_step=93, grad_norm=5.879654, loss=8.830878
I0609 07:40:23.010312 140682684479296 submission.py:296] 93) loss = 8.831, grad_norm = 5.880
I0609 07:40:23.955497 140651714393856 logging_writer.py:48] [94] global_step=94, grad_norm=4.719285, loss=8.603683
I0609 07:40:23.959020 140682684479296 submission.py:296] 94) loss = 8.604, grad_norm = 4.719
I0609 07:40:24.904531 140651706001152 logging_writer.py:48] [95] global_step=95, grad_norm=4.764620, loss=8.621137
I0609 07:40:24.907987 140682684479296 submission.py:296] 95) loss = 8.621, grad_norm = 4.765
I0609 07:40:25.849915 140651714393856 logging_writer.py:48] [96] global_step=96, grad_norm=3.926889, loss=8.505895
I0609 07:40:25.853337 140682684479296 submission.py:296] 96) loss = 8.506, grad_norm = 3.927
I0609 07:40:26.816315 140651706001152 logging_writer.py:48] [97] global_step=97, grad_norm=4.533717, loss=8.546289
I0609 07:40:26.819850 140682684479296 submission.py:296] 97) loss = 8.546, grad_norm = 4.534
I0609 07:40:27.772461 140651714393856 logging_writer.py:48] [98] global_step=98, grad_norm=4.337645, loss=8.455572
I0609 07:40:27.775945 140682684479296 submission.py:296] 98) loss = 8.456, grad_norm = 4.338
I0609 07:40:28.712677 140651706001152 logging_writer.py:48] [99] global_step=99, grad_norm=3.726917, loss=8.554874
I0609 07:40:28.716129 140682684479296 submission.py:296] 99) loss = 8.555, grad_norm = 3.727
I0609 07:40:29.659710 140651714393856 logging_writer.py:48] [100] global_step=100, grad_norm=4.014389, loss=8.247746
I0609 07:40:29.663014 140682684479296 submission.py:296] 100) loss = 8.248, grad_norm = 4.014
I0609 07:46:47.525360 140651706001152 logging_writer.py:48] [500] global_step=500, grad_norm=0.571393, loss=5.802408
I0609 07:46:47.529522 140682684479296 submission.py:296] 500) loss = 5.802, grad_norm = 0.571
I0609 07:54:38.952072 140651714393856 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.909957, loss=5.011715
I0609 07:54:38.957063 140682684479296 submission.py:296] 1000) loss = 5.012, grad_norm = 0.910
I0609 08:02:31.471764 140651714393856 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.743307, loss=3.516283
I0609 08:02:31.483358 140682684479296 submission.py:296] 1500) loss = 3.516, grad_norm = 2.743
I0609 08:10:21.826605 140651706001152 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.444089, loss=2.919553
I0609 08:10:21.835891 140682684479296 submission.py:296] 2000) loss = 2.920, grad_norm = 3.444
I0609 08:18:14.264019 140651714393856 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.197202, loss=2.596453
I0609 08:18:14.271723 140682684479296 submission.py:296] 2500) loss = 2.596, grad_norm = 4.197
I0609 08:18:54.607710 140682684479296 spec.py:298] Evaluating on the training split.
I0609 08:19:05.261499 140682684479296 spec.py:310] Evaluating on the validation split.
I0609 08:19:14.738665 140682684479296 spec.py:326] Evaluating on the test split.
I0609 08:19:19.888149 140682684479296 submission_runner.py:419] Time since start: 2476.81s, 	Step: 2544, 	{'train/ctc_loss': 5.547053079122714, 'train/wer': 0.9381106600873114, 'validation/ctc_loss': 5.4596491517229255, 'validation/wer': 0.8951672862453531, 'validation/num_examples': 5348, 'test/ctc_loss': 5.33285211949158, 'test/wer': 0.8967359291532102, 'test/num_examples': 2472, 'score': 2409.2342808246613, 'total_duration': 2476.811873435974, 'accumulated_submission_time': 2409.2342808246613, 'accumulated_eval_time': 66.57513499259949, 'accumulated_logging_time': 0.02994060516357422}
I0609 08:19:19.906571 140651714393856 logging_writer.py:48] [2544] accumulated_eval_time=66.575135, accumulated_logging_time=0.029941, accumulated_submission_time=2409.234281, global_step=2544, preemption_count=0, score=2409.234281, test/ctc_loss=5.332852, test/num_examples=2472, test/wer=0.896736, total_duration=2476.811873, train/ctc_loss=5.547053, train/wer=0.938111, validation/ctc_loss=5.459649, validation/num_examples=5348, validation/wer=0.895167
I0609 08:26:31.400973 140651706001152 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.841622, loss=2.479320
I0609 08:26:31.405971 140682684479296 submission.py:296] 3000) loss = 2.479, grad_norm = 3.842
I0609 08:34:22.714662 140651714393856 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.929469, loss=2.275072
I0609 08:34:22.722062 140682684479296 submission.py:296] 3500) loss = 2.275, grad_norm = 4.929
I0609 08:42:15.463756 140651706001152 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.004349, loss=2.153526
I0609 08:42:15.471344 140682684479296 submission.py:296] 4000) loss = 2.154, grad_norm = 3.004
I0609 08:50:05.805902 140651714393856 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.487216, loss=2.002482
I0609 08:50:05.813526 140682684479296 submission.py:296] 4500) loss = 2.002, grad_norm = 2.487
I0609 08:57:55.583924 140651706001152 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.740619, loss=1.964311
I0609 08:57:55.589618 140682684479296 submission.py:296] 5000) loss = 1.964, grad_norm = 2.741
I0609 08:59:20.832366 140682684479296 spec.py:298] Evaluating on the training split.
I0609 08:59:33.433148 140682684479296 spec.py:310] Evaluating on the validation split.
I0609 08:59:43.654405 140682684479296 spec.py:326] Evaluating on the test split.
I0609 08:59:49.289840 140682684479296 submission_runner.py:419] Time since start: 4906.21s, 	Step: 5091, 	{'train/ctc_loss': 0.8548650874276567, 'train/wer': 0.2739750757683513, 'validation/ctc_loss': 1.1004640879294756, 'validation/wer': 0.31041374981895425, 'validation/num_examples': 5348, 'test/ctc_loss': 0.7498393041486547, 'test/wer': 0.24463266508236345, 'test/num_examples': 2472, 'score': 4809.135463476181, 'total_duration': 4906.213579654694, 'accumulated_submission_time': 4809.135463476181, 'accumulated_eval_time': 95.03232002258301, 'accumulated_logging_time': 0.05747532844543457}
I0609 08:59:49.309950 140651714393856 logging_writer.py:48] [5091] accumulated_eval_time=95.032320, accumulated_logging_time=0.057475, accumulated_submission_time=4809.135463, global_step=5091, preemption_count=0, score=4809.135463, test/ctc_loss=0.749839, test/num_examples=2472, test/wer=0.244633, total_duration=4906.213580, train/ctc_loss=0.854865, train/wer=0.273975, validation/ctc_loss=1.100464, validation/num_examples=5348, validation/wer=0.310414
I0609 09:06:15.219948 140651714393856 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.089253, loss=1.963496
I0609 09:06:15.226607 140682684479296 submission.py:296] 5500) loss = 1.963, grad_norm = 3.089
I0609 09:14:07.544149 140651706001152 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.039911, loss=1.898003
I0609 09:14:07.548930 140682684479296 submission.py:296] 6000) loss = 1.898, grad_norm = 2.040
I0609 09:22:01.522682 140651714393856 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.088729, loss=1.824690
I0609 09:22:01.532199 140682684479296 submission.py:296] 6500) loss = 1.825, grad_norm = 3.089
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0609 09:29:51.714973 140651706001152 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.208143, loss=1.800890
I0609 09:29:51.720042 140682684479296 submission.py:296] 7000) loss = 1.801, grad_norm = 3.208
I0609 09:37:42.091115 140651714393856 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.337148, loss=1.771973
I0609 09:37:42.099686 140682684479296 submission.py:296] 7500) loss = 1.772, grad_norm = 2.337
I0609 09:39:49.396233 140682684479296 spec.py:298] Evaluating on the training split.
I0609 09:40:02.114927 140682684479296 spec.py:310] Evaluating on the validation split.
I0609 09:40:12.631587 140682684479296 spec.py:326] Evaluating on the test split.
I0609 09:40:18.125233 140682684479296 submission_runner.py:419] Time since start: 7335.05s, 	Step: 7637, 	{'train/ctc_loss': 0.6750752934778147, 'train/wer': 0.21990874974014465, 'validation/ctc_loss': 0.9383625615330521, 'validation/wer': 0.2645874571525129, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5820346776404929, 'test/wer': 0.19036012430686736, 'test/num_examples': 2472, 'score': 7208.18540763855, 'total_duration': 7335.048976421356, 'accumulated_submission_time': 7208.18540763855, 'accumulated_eval_time': 123.76101541519165, 'accumulated_logging_time': 0.08675122261047363}
I0609 09:40:18.144619 140651714393856 logging_writer.py:48] [7637] accumulated_eval_time=123.761015, accumulated_logging_time=0.086751, accumulated_submission_time=7208.185408, global_step=7637, preemption_count=0, score=7208.185408, test/ctc_loss=0.582035, test/num_examples=2472, test/wer=0.190360, total_duration=7335.048976, train/ctc_loss=0.675075, train/wer=0.219909, validation/ctc_loss=0.938363, validation/num_examples=5348, validation/wer=0.264587
I0609 09:45:59.476557 140651706001152 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.164311, loss=1.757753
I0609 09:45:59.480816 140682684479296 submission.py:296] 8000) loss = 1.758, grad_norm = 3.164
I0609 09:53:50.196193 140651714393856 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.915992, loss=1.729196
I0609 09:53:50.204310 140682684479296 submission.py:296] 8500) loss = 1.729, grad_norm = 2.916
I0609 10:01:40.566644 140651706001152 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.801541, loss=1.708503
I0609 10:01:40.571475 140682684479296 submission.py:296] 9000) loss = 1.709, grad_norm = 2.802
I0609 10:09:33.699781 140651714393856 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.689877, loss=1.695531
I0609 10:09:33.707698 140682684479296 submission.py:296] 9500) loss = 1.696, grad_norm = 2.690
I0609 10:17:22.612260 140651706001152 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.678721, loss=1.663735
I0609 10:17:22.617581 140682684479296 submission.py:296] 10000) loss = 1.664, grad_norm = 2.679
I0609 10:20:18.834106 140682684479296 spec.py:298] Evaluating on the training split.
I0609 10:20:31.581075 140682684479296 spec.py:310] Evaluating on the validation split.
I0609 10:20:42.094780 140682684479296 spec.py:326] Evaluating on the test split.
I0609 10:20:47.791001 140682684479296 submission_runner.py:419] Time since start: 9764.71s, 	Step: 10189, 	{'train/ctc_loss': 0.5359222253861731, 'train/wer': 0.17664693589505126, 'validation/ctc_loss': 0.7807185514303295, 'validation/wer': 0.22107854970308502, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4727848369249629, 'test/wer': 0.1541648894034489, 'test/num_examples': 2472, 'score': 9607.826256275177, 'total_duration': 9764.714654445648, 'accumulated_submission_time': 9607.826256275177, 'accumulated_eval_time': 152.7175588607788, 'accumulated_logging_time': 0.11608052253723145}
I0609 10:20:47.810806 140651714393856 logging_writer.py:48] [10189] accumulated_eval_time=152.717559, accumulated_logging_time=0.116081, accumulated_submission_time=9607.826256, global_step=10189, preemption_count=0, score=9607.826256, test/ctc_loss=0.472785, test/num_examples=2472, test/wer=0.154165, total_duration=9764.714654, train/ctc_loss=0.535922, train/wer=0.176647, validation/ctc_loss=0.780719, validation/num_examples=5348, validation/wer=0.221079
I0609 10:25:43.340016 140651714393856 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.132144, loss=1.654316
I0609 10:25:43.347014 140682684479296 submission.py:296] 10500) loss = 1.654, grad_norm = 3.132
I0609 10:33:31.795935 140651706001152 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.800218, loss=1.642463
I0609 10:33:31.802578 140682684479296 submission.py:296] 11000) loss = 1.642, grad_norm = 2.800
I0609 10:41:22.922353 140651714393856 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.934091, loss=1.611577
I0609 10:41:22.930607 140682684479296 submission.py:296] 11500) loss = 1.612, grad_norm = 3.934
I0609 10:49:11.210580 140651706001152 logging_writer.py:48] [12000] global_step=12000, grad_norm=5.273068, loss=1.588752
I0609 10:49:11.217943 140682684479296 submission.py:296] 12000) loss = 1.589, grad_norm = 5.273
I0609 10:57:03.140577 140651714393856 logging_writer.py:48] [12500] global_step=12500, grad_norm=4.359932, loss=1.599463
I0609 10:57:03.147006 140682684479296 submission.py:296] 12500) loss = 1.599, grad_norm = 4.360
I0609 11:00:47.876418 140682684479296 spec.py:298] Evaluating on the training split.
I0609 11:01:00.482116 140682684479296 spec.py:310] Evaluating on the validation split.
I0609 11:01:11.538794 140682684479296 spec.py:326] Evaluating on the test split.
I0609 11:01:17.070791 140682684479296 submission_runner.py:419] Time since start: 12193.99s, 	Step: 12741, 	{'train/ctc_loss': 0.47596525084143987, 'train/wer': 0.15801393918837597, 'validation/ctc_loss': 0.7247020375759745, 'validation/wer': 0.20562931492299522, 'validation/num_examples': 5348, 'test/ctc_loss': 0.42773344248016, 'test/wer': 0.13876871204273555, 'test/num_examples': 2472, 'score': 12006.853598833084, 'total_duration': 12193.994516134262, 'accumulated_submission_time': 12006.853598833084, 'accumulated_eval_time': 181.91159439086914, 'accumulated_logging_time': 0.1448516845703125}
I0609 11:01:17.091183 140651714393856 logging_writer.py:48] [12741] accumulated_eval_time=181.911594, accumulated_logging_time=0.144852, accumulated_submission_time=12006.853599, global_step=12741, preemption_count=0, score=12006.853599, test/ctc_loss=0.427733, test/num_examples=2472, test/wer=0.138769, total_duration=12193.994516, train/ctc_loss=0.475965, train/wer=0.158014, validation/ctc_loss=0.724702, validation/num_examples=5348, validation/wer=0.205629
I0609 11:05:22.780166 140651706001152 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.999272, loss=1.644821
I0609 11:05:22.784339 140682684479296 submission.py:296] 13000) loss = 1.645, grad_norm = 3.999
I0609 11:13:15.150811 140651714393856 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.696907, loss=1.499372
I0609 11:13:15.158382 140682684479296 submission.py:296] 13500) loss = 1.499, grad_norm = 3.697
I0609 11:21:04.253661 140651706001152 logging_writer.py:48] [14000] global_step=14000, grad_norm=4.251423, loss=1.567867
I0609 11:21:04.259957 140682684479296 submission.py:296] 14000) loss = 1.568, grad_norm = 4.251
I0609 11:28:57.658575 140651714393856 logging_writer.py:48] [14500] global_step=14500, grad_norm=5.361430, loss=1.555605
I0609 11:28:57.667816 140682684479296 submission.py:296] 14500) loss = 1.556, grad_norm = 5.361
I0609 11:36:45.917855 140651706001152 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.923712, loss=1.650642
I0609 11:36:45.948414 140682684479296 submission.py:296] 15000) loss = 1.651, grad_norm = 4.924
I0609 11:41:17.142438 140682684479296 spec.py:298] Evaluating on the training split.
I0609 11:41:29.837447 140682684479296 spec.py:310] Evaluating on the validation split.
I0609 11:41:40.523259 140682684479296 spec.py:326] Evaluating on the test split.
I0609 11:41:46.284330 140682684479296 submission_runner.py:419] Time since start: 14623.21s, 	Step: 15290, 	{'train/ctc_loss': 0.43413552312933545, 'train/wer': 0.14520170246288172, 'validation/ctc_loss': 0.6759316488660337, 'validation/wer': 0.1939941099792401, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3922549942738241, 'test/wer': 0.12639896004712287, 'test/num_examples': 2472, 'score': 14405.857455730438, 'total_duration': 14623.208038806915, 'accumulated_submission_time': 14405.857455730438, 'accumulated_eval_time': 211.05314421653748, 'accumulated_logging_time': 0.1751267910003662}
I0609 11:41:46.303858 140651714393856 logging_writer.py:48] [15290] accumulated_eval_time=211.053144, accumulated_logging_time=0.175127, accumulated_submission_time=14405.857456, global_step=15290, preemption_count=0, score=14405.857456, test/ctc_loss=0.392255, test/num_examples=2472, test/wer=0.126399, total_duration=14623.208039, train/ctc_loss=0.434136, train/wer=0.145202, validation/ctc_loss=0.675932, validation/num_examples=5348, validation/wer=0.193994
I0609 11:45:06.844859 140651714393856 logging_writer.py:48] [15500] global_step=15500, grad_norm=4.166346, loss=1.491461
I0609 11:45:06.852276 140682684479296 submission.py:296] 15500) loss = 1.491, grad_norm = 4.166
I0609 11:52:54.091834 140682684479296 spec.py:298] Evaluating on the training split.
I0609 11:53:06.782122 140682684479296 spec.py:310] Evaluating on the validation split.
I0609 11:53:17.005681 140682684479296 spec.py:326] Evaluating on the test split.
I0609 11:53:22.593925 140682684479296 submission_runner.py:419] Time since start: 15319.52s, 	Step: 16000, 	{'train/ctc_loss': 0.41770088308432185, 'train/wer': 0.14141054958040197, 'validation/ctc_loss': 0.6589000319438165, 'validation/wer': 0.18956211075170182, 'validation/num_examples': 5348, 'test/ctc_loss': 0.38565401396864313, 'test/wer': 0.12605366319338654, 'test/num_examples': 2472, 'score': 15073.345287561417, 'total_duration': 15319.516749858856, 'accumulated_submission_time': 15073.345287561417, 'accumulated_eval_time': 239.55403804779053, 'accumulated_logging_time': 0.20565199851989746}
I0609 11:53:22.618978 140651714393856 logging_writer.py:48] [16000] accumulated_eval_time=239.554038, accumulated_logging_time=0.205652, accumulated_submission_time=15073.345288, global_step=16000, preemption_count=0, score=15073.345288, test/ctc_loss=0.385654, test/num_examples=2472, test/wer=0.126054, total_duration=15319.516750, train/ctc_loss=0.417701, train/wer=0.141411, validation/ctc_loss=0.658900, validation/num_examples=5348, validation/wer=0.189562
I0609 11:53:22.647249 140651706001152 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=15073.345288
I0609 11:53:23.100013 140682684479296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/nadamw/librispeech_deepspeech_pytorch/trial_1/checkpoint_16000.
I0609 11:53:23.238693 140682684479296 submission_runner.py:581] Tuning trial 1/1
I0609 11:53:23.238947 140682684479296 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0609 11:53:23.239678 140682684479296 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ctc_loss': 32.7659976193408, 'train/wer': 3.4854645119642877, 'validation/ctc_loss': 31.576049829214387, 'validation/wer': 3.1147781586443295, 'validation/num_examples': 5348, 'test/ctc_loss': 31.678717981805278, 'test/wer': 3.3555338898706153, 'test/num_examples': 2472, 'score': 9.537626504898071, 'total_duration': 50.83308291435242, 'accumulated_submission_time': 9.537626504898071, 'accumulated_eval_time': 41.2950279712677, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2544, {'train/ctc_loss': 5.547053079122714, 'train/wer': 0.9381106600873114, 'validation/ctc_loss': 5.4596491517229255, 'validation/wer': 0.8951672862453531, 'validation/num_examples': 5348, 'test/ctc_loss': 5.33285211949158, 'test/wer': 0.8967359291532102, 'test/num_examples': 2472, 'score': 2409.2342808246613, 'total_duration': 2476.811873435974, 'accumulated_submission_time': 2409.2342808246613, 'accumulated_eval_time': 66.57513499259949, 'accumulated_logging_time': 0.02994060516357422, 'global_step': 2544, 'preemption_count': 0}), (5091, {'train/ctc_loss': 0.8548650874276567, 'train/wer': 0.2739750757683513, 'validation/ctc_loss': 1.1004640879294756, 'validation/wer': 0.31041374981895425, 'validation/num_examples': 5348, 'test/ctc_loss': 0.7498393041486547, 'test/wer': 0.24463266508236345, 'test/num_examples': 2472, 'score': 4809.135463476181, 'total_duration': 4906.213579654694, 'accumulated_submission_time': 4809.135463476181, 'accumulated_eval_time': 95.03232002258301, 'accumulated_logging_time': 0.05747532844543457, 'global_step': 5091, 'preemption_count': 0}), (7637, {'train/ctc_loss': 0.6750752934778147, 'train/wer': 0.21990874974014465, 'validation/ctc_loss': 0.9383625615330521, 'validation/wer': 0.2645874571525129, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5820346776404929, 'test/wer': 0.19036012430686736, 'test/num_examples': 2472, 'score': 7208.18540763855, 'total_duration': 7335.048976421356, 'accumulated_submission_time': 7208.18540763855, 'accumulated_eval_time': 123.76101541519165, 'accumulated_logging_time': 0.08675122261047363, 'global_step': 7637, 'preemption_count': 0}), (10189, {'train/ctc_loss': 0.5359222253861731, 'train/wer': 0.17664693589505126, 'validation/ctc_loss': 0.7807185514303295, 'validation/wer': 0.22107854970308502, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4727848369249629, 'test/wer': 0.1541648894034489, 'test/num_examples': 2472, 'score': 9607.826256275177, 'total_duration': 9764.714654445648, 'accumulated_submission_time': 9607.826256275177, 'accumulated_eval_time': 152.7175588607788, 'accumulated_logging_time': 0.11608052253723145, 'global_step': 10189, 'preemption_count': 0}), (12741, {'train/ctc_loss': 0.47596525084143987, 'train/wer': 0.15801393918837597, 'validation/ctc_loss': 0.7247020375759745, 'validation/wer': 0.20562931492299522, 'validation/num_examples': 5348, 'test/ctc_loss': 0.42773344248016, 'test/wer': 0.13876871204273555, 'test/num_examples': 2472, 'score': 12006.853598833084, 'total_duration': 12193.994516134262, 'accumulated_submission_time': 12006.853598833084, 'accumulated_eval_time': 181.91159439086914, 'accumulated_logging_time': 0.1448516845703125, 'global_step': 12741, 'preemption_count': 0}), (15290, {'train/ctc_loss': 0.43413552312933545, 'train/wer': 0.14520170246288172, 'validation/ctc_loss': 0.6759316488660337, 'validation/wer': 0.1939941099792401, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3922549942738241, 'test/wer': 0.12639896004712287, 'test/num_examples': 2472, 'score': 14405.857455730438, 'total_duration': 14623.208038806915, 'accumulated_submission_time': 14405.857455730438, 'accumulated_eval_time': 211.05314421653748, 'accumulated_logging_time': 0.1751267910003662, 'global_step': 15290, 'preemption_count': 0}), (16000, {'train/ctc_loss': 0.41770088308432185, 'train/wer': 0.14141054958040197, 'validation/ctc_loss': 0.6589000319438165, 'validation/wer': 0.18956211075170182, 'validation/num_examples': 5348, 'test/ctc_loss': 0.38565401396864313, 'test/wer': 0.12605366319338654, 'test/num_examples': 2472, 'score': 15073.345287561417, 'total_duration': 15319.516749858856, 'accumulated_submission_time': 15073.345287561417, 'accumulated_eval_time': 239.55403804779053, 'accumulated_logging_time': 0.20565199851989746, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0609 11:53:23.239802 140682684479296 submission_runner.py:584] Timing: 15073.345287561417
I0609 11:53:23.239862 140682684479296 submission_runner.py:586] Total number of evals: 8
I0609 11:53:23.239916 140682684479296 submission_runner.py:587] ====================
I0609 11:53:23.240077 140682684479296 submission_runner.py:655] Final librispeech_deepspeech score: 15073.345287561417
