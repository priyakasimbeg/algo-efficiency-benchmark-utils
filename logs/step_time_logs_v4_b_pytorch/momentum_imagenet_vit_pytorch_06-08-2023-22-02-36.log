torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_vit_pytorch_06-08-2023-22-02-36.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0608 22:02:59.236173 139864469907264 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0608 22:02:59.236214 140224454596416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0608 22:02:59.236248 140058494662464 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0608 22:03:00.222797 139701773821760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0608 22:03:00.222822 140568677848896 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0608 22:03:00.222843 140575528421184 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0608 22:03:00.223072 140410316023616 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0608 22:03:00.232037 139724602201920 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0608 22:03:00.232380 139724602201920 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:03:00.233424 139701773821760 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:03:00.233445 140568677848896 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:03:00.233518 140575528421184 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:03:00.233784 140410316023616 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:03:00.241505 139864469907264 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:03:00.241546 140224454596416 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:03:00.241620 140058494662464 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:03:02.557799 139724602201920 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_vit_pytorch.
W0608 22:03:02.680460 139724602201920 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:03:02.680467 140058494662464 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:03:02.680696 140568677848896 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:03:02.680865 139701773821760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:03:02.680884 140575528421184 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:03:02.681015 139864469907264 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:03:02.681318 140224454596416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:03:02.681631 140410316023616 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0608 22:03:02.686705 139724602201920 submission_runner.py:541] Using RNG seed 1898815650
I0608 22:03:02.688215 139724602201920 submission_runner.py:550] --- Tuning run 1/1 ---
I0608 22:03:02.688337 139724602201920 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_vit_pytorch/trial_1.
I0608 22:03:02.688569 139724602201920 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_vit_pytorch/trial_1/hparams.json.
I0608 22:03:02.689605 139724602201920 submission_runner.py:255] Initializing dataset.
I0608 22:03:09.136804 139724602201920 submission_runner.py:262] Initializing model.
I0608 22:03:13.465246 139724602201920 submission_runner.py:272] Initializing optimizer.
I0608 22:03:13.959895 139724602201920 submission_runner.py:279] Initializing metrics bundle.
I0608 22:03:13.960098 139724602201920 submission_runner.py:297] Initializing checkpoint and logger.
I0608 22:03:14.468269 139724602201920 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0608 22:03:14.469281 139724602201920 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_vit_pytorch/trial_1/flags_0.json.
I0608 22:03:14.523531 139724602201920 submission_runner.py:332] Starting training loop.
I0608 22:03:20.997508 139695975552768 logging_writer.py:48] [0] global_step=0, grad_norm=0.299015, loss=6.907754
I0608 22:03:21.035853 139724602201920 spec.py:298] Evaluating on the training split.
I0608 22:04:20.333631 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 22:05:14.361792 139724602201920 spec.py:326] Evaluating on the test split.
I0608 22:05:14.380922 139724602201920 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 22:05:14.388027 139724602201920 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0608 22:05:14.470868 139724602201920 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 22:05:26.225311 139724602201920 submission_runner.py:419] Time since start: 131.70s, 	Step: 1, 	{'train/accuracy': 0.00109375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.512206315994263, 'total_duration': 131.7021460533142, 'accumulated_submission_time': 6.512206315994263, 'accumulated_eval_time': 125.18945837020874, 'accumulated_logging_time': 0}
I0608 22:05:26.242359 139690355193600 logging_writer.py:48] [1] accumulated_eval_time=125.189458, accumulated_logging_time=0, accumulated_submission_time=6.512206, global_step=1, preemption_count=0, score=6.512206, test/accuracy=0.001000, test/loss=6.907755, test/num_examples=10000, total_duration=131.702146, train/accuracy=0.001094, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0608 22:05:26.260915 139724602201920 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:26.260971 139701773821760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:26.260979 140568677848896 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:26.260979 140575528421184 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:26.261017 139864469907264 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:26.261049 140224454596416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:26.261292 140410316023616 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:26.261340 140058494662464 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:26.814907 139690346800896 logging_writer.py:48] [1] global_step=1, grad_norm=0.309410, loss=6.907754
I0608 22:05:27.205897 139690355193600 logging_writer.py:48] [2] global_step=2, grad_norm=0.303148, loss=6.907754
I0608 22:05:27.595062 139690346800896 logging_writer.py:48] [3] global_step=3, grad_norm=0.307602, loss=6.907753
I0608 22:05:27.983821 139690355193600 logging_writer.py:48] [4] global_step=4, grad_norm=0.304600, loss=6.907754
I0608 22:05:28.372881 139690346800896 logging_writer.py:48] [5] global_step=5, grad_norm=0.302673, loss=6.907753
I0608 22:05:28.764273 139690355193600 logging_writer.py:48] [6] global_step=6, grad_norm=0.313067, loss=6.907750
I0608 22:05:29.155144 139690346800896 logging_writer.py:48] [7] global_step=7, grad_norm=0.309175, loss=6.907746
I0608 22:05:29.546407 139690355193600 logging_writer.py:48] [8] global_step=8, grad_norm=0.307722, loss=6.907749
I0608 22:05:29.935352 139690346800896 logging_writer.py:48] [9] global_step=9, grad_norm=0.313797, loss=6.907743
I0608 22:05:30.323961 139690355193600 logging_writer.py:48] [10] global_step=10, grad_norm=0.310363, loss=6.907728
I0608 22:05:30.716851 139690346800896 logging_writer.py:48] [11] global_step=11, grad_norm=0.310953, loss=6.907722
I0608 22:05:31.106556 139690355193600 logging_writer.py:48] [12] global_step=12, grad_norm=0.303054, loss=6.907736
I0608 22:05:31.496364 139690346800896 logging_writer.py:48] [13] global_step=13, grad_norm=0.301264, loss=6.907700
I0608 22:05:31.891154 139690355193600 logging_writer.py:48] [14] global_step=14, grad_norm=0.314507, loss=6.907720
I0608 22:05:32.281568 139690346800896 logging_writer.py:48] [15] global_step=15, grad_norm=0.307595, loss=6.907686
I0608 22:05:32.672659 139690355193600 logging_writer.py:48] [16] global_step=16, grad_norm=0.300208, loss=6.907699
I0608 22:05:33.066957 139690346800896 logging_writer.py:48] [17] global_step=17, grad_norm=0.297573, loss=6.907679
I0608 22:05:33.460053 139690355193600 logging_writer.py:48] [18] global_step=18, grad_norm=0.304934, loss=6.907697
I0608 22:05:33.857499 139690346800896 logging_writer.py:48] [19] global_step=19, grad_norm=0.308019, loss=6.907666
I0608 22:05:34.251374 139690355193600 logging_writer.py:48] [20] global_step=20, grad_norm=0.313150, loss=6.907635
I0608 22:05:34.642051 139690346800896 logging_writer.py:48] [21] global_step=21, grad_norm=0.313391, loss=6.907564
I0608 22:05:35.036366 139690355193600 logging_writer.py:48] [22] global_step=22, grad_norm=0.307492, loss=6.907706
I0608 22:05:35.430266 139690346800896 logging_writer.py:48] [23] global_step=23, grad_norm=0.302837, loss=6.907601
I0608 22:05:35.823476 139690355193600 logging_writer.py:48] [24] global_step=24, grad_norm=0.305535, loss=6.907538
I0608 22:05:36.216427 139690346800896 logging_writer.py:48] [25] global_step=25, grad_norm=0.303313, loss=6.907488
I0608 22:05:36.607079 139690355193600 logging_writer.py:48] [26] global_step=26, grad_norm=0.307793, loss=6.907537
I0608 22:05:37.009939 139690346800896 logging_writer.py:48] [27] global_step=27, grad_norm=0.299928, loss=6.907632
I0608 22:05:37.405808 139690355193600 logging_writer.py:48] [28] global_step=28, grad_norm=0.309279, loss=6.907469
I0608 22:05:37.801290 139690346800896 logging_writer.py:48] [29] global_step=29, grad_norm=0.308221, loss=6.907614
I0608 22:05:38.193719 139690355193600 logging_writer.py:48] [30] global_step=30, grad_norm=0.308010, loss=6.907379
I0608 22:05:38.587548 139690346800896 logging_writer.py:48] [31] global_step=31, grad_norm=0.301686, loss=6.907557
I0608 22:05:38.977723 139690355193600 logging_writer.py:48] [32] global_step=32, grad_norm=0.301666, loss=6.907511
I0608 22:05:39.370232 139690346800896 logging_writer.py:48] [33] global_step=33, grad_norm=0.309342, loss=6.907303
I0608 22:05:39.769500 139690355193600 logging_writer.py:48] [34] global_step=34, grad_norm=0.306721, loss=6.907236
I0608 22:05:40.166079 139690346800896 logging_writer.py:48] [35] global_step=35, grad_norm=0.300898, loss=6.907343
I0608 22:05:40.558863 139690355193600 logging_writer.py:48] [36] global_step=36, grad_norm=0.306008, loss=6.907351
I0608 22:05:40.954327 139690346800896 logging_writer.py:48] [37] global_step=37, grad_norm=0.302114, loss=6.907336
I0608 22:05:41.351274 139690355193600 logging_writer.py:48] [38] global_step=38, grad_norm=0.300228, loss=6.907212
I0608 22:05:41.750635 139690346800896 logging_writer.py:48] [39] global_step=39, grad_norm=0.309193, loss=6.907269
I0608 22:05:42.151621 139690355193600 logging_writer.py:48] [40] global_step=40, grad_norm=0.307860, loss=6.907174
I0608 22:05:42.549810 139690346800896 logging_writer.py:48] [41] global_step=41, grad_norm=0.305908, loss=6.907201
I0608 22:05:42.944153 139690355193600 logging_writer.py:48] [42] global_step=42, grad_norm=0.303502, loss=6.907450
I0608 22:05:43.350197 139690346800896 logging_writer.py:48] [43] global_step=43, grad_norm=0.308992, loss=6.907270
I0608 22:05:43.742732 139690355193600 logging_writer.py:48] [44] global_step=44, grad_norm=0.303175, loss=6.907011
I0608 22:05:44.136567 139690346800896 logging_writer.py:48] [45] global_step=45, grad_norm=0.292544, loss=6.907039
I0608 22:05:44.542425 139690355193600 logging_writer.py:48] [46] global_step=46, grad_norm=0.310384, loss=6.906487
I0608 22:05:44.942010 139690346800896 logging_writer.py:48] [47] global_step=47, grad_norm=0.305637, loss=6.907055
I0608 22:05:45.340216 139690355193600 logging_writer.py:48] [48] global_step=48, grad_norm=0.300728, loss=6.906904
I0608 22:05:45.739219 139690346800896 logging_writer.py:48] [49] global_step=49, grad_norm=0.309188, loss=6.907072
I0608 22:05:46.134749 139690355193600 logging_writer.py:48] [50] global_step=50, grad_norm=0.297837, loss=6.906691
I0608 22:05:46.528219 139690346800896 logging_writer.py:48] [51] global_step=51, grad_norm=0.304278, loss=6.906936
I0608 22:05:46.919588 139690355193600 logging_writer.py:48] [52] global_step=52, grad_norm=0.309187, loss=6.906658
I0608 22:05:47.329377 139690346800896 logging_writer.py:48] [53] global_step=53, grad_norm=0.297604, loss=6.906478
I0608 22:05:47.728493 139690355193600 logging_writer.py:48] [54] global_step=54, grad_norm=0.307776, loss=6.906506
I0608 22:05:48.124223 139690346800896 logging_writer.py:48] [55] global_step=55, grad_norm=0.303414, loss=6.907215
I0608 22:05:48.523236 139690355193600 logging_writer.py:48] [56] global_step=56, grad_norm=0.304197, loss=6.906685
I0608 22:05:48.925850 139690346800896 logging_writer.py:48] [57] global_step=57, grad_norm=0.311017, loss=6.907072
I0608 22:05:49.329941 139690355193600 logging_writer.py:48] [58] global_step=58, grad_norm=0.309143, loss=6.906374
I0608 22:05:49.723968 139690346800896 logging_writer.py:48] [59] global_step=59, grad_norm=0.306604, loss=6.906833
I0608 22:05:50.116574 139690355193600 logging_writer.py:48] [60] global_step=60, grad_norm=0.307378, loss=6.905894
I0608 22:05:50.523157 139690346800896 logging_writer.py:48] [61] global_step=61, grad_norm=0.295265, loss=6.905839
I0608 22:05:50.916821 139690355193600 logging_writer.py:48] [62] global_step=62, grad_norm=0.297871, loss=6.906651
I0608 22:05:51.311278 139690346800896 logging_writer.py:48] [63] global_step=63, grad_norm=0.300984, loss=6.906497
I0608 22:05:51.705765 139690355193600 logging_writer.py:48] [64] global_step=64, grad_norm=0.311202, loss=6.905830
I0608 22:05:52.101721 139690346800896 logging_writer.py:48] [65] global_step=65, grad_norm=0.296510, loss=6.906384
I0608 22:05:52.500410 139690355193600 logging_writer.py:48] [66] global_step=66, grad_norm=0.302670, loss=6.905646
I0608 22:05:52.898014 139690346800896 logging_writer.py:48] [67] global_step=67, grad_norm=0.310884, loss=6.906548
I0608 22:05:53.290843 139690355193600 logging_writer.py:48] [68] global_step=68, grad_norm=0.296172, loss=6.906384
I0608 22:05:53.683485 139690346800896 logging_writer.py:48] [69] global_step=69, grad_norm=0.293220, loss=6.905838
I0608 22:05:54.076200 139690355193600 logging_writer.py:48] [70] global_step=70, grad_norm=0.300994, loss=6.905488
I0608 22:05:54.468430 139690346800896 logging_writer.py:48] [71] global_step=71, grad_norm=0.307137, loss=6.905721
I0608 22:05:54.867865 139690355193600 logging_writer.py:48] [72] global_step=72, grad_norm=0.300891, loss=6.904959
I0608 22:05:55.271042 139690346800896 logging_writer.py:48] [73] global_step=73, grad_norm=0.305312, loss=6.905697
I0608 22:05:55.666532 139690355193600 logging_writer.py:48] [74] global_step=74, grad_norm=0.294766, loss=6.905359
I0608 22:05:56.059072 139690346800896 logging_writer.py:48] [75] global_step=75, grad_norm=0.305096, loss=6.905620
I0608 22:05:56.459990 139690355193600 logging_writer.py:48] [76] global_step=76, grad_norm=0.299518, loss=6.905898
I0608 22:05:56.855872 139690346800896 logging_writer.py:48] [77] global_step=77, grad_norm=0.299588, loss=6.906190
I0608 22:05:57.256192 139690355193600 logging_writer.py:48] [78] global_step=78, grad_norm=0.303048, loss=6.905012
I0608 22:05:57.660423 139690346800896 logging_writer.py:48] [79] global_step=79, grad_norm=0.312738, loss=6.906402
I0608 22:05:58.071229 139690355193600 logging_writer.py:48] [80] global_step=80, grad_norm=0.303707, loss=6.904874
I0608 22:05:58.473403 139690346800896 logging_writer.py:48] [81] global_step=81, grad_norm=0.301670, loss=6.906014
I0608 22:05:58.865694 139690355193600 logging_writer.py:48] [82] global_step=82, grad_norm=0.299412, loss=6.904684
I0608 22:05:59.270002 139690346800896 logging_writer.py:48] [83] global_step=83, grad_norm=0.305638, loss=6.905136
I0608 22:05:59.732879 139690355193600 logging_writer.py:48] [84] global_step=84, grad_norm=0.302744, loss=6.905745
I0608 22:06:00.124036 139690346800896 logging_writer.py:48] [85] global_step=85, grad_norm=0.301405, loss=6.904282
I0608 22:06:00.518922 139690355193600 logging_writer.py:48] [86] global_step=86, grad_norm=0.311224, loss=6.904281
I0608 22:06:00.913448 139690346800896 logging_writer.py:48] [87] global_step=87, grad_norm=0.293169, loss=6.904879
I0608 22:06:01.308611 139690355193600 logging_writer.py:48] [88] global_step=88, grad_norm=0.309730, loss=6.903918
I0608 22:06:01.713550 139690346800896 logging_writer.py:48] [89] global_step=89, grad_norm=0.307135, loss=6.903441
I0608 22:06:02.109646 139690355193600 logging_writer.py:48] [90] global_step=90, grad_norm=0.309210, loss=6.905184
I0608 22:06:02.622463 139690346800896 logging_writer.py:48] [91] global_step=91, grad_norm=0.296621, loss=6.903375
I0608 22:06:03.019505 139690355193600 logging_writer.py:48] [92] global_step=92, grad_norm=0.308004, loss=6.903923
I0608 22:06:03.423475 139690346800896 logging_writer.py:48] [93] global_step=93, grad_norm=0.306638, loss=6.904247
I0608 22:06:03.821602 139690355193600 logging_writer.py:48] [94] global_step=94, grad_norm=0.300419, loss=6.903842
I0608 22:06:04.214689 139690346800896 logging_writer.py:48] [95] global_step=95, grad_norm=0.306405, loss=6.904167
I0608 22:06:04.612502 139690355193600 logging_writer.py:48] [96] global_step=96, grad_norm=0.308205, loss=6.904430
I0608 22:06:05.006051 139690346800896 logging_writer.py:48] [97] global_step=97, grad_norm=0.307117, loss=6.902854
I0608 22:06:05.401518 139690355193600 logging_writer.py:48] [98] global_step=98, grad_norm=0.304984, loss=6.903701
I0608 22:06:05.798283 139690346800896 logging_writer.py:48] [99] global_step=99, grad_norm=0.301305, loss=6.903477
I0608 22:06:06.198161 139690355193600 logging_writer.py:48] [100] global_step=100, grad_norm=0.305905, loss=6.904166
I0608 22:08:46.011127 139690346800896 logging_writer.py:48] [500] global_step=500, grad_norm=0.770224, loss=6.755209
I0608 22:12:06.790259 139690355193600 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.003106, loss=6.551130
I0608 22:12:26.260712 139724602201920 spec.py:298] Evaluating on the training split.
I0608 22:13:09.298522 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 22:13:53.355393 139724602201920 spec.py:326] Evaluating on the test split.
I0608 22:13:54.804063 139724602201920 submission_runner.py:419] Time since start: 640.28s, 	Step: 1049, 	{'train/accuracy': 0.04203125, 'train/loss': 5.972650756835938, 'validation/accuracy': 0.04124, 'validation/loss': 5.997063125, 'validation/num_examples': 50000, 'test/accuracy': 0.0309, 'test/loss': 6.111936328125, 'test/num_examples': 10000, 'score': 425.8988502025604, 'total_duration': 640.280957698822, 'accumulated_submission_time': 425.8988502025604, 'accumulated_eval_time': 213.73279643058777, 'accumulated_logging_time': 0.025358200073242188}
I0608 22:13:54.814916 139681295480576 logging_writer.py:48] [1049] accumulated_eval_time=213.732796, accumulated_logging_time=0.025358, accumulated_submission_time=425.898850, global_step=1049, preemption_count=0, score=425.898850, test/accuracy=0.030900, test/loss=6.111936, test/num_examples=10000, total_duration=640.280958, train/accuracy=0.042031, train/loss=5.972651, validation/accuracy=0.041240, validation/loss=5.997063, validation/num_examples=50000
I0608 22:16:53.315853 139681303873280 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.017829, loss=6.409945
I0608 22:20:05.166115 139681295480576 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.683825, loss=6.364762
I0608 22:20:54.935356 139724602201920 spec.py:298] Evaluating on the training split.
I0608 22:21:38.381439 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 22:22:27.491050 139724602201920 spec.py:326] Evaluating on the test split.
I0608 22:22:28.902436 139724602201920 submission_runner.py:419] Time since start: 1154.38s, 	Step: 2130, 	{'train/accuracy': 0.080234375, 'train/loss': 5.459764404296875, 'validation/accuracy': 0.0748, 'validation/loss': 5.4979775, 'validation/num_examples': 50000, 'test/accuracy': 0.0558, 'test/loss': 5.67322421875, 'test/num_examples': 10000, 'score': 845.3234307765961, 'total_duration': 1154.379370212555, 'accumulated_submission_time': 845.3234307765961, 'accumulated_eval_time': 307.6998801231384, 'accumulated_logging_time': 0.04538440704345703}
I0608 22:22:28.912713 139681303873280 logging_writer.py:48] [2130] accumulated_eval_time=307.699880, accumulated_logging_time=0.045384, accumulated_submission_time=845.323431, global_step=2130, preemption_count=0, score=845.323431, test/accuracy=0.055800, test/loss=5.673224, test/num_examples=10000, total_duration=1154.379370, train/accuracy=0.080234, train/loss=5.459764, validation/accuracy=0.074800, validation/loss=5.497978, validation/num_examples=50000
I0608 22:24:55.681288 139681295480576 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.798940, loss=6.261022
I0608 22:28:09.608894 139681303873280 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.843650, loss=6.298407
I0608 22:29:29.031552 139724602201920 spec.py:298] Evaluating on the training split.
I0608 22:30:13.486813 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 22:31:07.957614 139724602201920 spec.py:326] Evaluating on the test split.
I0608 22:31:09.375815 139724602201920 submission_runner.py:419] Time since start: 1674.85s, 	Step: 3208, 	{'train/accuracy': 0.1105859375, 'train/loss': 5.091164245605468, 'validation/accuracy': 0.10134, 'validation/loss': 5.1486984375, 'validation/num_examples': 50000, 'test/accuracy': 0.0792, 'test/loss': 5.39533046875, 'test/num_examples': 10000, 'score': 1264.7535791397095, 'total_duration': 1674.8526811599731, 'accumulated_submission_time': 1264.7535791397095, 'accumulated_eval_time': 408.0441527366638, 'accumulated_logging_time': 0.06416440010070801}
I0608 22:31:09.386288 139681295480576 logging_writer.py:48] [3208] accumulated_eval_time=408.044153, accumulated_logging_time=0.064164, accumulated_submission_time=1264.753579, global_step=3208, preemption_count=0, score=1264.753579, test/accuracy=0.079200, test/loss=5.395330, test/num_examples=10000, total_duration=1674.852681, train/accuracy=0.110586, train/loss=5.091164, validation/accuracy=0.101340, validation/loss=5.148698, validation/num_examples=50000
I0608 22:33:01.748730 139681303873280 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.713315, loss=6.226721
I0608 22:36:16.346748 139681295480576 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.617654, loss=6.262656
I0608 22:38:09.466968 139724602201920 spec.py:298] Evaluating on the training split.
I0608 22:38:53.099689 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 22:39:38.753839 139724602201920 spec.py:326] Evaluating on the test split.
I0608 22:39:40.167796 139724602201920 submission_runner.py:419] Time since start: 2185.64s, 	Step: 4296, 	{'train/accuracy': 0.14220703125, 'train/loss': 4.746715087890625, 'validation/accuracy': 0.12986, 'validation/loss': 4.82099, 'validation/num_examples': 50000, 'test/accuracy': 0.1027, 'test/loss': 5.119153125, 'test/num_examples': 10000, 'score': 1684.136578321457, 'total_duration': 2185.6446311473846, 'accumulated_submission_time': 1684.136578321457, 'accumulated_eval_time': 498.7449474334717, 'accumulated_logging_time': 0.08309650421142578}
I0608 22:39:40.178168 139681303873280 logging_writer.py:48] [4296] accumulated_eval_time=498.744947, accumulated_logging_time=0.083097, accumulated_submission_time=1684.136578, global_step=4296, preemption_count=0, score=1684.136578, test/accuracy=0.102700, test/loss=5.119153, test/num_examples=10000, total_duration=2185.644631, train/accuracy=0.142207, train/loss=4.746715, validation/accuracy=0.129860, validation/loss=4.820990, validation/num_examples=50000
I0608 22:40:58.831053 139681295480576 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.664945, loss=6.040047
I0608 22:44:13.639965 139681303873280 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.635720, loss=6.083900
I0608 22:46:40.253529 139724602201920 spec.py:298] Evaluating on the training split.
I0608 22:47:24.669854 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 22:48:10.577851 139724602201920 spec.py:326] Evaluating on the test split.
I0608 22:48:11.997505 139724602201920 submission_runner.py:419] Time since start: 2697.47s, 	Step: 5377, 	{'train/accuracy': 0.17154296875, 'train/loss': 4.536736145019531, 'validation/accuracy': 0.15912, 'validation/loss': 4.6192675, 'validation/num_examples': 50000, 'test/accuracy': 0.1219, 'test/loss': 4.921911328125, 'test/num_examples': 10000, 'score': 2103.5309641361237, 'total_duration': 2697.472895383835, 'accumulated_submission_time': 2103.5309641361237, 'accumulated_eval_time': 590.4873900413513, 'accumulated_logging_time': 0.10241031646728516}
I0608 22:48:12.007623 139681295480576 logging_writer.py:48] [5377] accumulated_eval_time=590.487390, accumulated_logging_time=0.102410, accumulated_submission_time=2103.530964, global_step=5377, preemption_count=0, score=2103.530964, test/accuracy=0.121900, test/loss=4.921911, test/num_examples=10000, total_duration=2697.472895, train/accuracy=0.171543, train/loss=4.536736, validation/accuracy=0.159120, validation/loss=4.619268, validation/num_examples=50000
I0608 22:48:59.295292 139681303873280 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.662432, loss=5.935826
I0608 22:52:11.613155 139681295480576 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.627827, loss=5.902082
I0608 22:55:12.142658 139724602201920 spec.py:298] Evaluating on the training split.
I0608 22:55:56.437246 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 22:56:41.469868 139724602201920 spec.py:326] Evaluating on the test split.
I0608 22:56:42.883351 139724602201920 submission_runner.py:419] Time since start: 3208.36s, 	Step: 6458, 	{'train/accuracy': 0.21005859375, 'train/loss': 4.217542724609375, 'validation/accuracy': 0.19206, 'validation/loss': 4.3205046875, 'validation/num_examples': 50000, 'test/accuracy': 0.1471, 'test/loss': 4.681320703125, 'test/num_examples': 10000, 'score': 2522.990050792694, 'total_duration': 3208.3602385520935, 'accumulated_submission_time': 2522.990050792694, 'accumulated_eval_time': 681.2280774116516, 'accumulated_logging_time': 0.1212470531463623}
I0608 22:56:42.895055 139681303873280 logging_writer.py:48] [6458] accumulated_eval_time=681.228077, accumulated_logging_time=0.121247, accumulated_submission_time=2522.990051, global_step=6458, preemption_count=0, score=2522.990051, test/accuracy=0.147100, test/loss=4.681321, test/num_examples=10000, total_duration=3208.360239, train/accuracy=0.210059, train/loss=4.217543, validation/accuracy=0.192060, validation/loss=4.320505, validation/num_examples=50000
I0608 22:56:59.290975 139681295480576 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.603077, loss=5.751675
I0608 23:00:11.108708 139681303873280 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.632034, loss=5.731648
I0608 23:03:27.081694 139681295480576 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.587913, loss=5.468729
I0608 23:03:43.147387 139724602201920 spec.py:298] Evaluating on the training split.
I0608 23:04:28.919680 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 23:05:18.620608 139724602201920 spec.py:326] Evaluating on the test split.
I0608 23:05:20.037145 139724602201920 submission_runner.py:419] Time since start: 3725.51s, 	Step: 7537, 	{'train/accuracy': 0.24765625, 'train/loss': 3.930257568359375, 'validation/accuracy': 0.22796, 'validation/loss': 4.0331696875, 'validation/num_examples': 50000, 'test/accuracy': 0.1761, 'test/loss': 4.439977734375, 'test/num_examples': 10000, 'score': 2942.560126066208, 'total_duration': 3725.5140585899353, 'accumulated_submission_time': 2942.560126066208, 'accumulated_eval_time': 778.118017911911, 'accumulated_logging_time': 0.14264917373657227}
I0608 23:05:20.047752 139681303873280 logging_writer.py:48] [7537] accumulated_eval_time=778.118018, accumulated_logging_time=0.142649, accumulated_submission_time=2942.560126, global_step=7537, preemption_count=0, score=2942.560126, test/accuracy=0.176100, test/loss=4.439978, test/num_examples=10000, total_duration=3725.514059, train/accuracy=0.247656, train/loss=3.930258, validation/accuracy=0.227960, validation/loss=4.033170, validation/num_examples=50000
I0608 23:08:17.845515 139681295480576 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.541604, loss=5.766640
I0608 23:11:30.101372 139681303873280 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.613791, loss=5.616246
I0608 23:12:20.401662 139724602201920 spec.py:298] Evaluating on the training split.
I0608 23:13:06.304402 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 23:13:52.472189 139724602201920 spec.py:326] Evaluating on the test split.
I0608 23:13:53.891857 139724602201920 submission_runner.py:419] Time since start: 4239.37s, 	Step: 8629, 	{'train/accuracy': 0.278125, 'train/loss': 3.6975979614257812, 'validation/accuracy': 0.25704, 'validation/loss': 3.8172096875, 'validation/num_examples': 50000, 'test/accuracy': 0.198, 'test/loss': 4.245627734375, 'test/num_examples': 10000, 'score': 3362.21826839447, 'total_duration': 4239.368790149689, 'accumulated_submission_time': 3362.21826839447, 'accumulated_eval_time': 871.6083207130432, 'accumulated_logging_time': 0.16131043434143066}
I0608 23:13:53.902908 139681295480576 logging_writer.py:48] [8629] accumulated_eval_time=871.608321, accumulated_logging_time=0.161310, accumulated_submission_time=3362.218268, global_step=8629, preemption_count=0, score=3362.218268, test/accuracy=0.198000, test/loss=4.245628, test/num_examples=10000, total_duration=4239.368790, train/accuracy=0.278125, train/loss=3.697598, validation/accuracy=0.257040, validation/loss=3.817210, validation/num_examples=50000
I0608 23:16:19.945892 139681303873280 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.601831, loss=5.544878
I0608 23:19:32.255685 139681295480576 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.566899, loss=5.597754
I0608 23:20:53.913981 139724602201920 spec.py:298] Evaluating on the training split.
I0608 23:21:38.036361 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 23:22:24.113880 139724602201920 spec.py:326] Evaluating on the test split.
I0608 23:22:25.526936 139724602201920 submission_runner.py:419] Time since start: 4751.00s, 	Step: 9713, 	{'train/accuracy': 0.31453125, 'train/loss': 3.5063861083984373, 'validation/accuracy': 0.28666, 'validation/loss': 3.6373209375, 'validation/num_examples': 50000, 'test/accuracy': 0.2199, 'test/loss': 4.09714140625, 'test/num_examples': 10000, 'score': 3781.530790567398, 'total_duration': 4751.003882646561, 'accumulated_submission_time': 3781.530790567398, 'accumulated_eval_time': 963.2212760448456, 'accumulated_logging_time': 0.18123579025268555}
I0608 23:22:25.537872 139681303873280 logging_writer.py:48] [9713] accumulated_eval_time=963.221276, accumulated_logging_time=0.181236, accumulated_submission_time=3781.530791, global_step=9713, preemption_count=0, score=3781.530791, test/accuracy=0.219900, test/loss=4.097141, test/num_examples=10000, total_duration=4751.003883, train/accuracy=0.314531, train/loss=3.506386, validation/accuracy=0.286660, validation/loss=3.637321, validation/num_examples=50000
I0608 23:24:19.298829 139681295480576 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.572128, loss=5.351512
I0608 23:27:33.020003 139681303873280 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.497617, loss=5.536295
I0608 23:29:25.804714 139724602201920 spec.py:298] Evaluating on the training split.
I0608 23:30:10.596120 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 23:30:56.580791 139724602201920 spec.py:326] Evaluating on the test split.
I0608 23:30:57.994012 139724602201920 submission_runner.py:419] Time since start: 5263.47s, 	Step: 10795, 	{'train/accuracy': 0.345703125, 'train/loss': 3.34357177734375, 'validation/accuracy': 0.31688, 'validation/loss': 3.487991875, 'validation/num_examples': 50000, 'test/accuracy': 0.2481, 'test/loss': 3.9675671875, 'test/num_examples': 10000, 'score': 4201.108502864838, 'total_duration': 5263.470956802368, 'accumulated_submission_time': 4201.108502864838, 'accumulated_eval_time': 1055.410625219345, 'accumulated_logging_time': 0.20115423202514648}
I0608 23:30:58.005204 139681295480576 logging_writer.py:48] [10795] accumulated_eval_time=1055.410625, accumulated_logging_time=0.201154, accumulated_submission_time=4201.108503, global_step=10795, preemption_count=0, score=4201.108503, test/accuracy=0.248100, test/loss=3.967567, test/num_examples=10000, total_duration=5263.470957, train/accuracy=0.345703, train/loss=3.343572, validation/accuracy=0.316880, validation/loss=3.487992, validation/num_examples=50000
I0608 23:32:17.148029 139681303873280 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.585375, loss=5.337755
I0608 23:35:34.873454 139681295480576 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.595215, loss=5.178590
I0608 23:37:58.187456 139724602201920 spec.py:298] Evaluating on the training split.
I0608 23:38:42.926322 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 23:39:28.955078 139724602201920 spec.py:326] Evaluating on the test split.
I0608 23:39:30.371130 139724602201920 submission_runner.py:419] Time since start: 5775.85s, 	Step: 11874, 	{'train/accuracy': 0.3717578125, 'train/loss': 3.096274108886719, 'validation/accuracy': 0.33978, 'validation/loss': 3.2589484375, 'validation/num_examples': 50000, 'test/accuracy': 0.263, 'test/loss': 3.791540234375, 'test/num_examples': 10000, 'score': 4620.604208469391, 'total_duration': 5775.848053693771, 'accumulated_submission_time': 4620.604208469391, 'accumulated_eval_time': 1147.5943143367767, 'accumulated_logging_time': 0.22326278686523438}
I0608 23:39:30.382086 139681303873280 logging_writer.py:48] [11874] accumulated_eval_time=1147.594314, accumulated_logging_time=0.223263, accumulated_submission_time=4620.604208, global_step=11874, preemption_count=0, score=4620.604208, test/accuracy=0.263000, test/loss=3.791540, test/num_examples=10000, total_duration=5775.848054, train/accuracy=0.371758, train/loss=3.096274, validation/accuracy=0.339780, validation/loss=3.258948, validation/num_examples=50000
I0608 23:40:18.932321 139681295480576 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.527439, loss=5.220625
I0608 23:43:35.007732 139681303873280 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.560762, loss=5.250754
I0608 23:46:30.485602 139724602201920 spec.py:298] Evaluating on the training split.
I0608 23:47:15.176448 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 23:48:01.321459 139724602201920 spec.py:326] Evaluating on the test split.
I0608 23:48:02.736764 139724602201920 submission_runner.py:419] Time since start: 6288.21s, 	Step: 12952, 	{'train/accuracy': 0.3945703125, 'train/loss': 3.005664978027344, 'validation/accuracy': 0.3602, 'validation/loss': 3.162241875, 'validation/num_examples': 50000, 'test/accuracy': 0.2767, 'test/loss': 3.705543359375, 'test/num_examples': 10000, 'score': 5040.032082080841, 'total_duration': 6288.213706970215, 'accumulated_submission_time': 5040.032082080841, 'accumulated_eval_time': 1239.8454813957214, 'accumulated_logging_time': 0.2423415184020996}
I0608 23:48:02.748150 139681295480576 logging_writer.py:48] [12952] accumulated_eval_time=1239.845481, accumulated_logging_time=0.242342, accumulated_submission_time=5040.032082, global_step=12952, preemption_count=0, score=5040.032082, test/accuracy=0.276700, test/loss=3.705543, test/num_examples=10000, total_duration=6288.213707, train/accuracy=0.394570, train/loss=3.005665, validation/accuracy=0.360200, validation/loss=3.162242, validation/num_examples=50000
I0608 23:48:21.590121 139681303873280 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.613130, loss=5.177360
I0608 23:51:33.230242 139681295480576 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.643233, loss=5.309865
I0608 23:54:50.009824 139681303873280 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.664235, loss=4.870454
I0608 23:55:03.053698 139724602201920 spec.py:298] Evaluating on the training split.
I0608 23:55:47.599610 139724602201920 spec.py:310] Evaluating on the validation split.
I0608 23:56:33.931376 139724602201920 spec.py:326] Evaluating on the test split.
I0608 23:56:35.348243 139724602201920 submission_runner.py:419] Time since start: 6800.83s, 	Step: 14035, 	{'train/accuracy': 0.413125, 'train/loss': 2.941761169433594, 'validation/accuracy': 0.37964, 'validation/loss': 3.11401625, 'validation/num_examples': 50000, 'test/accuracy': 0.2922, 'test/loss': 3.635890625, 'test/num_examples': 10000, 'score': 5459.646435499191, 'total_duration': 6800.825165987015, 'accumulated_submission_time': 5459.646435499191, 'accumulated_eval_time': 1332.1400785446167, 'accumulated_logging_time': 0.26314258575439453}
I0608 23:56:35.359101 139681295480576 logging_writer.py:48] [14035] accumulated_eval_time=1332.140079, accumulated_logging_time=0.263143, accumulated_submission_time=5459.646435, global_step=14035, preemption_count=0, score=5459.646435, test/accuracy=0.292200, test/loss=3.635891, test/num_examples=10000, total_duration=6800.825166, train/accuracy=0.413125, train/loss=2.941761, validation/accuracy=0.379640, validation/loss=3.114016, validation/num_examples=50000
I0608 23:59:34.521068 139681303873280 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.596911, loss=4.947909
I0609 00:02:49.654734 139681295480576 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.625833, loss=4.838748
I0609 00:03:35.372449 139724602201920 spec.py:298] Evaluating on the training split.
I0609 00:04:20.238342 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 00:05:06.031394 139724602201920 spec.py:326] Evaluating on the test split.
I0609 00:05:07.447365 139724602201920 submission_runner.py:419] Time since start: 7312.92s, 	Step: 15114, 	{'train/accuracy': 0.4309375, 'train/loss': 2.8474212646484376, 'validation/accuracy': 0.39552, 'validation/loss': 3.02208875, 'validation/num_examples': 50000, 'test/accuracy': 0.302, 'test/loss': 3.55537265625, 'test/num_examples': 10000, 'score': 5878.971431016922, 'total_duration': 7312.924280405045, 'accumulated_submission_time': 5878.971431016922, 'accumulated_eval_time': 1424.2151110172272, 'accumulated_logging_time': 0.2823631763458252}
I0609 00:05:07.458869 139681303873280 logging_writer.py:48] [15114] accumulated_eval_time=1424.215111, accumulated_logging_time=0.282363, accumulated_submission_time=5878.971431, global_step=15114, preemption_count=0, score=5878.971431, test/accuracy=0.302000, test/loss=3.555373, test/num_examples=10000, total_duration=7312.924280, train/accuracy=0.430937, train/loss=2.847421, validation/accuracy=0.395520, validation/loss=3.022089, validation/num_examples=50000
I0609 00:07:36.520585 139681295480576 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.576139, loss=4.596278
I0609 00:10:48.430958 139681303873280 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.549666, loss=5.067209
I0609 00:12:07.756762 139724602201920 spec.py:298] Evaluating on the training split.
I0609 00:12:53.521288 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 00:13:48.284458 139724602201920 spec.py:326] Evaluating on the test split.
I0609 00:13:49.696896 139724602201920 submission_runner.py:419] Time since start: 7835.17s, 	Step: 16202, 	{'train/accuracy': 0.45552734375, 'train/loss': 2.6575418090820313, 'validation/accuracy': 0.42118, 'validation/loss': 2.8402753125, 'validation/num_examples': 50000, 'test/accuracy': 0.3219, 'test/loss': 3.404101171875, 'test/num_examples': 10000, 'score': 6298.574860572815, 'total_duration': 7835.173810005188, 'accumulated_submission_time': 6298.574860572815, 'accumulated_eval_time': 1526.1553792953491, 'accumulated_logging_time': 0.3031024932861328}
I0609 00:13:49.707806 139681295480576 logging_writer.py:48] [16202] accumulated_eval_time=1526.155379, accumulated_logging_time=0.303102, accumulated_submission_time=6298.574861, global_step=16202, preemption_count=0, score=6298.574861, test/accuracy=0.321900, test/loss=3.404101, test/num_examples=10000, total_duration=7835.173810, train/accuracy=0.455527, train/loss=2.657542, validation/accuracy=0.421180, validation/loss=2.840275, validation/num_examples=50000
I0609 00:15:46.528181 139681303873280 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.573381, loss=5.271482
I0609 00:18:58.817982 139681295480576 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.617289, loss=5.121726
I0609 00:20:49.896349 139724602201920 spec.py:298] Evaluating on the training split.
I0609 00:21:35.175373 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 00:22:21.243886 139724602201920 spec.py:326] Evaluating on the test split.
I0609 00:22:22.658840 139724602201920 submission_runner.py:419] Time since start: 8348.14s, 	Step: 17287, 	{'train/accuracy': 0.47265625, 'train/loss': 2.5531298828125, 'validation/accuracy': 0.43288, 'validation/loss': 2.748838125, 'validation/num_examples': 50000, 'test/accuracy': 0.3318, 'test/loss': 3.327375390625, 'test/num_examples': 10000, 'score': 6718.074041604996, 'total_duration': 8348.135802984238, 'accumulated_submission_time': 6718.074041604996, 'accumulated_eval_time': 1618.917900800705, 'accumulated_logging_time': 0.32452893257141113}
I0609 00:22:22.669769 139681303873280 logging_writer.py:48] [17287] accumulated_eval_time=1618.917901, accumulated_logging_time=0.324529, accumulated_submission_time=6718.074042, global_step=17287, preemption_count=0, score=6718.074042, test/accuracy=0.331800, test/loss=3.327375, test/num_examples=10000, total_duration=8348.135803, train/accuracy=0.472656, train/loss=2.553130, validation/accuracy=0.432880, validation/loss=2.748838, validation/num_examples=50000
I0609 00:23:48.007828 139681295480576 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.613454, loss=4.929204
I0609 00:27:02.149795 139681303873280 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.653279, loss=4.839044
I0609 00:29:22.669521 139724602201920 spec.py:298] Evaluating on the training split.
I0609 00:30:07.236181 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 00:30:53.253399 139724602201920 spec.py:326] Evaluating on the test split.
I0609 00:30:54.668098 139724602201920 submission_runner.py:419] Time since start: 8860.15s, 	Step: 18366, 	{'train/accuracy': 0.48986328125, 'train/loss': 2.4748135375976563, 'validation/accuracy': 0.44892, 'validation/loss': 2.6802884375, 'validation/num_examples': 50000, 'test/accuracy': 0.3485, 'test/loss': 3.27262890625, 'test/num_examples': 10000, 'score': 7137.387549161911, 'total_duration': 8860.145043373108, 'accumulated_submission_time': 7137.387549161911, 'accumulated_eval_time': 1710.9167017936707, 'accumulated_logging_time': 0.3446993827819824}
I0609 00:30:54.678874 139681295480576 logging_writer.py:48] [18366] accumulated_eval_time=1710.916702, accumulated_logging_time=0.344699, accumulated_submission_time=7137.387549, global_step=18366, preemption_count=0, score=7137.387549, test/accuracy=0.348500, test/loss=3.272629, test/num_examples=10000, total_duration=8860.145043, train/accuracy=0.489863, train/loss=2.474814, validation/accuracy=0.448920, validation/loss=2.680288, validation/num_examples=50000
I0609 00:31:46.703862 139681303873280 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.569858, loss=4.974540
I0609 00:35:04.211645 139681295480576 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.593614, loss=4.669300
I0609 00:37:54.943564 139724602201920 spec.py:298] Evaluating on the training split.
I0609 00:38:39.748615 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 00:39:25.861997 139724602201920 spec.py:326] Evaluating on the test split.
I0609 00:39:27.278442 139724602201920 submission_runner.py:419] Time since start: 9372.76s, 	Step: 19446, 	{'train/accuracy': 0.50876953125, 'train/loss': 2.4081268310546875, 'validation/accuracy': 0.46108, 'validation/loss': 2.6232475, 'validation/num_examples': 50000, 'test/accuracy': 0.3579, 'test/loss': 3.2109384765625, 'test/num_examples': 10000, 'score': 7556.97265124321, 'total_duration': 9372.75537443161, 'accumulated_submission_time': 7556.97265124321, 'accumulated_eval_time': 1803.2517108917236, 'accumulated_logging_time': 0.36464405059814453}
I0609 00:39:27.290370 139681303873280 logging_writer.py:48] [19446] accumulated_eval_time=1803.251711, accumulated_logging_time=0.364644, accumulated_submission_time=7556.972651, global_step=19446, preemption_count=0, score=7556.972651, test/accuracy=0.357900, test/loss=3.210938, test/num_examples=10000, total_duration=9372.755374, train/accuracy=0.508770, train/loss=2.408127, validation/accuracy=0.461080, validation/loss=2.623248, validation/num_examples=50000
I0609 00:39:48.440804 139681295480576 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.620065, loss=4.595652
I0609 00:43:03.812719 139681303873280 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.626074, loss=4.628221
I0609 00:46:17.837115 139681295480576 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.550278, loss=4.742893
I0609 00:46:27.456140 139724602201920 spec.py:298] Evaluating on the training split.
I0609 00:47:12.542399 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 00:47:58.784141 139724602201920 spec.py:326] Evaluating on the test split.
I0609 00:48:00.199142 139724602201920 submission_runner.py:419] Time since start: 9885.68s, 	Step: 20526, 	{'train/accuracy': 0.51154296875, 'train/loss': 2.3534507751464844, 'validation/accuracy': 0.46686, 'validation/loss': 2.568821875, 'validation/num_examples': 50000, 'test/accuracy': 0.3702, 'test/loss': 3.1335873046875, 'test/num_examples': 10000, 'score': 7976.460025548935, 'total_duration': 9885.676082134247, 'accumulated_submission_time': 7976.460025548935, 'accumulated_eval_time': 1895.9947192668915, 'accumulated_logging_time': 0.38517093658447266}
I0609 00:48:00.210633 139681303873280 logging_writer.py:48] [20526] accumulated_eval_time=1895.994719, accumulated_logging_time=0.385171, accumulated_submission_time=7976.460026, global_step=20526, preemption_count=0, score=7976.460026, test/accuracy=0.370200, test/loss=3.133587, test/num_examples=10000, total_duration=9885.676082, train/accuracy=0.511543, train/loss=2.353451, validation/accuracy=0.466860, validation/loss=2.568822, validation/num_examples=50000
I0609 00:51:03.302303 139681295480576 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.590613, loss=4.958465
I0609 00:54:21.299381 139681303873280 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.623613, loss=4.520582
I0609 00:55:00.439381 139724602201920 spec.py:298] Evaluating on the training split.
I0609 00:55:45.374226 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 00:56:32.001911 139724602201920 spec.py:326] Evaluating on the test split.
I0609 00:56:33.414957 139724602201920 submission_runner.py:419] Time since start: 10398.89s, 	Step: 21603, 	{'train/accuracy': 0.5262109375, 'train/loss': 2.299521484375, 'validation/accuracy': 0.48026, 'validation/loss': 2.5188703125, 'validation/num_examples': 50000, 'test/accuracy': 0.3742, 'test/loss': 3.095450390625, 'test/num_examples': 10000, 'score': 8396.01089143753, 'total_duration': 10398.891847610474, 'accumulated_submission_time': 8396.01089143753, 'accumulated_eval_time': 1988.9704315662384, 'accumulated_logging_time': 0.4051327705383301}
I0609 00:56:33.425444 139681295480576 logging_writer.py:48] [21603] accumulated_eval_time=1988.970432, accumulated_logging_time=0.405133, accumulated_submission_time=8396.010891, global_step=21603, preemption_count=0, score=8396.010891, test/accuracy=0.374200, test/loss=3.095450, test/num_examples=10000, total_duration=10398.891848, train/accuracy=0.526211, train/loss=2.299521, validation/accuracy=0.480260, validation/loss=2.518870, validation/num_examples=50000
I0609 00:59:06.576662 139681303873280 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.588815, loss=4.735522
I0609 01:02:21.692966 139681295480576 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.628189, loss=4.402501
I0609 01:03:33.742309 139724602201920 spec.py:298] Evaluating on the training split.
I0609 01:04:18.822208 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 01:05:04.929814 139724602201920 spec.py:326] Evaluating on the test split.
I0609 01:05:06.345538 139724602201920 submission_runner.py:419] Time since start: 10911.82s, 	Step: 22682, 	{'train/accuracy': 0.54404296875, 'train/loss': 2.204906768798828, 'validation/accuracy': 0.49798, 'validation/loss': 2.43374421875, 'validation/num_examples': 50000, 'test/accuracy': 0.3858, 'test/loss': 3.033079296875, 'test/num_examples': 10000, 'score': 8815.653045415878, 'total_duration': 10911.822468996048, 'accumulated_submission_time': 8815.653045415878, 'accumulated_eval_time': 2081.5736813545227, 'accumulated_logging_time': 0.4238708019256592}
I0609 01:05:06.358003 139681303873280 logging_writer.py:48] [22682] accumulated_eval_time=2081.573681, accumulated_logging_time=0.423871, accumulated_submission_time=8815.653045, global_step=22682, preemption_count=0, score=8815.653045, test/accuracy=0.385800, test/loss=3.033079, test/num_examples=10000, total_duration=10911.822469, train/accuracy=0.544043, train/loss=2.204907, validation/accuracy=0.497980, validation/loss=2.433744, validation/num_examples=50000
I0609 01:07:08.816353 139681295480576 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.606106, loss=4.764846
I0609 01:10:20.881359 139681303873280 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.628317, loss=4.593205
I0609 01:12:06.366326 139724602201920 spec.py:298] Evaluating on the training split.
I0609 01:12:53.179429 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 01:13:40.199063 139724602201920 spec.py:326] Evaluating on the test split.
I0609 01:13:41.616829 139724602201920 submission_runner.py:419] Time since start: 11427.09s, 	Step: 23765, 	{'train/accuracy': 0.55125, 'train/loss': 2.1547647094726563, 'validation/accuracy': 0.50358, 'validation/loss': 2.38638515625, 'validation/num_examples': 50000, 'test/accuracy': 0.3991, 'test/loss': 2.9879146484375, 'test/num_examples': 10000, 'score': 9234.983407020569, 'total_duration': 11427.093763589859, 'accumulated_submission_time': 9234.983407020569, 'accumulated_eval_time': 2176.824099302292, 'accumulated_logging_time': 0.4445512294769287}
I0609 01:13:41.627876 139681295480576 logging_writer.py:48] [23765] accumulated_eval_time=2176.824099, accumulated_logging_time=0.444551, accumulated_submission_time=9234.983407, global_step=23765, preemption_count=0, score=9234.983407, test/accuracy=0.399100, test/loss=2.987915, test/num_examples=10000, total_duration=11427.093764, train/accuracy=0.551250, train/loss=2.154765, validation/accuracy=0.503580, validation/loss=2.386385, validation/num_examples=50000
I0609 01:15:14.022673 139681303873280 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.572154, loss=4.993403
I0609 01:18:26.666432 139681295480576 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.577780, loss=4.723907
I0609 01:20:41.992482 139724602201920 spec.py:298] Evaluating on the training split.
I0609 01:21:27.706409 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 01:22:14.230026 139724602201920 spec.py:326] Evaluating on the test split.
I0609 01:22:15.646198 139724602201920 submission_runner.py:419] Time since start: 11941.12s, 	Step: 24849, 	{'train/accuracy': 0.5599609375, 'train/loss': 2.13779541015625, 'validation/accuracy': 0.5105, 'validation/loss': 2.37689078125, 'validation/num_examples': 50000, 'test/accuracy': 0.4026, 'test/loss': 2.959683984375, 'test/num_examples': 10000, 'score': 9654.659735441208, 'total_duration': 11941.123112678528, 'accumulated_submission_time': 9654.659735441208, 'accumulated_eval_time': 2270.4778730869293, 'accumulated_logging_time': 0.4645524024963379}
I0609 01:22:15.657349 139681303873280 logging_writer.py:48] [24849] accumulated_eval_time=2270.477873, accumulated_logging_time=0.464552, accumulated_submission_time=9654.659735, global_step=24849, preemption_count=0, score=9654.659735, test/accuracy=0.402600, test/loss=2.959684, test/num_examples=10000, total_duration=11941.123113, train/accuracy=0.559961, train/loss=2.137795, validation/accuracy=0.510500, validation/loss=2.376891, validation/num_examples=50000
I0609 01:23:16.080027 139681295480576 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.607775, loss=4.610086
I0609 01:26:30.822339 139681303873280 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.595243, loss=4.424861
I0609 01:29:15.993720 139724602201920 spec.py:298] Evaluating on the training split.
I0609 01:30:00.395167 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 01:30:46.611236 139724602201920 spec.py:326] Evaluating on the test split.
I0609 01:30:48.025685 139724602201920 submission_runner.py:419] Time since start: 12453.50s, 	Step: 25930, 	{'train/accuracy': 0.57302734375, 'train/loss': 2.135884246826172, 'validation/accuracy': 0.52198, 'validation/loss': 2.3685675, 'validation/num_examples': 50000, 'test/accuracy': 0.411, 'test/loss': 2.94842890625, 'test/num_examples': 10000, 'score': 10074.313292980194, 'total_duration': 12453.502605676651, 'accumulated_submission_time': 10074.313292980194, 'accumulated_eval_time': 2362.509941339493, 'accumulated_logging_time': 0.4861016273498535}
I0609 01:30:48.037297 139681295480576 logging_writer.py:48] [25930] accumulated_eval_time=2362.509941, accumulated_logging_time=0.486102, accumulated_submission_time=10074.313293, global_step=25930, preemption_count=0, score=10074.313293, test/accuracy=0.411000, test/loss=2.948429, test/num_examples=10000, total_duration=12453.502606, train/accuracy=0.573027, train/loss=2.135884, validation/accuracy=0.521980, validation/loss=2.368568, validation/num_examples=50000
I0609 01:31:15.499816 139681303873280 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.600577, loss=4.275902
I0609 01:34:33.528331 139681295480576 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.584542, loss=4.506132
I0609 01:37:45.834486 139681303873280 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.597282, loss=4.572591
I0609 01:37:48.148129 139724602201920 spec.py:298] Evaluating on the training split.
I0609 01:38:32.907212 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 01:39:18.987817 139724602201920 spec.py:326] Evaluating on the test split.
I0609 01:39:20.404338 139724602201920 submission_runner.py:419] Time since start: 12965.88s, 	Step: 27007, 	{'train/accuracy': 0.5816015625, 'train/loss': 1.9909492492675782, 'validation/accuracy': 0.52978, 'validation/loss': 2.243266875, 'validation/num_examples': 50000, 'test/accuracy': 0.4154, 'test/loss': 2.8431671875, 'test/num_examples': 10000, 'score': 10493.741005659103, 'total_duration': 12965.881271839142, 'accumulated_submission_time': 10493.741005659103, 'accumulated_eval_time': 2454.766232728958, 'accumulated_logging_time': 0.5062129497528076}
I0609 01:39:20.416230 139681295480576 logging_writer.py:48] [27007] accumulated_eval_time=2454.766233, accumulated_logging_time=0.506213, accumulated_submission_time=10493.741006, global_step=27007, preemption_count=0, score=10493.741006, test/accuracy=0.415400, test/loss=2.843167, test/num_examples=10000, total_duration=12965.881272, train/accuracy=0.581602, train/loss=1.990949, validation/accuracy=0.529780, validation/loss=2.243267, validation/num_examples=50000
I0609 01:42:34.000406 139681303873280 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.615386, loss=4.463108
I0609 01:45:47.794728 139724602201920 spec.py:298] Evaluating on the training split.
I0609 01:46:32.584144 139724602201920 spec.py:310] Evaluating on the validation split.
I0609 01:47:19.008461 139724602201920 spec.py:326] Evaluating on the test split.
I0609 01:47:20.424284 139724602201920 submission_runner.py:419] Time since start: 13445.90s, 	Step: 28000, 	{'train/accuracy': 0.59193359375, 'train/loss': 1.9753105163574218, 'validation/accuracy': 0.53888, 'validation/loss': 2.22656, 'validation/num_examples': 50000, 'test/accuracy': 0.4173, 'test/loss': 2.8565974609375, 'test/num_examples': 10000, 'score': 10880.48675942421, 'total_duration': 13445.901243448257, 'accumulated_submission_time': 10880.48675942421, 'accumulated_eval_time': 2547.3960349559784, 'accumulated_logging_time': 0.5260634422302246}
I0609 01:47:20.435589 139681295480576 logging_writer.py:48] [28000] accumulated_eval_time=2547.396035, accumulated_logging_time=0.526063, accumulated_submission_time=10880.486759, global_step=28000, preemption_count=0, score=10880.486759, test/accuracy=0.417300, test/loss=2.856597, test/num_examples=10000, total_duration=13445.901243, train/accuracy=0.591934, train/loss=1.975311, validation/accuracy=0.538880, validation/loss=2.226560, validation/num_examples=50000
I0609 01:47:20.453935 139681303873280 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10880.486759
I0609 01:47:20.974873 139724602201920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_vit_pytorch/trial_1/checkpoint_28000.
I0609 01:47:21.249687 139724602201920 submission_runner.py:581] Tuning trial 1/1
I0609 01:47:21.249923 139724602201920 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0609 01:47:21.251112 139724602201920 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00109375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.512206315994263, 'total_duration': 131.7021460533142, 'accumulated_submission_time': 6.512206315994263, 'accumulated_eval_time': 125.18945837020874, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1049, {'train/accuracy': 0.04203125, 'train/loss': 5.972650756835938, 'validation/accuracy': 0.04124, 'validation/loss': 5.997063125, 'validation/num_examples': 50000, 'test/accuracy': 0.0309, 'test/loss': 6.111936328125, 'test/num_examples': 10000, 'score': 425.8988502025604, 'total_duration': 640.280957698822, 'accumulated_submission_time': 425.8988502025604, 'accumulated_eval_time': 213.73279643058777, 'accumulated_logging_time': 0.025358200073242188, 'global_step': 1049, 'preemption_count': 0}), (2130, {'train/accuracy': 0.080234375, 'train/loss': 5.459764404296875, 'validation/accuracy': 0.0748, 'validation/loss': 5.4979775, 'validation/num_examples': 50000, 'test/accuracy': 0.0558, 'test/loss': 5.67322421875, 'test/num_examples': 10000, 'score': 845.3234307765961, 'total_duration': 1154.379370212555, 'accumulated_submission_time': 845.3234307765961, 'accumulated_eval_time': 307.6998801231384, 'accumulated_logging_time': 0.04538440704345703, 'global_step': 2130, 'preemption_count': 0}), (3208, {'train/accuracy': 0.1105859375, 'train/loss': 5.091164245605468, 'validation/accuracy': 0.10134, 'validation/loss': 5.1486984375, 'validation/num_examples': 50000, 'test/accuracy': 0.0792, 'test/loss': 5.39533046875, 'test/num_examples': 10000, 'score': 1264.7535791397095, 'total_duration': 1674.8526811599731, 'accumulated_submission_time': 1264.7535791397095, 'accumulated_eval_time': 408.0441527366638, 'accumulated_logging_time': 0.06416440010070801, 'global_step': 3208, 'preemption_count': 0}), (4296, {'train/accuracy': 0.14220703125, 'train/loss': 4.746715087890625, 'validation/accuracy': 0.12986, 'validation/loss': 4.82099, 'validation/num_examples': 50000, 'test/accuracy': 0.1027, 'test/loss': 5.119153125, 'test/num_examples': 10000, 'score': 1684.136578321457, 'total_duration': 2185.6446311473846, 'accumulated_submission_time': 1684.136578321457, 'accumulated_eval_time': 498.7449474334717, 'accumulated_logging_time': 0.08309650421142578, 'global_step': 4296, 'preemption_count': 0}), (5377, {'train/accuracy': 0.17154296875, 'train/loss': 4.536736145019531, 'validation/accuracy': 0.15912, 'validation/loss': 4.6192675, 'validation/num_examples': 50000, 'test/accuracy': 0.1219, 'test/loss': 4.921911328125, 'test/num_examples': 10000, 'score': 2103.5309641361237, 'total_duration': 2697.472895383835, 'accumulated_submission_time': 2103.5309641361237, 'accumulated_eval_time': 590.4873900413513, 'accumulated_logging_time': 0.10241031646728516, 'global_step': 5377, 'preemption_count': 0}), (6458, {'train/accuracy': 0.21005859375, 'train/loss': 4.217542724609375, 'validation/accuracy': 0.19206, 'validation/loss': 4.3205046875, 'validation/num_examples': 50000, 'test/accuracy': 0.1471, 'test/loss': 4.681320703125, 'test/num_examples': 10000, 'score': 2522.990050792694, 'total_duration': 3208.3602385520935, 'accumulated_submission_time': 2522.990050792694, 'accumulated_eval_time': 681.2280774116516, 'accumulated_logging_time': 0.1212470531463623, 'global_step': 6458, 'preemption_count': 0}), (7537, {'train/accuracy': 0.24765625, 'train/loss': 3.930257568359375, 'validation/accuracy': 0.22796, 'validation/loss': 4.0331696875, 'validation/num_examples': 50000, 'test/accuracy': 0.1761, 'test/loss': 4.439977734375, 'test/num_examples': 10000, 'score': 2942.560126066208, 'total_duration': 3725.5140585899353, 'accumulated_submission_time': 2942.560126066208, 'accumulated_eval_time': 778.118017911911, 'accumulated_logging_time': 0.14264917373657227, 'global_step': 7537, 'preemption_count': 0}), (8629, {'train/accuracy': 0.278125, 'train/loss': 3.6975979614257812, 'validation/accuracy': 0.25704, 'validation/loss': 3.8172096875, 'validation/num_examples': 50000, 'test/accuracy': 0.198, 'test/loss': 4.245627734375, 'test/num_examples': 10000, 'score': 3362.21826839447, 'total_duration': 4239.368790149689, 'accumulated_submission_time': 3362.21826839447, 'accumulated_eval_time': 871.6083207130432, 'accumulated_logging_time': 0.16131043434143066, 'global_step': 8629, 'preemption_count': 0}), (9713, {'train/accuracy': 0.31453125, 'train/loss': 3.5063861083984373, 'validation/accuracy': 0.28666, 'validation/loss': 3.6373209375, 'validation/num_examples': 50000, 'test/accuracy': 0.2199, 'test/loss': 4.09714140625, 'test/num_examples': 10000, 'score': 3781.530790567398, 'total_duration': 4751.003882646561, 'accumulated_submission_time': 3781.530790567398, 'accumulated_eval_time': 963.2212760448456, 'accumulated_logging_time': 0.18123579025268555, 'global_step': 9713, 'preemption_count': 0}), (10795, {'train/accuracy': 0.345703125, 'train/loss': 3.34357177734375, 'validation/accuracy': 0.31688, 'validation/loss': 3.487991875, 'validation/num_examples': 50000, 'test/accuracy': 0.2481, 'test/loss': 3.9675671875, 'test/num_examples': 10000, 'score': 4201.108502864838, 'total_duration': 5263.470956802368, 'accumulated_submission_time': 4201.108502864838, 'accumulated_eval_time': 1055.410625219345, 'accumulated_logging_time': 0.20115423202514648, 'global_step': 10795, 'preemption_count': 0}), (11874, {'train/accuracy': 0.3717578125, 'train/loss': 3.096274108886719, 'validation/accuracy': 0.33978, 'validation/loss': 3.2589484375, 'validation/num_examples': 50000, 'test/accuracy': 0.263, 'test/loss': 3.791540234375, 'test/num_examples': 10000, 'score': 4620.604208469391, 'total_duration': 5775.848053693771, 'accumulated_submission_time': 4620.604208469391, 'accumulated_eval_time': 1147.5943143367767, 'accumulated_logging_time': 0.22326278686523438, 'global_step': 11874, 'preemption_count': 0}), (12952, {'train/accuracy': 0.3945703125, 'train/loss': 3.005664978027344, 'validation/accuracy': 0.3602, 'validation/loss': 3.162241875, 'validation/num_examples': 50000, 'test/accuracy': 0.2767, 'test/loss': 3.705543359375, 'test/num_examples': 10000, 'score': 5040.032082080841, 'total_duration': 6288.213706970215, 'accumulated_submission_time': 5040.032082080841, 'accumulated_eval_time': 1239.8454813957214, 'accumulated_logging_time': 0.2423415184020996, 'global_step': 12952, 'preemption_count': 0}), (14035, {'train/accuracy': 0.413125, 'train/loss': 2.941761169433594, 'validation/accuracy': 0.37964, 'validation/loss': 3.11401625, 'validation/num_examples': 50000, 'test/accuracy': 0.2922, 'test/loss': 3.635890625, 'test/num_examples': 10000, 'score': 5459.646435499191, 'total_duration': 6800.825165987015, 'accumulated_submission_time': 5459.646435499191, 'accumulated_eval_time': 1332.1400785446167, 'accumulated_logging_time': 0.26314258575439453, 'global_step': 14035, 'preemption_count': 0}), (15114, {'train/accuracy': 0.4309375, 'train/loss': 2.8474212646484376, 'validation/accuracy': 0.39552, 'validation/loss': 3.02208875, 'validation/num_examples': 50000, 'test/accuracy': 0.302, 'test/loss': 3.55537265625, 'test/num_examples': 10000, 'score': 5878.971431016922, 'total_duration': 7312.924280405045, 'accumulated_submission_time': 5878.971431016922, 'accumulated_eval_time': 1424.2151110172272, 'accumulated_logging_time': 0.2823631763458252, 'global_step': 15114, 'preemption_count': 0}), (16202, {'train/accuracy': 0.45552734375, 'train/loss': 2.6575418090820313, 'validation/accuracy': 0.42118, 'validation/loss': 2.8402753125, 'validation/num_examples': 50000, 'test/accuracy': 0.3219, 'test/loss': 3.404101171875, 'test/num_examples': 10000, 'score': 6298.574860572815, 'total_duration': 7835.173810005188, 'accumulated_submission_time': 6298.574860572815, 'accumulated_eval_time': 1526.1553792953491, 'accumulated_logging_time': 0.3031024932861328, 'global_step': 16202, 'preemption_count': 0}), (17287, {'train/accuracy': 0.47265625, 'train/loss': 2.5531298828125, 'validation/accuracy': 0.43288, 'validation/loss': 2.748838125, 'validation/num_examples': 50000, 'test/accuracy': 0.3318, 'test/loss': 3.327375390625, 'test/num_examples': 10000, 'score': 6718.074041604996, 'total_duration': 8348.135802984238, 'accumulated_submission_time': 6718.074041604996, 'accumulated_eval_time': 1618.917900800705, 'accumulated_logging_time': 0.32452893257141113, 'global_step': 17287, 'preemption_count': 0}), (18366, {'train/accuracy': 0.48986328125, 'train/loss': 2.4748135375976563, 'validation/accuracy': 0.44892, 'validation/loss': 2.6802884375, 'validation/num_examples': 50000, 'test/accuracy': 0.3485, 'test/loss': 3.27262890625, 'test/num_examples': 10000, 'score': 7137.387549161911, 'total_duration': 8860.145043373108, 'accumulated_submission_time': 7137.387549161911, 'accumulated_eval_time': 1710.9167017936707, 'accumulated_logging_time': 0.3446993827819824, 'global_step': 18366, 'preemption_count': 0}), (19446, {'train/accuracy': 0.50876953125, 'train/loss': 2.4081268310546875, 'validation/accuracy': 0.46108, 'validation/loss': 2.6232475, 'validation/num_examples': 50000, 'test/accuracy': 0.3579, 'test/loss': 3.2109384765625, 'test/num_examples': 10000, 'score': 7556.97265124321, 'total_duration': 9372.75537443161, 'accumulated_submission_time': 7556.97265124321, 'accumulated_eval_time': 1803.2517108917236, 'accumulated_logging_time': 0.36464405059814453, 'global_step': 19446, 'preemption_count': 0}), (20526, {'train/accuracy': 0.51154296875, 'train/loss': 2.3534507751464844, 'validation/accuracy': 0.46686, 'validation/loss': 2.568821875, 'validation/num_examples': 50000, 'test/accuracy': 0.3702, 'test/loss': 3.1335873046875, 'test/num_examples': 10000, 'score': 7976.460025548935, 'total_duration': 9885.676082134247, 'accumulated_submission_time': 7976.460025548935, 'accumulated_eval_time': 1895.9947192668915, 'accumulated_logging_time': 0.38517093658447266, 'global_step': 20526, 'preemption_count': 0}), (21603, {'train/accuracy': 0.5262109375, 'train/loss': 2.299521484375, 'validation/accuracy': 0.48026, 'validation/loss': 2.5188703125, 'validation/num_examples': 50000, 'test/accuracy': 0.3742, 'test/loss': 3.095450390625, 'test/num_examples': 10000, 'score': 8396.01089143753, 'total_duration': 10398.891847610474, 'accumulated_submission_time': 8396.01089143753, 'accumulated_eval_time': 1988.9704315662384, 'accumulated_logging_time': 0.4051327705383301, 'global_step': 21603, 'preemption_count': 0}), (22682, {'train/accuracy': 0.54404296875, 'train/loss': 2.204906768798828, 'validation/accuracy': 0.49798, 'validation/loss': 2.43374421875, 'validation/num_examples': 50000, 'test/accuracy': 0.3858, 'test/loss': 3.033079296875, 'test/num_examples': 10000, 'score': 8815.653045415878, 'total_duration': 10911.822468996048, 'accumulated_submission_time': 8815.653045415878, 'accumulated_eval_time': 2081.5736813545227, 'accumulated_logging_time': 0.4238708019256592, 'global_step': 22682, 'preemption_count': 0}), (23765, {'train/accuracy': 0.55125, 'train/loss': 2.1547647094726563, 'validation/accuracy': 0.50358, 'validation/loss': 2.38638515625, 'validation/num_examples': 50000, 'test/accuracy': 0.3991, 'test/loss': 2.9879146484375, 'test/num_examples': 10000, 'score': 9234.983407020569, 'total_duration': 11427.093763589859, 'accumulated_submission_time': 9234.983407020569, 'accumulated_eval_time': 2176.824099302292, 'accumulated_logging_time': 0.4445512294769287, 'global_step': 23765, 'preemption_count': 0}), (24849, {'train/accuracy': 0.5599609375, 'train/loss': 2.13779541015625, 'validation/accuracy': 0.5105, 'validation/loss': 2.37689078125, 'validation/num_examples': 50000, 'test/accuracy': 0.4026, 'test/loss': 2.959683984375, 'test/num_examples': 10000, 'score': 9654.659735441208, 'total_duration': 11941.123112678528, 'accumulated_submission_time': 9654.659735441208, 'accumulated_eval_time': 2270.4778730869293, 'accumulated_logging_time': 0.4645524024963379, 'global_step': 24849, 'preemption_count': 0}), (25930, {'train/accuracy': 0.57302734375, 'train/loss': 2.135884246826172, 'validation/accuracy': 0.52198, 'validation/loss': 2.3685675, 'validation/num_examples': 50000, 'test/accuracy': 0.411, 'test/loss': 2.94842890625, 'test/num_examples': 10000, 'score': 10074.313292980194, 'total_duration': 12453.502605676651, 'accumulated_submission_time': 10074.313292980194, 'accumulated_eval_time': 2362.509941339493, 'accumulated_logging_time': 0.4861016273498535, 'global_step': 25930, 'preemption_count': 0}), (27007, {'train/accuracy': 0.5816015625, 'train/loss': 1.9909492492675782, 'validation/accuracy': 0.52978, 'validation/loss': 2.243266875, 'validation/num_examples': 50000, 'test/accuracy': 0.4154, 'test/loss': 2.8431671875, 'test/num_examples': 10000, 'score': 10493.741005659103, 'total_duration': 12965.881271839142, 'accumulated_submission_time': 10493.741005659103, 'accumulated_eval_time': 2454.766232728958, 'accumulated_logging_time': 0.5062129497528076, 'global_step': 27007, 'preemption_count': 0}), (28000, {'train/accuracy': 0.59193359375, 'train/loss': 1.9753105163574218, 'validation/accuracy': 0.53888, 'validation/loss': 2.22656, 'validation/num_examples': 50000, 'test/accuracy': 0.4173, 'test/loss': 2.8565974609375, 'test/num_examples': 10000, 'score': 10880.48675942421, 'total_duration': 13445.901243448257, 'accumulated_submission_time': 10880.48675942421, 'accumulated_eval_time': 2547.3960349559784, 'accumulated_logging_time': 0.5260634422302246, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0609 01:47:21.251248 139724602201920 submission_runner.py:584] Timing: 10880.48675942421
I0609 01:47:21.251307 139724602201920 submission_runner.py:586] Total number of evals: 27
I0609 01:47:21.251359 139724602201920 submission_runner.py:587] ====================
I0609 01:47:21.251509 139724602201920 submission_runner.py:655] Final imagenet_vit score: 10880.48675942421
