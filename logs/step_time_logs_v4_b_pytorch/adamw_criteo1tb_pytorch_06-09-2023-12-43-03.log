torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/adamw --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_pytorch_06-09-2023-12-43-03.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 12:43:25.973137 140011964507968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 12:43:25.973159 140546433648448 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 12:43:25.973180 140626054235968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 12:43:25.973205 139961314998080 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 12:43:25.973910 140667674560320 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 12:43:25.973986 140529002223424 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 12:43:25.974026 139840136402752 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 12:43:25.974091 140496353691456 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 12:43:25.974250 140667674560320 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:43:25.974293 140529002223424 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:43:25.974354 139840136402752 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:43:25.974406 140496353691456 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:43:25.983916 140546433648448 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:43:25.983939 140626054235968 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:43:25.983962 139961314998080 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:43:25.983995 140011964507968 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:43:25.991922 140496353691456 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/adamw/criteo1tb_pytorch.
W0609 12:43:26.031899 140667674560320 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:43:26.032003 140546433648448 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:43:26.032169 140529002223424 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:43:26.032475 140626054235968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:43:26.033289 140496353691456 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:43:26.034323 139840136402752 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:43:26.034462 140011964507968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:43:26.034578 139961314998080 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 12:43:26.038490 140496353691456 submission_runner.py:541] Using RNG seed 3473824156
I0609 12:43:26.040191 140496353691456 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 12:43:26.040309 140496353691456 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/adamw/criteo1tb_pytorch/trial_1.
I0609 12:43:26.040871 140496353691456 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/adamw/criteo1tb_pytorch/trial_1/hparams.json.
I0609 12:43:26.042896 140496353691456 submission_runner.py:255] Initializing dataset.
I0609 12:43:26.043022 140496353691456 submission_runner.py:262] Initializing model.
I0609 12:43:39.330192 140496353691456 submission_runner.py:272] Initializing optimizer.
I0609 12:43:39.330868 140496353691456 submission_runner.py:279] Initializing metrics bundle.
I0609 12:43:39.331007 140496353691456 submission_runner.py:297] Initializing checkpoint and logger.
I0609 12:43:39.333869 140496353691456 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 12:43:39.334002 140496353691456 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 12:43:39.787564 140496353691456 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/adamw/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0609 12:43:39.788540 140496353691456 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/adamw/criteo1tb_pytorch/trial_1/flags_0.json.
I0609 12:43:39.837739 140496353691456 submission_runner.py:332] Starting training loop.
I0609 12:43:45.611222 140457858295552 logging_writer.py:48] [0] global_step=0, grad_norm=9.584973, loss=1.513207
I0609 12:43:45.618558 140496353691456 submission.py:120] 0) loss = 1.513, grad_norm = 9.585
I0609 12:43:45.721145 140496353691456 spec.py:298] Evaluating on the training split.
I0609 12:48:30.169846 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 12:53:15.269584 140496353691456 spec.py:326] Evaluating on the test split.
I0609 12:57:58.346837 140496353691456 submission_runner.py:419] Time since start: 858.51s, 	Step: 1, 	{'train/loss': 1.5079770568892568, 'validation/loss': 1.5135611685393258, 'validation/num_examples': 89000000, 'test/loss': 1.5090419017889707, 'test/num_examples': 89274637, 'score': 5.883362531661987, 'total_duration': 858.5095055103302, 'accumulated_submission_time': 5.883362531661987, 'accumulated_eval_time': 852.6257433891296, 'accumulated_logging_time': 0}
I0609 12:57:58.363468 140429995538176 logging_writer.py:48] [1] accumulated_eval_time=852.625743, accumulated_logging_time=0, accumulated_submission_time=5.883363, global_step=1, preemption_count=0, score=5.883363, test/loss=1.509042, test/num_examples=89274637, total_duration=858.509506, train/loss=1.507977, validation/loss=1.513561, validation/num_examples=89000000
I0609 12:57:58.388056 140496353691456 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:57:58.388097 139840136402752 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:57:58.388098 140546433648448 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:57:58.388107 140667674560320 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:57:58.388108 140626054235968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:57:58.388118 140011964507968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:57:58.388103 140529002223424 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:57:58.388111 139961314998080 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:57:59.563706 140429987145472 logging_writer.py:48] [1] global_step=1, grad_norm=9.563947, loss=1.512710
I0609 12:57:59.567015 140496353691456 submission.py:120] 1) loss = 1.513, grad_norm = 9.564
I0609 12:58:00.714525 140429995538176 logging_writer.py:48] [2] global_step=2, grad_norm=9.535508, loss=1.498512
I0609 12:58:00.718001 140496353691456 submission.py:120] 2) loss = 1.499, grad_norm = 9.536
I0609 12:58:01.857377 140429987145472 logging_writer.py:48] [3] global_step=3, grad_norm=9.506567, loss=1.469762
I0609 12:58:01.861184 140496353691456 submission.py:120] 3) loss = 1.470, grad_norm = 9.507
I0609 12:58:03.075264 140429995538176 logging_writer.py:48] [4] global_step=4, grad_norm=9.424909, loss=1.428619
I0609 12:58:03.079929 140496353691456 submission.py:120] 4) loss = 1.429, grad_norm = 9.425
I0609 12:58:04.224617 140429987145472 logging_writer.py:48] [5] global_step=5, grad_norm=9.291097, loss=1.374027
I0609 12:58:04.228128 140496353691456 submission.py:120] 5) loss = 1.374, grad_norm = 9.291
I0609 12:58:05.386140 140429995538176 logging_writer.py:48] [6] global_step=6, grad_norm=9.040626, loss=1.306450
I0609 12:58:05.389696 140496353691456 submission.py:120] 6) loss = 1.306, grad_norm = 9.041
I0609 12:58:06.520686 140429987145472 logging_writer.py:48] [7] global_step=7, grad_norm=8.666254, loss=1.229497
I0609 12:58:06.523950 140496353691456 submission.py:120] 7) loss = 1.229, grad_norm = 8.666
I0609 12:58:07.688293 140429995538176 logging_writer.py:48] [8] global_step=8, grad_norm=8.429223, loss=1.143035
I0609 12:58:07.691568 140496353691456 submission.py:120] 8) loss = 1.143, grad_norm = 8.429
I0609 12:58:08.823163 140429987145472 logging_writer.py:48] [9] global_step=9, grad_norm=8.211390, loss=1.047378
I0609 12:58:08.826498 140496353691456 submission.py:120] 9) loss = 1.047, grad_norm = 8.211
I0609 12:58:10.001218 140429995538176 logging_writer.py:48] [10] global_step=10, grad_norm=7.999858, loss=0.941516
I0609 12:58:10.004768 140496353691456 submission.py:120] 10) loss = 0.942, grad_norm = 8.000
I0609 12:58:11.155001 140429987145472 logging_writer.py:48] [11] global_step=11, grad_norm=7.675236, loss=0.829137
I0609 12:58:11.158338 140496353691456 submission.py:120] 11) loss = 0.829, grad_norm = 7.675
I0609 12:58:12.304816 140429995538176 logging_writer.py:48] [12] global_step=12, grad_norm=7.160566, loss=0.714461
I0609 12:58:12.308211 140496353691456 submission.py:120] 12) loss = 0.714, grad_norm = 7.161
I0609 12:58:13.445930 140429987145472 logging_writer.py:48] [13] global_step=13, grad_norm=6.405262, loss=0.598487
I0609 12:58:13.449364 140496353691456 submission.py:120] 13) loss = 0.598, grad_norm = 6.405
I0609 12:58:14.621219 140429995538176 logging_writer.py:48] [14] global_step=14, grad_norm=5.468715, loss=0.494579
I0609 12:58:14.624564 140496353691456 submission.py:120] 14) loss = 0.495, grad_norm = 5.469
I0609 12:58:15.783226 140429987145472 logging_writer.py:48] [15] global_step=15, grad_norm=4.486795, loss=0.402825
I0609 12:58:15.786671 140496353691456 submission.py:120] 15) loss = 0.403, grad_norm = 4.487
I0609 12:58:16.956458 140429995538176 logging_writer.py:48] [16] global_step=16, grad_norm=3.488115, loss=0.327612
I0609 12:58:16.959825 140496353691456 submission.py:120] 16) loss = 0.328, grad_norm = 3.488
I0609 12:58:18.090816 140429987145472 logging_writer.py:48] [17] global_step=17, grad_norm=2.512717, loss=0.272118
I0609 12:58:18.094448 140496353691456 submission.py:120] 17) loss = 0.272, grad_norm = 2.513
I0609 12:58:19.234779 140429995538176 logging_writer.py:48] [18] global_step=18, grad_norm=1.719775, loss=0.232482
I0609 12:58:19.238419 140496353691456 submission.py:120] 18) loss = 0.232, grad_norm = 1.720
I0609 12:58:20.394456 140429987145472 logging_writer.py:48] [19] global_step=19, grad_norm=0.816992, loss=0.212740
I0609 12:58:20.398232 140496353691456 submission.py:120] 19) loss = 0.213, grad_norm = 0.817
I0609 12:58:21.546728 140429995538176 logging_writer.py:48] [20] global_step=20, grad_norm=0.339180, loss=0.207087
I0609 12:58:21.551232 140496353691456 submission.py:120] 20) loss = 0.207, grad_norm = 0.339
I0609 12:58:22.723381 140429987145472 logging_writer.py:48] [21] global_step=21, grad_norm=0.301566, loss=0.203043
I0609 12:58:22.726974 140496353691456 submission.py:120] 21) loss = 0.203, grad_norm = 0.302
I0609 12:58:23.894683 140429995538176 logging_writer.py:48] [22] global_step=22, grad_norm=0.463346, loss=0.201983
I0609 12:58:23.897991 140496353691456 submission.py:120] 22) loss = 0.202, grad_norm = 0.463
I0609 12:58:25.038491 140429987145472 logging_writer.py:48] [23] global_step=23, grad_norm=0.650327, loss=0.206799
I0609 12:58:25.042278 140496353691456 submission.py:120] 23) loss = 0.207, grad_norm = 0.650
I0609 12:58:26.178210 140429995538176 logging_writer.py:48] [24] global_step=24, grad_norm=0.724453, loss=0.204640
I0609 12:58:26.181492 140496353691456 submission.py:120] 24) loss = 0.205, grad_norm = 0.724
I0609 12:58:27.307902 140429987145472 logging_writer.py:48] [25] global_step=25, grad_norm=0.765311, loss=0.204604
I0609 12:58:27.311254 140496353691456 submission.py:120] 25) loss = 0.205, grad_norm = 0.765
I0609 12:58:28.444851 140429995538176 logging_writer.py:48] [26] global_step=26, grad_norm=0.801071, loss=0.207538
I0609 12:58:28.448162 140496353691456 submission.py:120] 26) loss = 0.208, grad_norm = 0.801
I0609 12:58:29.617831 140429987145472 logging_writer.py:48] [27] global_step=27, grad_norm=0.734968, loss=0.200698
I0609 12:58:29.621159 140496353691456 submission.py:120] 27) loss = 0.201, grad_norm = 0.735
I0609 12:58:30.756651 140429995538176 logging_writer.py:48] [28] global_step=28, grad_norm=0.675823, loss=0.195936
I0609 12:58:30.759834 140496353691456 submission.py:120] 28) loss = 0.196, grad_norm = 0.676
I0609 12:58:31.897431 140429987145472 logging_writer.py:48] [29] global_step=29, grad_norm=0.633974, loss=0.193945
I0609 12:58:31.900694 140496353691456 submission.py:120] 29) loss = 0.194, grad_norm = 0.634
I0609 12:58:33.060054 140429995538176 logging_writer.py:48] [30] global_step=30, grad_norm=0.547309, loss=0.191593
I0609 12:58:33.063452 140496353691456 submission.py:120] 30) loss = 0.192, grad_norm = 0.547
I0609 12:58:34.221798 140429987145472 logging_writer.py:48] [31] global_step=31, grad_norm=0.450668, loss=0.186765
I0609 12:58:34.224890 140496353691456 submission.py:120] 31) loss = 0.187, grad_norm = 0.451
I0609 12:58:35.358543 140429995538176 logging_writer.py:48] [32] global_step=32, grad_norm=0.337664, loss=0.180728
I0609 12:58:35.362144 140496353691456 submission.py:120] 32) loss = 0.181, grad_norm = 0.338
I0609 12:58:36.495088 140429987145472 logging_writer.py:48] [33] global_step=33, grad_norm=0.293319, loss=0.181046
I0609 12:58:36.498780 140496353691456 submission.py:120] 33) loss = 0.181, grad_norm = 0.293
I0609 12:58:37.636208 140429995538176 logging_writer.py:48] [34] global_step=34, grad_norm=0.246059, loss=0.180086
I0609 12:58:37.640115 140496353691456 submission.py:120] 34) loss = 0.180, grad_norm = 0.246
I0609 12:58:38.776544 140429987145472 logging_writer.py:48] [35] global_step=35, grad_norm=0.196358, loss=0.175680
I0609 12:58:38.779857 140496353691456 submission.py:120] 35) loss = 0.176, grad_norm = 0.196
I0609 12:58:39.911457 140429995538176 logging_writer.py:48] [36] global_step=36, grad_norm=0.177088, loss=0.172504
I0609 12:58:39.914644 140496353691456 submission.py:120] 36) loss = 0.173, grad_norm = 0.177
I0609 12:58:41.073975 140429987145472 logging_writer.py:48] [37] global_step=37, grad_norm=0.174058, loss=0.170722
I0609 12:58:41.077241 140496353691456 submission.py:120] 37) loss = 0.171, grad_norm = 0.174
I0609 12:58:42.247044 140429995538176 logging_writer.py:48] [38] global_step=38, grad_norm=0.189573, loss=0.154920
I0609 12:58:42.250248 140496353691456 submission.py:120] 38) loss = 0.155, grad_norm = 0.190
I0609 12:58:43.381005 140429987145472 logging_writer.py:48] [39] global_step=39, grad_norm=0.137026, loss=0.153922
I0609 12:58:43.384251 140496353691456 submission.py:120] 39) loss = 0.154, grad_norm = 0.137
I0609 12:58:44.512326 140429995538176 logging_writer.py:48] [40] global_step=40, grad_norm=0.217951, loss=0.152251
I0609 12:58:44.516258 140496353691456 submission.py:120] 40) loss = 0.152, grad_norm = 0.218
I0609 12:58:45.645247 140429987145472 logging_writer.py:48] [41] global_step=41, grad_norm=0.210276, loss=0.149681
I0609 12:58:45.649120 140496353691456 submission.py:120] 41) loss = 0.150, grad_norm = 0.210
I0609 12:58:46.792760 140429995538176 logging_writer.py:48] [42] global_step=42, grad_norm=0.150860, loss=0.150845
I0609 12:58:46.795976 140496353691456 submission.py:120] 42) loss = 0.151, grad_norm = 0.151
I0609 12:58:47.931664 140429987145472 logging_writer.py:48] [43] global_step=43, grad_norm=0.141886, loss=0.148270
I0609 12:58:47.934840 140496353691456 submission.py:120] 43) loss = 0.148, grad_norm = 0.142
I0609 12:58:49.073902 140429995538176 logging_writer.py:48] [44] global_step=44, grad_norm=0.119899, loss=0.149133
I0609 12:58:49.077195 140496353691456 submission.py:120] 44) loss = 0.149, grad_norm = 0.120
I0609 12:58:50.225755 140429987145472 logging_writer.py:48] [45] global_step=45, grad_norm=0.156763, loss=0.146513
I0609 12:58:50.229076 140496353691456 submission.py:120] 45) loss = 0.147, grad_norm = 0.157
I0609 12:58:51.390493 140429995538176 logging_writer.py:48] [46] global_step=46, grad_norm=0.097794, loss=0.142202
I0609 12:58:51.393802 140496353691456 submission.py:120] 46) loss = 0.142, grad_norm = 0.098
I0609 12:58:52.551395 140429987145472 logging_writer.py:48] [47] global_step=47, grad_norm=0.095797, loss=0.143736
I0609 12:58:52.554719 140496353691456 submission.py:120] 47) loss = 0.144, grad_norm = 0.096
I0609 12:58:53.687045 140429995538176 logging_writer.py:48] [48] global_step=48, grad_norm=0.073218, loss=0.138924
I0609 12:58:53.690303 140496353691456 submission.py:120] 48) loss = 0.139, grad_norm = 0.073
I0609 12:58:54.829137 140429987145472 logging_writer.py:48] [49] global_step=49, grad_norm=0.129346, loss=0.138539
I0609 12:58:54.832616 140496353691456 submission.py:120] 49) loss = 0.139, grad_norm = 0.129
I0609 12:58:55.967298 140429995538176 logging_writer.py:48] [50] global_step=50, grad_norm=0.065610, loss=0.139305
I0609 12:58:55.970971 140496353691456 submission.py:120] 50) loss = 0.139, grad_norm = 0.066
I0609 12:58:57.124152 140429987145472 logging_writer.py:48] [51] global_step=51, grad_norm=0.047183, loss=0.135234
I0609 12:58:57.127440 140496353691456 submission.py:120] 51) loss = 0.135, grad_norm = 0.047
I0609 12:58:58.261883 140429995538176 logging_writer.py:48] [52] global_step=52, grad_norm=0.198535, loss=0.137033
I0609 12:58:58.265244 140496353691456 submission.py:120] 52) loss = 0.137, grad_norm = 0.199
I0609 12:58:59.400366 140429987145472 logging_writer.py:48] [53] global_step=53, grad_norm=0.438944, loss=0.138333
I0609 12:58:59.404451 140496353691456 submission.py:120] 53) loss = 0.138, grad_norm = 0.439
I0609 12:59:00.541440 140429995538176 logging_writer.py:48] [54] global_step=54, grad_norm=0.549216, loss=0.137681
I0609 12:59:00.545515 140496353691456 submission.py:120] 54) loss = 0.138, grad_norm = 0.549
I0609 12:59:01.720048 140429987145472 logging_writer.py:48] [55] global_step=55, grad_norm=0.257429, loss=0.136355
I0609 12:59:01.724048 140496353691456 submission.py:120] 55) loss = 0.136, grad_norm = 0.257
I0609 12:59:02.876405 140429995538176 logging_writer.py:48] [56] global_step=56, grad_norm=0.204927, loss=0.135716
I0609 12:59:02.879983 140496353691456 submission.py:120] 56) loss = 0.136, grad_norm = 0.205
I0609 12:59:04.017453 140429987145472 logging_writer.py:48] [57] global_step=57, grad_norm=0.465073, loss=0.146272
I0609 12:59:04.020948 140496353691456 submission.py:120] 57) loss = 0.146, grad_norm = 0.465
I0609 12:59:05.172162 140429995538176 logging_writer.py:48] [58] global_step=58, grad_norm=0.270266, loss=0.147997
I0609 12:59:05.176242 140496353691456 submission.py:120] 58) loss = 0.148, grad_norm = 0.270
I0609 12:59:06.309536 140429987145472 logging_writer.py:48] [59] global_step=59, grad_norm=0.178414, loss=0.147790
I0609 12:59:06.312855 140496353691456 submission.py:120] 59) loss = 0.148, grad_norm = 0.178
I0609 12:59:07.441452 140429995538176 logging_writer.py:48] [60] global_step=60, grad_norm=0.363601, loss=0.149563
I0609 12:59:07.444923 140496353691456 submission.py:120] 60) loss = 0.150, grad_norm = 0.364
I0609 12:59:08.572818 140429987145472 logging_writer.py:48] [61] global_step=61, grad_norm=0.154064, loss=0.146797
I0609 12:59:08.576146 140496353691456 submission.py:120] 61) loss = 0.147, grad_norm = 0.154
I0609 12:59:09.711136 140429995538176 logging_writer.py:48] [62] global_step=62, grad_norm=0.174253, loss=0.144677
I0609 12:59:09.714644 140496353691456 submission.py:120] 62) loss = 0.145, grad_norm = 0.174
I0609 12:59:10.856464 140429987145472 logging_writer.py:48] [63] global_step=63, grad_norm=0.298194, loss=0.145015
I0609 12:59:10.860065 140496353691456 submission.py:120] 63) loss = 0.145, grad_norm = 0.298
I0609 12:59:11.996305 140429995538176 logging_writer.py:48] [64] global_step=64, grad_norm=0.107353, loss=0.146081
I0609 12:59:11.999879 140496353691456 submission.py:120] 64) loss = 0.146, grad_norm = 0.107
I0609 12:59:13.135144 140429987145472 logging_writer.py:48] [65] global_step=65, grad_norm=0.177160, loss=0.143263
I0609 12:59:13.138826 140496353691456 submission.py:120] 65) loss = 0.143, grad_norm = 0.177
I0609 12:59:14.287656 140429995538176 logging_writer.py:48] [66] global_step=66, grad_norm=0.325611, loss=0.144508
I0609 12:59:14.290962 140496353691456 submission.py:120] 66) loss = 0.145, grad_norm = 0.326
I0609 12:59:15.445093 140429987145472 logging_writer.py:48] [67] global_step=67, grad_norm=0.277420, loss=0.141855
I0609 12:59:15.448598 140496353691456 submission.py:120] 67) loss = 0.142, grad_norm = 0.277
I0609 12:59:16.584560 140429995538176 logging_writer.py:48] [68] global_step=68, grad_norm=0.129006, loss=0.141776
I0609 12:59:16.587726 140496353691456 submission.py:120] 68) loss = 0.142, grad_norm = 0.129
I0609 12:59:17.725016 140429987145472 logging_writer.py:48] [69] global_step=69, grad_norm=0.116821, loss=0.145303
I0609 12:59:17.728370 140496353691456 submission.py:120] 69) loss = 0.145, grad_norm = 0.117
I0609 12:59:18.883614 140429995538176 logging_writer.py:48] [70] global_step=70, grad_norm=0.404804, loss=0.140655
I0609 12:59:18.887120 140496353691456 submission.py:120] 70) loss = 0.141, grad_norm = 0.405
I0609 12:59:20.024516 140429987145472 logging_writer.py:48] [71] global_step=71, grad_norm=0.569548, loss=0.143117
I0609 12:59:20.027775 140496353691456 submission.py:120] 71) loss = 0.143, grad_norm = 0.570
I0609 12:59:21.162013 140429995538176 logging_writer.py:48] [72] global_step=72, grad_norm=0.420815, loss=0.140694
I0609 12:59:21.165473 140496353691456 submission.py:120] 72) loss = 0.141, grad_norm = 0.421
I0609 12:59:22.321901 140429987145472 logging_writer.py:48] [73] global_step=73, grad_norm=0.076825, loss=0.138123
I0609 12:59:22.325271 140496353691456 submission.py:120] 73) loss = 0.138, grad_norm = 0.077
I0609 12:59:23.473158 140429995538176 logging_writer.py:48] [74] global_step=74, grad_norm=0.296039, loss=0.141822
I0609 12:59:23.477016 140496353691456 submission.py:120] 74) loss = 0.142, grad_norm = 0.296
I0609 12:59:24.615168 140429987145472 logging_writer.py:48] [75] global_step=75, grad_norm=0.540664, loss=0.141655
I0609 12:59:24.618914 140496353691456 submission.py:120] 75) loss = 0.142, grad_norm = 0.541
I0609 12:59:25.755157 140429995538176 logging_writer.py:48] [76] global_step=76, grad_norm=0.418356, loss=0.134542
I0609 12:59:25.758511 140496353691456 submission.py:120] 76) loss = 0.135, grad_norm = 0.418
I0609 12:59:26.891514 140429987145472 logging_writer.py:48] [77] global_step=77, grad_norm=0.025551, loss=0.130645
I0609 12:59:26.894787 140496353691456 submission.py:120] 77) loss = 0.131, grad_norm = 0.026
I0609 12:59:28.032564 140429995538176 logging_writer.py:48] [78] global_step=78, grad_norm=0.282253, loss=0.131086
I0609 12:59:28.035987 140496353691456 submission.py:120] 78) loss = 0.131, grad_norm = 0.282
I0609 12:59:29.167922 140429987145472 logging_writer.py:48] [79] global_step=79, grad_norm=0.437331, loss=0.132101
I0609 12:59:29.171401 140496353691456 submission.py:120] 79) loss = 0.132, grad_norm = 0.437
I0609 12:59:30.320361 140429995538176 logging_writer.py:48] [80] global_step=80, grad_norm=0.440035, loss=0.131935
I0609 12:59:30.323646 140496353691456 submission.py:120] 80) loss = 0.132, grad_norm = 0.440
I0609 12:59:31.452911 140429987145472 logging_writer.py:48] [81] global_step=81, grad_norm=0.255295, loss=0.130374
I0609 12:59:31.456313 140496353691456 submission.py:120] 81) loss = 0.130, grad_norm = 0.255
I0609 12:59:32.590462 140429995538176 logging_writer.py:48] [82] global_step=82, grad_norm=0.016425, loss=0.128239
I0609 12:59:32.593875 140496353691456 submission.py:120] 82) loss = 0.128, grad_norm = 0.016
I0609 12:59:33.725149 140429987145472 logging_writer.py:48] [83] global_step=83, grad_norm=0.122279, loss=0.131522
I0609 12:59:33.728545 140496353691456 submission.py:120] 83) loss = 0.132, grad_norm = 0.122
I0609 12:59:34.893683 140429995538176 logging_writer.py:48] [84] global_step=84, grad_norm=0.066268, loss=0.128256
I0609 12:59:34.897083 140496353691456 submission.py:120] 84) loss = 0.128, grad_norm = 0.066
I0609 12:59:36.024302 140429987145472 logging_writer.py:48] [85] global_step=85, grad_norm=0.057562, loss=0.128173
I0609 12:59:36.027702 140496353691456 submission.py:120] 85) loss = 0.128, grad_norm = 0.058
I0609 12:59:37.157896 140429995538176 logging_writer.py:48] [86] global_step=86, grad_norm=0.097678, loss=0.129959
I0609 12:59:37.161225 140496353691456 submission.py:120] 86) loss = 0.130, grad_norm = 0.098
I0609 12:59:38.293917 140429987145472 logging_writer.py:48] [87] global_step=87, grad_norm=0.061383, loss=0.127199
I0609 12:59:38.297423 140496353691456 submission.py:120] 87) loss = 0.127, grad_norm = 0.061
I0609 12:59:39.435634 140429995538176 logging_writer.py:48] [88] global_step=88, grad_norm=0.040428, loss=0.129528
I0609 12:59:39.439015 140496353691456 submission.py:120] 88) loss = 0.130, grad_norm = 0.040
I0609 12:59:40.635928 140429987145472 logging_writer.py:48] [89] global_step=89, grad_norm=0.179546, loss=0.127125
I0609 12:59:40.639337 140496353691456 submission.py:120] 89) loss = 0.127, grad_norm = 0.180
I0609 12:59:41.816711 140429995538176 logging_writer.py:48] [90] global_step=90, grad_norm=0.384095, loss=0.130858
I0609 12:59:41.820071 140496353691456 submission.py:120] 90) loss = 0.131, grad_norm = 0.384
I0609 12:59:42.951426 140429987145472 logging_writer.py:48] [91] global_step=91, grad_norm=0.698102, loss=0.132351
I0609 12:59:42.954871 140496353691456 submission.py:120] 91) loss = 0.132, grad_norm = 0.698
I0609 12:59:44.087306 140429995538176 logging_writer.py:48] [92] global_step=92, grad_norm=0.843342, loss=0.136560
I0609 12:59:44.090454 140496353691456 submission.py:120] 92) loss = 0.137, grad_norm = 0.843
I0609 12:59:45.226624 140429987145472 logging_writer.py:48] [93] global_step=93, grad_norm=0.560525, loss=0.129566
I0609 12:59:45.230435 140496353691456 submission.py:120] 93) loss = 0.130, grad_norm = 0.561
I0609 12:59:46.354428 140429995538176 logging_writer.py:48] [94] global_step=94, grad_norm=0.061844, loss=0.128242
I0609 12:59:46.358068 140496353691456 submission.py:120] 94) loss = 0.128, grad_norm = 0.062
I0609 12:59:47.483636 140429987145472 logging_writer.py:48] [95] global_step=95, grad_norm=0.259777, loss=0.128715
I0609 12:59:47.487529 140496353691456 submission.py:120] 95) loss = 0.129, grad_norm = 0.260
I0609 12:59:48.708667 140429995538176 logging_writer.py:48] [96] global_step=96, grad_norm=0.179936, loss=0.132361
I0609 12:59:48.711977 140496353691456 submission.py:120] 96) loss = 0.132, grad_norm = 0.180
I0609 12:59:49.879564 140429987145472 logging_writer.py:48] [97] global_step=97, grad_norm=0.052610, loss=0.130542
I0609 12:59:49.883199 140496353691456 submission.py:120] 97) loss = 0.131, grad_norm = 0.053
I0609 12:59:51.030890 140429995538176 logging_writer.py:48] [98] global_step=98, grad_norm=0.248224, loss=0.133322
I0609 12:59:51.034361 140496353691456 submission.py:120] 98) loss = 0.133, grad_norm = 0.248
I0609 12:59:52.171066 140429987145472 logging_writer.py:48] [99] global_step=99, grad_norm=0.280025, loss=0.132283
I0609 12:59:52.174599 140496353691456 submission.py:120] 99) loss = 0.132, grad_norm = 0.280
I0609 12:59:53.306720 140429995538176 logging_writer.py:48] [100] global_step=100, grad_norm=0.113610, loss=0.130103
I0609 12:59:53.310720 140496353691456 submission.py:120] 100) loss = 0.130, grad_norm = 0.114
I0609 12:59:58.904065 140496353691456 spec.py:298] Evaluating on the training split.
I0609 13:04:30.412915 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 13:09:07.005423 140496353691456 spec.py:326] Evaluating on the test split.
I0609 13:13:17.522934 140496353691456 submission_runner.py:419] Time since start: 1777.69s, 	Step: 106, 	{'train/loss': 0.12890795664896063, 'validation/loss': 0.13031624719101123, 'validation/num_examples': 89000000, 'test/loss': 0.13314718938593947, 'test/num_examples': 89274637, 'score': 126.3783323764801, 'total_duration': 1777.685575723648, 'accumulated_submission_time': 126.3783323764801, 'accumulated_eval_time': 1651.244477033615, 'accumulated_logging_time': 0.024333953857421875}
I0609 13:13:17.534268 140429987145472 logging_writer.py:48] [106] accumulated_eval_time=1651.244477, accumulated_logging_time=0.024334, accumulated_submission_time=126.378332, global_step=106, preemption_count=0, score=126.378332, test/loss=0.133147, test/num_examples=89274637, total_duration=1777.685576, train/loss=0.128908, validation/loss=0.130316, validation/num_examples=89000000
I0609 13:15:17.564682 140496353691456 spec.py:298] Evaluating on the training split.
I0609 13:19:59.481835 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 13:24:40.031330 140496353691456 spec.py:326] Evaluating on the test split.
I0609 13:29:12.210901 140496353691456 submission_runner.py:419] Time since start: 2732.37s, 	Step: 211, 	{'train/loss': 0.12914657312074596, 'validation/loss': 0.1289963595505618, 'validation/num_examples': 89000000, 'test/loss': 0.13232302473545762, 'test/num_examples': 89274637, 'score': 246.35908484458923, 'total_duration': 2732.3735098838806, 'accumulated_submission_time': 246.35908484458923, 'accumulated_eval_time': 2485.890517950058, 'accumulated_logging_time': 0.04271101951599121}
I0609 13:29:12.221619 140429995538176 logging_writer.py:48] [211] accumulated_eval_time=2485.890518, accumulated_logging_time=0.042711, accumulated_submission_time=246.359085, global_step=211, preemption_count=0, score=246.359085, test/loss=0.132323, test/num_examples=89274637, total_duration=2732.373510, train/loss=0.129147, validation/loss=0.128996, validation/num_examples=89000000
I0609 13:31:12.399779 140496353691456 spec.py:298] Evaluating on the training split.
I0609 13:35:40.099884 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 13:40:21.689527 140496353691456 spec.py:326] Evaluating on the test split.
I0609 13:44:55.776630 140496353691456 submission_runner.py:419] Time since start: 3675.94s, 	Step: 316, 	{'train/loss': 0.12855629713944078, 'validation/loss': 0.1279423595505618, 'validation/num_examples': 89000000, 'test/loss': 0.13085000838480026, 'test/num_examples': 89274637, 'score': 366.4879274368286, 'total_duration': 3675.9392976760864, 'accumulated_submission_time': 366.4879274368286, 'accumulated_eval_time': 3309.267275094986, 'accumulated_logging_time': 0.0617985725402832}
I0609 13:44:55.787161 140429987145472 logging_writer.py:48] [316] accumulated_eval_time=3309.267275, accumulated_logging_time=0.061799, accumulated_submission_time=366.487927, global_step=316, preemption_count=0, score=366.487927, test/loss=0.130850, test/num_examples=89274637, total_duration=3675.939298, train/loss=0.128556, validation/loss=0.127942, validation/num_examples=89000000
I0609 13:46:56.064023 140496353691456 spec.py:298] Evaluating on the training split.
I0609 13:51:37.307762 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 13:56:19.126881 140496353691456 spec.py:326] Evaluating on the test split.
I0609 14:00:36.603979 140496353691456 submission_runner.py:419] Time since start: 4616.77s, 	Step: 421, 	{'train/loss': 0.12699752032888384, 'validation/loss': 0.12733642696629213, 'validation/num_examples': 89000000, 'test/loss': 0.1302575780845796, 'test/num_examples': 89274637, 'score': 486.71432423591614, 'total_duration': 4616.76663851738, 'accumulated_submission_time': 486.71432423591614, 'accumulated_eval_time': 4129.807103633881, 'accumulated_logging_time': 0.08048486709594727}
I0609 14:00:36.614021 140429995538176 logging_writer.py:48] [421] accumulated_eval_time=4129.807104, accumulated_logging_time=0.080485, accumulated_submission_time=486.714324, global_step=421, preemption_count=0, score=486.714324, test/loss=0.130258, test/num_examples=89274637, total_duration=4616.766639, train/loss=0.126998, validation/loss=0.127336, validation/num_examples=89000000
I0609 14:02:08.519351 140429987145472 logging_writer.py:48] [500] global_step=500, grad_norm=0.007616, loss=0.132124
I0609 14:02:08.524355 140496353691456 submission.py:120] 500) loss = 0.132, grad_norm = 0.008
I0609 14:02:37.007253 140496353691456 spec.py:298] Evaluating on the training split.
I0609 14:07:08.336893 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 14:11:48.402134 140496353691456 spec.py:326] Evaluating on the test split.
I0609 14:16:23.103101 140496353691456 submission_runner.py:419] Time since start: 5563.27s, 	Step: 526, 	{'train/loss': 0.12691461311884888, 'validation/loss': 0.12721303370786516, 'validation/num_examples': 89000000, 'test/loss': 0.13009845114240004, 'test/num_examples': 89274637, 'score': 607.0565748214722, 'total_duration': 5563.265733957291, 'accumulated_submission_time': 607.0565748214722, 'accumulated_eval_time': 4955.902784347534, 'accumulated_logging_time': 0.09850382804870605}
I0609 14:16:23.113657 140429995538176 logging_writer.py:48] [526] accumulated_eval_time=4955.902784, accumulated_logging_time=0.098504, accumulated_submission_time=607.056575, global_step=526, preemption_count=0, score=607.056575, test/loss=0.130098, test/num_examples=89274637, total_duration=5563.265734, train/loss=0.126915, validation/loss=0.127213, validation/num_examples=89000000
I0609 14:18:24.306117 140496353691456 spec.py:298] Evaluating on the training split.
I0609 14:23:01.998938 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 14:27:43.385411 140496353691456 spec.py:326] Evaluating on the test split.
I0609 14:32:04.528798 140496353691456 submission_runner.py:419] Time since start: 6504.69s, 	Step: 615, 	{'train/loss': 0.126524329448641, 'validation/loss': 0.1269224606741573, 'validation/num_examples': 89000000, 'test/loss': 0.1297928772311894, 'test/num_examples': 89274637, 'score': 728.2036285400391, 'total_duration': 6504.691457986832, 'accumulated_submission_time': 728.2036285400391, 'accumulated_eval_time': 5776.1253843307495, 'accumulated_logging_time': 0.11621284484863281}
I0609 14:32:04.538993 140429987145472 logging_writer.py:48] [615] accumulated_eval_time=5776.125384, accumulated_logging_time=0.116213, accumulated_submission_time=728.203629, global_step=615, preemption_count=0, score=728.203629, test/loss=0.129793, test/num_examples=89274637, total_duration=6504.691458, train/loss=0.126524, validation/loss=0.126922, validation/num_examples=89000000
I0609 14:34:04.678550 140496353691456 spec.py:298] Evaluating on the training split.
I0609 14:38:46.263270 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 14:43:29.090226 140496353691456 spec.py:326] Evaluating on the test split.
I0609 14:48:02.511947 140496353691456 submission_runner.py:419] Time since start: 7462.67s, 	Step: 709, 	{'train/loss': 0.17640671438816455, 'validation/loss': 0.17119923595505618, 'validation/num_examples': 89000000, 'test/loss': 0.17734506162147712, 'test/num_examples': 89274637, 'score': 848.2983384132385, 'total_duration': 7462.674570322037, 'accumulated_submission_time': 848.2983384132385, 'accumulated_eval_time': 6613.958626031876, 'accumulated_logging_time': 0.13335824012756348}
I0609 14:48:02.523220 140429995538176 logging_writer.py:48] [709] accumulated_eval_time=6613.958626, accumulated_logging_time=0.133358, accumulated_submission_time=848.298338, global_step=709, preemption_count=0, score=848.298338, test/loss=0.177345, test/num_examples=89274637, total_duration=7462.674570, train/loss=0.176407, validation/loss=0.171199, validation/num_examples=89000000
I0609 14:50:02.881150 140496353691456 spec.py:298] Evaluating on the training split.
I0609 14:54:34.952032 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 14:59:13.983312 140496353691456 spec.py:326] Evaluating on the test split.
I0609 15:03:33.144232 140496353691456 submission_runner.py:419] Time since start: 8393.31s, 	Step: 807, 	{'train/loss': 0.12572921068024512, 'validation/loss': 0.12716853932584268, 'validation/num_examples': 89000000, 'test/loss': 0.12971413146154825, 'test/num_examples': 89274637, 'score': 968.6114630699158, 'total_duration': 8393.306928634644, 'accumulated_submission_time': 968.6114630699158, 'accumulated_eval_time': 7424.221614360809, 'accumulated_logging_time': 0.1522223949432373}
I0609 15:03:33.155827 140429987145472 logging_writer.py:48] [807] accumulated_eval_time=7424.221614, accumulated_logging_time=0.152222, accumulated_submission_time=968.611463, global_step=807, preemption_count=0, score=968.611463, test/loss=0.129714, test/num_examples=89274637, total_duration=8393.306929, train/loss=0.125729, validation/loss=0.127169, validation/num_examples=89000000
I0609 15:05:33.466219 140496353691456 spec.py:298] Evaluating on the training split.
I0609 15:10:01.970482 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 15:14:42.403046 140496353691456 spec.py:326] Evaluating on the test split.
I0609 15:19:18.194806 140496353691456 submission_runner.py:419] Time since start: 9338.36s, 	Step: 911, 	{'train/loss': 0.1270089391577849, 'validation/loss': 0.12649743820224718, 'validation/num_examples': 89000000, 'test/loss': 0.12932773952360063, 'test/num_examples': 89274637, 'score': 1088.8752508163452, 'total_duration': 9338.35745882988, 'accumulated_submission_time': 1088.8752508163452, 'accumulated_eval_time': 8248.950094461441, 'accumulated_logging_time': 0.17145609855651855}
I0609 15:19:18.205353 140429995538176 logging_writer.py:48] [911] accumulated_eval_time=8248.950094, accumulated_logging_time=0.171456, accumulated_submission_time=1088.875251, global_step=911, preemption_count=0, score=1088.875251, test/loss=0.129328, test/num_examples=89274637, total_duration=9338.357459, train/loss=0.127009, validation/loss=0.126497, validation/num_examples=89000000
I0609 15:21:01.337385 140429987145472 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.021268, loss=0.119416
I0609 15:21:01.341292 140496353691456 submission.py:120] 1000) loss = 0.119, grad_norm = 0.021
I0609 15:21:18.421057 140496353691456 spec.py:298] Evaluating on the training split.
I0609 15:25:56.856223 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 15:30:34.683462 140496353691456 spec.py:326] Evaluating on the test split.
I0609 15:35:11.064766 140496353691456 submission_runner.py:419] Time since start: 10291.23s, 	Step: 1016, 	{'train/loss': 0.12545422686646315, 'validation/loss': 0.12730194382022472, 'validation/num_examples': 89000000, 'test/loss': 0.12994104921423538, 'test/num_examples': 89274637, 'score': 1209.0398914813995, 'total_duration': 10291.227397680283, 'accumulated_submission_time': 1209.0398914813995, 'accumulated_eval_time': 9081.593648910522, 'accumulated_logging_time': 0.19031310081481934}
I0609 15:35:11.074620 140429995538176 logging_writer.py:48] [1016] accumulated_eval_time=9081.593649, accumulated_logging_time=0.190313, accumulated_submission_time=1209.039891, global_step=1016, preemption_count=0, score=1209.039891, test/loss=0.129941, test/num_examples=89274637, total_duration=10291.227398, train/loss=0.125454, validation/loss=0.127302, validation/num_examples=89000000
I0609 15:37:12.110560 140496353691456 spec.py:298] Evaluating on the training split.
I0609 15:41:45.070650 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 15:46:27.483035 140496353691456 spec.py:326] Evaluating on the test split.
I0609 15:50:51.795114 140496353691456 submission_runner.py:419] Time since start: 11231.96s, 	Step: 1122, 	{'train/loss': 0.12625050211408895, 'validation/loss': 0.1261676404494382, 'validation/num_examples': 89000000, 'test/loss': 0.1287749061359947, 'test/num_examples': 89274637, 'score': 1330.0270597934723, 'total_duration': 11231.95772099495, 'accumulated_submission_time': 1330.0270597934723, 'accumulated_eval_time': 9901.278064727783, 'accumulated_logging_time': 0.20702433586120605}
I0609 15:50:51.807850 140429987145472 logging_writer.py:48] [1122] accumulated_eval_time=9901.278065, accumulated_logging_time=0.207024, accumulated_submission_time=1330.027060, global_step=1122, preemption_count=0, score=1330.027060, test/loss=0.128775, test/num_examples=89274637, total_duration=11231.957721, train/loss=0.126251, validation/loss=0.126168, validation/num_examples=89000000
I0609 15:52:52.101430 140496353691456 spec.py:298] Evaluating on the training split.
I0609 15:57:33.154003 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 16:02:15.654456 140496353691456 spec.py:326] Evaluating on the test split.
I0609 16:06:52.015868 140496353691456 submission_runner.py:419] Time since start: 12192.18s, 	Step: 1202, 	{'train/loss': 0.12534348555923472, 'validation/loss': 0.1262403033707865, 'validation/num_examples': 89000000, 'test/loss': 0.12880308883249786, 'test/num_examples': 89274637, 'score': 1450.2800178527832, 'total_duration': 12192.17854142189, 'accumulated_submission_time': 1450.2800178527832, 'accumulated_eval_time': 10741.192414999008, 'accumulated_logging_time': 0.2293701171875}
I0609 16:06:52.026559 140429995538176 logging_writer.py:48] [1202] accumulated_eval_time=10741.192415, accumulated_logging_time=0.229370, accumulated_submission_time=1450.280018, global_step=1202, preemption_count=0, score=1450.280018, test/loss=0.128803, test/num_examples=89274637, total_duration=12192.178541, train/loss=0.125343, validation/loss=0.126240, validation/num_examples=89000000
I0609 16:08:53.157596 140496353691456 spec.py:298] Evaluating on the training split.
I0609 16:13:27.605046 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 16:18:07.647168 140496353691456 spec.py:326] Evaluating on the test split.
I0609 16:22:28.927429 140496353691456 submission_runner.py:419] Time since start: 13129.09s, 	Step: 1285, 	{'train/loss': 0.12758976120208448, 'validation/loss': 0.1265676966292135, 'validation/num_examples': 89000000, 'test/loss': 0.1287033292557661, 'test/num_examples': 89274637, 'score': 1571.3709683418274, 'total_duration': 13129.090058088303, 'accumulated_submission_time': 1571.3709683418274, 'accumulated_eval_time': 11556.962130069733, 'accumulated_logging_time': 0.2468876838684082}
I0609 16:22:28.937929 140429987145472 logging_writer.py:48] [1285] accumulated_eval_time=11556.962130, accumulated_logging_time=0.246888, accumulated_submission_time=1571.370968, global_step=1285, preemption_count=0, score=1571.370968, test/loss=0.128703, test/num_examples=89274637, total_duration=13129.090058, train/loss=0.127590, validation/loss=0.126568, validation/num_examples=89000000
I0609 16:24:29.383997 140496353691456 spec.py:298] Evaluating on the training split.
I0609 16:29:12.577572 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 16:33:55.481296 140496353691456 spec.py:326] Evaluating on the test split.
I0609 16:38:30.501818 140496353691456 submission_runner.py:419] Time since start: 14090.66s, 	Step: 1376, 	{'train/loss': 0.12564088036857868, 'validation/loss': 0.12620066292134832, 'validation/num_examples': 89000000, 'test/loss': 0.12844820640379642, 'test/num_examples': 89274637, 'score': 1691.7725722789764, 'total_duration': 14090.66448020935, 'accumulated_submission_time': 1691.7725722789764, 'accumulated_eval_time': 12398.07986664772, 'accumulated_logging_time': 0.2654135227203369}
I0609 16:38:30.511867 140429995538176 logging_writer.py:48] [1376] accumulated_eval_time=12398.079867, accumulated_logging_time=0.265414, accumulated_submission_time=1691.772572, global_step=1376, preemption_count=0, score=1691.772572, test/loss=0.128448, test/num_examples=89274637, total_duration=14090.664480, train/loss=0.125641, validation/loss=0.126201, validation/num_examples=89000000
I0609 16:40:31.496959 140496353691456 spec.py:298] Evaluating on the training split.
I0609 16:45:01.772964 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 16:49:43.777252 140496353691456 spec.py:326] Evaluating on the test split.
I0609 16:54:19.051137 140496353691456 submission_runner.py:419] Time since start: 15039.21s, 	Step: 1479, 	{'train/loss': 0.12536160747648778, 'validation/loss': 0.12595159550561796, 'validation/num_examples': 89000000, 'test/loss': 0.1286108281795646, 'test/num_examples': 89274637, 'score': 1812.7098808288574, 'total_duration': 15039.213794231415, 'accumulated_submission_time': 1812.7098808288574, 'accumulated_eval_time': 13225.633947134018, 'accumulated_logging_time': 0.2822904586791992}
I0609 16:54:19.061044 140429987145472 logging_writer.py:48] [1479] accumulated_eval_time=13225.633947, accumulated_logging_time=0.282290, accumulated_submission_time=1812.709881, global_step=1479, preemption_count=0, score=1812.709881, test/loss=0.128611, test/num_examples=89274637, total_duration=15039.213794, train/loss=0.125362, validation/loss=0.125952, validation/num_examples=89000000
I0609 16:54:44.533938 140429995538176 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.014954, loss=0.128615
I0609 16:54:44.537370 140496353691456 submission.py:120] 1500) loss = 0.129, grad_norm = 0.015
I0609 16:56:20.016413 140496353691456 spec.py:298] Evaluating on the training split.
I0609 17:00:59.712832 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 17:05:40.302969 140496353691456 spec.py:326] Evaluating on the test split.
I0609 17:10:16.296202 140496353691456 submission_runner.py:419] Time since start: 15996.46s, 	Step: 1585, 	{'train/loss': 0.12493637118644799, 'validation/loss': 0.12587185393258427, 'validation/num_examples': 89000000, 'test/loss': 0.12862337373603658, 'test/num_examples': 89274637, 'score': 1933.6187348365784, 'total_duration': 15996.458906173706, 'accumulated_submission_time': 1933.6187348365784, 'accumulated_eval_time': 14061.913671731949, 'accumulated_logging_time': 0.29936838150024414}
I0609 17:10:16.306652 140429987145472 logging_writer.py:48] [1585] accumulated_eval_time=14061.913672, accumulated_logging_time=0.299368, accumulated_submission_time=1933.618735, global_step=1585, preemption_count=0, score=1933.618735, test/loss=0.128623, test/num_examples=89274637, total_duration=15996.458906, train/loss=0.124936, validation/loss=0.125872, validation/num_examples=89000000
I0609 17:10:34.977197 140496353691456 spec.py:298] Evaluating on the training split.
I0609 17:15:02.456470 140496353691456 spec.py:310] Evaluating on the validation split.
I0609 17:19:44.265741 140496353691456 spec.py:326] Evaluating on the test split.
I0609 17:24:04.911677 140496353691456 submission_runner.py:419] Time since start: 16825.07s, 	Step: 1600, 	{'train/loss': 0.12512404643234915, 'validation/loss': 0.12574856179775282, 'validation/num_examples': 89000000, 'test/loss': 0.12856390555808142, 'test/num_examples': 89274637, 'score': 1952.2754850387573, 'total_duration': 16825.074343681335, 'accumulated_submission_time': 1952.2754850387573, 'accumulated_eval_time': 14871.84805393219, 'accumulated_logging_time': 0.3171350955963135}
I0609 17:24:04.923568 140429995538176 logging_writer.py:48] [1600] accumulated_eval_time=14871.848054, accumulated_logging_time=0.317135, accumulated_submission_time=1952.275485, global_step=1600, preemption_count=0, score=1952.275485, test/loss=0.128564, test/num_examples=89274637, total_duration=16825.074344, train/loss=0.125124, validation/loss=0.125749, validation/num_examples=89000000
I0609 17:24:04.939444 140429987145472 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=1952.275485
I0609 17:24:15.781295 140496353691456 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/adamw/criteo1tb_pytorch/trial_1/checkpoint_1600.
I0609 17:24:15.870971 140496353691456 submission_runner.py:581] Tuning trial 1/1
I0609 17:24:15.871222 140496353691456 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0609 17:24:15.872403 140496353691456 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/loss': 1.5079770568892568, 'validation/loss': 1.5135611685393258, 'validation/num_examples': 89000000, 'test/loss': 1.5090419017889707, 'test/num_examples': 89274637, 'score': 5.883362531661987, 'total_duration': 858.5095055103302, 'accumulated_submission_time': 5.883362531661987, 'accumulated_eval_time': 852.6257433891296, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (106, {'train/loss': 0.12890795664896063, 'validation/loss': 0.13031624719101123, 'validation/num_examples': 89000000, 'test/loss': 0.13314718938593947, 'test/num_examples': 89274637, 'score': 126.3783323764801, 'total_duration': 1777.685575723648, 'accumulated_submission_time': 126.3783323764801, 'accumulated_eval_time': 1651.244477033615, 'accumulated_logging_time': 0.024333953857421875, 'global_step': 106, 'preemption_count': 0}), (211, {'train/loss': 0.12914657312074596, 'validation/loss': 0.1289963595505618, 'validation/num_examples': 89000000, 'test/loss': 0.13232302473545762, 'test/num_examples': 89274637, 'score': 246.35908484458923, 'total_duration': 2732.3735098838806, 'accumulated_submission_time': 246.35908484458923, 'accumulated_eval_time': 2485.890517950058, 'accumulated_logging_time': 0.04271101951599121, 'global_step': 211, 'preemption_count': 0}), (316, {'train/loss': 0.12855629713944078, 'validation/loss': 0.1279423595505618, 'validation/num_examples': 89000000, 'test/loss': 0.13085000838480026, 'test/num_examples': 89274637, 'score': 366.4879274368286, 'total_duration': 3675.9392976760864, 'accumulated_submission_time': 366.4879274368286, 'accumulated_eval_time': 3309.267275094986, 'accumulated_logging_time': 0.0617985725402832, 'global_step': 316, 'preemption_count': 0}), (421, {'train/loss': 0.12699752032888384, 'validation/loss': 0.12733642696629213, 'validation/num_examples': 89000000, 'test/loss': 0.1302575780845796, 'test/num_examples': 89274637, 'score': 486.71432423591614, 'total_duration': 4616.76663851738, 'accumulated_submission_time': 486.71432423591614, 'accumulated_eval_time': 4129.807103633881, 'accumulated_logging_time': 0.08048486709594727, 'global_step': 421, 'preemption_count': 0}), (526, {'train/loss': 0.12691461311884888, 'validation/loss': 0.12721303370786516, 'validation/num_examples': 89000000, 'test/loss': 0.13009845114240004, 'test/num_examples': 89274637, 'score': 607.0565748214722, 'total_duration': 5563.265733957291, 'accumulated_submission_time': 607.0565748214722, 'accumulated_eval_time': 4955.902784347534, 'accumulated_logging_time': 0.09850382804870605, 'global_step': 526, 'preemption_count': 0}), (615, {'train/loss': 0.126524329448641, 'validation/loss': 0.1269224606741573, 'validation/num_examples': 89000000, 'test/loss': 0.1297928772311894, 'test/num_examples': 89274637, 'score': 728.2036285400391, 'total_duration': 6504.691457986832, 'accumulated_submission_time': 728.2036285400391, 'accumulated_eval_time': 5776.1253843307495, 'accumulated_logging_time': 0.11621284484863281, 'global_step': 615, 'preemption_count': 0}), (709, {'train/loss': 0.17640671438816455, 'validation/loss': 0.17119923595505618, 'validation/num_examples': 89000000, 'test/loss': 0.17734506162147712, 'test/num_examples': 89274637, 'score': 848.2983384132385, 'total_duration': 7462.674570322037, 'accumulated_submission_time': 848.2983384132385, 'accumulated_eval_time': 6613.958626031876, 'accumulated_logging_time': 0.13335824012756348, 'global_step': 709, 'preemption_count': 0}), (807, {'train/loss': 0.12572921068024512, 'validation/loss': 0.12716853932584268, 'validation/num_examples': 89000000, 'test/loss': 0.12971413146154825, 'test/num_examples': 89274637, 'score': 968.6114630699158, 'total_duration': 8393.306928634644, 'accumulated_submission_time': 968.6114630699158, 'accumulated_eval_time': 7424.221614360809, 'accumulated_logging_time': 0.1522223949432373, 'global_step': 807, 'preemption_count': 0}), (911, {'train/loss': 0.1270089391577849, 'validation/loss': 0.12649743820224718, 'validation/num_examples': 89000000, 'test/loss': 0.12932773952360063, 'test/num_examples': 89274637, 'score': 1088.8752508163452, 'total_duration': 9338.35745882988, 'accumulated_submission_time': 1088.8752508163452, 'accumulated_eval_time': 8248.950094461441, 'accumulated_logging_time': 0.17145609855651855, 'global_step': 911, 'preemption_count': 0}), (1016, {'train/loss': 0.12545422686646315, 'validation/loss': 0.12730194382022472, 'validation/num_examples': 89000000, 'test/loss': 0.12994104921423538, 'test/num_examples': 89274637, 'score': 1209.0398914813995, 'total_duration': 10291.227397680283, 'accumulated_submission_time': 1209.0398914813995, 'accumulated_eval_time': 9081.593648910522, 'accumulated_logging_time': 0.19031310081481934, 'global_step': 1016, 'preemption_count': 0}), (1122, {'train/loss': 0.12625050211408895, 'validation/loss': 0.1261676404494382, 'validation/num_examples': 89000000, 'test/loss': 0.1287749061359947, 'test/num_examples': 89274637, 'score': 1330.0270597934723, 'total_duration': 11231.95772099495, 'accumulated_submission_time': 1330.0270597934723, 'accumulated_eval_time': 9901.278064727783, 'accumulated_logging_time': 0.20702433586120605, 'global_step': 1122, 'preemption_count': 0}), (1202, {'train/loss': 0.12534348555923472, 'validation/loss': 0.1262403033707865, 'validation/num_examples': 89000000, 'test/loss': 0.12880308883249786, 'test/num_examples': 89274637, 'score': 1450.2800178527832, 'total_duration': 12192.17854142189, 'accumulated_submission_time': 1450.2800178527832, 'accumulated_eval_time': 10741.192414999008, 'accumulated_logging_time': 0.2293701171875, 'global_step': 1202, 'preemption_count': 0}), (1285, {'train/loss': 0.12758976120208448, 'validation/loss': 0.1265676966292135, 'validation/num_examples': 89000000, 'test/loss': 0.1287033292557661, 'test/num_examples': 89274637, 'score': 1571.3709683418274, 'total_duration': 13129.090058088303, 'accumulated_submission_time': 1571.3709683418274, 'accumulated_eval_time': 11556.962130069733, 'accumulated_logging_time': 0.2468876838684082, 'global_step': 1285, 'preemption_count': 0}), (1376, {'train/loss': 0.12564088036857868, 'validation/loss': 0.12620066292134832, 'validation/num_examples': 89000000, 'test/loss': 0.12844820640379642, 'test/num_examples': 89274637, 'score': 1691.7725722789764, 'total_duration': 14090.66448020935, 'accumulated_submission_time': 1691.7725722789764, 'accumulated_eval_time': 12398.07986664772, 'accumulated_logging_time': 0.2654135227203369, 'global_step': 1376, 'preemption_count': 0}), (1479, {'train/loss': 0.12536160747648778, 'validation/loss': 0.12595159550561796, 'validation/num_examples': 89000000, 'test/loss': 0.1286108281795646, 'test/num_examples': 89274637, 'score': 1812.7098808288574, 'total_duration': 15039.213794231415, 'accumulated_submission_time': 1812.7098808288574, 'accumulated_eval_time': 13225.633947134018, 'accumulated_logging_time': 0.2822904586791992, 'global_step': 1479, 'preemption_count': 0}), (1585, {'train/loss': 0.12493637118644799, 'validation/loss': 0.12587185393258427, 'validation/num_examples': 89000000, 'test/loss': 0.12862337373603658, 'test/num_examples': 89274637, 'score': 1933.6187348365784, 'total_duration': 15996.458906173706, 'accumulated_submission_time': 1933.6187348365784, 'accumulated_eval_time': 14061.913671731949, 'accumulated_logging_time': 0.29936838150024414, 'global_step': 1585, 'preemption_count': 0}), (1600, {'train/loss': 0.12512404643234915, 'validation/loss': 0.12574856179775282, 'validation/num_examples': 89000000, 'test/loss': 0.12856390555808142, 'test/num_examples': 89274637, 'score': 1952.2754850387573, 'total_duration': 16825.074343681335, 'accumulated_submission_time': 1952.2754850387573, 'accumulated_eval_time': 14871.84805393219, 'accumulated_logging_time': 0.3171350955963135, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0609 17:24:15.872507 140496353691456 submission_runner.py:584] Timing: 1952.2754850387573
I0609 17:24:15.872555 140496353691456 submission_runner.py:586] Total number of evals: 18
I0609 17:24:15.872610 140496353691456 submission_runner.py:587] ====================
I0609 17:24:15.872703 140496353691456 submission_runner.py:655] Final criteo1tb score: 1952.2754850387573
