torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/nadamw --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_pytorch_06-09-2023-02-06-54.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 02:07:17.594336 139912364853056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 02:07:17.594372 140223560689472 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 02:07:17.594396 140461710985024 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 02:07:17.595351 140411633264448 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 02:07:17.595420 140095247849280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 02:07:17.595463 139973353498432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 02:07:17.595564 140208334571328 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 02:07:17.595838 140095247849280 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:07:17.595886 139973353498432 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:07:17.595949 140208334571328 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:07:17.595815 139759189579584 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 02:07:17.596132 139759189579584 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:07:17.604960 139912364853056 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:07:17.604997 140223560689472 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:07:17.605037 140461710985024 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:07:17.605920 140411633264448 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:07:18.780017 140095247849280 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/nadamw/ogbg_pytorch.
W0609 02:07:18.828943 139912364853056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:07:18.829239 140208334571328 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:07:18.829449 139759189579584 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:07:18.829952 139973353498432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:07:18.830076 140461710985024 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:07:18.830497 140095247849280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:07:18.831207 140223560689472 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:07:18.831439 140411633264448 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 02:07:18.835429 140095247849280 submission_runner.py:541] Using RNG seed 2619747547
I0609 02:07:18.836693 140095247849280 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 02:07:18.836804 140095247849280 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/nadamw/ogbg_pytorch/trial_1.
I0609 02:07:18.837980 140095247849280 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/nadamw/ogbg_pytorch/trial_1/hparams.json.
I0609 02:07:18.839012 140095247849280 submission_runner.py:255] Initializing dataset.
I0609 02:07:18.839145 140095247849280 submission_runner.py:262] Initializing model.
I0609 02:07:22.849371 140095247849280 submission_runner.py:272] Initializing optimizer.
I0609 02:07:22.850280 140095247849280 submission_runner.py:279] Initializing metrics bundle.
I0609 02:07:22.850386 140095247849280 submission_runner.py:297] Initializing checkpoint and logger.
I0609 02:07:22.853894 140095247849280 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 02:07:22.854027 140095247849280 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 02:07:23.367449 140095247849280 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/nadamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0609 02:07:23.368351 140095247849280 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/nadamw/ogbg_pytorch/trial_1/flags_0.json.
I0609 02:07:23.413988 140095247849280 submission_runner.py:332] Starting training loop.
I0609 02:07:23.933856 140095247849280 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:07:23.940318 140095247849280 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:07:24.085295 140095247849280 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:07:28.689273 140057029617408 logging_writer.py:48] [0] global_step=0, grad_norm=2.574265, loss=0.718848
I0609 02:07:28.698234 140095247849280 submission.py:296] 0) loss = 0.719, grad_norm = 2.574
I0609 02:07:28.708906 140095247849280 spec.py:298] Evaluating on the training split.
I0609 02:07:28.715031 140095247849280 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:07:28.719521 140095247849280 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:07:28.773827 140095247849280 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:08:27.458548 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 02:08:27.461806 140095247849280 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:08:27.466413 140095247849280 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:08:27.522689 140095247849280 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:09:13.508478 140095247849280 spec.py:326] Evaluating on the test split.
I0609 02:09:13.511964 140095247849280 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:09:13.516309 140095247849280 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:09:13.572505 140095247849280 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:10:00.880381 140095247849280 submission_runner.py:419] Time since start: 157.47s, 	Step: 1, 	{'train/accuracy': 0.5109300492995043, 'train/loss': 0.7192444340365306, 'train/mean_average_precision': 0.021513561790081362, 'validation/accuracy': 0.5043878130107241, 'validation/loss': 0.7227870041332882, 'validation/mean_average_precision': 0.024981593475365164, 'validation/num_examples': 43793, 'test/accuracy': 0.5026642669801479, 'test/loss': 0.7247406914921622, 'test/mean_average_precision': 0.02699860918358328, 'test/num_examples': 43793, 'score': 5.295187473297119, 'total_duration': 157.4665801525116, 'accumulated_submission_time': 5.295187473297119, 'accumulated_eval_time': 152.17107105255127, 'accumulated_logging_time': 0}
I0609 02:10:00.898766 140043011815168 logging_writer.py:48] [1] accumulated_eval_time=152.171071, accumulated_logging_time=0, accumulated_submission_time=5.295187, global_step=1, preemption_count=0, score=5.295187, test/accuracy=0.502664, test/loss=0.724741, test/mean_average_precision=0.026999, test/num_examples=43793, total_duration=157.466580, train/accuracy=0.510930, train/loss=0.719244, train/mean_average_precision=0.021514, validation/accuracy=0.504388, validation/loss=0.722787, validation/mean_average_precision=0.024982, validation/num_examples=43793
I0609 02:10:01.195697 140095247849280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:10:01.202637 140411633264448 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:10:01.205165 140223560689472 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:10:01.205173 139973353498432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:10:01.205236 139912364853056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:10:01.205247 140208334571328 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:10:01.205270 139759189579584 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:10:01.205278 140461710985024 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:10:01.251847 140043020207872 logging_writer.py:48] [1] global_step=1, grad_norm=2.549916, loss=0.718468
I0609 02:10:01.258571 140095247849280 submission.py:296] 1) loss = 0.718, grad_norm = 2.550
I0609 02:10:01.610568 140043011815168 logging_writer.py:48] [2] global_step=2, grad_norm=2.555890, loss=0.718170
I0609 02:10:01.614974 140095247849280 submission.py:296] 2) loss = 0.718, grad_norm = 2.556
I0609 02:10:01.919169 140043020207872 logging_writer.py:48] [3] global_step=3, grad_norm=2.547388, loss=0.713790
I0609 02:10:01.923181 140095247849280 submission.py:296] 3) loss = 0.714, grad_norm = 2.547
I0609 02:10:02.225415 140043011815168 logging_writer.py:48] [4] global_step=4, grad_norm=2.530965, loss=0.713441
I0609 02:10:02.229358 140095247849280 submission.py:296] 4) loss = 0.713, grad_norm = 2.531
I0609 02:10:02.532434 140043020207872 logging_writer.py:48] [5] global_step=5, grad_norm=2.501321, loss=0.708467
I0609 02:10:02.536852 140095247849280 submission.py:296] 5) loss = 0.708, grad_norm = 2.501
I0609 02:10:02.841681 140043011815168 logging_writer.py:48] [6] global_step=6, grad_norm=2.428968, loss=0.706530
I0609 02:10:02.846365 140095247849280 submission.py:296] 6) loss = 0.707, grad_norm = 2.429
I0609 02:10:03.149419 140043020207872 logging_writer.py:48] [7] global_step=7, grad_norm=2.351123, loss=0.699882
I0609 02:10:03.153586 140095247849280 submission.py:296] 7) loss = 0.700, grad_norm = 2.351
I0609 02:10:03.458263 140043011815168 logging_writer.py:48] [8] global_step=8, grad_norm=2.309891, loss=0.695775
I0609 02:10:03.462327 140095247849280 submission.py:296] 8) loss = 0.696, grad_norm = 2.310
I0609 02:10:03.765036 140043020207872 logging_writer.py:48] [9] global_step=9, grad_norm=2.203676, loss=0.690485
I0609 02:10:03.769141 140095247849280 submission.py:296] 9) loss = 0.690, grad_norm = 2.204
I0609 02:10:04.073417 140043011815168 logging_writer.py:48] [10] global_step=10, grad_norm=2.125943, loss=0.684946
I0609 02:10:04.077512 140095247849280 submission.py:296] 10) loss = 0.685, grad_norm = 2.126
I0609 02:10:04.379454 140043020207872 logging_writer.py:48] [11] global_step=11, grad_norm=2.091701, loss=0.680072
I0609 02:10:04.383737 140095247849280 submission.py:296] 11) loss = 0.680, grad_norm = 2.092
I0609 02:10:04.692662 140043011815168 logging_writer.py:48] [12] global_step=12, grad_norm=2.023508, loss=0.672925
I0609 02:10:04.696838 140095247849280 submission.py:296] 12) loss = 0.673, grad_norm = 2.024
I0609 02:10:04.998910 140043020207872 logging_writer.py:48] [13] global_step=13, grad_norm=1.997409, loss=0.667585
I0609 02:10:05.002950 140095247849280 submission.py:296] 13) loss = 0.668, grad_norm = 1.997
I0609 02:10:05.310381 140043011815168 logging_writer.py:48] [14] global_step=14, grad_norm=1.945775, loss=0.657960
I0609 02:10:05.314330 140095247849280 submission.py:296] 14) loss = 0.658, grad_norm = 1.946
I0609 02:10:05.626114 140043020207872 logging_writer.py:48] [15] global_step=15, grad_norm=1.894057, loss=0.653187
I0609 02:10:05.630121 140095247849280 submission.py:296] 15) loss = 0.653, grad_norm = 1.894
I0609 02:10:05.934614 140043011815168 logging_writer.py:48] [16] global_step=16, grad_norm=1.844514, loss=0.645349
I0609 02:10:05.938786 140095247849280 submission.py:296] 16) loss = 0.645, grad_norm = 1.845
I0609 02:10:06.241090 140043020207872 logging_writer.py:48] [17] global_step=17, grad_norm=1.755396, loss=0.639696
I0609 02:10:06.246151 140095247849280 submission.py:296] 17) loss = 0.640, grad_norm = 1.755
I0609 02:10:06.548121 140043011815168 logging_writer.py:48] [18] global_step=18, grad_norm=1.686384, loss=0.632880
I0609 02:10:06.552340 140095247849280 submission.py:296] 18) loss = 0.633, grad_norm = 1.686
I0609 02:10:06.857263 140043020207872 logging_writer.py:48] [19] global_step=19, grad_norm=1.614859, loss=0.625651
I0609 02:10:06.861324 140095247849280 submission.py:296] 19) loss = 0.626, grad_norm = 1.615
I0609 02:10:07.163511 140043011815168 logging_writer.py:48] [20] global_step=20, grad_norm=1.575776, loss=0.617907
I0609 02:10:07.167638 140095247849280 submission.py:296] 20) loss = 0.618, grad_norm = 1.576
I0609 02:10:07.470062 140043020207872 logging_writer.py:48] [21] global_step=21, grad_norm=1.530310, loss=0.612132
I0609 02:10:07.473983 140095247849280 submission.py:296] 21) loss = 0.612, grad_norm = 1.530
I0609 02:10:07.777980 140043011815168 logging_writer.py:48] [22] global_step=22, grad_norm=1.494229, loss=0.606451
I0609 02:10:07.782192 140095247849280 submission.py:296] 22) loss = 0.606, grad_norm = 1.494
I0609 02:10:08.085883 140043020207872 logging_writer.py:48] [23] global_step=23, grad_norm=1.455819, loss=0.599607
I0609 02:10:08.089981 140095247849280 submission.py:296] 23) loss = 0.600, grad_norm = 1.456
I0609 02:10:08.409862 140043011815168 logging_writer.py:48] [24] global_step=24, grad_norm=1.435073, loss=0.593506
I0609 02:10:08.413829 140095247849280 submission.py:296] 24) loss = 0.594, grad_norm = 1.435
I0609 02:10:08.733363 140043020207872 logging_writer.py:48] [25] global_step=25, grad_norm=1.373224, loss=0.587694
I0609 02:10:08.737494 140095247849280 submission.py:296] 25) loss = 0.588, grad_norm = 1.373
I0609 02:10:09.043313 140043011815168 logging_writer.py:48] [26] global_step=26, grad_norm=1.340457, loss=0.582570
I0609 02:10:09.047566 140095247849280 submission.py:296] 26) loss = 0.583, grad_norm = 1.340
I0609 02:10:09.351042 140043020207872 logging_writer.py:48] [27] global_step=27, grad_norm=1.320546, loss=0.576855
I0609 02:10:09.354925 140095247849280 submission.py:296] 27) loss = 0.577, grad_norm = 1.321
I0609 02:10:09.658980 140043011815168 logging_writer.py:48] [28] global_step=28, grad_norm=1.305396, loss=0.572661
I0609 02:10:09.663034 140095247849280 submission.py:296] 28) loss = 0.573, grad_norm = 1.305
I0609 02:10:09.969489 140043020207872 logging_writer.py:48] [29] global_step=29, grad_norm=1.296450, loss=0.564685
I0609 02:10:09.973368 140095247849280 submission.py:296] 29) loss = 0.565, grad_norm = 1.296
I0609 02:10:10.274487 140043011815168 logging_writer.py:48] [30] global_step=30, grad_norm=1.274742, loss=0.561090
I0609 02:10:10.278399 140095247849280 submission.py:296] 30) loss = 0.561, grad_norm = 1.275
I0609 02:10:10.585422 140043020207872 logging_writer.py:48] [31] global_step=31, grad_norm=1.260443, loss=0.553606
I0609 02:10:10.589927 140095247849280 submission.py:296] 31) loss = 0.554, grad_norm = 1.260
I0609 02:10:10.893919 140043011815168 logging_writer.py:48] [32] global_step=32, grad_norm=1.243703, loss=0.549819
I0609 02:10:10.898383 140095247849280 submission.py:296] 32) loss = 0.550, grad_norm = 1.244
I0609 02:10:11.203463 140043020207872 logging_writer.py:48] [33] global_step=33, grad_norm=1.209136, loss=0.542517
I0609 02:10:11.207856 140095247849280 submission.py:296] 33) loss = 0.543, grad_norm = 1.209
I0609 02:10:11.511553 140043011815168 logging_writer.py:48] [34] global_step=34, grad_norm=1.172822, loss=0.538636
I0609 02:10:11.515562 140095247849280 submission.py:296] 34) loss = 0.539, grad_norm = 1.173
I0609 02:10:11.817820 140043020207872 logging_writer.py:48] [35] global_step=35, grad_norm=1.137709, loss=0.529409
I0609 02:10:11.821908 140095247849280 submission.py:296] 35) loss = 0.529, grad_norm = 1.138
I0609 02:10:12.126487 140043011815168 logging_writer.py:48] [36] global_step=36, grad_norm=1.103307, loss=0.525088
I0609 02:10:12.130447 140095247849280 submission.py:296] 36) loss = 0.525, grad_norm = 1.103
I0609 02:10:12.437583 140043020207872 logging_writer.py:48] [37] global_step=37, grad_norm=1.078272, loss=0.517478
I0609 02:10:12.441622 140095247849280 submission.py:296] 37) loss = 0.517, grad_norm = 1.078
I0609 02:10:12.744995 140043011815168 logging_writer.py:48] [38] global_step=38, grad_norm=1.040731, loss=0.515144
I0609 02:10:12.748990 140095247849280 submission.py:296] 38) loss = 0.515, grad_norm = 1.041
I0609 02:10:13.051604 140043020207872 logging_writer.py:48] [39] global_step=39, grad_norm=1.009865, loss=0.507464
I0609 02:10:13.055529 140095247849280 submission.py:296] 39) loss = 0.507, grad_norm = 1.010
I0609 02:10:13.358067 140043011815168 logging_writer.py:48] [40] global_step=40, grad_norm=0.961305, loss=0.502158
I0609 02:10:13.362036 140095247849280 submission.py:296] 40) loss = 0.502, grad_norm = 0.961
I0609 02:10:13.667221 140043020207872 logging_writer.py:48] [41] global_step=41, grad_norm=0.951901, loss=0.496573
I0609 02:10:13.671294 140095247849280 submission.py:296] 41) loss = 0.497, grad_norm = 0.952
I0609 02:10:13.994119 140043011815168 logging_writer.py:48] [42] global_step=42, grad_norm=0.916996, loss=0.493771
I0609 02:10:13.998171 140095247849280 submission.py:296] 42) loss = 0.494, grad_norm = 0.917
I0609 02:10:14.298918 140043020207872 logging_writer.py:48] [43] global_step=43, grad_norm=0.908166, loss=0.487518
I0609 02:10:14.302915 140095247849280 submission.py:296] 43) loss = 0.488, grad_norm = 0.908
I0609 02:10:14.606028 140043011815168 logging_writer.py:48] [44] global_step=44, grad_norm=0.898932, loss=0.483846
I0609 02:10:14.609913 140095247849280 submission.py:296] 44) loss = 0.484, grad_norm = 0.899
I0609 02:10:14.912707 140043020207872 logging_writer.py:48] [45] global_step=45, grad_norm=0.979238, loss=0.478492
I0609 02:10:14.916724 140095247849280 submission.py:296] 45) loss = 0.478, grad_norm = 0.979
I0609 02:10:15.217588 140043011815168 logging_writer.py:48] [46] global_step=46, grad_norm=0.956041, loss=0.476440
I0609 02:10:15.221650 140095247849280 submission.py:296] 46) loss = 0.476, grad_norm = 0.956
I0609 02:10:15.522564 140043020207872 logging_writer.py:48] [47] global_step=47, grad_norm=0.911554, loss=0.469676
I0609 02:10:15.526876 140095247849280 submission.py:296] 47) loss = 0.470, grad_norm = 0.912
I0609 02:10:15.829770 140043011815168 logging_writer.py:48] [48] global_step=48, grad_norm=0.883198, loss=0.464484
I0609 02:10:15.833729 140095247849280 submission.py:296] 48) loss = 0.464, grad_norm = 0.883
I0609 02:10:16.133322 140043020207872 logging_writer.py:48] [49] global_step=49, grad_norm=0.844925, loss=0.460262
I0609 02:10:16.137605 140095247849280 submission.py:296] 49) loss = 0.460, grad_norm = 0.845
I0609 02:10:16.433546 140043011815168 logging_writer.py:48] [50] global_step=50, grad_norm=0.807943, loss=0.456091
I0609 02:10:16.437676 140095247849280 submission.py:296] 50) loss = 0.456, grad_norm = 0.808
I0609 02:10:16.742740 140043020207872 logging_writer.py:48] [51] global_step=51, grad_norm=0.762403, loss=0.451372
I0609 02:10:16.747356 140095247849280 submission.py:296] 51) loss = 0.451, grad_norm = 0.762
I0609 02:10:17.075883 140043011815168 logging_writer.py:48] [52] global_step=52, grad_norm=0.747791, loss=0.448562
I0609 02:10:17.080263 140095247849280 submission.py:296] 52) loss = 0.449, grad_norm = 0.748
I0609 02:10:17.412781 140043020207872 logging_writer.py:48] [53] global_step=53, grad_norm=0.736590, loss=0.444046
I0609 02:10:17.417380 140095247849280 submission.py:296] 53) loss = 0.444, grad_norm = 0.737
I0609 02:10:17.733076 140043011815168 logging_writer.py:48] [54] global_step=54, grad_norm=0.723672, loss=0.441225
I0609 02:10:17.738337 140095247849280 submission.py:296] 54) loss = 0.441, grad_norm = 0.724
I0609 02:10:18.043174 140043020207872 logging_writer.py:48] [55] global_step=55, grad_norm=0.692318, loss=0.435913
I0609 02:10:18.047385 140095247849280 submission.py:296] 55) loss = 0.436, grad_norm = 0.692
I0609 02:10:18.350803 140043011815168 logging_writer.py:48] [56] global_step=56, grad_norm=0.687409, loss=0.431552
I0609 02:10:18.354860 140095247849280 submission.py:296] 56) loss = 0.432, grad_norm = 0.687
I0609 02:10:18.656391 140043020207872 logging_writer.py:48] [57] global_step=57, grad_norm=0.668504, loss=0.430894
I0609 02:10:18.660367 140095247849280 submission.py:296] 57) loss = 0.431, grad_norm = 0.669
I0609 02:10:18.970815 140043011815168 logging_writer.py:48] [58] global_step=58, grad_norm=0.634782, loss=0.428354
I0609 02:10:18.974754 140095247849280 submission.py:296] 58) loss = 0.428, grad_norm = 0.635
I0609 02:10:19.277238 140043020207872 logging_writer.py:48] [59] global_step=59, grad_norm=0.616185, loss=0.425079
I0609 02:10:19.281094 140095247849280 submission.py:296] 59) loss = 0.425, grad_norm = 0.616
I0609 02:10:19.582957 140043011815168 logging_writer.py:48] [60] global_step=60, grad_norm=0.588612, loss=0.422144
I0609 02:10:19.587506 140095247849280 submission.py:296] 60) loss = 0.422, grad_norm = 0.589
I0609 02:10:19.903338 140043020207872 logging_writer.py:48] [61] global_step=61, grad_norm=0.578180, loss=0.417638
I0609 02:10:19.907691 140095247849280 submission.py:296] 61) loss = 0.418, grad_norm = 0.578
I0609 02:10:20.215405 140043011815168 logging_writer.py:48] [62] global_step=62, grad_norm=0.614091, loss=0.413253
I0609 02:10:20.219897 140095247849280 submission.py:296] 62) loss = 0.413, grad_norm = 0.614
I0609 02:10:20.522089 140043020207872 logging_writer.py:48] [63] global_step=63, grad_norm=0.606651, loss=0.412158
I0609 02:10:20.526299 140095247849280 submission.py:296] 63) loss = 0.412, grad_norm = 0.607
I0609 02:10:20.835159 140043011815168 logging_writer.py:48] [64] global_step=64, grad_norm=0.593552, loss=0.410270
I0609 02:10:20.839121 140095247849280 submission.py:296] 64) loss = 0.410, grad_norm = 0.594
I0609 02:10:21.143553 140043020207872 logging_writer.py:48] [65] global_step=65, grad_norm=0.579217, loss=0.403458
I0609 02:10:21.147639 140095247849280 submission.py:296] 65) loss = 0.403, grad_norm = 0.579
I0609 02:10:21.450057 140043011815168 logging_writer.py:48] [66] global_step=66, grad_norm=0.558410, loss=0.401787
I0609 02:10:21.453938 140095247849280 submission.py:296] 66) loss = 0.402, grad_norm = 0.558
I0609 02:10:21.752951 140043020207872 logging_writer.py:48] [67] global_step=67, grad_norm=0.551382, loss=0.400249
I0609 02:10:21.757844 140095247849280 submission.py:296] 67) loss = 0.400, grad_norm = 0.551
I0609 02:10:22.059901 140043011815168 logging_writer.py:48] [68] global_step=68, grad_norm=0.534938, loss=0.396561
I0609 02:10:22.064307 140095247849280 submission.py:296] 68) loss = 0.397, grad_norm = 0.535
I0609 02:10:22.363581 140043020207872 logging_writer.py:48] [69] global_step=69, grad_norm=0.524614, loss=0.394667
I0609 02:10:22.368181 140095247849280 submission.py:296] 69) loss = 0.395, grad_norm = 0.525
I0609 02:10:22.669566 140043011815168 logging_writer.py:48] [70] global_step=70, grad_norm=0.519652, loss=0.389928
I0609 02:10:22.673620 140095247849280 submission.py:296] 70) loss = 0.390, grad_norm = 0.520
I0609 02:10:22.980565 140043020207872 logging_writer.py:48] [71] global_step=71, grad_norm=0.512392, loss=0.387278
I0609 02:10:22.984603 140095247849280 submission.py:296] 71) loss = 0.387, grad_norm = 0.512
I0609 02:10:23.289271 140043011815168 logging_writer.py:48] [72] global_step=72, grad_norm=0.494443, loss=0.385558
I0609 02:10:23.293359 140095247849280 submission.py:296] 72) loss = 0.386, grad_norm = 0.494
I0609 02:10:23.598894 140043020207872 logging_writer.py:48] [73] global_step=73, grad_norm=0.487399, loss=0.381891
I0609 02:10:23.602961 140095247849280 submission.py:296] 73) loss = 0.382, grad_norm = 0.487
I0609 02:10:23.936936 140043011815168 logging_writer.py:48] [74] global_step=74, grad_norm=0.473345, loss=0.381337
I0609 02:10:23.940882 140095247849280 submission.py:296] 74) loss = 0.381, grad_norm = 0.473
I0609 02:10:24.277825 140043020207872 logging_writer.py:48] [75] global_step=75, grad_norm=0.473615, loss=0.378006
I0609 02:10:24.282029 140095247849280 submission.py:296] 75) loss = 0.378, grad_norm = 0.474
I0609 02:10:24.583929 140043011815168 logging_writer.py:48] [76] global_step=76, grad_norm=0.459728, loss=0.376516
I0609 02:10:24.587984 140095247849280 submission.py:296] 76) loss = 0.377, grad_norm = 0.460
I0609 02:10:24.900674 140043020207872 logging_writer.py:48] [77] global_step=77, grad_norm=0.459049, loss=0.374801
I0609 02:10:24.905532 140095247849280 submission.py:296] 77) loss = 0.375, grad_norm = 0.459
I0609 02:10:25.238278 140043011815168 logging_writer.py:48] [78] global_step=78, grad_norm=0.446809, loss=0.373923
I0609 02:10:25.242839 140095247849280 submission.py:296] 78) loss = 0.374, grad_norm = 0.447
I0609 02:10:25.548446 140043020207872 logging_writer.py:48] [79] global_step=79, grad_norm=0.450370, loss=0.369068
I0609 02:10:25.552906 140095247849280 submission.py:296] 79) loss = 0.369, grad_norm = 0.450
I0609 02:10:25.850866 140043011815168 logging_writer.py:48] [80] global_step=80, grad_norm=0.445426, loss=0.368354
I0609 02:10:25.854847 140095247849280 submission.py:296] 80) loss = 0.368, grad_norm = 0.445
I0609 02:10:26.159617 140043020207872 logging_writer.py:48] [81] global_step=81, grad_norm=0.440073, loss=0.366690
I0609 02:10:26.164336 140095247849280 submission.py:296] 81) loss = 0.367, grad_norm = 0.440
I0609 02:10:26.473186 140043011815168 logging_writer.py:48] [82] global_step=82, grad_norm=0.434067, loss=0.363332
I0609 02:10:26.477061 140095247849280 submission.py:296] 82) loss = 0.363, grad_norm = 0.434
I0609 02:10:26.782319 140043020207872 logging_writer.py:48] [83] global_step=83, grad_norm=0.431309, loss=0.362718
I0609 02:10:26.786237 140095247849280 submission.py:296] 83) loss = 0.363, grad_norm = 0.431
I0609 02:10:27.100527 140043011815168 logging_writer.py:48] [84] global_step=84, grad_norm=0.424220, loss=0.361432
I0609 02:10:27.104397 140095247849280 submission.py:296] 84) loss = 0.361, grad_norm = 0.424
I0609 02:10:27.416703 140043020207872 logging_writer.py:48] [85] global_step=85, grad_norm=0.418952, loss=0.358341
I0609 02:10:27.420843 140095247849280 submission.py:296] 85) loss = 0.358, grad_norm = 0.419
I0609 02:10:27.732747 140043011815168 logging_writer.py:48] [86] global_step=86, grad_norm=0.412703, loss=0.355979
I0609 02:10:27.736636 140095247849280 submission.py:296] 86) loss = 0.356, grad_norm = 0.413
I0609 02:10:28.039136 140043020207872 logging_writer.py:48] [87] global_step=87, grad_norm=0.407683, loss=0.356138
I0609 02:10:28.043187 140095247849280 submission.py:296] 87) loss = 0.356, grad_norm = 0.408
I0609 02:10:28.352987 140043011815168 logging_writer.py:48] [88] global_step=88, grad_norm=0.403075, loss=0.354629
I0609 02:10:28.356895 140095247849280 submission.py:296] 88) loss = 0.355, grad_norm = 0.403
I0609 02:10:28.663465 140043020207872 logging_writer.py:48] [89] global_step=89, grad_norm=0.406876, loss=0.348287
I0609 02:10:28.667516 140095247849280 submission.py:296] 89) loss = 0.348, grad_norm = 0.407
I0609 02:10:28.972995 140043011815168 logging_writer.py:48] [90] global_step=90, grad_norm=0.400474, loss=0.348885
I0609 02:10:28.977608 140095247849280 submission.py:296] 90) loss = 0.349, grad_norm = 0.400
I0609 02:10:29.281990 140043020207872 logging_writer.py:48] [91] global_step=91, grad_norm=0.397047, loss=0.346828
I0609 02:10:29.286186 140095247849280 submission.py:296] 91) loss = 0.347, grad_norm = 0.397
I0609 02:10:29.593293 140043011815168 logging_writer.py:48] [92] global_step=92, grad_norm=0.395692, loss=0.346519
I0609 02:10:29.597478 140095247849280 submission.py:296] 92) loss = 0.347, grad_norm = 0.396
I0609 02:10:29.897348 140043020207872 logging_writer.py:48] [93] global_step=93, grad_norm=0.395300, loss=0.341098
I0609 02:10:29.901285 140095247849280 submission.py:296] 93) loss = 0.341, grad_norm = 0.395
I0609 02:10:30.209926 140043011815168 logging_writer.py:48] [94] global_step=94, grad_norm=0.390746, loss=0.339689
I0609 02:10:30.214091 140095247849280 submission.py:296] 94) loss = 0.340, grad_norm = 0.391
I0609 02:10:30.519831 140043020207872 logging_writer.py:48] [95] global_step=95, grad_norm=0.387927, loss=0.340169
I0609 02:10:30.523899 140095247849280 submission.py:296] 95) loss = 0.340, grad_norm = 0.388
I0609 02:10:30.832056 140043011815168 logging_writer.py:48] [96] global_step=96, grad_norm=0.388219, loss=0.336260
I0609 02:10:30.836066 140095247849280 submission.py:296] 96) loss = 0.336, grad_norm = 0.388
I0609 02:10:31.154345 140043020207872 logging_writer.py:48] [97] global_step=97, grad_norm=0.382243, loss=0.337847
I0609 02:10:31.158409 140095247849280 submission.py:296] 97) loss = 0.338, grad_norm = 0.382
I0609 02:10:31.459840 140043011815168 logging_writer.py:48] [98] global_step=98, grad_norm=0.381679, loss=0.332631
I0609 02:10:31.464077 140095247849280 submission.py:296] 98) loss = 0.333, grad_norm = 0.382
I0609 02:10:31.770966 140043020207872 logging_writer.py:48] [99] global_step=99, grad_norm=0.376212, loss=0.335372
I0609 02:10:31.774878 140095247849280 submission.py:296] 99) loss = 0.335, grad_norm = 0.376
I0609 02:10:32.083346 140043011815168 logging_writer.py:48] [100] global_step=100, grad_norm=0.375604, loss=0.331581
I0609 02:10:32.087865 140095247849280 submission.py:296] 100) loss = 0.332, grad_norm = 0.376
I0609 02:12:33.413802 140043020207872 logging_writer.py:48] [500] global_step=500, grad_norm=0.073025, loss=0.061276
I0609 02:12:33.419092 140095247849280 submission.py:296] 500) loss = 0.061, grad_norm = 0.073
I0609 02:14:00.959445 140095247849280 spec.py:298] Evaluating on the training split.
I0609 02:15:00.121877 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 02:15:03.588896 140095247849280 spec.py:326] Evaluating on the test split.
I0609 02:15:06.921413 140095247849280 submission_runner.py:419] Time since start: 463.51s, 	Step: 789, 	{'train/accuracy': 0.9868888423337463, 'train/loss': 0.050891767530308316, 'train/mean_average_precision': 0.05754635961717144, 'validation/accuracy': 0.9842831939738915, 'validation/loss': 0.06040381103216918, 'validation/mean_average_precision': 0.05773285715726611, 'validation/num_examples': 43793, 'test/accuracy': 0.9832857313140136, 'test/loss': 0.06361206584831347, 'test/mean_average_precision': 0.05780872142012332, 'test/num_examples': 43793, 'score': 245.12684321403503, 'total_duration': 463.50777864456177, 'accumulated_submission_time': 245.12684321403503, 'accumulated_eval_time': 218.1328067779541, 'accumulated_logging_time': 0.028039932250976562}
I0609 02:15:06.931554 140043011815168 logging_writer.py:48] [789] accumulated_eval_time=218.132807, accumulated_logging_time=0.028040, accumulated_submission_time=245.126843, global_step=789, preemption_count=0, score=245.126843, test/accuracy=0.983286, test/loss=0.063612, test/mean_average_precision=0.057809, test/num_examples=43793, total_duration=463.507779, train/accuracy=0.986889, train/loss=0.050892, train/mean_average_precision=0.057546, validation/accuracy=0.984283, validation/loss=0.060404, validation/mean_average_precision=0.057733, validation/num_examples=43793
I0609 02:16:11.855603 140043020207872 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.065803, loss=0.049592
I0609 02:16:11.861014 140095247849280 submission.py:296] 1000) loss = 0.050, grad_norm = 0.066
I0609 02:18:44.582034 140043011815168 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.038261, loss=0.046982
I0609 02:18:44.588257 140095247849280 submission.py:296] 1500) loss = 0.047, grad_norm = 0.038
I0609 02:19:07.051467 140095247849280 spec.py:298] Evaluating on the training split.
I0609 02:20:06.642544 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 02:20:09.962142 140095247849280 spec.py:326] Evaluating on the test split.
I0609 02:20:13.197493 140095247849280 submission_runner.py:419] Time since start: 769.78s, 	Step: 1575, 	{'train/accuracy': 0.9876673837997453, 'train/loss': 0.044747687905035395, 'train/mean_average_precision': 0.12931696999947206, 'validation/accuracy': 0.9848657188763237, 'validation/loss': 0.054504972570587, 'validation/mean_average_precision': 0.12735361186641733, 'validation/num_examples': 43793, 'test/accuracy': 0.9838842489614392, 'test/loss': 0.057666053898599065, 'test/mean_average_precision': 0.1283931638383857, 'test/num_examples': 43793, 'score': 485.0207579135895, 'total_duration': 769.7838649749756, 'accumulated_submission_time': 485.0207579135895, 'accumulated_eval_time': 284.27860498428345, 'accumulated_logging_time': 0.049570322036743164}
I0609 02:20:13.207622 140043020207872 logging_writer.py:48] [1575] accumulated_eval_time=284.278605, accumulated_logging_time=0.049570, accumulated_submission_time=485.020758, global_step=1575, preemption_count=0, score=485.020758, test/accuracy=0.983884, test/loss=0.057666, test/mean_average_precision=0.128393, test/num_examples=43793, total_duration=769.783865, train/accuracy=0.987667, train/loss=0.044748, train/mean_average_precision=0.129317, validation/accuracy=0.984866, validation/loss=0.054505, validation/mean_average_precision=0.127354, validation/num_examples=43793
I0609 02:22:23.954385 140043011815168 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.045524, loss=0.042147
I0609 02:22:23.960274 140095247849280 submission.py:296] 2000) loss = 0.042, grad_norm = 0.046
I0609 02:24:13.494254 140095247849280 spec.py:298] Evaluating on the training split.
I0609 02:25:13.319079 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 02:25:16.620485 140095247849280 spec.py:326] Evaluating on the test split.
I0609 02:25:19.906723 140095247849280 submission_runner.py:419] Time since start: 1076.49s, 	Step: 2354, 	{'train/accuracy': 0.9880959552351694, 'train/loss': 0.041956090632875705, 'train/mean_average_precision': 0.16871082212526506, 'validation/accuracy': 0.9853110358226429, 'validation/loss': 0.05087210460259623, 'validation/mean_average_precision': 0.15437284529072448, 'validation/num_examples': 43793, 'test/accuracy': 0.9844044243974495, 'test/loss': 0.05329504024304618, 'test/mean_average_precision': 0.15787206284380537, 'test/num_examples': 43793, 'score': 725.0844688415527, 'total_duration': 1076.4930248260498, 'accumulated_submission_time': 725.0844688415527, 'accumulated_eval_time': 350.6907727718353, 'accumulated_logging_time': 0.07200932502746582}
I0609 02:25:19.917711 140043020207872 logging_writer.py:48] [2354] accumulated_eval_time=350.690773, accumulated_logging_time=0.072009, accumulated_submission_time=725.084469, global_step=2354, preemption_count=0, score=725.084469, test/accuracy=0.984404, test/loss=0.053295, test/mean_average_precision=0.157872, test/num_examples=43793, total_duration=1076.493025, train/accuracy=0.988096, train/loss=0.041956, train/mean_average_precision=0.168711, validation/accuracy=0.985311, validation/loss=0.050872, validation/mean_average_precision=0.154373, validation/num_examples=43793
I0609 02:26:05.300311 140043011815168 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.057593, loss=0.042408
I0609 02:26:05.305759 140095247849280 submission.py:296] 2500) loss = 0.042, grad_norm = 0.058
I0609 02:28:37.759200 140043020207872 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.032112, loss=0.043902
I0609 02:28:37.764568 140095247849280 submission.py:296] 3000) loss = 0.044, grad_norm = 0.032
I0609 02:29:20.058389 140095247849280 spec.py:298] Evaluating on the training split.
I0609 02:30:21.571398 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 02:30:24.913278 140095247849280 spec.py:326] Evaluating on the test split.
I0609 02:30:28.136808 140095247849280 submission_runner.py:419] Time since start: 1384.72s, 	Step: 3140, 	{'train/accuracy': 0.9884417992501715, 'train/loss': 0.0404199420635359, 'train/mean_average_precision': 0.20081693065024164, 'validation/accuracy': 0.9855123824091282, 'validation/loss': 0.04949692022940521, 'validation/mean_average_precision': 0.18186784334096356, 'validation/num_examples': 43793, 'test/accuracy': 0.9846605107659467, 'test/loss': 0.05190346236246414, 'test/mean_average_precision': 0.18450855015731862, 'test/num_examples': 43793, 'score': 965.003059387207, 'total_duration': 1384.7231750488281, 'accumulated_submission_time': 965.003059387207, 'accumulated_eval_time': 418.7689526081085, 'accumulated_logging_time': 0.09376406669616699}
I0609 02:30:28.153812 140043011815168 logging_writer.py:48] [3140] accumulated_eval_time=418.768953, accumulated_logging_time=0.093764, accumulated_submission_time=965.003059, global_step=3140, preemption_count=0, score=965.003059, test/accuracy=0.984661, test/loss=0.051903, test/mean_average_precision=0.184509, test/num_examples=43793, total_duration=1384.723175, train/accuracy=0.988442, train/loss=0.040420, train/mean_average_precision=0.200817, validation/accuracy=0.985512, validation/loss=0.049497, validation/mean_average_precision=0.181868, validation/num_examples=43793
I0609 02:32:19.304387 140043020207872 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.039663, loss=0.041416
I0609 02:32:19.310108 140095247849280 submission.py:296] 3500) loss = 0.041, grad_norm = 0.040
I0609 02:34:28.360579 140095247849280 spec.py:298] Evaluating on the training split.
I0609 02:35:29.895577 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 02:35:33.194584 140095247849280 spec.py:326] Evaluating on the test split.
I0609 02:35:36.444961 140095247849280 submission_runner.py:419] Time since start: 1693.03s, 	Step: 3921, 	{'train/accuracy': 0.9885749672063253, 'train/loss': 0.03920362561532429, 'train/mean_average_precision': 0.21566386001360677, 'validation/accuracy': 0.9856296992710117, 'validation/loss': 0.04865926794075214, 'validation/mean_average_precision': 0.19134784645743758, 'validation/num_examples': 43793, 'test/accuracy': 0.9847645458531488, 'test/loss': 0.05121565473239606, 'test/mean_average_precision': 0.19510212191383533, 'test/num_examples': 43793, 'score': 1204.9887940883636, 'total_duration': 1693.0312807559967, 'accumulated_submission_time': 1204.9887940883636, 'accumulated_eval_time': 486.85308170318604, 'accumulated_logging_time': 0.12163877487182617}
I0609 02:35:36.455240 140043011815168 logging_writer.py:48] [3921] accumulated_eval_time=486.853082, accumulated_logging_time=0.121639, accumulated_submission_time=1204.988794, global_step=3921, preemption_count=0, score=1204.988794, test/accuracy=0.984765, test/loss=0.051216, test/mean_average_precision=0.195102, test/num_examples=43793, total_duration=1693.031281, train/accuracy=0.988575, train/loss=0.039204, train/mean_average_precision=0.215664, validation/accuracy=0.985630, validation/loss=0.048659, validation/mean_average_precision=0.191348, validation/num_examples=43793
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0609 02:36:01.065329 140043020207872 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.023462, loss=0.036821
I0609 02:36:01.070180 140095247849280 submission.py:296] 4000) loss = 0.037, grad_norm = 0.023
I0609 02:38:33.948236 140043011815168 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.019736, loss=0.040446
I0609 02:38:33.954589 140095247849280 submission.py:296] 4500) loss = 0.040, grad_norm = 0.020
I0609 02:39:36.566011 140095247849280 spec.py:298] Evaluating on the training split.
I0609 02:40:38.861837 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 02:40:42.197147 140095247849280 spec.py:326] Evaluating on the test split.
I0609 02:40:45.436064 140095247849280 submission_runner.py:419] Time since start: 2002.02s, 	Step: 4704, 	{'train/accuracy': 0.9887303204676593, 'train/loss': 0.038383482624129404, 'train/mean_average_precision': 0.23389723897156262, 'validation/accuracy': 0.985982867678758, 'validation/loss': 0.047629707186855316, 'validation/mean_average_precision': 0.21267790352150492, 'validation/num_examples': 43793, 'test/accuracy': 0.9850400071771575, 'test/loss': 0.05040385462633924, 'test/mean_average_precision': 0.208387699800446, 'test/num_examples': 43793, 'score': 1444.8746311664581, 'total_duration': 2002.0224206447601, 'accumulated_submission_time': 1444.8746311664581, 'accumulated_eval_time': 555.7229084968567, 'accumulated_logging_time': 0.14267683029174805}
I0609 02:40:45.446820 140043020207872 logging_writer.py:48] [4704] accumulated_eval_time=555.722908, accumulated_logging_time=0.142677, accumulated_submission_time=1444.874631, global_step=4704, preemption_count=0, score=1444.874631, test/accuracy=0.985040, test/loss=0.050404, test/mean_average_precision=0.208388, test/num_examples=43793, total_duration=2002.022421, train/accuracy=0.988730, train/loss=0.038383, train/mean_average_precision=0.233897, validation/accuracy=0.985983, validation/loss=0.047630, validation/mean_average_precision=0.212678, validation/num_examples=43793
I0609 02:42:17.337963 140043011815168 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.014627, loss=0.038927
I0609 02:42:17.343281 140095247849280 submission.py:296] 5000) loss = 0.039, grad_norm = 0.015
I0609 02:44:45.517978 140095247849280 spec.py:298] Evaluating on the training split.
I0609 02:45:48.310993 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 02:45:51.673196 140095247849280 spec.py:326] Evaluating on the test split.
I0609 02:45:54.953601 140095247849280 submission_runner.py:419] Time since start: 2311.54s, 	Step: 5480, 	{'train/accuracy': 0.9890885612022163, 'train/loss': 0.037013026099527155, 'train/mean_average_precision': 0.26830163077039987, 'validation/accuracy': 0.9860766399801252, 'validation/loss': 0.04734119117817793, 'validation/mean_average_precision': 0.2180545731845939, 'validation/num_examples': 43793, 'test/accuracy': 0.9851886889009724, 'test/loss': 0.04995108679285098, 'test/mean_average_precision': 0.22250528495295885, 'test/num_examples': 43793, 'score': 1684.7194366455078, 'total_duration': 2311.539963245392, 'accumulated_submission_time': 1684.7194366455078, 'accumulated_eval_time': 625.1582984924316, 'accumulated_logging_time': 0.16562724113464355}
I0609 02:45:54.964225 140043020207872 logging_writer.py:48] [5480] accumulated_eval_time=625.158298, accumulated_logging_time=0.165627, accumulated_submission_time=1684.719437, global_step=5480, preemption_count=0, score=1684.719437, test/accuracy=0.985189, test/loss=0.049951, test/mean_average_precision=0.222505, test/num_examples=43793, total_duration=2311.539963, train/accuracy=0.989089, train/loss=0.037013, train/mean_average_precision=0.268302, validation/accuracy=0.986077, validation/loss=0.047341, validation/mean_average_precision=0.218055, validation/num_examples=43793
I0609 02:46:01.660736 140043011815168 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.014940, loss=0.037721
I0609 02:46:01.680275 140095247849280 submission.py:296] 5500) loss = 0.038, grad_norm = 0.015
I0609 02:48:36.889695 140043020207872 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.012594, loss=0.037330
I0609 02:48:36.902264 140095247849280 submission.py:296] 6000) loss = 0.037, grad_norm = 0.013
I0609 02:49:55.177518 140095247849280 spec.py:298] Evaluating on the training split.
I0609 02:50:57.785347 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 02:51:01.154879 140095247849280 spec.py:326] Evaluating on the test split.
I0609 02:51:04.484971 140095247849280 submission_runner.py:419] Time since start: 2621.07s, 	Step: 6255, 	{'train/accuracy': 0.9894808684290851, 'train/loss': 0.035886646079468044, 'train/mean_average_precision': 0.30287925980150143, 'validation/accuracy': 0.9862804222108018, 'validation/loss': 0.04616456889199298, 'validation/mean_average_precision': 0.22475821437284701, 'validation/num_examples': 43793, 'test/accuracy': 0.9854249791192735, 'test/loss': 0.048679582303336834, 'test/mean_average_precision': 0.2308614455919447, 'test/num_examples': 43793, 'score': 1924.7050595283508, 'total_duration': 2621.0713183879852, 'accumulated_submission_time': 1924.7050595283508, 'accumulated_eval_time': 694.4654920101166, 'accumulated_logging_time': 0.18977737426757812}
I0609 02:51:04.495797 140043011815168 logging_writer.py:48] [6255] accumulated_eval_time=694.465492, accumulated_logging_time=0.189777, accumulated_submission_time=1924.705060, global_step=6255, preemption_count=0, score=1924.705060, test/accuracy=0.985425, test/loss=0.048680, test/mean_average_precision=0.230861, test/num_examples=43793, total_duration=2621.071318, train/accuracy=0.989481, train/loss=0.035887, train/mean_average_precision=0.302879, validation/accuracy=0.986280, validation/loss=0.046165, validation/mean_average_precision=0.224758, validation/num_examples=43793
I0609 02:52:20.933355 140043020207872 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.014696, loss=0.042374
I0609 02:52:20.942165 140095247849280 submission.py:296] 6500) loss = 0.042, grad_norm = 0.015
I0609 02:54:54.056641 140043011815168 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.016294, loss=0.042433
I0609 02:54:54.063264 140095247849280 submission.py:296] 7000) loss = 0.042, grad_norm = 0.016
I0609 02:55:04.508281 140095247849280 spec.py:298] Evaluating on the training split.
I0609 02:56:07.596006 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 02:56:10.930022 140095247849280 spec.py:326] Evaluating on the test split.
I0609 02:56:14.228643 140095247849280 submission_runner.py:419] Time since start: 2930.81s, 	Step: 7035, 	{'train/accuracy': 0.9896762237664584, 'train/loss': 0.03495039271024056, 'train/mean_average_precision': 0.3036100468826698, 'validation/accuracy': 0.9863193925178634, 'validation/loss': 0.04572084399435093, 'validation/mean_average_precision': 0.23492406651732037, 'validation/num_examples': 43793, 'test/accuracy': 0.9855041637200589, 'test/loss': 0.04830867760558403, 'test/mean_average_precision': 0.23424573434369703, 'test/num_examples': 43793, 'score': 2164.4966411590576, 'total_duration': 2930.814989566803, 'accumulated_submission_time': 2164.4966411590576, 'accumulated_eval_time': 764.1855943202972, 'accumulated_logging_time': 0.21167516708374023}
I0609 02:56:14.239111 140043020207872 logging_writer.py:48] [7035] accumulated_eval_time=764.185594, accumulated_logging_time=0.211675, accumulated_submission_time=2164.496641, global_step=7035, preemption_count=0, score=2164.496641, test/accuracy=0.985504, test/loss=0.048309, test/mean_average_precision=0.234246, test/num_examples=43793, total_duration=2930.814990, train/accuracy=0.989676, train/loss=0.034950, train/mean_average_precision=0.303610, validation/accuracy=0.986319, validation/loss=0.045721, validation/mean_average_precision=0.234924, validation/num_examples=43793
I0609 02:58:37.800648 140043011815168 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.010401, loss=0.034447
I0609 02:58:37.810971 140095247849280 submission.py:296] 7500) loss = 0.034, grad_norm = 0.010
I0609 03:00:14.286502 140095247849280 spec.py:298] Evaluating on the training split.
I0609 03:01:17.697223 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 03:01:21.068755 140095247849280 spec.py:326] Evaluating on the test split.
I0609 03:01:24.342225 140095247849280 submission_runner.py:419] Time since start: 3240.93s, 	Step: 7813, 	{'train/accuracy': 0.9898562546307184, 'train/loss': 0.034082615951219655, 'train/mean_average_precision': 0.3246106168083517, 'validation/accuracy': 0.9864427984902253, 'validation/loss': 0.04562536864489688, 'validation/mean_average_precision': 0.23816353854565617, 'validation/num_examples': 43793, 'test/accuracy': 0.9855517587194671, 'test/loss': 0.04835256411741391, 'test/mean_average_precision': 0.24056227061529384, 'test/num_examples': 43793, 'score': 2404.321559906006, 'total_duration': 3240.928556203842, 'accumulated_submission_time': 2404.321559906006, 'accumulated_eval_time': 834.2410452365875, 'accumulated_logging_time': 0.2335340976715088}
I0609 03:01:24.353217 140043020207872 logging_writer.py:48] [7813] accumulated_eval_time=834.241045, accumulated_logging_time=0.233534, accumulated_submission_time=2404.321560, global_step=7813, preemption_count=0, score=2404.321560, test/accuracy=0.985552, test/loss=0.048353, test/mean_average_precision=0.240562, test/num_examples=43793, total_duration=3240.928556, train/accuracy=0.989856, train/loss=0.034083, train/mean_average_precision=0.324611, validation/accuracy=0.986443, validation/loss=0.045625, validation/mean_average_precision=0.238164, validation/num_examples=43793
I0609 03:02:22.691845 140043011815168 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.010698, loss=0.038731
I0609 03:02:22.697405 140095247849280 submission.py:296] 8000) loss = 0.039, grad_norm = 0.011
I0609 03:04:55.803920 140043020207872 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.008998, loss=0.034638
I0609 03:04:55.810459 140095247849280 submission.py:296] 8500) loss = 0.035, grad_norm = 0.009
I0609 03:05:24.530531 140095247849280 spec.py:298] Evaluating on the training split.
I0609 03:06:27.032949 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 03:06:30.393815 140095247849280 spec.py:326] Evaluating on the test split.
I0609 03:06:33.689473 140095247849280 submission_runner.py:419] Time since start: 3550.28s, 	Step: 8595, 	{'train/accuracy': 0.9901023876667584, 'train/loss': 0.03316929786713535, 'train/mean_average_precision': 0.3586511718286017, 'validation/accuracy': 0.9865828480312282, 'validation/loss': 0.04513157032780523, 'validation/mean_average_precision': 0.2407911281574001, 'validation/num_examples': 43793, 'test/accuracy': 0.985728660487179, 'test/loss': 0.047949724307018916, 'test/mean_average_precision': 0.24895679353933867, 'test/num_examples': 43793, 'score': 2644.275691986084, 'total_duration': 3550.2758316993713, 'accumulated_submission_time': 2644.275691986084, 'accumulated_eval_time': 903.3997957706451, 'accumulated_logging_time': 0.25627946853637695}
I0609 03:06:33.700174 140043011815168 logging_writer.py:48] [8595] accumulated_eval_time=903.399796, accumulated_logging_time=0.256279, accumulated_submission_time=2644.275692, global_step=8595, preemption_count=0, score=2644.275692, test/accuracy=0.985729, test/loss=0.047950, test/mean_average_precision=0.248957, test/num_examples=43793, total_duration=3550.275832, train/accuracy=0.990102, train/loss=0.033169, train/mean_average_precision=0.358651, validation/accuracy=0.986583, validation/loss=0.045132, validation/mean_average_precision=0.240791, validation/num_examples=43793
I0609 03:08:39.438523 140043020207872 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.010037, loss=0.033630
I0609 03:08:39.444902 140095247849280 submission.py:296] 9000) loss = 0.034, grad_norm = 0.010
I0609 03:10:33.718140 140095247849280 spec.py:298] Evaluating on the training split.
I0609 03:11:37.402395 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 03:11:40.813237 140095247849280 spec.py:326] Evaluating on the test split.
I0609 03:11:44.164720 140095247849280 submission_runner.py:419] Time since start: 3860.75s, 	Step: 9364, 	{'train/accuracy': 0.990299959840598, 'train/loss': 0.03231176891343534, 'train/mean_average_precision': 0.37793907009037014, 'validation/accuracy': 0.9866380559662322, 'validation/loss': 0.0449086486274739, 'validation/mean_average_precision': 0.25155061098357034, 'validation/num_examples': 43793, 'test/accuracy': 0.9857644620354065, 'test/loss': 0.04760564115518539, 'test/mean_average_precision': 0.24685573518460793, 'test/num_examples': 43793, 'score': 2884.0713441371918, 'total_duration': 3860.7510528564453, 'accumulated_submission_time': 2884.0713441371918, 'accumulated_eval_time': 973.8462240695953, 'accumulated_logging_time': 0.27837252616882324}
I0609 03:11:44.176372 140043011815168 logging_writer.py:48] [9364] accumulated_eval_time=973.846224, accumulated_logging_time=0.278373, accumulated_submission_time=2884.071344, global_step=9364, preemption_count=0, score=2884.071344, test/accuracy=0.985764, test/loss=0.047606, test/mean_average_precision=0.246856, test/num_examples=43793, total_duration=3860.751053, train/accuracy=0.990300, train/loss=0.032312, train/mean_average_precision=0.377939, validation/accuracy=0.986638, validation/loss=0.044909, validation/mean_average_precision=0.251551, validation/num_examples=43793
I0609 03:12:26.349819 140043020207872 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.008848, loss=0.033309
I0609 03:12:26.355250 140095247849280 submission.py:296] 9500) loss = 0.033, grad_norm = 0.009
I0609 03:15:02.601400 140043011815168 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.012287, loss=0.035311
I0609 03:15:02.614603 140095247849280 submission.py:296] 10000) loss = 0.035, grad_norm = 0.012
I0609 03:15:44.193920 140095247849280 spec.py:298] Evaluating on the training split.
I0609 03:16:47.697581 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 03:16:51.123165 140095247849280 spec.py:326] Evaluating on the test split.
I0609 03:16:54.453441 140095247849280 submission_runner.py:419] Time since start: 4171.04s, 	Step: 10135, 	{'train/accuracy': 0.9907440419352213, 'train/loss': 0.030976964732994463, 'train/mean_average_precision': 0.40576625520272935, 'validation/accuracy': 0.98666687775583, 'validation/loss': 0.044786045022273964, 'validation/mean_average_precision': 0.25712484156063153, 'validation/num_examples': 43793, 'test/accuracy': 0.9857985788048937, 'test/loss': 0.04764214030710989, 'test/mean_average_precision': 0.2465889895089997, 'test/num_examples': 43793, 'score': 3123.866085290909, 'total_duration': 4171.0398025512695, 'accumulated_submission_time': 3123.866085290909, 'accumulated_eval_time': 1044.1055204868317, 'accumulated_logging_time': 0.30158519744873047}
I0609 03:16:54.470843 140043020207872 logging_writer.py:48] [10135] accumulated_eval_time=1044.105520, accumulated_logging_time=0.301585, accumulated_submission_time=3123.866085, global_step=10135, preemption_count=0, score=3123.866085, test/accuracy=0.985799, test/loss=0.047642, test/mean_average_precision=0.246589, test/num_examples=43793, total_duration=4171.039803, train/accuracy=0.990744, train/loss=0.030977, train/mean_average_precision=0.405766, validation/accuracy=0.986667, validation/loss=0.044786, validation/mean_average_precision=0.257125, validation/num_examples=43793
I0609 03:18:47.188266 140043011815168 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.009035, loss=0.035975
I0609 03:18:47.194287 140095247849280 submission.py:296] 10500) loss = 0.036, grad_norm = 0.009
I0609 03:20:54.530721 140095247849280 spec.py:298] Evaluating on the training split.
I0609 03:21:57.999144 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 03:22:01.535186 140095247849280 spec.py:326] Evaluating on the test split.
I0609 03:22:04.952295 140095247849280 submission_runner.py:419] Time since start: 4481.54s, 	Step: 10915, 	{'train/accuracy': 0.9908547530472688, 'train/loss': 0.030363320621552788, 'train/mean_average_precision': 0.4154861075444643, 'validation/accuracy': 0.9866831153837723, 'validation/loss': 0.044724944604317425, 'validation/mean_average_precision': 0.2543932423245059, 'validation/num_examples': 43793, 'test/accuracy': 0.9858520705298924, 'test/loss': 0.047500701157527234, 'test/mean_average_precision': 0.2500418608216621, 'test/num_examples': 43793, 'score': 3363.702891111374, 'total_duration': 4481.538646697998, 'accumulated_submission_time': 3363.702891111374, 'accumulated_eval_time': 1114.526871919632, 'accumulated_logging_time': 0.33295607566833496}
I0609 03:22:04.962976 140043020207872 logging_writer.py:48] [10915] accumulated_eval_time=1114.526872, accumulated_logging_time=0.332956, accumulated_submission_time=3363.702891, global_step=10915, preemption_count=0, score=3363.702891, test/accuracy=0.985852, test/loss=0.047501, test/mean_average_precision=0.250042, test/num_examples=43793, total_duration=4481.538647, train/accuracy=0.990855, train/loss=0.030363, train/mean_average_precision=0.415486, validation/accuracy=0.986683, validation/loss=0.044725, validation/mean_average_precision=0.254393, validation/num_examples=43793
I0609 03:22:31.022632 140043011815168 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.009590, loss=0.035739
I0609 03:22:31.028704 140095247849280 submission.py:296] 11000) loss = 0.036, grad_norm = 0.010
I0609 03:25:04.081006 140043020207872 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.014359, loss=0.035608
I0609 03:25:04.089091 140095247849280 submission.py:296] 11500) loss = 0.036, grad_norm = 0.014
I0609 03:26:05.016191 140095247849280 spec.py:298] Evaluating on the training split.
I0609 03:27:08.143396 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 03:27:11.577965 140095247849280 spec.py:326] Evaluating on the test split.
I0609 03:27:14.936880 140095247849280 submission_runner.py:419] Time since start: 4791.52s, 	Step: 11696, 	{'train/accuracy': 0.990901861970573, 'train/loss': 0.029993523417232663, 'train/mean_average_precision': 0.43391832152873333, 'validation/accuracy': 0.9867951550165746, 'validation/loss': 0.044948532301107326, 'validation/mean_average_precision': 0.2601841254134853, 'validation/num_examples': 43793, 'test/accuracy': 0.9859413638031185, 'test/loss': 0.04750883547988185, 'test/mean_average_precision': 0.255417884907817, 'test/num_examples': 43793, 'score': 3603.5317878723145, 'total_duration': 4791.523145198822, 'accumulated_submission_time': 3603.5317878723145, 'accumulated_eval_time': 1184.4472608566284, 'accumulated_logging_time': 0.35623693466186523}
I0609 03:27:14.948197 140043011815168 logging_writer.py:48] [11696] accumulated_eval_time=1184.447261, accumulated_logging_time=0.356237, accumulated_submission_time=3603.531788, global_step=11696, preemption_count=0, score=3603.531788, test/accuracy=0.985941, test/loss=0.047509, test/mean_average_precision=0.255418, test/num_examples=43793, total_duration=4791.523145, train/accuracy=0.990902, train/loss=0.029994, train/mean_average_precision=0.433918, validation/accuracy=0.986795, validation/loss=0.044949, validation/mean_average_precision=0.260184, validation/num_examples=43793
I0609 03:28:47.516040 140095247849280 spec.py:298] Evaluating on the training split.
I0609 03:29:50.363215 140095247849280 spec.py:310] Evaluating on the validation split.
I0609 03:29:53.801412 140095247849280 spec.py:326] Evaluating on the test split.
I0609 03:29:57.131294 140095247849280 submission_runner.py:419] Time since start: 4953.72s, 	Step: 12000, 	{'train/accuracy': 0.9909722521951516, 'train/loss': 0.029621974146549986, 'train/mean_average_precision': 0.45027297711484954, 'validation/accuracy': 0.9867813530328235, 'validation/loss': 0.044844379969221575, 'validation/mean_average_precision': 0.26353633516975716, 'validation/num_examples': 43793, 'test/accuracy': 0.9858828177418995, 'test/loss': 0.04779303330302136, 'test/mean_average_precision': 0.25026943251709327, 'test/num_examples': 43793, 'score': 3696.006712913513, 'total_duration': 4953.71764421463, 'accumulated_submission_time': 3696.006712913513, 'accumulated_eval_time': 1254.0622725486755, 'accumulated_logging_time': 0.3793489933013916}
I0609 03:29:57.142297 140043020207872 logging_writer.py:48] [12000] accumulated_eval_time=1254.062273, accumulated_logging_time=0.379349, accumulated_submission_time=3696.006713, global_step=12000, preemption_count=0, score=3696.006713, test/accuracy=0.985883, test/loss=0.047793, test/mean_average_precision=0.250269, test/num_examples=43793, total_duration=4953.717644, train/accuracy=0.990972, train/loss=0.029622, train/mean_average_precision=0.450273, validation/accuracy=0.986781, validation/loss=0.044844, validation/mean_average_precision=0.263536, validation/num_examples=43793
I0609 03:29:57.163494 140043011815168 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3696.006713
I0609 03:29:57.578220 140095247849280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/nadamw/ogbg_pytorch/trial_1/checkpoint_12000.
I0609 03:29:57.774329 140095247849280 submission_runner.py:581] Tuning trial 1/1
I0609 03:29:57.774570 140095247849280 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0609 03:29:57.776539 140095247849280 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5109300492995043, 'train/loss': 0.7192444340365306, 'train/mean_average_precision': 0.021513561790081362, 'validation/accuracy': 0.5043878130107241, 'validation/loss': 0.7227870041332882, 'validation/mean_average_precision': 0.024981593475365164, 'validation/num_examples': 43793, 'test/accuracy': 0.5026642669801479, 'test/loss': 0.7247406914921622, 'test/mean_average_precision': 0.02699860918358328, 'test/num_examples': 43793, 'score': 5.295187473297119, 'total_duration': 157.4665801525116, 'accumulated_submission_time': 5.295187473297119, 'accumulated_eval_time': 152.17107105255127, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (789, {'train/accuracy': 0.9868888423337463, 'train/loss': 0.050891767530308316, 'train/mean_average_precision': 0.05754635961717144, 'validation/accuracy': 0.9842831939738915, 'validation/loss': 0.06040381103216918, 'validation/mean_average_precision': 0.05773285715726611, 'validation/num_examples': 43793, 'test/accuracy': 0.9832857313140136, 'test/loss': 0.06361206584831347, 'test/mean_average_precision': 0.05780872142012332, 'test/num_examples': 43793, 'score': 245.12684321403503, 'total_duration': 463.50777864456177, 'accumulated_submission_time': 245.12684321403503, 'accumulated_eval_time': 218.1328067779541, 'accumulated_logging_time': 0.028039932250976562, 'global_step': 789, 'preemption_count': 0}), (1575, {'train/accuracy': 0.9876673837997453, 'train/loss': 0.044747687905035395, 'train/mean_average_precision': 0.12931696999947206, 'validation/accuracy': 0.9848657188763237, 'validation/loss': 0.054504972570587, 'validation/mean_average_precision': 0.12735361186641733, 'validation/num_examples': 43793, 'test/accuracy': 0.9838842489614392, 'test/loss': 0.057666053898599065, 'test/mean_average_precision': 0.1283931638383857, 'test/num_examples': 43793, 'score': 485.0207579135895, 'total_duration': 769.7838649749756, 'accumulated_submission_time': 485.0207579135895, 'accumulated_eval_time': 284.27860498428345, 'accumulated_logging_time': 0.049570322036743164, 'global_step': 1575, 'preemption_count': 0}), (2354, {'train/accuracy': 0.9880959552351694, 'train/loss': 0.041956090632875705, 'train/mean_average_precision': 0.16871082212526506, 'validation/accuracy': 0.9853110358226429, 'validation/loss': 0.05087210460259623, 'validation/mean_average_precision': 0.15437284529072448, 'validation/num_examples': 43793, 'test/accuracy': 0.9844044243974495, 'test/loss': 0.05329504024304618, 'test/mean_average_precision': 0.15787206284380537, 'test/num_examples': 43793, 'score': 725.0844688415527, 'total_duration': 1076.4930248260498, 'accumulated_submission_time': 725.0844688415527, 'accumulated_eval_time': 350.6907727718353, 'accumulated_logging_time': 0.07200932502746582, 'global_step': 2354, 'preemption_count': 0}), (3140, {'train/accuracy': 0.9884417992501715, 'train/loss': 0.0404199420635359, 'train/mean_average_precision': 0.20081693065024164, 'validation/accuracy': 0.9855123824091282, 'validation/loss': 0.04949692022940521, 'validation/mean_average_precision': 0.18186784334096356, 'validation/num_examples': 43793, 'test/accuracy': 0.9846605107659467, 'test/loss': 0.05190346236246414, 'test/mean_average_precision': 0.18450855015731862, 'test/num_examples': 43793, 'score': 965.003059387207, 'total_duration': 1384.7231750488281, 'accumulated_submission_time': 965.003059387207, 'accumulated_eval_time': 418.7689526081085, 'accumulated_logging_time': 0.09376406669616699, 'global_step': 3140, 'preemption_count': 0}), (3921, {'train/accuracy': 0.9885749672063253, 'train/loss': 0.03920362561532429, 'train/mean_average_precision': 0.21566386001360677, 'validation/accuracy': 0.9856296992710117, 'validation/loss': 0.04865926794075214, 'validation/mean_average_precision': 0.19134784645743758, 'validation/num_examples': 43793, 'test/accuracy': 0.9847645458531488, 'test/loss': 0.05121565473239606, 'test/mean_average_precision': 0.19510212191383533, 'test/num_examples': 43793, 'score': 1204.9887940883636, 'total_duration': 1693.0312807559967, 'accumulated_submission_time': 1204.9887940883636, 'accumulated_eval_time': 486.85308170318604, 'accumulated_logging_time': 0.12163877487182617, 'global_step': 3921, 'preemption_count': 0}), (4704, {'train/accuracy': 0.9887303204676593, 'train/loss': 0.038383482624129404, 'train/mean_average_precision': 0.23389723897156262, 'validation/accuracy': 0.985982867678758, 'validation/loss': 0.047629707186855316, 'validation/mean_average_precision': 0.21267790352150492, 'validation/num_examples': 43793, 'test/accuracy': 0.9850400071771575, 'test/loss': 0.05040385462633924, 'test/mean_average_precision': 0.208387699800446, 'test/num_examples': 43793, 'score': 1444.8746311664581, 'total_duration': 2002.0224206447601, 'accumulated_submission_time': 1444.8746311664581, 'accumulated_eval_time': 555.7229084968567, 'accumulated_logging_time': 0.14267683029174805, 'global_step': 4704, 'preemption_count': 0}), (5480, {'train/accuracy': 0.9890885612022163, 'train/loss': 0.037013026099527155, 'train/mean_average_precision': 0.26830163077039987, 'validation/accuracy': 0.9860766399801252, 'validation/loss': 0.04734119117817793, 'validation/mean_average_precision': 0.2180545731845939, 'validation/num_examples': 43793, 'test/accuracy': 0.9851886889009724, 'test/loss': 0.04995108679285098, 'test/mean_average_precision': 0.22250528495295885, 'test/num_examples': 43793, 'score': 1684.7194366455078, 'total_duration': 2311.539963245392, 'accumulated_submission_time': 1684.7194366455078, 'accumulated_eval_time': 625.1582984924316, 'accumulated_logging_time': 0.16562724113464355, 'global_step': 5480, 'preemption_count': 0}), (6255, {'train/accuracy': 0.9894808684290851, 'train/loss': 0.035886646079468044, 'train/mean_average_precision': 0.30287925980150143, 'validation/accuracy': 0.9862804222108018, 'validation/loss': 0.04616456889199298, 'validation/mean_average_precision': 0.22475821437284701, 'validation/num_examples': 43793, 'test/accuracy': 0.9854249791192735, 'test/loss': 0.048679582303336834, 'test/mean_average_precision': 0.2308614455919447, 'test/num_examples': 43793, 'score': 1924.7050595283508, 'total_duration': 2621.0713183879852, 'accumulated_submission_time': 1924.7050595283508, 'accumulated_eval_time': 694.4654920101166, 'accumulated_logging_time': 0.18977737426757812, 'global_step': 6255, 'preemption_count': 0}), (7035, {'train/accuracy': 0.9896762237664584, 'train/loss': 0.03495039271024056, 'train/mean_average_precision': 0.3036100468826698, 'validation/accuracy': 0.9863193925178634, 'validation/loss': 0.04572084399435093, 'validation/mean_average_precision': 0.23492406651732037, 'validation/num_examples': 43793, 'test/accuracy': 0.9855041637200589, 'test/loss': 0.04830867760558403, 'test/mean_average_precision': 0.23424573434369703, 'test/num_examples': 43793, 'score': 2164.4966411590576, 'total_duration': 2930.814989566803, 'accumulated_submission_time': 2164.4966411590576, 'accumulated_eval_time': 764.1855943202972, 'accumulated_logging_time': 0.21167516708374023, 'global_step': 7035, 'preemption_count': 0}), (7813, {'train/accuracy': 0.9898562546307184, 'train/loss': 0.034082615951219655, 'train/mean_average_precision': 0.3246106168083517, 'validation/accuracy': 0.9864427984902253, 'validation/loss': 0.04562536864489688, 'validation/mean_average_precision': 0.23816353854565617, 'validation/num_examples': 43793, 'test/accuracy': 0.9855517587194671, 'test/loss': 0.04835256411741391, 'test/mean_average_precision': 0.24056227061529384, 'test/num_examples': 43793, 'score': 2404.321559906006, 'total_duration': 3240.928556203842, 'accumulated_submission_time': 2404.321559906006, 'accumulated_eval_time': 834.2410452365875, 'accumulated_logging_time': 0.2335340976715088, 'global_step': 7813, 'preemption_count': 0}), (8595, {'train/accuracy': 0.9901023876667584, 'train/loss': 0.03316929786713535, 'train/mean_average_precision': 0.3586511718286017, 'validation/accuracy': 0.9865828480312282, 'validation/loss': 0.04513157032780523, 'validation/mean_average_precision': 0.2407911281574001, 'validation/num_examples': 43793, 'test/accuracy': 0.985728660487179, 'test/loss': 0.047949724307018916, 'test/mean_average_precision': 0.24895679353933867, 'test/num_examples': 43793, 'score': 2644.275691986084, 'total_duration': 3550.2758316993713, 'accumulated_submission_time': 2644.275691986084, 'accumulated_eval_time': 903.3997957706451, 'accumulated_logging_time': 0.25627946853637695, 'global_step': 8595, 'preemption_count': 0}), (9364, {'train/accuracy': 0.990299959840598, 'train/loss': 0.03231176891343534, 'train/mean_average_precision': 0.37793907009037014, 'validation/accuracy': 0.9866380559662322, 'validation/loss': 0.0449086486274739, 'validation/mean_average_precision': 0.25155061098357034, 'validation/num_examples': 43793, 'test/accuracy': 0.9857644620354065, 'test/loss': 0.04760564115518539, 'test/mean_average_precision': 0.24685573518460793, 'test/num_examples': 43793, 'score': 2884.0713441371918, 'total_duration': 3860.7510528564453, 'accumulated_submission_time': 2884.0713441371918, 'accumulated_eval_time': 973.8462240695953, 'accumulated_logging_time': 0.27837252616882324, 'global_step': 9364, 'preemption_count': 0}), (10135, {'train/accuracy': 0.9907440419352213, 'train/loss': 0.030976964732994463, 'train/mean_average_precision': 0.40576625520272935, 'validation/accuracy': 0.98666687775583, 'validation/loss': 0.044786045022273964, 'validation/mean_average_precision': 0.25712484156063153, 'validation/num_examples': 43793, 'test/accuracy': 0.9857985788048937, 'test/loss': 0.04764214030710989, 'test/mean_average_precision': 0.2465889895089997, 'test/num_examples': 43793, 'score': 3123.866085290909, 'total_duration': 4171.0398025512695, 'accumulated_submission_time': 3123.866085290909, 'accumulated_eval_time': 1044.1055204868317, 'accumulated_logging_time': 0.30158519744873047, 'global_step': 10135, 'preemption_count': 0}), (10915, {'train/accuracy': 0.9908547530472688, 'train/loss': 0.030363320621552788, 'train/mean_average_precision': 0.4154861075444643, 'validation/accuracy': 0.9866831153837723, 'validation/loss': 0.044724944604317425, 'validation/mean_average_precision': 0.2543932423245059, 'validation/num_examples': 43793, 'test/accuracy': 0.9858520705298924, 'test/loss': 0.047500701157527234, 'test/mean_average_precision': 0.2500418608216621, 'test/num_examples': 43793, 'score': 3363.702891111374, 'total_duration': 4481.538646697998, 'accumulated_submission_time': 3363.702891111374, 'accumulated_eval_time': 1114.526871919632, 'accumulated_logging_time': 0.33295607566833496, 'global_step': 10915, 'preemption_count': 0}), (11696, {'train/accuracy': 0.990901861970573, 'train/loss': 0.029993523417232663, 'train/mean_average_precision': 0.43391832152873333, 'validation/accuracy': 0.9867951550165746, 'validation/loss': 0.044948532301107326, 'validation/mean_average_precision': 0.2601841254134853, 'validation/num_examples': 43793, 'test/accuracy': 0.9859413638031185, 'test/loss': 0.04750883547988185, 'test/mean_average_precision': 0.255417884907817, 'test/num_examples': 43793, 'score': 3603.5317878723145, 'total_duration': 4791.523145198822, 'accumulated_submission_time': 3603.5317878723145, 'accumulated_eval_time': 1184.4472608566284, 'accumulated_logging_time': 0.35623693466186523, 'global_step': 11696, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9909722521951516, 'train/loss': 0.029621974146549986, 'train/mean_average_precision': 0.45027297711484954, 'validation/accuracy': 0.9867813530328235, 'validation/loss': 0.044844379969221575, 'validation/mean_average_precision': 0.26353633516975716, 'validation/num_examples': 43793, 'test/accuracy': 0.9858828177418995, 'test/loss': 0.04779303330302136, 'test/mean_average_precision': 0.25026943251709327, 'test/num_examples': 43793, 'score': 3696.006712913513, 'total_duration': 4953.71764421463, 'accumulated_submission_time': 3696.006712913513, 'accumulated_eval_time': 1254.0622725486755, 'accumulated_logging_time': 0.3793489933013916, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0609 03:29:57.776658 140095247849280 submission_runner.py:584] Timing: 3696.006712913513
I0609 03:29:57.776707 140095247849280 submission_runner.py:586] Total number of evals: 17
I0609 03:29:57.776750 140095247849280 submission_runner.py:587] ====================
I0609 03:29:57.776860 140095247849280 submission_runner.py:655] Final ogbg score: 3696.006712913513
