torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_conformer --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/adamw --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_pytorch_06-09-2023-17-37-51.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 17:38:14.001023 140717412345664 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 17:38:14.001035 140383645402944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 17:38:14.001087 139896592402240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 17:38:14.001882 140258867095360 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 17:38:14.001954 139994265753408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 17:38:14.001982 140000190895936 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 17:38:14.002105 140485423044416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 17:38:14.002639 140479537219392 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 17:38:14.003034 140479537219392 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 17:38:14.011695 140717412345664 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 17:38:14.011739 139896592402240 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 17:38:14.011729 140383645402944 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 17:38:14.012530 140258867095360 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 17:38:14.012556 139994265753408 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 17:38:14.012677 140485423044416 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 17:38:14.012700 140000190895936 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 17:38:14.364982 140485423044416 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/adamw/librispeech_conformer_pytorch.
W0609 17:38:14.396518 140717412345664 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 17:38:14.396865 140479537219392 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 17:38:14.397128 139994265753408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 17:38:14.398066 140000190895936 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 17:38:14.398566 140258867095360 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 17:38:14.398732 139896592402240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 17:38:14.399043 140383645402944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 17:38:14.399924 140485423044416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 17:38:14.404952 140485423044416 submission_runner.py:541] Using RNG seed 2572540086
I0609 17:38:14.406660 140485423044416 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 17:38:14.406789 140485423044416 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/adamw/librispeech_conformer_pytorch/trial_1.
I0609 17:38:14.407960 140485423044416 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/adamw/librispeech_conformer_pytorch/trial_1/hparams.json.
I0609 17:38:14.409009 140485423044416 submission_runner.py:255] Initializing dataset.
I0609 17:38:14.409141 140485423044416 input_pipeline.py:20] Loading split = train-clean-100
I0609 17:38:14.681843 140485423044416 input_pipeline.py:20] Loading split = train-clean-360
I0609 17:38:15.057177 140485423044416 input_pipeline.py:20] Loading split = train-other-500
I0609 17:38:15.506798 140485423044416 submission_runner.py:262] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0609 17:38:22.440513 140485423044416 submission_runner.py:272] Initializing optimizer.
I0609 17:38:22.441921 140485423044416 submission_runner.py:279] Initializing metrics bundle.
I0609 17:38:22.442047 140485423044416 submission_runner.py:297] Initializing checkpoint and logger.
I0609 17:38:22.443706 140485423044416 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 17:38:22.443799 140485423044416 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 17:38:23.079227 140485423044416 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/adamw/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0609 17:38:23.080399 140485423044416 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/adamw/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0609 17:38:23.087285 140485423044416 submission_runner.py:332] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0609 17:38:31.071429 140458840639232 logging_writer.py:48] [0] global_step=0, grad_norm=28.484591, loss=33.144272
I0609 17:38:31.093629 140485423044416 submission.py:120] 0) loss = 33.144, grad_norm = 28.485
I0609 17:38:31.094968 140485423044416 spec.py:298] Evaluating on the training split.
I0609 17:38:31.096211 140485423044416 input_pipeline.py:20] Loading split = train-clean-100
I0609 17:38:31.132963 140485423044416 input_pipeline.py:20] Loading split = train-clean-360
I0609 17:38:31.566598 140485423044416 input_pipeline.py:20] Loading split = train-other-500
I0609 17:38:46.857544 140485423044416 spec.py:310] Evaluating on the validation split.
I0609 17:38:46.859104 140485423044416 input_pipeline.py:20] Loading split = dev-clean
I0609 17:38:46.863864 140485423044416 input_pipeline.py:20] Loading split = dev-other
I0609 17:38:58.247319 140485423044416 spec.py:326] Evaluating on the test split.
I0609 17:38:58.249074 140485423044416 input_pipeline.py:20] Loading split = test-clean
I0609 17:39:04.117940 140485423044416 submission_runner.py:419] Time since start: 41.03s, 	Step: 1, 	{'train/ctc_loss': 32.39550387123272, 'train/wer': 1.7421275440976933, 'validation/ctc_loss': 30.902181283905968, 'validation/wer': 2.256061410708251, 'validation/num_examples': 5348, 'test/ctc_loss': 31.032818246338472, 'test/wer': 2.2237320496415007, 'test/num_examples': 2472, 'score': 8.007726430892944, 'total_duration': 41.03039622306824, 'accumulated_submission_time': 8.007726430892944, 'accumulated_eval_time': 33.022249698638916, 'accumulated_logging_time': 0}
I0609 17:39:04.148154 140457124288256 logging_writer.py:48] [1] accumulated_eval_time=33.022250, accumulated_logging_time=0, accumulated_submission_time=8.007726, global_step=1, preemption_count=0, score=8.007726, test/ctc_loss=31.032818, test/num_examples=2472, test/wer=2.223732, total_duration=41.030396, train/ctc_loss=32.395504, train/wer=1.742128, validation/ctc_loss=30.902181, validation/num_examples=5348, validation/wer=2.256061
I0609 17:39:04.195006 140485423044416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 17:39:04.195032 140479537219392 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 17:39:04.195085 139896592402240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 17:39:04.195082 140383645402944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 17:39:04.195123 140258867095360 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 17:39:04.195161 140717412345664 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 17:39:04.195625 140000190895936 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 17:39:04.195803 139994265753408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 17:39:05.256971 140450745562880 logging_writer.py:48] [1] global_step=1, grad_norm=30.535576, loss=32.555298
I0609 17:39:05.260939 140485423044416 submission.py:120] 1) loss = 32.555, grad_norm = 30.536
I0609 17:39:06.119806 140457124288256 logging_writer.py:48] [2] global_step=2, grad_norm=30.261799, loss=32.918251
I0609 17:39:06.123506 140485423044416 submission.py:120] 2) loss = 32.918, grad_norm = 30.262
I0609 17:39:07.089758 140450745562880 logging_writer.py:48] [3] global_step=3, grad_norm=27.950003, loss=33.064354
I0609 17:39:07.093781 140485423044416 submission.py:120] 3) loss = 33.064, grad_norm = 27.950
I0609 17:39:07.892669 140457124288256 logging_writer.py:48] [4] global_step=4, grad_norm=31.172722, loss=32.542316
I0609 17:39:07.896367 140485423044416 submission.py:120] 4) loss = 32.542, grad_norm = 31.173
I0609 17:39:08.697973 140450745562880 logging_writer.py:48] [5] global_step=5, grad_norm=30.232960, loss=32.888172
I0609 17:39:08.701619 140485423044416 submission.py:120] 5) loss = 32.888, grad_norm = 30.233
I0609 17:39:09.496573 140457124288256 logging_writer.py:48] [6] global_step=6, grad_norm=30.951004, loss=32.987206
I0609 17:39:09.500024 140485423044416 submission.py:120] 6) loss = 32.987, grad_norm = 30.951
I0609 17:39:10.297825 140450745562880 logging_writer.py:48] [7] global_step=7, grad_norm=33.535660, loss=31.802790
I0609 17:39:10.301742 140485423044416 submission.py:120] 7) loss = 31.803, grad_norm = 33.536
I0609 17:39:11.102919 140457124288256 logging_writer.py:48] [8] global_step=8, grad_norm=34.964794, loss=32.166317
I0609 17:39:11.106889 140485423044416 submission.py:120] 8) loss = 32.166, grad_norm = 34.965
I0609 17:39:11.906273 140450745562880 logging_writer.py:48] [9] global_step=9, grad_norm=34.294624, loss=31.745417
I0609 17:39:11.909691 140485423044416 submission.py:120] 9) loss = 31.745, grad_norm = 34.295
I0609 17:39:12.706164 140457124288256 logging_writer.py:48] [10] global_step=10, grad_norm=35.458107, loss=31.613115
I0609 17:39:12.710296 140485423044416 submission.py:120] 10) loss = 31.613, grad_norm = 35.458
I0609 17:39:13.507183 140450745562880 logging_writer.py:48] [11] global_step=11, grad_norm=35.861446, loss=31.591129
I0609 17:39:13.511026 140485423044416 submission.py:120] 11) loss = 31.591, grad_norm = 35.861
I0609 17:39:14.310924 140457124288256 logging_writer.py:48] [12] global_step=12, grad_norm=36.107803, loss=31.599567
I0609 17:39:14.314228 140485423044416 submission.py:120] 12) loss = 31.600, grad_norm = 36.108
I0609 17:39:15.115748 140450745562880 logging_writer.py:48] [13] global_step=13, grad_norm=45.933483, loss=30.823318
I0609 17:39:15.119198 140485423044416 submission.py:120] 13) loss = 30.823, grad_norm = 45.933
I0609 17:39:15.919188 140457124288256 logging_writer.py:48] [14] global_step=14, grad_norm=39.913410, loss=30.976530
I0609 17:39:15.922502 140485423044416 submission.py:120] 14) loss = 30.977, grad_norm = 39.913
I0609 17:39:16.716401 140450745562880 logging_writer.py:48] [15] global_step=15, grad_norm=42.334679, loss=29.807177
I0609 17:39:16.719579 140485423044416 submission.py:120] 15) loss = 29.807, grad_norm = 42.335
I0609 17:39:17.519659 140457124288256 logging_writer.py:48] [16] global_step=16, grad_norm=51.199600, loss=29.803879
I0609 17:39:17.523046 140485423044416 submission.py:120] 16) loss = 29.804, grad_norm = 51.200
I0609 17:39:18.323529 140450745562880 logging_writer.py:48] [17] global_step=17, grad_norm=60.564121, loss=29.246582
I0609 17:39:18.326968 140485423044416 submission.py:120] 17) loss = 29.247, grad_norm = 60.564
I0609 17:39:19.128072 140457124288256 logging_writer.py:48] [18] global_step=18, grad_norm=79.826653, loss=28.586039
I0609 17:39:19.131452 140485423044416 submission.py:120] 18) loss = 28.586, grad_norm = 79.827
I0609 17:39:19.931279 140450745562880 logging_writer.py:48] [19] global_step=19, grad_norm=101.169327, loss=26.903173
I0609 17:39:19.934691 140485423044416 submission.py:120] 19) loss = 26.903, grad_norm = 101.169
I0609 17:39:20.731466 140457124288256 logging_writer.py:48] [20] global_step=20, grad_norm=121.722511, loss=25.055935
I0609 17:39:20.734766 140485423044416 submission.py:120] 20) loss = 25.056, grad_norm = 121.723
I0609 17:39:21.535388 140450745562880 logging_writer.py:48] [21] global_step=21, grad_norm=141.266541, loss=23.000443
I0609 17:39:21.538518 140485423044416 submission.py:120] 21) loss = 23.000, grad_norm = 141.267
I0609 17:39:22.337805 140457124288256 logging_writer.py:48] [22] global_step=22, grad_norm=148.823624, loss=20.564547
I0609 17:39:22.341554 140485423044416 submission.py:120] 22) loss = 20.565, grad_norm = 148.824
I0609 17:39:23.138570 140450745562880 logging_writer.py:48] [23] global_step=23, grad_norm=151.109695, loss=17.187325
I0609 17:39:23.141939 140485423044416 submission.py:120] 23) loss = 17.187, grad_norm = 151.110
I0609 17:39:23.940540 140457124288256 logging_writer.py:48] [24] global_step=24, grad_norm=141.062653, loss=14.256508
I0609 17:39:23.944062 140485423044416 submission.py:120] 24) loss = 14.257, grad_norm = 141.063
I0609 17:39:24.742699 140450745562880 logging_writer.py:48] [25] global_step=25, grad_norm=113.325523, loss=11.408187
I0609 17:39:24.745994 140485423044416 submission.py:120] 25) loss = 11.408, grad_norm = 113.326
I0609 17:39:25.547530 140457124288256 logging_writer.py:48] [26] global_step=26, grad_norm=79.878540, loss=9.333032
I0609 17:39:25.550802 140485423044416 submission.py:120] 26) loss = 9.333, grad_norm = 79.879
I0609 17:39:26.353228 140450745562880 logging_writer.py:48] [27] global_step=27, grad_norm=46.379738, loss=8.048494
I0609 17:39:26.357153 140485423044416 submission.py:120] 27) loss = 8.048, grad_norm = 46.380
I0609 17:39:27.155409 140457124288256 logging_writer.py:48] [28] global_step=28, grad_norm=19.920975, loss=7.429219
I0609 17:39:27.159201 140485423044416 submission.py:120] 28) loss = 7.429, grad_norm = 19.921
I0609 17:39:27.957237 140450745562880 logging_writer.py:48] [29] global_step=29, grad_norm=4.219065, loss=7.254511
I0609 17:39:27.960775 140485423044416 submission.py:120] 29) loss = 7.255, grad_norm = 4.219
I0609 17:39:28.761425 140457124288256 logging_writer.py:48] [30] global_step=30, grad_norm=7.729405, loss=7.276570
I0609 17:39:28.765401 140485423044416 submission.py:120] 30) loss = 7.277, grad_norm = 7.729
I0609 17:39:29.565089 140450745562880 logging_writer.py:48] [31] global_step=31, grad_norm=12.752829, loss=7.345860
I0609 17:39:29.568348 140485423044416 submission.py:120] 31) loss = 7.346, grad_norm = 12.753
I0609 17:39:30.367895 140457124288256 logging_writer.py:48] [32] global_step=32, grad_norm=15.632777, loss=7.440022
I0609 17:39:30.371298 140485423044416 submission.py:120] 32) loss = 7.440, grad_norm = 15.633
I0609 17:39:31.167739 140450745562880 logging_writer.py:48] [33] global_step=33, grad_norm=17.007481, loss=7.486803
I0609 17:39:31.171292 140485423044416 submission.py:120] 33) loss = 7.487, grad_norm = 17.007
I0609 17:39:31.969739 140457124288256 logging_writer.py:48] [34] global_step=34, grad_norm=17.849373, loss=7.509527
I0609 17:39:31.973116 140485423044416 submission.py:120] 34) loss = 7.510, grad_norm = 17.849
I0609 17:39:32.777589 140450745562880 logging_writer.py:48] [35] global_step=35, grad_norm=17.881451, loss=7.513088
I0609 17:39:32.781736 140485423044416 submission.py:120] 35) loss = 7.513, grad_norm = 17.881
I0609 17:39:33.581129 140457124288256 logging_writer.py:48] [36] global_step=36, grad_norm=17.645966, loss=7.496743
I0609 17:39:33.584297 140485423044416 submission.py:120] 36) loss = 7.497, grad_norm = 17.646
I0609 17:39:34.384314 140450745562880 logging_writer.py:48] [37] global_step=37, grad_norm=17.114189, loss=7.448812
I0609 17:39:34.387641 140485423044416 submission.py:120] 37) loss = 7.449, grad_norm = 17.114
I0609 17:39:35.183262 140457124288256 logging_writer.py:48] [38] global_step=38, grad_norm=15.605124, loss=7.376637
I0609 17:39:35.186771 140485423044416 submission.py:120] 38) loss = 7.377, grad_norm = 15.605
I0609 17:39:35.986035 140450745562880 logging_writer.py:48] [39] global_step=39, grad_norm=14.174503, loss=7.313425
I0609 17:39:35.989290 140485423044416 submission.py:120] 39) loss = 7.313, grad_norm = 14.175
I0609 17:39:36.789883 140457124288256 logging_writer.py:48] [40] global_step=40, grad_norm=12.331458, loss=7.234196
I0609 17:39:36.793370 140485423044416 submission.py:120] 40) loss = 7.234, grad_norm = 12.331
I0609 17:39:37.590038 140450745562880 logging_writer.py:48] [41] global_step=41, grad_norm=10.422335, loss=7.174606
I0609 17:39:37.593965 140485423044416 submission.py:120] 41) loss = 7.175, grad_norm = 10.422
I0609 17:39:38.393161 140457124288256 logging_writer.py:48] [42] global_step=42, grad_norm=7.923527, loss=7.131718
I0609 17:39:38.396420 140485423044416 submission.py:120] 42) loss = 7.132, grad_norm = 7.924
I0609 17:39:39.192825 140450745562880 logging_writer.py:48] [43] global_step=43, grad_norm=5.692618, loss=7.093960
I0609 17:39:39.196321 140485423044416 submission.py:120] 43) loss = 7.094, grad_norm = 5.693
I0609 17:39:39.996082 140457124288256 logging_writer.py:48] [44] global_step=44, grad_norm=3.685178, loss=7.060148
I0609 17:39:40.000278 140485423044416 submission.py:120] 44) loss = 7.060, grad_norm = 3.685
I0609 17:39:40.803526 140450745562880 logging_writer.py:48] [45] global_step=45, grad_norm=3.108814, loss=7.041770
I0609 17:39:40.807274 140485423044416 submission.py:120] 45) loss = 7.042, grad_norm = 3.109
I0609 17:39:41.607074 140457124288256 logging_writer.py:48] [46] global_step=46, grad_norm=3.661111, loss=7.029169
I0609 17:39:41.610569 140485423044416 submission.py:120] 46) loss = 7.029, grad_norm = 3.661
I0609 17:39:42.408071 140450745562880 logging_writer.py:48] [47] global_step=47, grad_norm=5.383060, loss=7.027034
I0609 17:39:42.411325 140485423044416 submission.py:120] 47) loss = 7.027, grad_norm = 5.383
I0609 17:39:43.209612 140457124288256 logging_writer.py:48] [48] global_step=48, grad_norm=6.851061, loss=7.039340
I0609 17:39:43.213327 140485423044416 submission.py:120] 48) loss = 7.039, grad_norm = 6.851
I0609 17:39:44.016006 140450745562880 logging_writer.py:48] [49] global_step=49, grad_norm=6.901545, loss=7.002852
I0609 17:39:44.019409 140485423044416 submission.py:120] 49) loss = 7.003, grad_norm = 6.902
I0609 17:39:44.823115 140457124288256 logging_writer.py:48] [50] global_step=50, grad_norm=5.920414, loss=6.994750
I0609 17:39:44.826955 140485423044416 submission.py:120] 50) loss = 6.995, grad_norm = 5.920
I0609 17:39:45.626480 140450745562880 logging_writer.py:48] [51] global_step=51, grad_norm=4.968277, loss=6.975576
I0609 17:39:45.629776 140485423044416 submission.py:120] 51) loss = 6.976, grad_norm = 4.968
I0609 17:39:46.429011 140457124288256 logging_writer.py:48] [52] global_step=52, grad_norm=3.915514, loss=6.945242
I0609 17:39:46.432636 140485423044416 submission.py:120] 52) loss = 6.945, grad_norm = 3.916
I0609 17:39:47.232796 140450745562880 logging_writer.py:48] [53] global_step=53, grad_norm=3.582206, loss=6.927190
I0609 17:39:47.236140 140485423044416 submission.py:120] 53) loss = 6.927, grad_norm = 3.582
I0609 17:39:48.035587 140457124288256 logging_writer.py:48] [54] global_step=54, grad_norm=3.472980, loss=6.930361
I0609 17:39:48.038922 140485423044416 submission.py:120] 54) loss = 6.930, grad_norm = 3.473
I0609 17:39:48.842420 140450745562880 logging_writer.py:48] [55] global_step=55, grad_norm=3.246081, loss=6.917872
I0609 17:39:48.845603 140485423044416 submission.py:120] 55) loss = 6.918, grad_norm = 3.246
I0609 17:39:49.647790 140457124288256 logging_writer.py:48] [56] global_step=56, grad_norm=2.919235, loss=6.873249
I0609 17:39:49.651085 140485423044416 submission.py:120] 56) loss = 6.873, grad_norm = 2.919
I0609 17:39:50.450162 140450745562880 logging_writer.py:48] [57] global_step=57, grad_norm=2.866280, loss=6.868102
I0609 17:39:50.453688 140485423044416 submission.py:120] 57) loss = 6.868, grad_norm = 2.866
I0609 17:39:51.253518 140457124288256 logging_writer.py:48] [58] global_step=58, grad_norm=2.962661, loss=6.844181
I0609 17:39:51.256754 140485423044416 submission.py:120] 58) loss = 6.844, grad_norm = 2.963
I0609 17:39:52.063610 140450745562880 logging_writer.py:48] [59] global_step=59, grad_norm=2.926259, loss=6.840806
I0609 17:39:52.067048 140485423044416 submission.py:120] 59) loss = 6.841, grad_norm = 2.926
I0609 17:39:52.871037 140457124288256 logging_writer.py:48] [60] global_step=60, grad_norm=2.794226, loss=6.811145
I0609 17:39:52.874519 140485423044416 submission.py:120] 60) loss = 6.811, grad_norm = 2.794
I0609 17:39:53.674278 140450745562880 logging_writer.py:48] [61] global_step=61, grad_norm=2.815134, loss=6.806229
I0609 17:39:53.677592 140485423044416 submission.py:120] 61) loss = 6.806, grad_norm = 2.815
I0609 17:39:54.476540 140457124288256 logging_writer.py:48] [62] global_step=62, grad_norm=2.754572, loss=6.789313
I0609 17:39:54.479915 140485423044416 submission.py:120] 62) loss = 6.789, grad_norm = 2.755
I0609 17:39:55.281382 140450745562880 logging_writer.py:48] [63] global_step=63, grad_norm=3.046678, loss=6.770960
I0609 17:39:55.284695 140485423044416 submission.py:120] 63) loss = 6.771, grad_norm = 3.047
I0609 17:39:56.086796 140457124288256 logging_writer.py:48] [64] global_step=64, grad_norm=2.925697, loss=6.743012
I0609 17:39:56.090408 140485423044416 submission.py:120] 64) loss = 6.743, grad_norm = 2.926
I0609 17:39:56.892255 140450745562880 logging_writer.py:48] [65] global_step=65, grad_norm=2.806050, loss=6.731641
I0609 17:39:56.895552 140485423044416 submission.py:120] 65) loss = 6.732, grad_norm = 2.806
I0609 17:39:57.695179 140457124288256 logging_writer.py:48] [66] global_step=66, grad_norm=2.813426, loss=6.708172
I0609 17:39:57.698490 140485423044416 submission.py:120] 66) loss = 6.708, grad_norm = 2.813
I0609 17:39:58.499837 140450745562880 logging_writer.py:48] [67] global_step=67, grad_norm=2.691245, loss=6.695811
I0609 17:39:58.503171 140485423044416 submission.py:120] 67) loss = 6.696, grad_norm = 2.691
I0609 17:39:59.304692 140457124288256 logging_writer.py:48] [68] global_step=68, grad_norm=2.602039, loss=6.676192
I0609 17:39:59.308080 140485423044416 submission.py:120] 68) loss = 6.676, grad_norm = 2.602
I0609 17:40:00.110760 140450745562880 logging_writer.py:48] [69] global_step=69, grad_norm=3.288040, loss=6.683600
I0609 17:40:00.114361 140485423044416 submission.py:120] 69) loss = 6.684, grad_norm = 3.288
I0609 17:40:00.914766 140457124288256 logging_writer.py:48] [70] global_step=70, grad_norm=2.776326, loss=6.643075
I0609 17:40:00.918046 140485423044416 submission.py:120] 70) loss = 6.643, grad_norm = 2.776
I0609 17:40:01.717035 140450745562880 logging_writer.py:48] [71] global_step=71, grad_norm=2.825581, loss=6.638031
I0609 17:40:01.720339 140485423044416 submission.py:120] 71) loss = 6.638, grad_norm = 2.826
I0609 17:40:02.519913 140457124288256 logging_writer.py:48] [72] global_step=72, grad_norm=2.379245, loss=6.601905
I0609 17:40:02.524202 140485423044416 submission.py:120] 72) loss = 6.602, grad_norm = 2.379
I0609 17:40:03.325893 140450745562880 logging_writer.py:48] [73] global_step=73, grad_norm=2.386334, loss=6.595334
I0609 17:40:03.330040 140485423044416 submission.py:120] 73) loss = 6.595, grad_norm = 2.386
I0609 17:40:04.131255 140457124288256 logging_writer.py:48] [74] global_step=74, grad_norm=2.170050, loss=6.596161
I0609 17:40:04.134824 140485423044416 submission.py:120] 74) loss = 6.596, grad_norm = 2.170
I0609 17:40:04.935593 140450745562880 logging_writer.py:48] [75] global_step=75, grad_norm=2.419370, loss=6.565228
I0609 17:40:04.938831 140485423044416 submission.py:120] 75) loss = 6.565, grad_norm = 2.419
I0609 17:40:05.739450 140457124288256 logging_writer.py:48] [76] global_step=76, grad_norm=2.154150, loss=6.552902
I0609 17:40:05.742843 140485423044416 submission.py:120] 76) loss = 6.553, grad_norm = 2.154
I0609 17:40:06.543310 140450745562880 logging_writer.py:48] [77] global_step=77, grad_norm=2.137734, loss=6.546216
I0609 17:40:06.546660 140485423044416 submission.py:120] 77) loss = 6.546, grad_norm = 2.138
I0609 17:40:07.348888 140457124288256 logging_writer.py:48] [78] global_step=78, grad_norm=2.151394, loss=6.529519
I0609 17:40:07.352030 140485423044416 submission.py:120] 78) loss = 6.530, grad_norm = 2.151
I0609 17:40:08.153314 140450745562880 logging_writer.py:48] [79] global_step=79, grad_norm=2.146966, loss=6.515206
I0609 17:40:08.156591 140485423044416 submission.py:120] 79) loss = 6.515, grad_norm = 2.147
I0609 17:40:08.955460 140457124288256 logging_writer.py:48] [80] global_step=80, grad_norm=2.392564, loss=6.505438
I0609 17:40:08.958673 140485423044416 submission.py:120] 80) loss = 6.505, grad_norm = 2.393
I0609 17:40:09.757407 140450745562880 logging_writer.py:48] [81] global_step=81, grad_norm=2.137270, loss=6.489073
I0609 17:40:09.760649 140485423044416 submission.py:120] 81) loss = 6.489, grad_norm = 2.137
I0609 17:40:10.560140 140457124288256 logging_writer.py:48] [82] global_step=82, grad_norm=2.062198, loss=6.476332
I0609 17:40:10.563596 140485423044416 submission.py:120] 82) loss = 6.476, grad_norm = 2.062
I0609 17:40:11.368034 140450745562880 logging_writer.py:48] [83] global_step=83, grad_norm=2.085210, loss=6.476947
I0609 17:40:11.372114 140485423044416 submission.py:120] 83) loss = 6.477, grad_norm = 2.085
I0609 17:40:12.175672 140457124288256 logging_writer.py:48] [84] global_step=84, grad_norm=1.758966, loss=6.453930
I0609 17:40:12.179112 140485423044416 submission.py:120] 84) loss = 6.454, grad_norm = 1.759
I0609 17:40:12.977763 140450745562880 logging_writer.py:48] [85] global_step=85, grad_norm=1.819854, loss=6.427145
I0609 17:40:12.981099 140485423044416 submission.py:120] 85) loss = 6.427, grad_norm = 1.820
I0609 17:40:13.778053 140457124288256 logging_writer.py:48] [86] global_step=86, grad_norm=1.735676, loss=6.415080
I0609 17:40:13.781386 140485423044416 submission.py:120] 86) loss = 6.415, grad_norm = 1.736
I0609 17:40:14.583283 140450745562880 logging_writer.py:48] [87] global_step=87, grad_norm=1.682276, loss=6.402984
I0609 17:40:14.586825 140485423044416 submission.py:120] 87) loss = 6.403, grad_norm = 1.682
I0609 17:40:15.394569 140457124288256 logging_writer.py:48] [88] global_step=88, grad_norm=1.997372, loss=6.399866
I0609 17:40:15.397865 140485423044416 submission.py:120] 88) loss = 6.400, grad_norm = 1.997
I0609 17:40:16.197778 140450745562880 logging_writer.py:48] [89] global_step=89, grad_norm=2.144768, loss=6.400885
I0609 17:40:16.201343 140485423044416 submission.py:120] 89) loss = 6.401, grad_norm = 2.145
I0609 17:40:17.001680 140457124288256 logging_writer.py:48] [90] global_step=90, grad_norm=1.575732, loss=6.374625
I0609 17:40:17.005300 140485423044416 submission.py:120] 90) loss = 6.375, grad_norm = 1.576
I0609 17:40:17.807500 140450745562880 logging_writer.py:48] [91] global_step=91, grad_norm=1.918985, loss=6.370656
I0609 17:40:17.811009 140485423044416 submission.py:120] 91) loss = 6.371, grad_norm = 1.919
I0609 17:40:18.614865 140457124288256 logging_writer.py:48] [92] global_step=92, grad_norm=1.564480, loss=6.344987
I0609 17:40:18.618883 140485423044416 submission.py:120] 92) loss = 6.345, grad_norm = 1.564
I0609 17:40:19.418354 140450745562880 logging_writer.py:48] [93] global_step=93, grad_norm=1.480429, loss=6.340769
I0609 17:40:19.421885 140485423044416 submission.py:120] 93) loss = 6.341, grad_norm = 1.480
I0609 17:40:20.221197 140457124288256 logging_writer.py:48] [94] global_step=94, grad_norm=1.428232, loss=6.337814
I0609 17:40:20.224602 140485423044416 submission.py:120] 94) loss = 6.338, grad_norm = 1.428
I0609 17:40:21.024426 140450745562880 logging_writer.py:48] [95] global_step=95, grad_norm=1.395226, loss=6.317064
I0609 17:40:21.028540 140485423044416 submission.py:120] 95) loss = 6.317, grad_norm = 1.395
I0609 17:40:21.835041 140457124288256 logging_writer.py:48] [96] global_step=96, grad_norm=1.598657, loss=6.325290
I0609 17:40:21.838408 140485423044416 submission.py:120] 96) loss = 6.325, grad_norm = 1.599
I0609 17:40:22.641715 140450745562880 logging_writer.py:48] [97] global_step=97, grad_norm=1.411243, loss=6.307854
I0609 17:40:22.644961 140485423044416 submission.py:120] 97) loss = 6.308, grad_norm = 1.411
I0609 17:40:23.446963 140457124288256 logging_writer.py:48] [98] global_step=98, grad_norm=1.614506, loss=6.288099
I0609 17:40:23.450314 140485423044416 submission.py:120] 98) loss = 6.288, grad_norm = 1.615
I0609 17:40:24.250571 140450745562880 logging_writer.py:48] [99] global_step=99, grad_norm=1.562013, loss=6.281310
I0609 17:40:24.253902 140485423044416 submission.py:120] 99) loss = 6.281, grad_norm = 1.562
I0609 17:40:25.056202 140457124288256 logging_writer.py:48] [100] global_step=100, grad_norm=1.264830, loss=6.274187
I0609 17:40:25.059811 140485423044416 submission.py:120] 100) loss = 6.274, grad_norm = 1.265
I0609 17:45:42.160691 140450745562880 logging_writer.py:48] [500] global_step=500, grad_norm=0.659170, loss=5.792212
I0609 17:45:42.165242 140485423044416 submission.py:120] 500) loss = 5.792, grad_norm = 0.659
I0609 17:52:18.672837 140457124288256 logging_writer.py:48] [1000] global_step=1000, grad_norm=6.672655, loss=5.399524
I0609 17:52:18.678954 140485423044416 submission.py:120] 1000) loss = 5.400, grad_norm = 6.673
I0609 17:58:56.438293 140457124288256 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.960460, loss=3.418659
I0609 17:58:56.447693 140485423044416 submission.py:120] 1500) loss = 3.419, grad_norm = 0.960
I0609 18:05:32.327872 140450745562880 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.261964, loss=2.886367
I0609 18:05:32.332827 140485423044416 submission.py:120] 2000) loss = 2.886, grad_norm = 1.262
I0609 18:12:09.289552 140457124288256 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.987903, loss=2.623663
I0609 18:12:09.296957 140485423044416 submission.py:120] 2500) loss = 2.624, grad_norm = 0.988
I0609 18:18:44.458178 140450745562880 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.817785, loss=2.410622
I0609 18:18:44.463833 140485423044416 submission.py:120] 3000) loss = 2.411, grad_norm = 0.818
I0609 18:19:04.213021 140485423044416 spec.py:298] Evaluating on the training split.
I0609 18:19:16.227356 140485423044416 spec.py:310] Evaluating on the validation split.
I0609 18:19:26.334810 140485423044416 spec.py:326] Evaluating on the test split.
I0609 18:19:31.840891 140485423044416 submission_runner.py:419] Time since start: 2468.75s, 	Step: 3026, 	{'train/ctc_loss': 2.7186054227807026, 'train/wer': 0.5893514246947082, 'validation/ctc_loss': 2.8653564207856137, 'validation/wer': 0.5891565683387244, 'validation/num_examples': 5348, 'test/ctc_loss': 2.5517170462610492, 'test/wer': 0.5506266122316332, 'test/num_examples': 2472, 'score': 2406.9419887065887, 'total_duration': 2468.753018140793, 'accumulated_submission_time': 2406.9419887065887, 'accumulated_eval_time': 60.64905762672424, 'accumulated_logging_time': 0.0401005744934082}
I0609 18:19:31.866394 140457124288256 logging_writer.py:48] [3026] accumulated_eval_time=60.649058, accumulated_logging_time=0.040101, accumulated_submission_time=2406.941989, global_step=3026, preemption_count=0, score=2406.941989, test/ctc_loss=2.551717, test/num_examples=2472, test/wer=0.550627, total_duration=2468.753018, train/ctc_loss=2.718605, train/wer=0.589351, validation/ctc_loss=2.865356, validation/num_examples=5348, validation/wer=0.589157
I0609 18:25:48.660294 140457124288256 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.843642, loss=2.254165
I0609 18:25:48.668029 140485423044416 submission.py:120] 3500) loss = 2.254, grad_norm = 0.844
I0609 18:32:23.645834 140450745562880 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.817210, loss=2.103245
I0609 18:32:23.650758 140485423044416 submission.py:120] 4000) loss = 2.103, grad_norm = 0.817
I0609 18:39:00.070911 140457124288256 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.688148, loss=1.947987
I0609 18:39:00.077488 140485423044416 submission.py:120] 4500) loss = 1.948, grad_norm = 0.688
I0609 18:45:34.704563 140450745562880 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.631158, loss=1.866689
I0609 18:45:34.710747 140485423044416 submission.py:120] 5000) loss = 1.867, grad_norm = 0.631
I0609 18:52:10.566164 140457124288256 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.757307, loss=1.891964
I0609 18:52:10.573913 140485423044416 submission.py:120] 5500) loss = 1.892, grad_norm = 0.757
I0609 18:58:45.166506 140450745562880 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.716910, loss=1.854012
I0609 18:58:45.172287 140485423044416 submission.py:120] 6000) loss = 1.854, grad_norm = 0.717
I0609 18:59:32.535442 140485423044416 spec.py:298] Evaluating on the training split.
I0609 18:59:44.883398 140485423044416 spec.py:310] Evaluating on the validation split.
I0609 18:59:55.201841 140485423044416 spec.py:326] Evaluating on the test split.
I0609 19:00:00.761610 140485423044416 submission_runner.py:419] Time since start: 4897.67s, 	Step: 6061, 	{'train/ctc_loss': 0.657419717744536, 'train/wer': 0.22186160108548167, 'validation/ctc_loss': 0.8763135619160639, 'validation/wer': 0.2634094530005311, 'validation/num_examples': 5348, 'test/ctc_loss': 0.597972853087296, 'test/wer': 0.197205126642699, 'test/num_examples': 2472, 'score': 4806.470524311066, 'total_duration': 4897.674141168594, 'accumulated_submission_time': 4806.470524311066, 'accumulated_eval_time': 88.874587059021, 'accumulated_logging_time': 0.07503771781921387}
I0609 19:00:00.784814 140457124288256 logging_writer.py:48] [6061] accumulated_eval_time=88.874587, accumulated_logging_time=0.075038, accumulated_submission_time=4806.470524, global_step=6061, preemption_count=0, score=4806.470524, test/ctc_loss=0.597973, test/num_examples=2472, test/wer=0.197205, total_duration=4897.674141, train/ctc_loss=0.657420, train/wer=0.221862, validation/ctc_loss=0.876314, validation/num_examples=5348, validation/wer=0.263409
I0609 19:05:49.274682 140457124288256 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.453686, loss=1.766922
I0609 19:05:49.283228 140485423044416 submission.py:120] 6500) loss = 1.767, grad_norm = 0.454
I0609 19:12:23.395983 140450745562880 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.546738, loss=1.713678
I0609 19:12:23.401126 140485423044416 submission.py:120] 7000) loss = 1.714, grad_norm = 0.547
I0609 19:18:59.158362 140457124288256 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.427081, loss=1.699321
I0609 19:18:59.166417 140485423044416 submission.py:120] 7500) loss = 1.699, grad_norm = 0.427
I0609 19:25:33.463358 140450745562880 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.492217, loss=1.563309
I0609 19:25:33.467852 140485423044416 submission.py:120] 8000) loss = 1.563, grad_norm = 0.492
I0609 19:32:09.160613 140457124288256 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.526531, loss=1.557167
I0609 19:32:09.167495 140485423044416 submission.py:120] 8500) loss = 1.557, grad_norm = 0.527
I0609 19:38:43.308044 140450745562880 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.505670, loss=1.602765
I0609 19:38:43.313449 140485423044416 submission.py:120] 9000) loss = 1.603, grad_norm = 0.506
I0609 19:40:01.359261 140485423044416 spec.py:298] Evaluating on the training split.
I0609 19:40:13.584930 140485423044416 spec.py:310] Evaluating on the validation split.
I0609 19:40:24.055613 140485423044416 spec.py:326] Evaluating on the test split.
I0609 19:40:29.526537 140485423044416 submission_runner.py:419] Time since start: 7326.44s, 	Step: 9100, 	{'train/ctc_loss': 0.488623957940864, 'train/wer': 0.1686078697421981, 'validation/ctc_loss': 0.7222781435227045, 'validation/wer': 0.216772075508135, 'validation/num_examples': 5348, 'test/ctc_loss': 0.45401296051358153, 'test/wer': 0.15422582414234354, 'test/num_examples': 2472, 'score': 7205.893730401993, 'total_duration': 7326.438781499863, 'accumulated_submission_time': 7205.893730401993, 'accumulated_eval_time': 117.04092288017273, 'accumulated_logging_time': 0.10816812515258789}
I0609 19:40:29.549469 140457124288256 logging_writer.py:48] [9100] accumulated_eval_time=117.040923, accumulated_logging_time=0.108168, accumulated_submission_time=7205.893730, global_step=9100, preemption_count=0, score=7205.893730, test/ctc_loss=0.454013, test/num_examples=2472, test/wer=0.154226, total_duration=7326.438781, train/ctc_loss=0.488624, train/wer=0.168608, validation/ctc_loss=0.722278, validation/num_examples=5348, validation/wer=0.216772
I0609 19:45:47.069283 140457124288256 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.607972, loss=1.568947
I0609 19:45:47.078260 140485423044416 submission.py:120] 9500) loss = 1.569, grad_norm = 0.608
I0609 19:52:21.034511 140450745562880 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.496803, loss=1.557444
I0609 19:52:21.039432 140485423044416 submission.py:120] 10000) loss = 1.557, grad_norm = 0.497
I0609 19:58:56.882046 140457124288256 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.529504, loss=1.530298
I0609 19:58:56.889027 140485423044416 submission.py:120] 10500) loss = 1.530, grad_norm = 0.530
I0609 20:05:31.149089 140450745562880 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.348869, loss=1.533200
I0609 20:05:31.156881 140485423044416 submission.py:120] 11000) loss = 1.533, grad_norm = 0.349
I0609 20:12:06.691351 140457124288256 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.524354, loss=1.555880
I0609 20:12:06.698571 140485423044416 submission.py:120] 11500) loss = 1.556, grad_norm = 0.524
I0609 20:18:40.866517 140450745562880 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.417663, loss=1.453625
I0609 20:18:40.872559 140485423044416 submission.py:120] 12000) loss = 1.454, grad_norm = 0.418
I0609 20:20:29.686686 140485423044416 spec.py:298] Evaluating on the training split.
I0609 20:20:42.207230 140485423044416 spec.py:310] Evaluating on the validation split.
I0609 20:20:52.509633 140485423044416 spec.py:326] Evaluating on the test split.
I0609 20:20:58.115811 140485423044416 submission_runner.py:419] Time since start: 9755.03s, 	Step: 12139, 	{'train/ctc_loss': 0.393882495527971, 'train/wer': 0.14002170963364993, 'validation/ctc_loss': 0.6249276947395519, 'validation/wer': 0.18937865108868826, 'validation/num_examples': 5348, 'test/ctc_loss': 0.38761764871927223, 'test/wer': 0.1320455791846932, 'test/num_examples': 2472, 'score': 9604.876645565033, 'total_duration': 9755.028055906296, 'accumulated_submission_time': 9604.876645565033, 'accumulated_eval_time': 145.46913862228394, 'accumulated_logging_time': 0.14036178588867188}
I0609 20:20:58.138660 140457124288256 logging_writer.py:48] [12139] accumulated_eval_time=145.469139, accumulated_logging_time=0.140362, accumulated_submission_time=9604.876646, global_step=12139, preemption_count=0, score=9604.876646, test/ctc_loss=0.387618, test/num_examples=2472, test/wer=0.132046, total_duration=9755.028056, train/ctc_loss=0.393882, train/wer=0.140022, validation/ctc_loss=0.624928, validation/num_examples=5348, validation/wer=0.189379
I0609 20:25:44.895792 140457124288256 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.364983, loss=1.507153
I0609 20:25:44.903410 140485423044416 submission.py:120] 12500) loss = 1.507, grad_norm = 0.365
I0609 20:32:18.647215 140450745562880 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.356837, loss=1.511668
I0609 20:32:18.651930 140485423044416 submission.py:120] 13000) loss = 1.512, grad_norm = 0.357
I0609 20:38:53.926879 140457124288256 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.341882, loss=1.343680
I0609 20:38:53.934064 140485423044416 submission.py:120] 13500) loss = 1.344, grad_norm = 0.342
I0609 20:45:27.690233 140450745562880 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.343675, loss=1.402340
I0609 20:45:27.698383 140485423044416 submission.py:120] 14000) loss = 1.402, grad_norm = 0.344
I0609 20:52:03.400880 140457124288256 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.378707, loss=1.400084
I0609 20:52:03.408157 140485423044416 submission.py:120] 14500) loss = 1.400, grad_norm = 0.379
I0609 20:58:37.198229 140450745562880 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.311590, loss=1.410782
I0609 20:58:37.204034 140485423044416 submission.py:120] 15000) loss = 1.411, grad_norm = 0.312
I0609 21:00:58.134106 140485423044416 spec.py:298] Evaluating on the training split.
I0609 21:01:10.453793 140485423044416 spec.py:310] Evaluating on the validation split.
I0609 21:01:20.689600 140485423044416 spec.py:326] Evaluating on the test split.
I0609 21:01:26.344005 140485423044416 submission_runner.py:419] Time since start: 12183.26s, 	Step: 15180, 	{'train/ctc_loss': 0.3168684489441938, 'train/wer': 0.11489823609226595, 'validation/ctc_loss': 0.5414772700516124, 'validation/wer': 0.16444744846231835, 'validation/num_examples': 5348, 'test/ctc_loss': 0.33150654376895283, 'test/wer': 0.11049499319562082, 'test/num_examples': 2472, 'score': 12003.719080209732, 'total_duration': 12183.256258010864, 'accumulated_submission_time': 12003.719080209732, 'accumulated_eval_time': 173.67812418937683, 'accumulated_logging_time': 0.17334818840026855}
I0609 21:01:26.366063 140457124288256 logging_writer.py:48] [15180] accumulated_eval_time=173.678124, accumulated_logging_time=0.173348, accumulated_submission_time=12003.719080, global_step=15180, preemption_count=0, score=12003.719080, test/ctc_loss=0.331507, test/num_examples=2472, test/wer=0.110495, total_duration=12183.256258, train/ctc_loss=0.316868, train/wer=0.114898, validation/ctc_loss=0.541477, validation/num_examples=5348, validation/wer=0.164447
I0609 21:05:40.776486 140457124288256 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.339447, loss=1.333881
I0609 21:05:40.787521 140485423044416 submission.py:120] 15500) loss = 1.334, grad_norm = 0.339
I0609 21:12:14.807280 140450745562880 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.333888, loss=1.398756
I0609 21:12:14.812105 140485423044416 submission.py:120] 16000) loss = 1.399, grad_norm = 0.334
I0609 21:18:50.627066 140457124288256 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.314259, loss=1.324758
I0609 21:18:50.634349 140485423044416 submission.py:120] 16500) loss = 1.325, grad_norm = 0.314
I0609 21:25:24.352875 140450745562880 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.365016, loss=1.316943
I0609 21:25:24.384038 140485423044416 submission.py:120] 17000) loss = 1.317, grad_norm = 0.365
I0609 21:31:58.366959 140457124288256 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.373559, loss=1.335147
I0609 21:31:58.371894 140485423044416 submission.py:120] 17500) loss = 1.335, grad_norm = 0.374
I0609 21:38:33.839462 140457124288256 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.360175, loss=1.321091
I0609 21:38:33.872243 140485423044416 submission.py:120] 18000) loss = 1.321, grad_norm = 0.360
I0609 21:41:27.085651 140485423044416 spec.py:298] Evaluating on the training split.
I0609 21:41:39.349372 140485423044416 spec.py:310] Evaluating on the validation split.
I0609 21:41:49.759254 140485423044416 spec.py:326] Evaluating on the test split.
I0609 21:41:55.407996 140485423044416 submission_runner.py:419] Time since start: 14612.32s, 	Step: 18221, 	{'train/ctc_loss': 0.2859796002922518, 'train/wer': 0.10551967435549525, 'validation/ctc_loss': 0.5275388859064195, 'validation/wer': 0.15755322744170328, 'validation/num_examples': 5348, 'test/ctc_loss': 0.31425404461578166, 'test/wer': 0.10598582251741719, 'test/num_examples': 2472, 'score': 14403.274402618408, 'total_duration': 14612.320591449738, 'accumulated_submission_time': 14403.274402618408, 'accumulated_eval_time': 201.99989700317383, 'accumulated_logging_time': 0.20523476600646973}
I0609 21:41:55.429535 140457124288256 logging_writer.py:48] [18221] accumulated_eval_time=201.999897, accumulated_logging_time=0.205235, accumulated_submission_time=14403.274403, global_step=18221, preemption_count=0, score=14403.274403, test/ctc_loss=0.314254, test/num_examples=2472, test/wer=0.105986, total_duration=14612.320591, train/ctc_loss=0.285980, train/wer=0.105520, validation/ctc_loss=0.527539, validation/num_examples=5348, validation/wer=0.157553
I0609 21:45:35.849874 140450745562880 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.330471, loss=1.327050
I0609 21:45:35.859080 140485423044416 submission.py:120] 18500) loss = 1.327, grad_norm = 0.330
I0609 21:52:11.261563 140457124288256 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.375179, loss=1.236403
I0609 21:52:11.269628 140485423044416 submission.py:120] 19000) loss = 1.236, grad_norm = 0.375
I0609 21:58:44.855964 140450745562880 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.309918, loss=1.328188
I0609 21:58:44.886124 140485423044416 submission.py:120] 19500) loss = 1.328, grad_norm = 0.310
I0609 22:05:19.686468 140485423044416 spec.py:298] Evaluating on the training split.
I0609 22:05:31.833462 140485423044416 spec.py:310] Evaluating on the validation split.
I0609 22:05:42.151745 140485423044416 spec.py:326] Evaluating on the test split.
I0609 22:05:47.633909 140485423044416 submission_runner.py:419] Time since start: 16044.55s, 	Step: 20000, 	{'train/ctc_loss': 0.269938688863884, 'train/wer': 0.10047218453188603, 'validation/ctc_loss': 0.5111339309260097, 'validation/wer': 0.15405783807270795, 'validation/num_examples': 5348, 'test/ctc_loss': 0.30691110616007483, 'test/wer': 0.10306095505047427, 'test/num_examples': 2472, 'score': 15806.843983411789, 'total_duration': 16044.546288251877, 'accumulated_submission_time': 15806.843983411789, 'accumulated_eval_time': 229.94672107696533, 'accumulated_logging_time': 0.23627781867980957}
I0609 22:05:47.655019 140457124288256 logging_writer.py:48] [20000] accumulated_eval_time=229.946721, accumulated_logging_time=0.236278, accumulated_submission_time=15806.843983, global_step=20000, preemption_count=0, score=15806.843983, test/ctc_loss=0.306911, test/num_examples=2472, test/wer=0.103061, total_duration=16044.546288, train/ctc_loss=0.269939, train/wer=0.100472, validation/ctc_loss=0.511134, validation/num_examples=5348, validation/wer=0.154058
I0609 22:05:47.676635 140450745562880 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15806.843983
I0609 22:05:48.365284 140485423044416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/adamw/librispeech_conformer_pytorch/trial_1/checkpoint_20000.
I0609 22:05:48.483583 140485423044416 submission_runner.py:581] Tuning trial 1/1
I0609 22:05:48.483824 140485423044416 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0609 22:05:48.484373 140485423044416 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ctc_loss': 32.39550387123272, 'train/wer': 1.7421275440976933, 'validation/ctc_loss': 30.902181283905968, 'validation/wer': 2.256061410708251, 'validation/num_examples': 5348, 'test/ctc_loss': 31.032818246338472, 'test/wer': 2.2237320496415007, 'test/num_examples': 2472, 'score': 8.007726430892944, 'total_duration': 41.03039622306824, 'accumulated_submission_time': 8.007726430892944, 'accumulated_eval_time': 33.022249698638916, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3026, {'train/ctc_loss': 2.7186054227807026, 'train/wer': 0.5893514246947082, 'validation/ctc_loss': 2.8653564207856137, 'validation/wer': 0.5891565683387244, 'validation/num_examples': 5348, 'test/ctc_loss': 2.5517170462610492, 'test/wer': 0.5506266122316332, 'test/num_examples': 2472, 'score': 2406.9419887065887, 'total_duration': 2468.753018140793, 'accumulated_submission_time': 2406.9419887065887, 'accumulated_eval_time': 60.64905762672424, 'accumulated_logging_time': 0.0401005744934082, 'global_step': 3026, 'preemption_count': 0}), (6061, {'train/ctc_loss': 0.657419717744536, 'train/wer': 0.22186160108548167, 'validation/ctc_loss': 0.8763135619160639, 'validation/wer': 0.2634094530005311, 'validation/num_examples': 5348, 'test/ctc_loss': 0.597972853087296, 'test/wer': 0.197205126642699, 'test/num_examples': 2472, 'score': 4806.470524311066, 'total_duration': 4897.674141168594, 'accumulated_submission_time': 4806.470524311066, 'accumulated_eval_time': 88.874587059021, 'accumulated_logging_time': 0.07503771781921387, 'global_step': 6061, 'preemption_count': 0}), (9100, {'train/ctc_loss': 0.488623957940864, 'train/wer': 0.1686078697421981, 'validation/ctc_loss': 0.7222781435227045, 'validation/wer': 0.216772075508135, 'validation/num_examples': 5348, 'test/ctc_loss': 0.45401296051358153, 'test/wer': 0.15422582414234354, 'test/num_examples': 2472, 'score': 7205.893730401993, 'total_duration': 7326.438781499863, 'accumulated_submission_time': 7205.893730401993, 'accumulated_eval_time': 117.04092288017273, 'accumulated_logging_time': 0.10816812515258789, 'global_step': 9100, 'preemption_count': 0}), (12139, {'train/ctc_loss': 0.393882495527971, 'train/wer': 0.14002170963364993, 'validation/ctc_loss': 0.6249276947395519, 'validation/wer': 0.18937865108868826, 'validation/num_examples': 5348, 'test/ctc_loss': 0.38761764871927223, 'test/wer': 0.1320455791846932, 'test/num_examples': 2472, 'score': 9604.876645565033, 'total_duration': 9755.028055906296, 'accumulated_submission_time': 9604.876645565033, 'accumulated_eval_time': 145.46913862228394, 'accumulated_logging_time': 0.14036178588867188, 'global_step': 12139, 'preemption_count': 0}), (15180, {'train/ctc_loss': 0.3168684489441938, 'train/wer': 0.11489823609226595, 'validation/ctc_loss': 0.5414772700516124, 'validation/wer': 0.16444744846231835, 'validation/num_examples': 5348, 'test/ctc_loss': 0.33150654376895283, 'test/wer': 0.11049499319562082, 'test/num_examples': 2472, 'score': 12003.719080209732, 'total_duration': 12183.256258010864, 'accumulated_submission_time': 12003.719080209732, 'accumulated_eval_time': 173.67812418937683, 'accumulated_logging_time': 0.17334818840026855, 'global_step': 15180, 'preemption_count': 0}), (18221, {'train/ctc_loss': 0.2859796002922518, 'train/wer': 0.10551967435549525, 'validation/ctc_loss': 0.5275388859064195, 'validation/wer': 0.15755322744170328, 'validation/num_examples': 5348, 'test/ctc_loss': 0.31425404461578166, 'test/wer': 0.10598582251741719, 'test/num_examples': 2472, 'score': 14403.274402618408, 'total_duration': 14612.320591449738, 'accumulated_submission_time': 14403.274402618408, 'accumulated_eval_time': 201.99989700317383, 'accumulated_logging_time': 0.20523476600646973, 'global_step': 18221, 'preemption_count': 0}), (20000, {'train/ctc_loss': 0.269938688863884, 'train/wer': 0.10047218453188603, 'validation/ctc_loss': 0.5111339309260097, 'validation/wer': 0.15405783807270795, 'validation/num_examples': 5348, 'test/ctc_loss': 0.30691110616007483, 'test/wer': 0.10306095505047427, 'test/num_examples': 2472, 'score': 15806.843983411789, 'total_duration': 16044.546288251877, 'accumulated_submission_time': 15806.843983411789, 'accumulated_eval_time': 229.94672107696533, 'accumulated_logging_time': 0.23627781867980957, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0609 22:05:48.484505 140485423044416 submission_runner.py:584] Timing: 15806.843983411789
I0609 22:05:48.484562 140485423044416 submission_runner.py:586] Total number of evals: 8
I0609 22:05:48.484618 140485423044416 submission_runner.py:587] ====================
I0609 22:05:48.484830 140485423044416 submission_runner.py:655] Final librispeech_conformer score: 15806.843983411789
