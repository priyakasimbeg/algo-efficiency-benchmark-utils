torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/nesterov --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_resnet_pytorch_06-08-2023-18-07-21.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0608 18:07:45.304449 140641582921536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0608 18:07:45.304485 139790659319616 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0608 18:07:45.304509 140380142942016 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0608 18:07:45.305299 139745873073984 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0608 18:07:45.305335 140088059696960 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0608 18:07:45.305411 140078584252224 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0608 18:07:45.305621 139627895228224 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0608 18:07:45.315882 139745873073984 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:45.315971 140088059696960 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:45.315857 140006680004416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0608 18:07:45.316004 140078584252224 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:45.316187 140006680004416 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:45.316265 139627895228224 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:45.325492 140641582921536 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:45.325520 139790659319616 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:45.325549 140380142942016 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:47.612560 140006680004416 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_resnet_pytorch.
W0608 18:07:47.742020 140078584252224 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:47.743017 140641582921536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:47.743129 139790659319616 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:47.743378 139745873073984 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:47.743934 140088059696960 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:47.744275 139627895228224 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:47.745062 140006680004416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:47.745138 140380142942016 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0608 18:07:47.750721 140006680004416 submission_runner.py:541] Using RNG seed 4011968202
I0608 18:07:47.752173 140006680004416 submission_runner.py:550] --- Tuning run 1/1 ---
I0608 18:07:47.752305 140006680004416 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_resnet_pytorch/trial_1.
I0608 18:07:47.752687 140006680004416 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_resnet_pytorch/trial_1/hparams.json.
I0608 18:07:47.753799 140006680004416 submission_runner.py:255] Initializing dataset.
I0608 18:07:54.198469 140006680004416 submission_runner.py:262] Initializing model.
I0608 18:07:58.636004 140006680004416 submission_runner.py:272] Initializing optimizer.
I0608 18:07:59.126061 140006680004416 submission_runner.py:279] Initializing metrics bundle.
I0608 18:07:59.126296 140006680004416 submission_runner.py:297] Initializing checkpoint and logger.
I0608 18:07:59.650024 140006680004416 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0608 18:07:59.651078 140006680004416 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0608 18:07:59.706343 140006680004416 submission_runner.py:332] Starting training loop.
I0608 18:08:07.980760 139977463682816 logging_writer.py:48] [0] global_step=0, grad_norm=0.525264, loss=6.931971
I0608 18:08:08.000169 140006680004416 submission.py:139] 0) loss = 6.932, grad_norm = 0.525
I0608 18:08:08.001310 140006680004416 spec.py:298] Evaluating on the training split.
I0608 18:09:09.465162 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 18:10:04.329296 140006680004416 spec.py:326] Evaluating on the test split.
I0608 18:10:04.348305 140006680004416 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 18:10:04.354521 140006680004416 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0608 18:10:04.432102 140006680004416 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 18:10:16.837642 140006680004416 submission_runner.py:419] Time since start: 137.13s, 	Step: 1, 	{'train/accuracy': 0.000896843112244898, 'train/loss': 6.924565529336735, 'validation/accuracy': 0.0012, 'validation/loss': 6.924400625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.92709296875, 'test/num_examples': 10000, 'score': 8.294957637786865, 'total_duration': 137.13165378570557, 'accumulated_submission_time': 8.294957637786865, 'accumulated_eval_time': 128.83624172210693, 'accumulated_logging_time': 0}
I0608 18:10:16.856595 139953891686144 logging_writer.py:48] [1] accumulated_eval_time=128.836242, accumulated_logging_time=0, accumulated_submission_time=8.294958, global_step=1, preemption_count=0, score=8.294958, test/accuracy=0.001000, test/loss=6.927093, test/num_examples=10000, total_duration=137.131654, train/accuracy=0.000897, train/loss=6.924566, validation/accuracy=0.001200, validation/loss=6.924401, validation/num_examples=50000
I0608 18:10:16.898860 140006680004416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:16.899165 139627895228224 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:16.899193 140078584252224 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:16.899198 140641582921536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:16.899219 140088059696960 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:16.899229 139790659319616 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:16.899234 139745873073984 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:16.899374 140380142942016 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:17.276216 139953883293440 logging_writer.py:48] [1] global_step=1, grad_norm=0.539781, loss=6.916802
I0608 18:10:17.280345 140006680004416 submission.py:139] 1) loss = 6.917, grad_norm = 0.540
I0608 18:10:17.659343 139953891686144 logging_writer.py:48] [2] global_step=2, grad_norm=0.536637, loss=6.929759
I0608 18:10:17.664118 140006680004416 submission.py:139] 2) loss = 6.930, grad_norm = 0.537
I0608 18:10:18.043028 139953883293440 logging_writer.py:48] [3] global_step=3, grad_norm=0.531710, loss=6.921318
I0608 18:10:18.046668 140006680004416 submission.py:139] 3) loss = 6.921, grad_norm = 0.532
I0608 18:10:18.422972 139953891686144 logging_writer.py:48] [4] global_step=4, grad_norm=0.527667, loss=6.925011
I0608 18:10:18.427821 140006680004416 submission.py:139] 4) loss = 6.925, grad_norm = 0.528
I0608 18:10:18.812280 139953883293440 logging_writer.py:48] [5] global_step=5, grad_norm=0.526902, loss=6.927318
I0608 18:10:18.815991 140006680004416 submission.py:139] 5) loss = 6.927, grad_norm = 0.527
I0608 18:10:19.196050 139953891686144 logging_writer.py:48] [6] global_step=6, grad_norm=0.544012, loss=6.926607
I0608 18:10:19.199954 140006680004416 submission.py:139] 6) loss = 6.927, grad_norm = 0.544
I0608 18:10:19.581383 139953883293440 logging_writer.py:48] [7] global_step=7, grad_norm=0.547281, loss=6.935565
I0608 18:10:19.586227 140006680004416 submission.py:139] 7) loss = 6.936, grad_norm = 0.547
I0608 18:10:19.964226 139953891686144 logging_writer.py:48] [8] global_step=8, grad_norm=0.541505, loss=6.933259
I0608 18:10:19.968216 140006680004416 submission.py:139] 8) loss = 6.933, grad_norm = 0.542
I0608 18:10:20.344817 139953883293440 logging_writer.py:48] [9] global_step=9, grad_norm=0.549362, loss=6.926957
I0608 18:10:20.348639 140006680004416 submission.py:139] 9) loss = 6.927, grad_norm = 0.549
I0608 18:10:20.727486 139953891686144 logging_writer.py:48] [10] global_step=10, grad_norm=0.540751, loss=6.931387
I0608 18:10:20.736307 140006680004416 submission.py:139] 10) loss = 6.931, grad_norm = 0.541
I0608 18:10:21.114010 139953883293440 logging_writer.py:48] [11] global_step=11, grad_norm=0.528849, loss=6.926974
I0608 18:10:21.119799 140006680004416 submission.py:139] 11) loss = 6.927, grad_norm = 0.529
I0608 18:10:21.502876 139953891686144 logging_writer.py:48] [12] global_step=12, grad_norm=0.530346, loss=6.934356
I0608 18:10:21.507236 140006680004416 submission.py:139] 12) loss = 6.934, grad_norm = 0.530
I0608 18:10:21.891380 139953883293440 logging_writer.py:48] [13] global_step=13, grad_norm=0.533559, loss=6.923942
I0608 18:10:21.895388 140006680004416 submission.py:139] 13) loss = 6.924, grad_norm = 0.534
I0608 18:10:22.274129 139953891686144 logging_writer.py:48] [14] global_step=14, grad_norm=0.543579, loss=6.936102
I0608 18:10:22.278301 140006680004416 submission.py:139] 14) loss = 6.936, grad_norm = 0.544
I0608 18:10:22.657218 139953883293440 logging_writer.py:48] [15] global_step=15, grad_norm=0.528810, loss=6.932203
I0608 18:10:22.663031 140006680004416 submission.py:139] 15) loss = 6.932, grad_norm = 0.529
I0608 18:10:23.040492 139953891686144 logging_writer.py:48] [16] global_step=16, grad_norm=0.517844, loss=6.929749
I0608 18:10:23.045362 140006680004416 submission.py:139] 16) loss = 6.930, grad_norm = 0.518
I0608 18:10:23.425286 139953883293440 logging_writer.py:48] [17] global_step=17, grad_norm=0.531232, loss=6.924508
I0608 18:10:23.430368 140006680004416 submission.py:139] 17) loss = 6.925, grad_norm = 0.531
I0608 18:10:23.809234 139953891686144 logging_writer.py:48] [18] global_step=18, grad_norm=0.535105, loss=6.922633
I0608 18:10:23.814485 140006680004416 submission.py:139] 18) loss = 6.923, grad_norm = 0.535
I0608 18:10:24.192330 139953883293440 logging_writer.py:48] [19] global_step=19, grad_norm=0.536881, loss=6.934087
I0608 18:10:24.196918 140006680004416 submission.py:139] 19) loss = 6.934, grad_norm = 0.537
I0608 18:10:24.573153 139953891686144 logging_writer.py:48] [20] global_step=20, grad_norm=0.544395, loss=6.928386
I0608 18:10:24.577802 140006680004416 submission.py:139] 20) loss = 6.928, grad_norm = 0.544
I0608 18:10:24.955731 139953883293440 logging_writer.py:48] [21] global_step=21, grad_norm=0.528170, loss=6.925750
I0608 18:10:24.961882 140006680004416 submission.py:139] 21) loss = 6.926, grad_norm = 0.528
I0608 18:10:25.350935 139953891686144 logging_writer.py:48] [22] global_step=22, grad_norm=0.528000, loss=6.912488
I0608 18:10:25.356064 140006680004416 submission.py:139] 22) loss = 6.912, grad_norm = 0.528
I0608 18:10:25.738475 139953883293440 logging_writer.py:48] [23] global_step=23, grad_norm=0.523880, loss=6.919353
I0608 18:10:25.742355 140006680004416 submission.py:139] 23) loss = 6.919, grad_norm = 0.524
I0608 18:10:26.121611 139953891686144 logging_writer.py:48] [24] global_step=24, grad_norm=0.536674, loss=6.928214
I0608 18:10:26.126362 140006680004416 submission.py:139] 24) loss = 6.928, grad_norm = 0.537
I0608 18:10:26.504437 139953883293440 logging_writer.py:48] [25] global_step=25, grad_norm=0.528661, loss=6.931973
I0608 18:10:26.509424 140006680004416 submission.py:139] 25) loss = 6.932, grad_norm = 0.529
I0608 18:10:26.887782 139953891686144 logging_writer.py:48] [26] global_step=26, grad_norm=0.538320, loss=6.925739
I0608 18:10:26.891760 140006680004416 submission.py:139] 26) loss = 6.926, grad_norm = 0.538
I0608 18:10:27.279110 139953883293440 logging_writer.py:48] [27] global_step=27, grad_norm=0.525957, loss=6.922849
I0608 18:10:27.283206 140006680004416 submission.py:139] 27) loss = 6.923, grad_norm = 0.526
I0608 18:10:27.661933 139953891686144 logging_writer.py:48] [28] global_step=28, grad_norm=0.535759, loss=6.926054
I0608 18:10:27.667458 140006680004416 submission.py:139] 28) loss = 6.926, grad_norm = 0.536
I0608 18:10:28.048005 139953883293440 logging_writer.py:48] [29] global_step=29, grad_norm=0.533801, loss=6.916967
I0608 18:10:28.052292 140006680004416 submission.py:139] 29) loss = 6.917, grad_norm = 0.534
I0608 18:10:28.429204 139953891686144 logging_writer.py:48] [30] global_step=30, grad_norm=0.534161, loss=6.924884
I0608 18:10:28.434225 140006680004416 submission.py:139] 30) loss = 6.925, grad_norm = 0.534
I0608 18:10:28.815611 139953883293440 logging_writer.py:48] [31] global_step=31, grad_norm=0.520022, loss=6.931110
I0608 18:10:28.820199 140006680004416 submission.py:139] 31) loss = 6.931, grad_norm = 0.520
I0608 18:10:29.203503 139953891686144 logging_writer.py:48] [32] global_step=32, grad_norm=0.517111, loss=6.929092
I0608 18:10:29.207580 140006680004416 submission.py:139] 32) loss = 6.929, grad_norm = 0.517
I0608 18:10:29.585048 139953883293440 logging_writer.py:48] [33] global_step=33, grad_norm=0.548008, loss=6.927708
I0608 18:10:29.595499 140006680004416 submission.py:139] 33) loss = 6.928, grad_norm = 0.548
I0608 18:10:29.975472 139953891686144 logging_writer.py:48] [34] global_step=34, grad_norm=0.524691, loss=6.910198
I0608 18:10:29.979419 140006680004416 submission.py:139] 34) loss = 6.910, grad_norm = 0.525
I0608 18:10:30.357619 139953883293440 logging_writer.py:48] [35] global_step=35, grad_norm=0.526885, loss=6.927043
I0608 18:10:30.361796 140006680004416 submission.py:139] 35) loss = 6.927, grad_norm = 0.527
I0608 18:10:30.744245 139953891686144 logging_writer.py:48] [36] global_step=36, grad_norm=0.541540, loss=6.927078
I0608 18:10:30.750140 140006680004416 submission.py:139] 36) loss = 6.927, grad_norm = 0.542
I0608 18:10:31.132633 139953883293440 logging_writer.py:48] [37] global_step=37, grad_norm=0.536365, loss=6.924824
I0608 18:10:31.136329 140006680004416 submission.py:139] 37) loss = 6.925, grad_norm = 0.536
I0608 18:10:31.513837 139953891686144 logging_writer.py:48] [38] global_step=38, grad_norm=0.521355, loss=6.919531
I0608 18:10:31.517966 140006680004416 submission.py:139] 38) loss = 6.920, grad_norm = 0.521
I0608 18:10:31.898298 139953883293440 logging_writer.py:48] [39] global_step=39, grad_norm=0.524270, loss=6.915146
I0608 18:10:31.902601 140006680004416 submission.py:139] 39) loss = 6.915, grad_norm = 0.524
I0608 18:10:32.285932 139953891686144 logging_writer.py:48] [40] global_step=40, grad_norm=0.534105, loss=6.926328
I0608 18:10:32.290535 140006680004416 submission.py:139] 40) loss = 6.926, grad_norm = 0.534
I0608 18:10:32.671202 139953883293440 logging_writer.py:48] [41] global_step=41, grad_norm=0.513697, loss=6.915631
I0608 18:10:32.675725 140006680004416 submission.py:139] 41) loss = 6.916, grad_norm = 0.514
I0608 18:10:33.054753 139953891686144 logging_writer.py:48] [42] global_step=42, grad_norm=0.521410, loss=6.916233
I0608 18:10:33.059727 140006680004416 submission.py:139] 42) loss = 6.916, grad_norm = 0.521
I0608 18:10:33.437059 139953883293440 logging_writer.py:48] [43] global_step=43, grad_norm=0.527980, loss=6.919093
I0608 18:10:33.446563 140006680004416 submission.py:139] 43) loss = 6.919, grad_norm = 0.528
I0608 18:10:33.826526 139953891686144 logging_writer.py:48] [44] global_step=44, grad_norm=0.529750, loss=6.916530
I0608 18:10:33.830477 140006680004416 submission.py:139] 44) loss = 6.917, grad_norm = 0.530
I0608 18:10:34.210671 139953883293440 logging_writer.py:48] [45] global_step=45, grad_norm=0.519701, loss=6.920929
I0608 18:10:34.214775 140006680004416 submission.py:139] 45) loss = 6.921, grad_norm = 0.520
I0608 18:10:34.601643 139953891686144 logging_writer.py:48] [46] global_step=46, grad_norm=0.539632, loss=6.908905
I0608 18:10:34.607340 140006680004416 submission.py:139] 46) loss = 6.909, grad_norm = 0.540
I0608 18:10:34.990766 139953883293440 logging_writer.py:48] [47] global_step=47, grad_norm=0.520834, loss=6.918766
I0608 18:10:34.995638 140006680004416 submission.py:139] 47) loss = 6.919, grad_norm = 0.521
I0608 18:10:35.376903 139953891686144 logging_writer.py:48] [48] global_step=48, grad_norm=0.525489, loss=6.921603
I0608 18:10:35.380886 140006680004416 submission.py:139] 48) loss = 6.922, grad_norm = 0.525
I0608 18:10:35.762328 139953883293440 logging_writer.py:48] [49] global_step=49, grad_norm=0.531605, loss=6.911294
I0608 18:10:35.766183 140006680004416 submission.py:139] 49) loss = 6.911, grad_norm = 0.532
I0608 18:10:36.146388 139953891686144 logging_writer.py:48] [50] global_step=50, grad_norm=0.525860, loss=6.923029
I0608 18:10:36.150290 140006680004416 submission.py:139] 50) loss = 6.923, grad_norm = 0.526
I0608 18:10:36.537524 139953883293440 logging_writer.py:48] [51] global_step=51, grad_norm=0.527579, loss=6.918919
I0608 18:10:36.541911 140006680004416 submission.py:139] 51) loss = 6.919, grad_norm = 0.528
I0608 18:10:36.923501 139953891686144 logging_writer.py:48] [52] global_step=52, grad_norm=0.517517, loss=6.916057
I0608 18:10:36.927755 140006680004416 submission.py:139] 52) loss = 6.916, grad_norm = 0.518
I0608 18:10:37.314641 139953883293440 logging_writer.py:48] [53] global_step=53, grad_norm=0.513749, loss=6.910686
I0608 18:10:37.319255 140006680004416 submission.py:139] 53) loss = 6.911, grad_norm = 0.514
I0608 18:10:37.699256 139953891686144 logging_writer.py:48] [54] global_step=54, grad_norm=0.537819, loss=6.920507
I0608 18:10:37.703767 140006680004416 submission.py:139] 54) loss = 6.921, grad_norm = 0.538
I0608 18:10:38.083096 139953883293440 logging_writer.py:48] [55] global_step=55, grad_norm=0.518464, loss=6.915176
I0608 18:10:38.087657 140006680004416 submission.py:139] 55) loss = 6.915, grad_norm = 0.518
I0608 18:10:38.468259 139953891686144 logging_writer.py:48] [56] global_step=56, grad_norm=0.523914, loss=6.904362
I0608 18:10:38.474637 140006680004416 submission.py:139] 56) loss = 6.904, grad_norm = 0.524
I0608 18:10:38.857101 139953883293440 logging_writer.py:48] [57] global_step=57, grad_norm=0.528064, loss=6.913017
I0608 18:10:38.861334 140006680004416 submission.py:139] 57) loss = 6.913, grad_norm = 0.528
I0608 18:10:39.239919 139953891686144 logging_writer.py:48] [58] global_step=58, grad_norm=0.539854, loss=6.907646
I0608 18:10:39.243871 140006680004416 submission.py:139] 58) loss = 6.908, grad_norm = 0.540
I0608 18:10:39.630873 139953883293440 logging_writer.py:48] [59] global_step=59, grad_norm=0.534398, loss=6.913098
I0608 18:10:39.636298 140006680004416 submission.py:139] 59) loss = 6.913, grad_norm = 0.534
I0608 18:10:40.021595 139953891686144 logging_writer.py:48] [60] global_step=60, grad_norm=0.523323, loss=6.911005
I0608 18:10:40.026185 140006680004416 submission.py:139] 60) loss = 6.911, grad_norm = 0.523
I0608 18:10:40.403107 139953883293440 logging_writer.py:48] [61] global_step=61, grad_norm=0.511301, loss=6.910264
I0608 18:10:40.408174 140006680004416 submission.py:139] 61) loss = 6.910, grad_norm = 0.511
I0608 18:10:40.787418 139953891686144 logging_writer.py:48] [62] global_step=62, grad_norm=0.507392, loss=6.909077
I0608 18:10:40.792246 140006680004416 submission.py:139] 62) loss = 6.909, grad_norm = 0.507
I0608 18:10:41.172230 139953883293440 logging_writer.py:48] [63] global_step=63, grad_norm=0.529855, loss=6.907440
I0608 18:10:41.176193 140006680004416 submission.py:139] 63) loss = 6.907, grad_norm = 0.530
I0608 18:10:41.558737 139953891686144 logging_writer.py:48] [64] global_step=64, grad_norm=0.547011, loss=6.906387
I0608 18:10:41.562914 140006680004416 submission.py:139] 64) loss = 6.906, grad_norm = 0.547
I0608 18:10:41.957290 139953883293440 logging_writer.py:48] [65] global_step=65, grad_norm=0.524466, loss=6.907940
I0608 18:10:41.962618 140006680004416 submission.py:139] 65) loss = 6.908, grad_norm = 0.524
I0608 18:10:42.344480 139953891686144 logging_writer.py:48] [66] global_step=66, grad_norm=0.526788, loss=6.911175
I0608 18:10:42.348653 140006680004416 submission.py:139] 66) loss = 6.911, grad_norm = 0.527
I0608 18:10:42.729826 139953883293440 logging_writer.py:48] [67] global_step=67, grad_norm=0.531289, loss=6.908977
I0608 18:10:42.734225 140006680004416 submission.py:139] 67) loss = 6.909, grad_norm = 0.531
I0608 18:10:43.114941 139953891686144 logging_writer.py:48] [68] global_step=68, grad_norm=0.527146, loss=6.909317
I0608 18:10:43.120082 140006680004416 submission.py:139] 68) loss = 6.909, grad_norm = 0.527
I0608 18:10:43.501455 139953883293440 logging_writer.py:48] [69] global_step=69, grad_norm=0.507932, loss=6.903761
I0608 18:10:43.512188 140006680004416 submission.py:139] 69) loss = 6.904, grad_norm = 0.508
I0608 18:10:43.891409 139953891686144 logging_writer.py:48] [70] global_step=70, grad_norm=0.520165, loss=6.901332
I0608 18:10:43.896390 140006680004416 submission.py:139] 70) loss = 6.901, grad_norm = 0.520
I0608 18:10:44.275275 139953883293440 logging_writer.py:48] [71] global_step=71, grad_norm=0.524234, loss=6.902841
I0608 18:10:44.280585 140006680004416 submission.py:139] 71) loss = 6.903, grad_norm = 0.524
I0608 18:10:44.659911 139953891686144 logging_writer.py:48] [72] global_step=72, grad_norm=0.524666, loss=6.899766
I0608 18:10:44.665563 140006680004416 submission.py:139] 72) loss = 6.900, grad_norm = 0.525
I0608 18:10:45.048669 139953883293440 logging_writer.py:48] [73] global_step=73, grad_norm=0.542207, loss=6.898847
I0608 18:10:45.055152 140006680004416 submission.py:139] 73) loss = 6.899, grad_norm = 0.542
I0608 18:10:45.436611 139953891686144 logging_writer.py:48] [74] global_step=74, grad_norm=0.508969, loss=6.898413
I0608 18:10:45.440729 140006680004416 submission.py:139] 74) loss = 6.898, grad_norm = 0.509
I0608 18:10:45.816936 139953883293440 logging_writer.py:48] [75] global_step=75, grad_norm=0.522066, loss=6.906230
I0608 18:10:45.820892 140006680004416 submission.py:139] 75) loss = 6.906, grad_norm = 0.522
I0608 18:10:46.203558 139953891686144 logging_writer.py:48] [76] global_step=76, grad_norm=0.517658, loss=6.905914
I0608 18:10:46.207922 140006680004416 submission.py:139] 76) loss = 6.906, grad_norm = 0.518
I0608 18:10:46.587953 139953883293440 logging_writer.py:48] [77] global_step=77, grad_norm=0.518404, loss=6.899621
I0608 18:10:46.593299 140006680004416 submission.py:139] 77) loss = 6.900, grad_norm = 0.518
I0608 18:10:46.974081 139953891686144 logging_writer.py:48] [78] global_step=78, grad_norm=0.526899, loss=6.896272
I0608 18:10:46.979124 140006680004416 submission.py:139] 78) loss = 6.896, grad_norm = 0.527
I0608 18:10:47.360768 139953883293440 logging_writer.py:48] [79] global_step=79, grad_norm=0.541898, loss=6.904053
I0608 18:10:47.364526 140006680004416 submission.py:139] 79) loss = 6.904, grad_norm = 0.542
I0608 18:10:47.744570 139953891686144 logging_writer.py:48] [80] global_step=80, grad_norm=0.510928, loss=6.898731
I0608 18:10:47.749001 140006680004416 submission.py:139] 80) loss = 6.899, grad_norm = 0.511
I0608 18:10:48.132602 139953883293440 logging_writer.py:48] [81] global_step=81, grad_norm=0.514653, loss=6.904319
I0608 18:10:48.137509 140006680004416 submission.py:139] 81) loss = 6.904, grad_norm = 0.515
I0608 18:10:48.519506 139953891686144 logging_writer.py:48] [82] global_step=82, grad_norm=0.517157, loss=6.896960
I0608 18:10:48.523739 140006680004416 submission.py:139] 82) loss = 6.897, grad_norm = 0.517
I0608 18:10:48.905661 139953883293440 logging_writer.py:48] [83] global_step=83, grad_norm=0.529725, loss=6.896297
I0608 18:10:48.910836 140006680004416 submission.py:139] 83) loss = 6.896, grad_norm = 0.530
I0608 18:10:49.292392 139953891686144 logging_writer.py:48] [84] global_step=84, grad_norm=0.519832, loss=6.907932
I0608 18:10:49.297066 140006680004416 submission.py:139] 84) loss = 6.908, grad_norm = 0.520
I0608 18:10:49.680604 139953883293440 logging_writer.py:48] [85] global_step=85, grad_norm=0.512596, loss=6.900899
I0608 18:10:49.687603 140006680004416 submission.py:139] 85) loss = 6.901, grad_norm = 0.513
I0608 18:10:50.069499 139953891686144 logging_writer.py:48] [86] global_step=86, grad_norm=0.526777, loss=6.894686
I0608 18:10:50.074329 140006680004416 submission.py:139] 86) loss = 6.895, grad_norm = 0.527
I0608 18:10:50.456923 139953883293440 logging_writer.py:48] [87] global_step=87, grad_norm=0.517891, loss=6.892114
I0608 18:10:50.462185 140006680004416 submission.py:139] 87) loss = 6.892, grad_norm = 0.518
I0608 18:10:50.843074 139953891686144 logging_writer.py:48] [88] global_step=88, grad_norm=0.523255, loss=6.897896
I0608 18:10:50.848017 140006680004416 submission.py:139] 88) loss = 6.898, grad_norm = 0.523
I0608 18:10:51.234698 139953883293440 logging_writer.py:48] [89] global_step=89, grad_norm=0.518827, loss=6.887026
I0608 18:10:51.240017 140006680004416 submission.py:139] 89) loss = 6.887, grad_norm = 0.519
I0608 18:10:51.623913 139953891686144 logging_writer.py:48] [90] global_step=90, grad_norm=0.533370, loss=6.896444
I0608 18:10:51.630646 140006680004416 submission.py:139] 90) loss = 6.896, grad_norm = 0.533
I0608 18:10:52.010636 139953883293440 logging_writer.py:48] [91] global_step=91, grad_norm=0.517696, loss=6.890913
I0608 18:10:52.014817 140006680004416 submission.py:139] 91) loss = 6.891, grad_norm = 0.518
I0608 18:10:52.402355 139953891686144 logging_writer.py:48] [92] global_step=92, grad_norm=0.532099, loss=6.891954
I0608 18:10:52.406291 140006680004416 submission.py:139] 92) loss = 6.892, grad_norm = 0.532
I0608 18:10:52.785722 139953883293440 logging_writer.py:48] [93] global_step=93, grad_norm=0.514669, loss=6.889782
I0608 18:10:52.789823 140006680004416 submission.py:139] 93) loss = 6.890, grad_norm = 0.515
I0608 18:10:53.170408 139953891686144 logging_writer.py:48] [94] global_step=94, grad_norm=0.513095, loss=6.894588
I0608 18:10:53.178121 140006680004416 submission.py:139] 94) loss = 6.895, grad_norm = 0.513
I0608 18:10:53.559433 139953883293440 logging_writer.py:48] [95] global_step=95, grad_norm=0.525183, loss=6.892871
I0608 18:10:53.563505 140006680004416 submission.py:139] 95) loss = 6.893, grad_norm = 0.525
I0608 18:10:53.949657 139953891686144 logging_writer.py:48] [96] global_step=96, grad_norm=0.516789, loss=6.891920
I0608 18:10:53.954652 140006680004416 submission.py:139] 96) loss = 6.892, grad_norm = 0.517
I0608 18:10:54.336845 139953883293440 logging_writer.py:48] [97] global_step=97, grad_norm=0.521187, loss=6.885851
I0608 18:10:54.340621 140006680004416 submission.py:139] 97) loss = 6.886, grad_norm = 0.521
I0608 18:10:54.732238 139953891686144 logging_writer.py:48] [98] global_step=98, grad_norm=0.528173, loss=6.885475
I0608 18:10:54.736924 140006680004416 submission.py:139] 98) loss = 6.885, grad_norm = 0.528
I0608 18:10:55.123892 139953883293440 logging_writer.py:48] [99] global_step=99, grad_norm=0.530167, loss=6.880705
I0608 18:10:55.127815 140006680004416 submission.py:139] 99) loss = 6.881, grad_norm = 0.530
I0608 18:10:55.509280 139953891686144 logging_writer.py:48] [100] global_step=100, grad_norm=0.523066, loss=6.893568
I0608 18:10:55.513059 140006680004416 submission.py:139] 100) loss = 6.894, grad_norm = 0.523
I0608 18:13:25.427852 139953883293440 logging_writer.py:48] [500] global_step=500, grad_norm=0.623797, loss=6.568675
I0608 18:13:25.432548 140006680004416 submission.py:139] 500) loss = 6.569, grad_norm = 0.624
I0608 18:16:32.880230 139953891686144 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.799327, loss=6.263292
I0608 18:16:32.885967 140006680004416 submission.py:139] 1000) loss = 6.263, grad_norm = 0.799
I0608 18:18:47.184715 140006680004416 spec.py:298] Evaluating on the training split.
I0608 18:19:28.669536 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 18:20:23.694279 140006680004416 spec.py:326] Evaluating on the test split.
I0608 18:20:25.139950 140006680004416 submission_runner.py:419] Time since start: 745.43s, 	Step: 1356, 	{'train/accuracy': 0.0653499681122449, 'train/loss': 5.55831224091199, 'validation/accuracy': 0.0611, 'validation/loss': 5.621090625, 'validation/num_examples': 50000, 'test/accuracy': 0.0396, 'test/loss': 5.849264453125, 'test/num_examples': 10000, 'score': 517.6732680797577, 'total_duration': 745.4340023994446, 'accumulated_submission_time': 517.6732680797577, 'accumulated_eval_time': 226.79152274131775, 'accumulated_logging_time': 0.028676748275756836}
I0608 18:20:25.150366 139953900078848 logging_writer.py:48] [1356] accumulated_eval_time=226.791523, accumulated_logging_time=0.028677, accumulated_submission_time=517.673268, global_step=1356, preemption_count=0, score=517.673268, test/accuracy=0.039600, test/loss=5.849264, test/num_examples=10000, total_duration=745.434002, train/accuracy=0.065350, train/loss=5.558312, validation/accuracy=0.061100, validation/loss=5.621091, validation/num_examples=50000
I0608 18:21:19.204107 139953908471552 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.836198, loss=5.988279
I0608 18:21:19.208466 140006680004416 submission.py:139] 1500) loss = 5.988, grad_norm = 0.836
I0608 18:24:26.250492 139953900078848 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.823300, loss=5.720612
I0608 18:24:26.255827 140006680004416 submission.py:139] 2000) loss = 5.721, grad_norm = 0.823
I0608 18:27:33.346850 139953908471552 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.966788, loss=5.444454
I0608 18:27:33.351080 140006680004416 submission.py:139] 2500) loss = 5.444, grad_norm = 0.967
I0608 18:28:55.468592 140006680004416 spec.py:298] Evaluating on the training split.
I0608 18:29:42.765310 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 18:30:39.542138 140006680004416 spec.py:326] Evaluating on the test split.
I0608 18:30:40.945629 140006680004416 submission_runner.py:419] Time since start: 1361.24s, 	Step: 2716, 	{'train/accuracy': 0.1757015306122449, 'train/loss': 4.338580073142539, 'validation/accuracy': 0.15778, 'validation/loss': 4.448470625, 'validation/num_examples': 50000, 'test/accuracy': 0.1089, 'test/loss': 4.9316546875, 'test/num_examples': 10000, 'score': 1027.1829342842102, 'total_duration': 1361.238109111786, 'accumulated_submission_time': 1027.1829342842102, 'accumulated_eval_time': 332.26693630218506, 'accumulated_logging_time': 0.04788327217102051}
I0608 18:30:40.958093 139953900078848 logging_writer.py:48] [2716] accumulated_eval_time=332.266936, accumulated_logging_time=0.047883, accumulated_submission_time=1027.182934, global_step=2716, preemption_count=0, score=1027.182934, test/accuracy=0.108900, test/loss=4.931655, test/num_examples=10000, total_duration=1361.238109, train/accuracy=0.175702, train/loss=4.338580, validation/accuracy=0.157780, validation/loss=4.448471, validation/num_examples=50000
I0608 18:32:27.280697 139953908471552 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.925385, loss=5.188969
I0608 18:32:27.285207 140006680004416 submission.py:139] 3000) loss = 5.189, grad_norm = 0.925
I0608 18:35:34.229393 139953900078848 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.977110, loss=4.859421
I0608 18:35:34.233821 140006680004416 submission.py:139] 3500) loss = 4.859, grad_norm = 0.977
I0608 18:38:42.729762 139953908471552 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.868770, loss=4.808224
I0608 18:38:42.734197 140006680004416 submission.py:139] 4000) loss = 4.808, grad_norm = 0.869
I0608 18:39:11.110747 140006680004416 spec.py:298] Evaluating on the training split.
I0608 18:39:52.761531 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 18:40:36.965370 140006680004416 spec.py:326] Evaluating on the test split.
I0608 18:40:38.363913 140006680004416 submission_runner.py:419] Time since start: 1958.66s, 	Step: 4077, 	{'train/accuracy': 0.30604272959183676, 'train/loss': 3.4379403250558034, 'validation/accuracy': 0.28186, 'validation/loss': 3.5829640625, 'validation/num_examples': 50000, 'test/accuracy': 0.2082, 'test/loss': 4.1565984375, 'test/num_examples': 10000, 'score': 1536.542601108551, 'total_duration': 1958.6580181121826, 'accumulated_submission_time': 1536.542601108551, 'accumulated_eval_time': 419.5202021598816, 'accumulated_logging_time': 0.06949114799499512}
I0608 18:40:38.373805 139953900078848 logging_writer.py:48] [4077] accumulated_eval_time=419.520202, accumulated_logging_time=0.069491, accumulated_submission_time=1536.542601, global_step=4077, preemption_count=0, score=1536.542601, test/accuracy=0.208200, test/loss=4.156598, test/num_examples=10000, total_duration=1958.658018, train/accuracy=0.306043, train/loss=3.437940, validation/accuracy=0.281860, validation/loss=3.582964, validation/num_examples=50000
I0608 18:43:16.613083 139953908471552 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.784416, loss=4.487218
I0608 18:43:16.619294 140006680004416 submission.py:139] 4500) loss = 4.487, grad_norm = 0.784
I0608 18:46:23.510788 139953900078848 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.752574, loss=4.392561
I0608 18:46:23.514867 140006680004416 submission.py:139] 5000) loss = 4.393, grad_norm = 0.753
I0608 18:49:08.379966 140006680004416 spec.py:298] Evaluating on the training split.
I0608 18:49:52.413467 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 18:50:49.070853 140006680004416 spec.py:326] Evaluating on the test split.
I0608 18:50:50.465116 140006680004416 submission_runner.py:419] Time since start: 2570.76s, 	Step: 5438, 	{'train/accuracy': 0.4176498724489796, 'train/loss': 2.891338114835778, 'validation/accuracy': 0.38306, 'validation/loss': 3.06608625, 'validation/num_examples': 50000, 'test/accuracy': 0.2758, 'test/loss': 3.716065234375, 'test/num_examples': 10000, 'score': 2045.7616124153137, 'total_duration': 2570.7574701309204, 'accumulated_submission_time': 2045.7616124153137, 'accumulated_eval_time': 521.6036593914032, 'accumulated_logging_time': 0.08748221397399902}
I0608 18:50:50.475458 139953908471552 logging_writer.py:48] [5438] accumulated_eval_time=521.603659, accumulated_logging_time=0.087482, accumulated_submission_time=2045.761612, global_step=5438, preemption_count=0, score=2045.761612, test/accuracy=0.275800, test/loss=3.716065, test/num_examples=10000, total_duration=2570.757470, train/accuracy=0.417650, train/loss=2.891338, validation/accuracy=0.383060, validation/loss=3.066086, validation/num_examples=50000
I0608 18:51:14.020725 139953900078848 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.770375, loss=4.372702
I0608 18:51:14.024694 140006680004416 submission.py:139] 5500) loss = 4.373, grad_norm = 0.770
I0608 18:54:20.903880 139953908471552 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.711443, loss=4.218791
I0608 18:54:20.908352 140006680004416 submission.py:139] 6000) loss = 4.219, grad_norm = 0.711
I0608 18:57:29.202986 139953900078848 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.722867, loss=4.130226
I0608 18:57:29.208376 140006680004416 submission.py:139] 6500) loss = 4.130, grad_norm = 0.723
I0608 18:59:20.494035 140006680004416 spec.py:298] Evaluating on the training split.
I0608 19:00:03.244663 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 19:00:48.122216 140006680004416 spec.py:326] Evaluating on the test split.
I0608 19:00:49.518142 140006680004416 submission_runner.py:419] Time since start: 3169.81s, 	Step: 6799, 	{'train/accuracy': 0.48026945153061223, 'train/loss': 2.4624608876753826, 'validation/accuracy': 0.43782, 'validation/loss': 2.6665840625, 'validation/num_examples': 50000, 'test/accuracy': 0.3121, 'test/loss': 3.411365625, 'test/num_examples': 10000, 'score': 2554.9893622398376, 'total_duration': 3169.8122313022614, 'accumulated_submission_time': 2554.9893622398376, 'accumulated_eval_time': 610.6279594898224, 'accumulated_logging_time': 0.10588264465332031}
I0608 19:00:49.528547 139953908471552 logging_writer.py:48] [6799] accumulated_eval_time=610.627959, accumulated_logging_time=0.105883, accumulated_submission_time=2554.989362, global_step=6799, preemption_count=0, score=2554.989362, test/accuracy=0.312100, test/loss=3.411366, test/num_examples=10000, total_duration=3169.812231, train/accuracy=0.480269, train/loss=2.462461, validation/accuracy=0.437820, validation/loss=2.666584, validation/num_examples=50000
I0608 19:02:04.819139 139953900078848 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.631491, loss=4.073456
I0608 19:02:04.823834 140006680004416 submission.py:139] 7000) loss = 4.073, grad_norm = 0.631
I0608 19:05:11.732799 139953908471552 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.603379, loss=4.039064
I0608 19:05:11.737126 140006680004416 submission.py:139] 7500) loss = 4.039, grad_norm = 0.603
I0608 19:08:19.770352 139953900078848 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.620194, loss=3.941955
I0608 19:08:19.774929 140006680004416 submission.py:139] 8000) loss = 3.942, grad_norm = 0.620
I0608 19:09:19.855502 140006680004416 spec.py:298] Evaluating on the training split.
I0608 19:10:03.452594 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 19:10:59.686206 140006680004416 spec.py:326] Evaluating on the test split.
I0608 19:11:01.079614 140006680004416 submission_runner.py:419] Time since start: 3781.37s, 	Step: 8162, 	{'train/accuracy': 0.5126953125, 'train/loss': 2.349236624581473, 'validation/accuracy': 0.4716, 'validation/loss': 2.54776921875, 'validation/num_examples': 50000, 'test/accuracy': 0.3584, 'test/loss': 3.2217287109375, 'test/num_examples': 10000, 'score': 3064.524979352951, 'total_duration': 3781.3736600875854, 'accumulated_submission_time': 3064.524979352951, 'accumulated_eval_time': 711.8522181510925, 'accumulated_logging_time': 0.1246027946472168}
I0608 19:11:01.092206 139953908471552 logging_writer.py:48] [8162] accumulated_eval_time=711.852218, accumulated_logging_time=0.124603, accumulated_submission_time=3064.524979, global_step=8162, preemption_count=0, score=3064.524979, test/accuracy=0.358400, test/loss=3.221729, test/num_examples=10000, total_duration=3781.373660, train/accuracy=0.512695, train/loss=2.349237, validation/accuracy=0.471600, validation/loss=2.547769, validation/num_examples=50000
I0608 19:13:07.741704 139953900078848 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.569039, loss=3.757287
I0608 19:13:07.746692 140006680004416 submission.py:139] 8500) loss = 3.757, grad_norm = 0.569
I0608 19:16:16.236720 139953908471552 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.567151, loss=3.759839
I0608 19:16:16.241847 140006680004416 submission.py:139] 9000) loss = 3.760, grad_norm = 0.567
I0608 19:19:22.944438 139953900078848 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.547744, loss=3.772948
I0608 19:19:22.949303 140006680004416 submission.py:139] 9500) loss = 3.773, grad_norm = 0.548
I0608 19:19:31.166280 140006680004416 spec.py:298] Evaluating on the training split.
I0608 19:20:16.771280 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 19:21:03.003180 140006680004416 spec.py:326] Evaluating on the test split.
I0608 19:21:04.398602 140006680004416 submission_runner.py:419] Time since start: 4384.69s, 	Step: 9523, 	{'train/accuracy': 0.5837252869897959, 'train/loss': 1.96003380600287, 'validation/accuracy': 0.5329, 'validation/loss': 2.1948328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4076, 'test/loss': 2.88578125, 'test/num_examples': 10000, 'score': 3573.8027267456055, 'total_duration': 4384.692658185959, 'accumulated_submission_time': 3573.8027267456055, 'accumulated_eval_time': 805.0845923423767, 'accumulated_logging_time': 0.1455526351928711}
I0608 19:21:04.408957 139953908471552 logging_writer.py:48] [9523] accumulated_eval_time=805.084592, accumulated_logging_time=0.145553, accumulated_submission_time=3573.802727, global_step=9523, preemption_count=0, score=3573.802727, test/accuracy=0.407600, test/loss=2.885781, test/num_examples=10000, total_duration=4384.692658, train/accuracy=0.583725, train/loss=1.960034, validation/accuracy=0.532900, validation/loss=2.194833, validation/num_examples=50000
I0608 19:24:02.941905 139953900078848 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.544445, loss=3.655271
I0608 19:24:02.946598 140006680004416 submission.py:139] 10000) loss = 3.655, grad_norm = 0.544
I0608 19:27:11.116950 139953908471552 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.510378, loss=3.645231
I0608 19:27:11.121953 140006680004416 submission.py:139] 10500) loss = 3.645, grad_norm = 0.510
I0608 19:29:34.712451 140006680004416 spec.py:298] Evaluating on the training split.
I0608 19:30:19.101327 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 19:31:12.594475 140006680004416 spec.py:326] Evaluating on the test split.
I0608 19:31:13.992703 140006680004416 submission_runner.py:419] Time since start: 4994.29s, 	Step: 10885, 	{'train/accuracy': 0.6113480548469388, 'train/loss': 1.888406870316486, 'validation/accuracy': 0.5597, 'validation/loss': 2.128869375, 'validation/num_examples': 50000, 'test/accuracy': 0.4408, 'test/loss': 2.7625251953125, 'test/num_examples': 10000, 'score': 4083.3121089935303, 'total_duration': 4994.286746740341, 'accumulated_submission_time': 4083.3121089935303, 'accumulated_eval_time': 904.3648903369904, 'accumulated_logging_time': 0.1652359962463379}
I0608 19:31:14.004132 139953900078848 logging_writer.py:48] [10885] accumulated_eval_time=904.364890, accumulated_logging_time=0.165236, accumulated_submission_time=4083.312109, global_step=10885, preemption_count=0, score=4083.312109, test/accuracy=0.440800, test/loss=2.762525, test/num_examples=10000, total_duration=4994.286747, train/accuracy=0.611348, train/loss=1.888407, validation/accuracy=0.559700, validation/loss=2.128869, validation/num_examples=50000
I0608 19:31:57.274435 139953908471552 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.540755, loss=3.613524
I0608 19:31:57.281495 140006680004416 submission.py:139] 11000) loss = 3.614, grad_norm = 0.541
I0608 19:35:05.512105 139953900078848 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.501747, loss=3.537306
I0608 19:35:05.517524 140006680004416 submission.py:139] 11500) loss = 3.537, grad_norm = 0.502
I0608 19:38:12.284994 139953908471552 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.516926, loss=3.543693
I0608 19:38:12.289623 140006680004416 submission.py:139] 12000) loss = 3.544, grad_norm = 0.517
I0608 19:39:44.279794 140006680004416 spec.py:298] Evaluating on the training split.
I0608 19:40:28.229162 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 19:41:13.108697 140006680004416 spec.py:326] Evaluating on the test split.
I0608 19:41:14.504609 140006680004416 submission_runner.py:419] Time since start: 5594.80s, 	Step: 12247, 	{'train/accuracy': 0.6455476721938775, 'train/loss': 1.7252426147460938, 'validation/accuracy': 0.58672, 'validation/loss': 1.9949953125, 'validation/num_examples': 50000, 'test/accuracy': 0.455, 'test/loss': 2.686128125, 'test/num_examples': 10000, 'score': 4592.796915769577, 'total_duration': 5594.798699617386, 'accumulated_submission_time': 4592.796915769577, 'accumulated_eval_time': 994.5898234844208, 'accumulated_logging_time': 0.18460631370544434}
I0608 19:41:14.515638 139953900078848 logging_writer.py:48] [12247] accumulated_eval_time=994.589823, accumulated_logging_time=0.184606, accumulated_submission_time=4592.796916, global_step=12247, preemption_count=0, score=4592.796916, test/accuracy=0.455000, test/loss=2.686128, test/num_examples=10000, total_duration=5594.798700, train/accuracy=0.645548, train/loss=1.725243, validation/accuracy=0.586720, validation/loss=1.994995, validation/num_examples=50000
I0608 19:42:49.345406 139953908471552 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.505416, loss=3.505466
I0608 19:42:49.350542 140006680004416 submission.py:139] 12500) loss = 3.505, grad_norm = 0.505
I0608 19:45:57.424988 139953900078848 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.511781, loss=3.451607
I0608 19:45:57.429260 140006680004416 submission.py:139] 13000) loss = 3.452, grad_norm = 0.512
I0608 19:49:04.297816 139953908471552 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.491878, loss=3.438637
I0608 19:49:04.302507 140006680004416 submission.py:139] 13500) loss = 3.439, grad_norm = 0.492
I0608 19:49:44.743228 140006680004416 spec.py:298] Evaluating on the training split.
I0608 19:50:28.111197 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 19:51:25.997838 140006680004416 spec.py:326] Evaluating on the test split.
I0608 19:51:27.395393 140006680004416 submission_runner.py:419] Time since start: 6207.69s, 	Step: 13609, 	{'train/accuracy': 0.6655373086734694, 'train/loss': 1.554753829021843, 'validation/accuracy': 0.60386, 'validation/loss': 1.843734375, 'validation/num_examples': 50000, 'test/accuracy': 0.4735, 'test/loss': 2.5420916015625, 'test/num_examples': 10000, 'score': 5102.22488284111, 'total_duration': 6207.687874555588, 'accumulated_submission_time': 5102.22488284111, 'accumulated_eval_time': 1097.240421295166, 'accumulated_logging_time': 0.2052922248840332}
I0608 19:51:27.406021 139953900078848 logging_writer.py:48] [13609] accumulated_eval_time=1097.240421, accumulated_logging_time=0.205292, accumulated_submission_time=5102.224883, global_step=13609, preemption_count=0, score=5102.224883, test/accuracy=0.473500, test/loss=2.542092, test/num_examples=10000, total_duration=6207.687875, train/accuracy=0.665537, train/loss=1.554754, validation/accuracy=0.603860, validation/loss=1.843734, validation/num_examples=50000
I0608 19:53:55.266082 139953908471552 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.467681, loss=3.360667
I0608 19:53:55.271629 140006680004416 submission.py:139] 14000) loss = 3.361, grad_norm = 0.468
I0608 19:57:01.939882 139953900078848 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.471029, loss=3.365344
I0608 19:57:01.945205 140006680004416 submission.py:139] 14500) loss = 3.365, grad_norm = 0.471
I0608 19:59:57.670704 140006680004416 spec.py:298] Evaluating on the training split.
I0608 20:00:41.438271 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 20:01:28.024977 140006680004416 spec.py:326] Evaluating on the test split.
I0608 20:01:29.432926 140006680004416 submission_runner.py:419] Time since start: 6809.73s, 	Step: 14971, 	{'train/accuracy': 0.6829161352040817, 'train/loss': 1.501330161581234, 'validation/accuracy': 0.61794, 'validation/loss': 1.80166, 'validation/num_examples': 50000, 'test/accuracy': 0.4725, 'test/loss': 2.5571177734375, 'test/num_examples': 10000, 'score': 5611.70737361908, 'total_duration': 6809.726997852325, 'accumulated_submission_time': 5611.70737361908, 'accumulated_eval_time': 1189.0025811195374, 'accumulated_logging_time': 0.22553801536560059}
I0608 20:01:29.443653 139953908471552 logging_writer.py:48] [14971] accumulated_eval_time=1189.002581, accumulated_logging_time=0.225538, accumulated_submission_time=5611.707374, global_step=14971, preemption_count=0, score=5611.707374, test/accuracy=0.472500, test/loss=2.557118, test/num_examples=10000, total_duration=6809.726998, train/accuracy=0.682916, train/loss=1.501330, validation/accuracy=0.617940, validation/loss=1.801660, validation/num_examples=50000
I0608 20:01:40.625915 139953900078848 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.453682, loss=3.336128
I0608 20:01:40.630826 140006680004416 submission.py:139] 15000) loss = 3.336, grad_norm = 0.454
I0608 20:04:48.710776 139953908471552 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.465307, loss=3.299784
I0608 20:04:48.715316 140006680004416 submission.py:139] 15500) loss = 3.300, grad_norm = 0.465
I0608 20:07:55.446547 139953900078848 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.475655, loss=3.364558
I0608 20:07:55.452399 140006680004416 submission.py:139] 16000) loss = 3.365, grad_norm = 0.476
I0608 20:09:59.606827 140006680004416 spec.py:298] Evaluating on the training split.
I0608 20:10:43.891141 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 20:11:40.819888 140006680004416 spec.py:326] Evaluating on the test split.
I0608 20:11:42.212483 140006680004416 submission_runner.py:419] Time since start: 7422.51s, 	Step: 16328, 	{'train/accuracy': 0.6994379783163265, 'train/loss': 1.481515378368144, 'validation/accuracy': 0.62856, 'validation/loss': 1.8021921875, 'validation/num_examples': 50000, 'test/accuracy': 0.4922, 'test/loss': 2.5002375, 'test/num_examples': 10000, 'score': 6121.0934019088745, 'total_duration': 7422.506525278091, 'accumulated_submission_time': 6121.0934019088745, 'accumulated_eval_time': 1291.6083552837372, 'accumulated_logging_time': 0.2459256649017334}
I0608 20:11:42.223967 139953908471552 logging_writer.py:48] [16328] accumulated_eval_time=1291.608355, accumulated_logging_time=0.245926, accumulated_submission_time=6121.093402, global_step=16328, preemption_count=0, score=6121.093402, test/accuracy=0.492200, test/loss=2.500237, test/num_examples=10000, total_duration=7422.506525, train/accuracy=0.699438, train/loss=1.481515, validation/accuracy=0.628560, validation/loss=1.802192, validation/num_examples=50000
I0608 20:12:46.631863 139953900078848 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.478034, loss=3.321690
I0608 20:12:46.636085 140006680004416 submission.py:139] 16500) loss = 3.322, grad_norm = 0.478
I0608 20:15:53.378890 139953908471552 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.471464, loss=3.322679
I0608 20:15:53.383465 140006680004416 submission.py:139] 17000) loss = 3.323, grad_norm = 0.471
I0608 20:19:00.254916 139953900078848 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.469009, loss=3.327741
I0608 20:19:00.260483 140006680004416 submission.py:139] 17500) loss = 3.328, grad_norm = 0.469
I0608 20:20:12.418821 140006680004416 spec.py:298] Evaluating on the training split.
I0608 20:20:56.924935 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 20:21:55.354461 140006680004416 spec.py:326] Evaluating on the test split.
I0608 20:21:56.749416 140006680004416 submission_runner.py:419] Time since start: 8037.04s, 	Step: 17690, 	{'train/accuracy': 0.7140465561224489, 'train/loss': 1.328186813665896, 'validation/accuracy': 0.6398, 'validation/loss': 1.66695, 'validation/num_examples': 50000, 'test/accuracy': 0.4983, 'test/loss': 2.4077443359375, 'test/num_examples': 10000, 'score': 6630.486701965332, 'total_duration': 8037.043489456177, 'accumulated_submission_time': 6630.486701965332, 'accumulated_eval_time': 1395.9393773078918, 'accumulated_logging_time': 0.26624131202697754}
I0608 20:21:56.759923 139953908471552 logging_writer.py:48] [17690] accumulated_eval_time=1395.939377, accumulated_logging_time=0.266241, accumulated_submission_time=6630.486702, global_step=17690, preemption_count=0, score=6630.486702, test/accuracy=0.498300, test/loss=2.407744, test/num_examples=10000, total_duration=8037.043489, train/accuracy=0.714047, train/loss=1.328187, validation/accuracy=0.639800, validation/loss=1.666950, validation/num_examples=50000
I0608 20:23:52.677814 139953900078848 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.467170, loss=3.252924
I0608 20:23:52.682693 140006680004416 submission.py:139] 18000) loss = 3.253, grad_norm = 0.467
I0608 20:26:59.510540 139953908471552 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.476018, loss=3.313989
I0608 20:26:59.515236 140006680004416 submission.py:139] 18500) loss = 3.314, grad_norm = 0.476
I0608 20:30:07.703059 139953900078848 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.467403, loss=3.225410
I0608 20:30:07.707791 140006680004416 submission.py:139] 19000) loss = 3.225, grad_norm = 0.467
I0608 20:30:27.116440 140006680004416 spec.py:298] Evaluating on the training split.
I0608 20:31:11.650885 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 20:31:59.652735 140006680004416 spec.py:326] Evaluating on the test split.
I0608 20:32:01.044773 140006680004416 submission_runner.py:419] Time since start: 8641.34s, 	Step: 19053, 	{'train/accuracy': 0.7262236926020408, 'train/loss': 1.3265684478136959, 'validation/accuracy': 0.64838, 'validation/loss': 1.675075625, 'validation/num_examples': 50000, 'test/accuracy': 0.5118, 'test/loss': 2.3607435546875, 'test/num_examples': 10000, 'score': 7140.053015947342, 'total_duration': 8641.338813304901, 'accumulated_submission_time': 7140.053015947342, 'accumulated_eval_time': 1489.8676409721375, 'accumulated_logging_time': 0.2849113941192627}
I0608 20:32:01.055862 139953908471552 logging_writer.py:48] [19053] accumulated_eval_time=1489.867641, accumulated_logging_time=0.284911, accumulated_submission_time=7140.053016, global_step=19053, preemption_count=0, score=7140.053016, test/accuracy=0.511800, test/loss=2.360744, test/num_examples=10000, total_duration=8641.338813, train/accuracy=0.726224, train/loss=1.326568, validation/accuracy=0.648380, validation/loss=1.675076, validation/num_examples=50000
I0608 20:34:48.248195 139953900078848 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.462353, loss=3.253340
I0608 20:34:48.253292 140006680004416 submission.py:139] 19500) loss = 3.253, grad_norm = 0.462
I0608 20:37:55.049169 139953908471552 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.457290, loss=3.206322
I0608 20:37:55.055680 140006680004416 submission.py:139] 20000) loss = 3.206, grad_norm = 0.457
I0608 20:40:31.369666 140006680004416 spec.py:298] Evaluating on the training split.
I0608 20:41:14.905727 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 20:42:08.666607 140006680004416 spec.py:326] Evaluating on the test split.
I0608 20:42:10.065271 140006680004416 submission_runner.py:419] Time since start: 9250.36s, 	Step: 20415, 	{'train/accuracy': 0.7304488201530612, 'train/loss': 1.345739714953364, 'validation/accuracy': 0.64896, 'validation/loss': 1.70996953125, 'validation/num_examples': 50000, 'test/accuracy': 0.5012, 'test/loss': 2.4635826171875, 'test/num_examples': 10000, 'score': 7649.579669713974, 'total_duration': 9250.3593044281, 'accumulated_submission_time': 7649.579669713974, 'accumulated_eval_time': 1588.5634014606476, 'accumulated_logging_time': 0.30405187606811523}
I0608 20:42:10.075826 139953900078848 logging_writer.py:48] [20415] accumulated_eval_time=1588.563401, accumulated_logging_time=0.304052, accumulated_submission_time=7649.579670, global_step=20415, preemption_count=0, score=7649.579670, test/accuracy=0.501200, test/loss=2.463583, test/num_examples=10000, total_duration=9250.359304, train/accuracy=0.730449, train/loss=1.345740, validation/accuracy=0.648960, validation/loss=1.709970, validation/num_examples=50000
I0608 20:42:42.060487 139953908471552 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.489210, loss=3.258376
I0608 20:42:42.064521 140006680004416 submission.py:139] 20500) loss = 3.258, grad_norm = 0.489
I0608 20:45:48.828791 139953900078848 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.469814, loss=3.175527
I0608 20:45:48.833876 140006680004416 submission.py:139] 21000) loss = 3.176, grad_norm = 0.470
I0608 20:48:57.187688 139953908471552 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.457081, loss=3.258577
I0608 20:48:57.193060 140006680004416 submission.py:139] 21500) loss = 3.259, grad_norm = 0.457
I0608 20:50:40.224221 140006680004416 spec.py:298] Evaluating on the training split.
I0608 20:51:24.066091 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 20:52:11.808863 140006680004416 spec.py:326] Evaluating on the test split.
I0608 20:52:13.204711 140006680004416 submission_runner.py:419] Time since start: 9853.50s, 	Step: 21777, 	{'train/accuracy': 0.7461535395408163, 'train/loss': 1.2455416309590241, 'validation/accuracy': 0.66054, 'validation/loss': 1.6208809375, 'validation/num_examples': 50000, 'test/accuracy': 0.5188, 'test/loss': 2.32755859375, 'test/num_examples': 10000, 'score': 8158.933223724365, 'total_duration': 9853.498740434647, 'accumulated_submission_time': 8158.933223724365, 'accumulated_eval_time': 1681.5438332557678, 'accumulated_logging_time': 0.3237428665161133}
I0608 20:52:13.217830 139953900078848 logging_writer.py:48] [21777] accumulated_eval_time=1681.543833, accumulated_logging_time=0.323743, accumulated_submission_time=8158.933224, global_step=21777, preemption_count=0, score=8158.933224, test/accuracy=0.518800, test/loss=2.327559, test/num_examples=10000, total_duration=9853.498740, train/accuracy=0.746154, train/loss=1.245542, validation/accuracy=0.660540, validation/loss=1.620881, validation/num_examples=50000
I0608 20:53:36.708980 139953908471552 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.491045, loss=3.240866
I0608 20:53:36.713348 140006680004416 submission.py:139] 22000) loss = 3.241, grad_norm = 0.491
I0608 20:56:43.541032 139953900078848 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.458434, loss=3.202663
I0608 20:56:43.547893 140006680004416 submission.py:139] 22500) loss = 3.203, grad_norm = 0.458
I0608 20:59:51.909659 139953908471552 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.453961, loss=3.211147
I0608 20:59:51.914309 140006680004416 submission.py:139] 23000) loss = 3.211, grad_norm = 0.454
I0608 21:00:43.447459 140006680004416 spec.py:298] Evaluating on the training split.
I0608 21:01:28.925656 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 21:02:20.288053 140006680004416 spec.py:326] Evaluating on the test split.
I0608 21:02:21.680923 140006680004416 submission_runner.py:419] Time since start: 10461.97s, 	Step: 23139, 	{'train/accuracy': 0.7504982461734694, 'train/loss': 1.2360457595513792, 'validation/accuracy': 0.6613, 'validation/loss': 1.62035734375, 'validation/num_examples': 50000, 'test/accuracy': 0.5239, 'test/loss': 2.3303466796875, 'test/num_examples': 10000, 'score': 8668.366391897202, 'total_duration': 10461.974987745285, 'accumulated_submission_time': 8668.366391897202, 'accumulated_eval_time': 1779.7772898674011, 'accumulated_logging_time': 0.34662604331970215}
I0608 21:02:21.693029 139953900078848 logging_writer.py:48] [23139] accumulated_eval_time=1779.777290, accumulated_logging_time=0.346626, accumulated_submission_time=8668.366392, global_step=23139, preemption_count=0, score=8668.366392, test/accuracy=0.523900, test/loss=2.330347, test/num_examples=10000, total_duration=10461.974988, train/accuracy=0.750498, train/loss=1.236046, validation/accuracy=0.661300, validation/loss=1.620357, validation/num_examples=50000
I0608 21:04:36.749834 139953908471552 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.446814, loss=3.146429
I0608 21:04:36.754111 140006680004416 submission.py:139] 23500) loss = 3.146, grad_norm = 0.447
I0608 21:07:45.090350 139953900078848 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.465678, loss=3.134813
I0608 21:07:45.096750 140006680004416 submission.py:139] 24000) loss = 3.135, grad_norm = 0.466
I0608 21:10:51.705440 139953908471552 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.474313, loss=3.155499
I0608 21:10:51.709971 140006680004416 submission.py:139] 24500) loss = 3.155, grad_norm = 0.474
I0608 21:10:51.711683 140006680004416 spec.py:298] Evaluating on the training split.
I0608 21:11:36.417160 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 21:12:23.987414 140006680004416 spec.py:326] Evaluating on the test split.
I0608 21:12:25.378607 140006680004416 submission_runner.py:419] Time since start: 11065.67s, 	Step: 24501, 	{'train/accuracy': 0.7627750318877551, 'train/loss': 1.169702335279815, 'validation/accuracy': 0.67264, 'validation/loss': 1.56484890625, 'validation/num_examples': 50000, 'test/accuracy': 0.5319, 'test/loss': 2.275808984375, 'test/num_examples': 10000, 'score': 9177.60017490387, 'total_duration': 11065.672602891922, 'accumulated_submission_time': 9177.60017490387, 'accumulated_eval_time': 1873.4441797733307, 'accumulated_logging_time': 0.36769890785217285}
I0608 21:12:25.395149 139953900078848 logging_writer.py:48] [24501] accumulated_eval_time=1873.444180, accumulated_logging_time=0.367699, accumulated_submission_time=9177.600175, global_step=24501, preemption_count=0, score=9177.600175, test/accuracy=0.531900, test/loss=2.275809, test/num_examples=10000, total_duration=11065.672603, train/accuracy=0.762775, train/loss=1.169702, validation/accuracy=0.672640, validation/loss=1.564849, validation/num_examples=50000
I0608 21:15:32.071046 139953908471552 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.439680, loss=3.094391
I0608 21:15:32.076314 140006680004416 submission.py:139] 25000) loss = 3.094, grad_norm = 0.440
I0608 21:18:40.226024 139953900078848 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.461761, loss=3.104552
I0608 21:18:40.231176 140006680004416 submission.py:139] 25500) loss = 3.105, grad_norm = 0.462
I0608 21:20:55.379869 140006680004416 spec.py:298] Evaluating on the training split.
I0608 21:21:40.616577 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 21:22:32.696683 140006680004416 spec.py:326] Evaluating on the test split.
I0608 21:22:34.091857 140006680004416 submission_runner.py:419] Time since start: 11674.39s, 	Step: 25863, 	{'train/accuracy': 0.7677176339285714, 'train/loss': 1.155410844452527, 'validation/accuracy': 0.67408, 'validation/loss': 1.57253140625, 'validation/num_examples': 50000, 'test/accuracy': 0.5304, 'test/loss': 2.2947453125, 'test/num_examples': 10000, 'score': 9686.795627593994, 'total_duration': 11674.385939836502, 'accumulated_submission_time': 9686.795627593994, 'accumulated_eval_time': 1972.1561965942383, 'accumulated_logging_time': 0.39317989349365234}
I0608 21:22:34.103523 139953908471552 logging_writer.py:48] [25863] accumulated_eval_time=1972.156197, accumulated_logging_time=0.393180, accumulated_submission_time=9686.795628, global_step=25863, preemption_count=0, score=9686.795628, test/accuracy=0.530400, test/loss=2.294745, test/num_examples=10000, total_duration=11674.385940, train/accuracy=0.767718, train/loss=1.155411, validation/accuracy=0.674080, validation/loss=1.572531, validation/num_examples=50000
I0608 21:23:25.503553 139953900078848 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.438876, loss=3.076842
I0608 21:23:25.508888 140006680004416 submission.py:139] 26000) loss = 3.077, grad_norm = 0.439
I0608 21:26:33.941285 139953908471552 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.472724, loss=3.146676
I0608 21:26:33.946894 140006680004416 submission.py:139] 26500) loss = 3.147, grad_norm = 0.473
I0608 21:29:40.572156 139953900078848 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.449363, loss=3.099903
I0608 21:29:40.578482 140006680004416 submission.py:139] 27000) loss = 3.100, grad_norm = 0.449
I0608 21:31:04.314789 140006680004416 spec.py:298] Evaluating on the training split.
I0608 21:31:48.562271 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 21:32:36.374857 140006680004416 spec.py:326] Evaluating on the test split.
I0608 21:32:37.765965 140006680004416 submission_runner.py:419] Time since start: 12278.06s, 	Step: 27225, 	{'train/accuracy': 0.7738759566326531, 'train/loss': 1.1251672238719708, 'validation/accuracy': 0.67552, 'validation/loss': 1.546568125, 'validation/num_examples': 50000, 'test/accuracy': 0.5401, 'test/loss': 2.250493359375, 'test/num_examples': 10000, 'score': 10196.206841230392, 'total_duration': 12278.060042142868, 'accumulated_submission_time': 10196.206841230392, 'accumulated_eval_time': 2065.607439517975, 'accumulated_logging_time': 0.41265368461608887}
I0608 21:32:37.777692 139953908471552 logging_writer.py:48] [27225] accumulated_eval_time=2065.607440, accumulated_logging_time=0.412654, accumulated_submission_time=10196.206841, global_step=27225, preemption_count=0, score=10196.206841, test/accuracy=0.540100, test/loss=2.250493, test/num_examples=10000, total_duration=12278.060042, train/accuracy=0.773876, train/loss=1.125167, validation/accuracy=0.675520, validation/loss=1.546568, validation/num_examples=50000
I0608 21:34:20.740694 139953900078848 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.451838, loss=3.056150
I0608 21:34:20.750336 140006680004416 submission.py:139] 27500) loss = 3.056, grad_norm = 0.452
I0608 21:37:28.478703 140006680004416 spec.py:298] Evaluating on the training split.
I0608 21:38:12.851336 140006680004416 spec.py:310] Evaluating on the validation split.
I0608 21:38:58.237281 140006680004416 spec.py:326] Evaluating on the test split.
I0608 21:38:59.628412 140006680004416 submission_runner.py:419] Time since start: 12659.92s, 	Step: 28000, 	{'train/accuracy': 0.7814891581632653, 'train/loss': 1.074487491529815, 'validation/accuracy': 0.67908, 'validation/loss': 1.51659015625, 'validation/num_examples': 50000, 'test/accuracy': 0.5361, 'test/loss': 2.2333107421875, 'test/num_examples': 10000, 'score': 10486.450303554535, 'total_duration': 12659.922473192215, 'accumulated_submission_time': 10486.450303554535, 'accumulated_eval_time': 2156.7571170330048, 'accumulated_logging_time': 0.43301868438720703}
I0608 21:38:59.639466 139953908471552 logging_writer.py:48] [28000] accumulated_eval_time=2156.757117, accumulated_logging_time=0.433019, accumulated_submission_time=10486.450304, global_step=28000, preemption_count=0, score=10486.450304, test/accuracy=0.536100, test/loss=2.233311, test/num_examples=10000, total_duration=12659.922473, train/accuracy=0.781489, train/loss=1.074487, validation/accuracy=0.679080, validation/loss=1.516590, validation/num_examples=50000
I0608 21:38:59.657518 139953900078848 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10486.450304
I0608 21:39:00.216306 140006680004416 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_28000.
I0608 21:39:00.484820 140006680004416 submission_runner.py:581] Tuning trial 1/1
I0608 21:39:00.485056 140006680004416 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0608 21:39:00.485922 140006680004416 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000896843112244898, 'train/loss': 6.924565529336735, 'validation/accuracy': 0.0012, 'validation/loss': 6.924400625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.92709296875, 'test/num_examples': 10000, 'score': 8.294957637786865, 'total_duration': 137.13165378570557, 'accumulated_submission_time': 8.294957637786865, 'accumulated_eval_time': 128.83624172210693, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1356, {'train/accuracy': 0.0653499681122449, 'train/loss': 5.55831224091199, 'validation/accuracy': 0.0611, 'validation/loss': 5.621090625, 'validation/num_examples': 50000, 'test/accuracy': 0.0396, 'test/loss': 5.849264453125, 'test/num_examples': 10000, 'score': 517.6732680797577, 'total_duration': 745.4340023994446, 'accumulated_submission_time': 517.6732680797577, 'accumulated_eval_time': 226.79152274131775, 'accumulated_logging_time': 0.028676748275756836, 'global_step': 1356, 'preemption_count': 0}), (2716, {'train/accuracy': 0.1757015306122449, 'train/loss': 4.338580073142539, 'validation/accuracy': 0.15778, 'validation/loss': 4.448470625, 'validation/num_examples': 50000, 'test/accuracy': 0.1089, 'test/loss': 4.9316546875, 'test/num_examples': 10000, 'score': 1027.1829342842102, 'total_duration': 1361.238109111786, 'accumulated_submission_time': 1027.1829342842102, 'accumulated_eval_time': 332.26693630218506, 'accumulated_logging_time': 0.04788327217102051, 'global_step': 2716, 'preemption_count': 0}), (4077, {'train/accuracy': 0.30604272959183676, 'train/loss': 3.4379403250558034, 'validation/accuracy': 0.28186, 'validation/loss': 3.5829640625, 'validation/num_examples': 50000, 'test/accuracy': 0.2082, 'test/loss': 4.1565984375, 'test/num_examples': 10000, 'score': 1536.542601108551, 'total_duration': 1958.6580181121826, 'accumulated_submission_time': 1536.542601108551, 'accumulated_eval_time': 419.5202021598816, 'accumulated_logging_time': 0.06949114799499512, 'global_step': 4077, 'preemption_count': 0}), (5438, {'train/accuracy': 0.4176498724489796, 'train/loss': 2.891338114835778, 'validation/accuracy': 0.38306, 'validation/loss': 3.06608625, 'validation/num_examples': 50000, 'test/accuracy': 0.2758, 'test/loss': 3.716065234375, 'test/num_examples': 10000, 'score': 2045.7616124153137, 'total_duration': 2570.7574701309204, 'accumulated_submission_time': 2045.7616124153137, 'accumulated_eval_time': 521.6036593914032, 'accumulated_logging_time': 0.08748221397399902, 'global_step': 5438, 'preemption_count': 0}), (6799, {'train/accuracy': 0.48026945153061223, 'train/loss': 2.4624608876753826, 'validation/accuracy': 0.43782, 'validation/loss': 2.6665840625, 'validation/num_examples': 50000, 'test/accuracy': 0.3121, 'test/loss': 3.411365625, 'test/num_examples': 10000, 'score': 2554.9893622398376, 'total_duration': 3169.8122313022614, 'accumulated_submission_time': 2554.9893622398376, 'accumulated_eval_time': 610.6279594898224, 'accumulated_logging_time': 0.10588264465332031, 'global_step': 6799, 'preemption_count': 0}), (8162, {'train/accuracy': 0.5126953125, 'train/loss': 2.349236624581473, 'validation/accuracy': 0.4716, 'validation/loss': 2.54776921875, 'validation/num_examples': 50000, 'test/accuracy': 0.3584, 'test/loss': 3.2217287109375, 'test/num_examples': 10000, 'score': 3064.524979352951, 'total_duration': 3781.3736600875854, 'accumulated_submission_time': 3064.524979352951, 'accumulated_eval_time': 711.8522181510925, 'accumulated_logging_time': 0.1246027946472168, 'global_step': 8162, 'preemption_count': 0}), (9523, {'train/accuracy': 0.5837252869897959, 'train/loss': 1.96003380600287, 'validation/accuracy': 0.5329, 'validation/loss': 2.1948328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4076, 'test/loss': 2.88578125, 'test/num_examples': 10000, 'score': 3573.8027267456055, 'total_duration': 4384.692658185959, 'accumulated_submission_time': 3573.8027267456055, 'accumulated_eval_time': 805.0845923423767, 'accumulated_logging_time': 0.1455526351928711, 'global_step': 9523, 'preemption_count': 0}), (10885, {'train/accuracy': 0.6113480548469388, 'train/loss': 1.888406870316486, 'validation/accuracy': 0.5597, 'validation/loss': 2.128869375, 'validation/num_examples': 50000, 'test/accuracy': 0.4408, 'test/loss': 2.7625251953125, 'test/num_examples': 10000, 'score': 4083.3121089935303, 'total_duration': 4994.286746740341, 'accumulated_submission_time': 4083.3121089935303, 'accumulated_eval_time': 904.3648903369904, 'accumulated_logging_time': 0.1652359962463379, 'global_step': 10885, 'preemption_count': 0}), (12247, {'train/accuracy': 0.6455476721938775, 'train/loss': 1.7252426147460938, 'validation/accuracy': 0.58672, 'validation/loss': 1.9949953125, 'validation/num_examples': 50000, 'test/accuracy': 0.455, 'test/loss': 2.686128125, 'test/num_examples': 10000, 'score': 4592.796915769577, 'total_duration': 5594.798699617386, 'accumulated_submission_time': 4592.796915769577, 'accumulated_eval_time': 994.5898234844208, 'accumulated_logging_time': 0.18460631370544434, 'global_step': 12247, 'preemption_count': 0}), (13609, {'train/accuracy': 0.6655373086734694, 'train/loss': 1.554753829021843, 'validation/accuracy': 0.60386, 'validation/loss': 1.843734375, 'validation/num_examples': 50000, 'test/accuracy': 0.4735, 'test/loss': 2.5420916015625, 'test/num_examples': 10000, 'score': 5102.22488284111, 'total_duration': 6207.687874555588, 'accumulated_submission_time': 5102.22488284111, 'accumulated_eval_time': 1097.240421295166, 'accumulated_logging_time': 0.2052922248840332, 'global_step': 13609, 'preemption_count': 0}), (14971, {'train/accuracy': 0.6829161352040817, 'train/loss': 1.501330161581234, 'validation/accuracy': 0.61794, 'validation/loss': 1.80166, 'validation/num_examples': 50000, 'test/accuracy': 0.4725, 'test/loss': 2.5571177734375, 'test/num_examples': 10000, 'score': 5611.70737361908, 'total_duration': 6809.726997852325, 'accumulated_submission_time': 5611.70737361908, 'accumulated_eval_time': 1189.0025811195374, 'accumulated_logging_time': 0.22553801536560059, 'global_step': 14971, 'preemption_count': 0}), (16328, {'train/accuracy': 0.6994379783163265, 'train/loss': 1.481515378368144, 'validation/accuracy': 0.62856, 'validation/loss': 1.8021921875, 'validation/num_examples': 50000, 'test/accuracy': 0.4922, 'test/loss': 2.5002375, 'test/num_examples': 10000, 'score': 6121.0934019088745, 'total_duration': 7422.506525278091, 'accumulated_submission_time': 6121.0934019088745, 'accumulated_eval_time': 1291.6083552837372, 'accumulated_logging_time': 0.2459256649017334, 'global_step': 16328, 'preemption_count': 0}), (17690, {'train/accuracy': 0.7140465561224489, 'train/loss': 1.328186813665896, 'validation/accuracy': 0.6398, 'validation/loss': 1.66695, 'validation/num_examples': 50000, 'test/accuracy': 0.4983, 'test/loss': 2.4077443359375, 'test/num_examples': 10000, 'score': 6630.486701965332, 'total_duration': 8037.043489456177, 'accumulated_submission_time': 6630.486701965332, 'accumulated_eval_time': 1395.9393773078918, 'accumulated_logging_time': 0.26624131202697754, 'global_step': 17690, 'preemption_count': 0}), (19053, {'train/accuracy': 0.7262236926020408, 'train/loss': 1.3265684478136959, 'validation/accuracy': 0.64838, 'validation/loss': 1.675075625, 'validation/num_examples': 50000, 'test/accuracy': 0.5118, 'test/loss': 2.3607435546875, 'test/num_examples': 10000, 'score': 7140.053015947342, 'total_duration': 8641.338813304901, 'accumulated_submission_time': 7140.053015947342, 'accumulated_eval_time': 1489.8676409721375, 'accumulated_logging_time': 0.2849113941192627, 'global_step': 19053, 'preemption_count': 0}), (20415, {'train/accuracy': 0.7304488201530612, 'train/loss': 1.345739714953364, 'validation/accuracy': 0.64896, 'validation/loss': 1.70996953125, 'validation/num_examples': 50000, 'test/accuracy': 0.5012, 'test/loss': 2.4635826171875, 'test/num_examples': 10000, 'score': 7649.579669713974, 'total_duration': 9250.3593044281, 'accumulated_submission_time': 7649.579669713974, 'accumulated_eval_time': 1588.5634014606476, 'accumulated_logging_time': 0.30405187606811523, 'global_step': 20415, 'preemption_count': 0}), (21777, {'train/accuracy': 0.7461535395408163, 'train/loss': 1.2455416309590241, 'validation/accuracy': 0.66054, 'validation/loss': 1.6208809375, 'validation/num_examples': 50000, 'test/accuracy': 0.5188, 'test/loss': 2.32755859375, 'test/num_examples': 10000, 'score': 8158.933223724365, 'total_duration': 9853.498740434647, 'accumulated_submission_time': 8158.933223724365, 'accumulated_eval_time': 1681.5438332557678, 'accumulated_logging_time': 0.3237428665161133, 'global_step': 21777, 'preemption_count': 0}), (23139, {'train/accuracy': 0.7504982461734694, 'train/loss': 1.2360457595513792, 'validation/accuracy': 0.6613, 'validation/loss': 1.62035734375, 'validation/num_examples': 50000, 'test/accuracy': 0.5239, 'test/loss': 2.3303466796875, 'test/num_examples': 10000, 'score': 8668.366391897202, 'total_duration': 10461.974987745285, 'accumulated_submission_time': 8668.366391897202, 'accumulated_eval_time': 1779.7772898674011, 'accumulated_logging_time': 0.34662604331970215, 'global_step': 23139, 'preemption_count': 0}), (24501, {'train/accuracy': 0.7627750318877551, 'train/loss': 1.169702335279815, 'validation/accuracy': 0.67264, 'validation/loss': 1.56484890625, 'validation/num_examples': 50000, 'test/accuracy': 0.5319, 'test/loss': 2.275808984375, 'test/num_examples': 10000, 'score': 9177.60017490387, 'total_duration': 11065.672602891922, 'accumulated_submission_time': 9177.60017490387, 'accumulated_eval_time': 1873.4441797733307, 'accumulated_logging_time': 0.36769890785217285, 'global_step': 24501, 'preemption_count': 0}), (25863, {'train/accuracy': 0.7677176339285714, 'train/loss': 1.155410844452527, 'validation/accuracy': 0.67408, 'validation/loss': 1.57253140625, 'validation/num_examples': 50000, 'test/accuracy': 0.5304, 'test/loss': 2.2947453125, 'test/num_examples': 10000, 'score': 9686.795627593994, 'total_duration': 11674.385939836502, 'accumulated_submission_time': 9686.795627593994, 'accumulated_eval_time': 1972.1561965942383, 'accumulated_logging_time': 0.39317989349365234, 'global_step': 25863, 'preemption_count': 0}), (27225, {'train/accuracy': 0.7738759566326531, 'train/loss': 1.1251672238719708, 'validation/accuracy': 0.67552, 'validation/loss': 1.546568125, 'validation/num_examples': 50000, 'test/accuracy': 0.5401, 'test/loss': 2.250493359375, 'test/num_examples': 10000, 'score': 10196.206841230392, 'total_duration': 12278.060042142868, 'accumulated_submission_time': 10196.206841230392, 'accumulated_eval_time': 2065.607439517975, 'accumulated_logging_time': 0.41265368461608887, 'global_step': 27225, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7814891581632653, 'train/loss': 1.074487491529815, 'validation/accuracy': 0.67908, 'validation/loss': 1.51659015625, 'validation/num_examples': 50000, 'test/accuracy': 0.5361, 'test/loss': 2.2333107421875, 'test/num_examples': 10000, 'score': 10486.450303554535, 'total_duration': 12659.922473192215, 'accumulated_submission_time': 10486.450303554535, 'accumulated_eval_time': 2156.7571170330048, 'accumulated_logging_time': 0.43301868438720703, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0608 21:39:00.486052 140006680004416 submission_runner.py:584] Timing: 10486.450303554535
I0608 21:39:00.486104 140006680004416 submission_runner.py:586] Total number of evals: 22
I0608 21:39:00.486149 140006680004416 submission_runner.py:587] ====================
I0608 21:39:00.486278 140006680004416 submission_runner.py:655] Final imagenet_resnet score: 10486.450303554535
