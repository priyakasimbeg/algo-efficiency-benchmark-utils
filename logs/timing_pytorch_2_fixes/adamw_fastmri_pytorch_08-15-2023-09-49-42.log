torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_after_pytorch_fixes/adamw --overwrite=True --save_checkpoints=False --max_global_steps=2714 --torch_compile=true 2>&1 | tee -a /logs/fastmri_pytorch_08-15-2023-09-49-42.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-15 09:49:51.866103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 09:49:51.866103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 09:49:51.866103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 09:49:51.866104: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 09:49:51.866099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 09:49:51.866112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 09:49:51.866107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 09:49:51.866113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0815 09:50:06.165634 140082314057536 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0815 09:50:06.165688 140581976233792 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0815 09:50:06.165673 139997794117440 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0815 09:50:06.166672 140218347767616 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0815 09:50:06.166786 139909710714688 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0815 09:50:06.166815 140511383603008 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0815 09:50:06.167233 140326913230656 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0815 09:50:06.177146 140262284560192 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0815 09:50:06.177228 140218347767616 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 09:50:06.177253 139909710714688 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 09:50:06.177298 140511383603008 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 09:50:06.177448 140262284560192 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 09:50:06.177726 140326913230656 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 09:50:06.186470 140082314057536 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 09:50:06.186503 139997794117440 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 09:50:06.186538 140581976233792 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 09:50:06.514740 140262284560192 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/fastmri_pytorch.
W0815 09:50:06.554721 140218347767616 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 09:50:06.554720 140511383603008 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 09:50:06.554721 139997794117440 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 09:50:06.554718 140262284560192 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 09:50:06.555145 140326913230656 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 09:50:06.555705 140581976233792 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 09:50:06.556097 139909710714688 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 09:50:06.557439 140082314057536 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0815 09:50:06.560921 140262284560192 submission_runner.py:494] Using RNG seed 426778686
I0815 09:50:06.563218 140262284560192 submission_runner.py:503] --- Tuning run 1/1 ---
I0815 09:50:06.563346 140262284560192 submission_runner.py:508] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/fastmri_pytorch/trial_1.
I0815 09:50:06.563627 140262284560192 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/fastmri_pytorch/trial_1/hparams.json.
I0815 09:50:06.564447 140262284560192 submission_runner.py:177] Initializing dataset.
I0815 09:50:06.564559 140262284560192 submission_runner.py:184] Initializing model.
I0815 09:50:10.847074 140262284560192 submission_runner.py:215] Performing `torch.compile`.
I0815 09:50:11.123937 140262284560192 submission_runner.py:218] Initializing optimizer.
I0815 09:50:11.124775 140262284560192 submission_runner.py:225] Initializing metrics bundle.
I0815 09:50:11.124875 140262284560192 submission_runner.py:243] Initializing checkpoint and logger.
I0815 09:50:11.125526 140262284560192 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0815 09:50:11.125622 140262284560192 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0815 09:50:11.578367 140262284560192 submission_runner.py:264] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/fastmri_pytorch/trial_1/meta_data_0.json.
I0815 09:50:11.580767 140262284560192 submission_runner.py:267] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/fastmri_pytorch/trial_1/flags_0.json.
I0815 09:50:11.669446 140262284560192 submission_runner.py:277] Starting training loop.
[2023-08-15 09:50:11,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-15 09:50:11,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-15 09:50:11,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-15 09:50:11,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-15 09:50:11,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-15 09:50:11,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-15 09:50:11,989] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-15 09:50:11,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:11,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:12,034] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:50:12,034] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:50:12,035] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:50:12,035] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:50:12,035] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:50:12,035] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:50:12,036] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:50:12,036] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:50:12,037] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:50:12,037] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:50:12,037] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:50:12,037] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:50:12,037] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:50:12,037] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:50:12,037] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:50:12,037] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:50:12,037] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:50:12,037] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:50:12,037] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:50:12,038] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:50:12,038] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:50:12,038] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:50:12,038] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:50:12,038] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:50:12,038] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:50:12,039] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:50:12,040] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:50:12,040] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:50:16,812] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 09:50:16,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 09:50:16,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 09:50:16,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 09:50:16,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 09:50:16,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 09:50:16,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 09:50:57,634] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:57,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 09:50:57,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 09:50:57,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 09:50:57,680] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:50:57,680] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:50:57,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 09:50:57,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 09:50:57,681] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 09:50:57,681] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:50:57,682] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:50:57,682] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:50:57,682] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:50:57,706] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 09:50:57,708] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:50:58,063] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-15 09:50:58,070] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:50:58,138] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:50:58,141] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:50:58,141] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:50:58,142] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:01,204] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:01,243] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:01,253] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:01,255] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:01,255] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:01,256] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:01,263] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:01,266] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:01,268] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:01,268] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:01,291] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:01,294] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:01,294] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:01,295] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:01,300] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:01,313] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:01,314] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:01,315] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:01,316] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:01,316] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:01,316] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:01,316] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:01,316] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:01,317] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:01,317] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:01,318] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:01,318] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:01,319] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:01,319] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:01,319] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:01,319] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:01,352] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:01,355] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:01,355] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:01,356] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:01,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 09:51:01,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 09:51:01,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 09:51:01,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 09:51:01,849] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 09:51:01,911] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 09:51:01,937] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 09:51:03,378] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 09:51:03,379] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:03,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 09:51:03,424] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:03,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 09:51:03,430] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:03,435] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:03,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 09:51:03,468] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 09:51:03,468] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:03,469] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:03,482] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:03,486] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:03,486] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:03,486] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:03,490] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:03,492] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:03,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 09:51:03,518] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:03,536] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:03,538] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:03,539] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:03,539] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:03,539] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:03,541] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:03,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:03,541] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:03,542] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:03,542] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:03,580] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:03,586] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:03,586] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:03,589] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:03,589] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:03,589] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:03,589] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:03,589] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:03,589] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:03,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:03,628] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:03,629] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:03,629] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:03,656] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 09:51:03,657] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:03,738] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:03,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:03,794] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:03,794] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:03,795] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:03,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 09:51:03,995] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 09:51:04,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 09:51:04,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 09:51:04,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 09:51:04,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 09:51:04,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 09:51:04,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 09:51:04,863] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:04,920] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:04,958] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 09:51:04,958] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:04,966] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 09:51:04,966] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:04,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:04,976] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:04,977] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:04,977] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:05,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 09:51:05,018] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:05,018] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:05,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 09:51:05,022] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:05,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 09:51:05,025] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:05,027] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:05,066] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:05,068] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:05,068] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:05,069] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:05,076] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:05,078] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:05,079] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:05,079] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:05,103] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:05,103] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:05,106] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:05,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:05,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:05,152] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:05,152] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:05,152] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:05,154] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:05,154] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:05,155] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:05,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:05,171] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:05,171] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:05,172] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:05,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 09:51:05,318] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 09:51:05,319] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:05,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 09:51:05,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 09:51:05,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 09:51:05,374] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:05,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 09:51:05,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:05,423] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:05,424] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:05,424] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:05,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 09:51:05,433] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 09:51:05,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 09:51:06,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 09:51:06,081] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 09:51:06,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 09:51:06,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 09:51:06,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 09:51:06,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 09:51:06,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 09:51:06,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 09:51:06,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 09:51:06,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 09:51:06,450] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 09:51:06,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 09:51:06,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 09:51:06,492] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:06,501] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 09:51:06,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:06,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 09:51:06,605] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:06,607] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:06,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:06,608] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:06,618] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 09:51:06,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:06,635] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:06,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 09:51:06,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 09:51:06,663] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:06,664] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:06,686] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:06,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:06,700] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 09:51:06,716] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:06,727] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:06,731] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:06,732] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:06,734] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 09:51:06,735] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:06,735] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:06,735] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:06,738] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:06,740] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:06,740] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:06,741] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:06,772] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:06,775] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:06,777] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:06,777] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:06,777] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:06,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:06,781] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:06,782] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:06,782] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:06,820] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:06,822] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:06,822] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:06,823] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:06,864] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 09:51:06,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 09:51:06,988] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:07,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 09:51:07,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 09:51:07,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 09:51:07,043] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:07,052] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 09:51:07,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 09:51:07,093] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:07,095] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:07,095] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:07,096] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:07,369] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 09:51:07,554] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 09:51:07,555] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:07,917] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 09:51:08,055] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 09:51:08,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 09:51:08,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 09:51:08,159] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 09:51:08,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 09:51:08,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 09:51:08,308] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 09:51:08,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 09:51:08,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 09:51:08,379] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:08,384] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 09:51:08,408] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 09:51:08,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 09:51:08,438] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:08,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 09:51:08,459] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:08,461] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:08,461] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:08,462] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:08,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 09:51:08,522] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:08,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 09:51:08,546] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:08,583] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:08,604] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:08,605] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:08,607] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:08,607] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:08,607] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:08,615] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 09:51:08,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 09:51:08,632] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:08,638] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:08,638] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:08,640] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:08,640] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 09:51:08,640] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:08,641] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:08,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 09:51:08,659] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:08,702] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:08,703] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:08,726] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:08,727] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:08,728] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:08,728] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:08,728] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:08,729] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:08,729] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 09:51:08,730] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:08,730] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:08,730] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:08,750] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:08,751] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:08,751] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:08,752] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:08,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 09:51:08,856] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:08,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 09:51:08,890] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 09:51:08,921] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:08,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 09:51:08,927] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:08,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:08,948] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:08,948] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:08,949] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:08,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 09:51:09,004] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 09:51:09,012] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:09,034] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 09:51:09,082] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:09,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 09:51:09,084] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:09,085] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:09,086] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:09,086] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:09,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 09:51:09,093] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:09,158] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:09,161] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:09,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 09:51:09,192] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:09,205] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:09,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 09:51:09,207] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:09,207] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:09,208] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:09,208] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:09,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:09,231] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:09,231] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:09,232] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:09,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 09:51:09,256] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:09,262] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:09,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 09:51:09,283] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:09,313] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:09,315] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:09,316] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:09,316] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:09,326] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:09,349] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:09,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 09:51:09,352] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:09,352] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:09,353] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:09,376] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:09,378] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:09,379] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:09,379] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:09,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 09:51:09,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 09:51:09,472] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:09,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 09:51:09,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:09,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 09:51:09,591] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:09,594] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:09,594] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:09,594] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:09,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 09:51:09,642] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 09:51:09,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 09:51:09,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 09:51:09,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 09:51:09,842] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 09:51:09,851] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 09:51:09,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 09:51:09,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 09:51:09,867] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 09:51:09,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 09:51:09,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 09:51:10,005] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 09:51:10,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 09:51:10,060] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,083] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,084] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,085] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,085] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,098] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 09:51:10,111] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 09:51:10,114] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 09:51:10,128] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 09:51:10,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 09:51:10,146] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,175] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,198] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,200] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,200] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,201] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,207] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,231] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,231] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,232] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 09:51:10,241] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 09:51:10,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 09:51:10,256] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,257] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,270] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 09:51:10,320] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,323] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 09:51:10,334] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,334] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,343] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,345] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,345] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,346] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,356] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,358] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,358] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,358] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,359] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,359] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,360] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,360] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 09:51:10,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 09:51:10,451] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 09:51:10,506] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 09:51:10,523] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,529] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,530] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,531] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,531] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 09:51:10,588] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,594] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 09:51:10,596] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 09:51:10,631] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,633] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,633] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,633] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,656] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 09:51:10,656] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,657] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 09:51:10,658] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,735] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,736] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 09:51:10,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 09:51:10,779] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,779] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,781] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,782] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,782] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,785] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,785] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,786] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 09:51:10,794] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 09:51:10,815] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:10,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,858] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,878] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:10,888] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,890] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,891] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,891] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,903] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,905] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,905] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,905] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,922] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:10,924] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:10,924] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:10,925] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:10,962] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 09:51:10,962] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:11,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:11,069] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:11,073] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:11,074] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:11,074] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:11,076] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 09:51:11,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 09:51:11,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 09:51:11,339] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 09:51:11,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 09:51:11,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 09:51:11,354] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:11,366] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 09:51:11,387] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:11,408] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:11,429] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:11,431] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:11,431] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:11,431] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:11,447] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:11,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:11,451] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:11,451] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:11,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 09:51:11,516] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:11,523] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 09:51:11,524] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:11,529] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 09:51:11,582] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:11,583] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:11,604] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:11,606] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:11,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:11,606] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:11,606] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:11,608] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:11,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:11,608] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:11,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 09:51:11,624] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:11,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 09:51:11,646] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:11,662] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 09:51:11,663] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 09:51:11,664] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:11,686] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:11,698] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:11,708] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:11,710] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:11,710] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:11,711] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:11,716] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:11,720] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:11,721] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:11,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:11,722] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:11,737] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:11,739] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:11,739] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:11,739] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:11,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 09:51:11,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 09:51:11,838] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:11,844] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 09:51:11,864] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 09:51:11,865] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:11,892] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:11,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:11,914] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:11,915] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:11,915] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:11,945] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:11,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 09:51:11,949] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 09:51:11,968] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 09:51:11,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:11,990] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:11,990] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:11,990] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:12,015] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 09:51:12,030] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 09:51:12,031] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:12,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 09:51:12,049] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:12,110] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:12,129] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:12,143] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 09:51:12,143] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:12,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 09:51:12,145] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 09:51:12,146] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:12,154] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:12,156] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:12,156] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:12,156] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:12,163] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 09:51:12,163] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:12,171] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:12,173] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:12,173] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:12,174] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:12,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:12,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:12,243] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:12,270] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:12,271] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:12,272] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:12,272] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:12,273] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:12,273] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:12,273] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:12,273] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:12,284] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:12,286] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:12,287] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:12,287] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:12,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 09:51:12,340] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:12,419] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:12,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 09:51:12,462] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:12,464] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:12,464] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:12,465] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:12,592] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 09:51:12,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 09:51:12,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 09:51:12,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 09:51:12,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 09:51:12,718] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:12,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 09:51:12,769] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:12,793] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:12,795] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:12,795] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:12,795] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:12,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 09:51:12,895] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:12,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 09:51:12,910] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:12,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 09:51:12,948] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:12,959] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:12,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:12,975] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:12,975] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:12,976] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:12,984] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:12,986] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:12,986] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:12,987] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 09:51:13,008] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,019] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 09:51:13,020] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,023] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 09:51:13,023] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 09:51:13,059] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,080] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,082] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,083] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:13,085] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:13,085] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:13,086] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,104] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:13,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:13,105] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:13,105] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:13,106] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,106] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:13,107] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:13,107] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 09:51:13,206] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,216] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 09:51:13,219] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 09:51:13,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,283] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:13,285] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:13,285] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:13,286] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,287] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 09:51:13,288] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 09:51:13,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 09:51:13,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 09:51:13,429] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 09:51:13,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 09:51:13,472] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,473] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,499] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:13,501] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:13,502] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:13,502] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 09:51:13,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 09:51:13,601] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 09:51:13,610] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 09:51:13,622] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,651] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,701] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:13,703] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:13,703] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:13,704] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,705] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:13,707] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:13,707] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:13,708] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,774] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 09:51:13,776] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,826] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:13,826] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:13,826] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:13,828] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:13,828] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:13,828] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:13,828] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:13,828] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:13,828] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,828] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:13,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,829] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,884] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 09:51:13,885] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:13,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,943] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:13,962] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:13,964] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:13,964] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:13,965] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:13,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 09:51:14,003] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:14,005] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:14,006] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:14,006] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:14,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 09:51:14,177] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 09:51:14,299] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 09:51:14,300] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 09:51:14,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 09:51:14,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 09:51:14,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 09:51:14,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 09:51:14,652] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:14,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-15 09:51:14,830] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 09:51:14,830] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:14,842] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 09:51:14,843] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:14,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-15 09:51:14,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-15 09:51:14,957] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 09:51:14,957] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:14,959] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 09:51:14,960] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:14,963] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 09:51:14,963] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:15,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-15 09:51:15,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-15 09:51:15,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-15 09:51:15,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 09:51:15,110] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:15,201] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-15 09:51:15,978] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 09:51:15,979] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:16,037] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:16,102] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:16,104] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:16,104] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:16,105] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:16,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 09:51:17,494] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 09:51:17,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 09:51:18,046] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 09:51:18,070] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:18,122] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:18,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:18,186] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:18,186] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:18,187] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:18,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 09:51:20,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 09:51:20,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 09:51:20,805] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 09:51:20,820] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:20,875] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:20,896] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:20,898] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:20,898] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:20,898] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:21,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 09:51:21,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 09:51:21,335] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:21,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:21,443] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:21,445] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:21,445] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:21,445] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:21,677] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 09:51:21,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 09:51:22,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 09:51:22,219] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 09:51:22,234] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:22,287] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:22,307] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:22,309] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:22,309] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:22,309] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:22,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 09:51:22,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 09:51:22,727] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:22,789] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:22,831] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:22,833] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:22,834] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:22,834] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:23,263] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 09:51:23,560] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 09:51:23,561] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:23,614] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:23,638] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:23,640] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:23,640] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:23,640] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:23,864] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 09:51:24,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 09:51:24,059] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:24,138] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:24,179] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:24,181] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:24,181] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:24,181] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:24,610] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 09:51:24,881] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 09:51:24,882] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:24,931] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:24,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:24,954] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:24,954] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:24,954] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:25,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 09:51:25,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 09:51:25,424] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:25,554] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:51:25,604] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:51:25,606] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:51:25,606] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:51:25,607] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:51:26,066] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 09:51:27,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 09:51:27,155] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:51:27,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-15 09:51:27,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-15 09:51:27,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-15 09:51:27,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-15 09:51:27,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-15 09:51:27,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-15 09:51:27,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-15 09:51:27,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-15 09:51:27,903] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-15 09:51:27,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-15 09:51:27,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-15 09:51:27,918] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-15 09:51:27,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-15 09:51:27,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-15 09:51:27,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-15 09:51:28,635] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-15 09:51:28,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-15 09:51:28,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-15 09:51:28,645] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-15 09:51:28,645] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-15 09:51:28,649] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-15 09:51:28,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-15 09:51:28,670] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-15 09:51:28,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-15 09:51:28,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-15 09:51:28,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-15 09:51:28,689] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-15 09:51:28,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-15 09:51:28,724] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-15 09:51:28,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-15 09:51:29,037] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-15 09:51:29,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-15 09:51:29,212] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-15 09:51:29,217] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-15 09:51:29,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-15 09:51:29,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-15 09:51:29,233] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-15 09:51:29,325] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-15 09:51:29,477] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-15 09:51:29,585] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-15 09:51:29,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-15 09:51:29,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-15 09:51:29,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-15 09:51:29,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-15 09:51:29,593] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-15 09:51:29,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-15 09:51:29,697] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-15 09:51:29,697] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-15 09:51:29,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-15 09:51:29,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-15 09:51:29,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-15 09:51:29,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-15 09:51:29,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-15 09:51:29,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-15 09:51:29,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-15 09:51:29,803] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-15 09:51:29,842] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-15 09:51:30,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-15 09:51:30,036] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-15 09:51:30,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-15 09:51:30,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-15 09:51:30,346] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-15 09:51:30,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-15 09:51:30,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-15 09:51:30,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-15 09:51:30,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-15 09:51:30,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-15 09:51:30,549] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-15 09:51:30,560] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-15 09:51:30,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-15 09:51:30,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-15 09:51:30,599] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-15 09:51:30,599] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-15 09:51:30,927] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-15 09:51:30,961] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-15 09:51:31,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-15 09:51:31,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-15 09:51:31,057] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-15 09:51:31,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-15 09:51:31,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-15 09:51:31,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-15 09:51:31,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-15 09:51:31,096] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-15 09:51:31,097] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-15 09:51:31,115] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-15 09:51:31,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-15 09:51:31,142] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-15 09:51:31,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-15 09:51:31,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-15 09:51:31,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-15 09:51:31,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-15 09:51:31,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-15 09:51:31,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-15 09:51:31,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-15 09:51:31,542] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-15 09:51:31,547] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-15 09:51:31,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-15 09:51:31,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-15 09:51:31,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-15 09:51:31,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-15 09:51:31,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-15 09:51:31,731] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-15 09:51:31,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-15 09:51:31,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-15 09:51:31,807] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-15 09:51:31,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-15 09:51:31,830] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-15 09:51:31,833] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-15 09:51:31,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-15 09:51:31,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-15 09:51:31,844] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-15 09:51:31,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-15 09:51:31,908] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-15 09:51:31,940] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-15 09:51:31,971] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-15 09:51:31,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-15 09:51:31,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-15 09:51:31,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-15 09:51:31,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-15 09:51:32,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-15 09:51:32,022] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-15 09:51:32,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-15 09:51:32,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-15 09:51:32,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-15 09:51:32,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-15 09:51:32,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-15 09:51:32,099] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-15 09:51:32,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-15 09:51:32,111] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-15 09:51:32,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-15 09:51:32,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-15 09:51:32,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-15 09:51:32,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-15 09:51:32,385] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-15 09:51:32,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-15 09:51:32,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-15 09:51:32,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-15 09:51:32,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-15 09:51:32,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-15 09:51:32,826] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-15 09:51:32,833] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-15 09:51:32,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-15 09:51:32,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-15 09:51:32,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-15 09:51:32,961] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-15 09:51:33,069] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-15 09:51:33,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-15 09:51:33,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 09:51:33,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 09:51:33,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-15 09:51:33,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-15 09:51:33,121] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-15 09:51:33,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-15 09:51:33,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-15 09:51:33,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 09:51:33,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 09:51:33,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 09:51:33,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 09:51:33,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-15 09:51:33,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-15 09:51:33,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 09:51:33,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-15 09:51:33,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 09:51:33,229] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-15 09:51:33,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-15 09:51:33,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 09:51:33,267] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-15 09:51:33,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 09:51:33,277] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 09:51:33,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 09:51:33,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 09:51:33,298] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-15 09:51:33,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 09:51:33,314] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 09:51:33,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 09:51:33,319] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 09:51:33,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 09:51:33,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 09:51:33,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 09:51:33,389] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 09:51:33,391] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 09:51:33,394] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 09:51:33,414] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-15 09:51:33,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 09:51:33,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 09:51:33,453] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 09:51:33,460] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 09:51:33,466] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 09:51:33,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 09:51:33,493] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 09:51:33,499] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 09:51:33,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 09:51:33,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 09:51:33,543] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 09:51:33,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 09:51:33,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 09:51:33,586] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 09:51:33,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 09:51:33,645] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 09:51:33,663] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 09:51:33,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 09:51:33,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 09:51:33,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 09:51:33,721] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 09:51:33,724] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 09:51:33,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 09:51:33,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 09:51:33,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 09:51:33,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 09:51:33,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 09:51:33,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 09:51:33,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 09:51:33,868] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 09:51:33,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 09:51:33,910] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 09:51:33,912] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 09:51:33,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 09:51:33,954] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 09:51:33,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 09:51:33,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 09:51:33,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-15 09:51:34,015] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 09:51:34,030] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 09:51:34,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-15 09:51:34,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 09:51:34,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 09:51:34,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-15 09:51:34,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 09:51:34,178] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 09:51:34,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 09:51:34,215] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 09:51:34,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-15 09:51:34,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 09:51:34,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-15 09:51:34,360] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-15 09:51:34,431] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 09:51:35,631] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-15 09:51:35,688] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-15 09:51:35,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-15 09:51:35,823] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 09:51:35,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 09:51:35,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 09:51:36,030] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 09:51:36,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 09:51:36,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 09:51:36,275] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 09:51:36,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 09:51:36,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 09:51:36,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I0815 09:51:36.795108 140220246783744 logging_writer.py:48] [0] global_step=0, grad_norm=3.970553, loss=0.958962
I0815 09:51:36.807177 140262284560192 submission.py:120] 0) loss = 0.959, grad_norm = 3.971
I0815 09:51:36.808641 140262284560192 spec.py:320] Evaluating on the training split.
[2023-08-15 09:52:29,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:29,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:29,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:29,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:29,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:29,758] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:29,758] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:29,758] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:29,803] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:29,803] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:29,803] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:29,804] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:29,804] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:29,804] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:29,804] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:29,805] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:29,805] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:29,806] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:29,806] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:29,806] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:29,806] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:29,806] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:29,806] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:29,806] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:29,806] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:29,806] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:29,806] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:29,806] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:29,806] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:29,806] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:29,807] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:29,807] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:29,807] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:29,807] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:29,807] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:29,807] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:29,823] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:29,825] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:29,825] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:29,826] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:30,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 09:52:30,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 09:52:30,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 09:52:30,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 09:52:30,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 09:52:30,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 09:52:30,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 09:52:30,122] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 09:52:30,635] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 09:52:30,636] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:30,643] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 09:52:30,643] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:30,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 09:52:30,660] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:30,661] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 09:52:30,662] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:30,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 09:52:30,673] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:30,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 09:52:30,679] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:30,708] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 09:52:30,708] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:30,850] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:30,887] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:30,888] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:30,900] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:30,903] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:30,903] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:30,903] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:30,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:30,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:30,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:30,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:30,939] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:30,939] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:30,939] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:30,940] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:30,940] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:30,940] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:30,941] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:30,948] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:30,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:30,980] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:30,980] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:30,981] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:30,985] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:30,987] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:30,987] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:30,988] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:30,989] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:30,991] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:30,991] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:30,992] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:30,998] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:31,000] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:31,001] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:31,001] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:31,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 09:52:31,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 09:52:31,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 09:52:31,150] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:31,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 09:52:31,198] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 09:52:31,204] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 09:52:31,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 09:52:31,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 09:52:31,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:31,404] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:31,406] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:31,407] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:31,407] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:31,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 09:52:31,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 09:52:31,770] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:31,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 09:52:31,773] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:31,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 09:52:31,785] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:31,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 09:52:31,836] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:31,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 09:52:31,840] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:31,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 09:52:31,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:31,888] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 09:52:31,888] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:32,004] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:32,015] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:32,021] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:32,051] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:32,053] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:32,054] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:32,054] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:32,062] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:32,064] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:32,064] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:32,065] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:32,068] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:32,070] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:32,070] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:32,071] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:32,158] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:32,174] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:32,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:32,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:32,207] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:32,210] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:32,210] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:32,211] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:32,222] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:32,224] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:32,225] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:32,225] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:32,233] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:32,236] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:32,236] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:32,237] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:32,239] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:32,242] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:32,242] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:32,242] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:32,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 09:52:32,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 09:52:32,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 09:52:32,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 09:52:32,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 09:52:32,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 09:52:32,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 09:52:32,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 09:52:32,693] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:32,879] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:32,911] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 09:52:32,912] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:32,917] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 09:52:32,918] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:32,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:32,955] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:32,955] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:32,955] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:33,034] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 09:52:33,035] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:33,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 09:52:33,072] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:33,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 09:52:33,090] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:33,112] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 09:52:33,112] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:33,114] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:33,116] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:33,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 09:52:33,119] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:33,159] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:33,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:33,161] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:33,162] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:33,162] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:33,164] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:33,164] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:33,165] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:33,247] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:33,261] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 09:52:33,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 09:52:33,292] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:33,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 09:52:33,295] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:33,295] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:33,295] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:33,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:33,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:33,369] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:33,369] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:33,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 09:52:33,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:33,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:33,414] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:33,415] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:33,415] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:33,415] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:33,415] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:33,416] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:33,416] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:33,417] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:33,417] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:33,418] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:33,418] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:33,419] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:33,419] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:33,420] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:33,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 09:52:33,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 09:52:33,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 09:52:33,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 09:52:33,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 09:52:33,786] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 09:52:33,896] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 09:52:33,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 09:52:33,918] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 09:52:34,018] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 09:52:34,036] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:34,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 09:52:34,045] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 09:52:34,045] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 09:52:34,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 09:52:34,057] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 09:52:34,064] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:34,103] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 09:52:34,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 09:52:34,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 09:52:34,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 09:52:34,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 09:52:34,198] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:34,208] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:34,218] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:34,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 09:52:34,247] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:34,249] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:34,250] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:34,250] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:34,263] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:34,266] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:34,266] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:34,266] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:34,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 09:52:34,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 09:52:34,314] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 09:52:34,319] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:34,319] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:34,332] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:34,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 09:52:34,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 09:52:34,375] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:34,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 09:52:34,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:34,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:34,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:34,450] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:34,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:34,559] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:34,559] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:34,560] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:34,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:34,575] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 09:52:34,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:34,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:34,608] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:34,608] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:34,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:34,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:34,609] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:34,609] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:34,612] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:34,615] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:34,615] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:34,615] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:34,627] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:34,629] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:34,629] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:34,630] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:34,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 09:52:34,651] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:34,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 09:52:34,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 09:52:34,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 09:52:34,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 09:52:34,836] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:34,902] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:34,904] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:34,904] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:34,905] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:35,062] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 09:52:35,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 09:52:35,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 09:52:35,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 09:52:35,248] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 09:52:35,314] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 09:52:35,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 09:52:35,367] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 09:52:35,373] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:35,385] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:35,460] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 09:52:35,465] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 09:52:35,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 09:52:35,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 09:52:35,493] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 09:52:35,539] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:35,554] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:35,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:35,565] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:35,566] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:35,566] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:35,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:35,580] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:35,580] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:35,581] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:35,582] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 09:52:35,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 09:52:35,600] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:35,608] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 09:52:35,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 09:52:35,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 09:52:35,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 09:52:35,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 09:52:35,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 09:52:35,731] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 09:52:35,733] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:35,744] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 09:52:35,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 09:52:35,749] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:35,762] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:35,764] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:35,766] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:35,787] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:35,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 09:52:35,789] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:35,789] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:35,790] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:35,790] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:35,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 09:52:35,816] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:35,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 09:52:35,903] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:35,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 09:52:35,944] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:35,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:35,948] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:35,948] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:35,948] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:35,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:35,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:35,968] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:35,968] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:35,969] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:35,984] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:35,985] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:35,986] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:35,994] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:35,996] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:35,997] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:35,997] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,008] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 09:52:36,010] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,010] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,010] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,011] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,011] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,013] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,013] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,014] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,018] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,020] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,021] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,021] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 09:52:36,076] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 09:52:36,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 09:52:36,111] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 09:52:36,118] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 09:52:36,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 09:52:36,120] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:36,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 09:52:36,163] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,165] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,165] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,166] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 09:52:36,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 09:52:36,193] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,222] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 09:52:36,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 09:52:36,235] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 09:52:36,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 09:52:36,236] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,236] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,245] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 09:52:36,252] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 09:52:36,293] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-15 09:52:36,299] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:36,345] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,348] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,348] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,348] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,350] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:36,355] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-15 09:52:36,370] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:36,379] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:36,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-15 09:52:36,394] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 09:52:36,396] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,398] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,399] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,399] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,409] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,418] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,420] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,420] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,421] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,425] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,427] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,427] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,428] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,439] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:36,455] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-15 09:52:36,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 09:52:36,471] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,504] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,506] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,507] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,507] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,514] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 09:52:36,520] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-15 09:52:36,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 09:52:36,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 09:52:36,580] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 09:52:36,598] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:36,620] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,622] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,622] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,623] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,627] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-15 09:52:36,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 09:52:36,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 09:52:36,650] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,653] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:36,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 09:52:36,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 09:52:36,675] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,676] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,676] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,677] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-15 09:52:36,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-15 09:52:36,750] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-15 09:52:36,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-15 09:52:36,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-15 09:52:36,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-15 09:52:36,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-15 09:52:36,813] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-15 09:52:36,839] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,841] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:36,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-15 09:52:36,861] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,862] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:36,863] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-15 09:52:36,863] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:36,864] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:36,864] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:36,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-15 09:52:36,879] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,880] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,898] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-15 09:52:36,899] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:36,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-15 09:52:36,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,015] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,017] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,017] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,017] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,063] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,065] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,065] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,065] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,070] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-15 09:52:37,088] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,114] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,116] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,117] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,117] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,124] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,124] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,125] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,146] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,147] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,147] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,148] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,148] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,148] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,149] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,149] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,149] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,149] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,149] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,150] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-15 09:52:37,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-15 09:52:37,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,255] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-15 09:52:37,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-15 09:52:37,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-15 09:52:37,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,277] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,277] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,278] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,298] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-15 09:52:37,299] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-15 09:52:37,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-15 09:52:37,376] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-15 09:52:37,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-15 09:52:37,377] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,377] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-15 09:52:37,416] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,422] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,468] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,468] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,469] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,475] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-15 09:52:37,519] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,525] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,531] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-15 09:52:37,533] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,565] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,567] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,568] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,568] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,570] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,573] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,573] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,573] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,580] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,580] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,581] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,598] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,620] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,622] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,622] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,623] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,675] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-15 09:52:37,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-15 09:52:37,686] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,717] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-15 09:52:37,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,741] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,741] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,742] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-15 09:52:37,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-15 09:52:37,782] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-15 09:52:37,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-15 09:52:37,859] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-15 09:52:37,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-15 09:52:37,880] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,887] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:37,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:37,912] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 09:52:37,912] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:37,912] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:37,912] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:37,968] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-15 09:52:37,969] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,977] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-15 09:52:37,977] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-15 09:52:37,990] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:37,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-15 09:52:38,000] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-15 09:52:38,074] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,081] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,097] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,099] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,099] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,099] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 09:52:38,139] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-15 09:52:38,139] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,142] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,145] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,145] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,146] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,194] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,200] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-15 09:52:38,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,205] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,206] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,207] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,207] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,208] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,208] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,208] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,209] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,216] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,217] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,218] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,218] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 09:52:38,247] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,250] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,250] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,250] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,258] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-15 09:52:38,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-15 09:52:38,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-15 09:52:38,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-15 09:52:38,330] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,336] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-15 09:52:38,380] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,382] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,382] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,383] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-15 09:52:38,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-15 09:52:38,440] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,440] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,444] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,445] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-15 09:52:38,450] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-15 09:52:38,450] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,476] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,478] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,478] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,479] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,517] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,538] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-15 09:52:38,538] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,561] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,563] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,564] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,564] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-15 09:52:38,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 09:52:38,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-15 09:52:38,641] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,681] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,681] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,681] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,723] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,726] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,726] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,726] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,726] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,726] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,727] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,727] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,727] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,727] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,735] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,740] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 09:52:38,741] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-15 09:52:38,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,759] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,759] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-15 09:52:38,779] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,856] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-15 09:52:38,862] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:38,884] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,885] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,886] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,886] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-15 09:52:38,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-15 09:52:38,924] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-15 09:52:38,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:38,927] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:38,927] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:38,928] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:38,956] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-15 09:52:38,956] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:38,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-15 09:52:38,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-15 09:52:38,989] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,013] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,034] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:39,036] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:39,036] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:39,036] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:39,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 09:52:39,111] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-15 09:52:39,111] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,112] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-15 09:52:39,113] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,115] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-15 09:52:39,116] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-15 09:52:39,117] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-15 09:52:39,177] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,198] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 09:52:39,199] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:39,200] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:39,201] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:39,201] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:39,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-15 09:52:39,257] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-15 09:52:39,305] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,353] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:39,355] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:39,356] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:39,356] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:39,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-15 09:52:39,363] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,384] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:39,385] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:39,386] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:39,386] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:39,387] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:39,388] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:39,388] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:39,389] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:39,390] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:39,392] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:39,392] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:39,392] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:39,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-15 09:52:39,432] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-15 09:52:39,455] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,475] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-15 09:52:39,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-15 09:52:39,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-15 09:52:39,506] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:39,508] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:39,508] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:39,509] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:39,555] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-15 09:52:39,559] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,610] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:39,613] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:39,613] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:39,614] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:39,614] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-15 09:52:39,615] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-15 09:52:39,618] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-15 09:52:39,633] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,669] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,701] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:39,703] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:39,703] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:39,704] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:39,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-15 09:52:39,726] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,785] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:39,787] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:39,788] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:39,788] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:39,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-15 09:52:39,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-15 09:52:39,921] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-15 09:52:39,935] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-15 09:52:39,968] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:39,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:39,976] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:39,976] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:39,976] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:39,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,987] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:39,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-15 09:52:40,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:40,041] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:40,042] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:40,043] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:40,043] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:40,044] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:40,044] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:40,044] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:40,095] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:40,111] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-15 09:52:40,112] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:40,159] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:40,161] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:40,161] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:40,162] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:40,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-15 09:52:40,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-15 09:52:40,207] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:40,253] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-15 09:52:40,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-08-15 09:52:40,389] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-15 09:52:40,390] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:40,450] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-08-15 09:52:40,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-15 09:52:40,582] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:40,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-15 09:52:40,639] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:40,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-15 09:52:40,658] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:40,676] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-15 09:52:40,677] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-08-15 09:52:40,907] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:40,938] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:40,940] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:40,941] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:40,941] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-08-15 09:52:41,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-15 09:52:41,218] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-15 09:52:41,219] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:41,400] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:41,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:41,468] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:41,468] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:41,469] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:41,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-15 09:52:41,936] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-15 09:52:41,937] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:42,148] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:42,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:42,184] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:42,184] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:42,185] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:42,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-15 09:52:42,452] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-15 09:52:42,453] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 09:52:42,753] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 09:52:42,831] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 09:52:42,834] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 09:52:42,834] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 09:52:42,835] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 09:52:43,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-15 09:52:43,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-15 09:52:43,915] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0815 09:53:09.929529 140262284560192 spec.py:332] Evaluating on the validation split.
I0815 09:54:10.728513 140262284560192 spec.py:348] Evaluating on the test split.
I0815 09:55:09.682000 140262284560192 submission_runner.py:365] Time since start: 298.01s, 	Step: 1, 	{'train/ssim': 0.20268978391374862, 'train/loss': 0.9714501244681222, 'validation/ssim': 0.19312887025402714, 'validation/loss': 0.980557640818444, 'validation/num_examples': 3554, 'test/ssim': 0.21524675929057002, 'test/loss': 0.983138343536198, 'test/num_examples': 3581, 'score': 85.13911485671997, 'total_duration': 298.0130469799042, 'accumulated_submission_time': 85.13911485671997, 'accumulated_eval_time': 212.87340307235718, 'accumulated_logging_time': 0}
I0815 09:55:09.705250 140197572364032 logging_writer.py:48] [1] accumulated_eval_time=212.873403, accumulated_logging_time=0, accumulated_submission_time=85.139115, global_step=1, preemption_count=0, score=85.139115, test/loss=0.983138, test/num_examples=3581, test/ssim=0.215247, total_duration=298.013047, train/loss=0.971450, train/ssim=0.202690, validation/loss=0.980558, validation/num_examples=3554, validation/ssim=0.193129
I0815 09:55:10.100641 140262284560192 submission_runner.py:396] Released all unoccupied cached memory.
I0815 09:55:10.179568 139997794117440 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 09:55:10.179589 140581976233792 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 09:55:10.179617 139909710714688 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 09:55:10.179702 140082314057536 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 09:55:10.179571 140511383603008 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 09:55:10.179614 140262284560192 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 09:55:10.179856 140218347767616 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 09:55:10.179821 140326913230656 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 09:55:10.349359 140197563971328 logging_writer.py:48] [1] global_step=1, grad_norm=3.866007, loss=0.926963
I0815 09:55:10.353429 140262284560192 submission.py:120] 1) loss = 0.927, grad_norm = 3.866
I0815 09:55:10.421507 140197572364032 logging_writer.py:48] [2] global_step=2, grad_norm=4.279092, loss=0.982611
I0815 09:55:10.425539 140262284560192 submission.py:120] 2) loss = 0.983, grad_norm = 4.279
I0815 09:55:10.492849 140197563971328 logging_writer.py:48] [3] global_step=3, grad_norm=3.306056, loss=0.896656
I0815 09:55:10.496976 140262284560192 submission.py:120] 3) loss = 0.897, grad_norm = 3.306
I0815 09:55:10.596616 140197572364032 logging_writer.py:48] [4] global_step=4, grad_norm=4.095560, loss=0.944817
I0815 09:55:10.603099 140262284560192 submission.py:120] 4) loss = 0.945, grad_norm = 4.096
I0815 09:55:10.691288 140197563971328 logging_writer.py:48] [5] global_step=5, grad_norm=4.047602, loss=0.981297
I0815 09:55:10.697203 140262284560192 submission.py:120] 5) loss = 0.981, grad_norm = 4.048
I0815 09:55:10.792000 140197572364032 logging_writer.py:48] [6] global_step=6, grad_norm=3.639695, loss=0.958976
I0815 09:55:10.796727 140262284560192 submission.py:120] 6) loss = 0.959, grad_norm = 3.640
I0815 09:55:10.893790 140197563971328 logging_writer.py:48] [7] global_step=7, grad_norm=3.208955, loss=1.027544
I0815 09:55:10.906101 140262284560192 submission.py:120] 7) loss = 1.028, grad_norm = 3.209
I0815 09:55:11.008543 140197572364032 logging_writer.py:48] [8] global_step=8, grad_norm=3.861657, loss=0.934678
I0815 09:55:11.015395 140262284560192 submission.py:120] 8) loss = 0.935, grad_norm = 3.862
I0815 09:55:11.111258 140197563971328 logging_writer.py:48] [9] global_step=9, grad_norm=3.776079, loss=0.958378
I0815 09:55:11.116572 140262284560192 submission.py:120] 9) loss = 0.958, grad_norm = 3.776
I0815 09:55:11.215664 140197572364032 logging_writer.py:48] [10] global_step=10, grad_norm=3.845122, loss=0.904026
I0815 09:55:11.222420 140262284560192 submission.py:120] 10) loss = 0.904, grad_norm = 3.845
I0815 09:56:29.860140 140262284560192 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0815 09:56:31.911496 140262284560192 spec.py:332] Evaluating on the validation split.
I0815 09:56:34.152574 140262284560192 spec.py:348] Evaluating on the test split.
I0815 09:56:36.228290 140262284560192 submission_runner.py:365] Time since start: 384.56s, 	Step: 309, 	{'train/ssim': 0.7054684502737862, 'train/loss': 0.30072430201939176, 'validation/ssim': 0.6802251726707583, 'validation/loss': 0.3254885354200373, 'validation/num_examples': 3554, 'test/ssim': 0.6985875022907359, 'test/loss': 0.32752848829979403, 'test/num_examples': 3581, 'score': 164.63320565223694, 'total_duration': 384.55932211875916, 'accumulated_submission_time': 164.63320565223694, 'accumulated_eval_time': 219.24162316322327, 'accumulated_logging_time': 0.4757382869720459}
I0815 09:56:36.248152 140197563971328 logging_writer.py:48] [309] accumulated_eval_time=219.241623, accumulated_logging_time=0.475738, accumulated_submission_time=164.633206, global_step=309, preemption_count=0, score=164.633206, test/loss=0.327528, test/num_examples=3581, test/ssim=0.698588, total_duration=384.559322, train/loss=0.300724, train/ssim=0.705468, validation/loss=0.325489, validation/num_examples=3554, validation/ssim=0.680225
I0815 09:56:36.639516 140262284560192 submission_runner.py:396] Released all unoccupied cached memory.
I0815 09:57:41.454629 140197572364032 logging_writer.py:48] [500] global_step=500, grad_norm=0.304420, loss=0.270835
I0815 09:57:41.459722 140262284560192 submission.py:120] 500) loss = 0.271, grad_norm = 0.304
I0815 09:57:56.279394 140262284560192 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0815 09:57:58.295739 140262284560192 spec.py:332] Evaluating on the validation split.
I0815 09:58:00.473371 140262284560192 spec.py:348] Evaluating on the test split.
I0815 09:58:02.619223 140262284560192 submission_runner.py:365] Time since start: 470.95s, 	Step: 542, 	{'train/ssim': 0.7190832410539899, 'train/loss': 0.2894787447793143, 'validation/ssim': 0.6947928940102701, 'validation/loss': 0.3135909733640792, 'validation/num_examples': 3554, 'test/ssim': 0.712229993149609, 'test/loss': 0.31585095109169925, 'test/num_examples': 3581, 'score': 244.08952450752258, 'total_duration': 470.9502217769623, 'accumulated_submission_time': 244.08952450752258, 'accumulated_eval_time': 225.58208799362183, 'accumulated_logging_time': 0.90938401222229}
I0815 09:58:02.637615 140197563971328 logging_writer.py:48] [542] accumulated_eval_time=225.582088, accumulated_logging_time=0.909384, accumulated_submission_time=244.089525, global_step=542, preemption_count=0, score=244.089525, test/loss=0.315851, test/num_examples=3581, test/ssim=0.712230, total_duration=470.950222, train/loss=0.289479, train/ssim=0.719083, validation/loss=0.313591, validation/num_examples=3554, validation/ssim=0.694793
I0815 09:58:03.085772 140262284560192 submission_runner.py:396] Released all unoccupied cached memory.
I0815 09:59:22.641295 140262284560192 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0815 09:59:24.671484 140262284560192 spec.py:332] Evaluating on the validation split.
I0815 09:59:26.877335 140262284560192 spec.py:348] Evaluating on the test split.
I0815 09:59:28.956125 140262284560192 submission_runner.py:365] Time since start: 557.29s, 	Step: 773, 	{'train/ssim': 0.7194481577192035, 'train/loss': 0.2874106339045933, 'validation/ssim': 0.6960385332767656, 'validation/loss': 0.3108868102512134, 'validation/num_examples': 3554, 'test/ssim': 0.7125693765707903, 'test/loss': 0.3134483714504503, 'test/num_examples': 3581, 'score': 323.47922468185425, 'total_duration': 557.2872078418732, 'accumulated_submission_time': 323.47922468185425, 'accumulated_eval_time': 231.89757084846497, 'accumulated_logging_time': 1.3761022090911865}
I0815 09:59:28.975576 140197572364032 logging_writer.py:48] [773] accumulated_eval_time=231.897571, accumulated_logging_time=1.376102, accumulated_submission_time=323.479225, global_step=773, preemption_count=0, score=323.479225, test/loss=0.313448, test/num_examples=3581, test/ssim=0.712569, total_duration=557.287208, train/loss=0.287411, train/ssim=0.719448, validation/loss=0.310887, validation/num_examples=3554, validation/ssim=0.696039
I0815 09:59:29.398893 140262284560192 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:00:46.945999 140197563971328 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.246529, loss=0.341298
I0815 10:00:46.952154 140262284560192 submission.py:120] 1000) loss = 0.341, grad_norm = 0.247
I0815 10:00:49.087679 140262284560192 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0815 10:00:51.041830 140262284560192 spec.py:332] Evaluating on the validation split.
I0815 10:00:53.449398 140262284560192 spec.py:348] Evaluating on the test split.
I0815 10:00:55.456195 140262284560192 submission_runner.py:365] Time since start: 643.79s, 	Step: 1009, 	{'train/ssim': 0.7323983056204659, 'train/loss': 0.27785708223070416, 'validation/ssim': 0.7079054575390406, 'validation/loss': 0.3016838627580719, 'validation/num_examples': 3554, 'test/ssim': 0.7250771350748744, 'test/loss': 0.30375825210311364, 'test/num_examples': 3581, 'score': 403.0004963874817, 'total_duration': 643.7872447967529, 'accumulated_submission_time': 403.0004963874817, 'accumulated_eval_time': 238.26608848571777, 'accumulated_logging_time': 1.8233919143676758}
I0815 10:00:55.472376 140197572364032 logging_writer.py:48] [1009] accumulated_eval_time=238.266088, accumulated_logging_time=1.823392, accumulated_submission_time=403.000496, global_step=1009, preemption_count=0, score=403.000496, test/loss=0.303758, test/num_examples=3581, test/ssim=0.725077, total_duration=643.787245, train/loss=0.277857, train/ssim=0.732398, validation/loss=0.301684, validation/num_examples=3554, validation/ssim=0.707905
I0815 10:00:55.886315 140262284560192 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:02:15.500158 140262284560192 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0815 10:02:17.475894 140262284560192 spec.py:332] Evaluating on the validation split.
I0815 10:02:19.579107 140262284560192 spec.py:348] Evaluating on the test split.
I0815 10:02:21.564331 140262284560192 submission_runner.py:365] Time since start: 729.90s, 	Step: 1319, 	{'train/ssim': 0.7334903308323452, 'train/loss': 0.2758855308805193, 'validation/ssim': 0.7089533250386888, 'validation/loss': 0.2995632259645822, 'validation/num_examples': 3554, 'test/ssim': 0.7259129809454412, 'test/loss': 0.30160271059498045, 'test/num_examples': 3581, 'score': 482.4715621471405, 'total_duration': 729.8954095840454, 'accumulated_submission_time': 482.4715621471405, 'accumulated_eval_time': 244.33033561706543, 'accumulated_logging_time': 2.2683651447296143}
I0815 10:02:21.579151 140197563971328 logging_writer.py:48] [1319] accumulated_eval_time=244.330336, accumulated_logging_time=2.268365, accumulated_submission_time=482.471562, global_step=1319, preemption_count=0, score=482.471562, test/loss=0.301603, test/num_examples=3581, test/ssim=0.725913, total_duration=729.895410, train/loss=0.275886, train/ssim=0.733490, validation/loss=0.299563, validation/num_examples=3554, validation/ssim=0.708953
I0815 10:02:22.004076 140262284560192 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:03:07.585711 140197572364032 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.069933, loss=0.304745
I0815 10:03:07.589295 140262284560192 submission.py:120] 1500) loss = 0.305, grad_norm = 0.070
I0815 10:03:41.816211 140262284560192 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0815 10:03:43.749761 140262284560192 spec.py:332] Evaluating on the validation split.
I0815 10:03:45.834418 140262284560192 spec.py:348] Evaluating on the test split.
I0815 10:03:47.813299 140262284560192 submission_runner.py:365] Time since start: 816.14s, 	Step: 1630, 	{'train/ssim': 0.7376091820853097, 'train/loss': 0.2735067776271275, 'validation/ssim': 0.7129696924899761, 'validation/loss': 0.2972047341024198, 'validation/num_examples': 3554, 'test/ssim': 0.7300823928590129, 'test/loss': 0.29904202927560386, 'test/num_examples': 3581, 'score': 562.1489160060883, 'total_duration': 816.144374370575, 'accumulated_submission_time': 562.1489160060883, 'accumulated_eval_time': 250.32737970352173, 'accumulated_logging_time': 2.7139575481414795}
I0815 10:03:47.828939 140197563971328 logging_writer.py:48] [1630] accumulated_eval_time=250.327380, accumulated_logging_time=2.713958, accumulated_submission_time=562.148916, global_step=1630, preemption_count=0, score=562.148916, test/loss=0.299042, test/num_examples=3581, test/ssim=0.730082, total_duration=816.144374, train/loss=0.273507, train/ssim=0.737609, validation/loss=0.297205, validation/num_examples=3554, validation/ssim=0.712970
I0815 10:03:48.263430 140262284560192 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:05:07.825981 140262284560192 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0815 10:05:09.793854 140262284560192 spec.py:332] Evaluating on the validation split.
I0815 10:05:11.892765 140262284560192 spec.py:348] Evaluating on the test split.
I0815 10:05:13.878082 140262284560192 submission_runner.py:365] Time since start: 902.21s, 	Step: 1941, 	{'train/ssim': 0.7373067310878209, 'train/loss': 0.272759062903268, 'validation/ssim': 0.7124737861388576, 'validation/loss': 0.2967085529728827, 'validation/num_examples': 3554, 'test/ssim': 0.7298502513264451, 'test/loss': 0.29831090275717326, 'test/num_examples': 3581, 'score': 641.5825884342194, 'total_duration': 902.209153175354, 'accumulated_submission_time': 641.5825884342194, 'accumulated_eval_time': 256.37945079803467, 'accumulated_logging_time': 3.164118528366089}
I0815 10:05:13.892502 140197572364032 logging_writer.py:48] [1941] accumulated_eval_time=256.379451, accumulated_logging_time=3.164119, accumulated_submission_time=641.582588, global_step=1941, preemption_count=0, score=641.582588, test/loss=0.298311, test/num_examples=3581, test/ssim=0.729850, total_duration=902.209153, train/loss=0.272759, train/ssim=0.737307, validation/loss=0.296709, validation/num_examples=3554, validation/ssim=0.712474
I0815 10:05:14.297646 140262284560192 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:05:27.814897 140197563971328 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.200973, loss=0.246818
I0815 10:05:27.818492 140262284560192 submission.py:120] 2000) loss = 0.247, grad_norm = 0.201
I0815 10:06:34.157964 140262284560192 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0815 10:06:36.136276 140262284560192 spec.py:332] Evaluating on the validation split.
I0815 10:06:38.266097 140262284560192 spec.py:348] Evaluating on the test split.
I0815 10:06:40.259470 140262284560192 submission_runner.py:365] Time since start: 988.59s, 	Step: 2249, 	{'train/ssim': 0.7281735965183803, 'train/loss': 0.27349397114345003, 'validation/ssim': 0.707169807017621, 'validation/loss': 0.29684927387055077, 'validation/num_examples': 3554, 'test/ssim': 0.7232578408614913, 'test/loss': 0.2986218224221586, 'test/num_examples': 3581, 'score': 721.2935335636139, 'total_duration': 988.5905706882477, 'accumulated_submission_time': 721.2935335636139, 'accumulated_eval_time': 262.4809799194336, 'accumulated_logging_time': 3.6055171489715576}
I0815 10:06:40.274994 140197572364032 logging_writer.py:48] [2249] accumulated_eval_time=262.480980, accumulated_logging_time=3.605517, accumulated_submission_time=721.293534, global_step=2249, preemption_count=0, score=721.293534, test/loss=0.298622, test/num_examples=3581, test/ssim=0.723258, total_duration=988.590571, train/loss=0.273494, train/ssim=0.728174, validation/loss=0.296849, validation/num_examples=3554, validation/ssim=0.707170
I0815 10:06:40.696025 140262284560192 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:07:45.313268 140197563971328 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.100218, loss=0.246166
I0815 10:07:45.317357 140262284560192 submission.py:120] 2500) loss = 0.246, grad_norm = 0.100
I0815 10:08:00.294767 140262284560192 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0815 10:08:02.263554 140262284560192 spec.py:332] Evaluating on the validation split.
I0815 10:08:04.379521 140262284560192 spec.py:348] Evaluating on the test split.
I0815 10:08:06.375120 140262284560192 submission_runner.py:365] Time since start: 1074.71s, 	Step: 2558, 	{'train/ssim': 0.7392261368887765, 'train/loss': 0.2720076356615339, 'validation/ssim': 0.7145987850397439, 'validation/loss': 0.29543437109814646, 'validation/num_examples': 3554, 'test/ssim': 0.7316915666015079, 'test/loss': 0.29724294944236945, 'test/num_examples': 3581, 'score': 800.7542502880096, 'total_duration': 1074.7061989307404, 'accumulated_submission_time': 800.7542502880096, 'accumulated_eval_time': 268.5612931251526, 'accumulated_logging_time': 4.052951335906982}
I0815 10:08:06.390823 140197572364032 logging_writer.py:48] [2558] accumulated_eval_time=268.561293, accumulated_logging_time=4.052951, accumulated_submission_time=800.754250, global_step=2558, preemption_count=0, score=800.754250, test/loss=0.297243, test/num_examples=3581, test/ssim=0.731692, total_duration=1074.706199, train/loss=0.272008, train/ssim=0.739226, validation/loss=0.295434, validation/num_examples=3554, validation/ssim=0.714599
I0815 10:08:06.785893 140262284560192 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:08:45.735641 140262284560192 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0815 10:08:47.686315 140262284560192 spec.py:332] Evaluating on the validation split.
I0815 10:08:49.743390 140262284560192 spec.py:348] Evaluating on the test split.
I0815 10:08:51.718934 140262284560192 submission_runner.py:365] Time since start: 1120.05s, 	Step: 2714, 	{'train/ssim': 0.7407509940011161, 'train/loss': 0.26831235204424175, 'validation/ssim': 0.7158030701542276, 'validation/loss': 0.29215454719813944, 'validation/num_examples': 3554, 'test/ssim': 0.732963129516022, 'test/loss': 0.2938268214622312, 'test/num_examples': 3581, 'score': 839.6026043891907, 'total_duration': 1120.0500092506409, 'accumulated_submission_time': 839.6026043891907, 'accumulated_eval_time': 274.54460167884827, 'accumulated_logging_time': 4.498829364776611}
I0815 10:08:51.734403 140197563971328 logging_writer.py:48] [2714] accumulated_eval_time=274.544602, accumulated_logging_time=4.498829, accumulated_submission_time=839.602604, global_step=2714, preemption_count=0, score=839.602604, test/loss=0.293827, test/num_examples=3581, test/ssim=0.732963, total_duration=1120.050009, train/loss=0.268312, train/ssim=0.740751, validation/loss=0.292155, validation/num_examples=3554, validation/ssim=0.715803
I0815 10:08:52.140464 140262284560192 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:08:52.160081 140197572364032 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=839.602604
I0815 10:08:52.295781 140262284560192 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/fastmri_pytorch/trial_1/checkpoint_2714.
I0815 10:08:53.790426 140262284560192 submission_runner.py:534] Tuning trial 1/1
I0815 10:08:53.790650 140262284560192 submission_runner.py:535] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0815 10:08:53.795353 140262284560192 submission_runner.py:536] Metrics: {'eval_results': [(1, {'train/ssim': 0.20268978391374862, 'train/loss': 0.9714501244681222, 'validation/ssim': 0.19312887025402714, 'validation/loss': 0.980557640818444, 'validation/num_examples': 3554, 'test/ssim': 0.21524675929057002, 'test/loss': 0.983138343536198, 'test/num_examples': 3581, 'score': 85.13911485671997, 'total_duration': 298.0130469799042, 'accumulated_submission_time': 85.13911485671997, 'accumulated_eval_time': 212.87340307235718, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (309, {'train/ssim': 0.7054684502737862, 'train/loss': 0.30072430201939176, 'validation/ssim': 0.6802251726707583, 'validation/loss': 0.3254885354200373, 'validation/num_examples': 3554, 'test/ssim': 0.6985875022907359, 'test/loss': 0.32752848829979403, 'test/num_examples': 3581, 'score': 164.63320565223694, 'total_duration': 384.55932211875916, 'accumulated_submission_time': 164.63320565223694, 'accumulated_eval_time': 219.24162316322327, 'accumulated_logging_time': 0.4757382869720459, 'global_step': 309, 'preemption_count': 0}), (542, {'train/ssim': 0.7190832410539899, 'train/loss': 0.2894787447793143, 'validation/ssim': 0.6947928940102701, 'validation/loss': 0.3135909733640792, 'validation/num_examples': 3554, 'test/ssim': 0.712229993149609, 'test/loss': 0.31585095109169925, 'test/num_examples': 3581, 'score': 244.08952450752258, 'total_duration': 470.9502217769623, 'accumulated_submission_time': 244.08952450752258, 'accumulated_eval_time': 225.58208799362183, 'accumulated_logging_time': 0.90938401222229, 'global_step': 542, 'preemption_count': 0}), (773, {'train/ssim': 0.7194481577192035, 'train/loss': 0.2874106339045933, 'validation/ssim': 0.6960385332767656, 'validation/loss': 0.3108868102512134, 'validation/num_examples': 3554, 'test/ssim': 0.7125693765707903, 'test/loss': 0.3134483714504503, 'test/num_examples': 3581, 'score': 323.47922468185425, 'total_duration': 557.2872078418732, 'accumulated_submission_time': 323.47922468185425, 'accumulated_eval_time': 231.89757084846497, 'accumulated_logging_time': 1.3761022090911865, 'global_step': 773, 'preemption_count': 0}), (1009, {'train/ssim': 0.7323983056204659, 'train/loss': 0.27785708223070416, 'validation/ssim': 0.7079054575390406, 'validation/loss': 0.3016838627580719, 'validation/num_examples': 3554, 'test/ssim': 0.7250771350748744, 'test/loss': 0.30375825210311364, 'test/num_examples': 3581, 'score': 403.0004963874817, 'total_duration': 643.7872447967529, 'accumulated_submission_time': 403.0004963874817, 'accumulated_eval_time': 238.26608848571777, 'accumulated_logging_time': 1.8233919143676758, 'global_step': 1009, 'preemption_count': 0}), (1319, {'train/ssim': 0.7334903308323452, 'train/loss': 0.2758855308805193, 'validation/ssim': 0.7089533250386888, 'validation/loss': 0.2995632259645822, 'validation/num_examples': 3554, 'test/ssim': 0.7259129809454412, 'test/loss': 0.30160271059498045, 'test/num_examples': 3581, 'score': 482.4715621471405, 'total_duration': 729.8954095840454, 'accumulated_submission_time': 482.4715621471405, 'accumulated_eval_time': 244.33033561706543, 'accumulated_logging_time': 2.2683651447296143, 'global_step': 1319, 'preemption_count': 0}), (1630, {'train/ssim': 0.7376091820853097, 'train/loss': 0.2735067776271275, 'validation/ssim': 0.7129696924899761, 'validation/loss': 0.2972047341024198, 'validation/num_examples': 3554, 'test/ssim': 0.7300823928590129, 'test/loss': 0.29904202927560386, 'test/num_examples': 3581, 'score': 562.1489160060883, 'total_duration': 816.144374370575, 'accumulated_submission_time': 562.1489160060883, 'accumulated_eval_time': 250.32737970352173, 'accumulated_logging_time': 2.7139575481414795, 'global_step': 1630, 'preemption_count': 0}), (1941, {'train/ssim': 0.7373067310878209, 'train/loss': 0.272759062903268, 'validation/ssim': 0.7124737861388576, 'validation/loss': 0.2967085529728827, 'validation/num_examples': 3554, 'test/ssim': 0.7298502513264451, 'test/loss': 0.29831090275717326, 'test/num_examples': 3581, 'score': 641.5825884342194, 'total_duration': 902.209153175354, 'accumulated_submission_time': 641.5825884342194, 'accumulated_eval_time': 256.37945079803467, 'accumulated_logging_time': 3.164118528366089, 'global_step': 1941, 'preemption_count': 0}), (2249, {'train/ssim': 0.7281735965183803, 'train/loss': 0.27349397114345003, 'validation/ssim': 0.707169807017621, 'validation/loss': 0.29684927387055077, 'validation/num_examples': 3554, 'test/ssim': 0.7232578408614913, 'test/loss': 0.2986218224221586, 'test/num_examples': 3581, 'score': 721.2935335636139, 'total_duration': 988.5905706882477, 'accumulated_submission_time': 721.2935335636139, 'accumulated_eval_time': 262.4809799194336, 'accumulated_logging_time': 3.6055171489715576, 'global_step': 2249, 'preemption_count': 0}), (2558, {'train/ssim': 0.7392261368887765, 'train/loss': 0.2720076356615339, 'validation/ssim': 0.7145987850397439, 'validation/loss': 0.29543437109814646, 'validation/num_examples': 3554, 'test/ssim': 0.7316915666015079, 'test/loss': 0.29724294944236945, 'test/num_examples': 3581, 'score': 800.7542502880096, 'total_duration': 1074.7061989307404, 'accumulated_submission_time': 800.7542502880096, 'accumulated_eval_time': 268.5612931251526, 'accumulated_logging_time': 4.052951335906982, 'global_step': 2558, 'preemption_count': 0}), (2714, {'train/ssim': 0.7407509940011161, 'train/loss': 0.26831235204424175, 'validation/ssim': 0.7158030701542276, 'validation/loss': 0.29215454719813944, 'validation/num_examples': 3554, 'test/ssim': 0.732963129516022, 'test/loss': 0.2938268214622312, 'test/num_examples': 3581, 'score': 839.6026043891907, 'total_duration': 1120.0500092506409, 'accumulated_submission_time': 839.6026043891907, 'accumulated_eval_time': 274.54460167884827, 'accumulated_logging_time': 4.498829364776611, 'global_step': 2714, 'preemption_count': 0})], 'global_step': 2714}
I0815 10:08:53.795495 140262284560192 submission_runner.py:537] Timing: 839.6026043891907
I0815 10:08:53.795544 140262284560192 submission_runner.py:539] Total number of evals: 11
I0815 10:08:53.795587 140262284560192 submission_runner.py:540] ====================
I0815 10:08:53.795690 140262284560192 submission_runner.py:608] Final fastmri score: 839.6026043891907
