torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_after_pytorch_fixes/adamw --overwrite=True --save_checkpoints=False --max_global_steps=14000 --imagenet_v2_data_dir=/data/imagenet/pytorch --torch_compile=true 2>&1 | tee -a /logs/imagenet_resnet_pytorch_08-15-2023-05-36-03.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-15 05:36:13.531779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 05:36:13.531780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 05:36:13.531779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 05:36:13.531779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 05:36:13.531787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 05:36:13.531779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 05:36:13.531779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 05:36:13.531779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0815 05:36:28.065577 139633595471680 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0815 05:36:28.065612 140218002184000 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0815 05:36:29.043321 140362618492736 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0815 05:36:29.043472 139775998064448 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0815 05:36:29.043522 140703107626816 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0815 05:36:29.043504 140243764496192 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0815 05:36:29.043640 140696797321024 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0815 05:36:29.045222 139906658768704 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0815 05:36:29.045620 139906658768704 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 05:36:29.053972 139633595471680 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 05:36:29.054000 140218002184000 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 05:36:29.054013 140362618492736 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 05:36:29.054055 139775998064448 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 05:36:29.054148 140703107626816 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 05:36:29.054205 140243764496192 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 05:36:29.054197 140696797321024 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0815 05:36:30.356038 139906658768704 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_resnet_pytorch.
W0815 05:36:30.399316 140696797321024 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 05:36:30.399424 140243764496192 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 05:36:30.399374 140218002184000 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 05:36:30.399685 140703107626816 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 05:36:30.399810 139775998064448 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 05:36:30.400253 139633595471680 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 05:36:30.401172 140362618492736 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 05:36:30.401821 139906658768704 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0815 05:36:30.407068 139906658768704 submission_runner.py:494] Using RNG seed 410343880
I0815 05:36:30.408399 139906658768704 submission_runner.py:503] --- Tuning run 1/1 ---
I0815 05:36:30.408518 139906658768704 submission_runner.py:508] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_resnet_pytorch/trial_1.
I0815 05:36:30.408769 139906658768704 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0815 05:36:30.409586 139906658768704 submission_runner.py:177] Initializing dataset.
I0815 05:36:36.910207 139906658768704 submission_runner.py:184] Initializing model.
I0815 05:36:41.601862 139906658768704 submission_runner.py:215] Performing `torch.compile`.
I0815 05:36:42.173988 139906658768704 submission_runner.py:218] Initializing optimizer.
I0815 05:36:42.175469 139906658768704 submission_runner.py:225] Initializing metrics bundle.
I0815 05:36:42.175569 139906658768704 submission_runner.py:243] Initializing checkpoint and logger.
I0815 05:36:42.623900 139906658768704 submission_runner.py:264] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0815 05:36:42.624875 139906658768704 submission_runner.py:267] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0815 05:36:42.704223 139906658768704 submission_runner.py:277] Starting training loop.
[2023-08-15 05:36:44,943] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:36:44,947] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:36:44,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:36:45,091] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:36:45,168] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:36:45,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:36:45,198] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:36:45,200] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:36:46,794] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:36:46,873] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:36:46,877] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:36:46,877] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:36:46,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:36:46,918] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:36:46,935] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:36:46,939] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:36:46,939] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:36:46,997] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:36:47,001] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:36:47,002] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:36:47,029] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:36:47,078] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:36:47,101] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:36:47,105] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:36:47,105] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:36:47,107] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:36:47,110] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:36:47,111] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:36:47,130] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:36:47,135] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:36:47,151] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:36:47,154] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:36:47,157] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:36:47,162] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:36:47,163] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:36:47,167] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:36:47,167] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:36:47,173] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:36:47,176] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:36:47,177] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:36:57,610] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 05:36:57,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 05:36:57,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 05:36:57,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 05:36:57,627] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 05:36:57,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 05:36:57,930] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 05:36:57,943] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 05:37:06,691] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 05:37:06,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 05:37:06,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 05:37:06,751] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 05:37:06,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 05:37:06,783] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 05:37:07,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 05:37:07,096] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 05:37:11,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 05:37:11,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 05:37:11,107] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 05:37:11,115] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 05:37:11,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 05:37:11,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 05:37:11,627] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 05:37:11,949] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 05:37:12,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 05:37:12,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 05:37:12,953] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 05:37:12,982] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 05:37:13,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 05:37:13,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 05:37:13,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 05:37:13,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 05:37:14,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 05:37:14,076] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 05:37:14,084] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 05:37:14,110] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 05:37:14,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 05:37:14,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 05:37:14,751] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 05:37:14,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 05:37:14,908] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 05:37:14,930] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 05:37:14,977] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 05:37:14,993] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 05:37:15,207] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 05:37:15,543] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 05:37:15,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 05:37:15,795] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 05:37:16,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 05:37:16,099] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 05:37:16,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 05:37:16,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 05:37:16,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 05:37:16,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 05:37:16,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 05:37:16,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 05:37:17,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 05:37:17,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 05:37:17,667] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 05:37:17,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 05:37:17,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 05:37:17,774] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 05:37:17,777] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 05:37:17,782] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:37:17,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 05:37:17,791] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:37:17,807] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 05:37:17,812] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 05:37:17,819] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:37:17,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 05:37:17,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 05:37:17,866] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:37:18,023] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 05:37:18,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 05:37:18,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 05:37:18,243] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:37:18,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 05:37:18,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 05:37:18,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 05:37:18,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 05:37:18,478] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:37:18,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 05:37:18,510] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 05:37:18,517] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:37:18,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 05:37:18,819] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 05:37:18,825] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 05:37:18,832] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:37:24,021] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 05:37:24,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 05:37:24,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 05:37:24,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 05:37:24,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 05:37:24,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 05:37:24,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 05:37:24,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 05:37:24,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 05:37:24,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 05:37:24,608] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 05:37:24,614] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 05:37:24,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 05:37:24,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 05:37:24,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 05:37:24,828] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 05:37:24,830] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 05:37:24,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 05:37:24,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 05:37:24,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 05:37:24,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 05:37:24,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 05:37:24,905] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 05:37:24,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 05:37:25,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 05:37:25,792] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 05:37:25,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 05:37:26,088] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 05:37:26,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 05:37:26,160] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 05:37:26,204] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 05:37:26,209] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 05:37:26,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 05:37:26,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 05:37:26,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 05:37:27,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 05:37:27,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 05:37:27,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 05:37:27,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 05:37:27,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 05:37:28,023] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 05:37:28,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 05:37:28,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 05:37:28,263] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 05:37:28,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 05:37:28,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 05:37:28,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 05:37:28,513] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 05:37:29,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 05:37:29,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 05:37:29,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 05:37:29,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 05:37:29,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 05:37:29,927] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 05:37:29,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 05:37:30,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 05:37:31,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 05:37:31,811] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 05:37:31,874] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 05:37:32,019] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 05:37:32,063] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 05:37:32,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 05:37:32,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 05:37:32,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 05:37:33,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 05:37:33,121] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 05:37:33,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 05:37:33,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 05:37:33,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 05:37:33,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 05:37:33,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 05:37:33,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 05:37:38,390] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 05:37:38,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 05:37:38,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 05:37:38,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 05:37:38,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 05:37:38,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 05:37:38,856] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 05:37:38,891] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I0815 05:37:42.264842 139879761569536 logging_writer.py:48] [0] global_step=0, grad_norm=0.588465, loss=6.921607
I0815 05:37:42.292768 139906658768704 submission.py:120] 0) loss = 6.922, grad_norm = 0.588
I0815 05:37:42.329212 139906658768704 spec.py:320] Evaluating on the training split.
[2023-08-15 05:37:51,981] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:37:52,004] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:37:52,022] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:37:52,078] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:37:52,140] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:37:52,763] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:37:52,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:37:52,860] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:37:53,278] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:37:53,297] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:37:53,300] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:37:53,301] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:37:53,328] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:37:53,334] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:37:53,347] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:37:53,350] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:37:53,351] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:37:53,353] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:37:53,356] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:37:53,357] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:37:53,409] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:37:53,428] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:37:53,431] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:37:53,432] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:37:53,448] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:37:53,467] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:37:53,470] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:37:53,471] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:37:54,154] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:37:54,175] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:37:54,176] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:37:54,179] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:37:54,179] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:37:54,198] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:37:54,201] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:37:54,202] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:37:54,251] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:37:54,274] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:37:54,277] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:37:54,278] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:37:56,324] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 05:37:56,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 05:37:56,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 05:37:56,475] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 05:37:56,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 05:37:57,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 05:37:57,633] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 05:37:57,649] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 05:37:59,349] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 05:37:59,355] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 05:37:59,387] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 05:37:59,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 05:37:59,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 05:38:00,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 05:38:00,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 05:38:00,918] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 05:38:01,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 05:38:01,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 05:38:01,765] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 05:38:01,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 05:38:01,901] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 05:38:02,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 05:38:03,027] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 05:38:03,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 05:38:03,075] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 05:38:03,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 05:38:03,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 05:38:03,163] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 05:38:03,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 05:38:03,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 05:38:03,750] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 05:38:03,777] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 05:38:03,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 05:38:03,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 05:38:04,211] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 05:38:04,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 05:38:04,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 05:38:04,385] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 05:38:04,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 05:38:04,433] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 05:38:04,475] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 05:38:04,493] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 05:38:04,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 05:38:04,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 05:38:04,957] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 05:38:04,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 05:38:04,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 05:38:05,022] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 05:38:05,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 05:38:05,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 05:38:05,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 05:38:05,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 05:38:05,596] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 05:38:05,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 05:38:05,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 05:38:05,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 05:38:05,657] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 05:38:05,663] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:38:05,665] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 05:38:05,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 05:38:05,700] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 05:38:05,750] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 05:38:05,755] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 05:38:05,755] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 05:38:05,759] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 05:38:05,760] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:38:05,764] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:38:05,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 05:38:05,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 05:38:05,804] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:38:05,816] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 05:38:05,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 05:38:05,826] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:38:06,099] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 05:38:06,146] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 05:38:06,248] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 05:38:06,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 05:38:06,754] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 05:38:06,863] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 05:38:06,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 05:38:06,867] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 05:38:06,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 05:38:06,873] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:38:06,875] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:38:06,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 05:38:06,982] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 05:38:06,987] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 05:38:06,992] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0815 05:38:56.623407 139906658768704 spec.py:332] Evaluating on the validation split.
[2023-08-15 05:39:47,608] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:39:47,787] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:39:47,894] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:39:48,406] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:39:48,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:39:48,972] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:39:48,975] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:39:48,975] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:39:49,106] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:39:49,125] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:39:49,128] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:39:49,128] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:39:49,308] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:39:49,330] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:39:49,333] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:39:49,334] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:39:49,348] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:39:49,742] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:39:49,761] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:39:49,764] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:39:49,764] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:39:50,444] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:39:50,648] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:39:50,666] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:39:50,670] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:39:50,670] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:39:51,129] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:39:51,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 05:39:51,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 05:39:51,891] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:39:51,912] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:39:51,916] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:39:51,916] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:39:52,079] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 05:39:52,300] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 05:39:52,486] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:39:52,530] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:39:52,551] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:39:52,554] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:39:52,554] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:39:53,213] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 05:39:53,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 05:39:53,992] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:39:54,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 05:39:54,013] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:39:54,016] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:39:54,017] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:39:54,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 05:39:54,663] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 05:39:54,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 05:39:55,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 05:39:55,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 05:39:56,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 05:39:56,136] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 05:39:56,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 05:39:56,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 05:39:56,786] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 05:39:57,063] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 05:39:57,847] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 05:39:57,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 05:39:57,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 05:39:57,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 05:39:58,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 05:39:58,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 05:39:58,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 05:39:58,651] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 05:39:59,156] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 05:39:59,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 05:39:59,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 05:39:59,275] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 05:39:59,287] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 05:39:59,317] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 05:39:59,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 05:39:59,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 05:39:59,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 05:39:59,850] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 05:39:59,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 05:40:00,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 05:40:00,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 05:40:00,318] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 05:40:00,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 05:40:00,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 05:40:00,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 05:40:00,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 05:40:00,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:00,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 05:40:00,537] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 05:40:00,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 05:40:00,546] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:00,932] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 05:40:01,023] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 05:40:01,046] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 05:40:01,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 05:40:01,057] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:01,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 05:40:01,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 05:40:01,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 05:40:01,205] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:01,235] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 05:40:01,346] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 05:40:01,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 05:40:01,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 05:40:02,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 05:40:02,383] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 05:40:02,491] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 05:40:02,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 05:40:02,501] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:02,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 05:40:02,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 05:40:03,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 05:40:03,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 05:40:03,273] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 05:40:03,826] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 05:40:03,833] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 05:40:03,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 05:40:03,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 05:40:03,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 05:40:03,951] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:04,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 05:40:04,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 05:40:04,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 05:40:04,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 05:40:04,583] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:05,080] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 05:40:05,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 05:40:05,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 05:40:05,768] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 05:40:05,774] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0815 05:40:06.825864 139906658768704 spec.py:348] Evaluating on the test split.
I0815 05:40:06.842919 139906658768704 dataset_info.py:578] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0815 05:40:06.849085 139906658768704 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0815 05:40:06.918747 139906658768704 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
[2023-08-15 05:40:09,007] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:09,014] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:09,050] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:09,153] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:09,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:09,209] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:09,257] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:09,278] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:11,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:11,987] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:11,990] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:11,991] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:12,166] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:12,185] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:12,188] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:12,189] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:12,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:12,235] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:12,248] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:12,253] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:12,254] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:12,256] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:12,257] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:12,257] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:12,257] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:12,267] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:12,269] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:12,270] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:12,270] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:12,287] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:12,290] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:12,291] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:12,332] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:12,350] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:12,354] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:12,354] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:12,430] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:12,449] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:12,452] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:12,453] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:14,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 05:40:14,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 05:40:14,774] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 05:40:14,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 05:40:14,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 05:40:14,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 05:40:14,898] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 05:40:14,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 05:40:21,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 05:40:21,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 05:40:21,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 05:40:21,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 05:40:21,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 05:40:21,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 05:40:21,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 05:40:21,614] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 05:40:23,296] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 05:40:23,323] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 05:40:23,325] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 05:40:23,408] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 05:40:23,499] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 05:40:23,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 05:40:23,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 05:40:23,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 05:40:27,900] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 05:40:27,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 05:40:27,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 05:40:27,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 05:40:28,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 05:40:28,157] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 05:40:28,169] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 05:40:28,185] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 05:40:28,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 05:40:28,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 05:40:28,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 05:40:28,636] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 05:40:28,745] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 05:40:28,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 05:40:28,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 05:40:28,812] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 05:40:29,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 05:40:29,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 05:40:29,250] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 05:40:29,279] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 05:40:29,397] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 05:40:29,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 05:40:29,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 05:40:29,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 05:40:29,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 05:40:29,811] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 05:40:29,826] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 05:40:29,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 05:40:29,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 05:40:29,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 05:40:30,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 05:40:30,039] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 05:40:32,164] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 05:40:32,182] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 05:40:32,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 05:40:32,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 05:40:32,273] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 05:40:32,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 05:40:32,282] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:32,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 05:40:32,299] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 05:40:32,304] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:32,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 05:40:32,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 05:40:32,320] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:32,335] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 05:40:32,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 05:40:32,343] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:32,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 05:40:32,386] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 05:40:32,400] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 05:40:32,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 05:40:32,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 05:40:32,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 05:40:32,466] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:32,497] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 05:40:32,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 05:40:32,506] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:32,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 05:40:32,512] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 05:40:32,517] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:32,540] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 05:40:32,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 05:40:32,549] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:40:44,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:45,121] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:45,165] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:45,212] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:45,227] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:45,238] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:45,241] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:45,298] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 05:40:46,338] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:46,357] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:46,360] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:46,361] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:46,473] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:46,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:46,493] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:46,496] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:46,496] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:46,508] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:46,511] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:46,512] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:46,522] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:46,541] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:46,544] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:46,545] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:46,557] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:46,560] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:46,576] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:46,579] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:46,579] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:46,579] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:46,579] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:46,583] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:46,583] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:46,598] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:46,601] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:46,602] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:46,635] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 05:40:46,654] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 05:40:46,657] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 05:40:46,657] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 05:40:49,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 05:40:49,555] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 05:40:49,582] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 05:40:49,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 05:40:49,678] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 05:40:49,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 05:40:49,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 05:40:49,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 05:40:53,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 05:40:53,867] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 05:40:53,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 05:40:54,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 05:40:54,055] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 05:40:54,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 05:40:54,112] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 05:40:54,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 05:40:55,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 05:40:55,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 05:40:55,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 05:40:56,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 05:40:56,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 05:40:56,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 05:40:56,126] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 05:40:56,146] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 05:40:58,662] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 05:40:58,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 05:40:58,895] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 05:40:58,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 05:40:58,984] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 05:40:59,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 05:40:59,094] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 05:40:59,123] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 05:40:59,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 05:40:59,358] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 05:40:59,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 05:40:59,579] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 05:40:59,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 05:40:59,678] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 05:40:59,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 05:40:59,767] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 05:40:59,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 05:40:59,949] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 05:41:00,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 05:41:00,174] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 05:41:00,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 05:41:00,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 05:41:00,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 05:41:00,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 05:41:00,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 05:41:00,538] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 05:41:00,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 05:41:00,736] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 05:41:00,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 05:41:00,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 05:41:00,863] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 05:41:00,964] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 05:41:02,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 05:41:02,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 05:41:02,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 05:41:02,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 05:41:02,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 05:41:02,238] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:41:02,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 05:41:02,254] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:41:02,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 05:41:02,405] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 05:41:02,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 05:41:02,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 05:41:02,444] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 05:41:02,450] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:41:02,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 05:41:02,513] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 05:41:02,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 05:41:02,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 05:41:02,532] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:41:02,545] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 05:41:02,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 05:41:02,556] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:41:02,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 05:41:02,616] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 05:41:02,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 05:41:02,621] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:41:02,624] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 05:41:02,629] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 05:41:02,656] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 05:41:02,766] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 05:41:02,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 05:41:02,777] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0815 05:41:07.132388 139906658768704 submission_runner.py:365] Time since start: 264.43s, 	Step: 1, 	{'train/accuracy': 0.0010363520408163266, 'train/loss': 6.923890405771684, 'validation/accuracy': 0.00072, 'validation/loss': 6.923479375, 'validation/num_examples': 50000, 'test/accuracy': 0.0008, 'test/loss': 6.92447109375, 'test/num_examples': 10000, 'score': 59.62487268447876, 'total_duration': 264.42850613594055, 'accumulated_submission_time': 59.62487268447876, 'accumulated_eval_time': 204.80307984352112, 'accumulated_logging_time': 0}
I0815 05:41:07.152460 139861096912640 logging_writer.py:48] [1] accumulated_eval_time=204.803080, accumulated_logging_time=0, accumulated_submission_time=59.624873, global_step=1, preemption_count=0, score=59.624873, test/accuracy=0.000800, test/loss=6.924471, test/num_examples=10000, total_duration=264.428506, train/accuracy=0.001036, train/loss=6.923890, validation/accuracy=0.000720, validation/loss=6.923479, validation/num_examples=50000
I0815 05:41:07.666675 139906658768704 submission_runner.py:396] Released all unoccupied cached memory.
I0815 05:41:07.812242 140362618492736 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 05:41:07.812253 140696797321024 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 05:41:07.812434 140218002184000 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 05:41:07.812434 139906658768704 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 05:41:07.814153 139633595471680 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 05:41:07.814176 139775998064448 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 05:41:07.814177 140703107626816 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 05:41:07.814814 140243764496192 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I0815 05:41:08.292003 139861088519936 logging_writer.py:48] [1] global_step=1, grad_norm=0.615017, loss=6.925622
I0815 05:41:08.295652 139906658768704 submission.py:120] 1) loss = 6.926, grad_norm = 0.615
I0815 05:41:08.644372 139861096912640 logging_writer.py:48] [2] global_step=2, grad_norm=0.608199, loss=6.929768
I0815 05:41:08.647920 139906658768704 submission.py:120] 2) loss = 6.930, grad_norm = 0.608
I0815 05:41:08.995171 139861088519936 logging_writer.py:48] [3] global_step=3, grad_norm=0.607616, loss=6.921391
I0815 05:41:08.999064 139906658768704 submission.py:120] 3) loss = 6.921, grad_norm = 0.608
I0815 05:41:09.348178 139861096912640 logging_writer.py:48] [4] global_step=4, grad_norm=0.598939, loss=6.926468
I0815 05:41:09.352834 139906658768704 submission.py:120] 4) loss = 6.926, grad_norm = 0.599
I0815 05:41:09.705733 139861088519936 logging_writer.py:48] [5] global_step=5, grad_norm=0.600572, loss=6.929698
I0815 05:41:09.709923 139906658768704 submission.py:120] 5) loss = 6.930, grad_norm = 0.601
I0815 05:41:10.062884 139861096912640 logging_writer.py:48] [6] global_step=6, grad_norm=0.616177, loss=6.932221
I0815 05:41:10.067648 139906658768704 submission.py:120] 6) loss = 6.932, grad_norm = 0.616
I0815 05:41:10.420093 139861088519936 logging_writer.py:48] [7] global_step=7, grad_norm=0.615265, loss=6.925324
I0815 05:41:10.423923 139906658768704 submission.py:120] 7) loss = 6.925, grad_norm = 0.615
I0815 05:41:10.778402 139861096912640 logging_writer.py:48] [8] global_step=8, grad_norm=0.608603, loss=6.930192
I0815 05:41:10.782670 139906658768704 submission.py:120] 8) loss = 6.930, grad_norm = 0.609
I0815 05:41:11.134841 139861088519936 logging_writer.py:48] [9] global_step=9, grad_norm=0.630009, loss=6.930361
I0815 05:41:11.139143 139906658768704 submission.py:120] 9) loss = 6.930, grad_norm = 0.630
I0815 05:41:11.496058 139861096912640 logging_writer.py:48] [10] global_step=10, grad_norm=0.611915, loss=6.924419
I0815 05:41:11.500040 139906658768704 submission.py:120] 10) loss = 6.924, grad_norm = 0.612
I0815 05:44:05.375687 139861088519936 logging_writer.py:48] [500] global_step=500, grad_norm=1.151088, loss=6.291400
I0815 05:44:05.380500 139906658768704 submission.py:120] 500) loss = 6.291, grad_norm = 1.151
I0815 05:47:03.431891 139861096912640 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.527202, loss=5.669049
I0815 05:47:03.437803 139906658768704 submission.py:120] 1000) loss = 5.669, grad_norm = 2.527
I0815 05:49:37.207439 139906658768704 spec.py:320] Evaluating on the training split.
I0815 05:50:19.230874 139906658768704 spec.py:332] Evaluating on the validation split.
I0815 05:51:15.090118 139906658768704 spec.py:348] Evaluating on the test split.
I0815 05:51:16.228879 139906658768704 submission_runner.py:365] Time since start: 873.52s, 	Step: 1433, 	{'train/accuracy': 0.11702806122448979, 'train/loss': 4.760440437161193, 'validation/accuracy': 0.10652, 'validation/loss': 4.840901875, 'validation/num_examples': 50000, 'test/accuracy': 0.0717, 'test/loss': 5.31030625, 'test/num_examples': 10000, 'score': 568.2091913223267, 'total_duration': 873.5249683856964, 'accumulated_submission_time': 568.2091913223267, 'accumulated_eval_time': 303.8244659900665, 'accumulated_logging_time': 0.6585209369659424}
I0815 05:51:16.246804 139861105305344 logging_writer.py:48] [1433] accumulated_eval_time=303.824466, accumulated_logging_time=0.658521, accumulated_submission_time=568.209191, global_step=1433, preemption_count=0, score=568.209191, test/accuracy=0.071700, test/loss=5.310306, test/num_examples=10000, total_duration=873.524968, train/accuracy=0.117028, train/loss=4.760440, validation/accuracy=0.106520, validation/loss=4.840902, validation/num_examples=50000
I0815 05:51:16.974793 139906658768704 submission_runner.py:396] Released all unoccupied cached memory.
I0815 05:51:40.667627 139861113698048 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.102478, loss=5.279198
I0815 05:51:40.671633 139906658768704 submission.py:120] 1500) loss = 5.279, grad_norm = 3.102
I0815 05:54:35.098315 139861105305344 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.070729, loss=4.897603
I0815 05:54:35.103110 139906658768704 submission.py:120] 2000) loss = 4.898, grad_norm = 3.071
I0815 05:57:30.044369 139861113698048 logging_writer.py:48] [2500] global_step=2500, grad_norm=5.268814, loss=4.637296
I0815 05:57:30.050135 139906658768704 submission.py:120] 2500) loss = 4.637, grad_norm = 5.269
I0815 05:59:46.325807 139906658768704 spec.py:320] Evaluating on the training split.
I0815 06:00:31.989104 139906658768704 spec.py:332] Evaluating on the validation split.
I0815 06:01:26.777703 139906658768704 spec.py:348] Evaluating on the test split.
I0815 06:01:27.882186 139906658768704 submission_runner.py:365] Time since start: 1485.18s, 	Step: 2887, 	{'train/accuracy': 0.25803172831632654, 'train/loss': 3.629748286033163, 'validation/accuracy': 0.23864, 'validation/loss': 3.7822225, 'validation/num_examples': 50000, 'test/accuracy': 0.1732, 'test/loss': 4.360331640625, 'test/num_examples': 10000, 'score': 1076.7658472061157, 'total_duration': 1485.178335428238, 'accumulated_submission_time': 1076.7658472061157, 'accumulated_eval_time': 405.3807797431946, 'accumulated_logging_time': 1.4053971767425537}
I0815 06:01:27.900249 139861105305344 logging_writer.py:48] [2887] accumulated_eval_time=405.380780, accumulated_logging_time=1.405397, accumulated_submission_time=1076.765847, global_step=2887, preemption_count=0, score=1076.765847, test/accuracy=0.173200, test/loss=4.360332, test/num_examples=10000, total_duration=1485.178335, train/accuracy=0.258032, train/loss=3.629748, validation/accuracy=0.238640, validation/loss=3.782223, validation/num_examples=50000
I0815 06:01:28.585578 139906658768704 submission_runner.py:396] Released all unoccupied cached memory.
I0815 06:02:08.534425 139861113698048 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.705240, loss=4.279253
I0815 06:02:08.538721 139906658768704 submission.py:120] 3000) loss = 4.279, grad_norm = 3.705
I0815 06:05:03.073725 139861105305344 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.488726, loss=3.971842
I0815 06:05:03.086021 139906658768704 submission.py:120] 3500) loss = 3.972, grad_norm = 2.489
I0815 06:07:58.927395 139861113698048 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.327739, loss=4.003692
I0815 06:07:58.932557 139906658768704 submission.py:120] 4000) loss = 4.004, grad_norm = 2.328
I0815 06:09:57.974225 139906658768704 spec.py:320] Evaluating on the training split.
I0815 06:10:40.640932 139906658768704 spec.py:332] Evaluating on the validation split.
I0815 06:11:35.877924 139906658768704 spec.py:348] Evaluating on the test split.
I0815 06:11:36.980363 139906658768704 submission_runner.py:365] Time since start: 2094.28s, 	Step: 4342, 	{'train/accuracy': 0.35684390943877553, 'train/loss': 3.0651463099888394, 'validation/accuracy': 0.3275, 'validation/loss': 3.239185, 'validation/num_examples': 50000, 'test/accuracy': 0.2365, 'test/loss': 3.94176640625, 'test/num_examples': 10000, 'score': 1585.3320059776306, 'total_duration': 2094.2765073776245, 'accumulated_submission_time': 1585.3320059776306, 'accumulated_eval_time': 504.386958360672, 'accumulated_logging_time': 2.1421310901641846}
I0815 06:11:36.997209 139861105305344 logging_writer.py:48] [4342] accumulated_eval_time=504.386958, accumulated_logging_time=2.142131, accumulated_submission_time=1585.332006, global_step=4342, preemption_count=0, score=1585.332006, test/accuracy=0.236500, test/loss=3.941766, test/num_examples=10000, total_duration=2094.276507, train/accuracy=0.356844, train/loss=3.065146, validation/accuracy=0.327500, validation/loss=3.239185, validation/num_examples=50000
I0815 06:11:37.717966 139906658768704 submission_runner.py:396] Released all unoccupied cached memory.
I0815 06:12:33.157960 139861113698048 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.116685, loss=3.613050
I0815 06:12:33.162650 139906658768704 submission.py:120] 4500) loss = 3.613, grad_norm = 2.117
I0815 06:15:27.783154 139861105305344 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.292907, loss=3.652032
I0815 06:15:27.789893 139906658768704 submission.py:120] 5000) loss = 3.652, grad_norm = 2.293
I0815 06:18:23.746238 139861113698048 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.619594, loss=3.591691
I0815 06:18:23.752015 139906658768704 submission.py:120] 5500) loss = 3.592, grad_norm = 1.620
I0815 06:20:07.045481 139906658768704 spec.py:320] Evaluating on the training split.
I0815 06:20:49.206591 139906658768704 spec.py:332] Evaluating on the validation split.
I0815 06:21:44.286984 139906658768704 spec.py:348] Evaluating on the test split.
I0815 06:21:45.393534 139906658768704 submission_runner.py:365] Time since start: 2702.69s, 	Step: 5797, 	{'train/accuracy': 0.4709622130102041, 'train/loss': 2.4202267393773917, 'validation/accuracy': 0.43018, 'validation/loss': 2.6310653125, 'validation/num_examples': 50000, 'test/accuracy': 0.3315, 'test/loss': 3.2589771484375, 'test/num_examples': 10000, 'score': 2093.855437517166, 'total_duration': 2702.689613342285, 'accumulated_submission_time': 2093.855437517166, 'accumulated_eval_time': 602.735013961792, 'accumulated_logging_time': 2.8898773193359375}
I0815 06:21:45.413548 139861105305344 logging_writer.py:48] [5797] accumulated_eval_time=602.735014, accumulated_logging_time=2.889877, accumulated_submission_time=2093.855438, global_step=5797, preemption_count=0, score=2093.855438, test/accuracy=0.331500, test/loss=3.258977, test/num_examples=10000, total_duration=2702.689613, train/accuracy=0.470962, train/loss=2.420227, validation/accuracy=0.430180, validation/loss=2.631065, validation/num_examples=50000
I0815 06:21:46.022886 139906658768704 submission_runner.py:396] Released all unoccupied cached memory.
I0815 06:22:57.349620 139861113698048 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.477041, loss=3.399321
I0815 06:22:57.356061 139906658768704 submission.py:120] 6000) loss = 3.399, grad_norm = 1.477
I0815 06:25:53.305194 139861105305344 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.626608, loss=3.301970
I0815 06:25:53.312357 139906658768704 submission.py:120] 6500) loss = 3.302, grad_norm = 1.627
I0815 06:28:47.760001 139861113698048 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.811418, loss=3.205670
I0815 06:28:47.765525 139906658768704 submission.py:120] 7000) loss = 3.206, grad_norm = 1.811
I0815 06:30:15.430565 139906658768704 spec.py:320] Evaluating on the training split.
I0815 06:30:58.075355 139906658768704 spec.py:332] Evaluating on the validation split.
I0815 06:31:53.156676 139906658768704 spec.py:348] Evaluating on the test split.
I0815 06:31:54.268029 139906658768704 submission_runner.py:365] Time since start: 3311.56s, 	Step: 7252, 	{'train/accuracy': 0.5130141900510204, 'train/loss': 2.1996547154017856, 'validation/accuracy': 0.4708, 'validation/loss': 2.42622734375, 'validation/num_examples': 50000, 'test/accuracy': 0.3543, 'test/loss': 3.1545171875, 'test/num_examples': 10000, 'score': 2602.3726341724396, 'total_duration': 3311.5611684322357, 'accumulated_submission_time': 2602.3726341724396, 'accumulated_eval_time': 701.5694217681885, 'accumulated_logging_time': 3.6121115684509277}
I0815 06:31:54.286693 139861105305344 logging_writer.py:48] [7252] accumulated_eval_time=701.569422, accumulated_logging_time=3.612112, accumulated_submission_time=2602.372634, global_step=7252, preemption_count=0, score=2602.372634, test/accuracy=0.354300, test/loss=3.154517, test/num_examples=10000, total_duration=3311.561168, train/accuracy=0.513014, train/loss=2.199655, validation/accuracy=0.470800, validation/loss=2.426227, validation/num_examples=50000
I0815 06:31:54.992469 139906658768704 submission_runner.py:396] Released all unoccupied cached memory.
I0815 06:33:21.809176 139861113698048 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.127884, loss=3.149210
I0815 06:33:21.812910 139906658768704 submission.py:120] 7500) loss = 3.149, grad_norm = 1.128
I0815 06:36:17.880327 139861105305344 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.313071, loss=3.139082
I0815 06:36:17.887384 139906658768704 submission.py:120] 8000) loss = 3.139, grad_norm = 1.313
I0815 06:39:12.325819 139861113698048 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.329583, loss=3.096214
I0815 06:39:12.334506 139906658768704 submission.py:120] 8500) loss = 3.096, grad_norm = 1.330
I0815 06:40:24.304438 139906658768704 spec.py:320] Evaluating on the training split.
I0815 06:41:06.597888 139906658768704 spec.py:332] Evaluating on the validation split.
I0815 06:42:02.313179 139906658768704 spec.py:348] Evaluating on the test split.
I0815 06:42:03.415651 139906658768704 submission_runner.py:365] Time since start: 3920.71s, 	Step: 8708, 	{'train/accuracy': 0.584602200255102, 'train/loss': 1.8481457768654337, 'validation/accuracy': 0.53264, 'validation/loss': 2.09283734375, 'validation/num_examples': 50000, 'test/accuracy': 0.4028, 'test/loss': 2.81761015625, 'test/num_examples': 10000, 'score': 3110.891104698181, 'total_duration': 3920.711780309677, 'accumulated_submission_time': 3110.891104698181, 'accumulated_eval_time': 800.6806352138519, 'accumulated_logging_time': 4.336621284484863}
I0815 06:42:03.434478 139861105305344 logging_writer.py:48] [8708] accumulated_eval_time=800.680635, accumulated_logging_time=4.336621, accumulated_submission_time=3110.891105, global_step=8708, preemption_count=0, score=3110.891105, test/accuracy=0.402800, test/loss=2.817610, test/num_examples=10000, total_duration=3920.711780, train/accuracy=0.584602, train/loss=1.848146, validation/accuracy=0.532640, validation/loss=2.092837, validation/num_examples=50000
I0815 06:42:04.166580 139906658768704 submission_runner.py:396] Released all unoccupied cached memory.
I0815 06:43:47.795081 139861113698048 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.025704, loss=3.019069
I0815 06:43:47.800547 139906658768704 submission.py:120] 9000) loss = 3.019, grad_norm = 1.026
I0815 06:46:42.107764 139861105305344 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.859892, loss=3.066520
I0815 06:46:42.112523 139906658768704 submission.py:120] 9500) loss = 3.067, grad_norm = 0.860
I0815 06:49:36.492521 139861113698048 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.000561, loss=2.874481
I0815 06:49:36.497249 139906658768704 submission.py:120] 10000) loss = 2.874, grad_norm = 1.001
I0815 06:50:33.718713 139906658768704 spec.py:320] Evaluating on the training split.
I0815 06:51:15.803154 139906658768704 spec.py:332] Evaluating on the validation split.
I0815 06:52:10.170929 139906658768704 spec.py:348] Evaluating on the test split.
I0815 06:52:11.272438 139906658768704 submission_runner.py:365] Time since start: 4528.57s, 	Step: 10161, 	{'train/accuracy': 0.6175661670918368, 'train/loss': 1.6822930160833864, 'validation/accuracy': 0.55672, 'validation/loss': 1.9768340625, 'validation/num_examples': 50000, 'test/accuracy': 0.4318, 'test/loss': 2.68425078125, 'test/num_examples': 10000, 'score': 3619.6550493240356, 'total_duration': 4528.568603038788, 'accumulated_submission_time': 3619.6550493240356, 'accumulated_eval_time': 898.2344036102295, 'accumulated_logging_time': 5.087632179260254}
I0815 06:52:11.287774 139861105305344 logging_writer.py:48] [10161] accumulated_eval_time=898.234404, accumulated_logging_time=5.087632, accumulated_submission_time=3619.655049, global_step=10161, preemption_count=0, score=3619.655049, test/accuracy=0.431800, test/loss=2.684251, test/num_examples=10000, total_duration=4528.568603, train/accuracy=0.617566, train/loss=1.682293, validation/accuracy=0.556720, validation/loss=1.976834, validation/num_examples=50000
I0815 06:52:11.931700 139906658768704 submission_runner.py:396] Released all unoccupied cached memory.
I0815 06:54:10.681912 139861113698048 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.703252, loss=2.869080
I0815 06:54:10.691019 139906658768704 submission.py:120] 10500) loss = 2.869, grad_norm = 0.703
I0815 06:57:05.057485 139861105305344 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.953587, loss=2.837643
I0815 06:57:05.062408 139906658768704 submission.py:120] 11000) loss = 2.838, grad_norm = 0.954
I0815 07:00:00.902702 139861113698048 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.629040, loss=2.713558
I0815 07:00:00.907704 139906658768704 submission.py:120] 11500) loss = 2.714, grad_norm = 0.629
I0815 07:00:41.408311 139906658768704 spec.py:320] Evaluating on the training split.
I0815 07:01:23.963221 139906658768704 spec.py:332] Evaluating on the validation split.
I0815 07:02:20.273091 139906658768704 spec.py:348] Evaluating on the test split.
I0815 07:02:21.370306 139906658768704 submission_runner.py:365] Time since start: 5138.67s, 	Step: 11617, 	{'train/accuracy': 0.6392099808673469, 'train/loss': 1.6219029329261, 'validation/accuracy': 0.57482, 'validation/loss': 1.916249375, 'validation/num_examples': 50000, 'test/accuracy': 0.4468, 'test/loss': 2.6512546875, 'test/num_examples': 10000, 'score': 4128.258343696594, 'total_duration': 5138.666431903839, 'accumulated_submission_time': 4128.258343696594, 'accumulated_eval_time': 998.1963493824005, 'accumulated_logging_time': 5.825338840484619}
I0815 07:02:21.388768 139861105305344 logging_writer.py:48] [11617] accumulated_eval_time=998.196349, accumulated_logging_time=5.825339, accumulated_submission_time=4128.258344, global_step=11617, preemption_count=0, score=4128.258344, test/accuracy=0.446800, test/loss=2.651255, test/num_examples=10000, total_duration=5138.666432, train/accuracy=0.639210, train/loss=1.621903, validation/accuracy=0.574820, validation/loss=1.916249, validation/num_examples=50000
I0815 07:02:22.079971 139906658768704 submission_runner.py:396] Released all unoccupied cached memory.
I0815 07:04:36.057420 139861113698048 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.834802, loss=2.689394
I0815 07:04:36.061570 139906658768704 submission.py:120] 12000) loss = 2.689, grad_norm = 0.835
I0815 07:07:30.642532 139861105305344 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.626815, loss=2.833181
I0815 07:07:30.648849 139906658768704 submission.py:120] 12500) loss = 2.833, grad_norm = 0.627
I0815 07:10:26.561419 139861113698048 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.647705, loss=2.654067
I0815 07:10:26.566742 139906658768704 submission.py:120] 13000) loss = 2.654, grad_norm = 0.648
I0815 07:10:51.634919 139906658768704 spec.py:320] Evaluating on the training split.
I0815 07:11:34.457801 139906658768704 spec.py:332] Evaluating on the validation split.
I0815 07:12:31.345210 139906658768704 spec.py:348] Evaluating on the test split.
I0815 07:12:32.447210 139906658768704 submission_runner.py:365] Time since start: 5749.74s, 	Step: 13073, 	{'train/accuracy': 0.6741071428571429, 'train/loss': 1.453349833585778, 'validation/accuracy': 0.60278, 'validation/loss': 1.78497, 'validation/num_examples': 50000, 'test/accuracy': 0.4747, 'test/loss': 2.492763671875, 'test/num_examples': 10000, 'score': 4637.010428905487, 'total_duration': 5749.743341684341, 'accumulated_submission_time': 4637.010428905487, 'accumulated_eval_time': 1099.0086431503296, 'accumulated_logging_time': 6.5350728034973145}
I0815 07:12:32.467092 139861105305344 logging_writer.py:48] [13073] accumulated_eval_time=1099.008643, accumulated_logging_time=6.535073, accumulated_submission_time=4637.010429, global_step=13073, preemption_count=0, score=4637.010429, test/accuracy=0.474700, test/loss=2.492764, test/num_examples=10000, total_duration=5749.743342, train/accuracy=0.674107, train/loss=1.453350, validation/accuracy=0.602780, validation/loss=1.784970, validation/num_examples=50000
I0815 07:12:33.126702 139906658768704 submission_runner.py:396] Released all unoccupied cached memory.
I0815 07:15:02.352546 139861113698048 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.623540, loss=2.689126
I0815 07:15:02.358423 139906658768704 submission.py:120] 13500) loss = 2.689, grad_norm = 0.624
I0815 07:18:00.144065 139906658768704 spec.py:320] Evaluating on the training split.
I0815 07:18:43.969691 139906658768704 spec.py:332] Evaluating on the validation split.
I0815 07:19:39.863306 139906658768704 spec.py:348] Evaluating on the test split.
I0815 07:19:40.968753 139906658768704 submission_runner.py:365] Time since start: 6178.26s, 	Step: 14000, 	{'train/accuracy': 0.6609534438775511, 'train/loss': 1.5135843705157845, 'validation/accuracy': 0.59022, 'validation/loss': 1.8579275, 'validation/num_examples': 50000, 'test/accuracy': 0.4574, 'test/loss': 2.55180703125, 'test/num_examples': 10000, 'score': 4963.51860833168, 'total_duration': 6178.263594865799, 'accumulated_submission_time': 4963.51860833168, 'accumulated_eval_time': 1199.8319733142853, 'accumulated_logging_time': 7.214643239974976}
I0815 07:19:40.987194 139861105305344 logging_writer.py:48] [14000] accumulated_eval_time=1199.831973, accumulated_logging_time=7.214643, accumulated_submission_time=4963.518608, global_step=14000, preemption_count=0, score=4963.518608, test/accuracy=0.457400, test/loss=2.551807, test/num_examples=10000, total_duration=6178.263595, train/accuracy=0.660953, train/loss=1.513584, validation/accuracy=0.590220, validation/loss=1.857927, validation/num_examples=50000
I0815 07:19:41.702323 139906658768704 submission_runner.py:396] Released all unoccupied cached memory.
I0815 07:19:41.711104 139861113698048 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4963.518608
I0815 07:19:42.470164 139906658768704 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0815 07:19:42.820144 139906658768704 submission_runner.py:534] Tuning trial 1/1
I0815 07:19:42.820371 139906658768704 submission_runner.py:535] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0815 07:19:42.821329 139906658768704 submission_runner.py:536] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010363520408163266, 'train/loss': 6.923890405771684, 'validation/accuracy': 0.00072, 'validation/loss': 6.923479375, 'validation/num_examples': 50000, 'test/accuracy': 0.0008, 'test/loss': 6.92447109375, 'test/num_examples': 10000, 'score': 59.62487268447876, 'total_duration': 264.42850613594055, 'accumulated_submission_time': 59.62487268447876, 'accumulated_eval_time': 204.80307984352112, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1433, {'train/accuracy': 0.11702806122448979, 'train/loss': 4.760440437161193, 'validation/accuracy': 0.10652, 'validation/loss': 4.840901875, 'validation/num_examples': 50000, 'test/accuracy': 0.0717, 'test/loss': 5.31030625, 'test/num_examples': 10000, 'score': 568.2091913223267, 'total_duration': 873.5249683856964, 'accumulated_submission_time': 568.2091913223267, 'accumulated_eval_time': 303.8244659900665, 'accumulated_logging_time': 0.6585209369659424, 'global_step': 1433, 'preemption_count': 0}), (2887, {'train/accuracy': 0.25803172831632654, 'train/loss': 3.629748286033163, 'validation/accuracy': 0.23864, 'validation/loss': 3.7822225, 'validation/num_examples': 50000, 'test/accuracy': 0.1732, 'test/loss': 4.360331640625, 'test/num_examples': 10000, 'score': 1076.7658472061157, 'total_duration': 1485.178335428238, 'accumulated_submission_time': 1076.7658472061157, 'accumulated_eval_time': 405.3807797431946, 'accumulated_logging_time': 1.4053971767425537, 'global_step': 2887, 'preemption_count': 0}), (4342, {'train/accuracy': 0.35684390943877553, 'train/loss': 3.0651463099888394, 'validation/accuracy': 0.3275, 'validation/loss': 3.239185, 'validation/num_examples': 50000, 'test/accuracy': 0.2365, 'test/loss': 3.94176640625, 'test/num_examples': 10000, 'score': 1585.3320059776306, 'total_duration': 2094.2765073776245, 'accumulated_submission_time': 1585.3320059776306, 'accumulated_eval_time': 504.386958360672, 'accumulated_logging_time': 2.1421310901641846, 'global_step': 4342, 'preemption_count': 0}), (5797, {'train/accuracy': 0.4709622130102041, 'train/loss': 2.4202267393773917, 'validation/accuracy': 0.43018, 'validation/loss': 2.6310653125, 'validation/num_examples': 50000, 'test/accuracy': 0.3315, 'test/loss': 3.2589771484375, 'test/num_examples': 10000, 'score': 2093.855437517166, 'total_duration': 2702.689613342285, 'accumulated_submission_time': 2093.855437517166, 'accumulated_eval_time': 602.735013961792, 'accumulated_logging_time': 2.8898773193359375, 'global_step': 5797, 'preemption_count': 0}), (7252, {'train/accuracy': 0.5130141900510204, 'train/loss': 2.1996547154017856, 'validation/accuracy': 0.4708, 'validation/loss': 2.42622734375, 'validation/num_examples': 50000, 'test/accuracy': 0.3543, 'test/loss': 3.1545171875, 'test/num_examples': 10000, 'score': 2602.3726341724396, 'total_duration': 3311.5611684322357, 'accumulated_submission_time': 2602.3726341724396, 'accumulated_eval_time': 701.5694217681885, 'accumulated_logging_time': 3.6121115684509277, 'global_step': 7252, 'preemption_count': 0}), (8708, {'train/accuracy': 0.584602200255102, 'train/loss': 1.8481457768654337, 'validation/accuracy': 0.53264, 'validation/loss': 2.09283734375, 'validation/num_examples': 50000, 'test/accuracy': 0.4028, 'test/loss': 2.81761015625, 'test/num_examples': 10000, 'score': 3110.891104698181, 'total_duration': 3920.711780309677, 'accumulated_submission_time': 3110.891104698181, 'accumulated_eval_time': 800.6806352138519, 'accumulated_logging_time': 4.336621284484863, 'global_step': 8708, 'preemption_count': 0}), (10161, {'train/accuracy': 0.6175661670918368, 'train/loss': 1.6822930160833864, 'validation/accuracy': 0.55672, 'validation/loss': 1.9768340625, 'validation/num_examples': 50000, 'test/accuracy': 0.4318, 'test/loss': 2.68425078125, 'test/num_examples': 10000, 'score': 3619.6550493240356, 'total_duration': 4528.568603038788, 'accumulated_submission_time': 3619.6550493240356, 'accumulated_eval_time': 898.2344036102295, 'accumulated_logging_time': 5.087632179260254, 'global_step': 10161, 'preemption_count': 0}), (11617, {'train/accuracy': 0.6392099808673469, 'train/loss': 1.6219029329261, 'validation/accuracy': 0.57482, 'validation/loss': 1.916249375, 'validation/num_examples': 50000, 'test/accuracy': 0.4468, 'test/loss': 2.6512546875, 'test/num_examples': 10000, 'score': 4128.258343696594, 'total_duration': 5138.666431903839, 'accumulated_submission_time': 4128.258343696594, 'accumulated_eval_time': 998.1963493824005, 'accumulated_logging_time': 5.825338840484619, 'global_step': 11617, 'preemption_count': 0}), (13073, {'train/accuracy': 0.6741071428571429, 'train/loss': 1.453349833585778, 'validation/accuracy': 0.60278, 'validation/loss': 1.78497, 'validation/num_examples': 50000, 'test/accuracy': 0.4747, 'test/loss': 2.492763671875, 'test/num_examples': 10000, 'score': 4637.010428905487, 'total_duration': 5749.743341684341, 'accumulated_submission_time': 4637.010428905487, 'accumulated_eval_time': 1099.0086431503296, 'accumulated_logging_time': 6.5350728034973145, 'global_step': 13073, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6609534438775511, 'train/loss': 1.5135843705157845, 'validation/accuracy': 0.59022, 'validation/loss': 1.8579275, 'validation/num_examples': 50000, 'test/accuracy': 0.4574, 'test/loss': 2.55180703125, 'test/num_examples': 10000, 'score': 4963.51860833168, 'total_duration': 6178.263594865799, 'accumulated_submission_time': 4963.51860833168, 'accumulated_eval_time': 1199.8319733142853, 'accumulated_logging_time': 7.214643239974976, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0815 07:19:42.821473 139906658768704 submission_runner.py:537] Timing: 4963.51860833168
I0815 07:19:42.821531 139906658768704 submission_runner.py:539] Total number of evals: 11
I0815 07:19:42.821584 139906658768704 submission_runner.py:540] ====================
I0815 07:19:42.821707 139906658768704 submission_runner.py:608] Final imagenet_resnet score: 4963.51860833168
