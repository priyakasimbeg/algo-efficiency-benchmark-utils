torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_after_pytorch_fixes/adamw --overwrite=True --save_checkpoints=False --max_global_steps=14000 --imagenet_v2_data_dir=/data/imagenet/pytorch --torch_compile=true 2>&1 | tee -a /logs/imagenet_vit_pytorch_08-15-2023-07-46-35.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-15 07:46:45.035569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 07:46:45.035569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 07:46:45.035571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 07:46:45.035571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 07:46:45.035569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 07:46:45.035569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 07:46:45.035571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 07:46:45.035577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0815 07:46:59.437651 140098709780288 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0815 07:46:59.437684 139913281382208 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0815 07:46:59.438565 140595458836288 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0815 07:46:59.438596 140243293222720 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0815 07:46:59.438703 139831505385280 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0815 07:46:59.439672 140592589956928 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0815 07:47:00.429898 140130834343744 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0815 07:47:00.438827 140636275791680 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0815 07:47:00.439112 140636275791680 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 07:47:00.440600 140130834343744 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 07:47:00.448062 140098709780288 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 07:47:00.448085 140243293222720 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 07:47:00.448103 139913281382208 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 07:47:00.448127 140595458836288 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 07:47:00.448147 140592589956928 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 07:47:00.448214 139831505385280 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0815 07:47:01.725712 140636275791680 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_vit_pytorch.
W0815 07:47:01.768157 140098709780288 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 07:47:01.768722 140595458836288 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 07:47:01.769120 139913281382208 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 07:47:01.769441 140636275791680 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 07:47:01.769476 140243293222720 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 07:47:01.769623 140592589956928 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 07:47:01.770116 140130834343744 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 07:47:01.770214 139831505385280 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0815 07:47:01.774960 140636275791680 submission_runner.py:494] Using RNG seed 3940028532
I0815 07:47:01.776989 140636275791680 submission_runner.py:503] --- Tuning run 1/1 ---
I0815 07:47:01.777151 140636275791680 submission_runner.py:508] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_vit_pytorch/trial_1.
I0815 07:47:01.777530 140636275791680 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0815 07:47:01.778503 140636275791680 submission_runner.py:177] Initializing dataset.
I0815 07:47:08.295987 140636275791680 submission_runner.py:184] Initializing model.
I0815 07:47:12.956972 140636275791680 submission_runner.py:215] Performing `torch.compile`.
I0815 07:47:13.519919 140636275791680 submission_runner.py:218] Initializing optimizer.
I0815 07:47:13.521584 140636275791680 submission_runner.py:225] Initializing metrics bundle.
I0815 07:47:13.521714 140636275791680 submission_runner.py:243] Initializing checkpoint and logger.
I0815 07:47:14.049029 140636275791680 submission_runner.py:264] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0815 07:47:14.049908 140636275791680 submission_runner.py:267] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0815 07:47:14.140937 140636275791680 submission_runner.py:277] Starting training loop.
[2023-08-15 07:47:16,582] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:47:16,594] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:47:16,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:47:16,694] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:47:16,716] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:47:16,780] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:47:16,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:47:16,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:47:18,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:47:18,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:47:19,018] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:47:19,023] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:47:19,024] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:47:19,044] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:47:19,047] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:47:19,048] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:47:19,049] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:47:19,100] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:47:19,105] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:47:19,105] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:47:19,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:47:19,135] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:47:19,169] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:47:19,173] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:47:19,175] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:47:19,178] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:47:19,189] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:47:19,193] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:47:19,199] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:47:19,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:47:19,228] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:47:19,233] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:47:19,233] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:47:19,256] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:47:19,280] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:47:19,284] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:47:19,285] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:47:19,347] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:47:19,351] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:47:19,358] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:47:24,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 07:47:24,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 07:47:24,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 07:47:24,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 07:47:24,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 07:47:24,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 07:47:24,936] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 07:47:24,954] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-15 07:47:31,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 07:47:31,094] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 07:47:31,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 07:47:31,210] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 07:47:31,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 07:47:31,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 07:47:31,288] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 07:47:31,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-15 07:47:34,763] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 07:47:34,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 07:47:34,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 07:47:34,864] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 07:47:34,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 07:47:34,916] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 07:47:34,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 07:47:34,925] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-15 07:47:41,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 07:47:41,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 07:47:41,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 07:47:41,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 07:47:41,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 07:47:41,812] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 07:47:41,850] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 07:47:42,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-15 07:47:45,058] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 07:47:45,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 07:47:45,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 07:47:45,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 07:47:45,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 07:47:45,153] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 07:47:45,317] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 07:47:45,383] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-15 07:47:46,277] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 07:47:46,295] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 07:47:46,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 07:47:46,325] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 07:47:46,348] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 07:47:46,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 07:47:46,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 07:47:46,656] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-15 07:47:49,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 07:47:49,558] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 07:47:49,577] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 07:47:49,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 07:47:49,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 07:47:49,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 07:47:49,785] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 07:47:50,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-15 07:47:53,201] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 07:47:53,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 07:47:53,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 07:47:53,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 07:47:53,293] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 07:47:53,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 07:47:53,434] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 07:47:53,436] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 07:47:53,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 07:47:53,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 07:47:53,447] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:47:53,449] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:47:53,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 07:47:53,512] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 07:47:53,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 07:47:53,521] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 07:47:53,521] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 07:47:53,525] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:47:53,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 07:47:53,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 07:47:53,533] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:47:53,534] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:47:53,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 07:47:53,563] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 07:47:53,570] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:47:53,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 07:47:53,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 07:47:53,711] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:47:53,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-15 07:47:53,962] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-15 07:47:53,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-15 07:47:53,974] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:48:02,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 07:48:02,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 07:48:02,580] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 07:48:02,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 07:48:02,601] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 07:48:02,606] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 07:48:02,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 07:48:02,618] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-15 07:48:02,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 07:48:02,982] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 07:48:03,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 07:48:03,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 07:48:03,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 07:48:03,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 07:48:03,096] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 07:48:03,098] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 07:48:03,107] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 07:48:03,110] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-15 07:48:03,118] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 07:48:03,121] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 07:48:03,123] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 07:48:03,131] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 07:48:03,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 07:48:03,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-15 07:48:07,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 07:48:08,001] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 07:48:08,019] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 07:48:08,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 07:48:08,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 07:48:08,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 07:48:08,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 07:48:08,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-15 07:48:09,884] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 07:48:09,958] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 07:48:09,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 07:48:10,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 07:48:10,034] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 07:48:10,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 07:48:10,103] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 07:48:10,160] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-15 07:48:15,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 07:48:15,110] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 07:48:15,178] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 07:48:15,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 07:48:15,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 07:48:15,283] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 07:48:15,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 07:48:15,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-15 07:48:16,217] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 07:48:16,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 07:48:16,389] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 07:48:16,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 07:48:16,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 07:48:16,494] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 07:48:16,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 07:48:16,610] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-15 07:48:18,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 07:48:18,521] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 07:48:18,594] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 07:48:18,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 07:48:18,691] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 07:48:18,706] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 07:48:18,744] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 07:48:18,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 07:48:18,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 07:48:18,768] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 07:48:18,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 07:48:18,795] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 07:48:18,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 07:48:18,810] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 07:48:18,882] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-15 07:48:18,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-15 07:48:19,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 07:48:19,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 07:48:19,261] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 07:48:19,308] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 07:48:19,333] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 07:48:19,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 07:48:19,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-15 07:48:19,462] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I0815 07:48:19.991591 140607884318464 logging_writer.py:48] [0] global_step=0, grad_norm=0.338907, loss=6.907756
I0815 07:48:20.018559 140636275791680 submission.py:120] 0) loss = 6.908, grad_norm = 0.339
I0815 07:48:20.019868 140636275791680 spec.py:320] Evaluating on the training split.
[2023-08-15 07:48:31,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:48:31,886] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:48:31,948] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:48:31,976] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:48:32,000] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:48:32,029] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:48:32,710] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:48:33,425] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:48:33,516] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:48:33,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:48:33,558] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:48:33,562] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:48:33,562] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:48:33,575] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:48:33,579] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:48:33,579] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:48:33,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:48:33,617] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:48:33,630] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:48:33,634] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:48:33,634] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:48:33,648] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:48:33,659] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:48:33,663] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:48:33,664] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:48:33,691] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:48:33,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:48:33,695] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:48:33,695] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:48:33,734] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:48:33,738] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:48:33,739] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:48:34,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 07:48:34,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 07:48:34,129] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 07:48:34,153] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 07:48:34,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 07:48:34,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 07:48:34,425] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:48:34,471] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:48:34,475] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:48:34,476] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:48:35,014] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 07:48:35,195] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:48:35,242] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:48:35,247] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:48:35,247] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:48:35,786] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-15 07:48:36,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 07:48:36,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 07:48:36,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 07:48:36,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 07:48:36,676] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 07:48:36,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 07:48:37,366] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 07:48:38,001] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 07:48:38,019] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 07:48:38,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-15 07:48:38,057] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 07:48:38,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 07:48:38,175] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 07:48:38,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 07:48:38,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 07:48:39,811] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-15 07:48:41,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 07:48:41,610] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 07:48:41,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 07:48:41,718] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 07:48:41,868] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 07:48:41,882] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 07:48:42,466] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 07:48:43,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-15 07:48:43,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 07:48:43,310] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 07:48:43,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 07:48:43,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 07:48:43,547] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 07:48:43,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 07:48:44,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 07:48:44,187] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 07:48:44,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 07:48:44,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 07:48:44,384] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 07:48:44,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 07:48:44,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 07:48:44,825] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-15 07:48:45,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 07:48:45,565] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 07:48:45,719] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 07:48:45,741] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 07:48:45,774] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 07:48:45,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-15 07:48:45,957] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 07:48:46,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 07:48:46,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 07:48:47,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-15 07:48:49,324] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 07:48:49,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 07:48:49,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 07:48:49,506] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 07:48:49,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 07:48:49,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 07:48:49,536] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:48:49,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 07:48:49,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 07:48:49,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 07:48:49,599] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 07:48:49,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 07:48:49,605] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:48:49,605] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:48:49,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 07:48:49,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 07:48:49,715] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:48:49,768] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 07:48:49,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 07:48:49,778] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:48:49,956] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 07:48:50,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 07:48:50,174] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 07:48:50,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 07:48:50,185] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:48:50,346] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 07:48:50,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 07:48:50,356] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:48:51,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-15 07:48:51,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-15 07:48:51,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-15 07:48:51,369] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0815 07:49:42.219732 140636275791680 spec.py:332] Evaluating on the validation split.
[2023-08-15 07:50:33,167] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:50:33,390] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:50:33,867] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:50:33,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:50:34,035] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:50:34,809] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:50:35,028] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:50:35,070] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:50:35,074] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:50:35,074] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:50:35,249] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:50:35,292] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:50:35,297] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:50:35,297] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:50:35,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:50:35,756] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:50:35,782] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:50:35,786] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:50:35,786] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:50:35,798] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:50:35,802] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:50:35,803] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:50:35,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 07:50:35,900] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:50:35,942] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:50:35,945] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:50:35,946] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:50:36,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 07:50:36,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 07:50:36,287] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 07:50:36,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 07:50:36,679] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:50:36,726] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:50:36,730] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:50:36,730] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:50:37,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:50:37,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 07:50:37,666] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:50:38,397] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 07:50:38,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 07:50:39,162] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:50:39,208] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:50:39,210] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 07:50:39,212] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:50:39,213] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:50:39,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 07:50:39,320] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 07:50:39,644] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:50:39,688] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:50:39,692] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:50:39,693] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:50:39,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 07:50:39,917] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 07:50:40,126] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 07:50:40,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 07:50:40,212] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-15 07:50:40,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 07:50:40,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 07:50:40,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 07:50:41,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 07:50:42,884] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 07:50:43,273] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 07:50:43,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-15 07:50:43,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 07:50:44,098] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 07:50:44,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 07:50:44,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 07:50:44,565] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 07:50:45,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-15 07:50:45,096] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 07:50:45,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 07:50:45,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 07:50:45,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 07:50:45,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 07:50:45,991] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 07:50:46,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 07:50:46,416] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 07:50:46,851] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 07:50:46,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 07:50:47,034] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 07:50:47,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 07:50:47,498] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 07:50:47,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 07:50:47,895] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 07:50:47,917] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 07:50:48,137] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-15 07:50:48,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 07:50:48,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 07:50:48,519] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 07:50:49,154] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 07:50:49,366] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 07:50:49,811] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-15 07:50:50,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 07:50:50,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 07:50:50,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 07:50:50,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 07:50:50,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 07:50:50,578] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:50:50,777] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 07:50:50,782] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 07:50:50,787] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:50:50,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-15 07:50:51,120] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 07:50:51,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 07:50:51,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 07:50:51,333] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 07:50:51,338] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 07:50:51,343] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:50:51,382] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 07:50:51,387] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 07:50:51,392] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:50:51,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 07:50:51,407] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 07:50:51,412] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:50:51,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 07:50:52,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 07:50:52,223] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 07:50:52,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 07:50:52,233] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:50:52,277] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-15 07:50:54,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 07:50:54,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 07:50:54,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 07:50:54,554] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:50:54,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-15 07:50:55,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-15 07:50:55,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-15 07:50:55,092] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0815 07:50:58.149194 140636275791680 spec.py:348] Evaluating on the test split.
I0815 07:50:58.165808 140636275791680 dataset_info.py:578] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0815 07:50:58.171857 140636275791680 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0815 07:50:58.246428 140636275791680 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
[2023-08-15 07:51:01,536] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:01,738] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:01,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:01,846] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:01,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:02,015] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:02,146] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:02,185] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:07,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:07,145] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:07,149] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:07,149] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:07,204] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:07,211] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:07,211] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:07,243] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:07,247] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:07,248] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:07,250] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:07,250] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:07,254] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:07,254] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:07,254] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:07,255] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:07,257] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:07,296] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:07,298] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:07,300] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:07,300] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:07,314] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:07,326] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:07,337] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:07,341] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:07,342] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:07,354] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:07,358] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:07,358] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:07,366] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:07,369] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:07,370] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:07,631] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 07:51:07,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 07:51:07,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 07:51:07,739] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 07:51:07,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 07:51:07,820] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 07:51:07,850] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 07:51:07,885] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-15 07:51:08,399] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 07:51:08,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 07:51:08,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 07:51:08,587] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 07:51:08,618] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 07:51:08,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 07:51:08,693] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 07:51:08,734] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-15 07:51:09,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 07:51:09,991] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 07:51:10,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 07:51:10,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 07:51:10,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 07:51:10,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 07:51:10,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 07:51:10,277] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-15 07:51:11,198] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 07:51:11,341] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 07:51:11,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 07:51:11,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 07:51:11,441] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 07:51:11,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 07:51:11,522] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 07:51:11,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-15 07:51:12,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 07:51:12,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 07:51:12,985] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 07:51:13,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 07:51:13,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 07:51:13,118] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 07:51:13,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 07:51:13,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-15 07:51:14,281] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 07:51:14,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 07:51:14,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 07:51:14,576] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 07:51:14,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 07:51:14,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 07:51:14,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 07:51:14,836] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-15 07:51:15,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 07:51:15,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 07:51:15,911] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 07:51:15,954] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 07:51:15,959] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 07:51:16,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 07:51:16,152] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 07:51:16,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-15 07:51:16,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 07:51:16,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 07:51:16,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 07:51:16,710] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:16,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 07:51:16,786] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 07:51:16,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 07:51:16,847] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 07:51:16,957] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 07:51:16,963] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 07:51:16,966] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 07:51:16,970] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:17,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 07:51:17,039] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 07:51:17,046] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:17,052] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 07:51:17,059] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 07:51:17,064] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 07:51:17,070] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:17,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 07:51:17,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 07:51:17,085] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:17,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-15 07:51:17,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 07:51:17,275] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 07:51:17,284] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:17,339] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 07:51:17,346] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 07:51:17,357] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:17,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-15 07:51:17,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-15 07:51:17,820] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:20,951] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:21,525] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:21,540] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:21,552] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:21,588] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:21,755] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:21,849] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:21,955] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-15 07:51:22,916] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:22,958] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:22,962] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:22,962] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:23,201] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:23,216] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:23,237] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:23,246] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:23,249] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:23,250] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:23,258] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:23,262] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:23,262] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:23,279] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:23,283] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:23,283] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:23,293] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:23,334] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:23,338] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:23,339] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:23,406] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:23,444] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 07:51:23,447] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:23,451] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:23,451] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:23,504] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:23,546] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:23,550] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:23,550] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:23,600] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-15 07:51:23,641] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-15 07:51:23,645] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-15 07:51:23,646] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-15 07:51:23,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 07:51:23,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 07:51:23,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 07:51:23,832] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 07:51:23,943] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 07:51:24,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 07:51:24,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-15 07:51:24,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 07:51:24,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 07:51:24,661] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 07:51:24,763] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 07:51:24,868] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 07:51:24,962] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 07:51:25,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 07:51:25,128] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-15 07:51:25,917] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 07:51:26,146] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 07:51:26,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 07:51:26,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 07:51:26,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 07:51:26,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 07:51:26,462] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 07:51:26,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-15 07:51:27,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 07:51:27,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 07:51:27,616] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 07:51:27,763] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 07:51:27,851] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 07:51:27,997] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 07:51:28,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 07:51:28,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-15 07:51:29,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 07:51:29,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 07:51:29,286] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 07:51:29,408] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 07:51:29,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 07:51:29,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 07:51:29,690] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 07:51:29,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-15 07:51:30,104] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 07:51:30,235] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 07:51:30,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 07:51:30,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 07:51:30,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 07:51:30,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 07:51:30,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 07:51:30,751] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-15 07:51:31,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 07:51:31,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 07:51:31,684] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 07:51:31,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 07:51:31,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 07:51:31,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 07:51:32,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 07:51:32,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-15 07:51:34,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 07:51:34,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 07:51:34,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 07:51:34,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 07:51:34,455] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 07:51:34,460] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 07:51:34,465] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:34,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 07:51:34,494] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 07:51:34,500] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:34,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 07:51:34,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 07:51:34,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 07:51:34,623] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:34,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 07:51:34,655] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 07:51:34,659] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 07:51:34,660] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:34,712] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 07:51:34,780] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 07:51:34,785] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 07:51:34,790] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:34,849] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-15 07:51:34,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 07:51:34,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 07:51:34,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:34,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 07:51:34,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 07:51:34,918] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-15 07:51:35,048] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-15 07:51:35,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-15 07:51:35,058] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0815 07:51:35.838814 140636275791680 submission_runner.py:365] Time since start: 261.70s, 	Step: 1, 	{'train/accuracy': 0.0024609375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00204, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0022, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 65.87883496284485, 'total_duration': 261.6981134414673, 'accumulated_submission_time': 65.87883496284485, 'accumulated_eval_time': 195.81877970695496, 'accumulated_logging_time': 0}
I0815 07:51:35.862257 140602163328768 logging_writer.py:48] [1] accumulated_eval_time=195.818780, accumulated_logging_time=0, accumulated_submission_time=65.878835, global_step=1, preemption_count=0, score=65.878835, test/accuracy=0.002200, test/loss=6.907755, test/num_examples=10000, total_duration=261.698113, train/accuracy=0.002461, train/loss=6.907756, validation/accuracy=0.002040, validation/loss=6.907756, validation/num_examples=50000
I0815 07:51:36.460623 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 07:51:36.522531 140636275791680 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 07:51:36.522590 139831505385280 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 07:51:36.526297 140130834343744 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 07:51:36.526338 140243293222720 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 07:51:36.526325 140595458836288 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 07:51:36.526379 139913281382208 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 07:51:36.526322 140592589956928 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 07:51:36.526500 140098709780288 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 07:51:37.145453 140602154936064 logging_writer.py:48] [1] global_step=1, grad_norm=0.349781, loss=6.907756
I0815 07:51:37.149171 140636275791680 submission.py:120] 1) loss = 6.908, grad_norm = 0.350
I0815 07:51:37.572000 140602163328768 logging_writer.py:48] [2] global_step=2, grad_norm=0.356969, loss=6.907754
I0815 07:51:37.576746 140636275791680 submission.py:120] 2) loss = 6.908, grad_norm = 0.357
I0815 07:51:37.964746 140602154936064 logging_writer.py:48] [3] global_step=3, grad_norm=0.347450, loss=6.907751
I0815 07:51:37.969860 140636275791680 submission.py:120] 3) loss = 6.908, grad_norm = 0.347
I0815 07:51:38.358408 140602163328768 logging_writer.py:48] [4] global_step=4, grad_norm=0.347310, loss=6.907752
I0815 07:51:38.363970 140636275791680 submission.py:120] 4) loss = 6.908, grad_norm = 0.347
I0815 07:51:38.752689 140602154936064 logging_writer.py:48] [5] global_step=5, grad_norm=0.340742, loss=6.907753
I0815 07:51:38.757710 140636275791680 submission.py:120] 5) loss = 6.908, grad_norm = 0.341
I0815 07:51:39.151745 140602163328768 logging_writer.py:48] [6] global_step=6, grad_norm=0.352832, loss=6.907739
I0815 07:51:39.156745 140636275791680 submission.py:120] 6) loss = 6.908, grad_norm = 0.353
I0815 07:51:39.547029 140602154936064 logging_writer.py:48] [7] global_step=7, grad_norm=0.351913, loss=6.907750
I0815 07:51:39.552079 140636275791680 submission.py:120] 7) loss = 6.908, grad_norm = 0.352
I0815 07:51:39.941424 140602163328768 logging_writer.py:48] [8] global_step=8, grad_norm=0.353889, loss=6.907740
I0815 07:51:39.946056 140636275791680 submission.py:120] 8) loss = 6.908, grad_norm = 0.354
I0815 07:51:40.342228 140602154936064 logging_writer.py:48] [9] global_step=9, grad_norm=0.358845, loss=6.907739
I0815 07:51:40.346786 140636275791680 submission.py:120] 9) loss = 6.908, grad_norm = 0.359
I0815 07:51:40.736989 140602163328768 logging_writer.py:48] [10] global_step=10, grad_norm=0.351045, loss=6.907723
I0815 07:51:40.740856 140636275791680 submission.py:120] 10) loss = 6.908, grad_norm = 0.351
I0815 07:54:58.086951 140602154936064 logging_writer.py:48] [500] global_step=500, grad_norm=1.063330, loss=6.700047
I0815 07:54:58.092350 140636275791680 submission.py:120] 500) loss = 6.700, grad_norm = 1.063
I0815 07:58:20.640590 140602163328768 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.868071, loss=6.414046
I0815 07:58:20.646782 140636275791680 submission.py:120] 1000) loss = 6.414, grad_norm = 0.868
I0815 07:58:36.143959 140636275791680 spec.py:320] Evaluating on the training split.
I0815 07:59:20.904395 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 08:00:15.805220 140636275791680 spec.py:348] Evaluating on the test split.
I0815 08:00:17.225170 140636275791680 submission_runner.py:365] Time since start: 783.08s, 	Step: 1039, 	{'train/accuracy': 0.03712890625, 'train/loss': 5.928589477539062, 'validation/accuracy': 0.03518, 'validation/loss': 5.95427375, 'validation/num_examples': 50000, 'test/accuracy': 0.0269, 'test/loss': 6.068562109375, 'test/num_examples': 10000, 'score': 484.874960899353, 'total_duration': 783.0845971107483, 'accumulated_submission_time': 484.874960899353, 'accumulated_eval_time': 296.89992451667786, 'accumulated_logging_time': 0.6582820415496826}
I0815 08:00:17.245115 140592686774016 logging_writer.py:48] [1039] accumulated_eval_time=296.899925, accumulated_logging_time=0.658282, accumulated_submission_time=484.874961, global_step=1039, preemption_count=0, score=484.874961, test/accuracy=0.026900, test/loss=6.068562, test/num_examples=10000, total_duration=783.084597, train/accuracy=0.037129, train/loss=5.928589, validation/accuracy=0.035180, validation/loss=5.954274, validation/num_examples=50000
I0815 08:00:18.157795 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 08:03:23.416311 140593757918976 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.051915, loss=6.071206
I0815 08:03:23.421600 140636275791680 submission.py:120] 1500) loss = 6.071, grad_norm = 1.052
I0815 08:06:39.152541 140592686774016 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.898176, loss=6.003745
I0815 08:06:39.158622 140636275791680 submission.py:120] 2000) loss = 6.004, grad_norm = 0.898
I0815 08:07:17.226428 140636275791680 spec.py:320] Evaluating on the training split.
I0815 08:08:00.637006 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 08:08:44.726651 140636275791680 spec.py:348] Evaluating on the test split.
I0815 08:08:46.116224 140636275791680 submission_runner.py:365] Time since start: 1291.98s, 	Step: 2096, 	{'train/accuracy': 0.08048828125, 'train/loss': 5.286326904296875, 'validation/accuracy': 0.07576, 'validation/loss': 5.327604375, 'validation/num_examples': 50000, 'test/accuracy': 0.0583, 'test/loss': 5.508772265625, 'test/num_examples': 10000, 'score': 903.2686536312103, 'total_duration': 1291.9757416248322, 'accumulated_submission_time': 903.2686536312103, 'accumulated_eval_time': 385.78977394104004, 'accumulated_logging_time': 1.601470947265625}
I0815 08:08:46.136557 140593757918976 logging_writer.py:48] [2096] accumulated_eval_time=385.789774, accumulated_logging_time=1.601471, accumulated_submission_time=903.268654, global_step=2096, preemption_count=0, score=903.268654, test/accuracy=0.058300, test/loss=5.508772, test/num_examples=10000, total_duration=1291.975742, train/accuracy=0.080488, train/loss=5.286327, validation/accuracy=0.075760, validation/loss=5.327604, validation/num_examples=50000
I0815 08:08:47.053760 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 08:11:31.492076 140592686774016 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.985218, loss=6.011492
I0815 08:11:31.500501 140636275791680 submission.py:120] 2500) loss = 6.011, grad_norm = 0.985
I0815 08:14:49.237936 140593757918976 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.089102, loss=5.743256
I0815 08:14:49.243407 140636275791680 submission.py:120] 3000) loss = 5.743, grad_norm = 1.089
I0815 08:15:46.445544 140636275791680 spec.py:320] Evaluating on the training split.
I0815 08:16:32.676145 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 08:17:18.986548 140636275791680 spec.py:348] Evaluating on the test split.
I0815 08:17:20.370114 140636275791680 submission_runner.py:365] Time since start: 1806.23s, 	Step: 3147, 	{'train/accuracy': 0.14060546875, 'train/loss': 4.706075134277344, 'validation/accuracy': 0.1314, 'validation/loss': 4.77539875, 'validation/num_examples': 50000, 'test/accuracy': 0.0992, 'test/loss': 5.0563828125, 'test/num_examples': 10000, 'score': 1321.986191034317, 'total_duration': 1806.2295696735382, 'accumulated_submission_time': 1321.986191034317, 'accumulated_eval_time': 479.71433782577515, 'accumulated_logging_time': 2.539736747741699}
I0815 08:17:20.388713 140592686774016 logging_writer.py:48] [3147] accumulated_eval_time=479.714338, accumulated_logging_time=2.539737, accumulated_submission_time=1321.986191, global_step=3147, preemption_count=0, score=1321.986191, test/accuracy=0.099200, test/loss=5.056383, test/num_examples=10000, total_duration=1806.229570, train/accuracy=0.140605, train/loss=4.706075, validation/accuracy=0.131400, validation/loss=4.775399, validation/num_examples=50000
I0815 08:17:21.247061 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 08:19:40.694678 140593757918976 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.092431, loss=5.600218
I0815 08:19:40.699223 140636275791680 submission.py:120] 3500) loss = 5.600, grad_norm = 1.092
I0815 08:23:02.122483 140592686774016 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.838455, loss=5.466582
I0815 08:23:02.127998 140636275791680 submission.py:120] 4000) loss = 5.467, grad_norm = 0.838
I0815 08:24:20.498812 140636275791680 spec.py:320] Evaluating on the training split.
I0815 08:25:05.639468 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 08:25:52.079750 140636275791680 spec.py:348] Evaluating on the test split.
I0815 08:25:53.474487 140636275791680 submission_runner.py:365] Time since start: 2319.33s, 	Step: 4201, 	{'train/accuracy': 0.19578125, 'train/loss': 4.228016662597656, 'validation/accuracy': 0.18128, 'validation/loss': 4.32185, 'validation/num_examples': 50000, 'test/accuracy': 0.1415, 'test/loss': 4.68926015625, 'test/num_examples': 10000, 'score': 1740.538893699646, 'total_duration': 2319.33233499527, 'accumulated_submission_time': 1740.538893699646, 'accumulated_eval_time': 572.6885182857513, 'accumulated_logging_time': 3.4489879608154297}
I0815 08:25:53.490206 140593757918976 logging_writer.py:48] [4201] accumulated_eval_time=572.688518, accumulated_logging_time=3.448988, accumulated_submission_time=1740.538894, global_step=4201, preemption_count=0, score=1740.538894, test/accuracy=0.141500, test/loss=4.689260, test/num_examples=10000, total_duration=2319.332335, train/accuracy=0.195781, train/loss=4.228017, validation/accuracy=0.181280, validation/loss=4.321850, validation/num_examples=50000
I0815 08:25:54.302319 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 08:27:51.797122 140592686774016 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.000453, loss=5.327864
I0815 08:27:51.803413 140636275791680 submission.py:120] 4500) loss = 5.328, grad_norm = 1.000
I0815 08:31:13.303723 140593757918976 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.859241, loss=5.567078
I0815 08:31:13.310538 140636275791680 submission.py:120] 5000) loss = 5.567, grad_norm = 0.859
I0815 08:32:53.475937 140636275791680 spec.py:320] Evaluating on the training split.
I0815 08:33:38.468225 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 08:34:23.538619 140636275791680 spec.py:348] Evaluating on the test split.
I0815 08:34:24.927510 140636275791680 submission_runner.py:365] Time since start: 2830.79s, 	Step: 5251, 	{'train/accuracy': 0.24927734375, 'train/loss': 3.824973449707031, 'validation/accuracy': 0.22786, 'validation/loss': 3.9500740625, 'validation/num_examples': 50000, 'test/accuracy': 0.1768, 'test/loss': 4.36345859375, 'test/num_examples': 10000, 'score': 2158.993733882904, 'total_duration': 2830.7869794368744, 'accumulated_submission_time': 2158.993733882904, 'accumulated_eval_time': 664.1401402950287, 'accumulated_logging_time': 4.337715148925781}
I0815 08:34:24.944622 140592686774016 logging_writer.py:48] [5251] accumulated_eval_time=664.140140, accumulated_logging_time=4.337715, accumulated_submission_time=2158.993734, global_step=5251, preemption_count=0, score=2158.993734, test/accuracy=0.176800, test/loss=4.363459, test/num_examples=10000, total_duration=2830.786979, train/accuracy=0.249277, train/loss=3.824973, validation/accuracy=0.227860, validation/loss=3.950074, validation/num_examples=50000
I0815 08:34:25.774611 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 08:36:04.060413 140593757918976 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.831563, loss=5.136937
I0815 08:36:04.066117 140636275791680 submission.py:120] 5500) loss = 5.137, grad_norm = 0.832
I0815 08:39:21.823721 140592686774016 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.831359, loss=5.327182
I0815 08:39:21.831863 140636275791680 submission.py:120] 6000) loss = 5.327, grad_norm = 0.831
I0815 08:41:25.298920 140636275791680 spec.py:320] Evaluating on the training split.
I0815 08:42:11.512033 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 08:43:07.214941 140636275791680 spec.py:348] Evaluating on the test split.
I0815 08:43:08.604962 140636275791680 submission_runner.py:365] Time since start: 3354.46s, 	Step: 6302, 	{'train/accuracy': 0.30119140625, 'train/loss': 3.501376953125, 'validation/accuracy': 0.27452, 'validation/loss': 3.632914375, 'validation/num_examples': 50000, 'test/accuracy': 0.2145, 'test/loss': 4.10241796875, 'test/num_examples': 10000, 'score': 2577.8097491264343, 'total_duration': 3354.4632284641266, 'accumulated_submission_time': 2577.8097491264343, 'accumulated_eval_time': 767.4450540542603, 'accumulated_logging_time': 5.238548040390015}
I0815 08:43:08.626970 140593757918976 logging_writer.py:48] [6302] accumulated_eval_time=767.445054, accumulated_logging_time=5.238548, accumulated_submission_time=2577.809749, global_step=6302, preemption_count=0, score=2577.809749, test/accuracy=0.214500, test/loss=4.102418, test/num_examples=10000, total_duration=3354.463228, train/accuracy=0.301191, train/loss=3.501377, validation/accuracy=0.274520, validation/loss=3.632914, validation/num_examples=50000
I0815 08:43:09.508016 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 08:44:27.617850 140592686774016 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.922790, loss=4.914827
I0815 08:44:27.623330 140636275791680 submission.py:120] 6500) loss = 4.915, grad_norm = 0.923
I0815 08:47:43.805358 140593757918976 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.240314, loss=4.768529
I0815 08:47:43.817469 140636275791680 submission.py:120] 7000) loss = 4.769, grad_norm = 1.240
I0815 08:50:08.613497 140636275791680 spec.py:320] Evaluating on the training split.
I0815 08:50:53.365053 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 08:51:39.306716 140636275791680 spec.py:348] Evaluating on the test split.
I0815 08:51:40.695727 140636275791680 submission_runner.py:365] Time since start: 3866.56s, 	Step: 7362, 	{'train/accuracy': 0.34576171875, 'train/loss': 3.1753338623046874, 'validation/accuracy': 0.31694, 'validation/loss': 3.3378840625, 'validation/num_examples': 50000, 'test/accuracy': 0.2449, 'test/loss': 3.83987421875, 'test/num_examples': 10000, 'score': 2996.225449323654, 'total_duration': 3866.5552554130554, 'accumulated_submission_time': 2996.225449323654, 'accumulated_eval_time': 859.5273530483246, 'accumulated_logging_time': 6.156409740447998}
I0815 08:51:40.714212 140592686774016 logging_writer.py:48] [7362] accumulated_eval_time=859.527353, accumulated_logging_time=6.156410, accumulated_submission_time=2996.225449, global_step=7362, preemption_count=0, score=2996.225449, test/accuracy=0.244900, test/loss=3.839874, test/num_examples=10000, total_duration=3866.555255, train/accuracy=0.345762, train/loss=3.175334, validation/accuracy=0.316940, validation/loss=3.337884, validation/num_examples=50000
I0815 08:51:41.611696 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 08:52:38.269457 140593757918976 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.766768, loss=4.622694
I0815 08:52:38.273236 140636275791680 submission.py:120] 7500) loss = 4.623, grad_norm = 0.767
I0815 08:55:56.332054 140592686774016 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.709251, loss=4.788017
I0815 08:55:56.336745 140636275791680 submission.py:120] 8000) loss = 4.788, grad_norm = 0.709
I0815 08:58:41.132395 140636275791680 spec.py:320] Evaluating on the training split.
I0815 08:59:25.392267 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 09:00:10.410875 140636275791680 spec.py:348] Evaluating on the test split.
I0815 09:00:11.802480 140636275791680 submission_runner.py:365] Time since start: 4377.66s, 	Step: 8418, 	{'train/accuracy': 0.39158203125, 'train/loss': 2.896939392089844, 'validation/accuracy': 0.35518, 'validation/loss': 3.0746928125, 'validation/num_examples': 50000, 'test/accuracy': 0.2767, 'test/loss': 3.612513671875, 'test/num_examples': 10000, 'score': 3415.08177113533, 'total_duration': 4377.661998033524, 'accumulated_submission_time': 3415.08177113533, 'accumulated_eval_time': 950.1974909305573, 'accumulated_logging_time': 7.072473049163818}
I0815 09:00:11.818647 140593757918976 logging_writer.py:48] [8418] accumulated_eval_time=950.197491, accumulated_logging_time=7.072473, accumulated_submission_time=3415.081771, global_step=8418, preemption_count=0, score=3415.081771, test/accuracy=0.276700, test/loss=3.612514, test/num_examples=10000, total_duration=4377.661998, train/accuracy=0.391582, train/loss=2.896939, validation/accuracy=0.355180, validation/loss=3.074693, validation/num_examples=50000
I0815 09:00:12.707759 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 09:00:45.778457 140592686774016 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.668380, loss=4.537523
I0815 09:00:45.783542 140636275791680 submission.py:120] 8500) loss = 4.538, grad_norm = 0.668
I0815 09:04:07.298274 140593757918976 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.734971, loss=4.588482
I0815 09:04:07.303775 140636275791680 submission.py:120] 9000) loss = 4.588, grad_norm = 0.735
I0815 09:07:12.073171 140636275791680 spec.py:320] Evaluating on the training split.
I0815 09:07:58.649842 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 09:08:47.383399 140636275791680 spec.py:348] Evaluating on the test split.
I0815 09:08:48.778857 140636275791680 submission_runner.py:365] Time since start: 4894.64s, 	Step: 9473, 	{'train/accuracy': 0.4227734375, 'train/loss': 2.6896380615234374, 'validation/accuracy': 0.38916, 'validation/loss': 2.8713865625, 'validation/num_examples': 50000, 'test/accuracy': 0.2994, 'test/loss': 3.414501171875, 'test/num_examples': 10000, 'score': 3833.78404712677, 'total_duration': 4894.638339519501, 'accumulated_submission_time': 3833.78404712677, 'accumulated_eval_time': 1046.9032049179077, 'accumulated_logging_time': 7.977798938751221}
I0815 09:08:48.794450 140592686774016 logging_writer.py:48] [9473] accumulated_eval_time=1046.903205, accumulated_logging_time=7.977799, accumulated_submission_time=3833.784047, global_step=9473, preemption_count=0, score=3833.784047, test/accuracy=0.299400, test/loss=3.414501, test/num_examples=10000, total_duration=4894.638340, train/accuracy=0.422773, train/loss=2.689638, validation/accuracy=0.389160, validation/loss=2.871387, validation/num_examples=50000
I0815 09:08:49.592846 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 09:09:00.849473 140593757918976 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.695628, loss=4.740664
I0815 09:09:00.854011 140636275791680 submission.py:120] 9500) loss = 4.741, grad_norm = 0.696
I0815 09:12:23.876450 140592686774016 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.604760, loss=4.904716
I0815 09:12:23.881200 140636275791680 submission.py:120] 10000) loss = 4.905, grad_norm = 0.605
I0815 09:15:41.568271 140593757918976 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.707173, loss=4.260231
I0815 09:15:41.573908 140636275791680 submission.py:120] 10500) loss = 4.260, grad_norm = 0.707
I0815 09:15:48.996188 140636275791680 spec.py:320] Evaluating on the training split.
I0815 09:16:33.778666 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 09:17:19.383052 140636275791680 spec.py:348] Evaluating on the test split.
I0815 09:17:20.774353 140636275791680 submission_runner.py:365] Time since start: 5406.63s, 	Step: 10520, 	{'train/accuracy': 0.45802734375, 'train/loss': 2.50091064453125, 'validation/accuracy': 0.41866, 'validation/loss': 2.706436875, 'validation/num_examples': 50000, 'test/accuracy': 0.3217, 'test/loss': 3.298245703125, 'test/num_examples': 10000, 'score': 4252.46196269989, 'total_duration': 5406.631858348846, 'accumulated_submission_time': 4252.46196269989, 'accumulated_eval_time': 1138.6794474124908, 'accumulated_logging_time': 8.865484237670898}
I0815 09:17:20.794985 140592686774016 logging_writer.py:48] [10520] accumulated_eval_time=1138.679447, accumulated_logging_time=8.865484, accumulated_submission_time=4252.461963, global_step=10520, preemption_count=0, score=4252.461963, test/accuracy=0.321700, test/loss=3.298246, test/num_examples=10000, total_duration=5406.631858, train/accuracy=0.458027, train/loss=2.500911, validation/accuracy=0.418660, validation/loss=2.706437, validation/num_examples=50000
I0815 09:17:21.625209 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 09:20:31.768023 140593757918976 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.683715, loss=4.448586
I0815 09:20:31.773798 140636275791680 submission.py:120] 11000) loss = 4.449, grad_norm = 0.684
I0815 09:23:53.538753 140592686774016 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.573618, loss=4.327237
I0815 09:23:53.545086 140636275791680 submission.py:120] 11500) loss = 4.327, grad_norm = 0.574
I0815 09:24:21.031982 140636275791680 spec.py:320] Evaluating on the training split.
I0815 09:25:06.603165 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 09:25:52.099014 140636275791680 spec.py:348] Evaluating on the test split.
I0815 09:25:53.485935 140636275791680 submission_runner.py:365] Time since start: 5919.35s, 	Step: 11571, 	{'train/accuracy': 0.4831640625, 'train/loss': 2.4547071838378907, 'validation/accuracy': 0.44482, 'validation/loss': 2.6558096875, 'validation/num_examples': 50000, 'test/accuracy': 0.3444, 'test/loss': 3.217311328125, 'test/num_examples': 10000, 'score': 4671.146176576614, 'total_duration': 5919.345408916473, 'accumulated_submission_time': 4671.146176576614, 'accumulated_eval_time': 1231.133488893509, 'accumulated_logging_time': 9.777480125427246}
I0815 09:25:53.501939 140593757918976 logging_writer.py:48] [11571] accumulated_eval_time=1231.133489, accumulated_logging_time=9.777480, accumulated_submission_time=4671.146177, global_step=11571, preemption_count=0, score=4671.146177, test/accuracy=0.344400, test/loss=3.217311, test/num_examples=10000, total_duration=5919.345409, train/accuracy=0.483164, train/loss=2.454707, validation/accuracy=0.444820, validation/loss=2.655810, validation/num_examples=50000
I0815 09:25:54.416674 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 09:28:43.396299 140592686774016 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.598457, loss=4.792739
I0815 09:28:43.400743 140636275791680 submission.py:120] 12000) loss = 4.793, grad_norm = 0.598
I0815 09:32:04.405167 140593757918976 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.768326, loss=3.967466
I0815 09:32:04.411862 140636275791680 submission.py:120] 12500) loss = 3.967, grad_norm = 0.768
I0815 09:32:53.549747 140636275791680 spec.py:320] Evaluating on the training split.
I0815 09:33:41.136849 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 09:34:26.426183 140636275791680 spec.py:348] Evaluating on the test split.
I0815 09:34:27.815116 140636275791680 submission_runner.py:365] Time since start: 6433.67s, 	Step: 12621, 	{'train/accuracy': 0.51087890625, 'train/loss': 2.240866851806641, 'validation/accuracy': 0.47032, 'validation/loss': 2.45736890625, 'validation/num_examples': 50000, 'test/accuracy': 0.362, 'test/loss': 3.043877734375, 'test/num_examples': 10000, 'score': 5089.59478020668, 'total_duration': 6433.673010110855, 'accumulated_submission_time': 5089.59478020668, 'accumulated_eval_time': 1325.397387266159, 'accumulated_logging_time': 10.73172926902771}
I0815 09:34:27.832503 140592686774016 logging_writer.py:48] [12621] accumulated_eval_time=1325.397387, accumulated_logging_time=10.731729, accumulated_submission_time=5089.594780, global_step=12621, preemption_count=0, score=5089.594780, test/accuracy=0.362000, test/loss=3.043878, test/num_examples=10000, total_duration=6433.673010, train/accuracy=0.510879, train/loss=2.240867, validation/accuracy=0.470320, validation/loss=2.457369, validation/num_examples=50000
I0815 09:34:28.745581 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 09:36:57.485182 140593757918976 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.715869, loss=4.187398
I0815 09:36:57.497491 140636275791680 submission.py:120] 13000) loss = 4.187, grad_norm = 0.716
I0815 09:40:14.719468 140592686774016 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.722104, loss=3.991617
I0815 09:40:14.724165 140636275791680 submission.py:120] 13500) loss = 3.992, grad_norm = 0.722
I0815 09:41:28.122796 140636275791680 spec.py:320] Evaluating on the training split.
I0815 09:42:14.590970 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 09:43:10.931857 140636275791680 spec.py:348] Evaluating on the test split.
I0815 09:43:12.316167 140636275791680 submission_runner.py:365] Time since start: 6958.18s, 	Step: 13682, 	{'train/accuracy': 0.53982421875, 'train/loss': 2.0937864685058596, 'validation/accuracy': 0.49478, 'validation/loss': 2.31137578125, 'validation/num_examples': 50000, 'test/accuracy': 0.3862, 'test/loss': 2.9090505859375, 'test/num_examples': 10000, 'score': 5508.28750705719, 'total_duration': 6958.1756772994995, 'accumulated_submission_time': 5508.28750705719, 'accumulated_eval_time': 1429.5909461975098, 'accumulated_logging_time': 11.67259955406189}
I0815 09:43:12.333047 140593757918976 logging_writer.py:48] [13682] accumulated_eval_time=1429.590946, accumulated_logging_time=11.672600, accumulated_submission_time=5508.287507, global_step=13682, preemption_count=0, score=5508.287507, test/accuracy=0.386200, test/loss=2.909051, test/num_examples=10000, total_duration=6958.175677, train/accuracy=0.539824, train/loss=2.093786, validation/accuracy=0.494780, validation/loss=2.311376, validation/num_examples=50000
I0815 09:43:13.159964 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 09:45:20.638231 140636275791680 spec.py:320] Evaluating on the training split.
I0815 09:46:05.086051 140636275791680 spec.py:332] Evaluating on the validation split.
I0815 09:46:50.115134 140636275791680 spec.py:348] Evaluating on the test split.
I0815 09:46:51.502368 140636275791680 submission_runner.py:365] Time since start: 7177.36s, 	Step: 14000, 	{'train/accuracy': 0.55107421875, 'train/loss': 2.0246437072753904, 'validation/accuracy': 0.50154, 'validation/loss': 2.2505028125, 'validation/num_examples': 50000, 'test/accuracy': 0.3878, 'test/loss': 2.8891, 'test/num_examples': 10000, 'score': 5635.490258932114, 'total_duration': 7177.36173415184, 'accumulated_submission_time': 5635.490258932114, 'accumulated_eval_time': 1520.4550046920776, 'accumulated_logging_time': 12.59447717666626}
I0815 09:46:51.517717 140592686774016 logging_writer.py:48] [14000] accumulated_eval_time=1520.455005, accumulated_logging_time=12.594477, accumulated_submission_time=5635.490259, global_step=14000, preemption_count=0, score=5635.490259, test/accuracy=0.387800, test/loss=2.889100, test/num_examples=10000, total_duration=7177.361734, train/accuracy=0.551074, train/loss=2.024644, validation/accuracy=0.501540, validation/loss=2.250503, validation/num_examples=50000
I0815 09:46:52.427544 140636275791680 submission_runner.py:396] Released all unoccupied cached memory.
I0815 09:46:52.486834 140593757918976 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5635.490259
I0815 09:46:53.167248 140636275791680 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0815 09:46:53.537392 140636275791680 submission_runner.py:534] Tuning trial 1/1
I0815 09:46:53.537621 140636275791680 submission_runner.py:535] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0815 09:46:53.538734 140636275791680 submission_runner.py:536] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0024609375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00204, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0022, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 65.87883496284485, 'total_duration': 261.6981134414673, 'accumulated_submission_time': 65.87883496284485, 'accumulated_eval_time': 195.81877970695496, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1039, {'train/accuracy': 0.03712890625, 'train/loss': 5.928589477539062, 'validation/accuracy': 0.03518, 'validation/loss': 5.95427375, 'validation/num_examples': 50000, 'test/accuracy': 0.0269, 'test/loss': 6.068562109375, 'test/num_examples': 10000, 'score': 484.874960899353, 'total_duration': 783.0845971107483, 'accumulated_submission_time': 484.874960899353, 'accumulated_eval_time': 296.89992451667786, 'accumulated_logging_time': 0.6582820415496826, 'global_step': 1039, 'preemption_count': 0}), (2096, {'train/accuracy': 0.08048828125, 'train/loss': 5.286326904296875, 'validation/accuracy': 0.07576, 'validation/loss': 5.327604375, 'validation/num_examples': 50000, 'test/accuracy': 0.0583, 'test/loss': 5.508772265625, 'test/num_examples': 10000, 'score': 903.2686536312103, 'total_duration': 1291.9757416248322, 'accumulated_submission_time': 903.2686536312103, 'accumulated_eval_time': 385.78977394104004, 'accumulated_logging_time': 1.601470947265625, 'global_step': 2096, 'preemption_count': 0}), (3147, {'train/accuracy': 0.14060546875, 'train/loss': 4.706075134277344, 'validation/accuracy': 0.1314, 'validation/loss': 4.77539875, 'validation/num_examples': 50000, 'test/accuracy': 0.0992, 'test/loss': 5.0563828125, 'test/num_examples': 10000, 'score': 1321.986191034317, 'total_duration': 1806.2295696735382, 'accumulated_submission_time': 1321.986191034317, 'accumulated_eval_time': 479.71433782577515, 'accumulated_logging_time': 2.539736747741699, 'global_step': 3147, 'preemption_count': 0}), (4201, {'train/accuracy': 0.19578125, 'train/loss': 4.228016662597656, 'validation/accuracy': 0.18128, 'validation/loss': 4.32185, 'validation/num_examples': 50000, 'test/accuracy': 0.1415, 'test/loss': 4.68926015625, 'test/num_examples': 10000, 'score': 1740.538893699646, 'total_duration': 2319.33233499527, 'accumulated_submission_time': 1740.538893699646, 'accumulated_eval_time': 572.6885182857513, 'accumulated_logging_time': 3.4489879608154297, 'global_step': 4201, 'preemption_count': 0}), (5251, {'train/accuracy': 0.24927734375, 'train/loss': 3.824973449707031, 'validation/accuracy': 0.22786, 'validation/loss': 3.9500740625, 'validation/num_examples': 50000, 'test/accuracy': 0.1768, 'test/loss': 4.36345859375, 'test/num_examples': 10000, 'score': 2158.993733882904, 'total_duration': 2830.7869794368744, 'accumulated_submission_time': 2158.993733882904, 'accumulated_eval_time': 664.1401402950287, 'accumulated_logging_time': 4.337715148925781, 'global_step': 5251, 'preemption_count': 0}), (6302, {'train/accuracy': 0.30119140625, 'train/loss': 3.501376953125, 'validation/accuracy': 0.27452, 'validation/loss': 3.632914375, 'validation/num_examples': 50000, 'test/accuracy': 0.2145, 'test/loss': 4.10241796875, 'test/num_examples': 10000, 'score': 2577.8097491264343, 'total_duration': 3354.4632284641266, 'accumulated_submission_time': 2577.8097491264343, 'accumulated_eval_time': 767.4450540542603, 'accumulated_logging_time': 5.238548040390015, 'global_step': 6302, 'preemption_count': 0}), (7362, {'train/accuracy': 0.34576171875, 'train/loss': 3.1753338623046874, 'validation/accuracy': 0.31694, 'validation/loss': 3.3378840625, 'validation/num_examples': 50000, 'test/accuracy': 0.2449, 'test/loss': 3.83987421875, 'test/num_examples': 10000, 'score': 2996.225449323654, 'total_duration': 3866.5552554130554, 'accumulated_submission_time': 2996.225449323654, 'accumulated_eval_time': 859.5273530483246, 'accumulated_logging_time': 6.156409740447998, 'global_step': 7362, 'preemption_count': 0}), (8418, {'train/accuracy': 0.39158203125, 'train/loss': 2.896939392089844, 'validation/accuracy': 0.35518, 'validation/loss': 3.0746928125, 'validation/num_examples': 50000, 'test/accuracy': 0.2767, 'test/loss': 3.612513671875, 'test/num_examples': 10000, 'score': 3415.08177113533, 'total_duration': 4377.661998033524, 'accumulated_submission_time': 3415.08177113533, 'accumulated_eval_time': 950.1974909305573, 'accumulated_logging_time': 7.072473049163818, 'global_step': 8418, 'preemption_count': 0}), (9473, {'train/accuracy': 0.4227734375, 'train/loss': 2.6896380615234374, 'validation/accuracy': 0.38916, 'validation/loss': 2.8713865625, 'validation/num_examples': 50000, 'test/accuracy': 0.2994, 'test/loss': 3.414501171875, 'test/num_examples': 10000, 'score': 3833.78404712677, 'total_duration': 4894.638339519501, 'accumulated_submission_time': 3833.78404712677, 'accumulated_eval_time': 1046.9032049179077, 'accumulated_logging_time': 7.977798938751221, 'global_step': 9473, 'preemption_count': 0}), (10520, {'train/accuracy': 0.45802734375, 'train/loss': 2.50091064453125, 'validation/accuracy': 0.41866, 'validation/loss': 2.706436875, 'validation/num_examples': 50000, 'test/accuracy': 0.3217, 'test/loss': 3.298245703125, 'test/num_examples': 10000, 'score': 4252.46196269989, 'total_duration': 5406.631858348846, 'accumulated_submission_time': 4252.46196269989, 'accumulated_eval_time': 1138.6794474124908, 'accumulated_logging_time': 8.865484237670898, 'global_step': 10520, 'preemption_count': 0}), (11571, {'train/accuracy': 0.4831640625, 'train/loss': 2.4547071838378907, 'validation/accuracy': 0.44482, 'validation/loss': 2.6558096875, 'validation/num_examples': 50000, 'test/accuracy': 0.3444, 'test/loss': 3.217311328125, 'test/num_examples': 10000, 'score': 4671.146176576614, 'total_duration': 5919.345408916473, 'accumulated_submission_time': 4671.146176576614, 'accumulated_eval_time': 1231.133488893509, 'accumulated_logging_time': 9.777480125427246, 'global_step': 11571, 'preemption_count': 0}), (12621, {'train/accuracy': 0.51087890625, 'train/loss': 2.240866851806641, 'validation/accuracy': 0.47032, 'validation/loss': 2.45736890625, 'validation/num_examples': 50000, 'test/accuracy': 0.362, 'test/loss': 3.043877734375, 'test/num_examples': 10000, 'score': 5089.59478020668, 'total_duration': 6433.673010110855, 'accumulated_submission_time': 5089.59478020668, 'accumulated_eval_time': 1325.397387266159, 'accumulated_logging_time': 10.73172926902771, 'global_step': 12621, 'preemption_count': 0}), (13682, {'train/accuracy': 0.53982421875, 'train/loss': 2.0937864685058596, 'validation/accuracy': 0.49478, 'validation/loss': 2.31137578125, 'validation/num_examples': 50000, 'test/accuracy': 0.3862, 'test/loss': 2.9090505859375, 'test/num_examples': 10000, 'score': 5508.28750705719, 'total_duration': 6958.1756772994995, 'accumulated_submission_time': 5508.28750705719, 'accumulated_eval_time': 1429.5909461975098, 'accumulated_logging_time': 11.67259955406189, 'global_step': 13682, 'preemption_count': 0}), (14000, {'train/accuracy': 0.55107421875, 'train/loss': 2.0246437072753904, 'validation/accuracy': 0.50154, 'validation/loss': 2.2505028125, 'validation/num_examples': 50000, 'test/accuracy': 0.3878, 'test/loss': 2.8891, 'test/num_examples': 10000, 'score': 5635.490258932114, 'total_duration': 7177.36173415184, 'accumulated_submission_time': 5635.490258932114, 'accumulated_eval_time': 1520.4550046920776, 'accumulated_logging_time': 12.59447717666626, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0815 09:46:53.538864 140636275791680 submission_runner.py:537] Timing: 5635.490258932114
I0815 09:46:53.538918 140636275791680 submission_runner.py:539] Total number of evals: 15
I0815 09:46:53.538969 140636275791680 submission_runner.py:540] ====================
I0815 09:46:53.539087 140636275791680 submission_runner.py:608] Final imagenet_vit score: 5635.490258932114
