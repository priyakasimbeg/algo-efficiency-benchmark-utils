torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_after_pytorch_fixes/adamw --overwrite=True --save_checkpoints=False --max_global_steps=6000 --torch_compile=true 2>&1 | tee -a /logs/ogbg_pytorch_08-15-2023-10-10-00.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-15 10:10:11.117159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 10:10:11.117158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 10:10:11.117160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 10:10:11.117158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 10:10:11.117159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 10:10:11.117159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 10:10:11.117159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-15 10:10:11.117159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0815 10:10:25.353576 140495190193984 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0815 10:10:25.353612 140426740987712 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0815 10:10:25.353631 140529127065408 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0815 10:10:25.354569 139942117099328 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0815 10:10:25.354753 140296757626688 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0815 10:10:25.354790 140286502799168 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0815 10:10:25.355085 139806462515008 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0815 10:10:25.365152 140716550170432 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0815 10:10:25.365451 140716550170432 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 10:10:25.365437 140286502799168 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 10:10:25.365454 140296757626688 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 10:10:25.365602 139806462515008 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 10:10:25.374573 140495190193984 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 10:10:25.374608 140426740987712 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 10:10:25.374653 140529127065408 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 10:10:25.375470 139942117099328 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0815 10:10:26.973042 140716550170432 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/ogbg_pytorch.
W0815 10:10:27.014003 139942117099328 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 10:10:27.014005 140529127065408 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 10:10:27.015330 140286502799168 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 10:10:27.015354 140296757626688 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 10:10:27.015680 139806462515008 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 10:10:27.015776 140495190193984 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 10:10:27.016173 140426740987712 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0815 10:10:27.016720 140716550170432 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0815 10:10:27.021801 140716550170432 submission_runner.py:494] Using RNG seed 3023025215
I0815 10:10:27.023442 140716550170432 submission_runner.py:503] --- Tuning run 1/1 ---
I0815 10:10:27.023592 140716550170432 submission_runner.py:508] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/ogbg_pytorch/trial_1.
I0815 10:10:27.024311 140716550170432 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/ogbg_pytorch/trial_1/hparams.json.
I0815 10:10:27.025305 140716550170432 submission_runner.py:177] Initializing dataset.
I0815 10:10:27.025533 140716550170432 submission_runner.py:184] Initializing model.
W0815 10:10:31.293796 140286502799168 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0815 10:10:31.293796 140495190193984 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0815 10:10:31.293817 140426740987712 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0815 10:10:31.293826 139942117099328 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0815 10:10:31.293824 139806462515008 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0815 10:10:31.293844 140529127065408 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0815 10:10:31.293937 140296757626688 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0815 10:10:31.294286 140716550170432 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0815 10:10:31.294590 140716550170432 submission_runner.py:218] Initializing optimizer.
I0815 10:10:31.295589 140716550170432 submission_runner.py:225] Initializing metrics bundle.
I0815 10:10:31.295727 140716550170432 submission_runner.py:243] Initializing checkpoint and logger.
I0815 10:10:31.296632 140716550170432 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0815 10:10:31.296745 140716550170432 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0815 10:10:31.755796 140716550170432 submission_runner.py:264] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0815 10:10:31.756807 140716550170432 submission_runner.py:267] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/ogbg_pytorch/trial_1/flags_0.json.
I0815 10:10:31.843451 140716550170432 submission_runner.py:277] Starting training loop.
I0815 10:10:32.396487 140716550170432 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0815 10:10:32.402062 140716550170432 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0815 10:10:32.737558 140716550170432 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0815 10:10:32.795518 140716550170432 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0815 10:10:37.381307 140678197671680 logging_writer.py:48] [0] global_step=0, grad_norm=3.470207, loss=0.738710
I0815 10:10:37.393724 140716550170432 submission.py:120] 0) loss = 0.739, grad_norm = 3.470
I0815 10:10:37.396631 140716550170432 spec.py:320] Evaluating on the training split.
I0815 10:10:37.401415 140716550170432 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0815 10:10:37.405414 140716550170432 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0815 10:10:37.467124 140716550170432 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0815 10:11:32.376946 140716550170432 spec.py:332] Evaluating on the validation split.
I0815 10:11:32.380057 140716550170432 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0815 10:11:32.385221 140716550170432 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0815 10:11:32.447750 140716550170432 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0815 10:12:15.505337 140716550170432 spec.py:348] Evaluating on the test split.
I0815 10:12:15.508823 140716550170432 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0815 10:12:15.512770 140716550170432 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0815 10:12:15.575368 140716550170432 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0815 10:13:00.364164 140716550170432 submission_runner.py:365] Time since start: 148.52s, 	Step: 1, 	{'train/accuracy': 0.5292460965126498, 'train/loss': 0.7383571329336538, 'train/mean_average_precision': 0.02007834088388548, 'validation/accuracy': 0.52937346300703, 'validation/loss': 0.7380277432051616, 'validation/mean_average_precision': 0.02458107616068408, 'validation/num_examples': 43793, 'test/accuracy': 0.5318092544053805, 'test/loss': 0.7373300953289931, 'test/mean_average_precision': 0.026619820273342486, 'test/num_examples': 43793, 'score': 5.553447961807251, 'total_duration': 148.52094650268555, 'accumulated_submission_time': 5.553447961807251, 'accumulated_eval_time': 142.96720123291016, 'accumulated_logging_time': 0}
I0815 10:13:00.384767 140663852685056 logging_writer.py:48] [1] accumulated_eval_time=142.967201, accumulated_logging_time=0, accumulated_submission_time=5.553448, global_step=1, preemption_count=0, score=5.553448, test/accuracy=0.531809, test/loss=0.737330, test/mean_average_precision=0.026620, test/num_examples=43793, total_duration=148.520947, train/accuracy=0.529246, train/loss=0.738357, train/mean_average_precision=0.020078, validation/accuracy=0.529373, validation/loss=0.738028, validation/mean_average_precision=0.024581, validation/num_examples=43793
I0815 10:13:00.760210 140716550170432 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:13:01.069903 140716550170432 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 10:13:01.075084 140426740987712 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 10:13:01.075335 139942117099328 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 10:13:01.075700 140529127065408 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 10:13:01.075721 140296757626688 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 10:13:01.076274 140286502799168 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 10:13:01.076324 140495190193984 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 10:13:01.076366 139806462515008 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0815 10:13:01.153377 140663861077760 logging_writer.py:48] [1] global_step=1, grad_norm=3.412498, loss=0.737864
I0815 10:13:01.158025 140716550170432 submission.py:120] 1) loss = 0.738, grad_norm = 3.412
I0815 10:13:01.588848 140663852685056 logging_writer.py:48] [2] global_step=2, grad_norm=3.384688, loss=0.737974
I0815 10:13:01.593842 140716550170432 submission.py:120] 2) loss = 0.738, grad_norm = 3.385
I0815 10:13:01.956139 140663861077760 logging_writer.py:48] [3] global_step=3, grad_norm=3.354032, loss=0.735895
I0815 10:13:01.960518 140716550170432 submission.py:120] 3) loss = 0.736, grad_norm = 3.354
I0815 10:13:02.287579 140663852685056 logging_writer.py:48] [4] global_step=4, grad_norm=3.314156, loss=0.732621
I0815 10:13:02.291850 140716550170432 submission.py:120] 4) loss = 0.733, grad_norm = 3.314
I0815 10:13:02.625415 140663861077760 logging_writer.py:48] [5] global_step=5, grad_norm=3.229860, loss=0.728938
I0815 10:13:02.629602 140716550170432 submission.py:120] 5) loss = 0.729, grad_norm = 3.230
I0815 10:13:02.962924 140663852685056 logging_writer.py:48] [6] global_step=6, grad_norm=3.127265, loss=0.723420
I0815 10:13:02.967241 140716550170432 submission.py:120] 6) loss = 0.723, grad_norm = 3.127
I0815 10:13:03.303838 140663861077760 logging_writer.py:48] [7] global_step=7, grad_norm=2.996590, loss=0.718262
I0815 10:13:03.308001 140716550170432 submission.py:120] 7) loss = 0.718, grad_norm = 2.997
I0815 10:13:03.639561 140663852685056 logging_writer.py:48] [8] global_step=8, grad_norm=2.884068, loss=0.712437
I0815 10:13:03.643888 140716550170432 submission.py:120] 8) loss = 0.712, grad_norm = 2.884
I0815 10:13:03.977665 140663861077760 logging_writer.py:48] [9] global_step=9, grad_norm=2.738493, loss=0.705946
I0815 10:13:03.981685 140716550170432 submission.py:120] 9) loss = 0.706, grad_norm = 2.738
I0815 10:13:04.315828 140663852685056 logging_writer.py:48] [10] global_step=10, grad_norm=2.626283, loss=0.698141
I0815 10:13:04.319995 140716550170432 submission.py:120] 10) loss = 0.698, grad_norm = 2.626
I0815 10:15:42.683936 140663861077760 logging_writer.py:48] [500] global_step=500, grad_norm=0.061602, loss=0.062011
I0815 10:15:42.689081 140716550170432 submission.py:120] 500) loss = 0.062, grad_norm = 0.062
I0815 10:17:00.375211 140716550170432 spec.py:320] Evaluating on the training split.
I0815 10:17:58.532493 140716550170432 spec.py:332] Evaluating on the validation split.
I0815 10:18:01.946500 140716550170432 spec.py:348] Evaluating on the test split.
I0815 10:18:05.248054 140716550170432 submission_runner.py:365] Time since start: 453.40s, 	Step: 742, 	{'train/accuracy': 0.9865441226687447, 'train/loss': 0.056026674597563254, 'train/mean_average_precision': 0.043239768584178356, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.0648548175925768, 'validation/mean_average_precision': 0.04394149729542692, 'validation/num_examples': 43793, 'test/accuracy': 0.983142946315789, 'test/loss': 0.06796844956972857, 'test/mean_average_precision': 0.04510807904753009, 'test/num_examples': 43793, 'score': 244.9157691001892, 'total_duration': 453.40493154525757, 'accumulated_submission_time': 244.9157691001892, 'accumulated_eval_time': 207.839834690094, 'accumulated_logging_time': 0.41968321800231934}
I0815 10:18:05.263624 140663852685056 logging_writer.py:48] [742] accumulated_eval_time=207.839835, accumulated_logging_time=0.419683, accumulated_submission_time=244.915769, global_step=742, preemption_count=0, score=244.915769, test/accuracy=0.983143, test/loss=0.067968, test/mean_average_precision=0.045108, test/num_examples=43793, total_duration=453.404932, train/accuracy=0.986544, train/loss=0.056027, train/mean_average_precision=0.043240, validation/accuracy=0.984118, validation/loss=0.064855, validation/mean_average_precision=0.043941, validation/num_examples=43793
I0815 10:18:05.687786 140716550170432 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:19:29.877800 140663861077760 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.087294, loss=0.050705
I0815 10:19:29.883354 140716550170432 submission.py:120] 1000) loss = 0.051, grad_norm = 0.087
I0815 10:22:05.355759 140716550170432 spec.py:320] Evaluating on the training split.
I0815 10:23:05.150707 140716550170432 spec.py:332] Evaluating on the validation split.
I0815 10:23:08.478032 140716550170432 spec.py:348] Evaluating on the test split.
I0815 10:23:11.735130 140716550170432 submission_runner.py:365] Time since start: 759.89s, 	Step: 1470, 	{'train/accuracy': 0.987422733752759, 'train/loss': 0.04699933147642716, 'train/mean_average_precision': 0.09648427342743895, 'validation/accuracy': 0.9845576098861174, 'validation/loss': 0.05626363453321285, 'validation/mean_average_precision': 0.10119329357499648, 'validation/num_examples': 43793, 'test/accuracy': 0.9835363421516057, 'test/loss': 0.059322757380489165, 'test/mean_average_precision': 0.09967765649738207, 'test/num_examples': 43793, 'score': 484.3297505378723, 'total_duration': 759.8919787406921, 'accumulated_submission_time': 484.3297505378723, 'accumulated_eval_time': 274.21895694732666, 'accumulated_logging_time': 0.8928680419921875}
I0815 10:23:11.751267 140663852685056 logging_writer.py:48] [1470] accumulated_eval_time=274.218957, accumulated_logging_time=0.892868, accumulated_submission_time=484.329751, global_step=1470, preemption_count=0, score=484.329751, test/accuracy=0.983536, test/loss=0.059323, test/mean_average_precision=0.099678, test/num_examples=43793, total_duration=759.891979, train/accuracy=0.987423, train/loss=0.046999, train/mean_average_precision=0.096484, validation/accuracy=0.984558, validation/loss=0.056264, validation/mean_average_precision=0.101193, validation/num_examples=43793
I0815 10:23:12.220708 140716550170432 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:23:22.476174 140663861077760 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.083361, loss=0.046619
I0815 10:23:22.482401 140716550170432 submission.py:120] 1500) loss = 0.047, grad_norm = 0.083
I0815 10:26:05.850881 140663852685056 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.033274, loss=0.047624
I0815 10:26:05.856866 140716550170432 submission.py:120] 2000) loss = 0.048, grad_norm = 0.033
I0815 10:27:11.743782 140716550170432 spec.py:320] Evaluating on the training split.
I0815 10:28:13.942656 140716550170432 spec.py:332] Evaluating on the validation split.
I0815 10:28:17.273850 140716550170432 spec.py:348] Evaluating on the test split.
I0815 10:28:20.556081 140716550170432 submission_runner.py:365] Time since start: 1068.71s, 	Step: 2201, 	{'train/accuracy': 0.9876022411620945, 'train/loss': 0.04519536901686107, 'train/mean_average_precision': 0.12802668235930142, 'validation/accuracy': 0.9847776297447364, 'validation/loss': 0.05456630133018648, 'validation/mean_average_precision': 0.1211232772568052, 'validation/num_examples': 43793, 'test/accuracy': 0.9837380944057343, 'test/loss': 0.05774535037922263, 'test/mean_average_precision': 0.12046976275097207, 'test/num_examples': 43793, 'score': 723.6202735900879, 'total_duration': 1068.7129406929016, 'accumulated_submission_time': 723.6202735900879, 'accumulated_eval_time': 343.03108191490173, 'accumulated_logging_time': 1.3973357677459717}
I0815 10:28:20.571191 140663861077760 logging_writer.py:48] [2201] accumulated_eval_time=343.031082, accumulated_logging_time=1.397336, accumulated_submission_time=723.620274, global_step=2201, preemption_count=0, score=723.620274, test/accuracy=0.983738, test/loss=0.057745, test/mean_average_precision=0.120470, test/num_examples=43793, total_duration=1068.712941, train/accuracy=0.987602, train/loss=0.045195, train/mean_average_precision=0.128027, validation/accuracy=0.984778, validation/loss=0.054566, validation/mean_average_precision=0.121123, validation/num_examples=43793
I0815 10:28:21.055992 140716550170432 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:29:59.973047 140663852685056 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.025585, loss=0.047165
I0815 10:29:59.979108 140716550170432 submission.py:120] 2500) loss = 0.047, grad_norm = 0.026
I0815 10:32:20.742064 140716550170432 spec.py:320] Evaluating on the training split.
I0815 10:33:24.492934 140716550170432 spec.py:332] Evaluating on the validation split.
I0815 10:33:27.795820 140716550170432 spec.py:348] Evaluating on the test split.
I0815 10:33:31.051832 140716550170432 submission_runner.py:365] Time since start: 1379.21s, 	Step: 2928, 	{'train/accuracy': 0.9879824780174351, 'train/loss': 0.04290051126745865, 'train/mean_average_precision': 0.1489318079461532, 'validation/accuracy': 0.9850849268535455, 'validation/loss': 0.05235314435575993, 'validation/mean_average_precision': 0.14214066882866266, 'validation/num_examples': 43793, 'test/accuracy': 0.9841529711704874, 'test/loss': 0.055126098581037224, 'test/mean_average_precision': 0.14018081859847706, 'test/num_examples': 43793, 'score': 963.0413632392883, 'total_duration': 1379.2086899280548, 'accumulated_submission_time': 963.0413632392883, 'accumulated_eval_time': 413.3406472206116, 'accumulated_logging_time': 1.9510643482208252}
I0815 10:33:31.068139 140663861077760 logging_writer.py:48] [2928] accumulated_eval_time=413.340647, accumulated_logging_time=1.951064, accumulated_submission_time=963.041363, global_step=2928, preemption_count=0, score=963.041363, test/accuracy=0.984153, test/loss=0.055126, test/mean_average_precision=0.140181, test/num_examples=43793, total_duration=1379.208690, train/accuracy=0.987982, train/loss=0.042901, train/mean_average_precision=0.148932, validation/accuracy=0.985085, validation/loss=0.052353, validation/mean_average_precision=0.142141, validation/num_examples=43793
I0815 10:33:31.582362 140716550170432 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:33:55.860571 140663852685056 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.015850, loss=0.042722
I0815 10:33:55.865942 140716550170432 submission.py:120] 3000) loss = 0.043, grad_norm = 0.016
I0815 10:36:38.994001 140663861077760 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.017166, loss=0.043610
I0815 10:36:38.999725 140716550170432 submission.py:120] 3500) loss = 0.044, grad_norm = 0.017
I0815 10:37:31.109271 140716550170432 spec.py:320] Evaluating on the training split.
I0815 10:38:35.605397 140716550170432 spec.py:332] Evaluating on the validation split.
I0815 10:38:38.958525 140716550170432 spec.py:348] Evaluating on the test split.
I0815 10:38:42.260192 140716550170432 submission_runner.py:365] Time since start: 1690.42s, 	Step: 3661, 	{'train/accuracy': 0.9880609959858837, 'train/loss': 0.041563473411400466, 'train/mean_average_precision': 0.18850878295474716, 'validation/accuracy': 0.9851527189502048, 'validation/loss': 0.051247961289697955, 'validation/mean_average_precision': 0.16583446269761343, 'validation/num_examples': 43793, 'test/accuracy': 0.9842544790895793, 'test/loss': 0.054176738923316876, 'test/mean_average_precision': 0.1604271478267737, 'test/num_examples': 43793, 'score': 1202.3320770263672, 'total_duration': 1690.417057275772, 'accumulated_submission_time': 1202.3320770263672, 'accumulated_eval_time': 484.49133110046387, 'accumulated_logging_time': 2.5069472789764404}
I0815 10:38:42.275822 140663852685056 logging_writer.py:48] [3661] accumulated_eval_time=484.491331, accumulated_logging_time=2.506947, accumulated_submission_time=1202.332077, global_step=3661, preemption_count=0, score=1202.332077, test/accuracy=0.984254, test/loss=0.054177, test/mean_average_precision=0.160427, test/num_examples=43793, total_duration=1690.417057, train/accuracy=0.988061, train/loss=0.041563, train/mean_average_precision=0.188509, validation/accuracy=0.985153, validation/loss=0.051248, validation/mean_average_precision=0.165834, validation/num_examples=43793
I0815 10:38:42.831944 140716550170432 submission_runner.py:396] Released all unoccupied cached memory.
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0815 10:40:33.719369 140663861077760 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.013487, loss=0.044904
I0815 10:40:33.725403 140716550170432 submission.py:120] 4000) loss = 0.045, grad_norm = 0.013
I0815 10:42:42.306925 140716550170432 spec.py:320] Evaluating on the training split.
I0815 10:43:49.840226 140716550170432 spec.py:332] Evaluating on the validation split.
I0815 10:43:53.214430 140716550170432 spec.py:348] Evaluating on the test split.
I0815 10:43:56.516557 140716550170432 submission_runner.py:365] Time since start: 2004.67s, 	Step: 4392, 	{'train/accuracy': 0.9882576922190592, 'train/loss': 0.04012929609476728, 'train/mean_average_precision': 0.2116166372203883, 'validation/accuracy': 0.985425105158938, 'validation/loss': 0.04989776446935026, 'validation/mean_average_precision': 0.17673697525280202, 'validation/num_examples': 43793, 'test/accuracy': 0.9845371007232334, 'test/loss': 0.05251873237036996, 'test/mean_average_precision': 0.17763728210740976, 'test/num_examples': 43793, 'score': 1441.5880365371704, 'total_duration': 2004.6734187602997, 'accumulated_submission_time': 1441.5880365371704, 'accumulated_eval_time': 558.700742483139, 'accumulated_logging_time': 3.094485282897949}
I0815 10:43:56.532299 140669305681664 logging_writer.py:48] [4392] accumulated_eval_time=558.700742, accumulated_logging_time=3.094485, accumulated_submission_time=1441.588037, global_step=4392, preemption_count=0, score=1441.588037, test/accuracy=0.984537, test/loss=0.052519, test/mean_average_precision=0.177637, test/num_examples=43793, total_duration=2004.673419, train/accuracy=0.988258, train/loss=0.040129, train/mean_average_precision=0.211617, validation/accuracy=0.985425, validation/loss=0.049898, validation/mean_average_precision=0.176737, validation/num_examples=43793
I0815 10:43:57.111119 140716550170432 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:44:33.413485 140669314074368 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.020658, loss=0.037626
I0815 10:44:33.418962 140716550170432 submission.py:120] 4500) loss = 0.038, grad_norm = 0.021
I0815 10:47:17.643079 140669305681664 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.009916, loss=0.039533
I0815 10:47:17.649401 140716550170432 submission.py:120] 5000) loss = 0.040, grad_norm = 0.010
I0815 10:47:56.634496 140716550170432 spec.py:320] Evaluating on the training split.
I0815 10:49:06.123967 140716550170432 spec.py:332] Evaluating on the validation split.
I0815 10:49:09.628706 140716550170432 spec.py:348] Evaluating on the test split.
I0815 10:49:12.998253 140716550170432 submission_runner.py:365] Time since start: 2321.16s, 	Step: 5119, 	{'train/accuracy': 0.9887375690455343, 'train/loss': 0.038780078496566735, 'train/mean_average_precision': 0.2342534807622136, 'validation/accuracy': 0.9856987091897668, 'validation/loss': 0.04858058521628927, 'validation/mean_average_precision': 0.18965199300299176, 'validation/num_examples': 43793, 'test/accuracy': 0.9848226707196828, 'test/loss': 0.05121507229912067, 'test/mean_average_precision': 0.19153576693915272, 'test/num_examples': 43793, 'score': 1680.8944771289825, 'total_duration': 2321.155102491379, 'accumulated_submission_time': 1680.8944771289825, 'accumulated_eval_time': 635.0642800331116, 'accumulated_logging_time': 3.700626850128174}
I0815 10:49:13.014420 140669314074368 logging_writer.py:48] [5119] accumulated_eval_time=635.064280, accumulated_logging_time=3.700627, accumulated_submission_time=1680.894477, global_step=5119, preemption_count=0, score=1680.894477, test/accuracy=0.984823, test/loss=0.051215, test/mean_average_precision=0.191536, test/num_examples=43793, total_duration=2321.155102, train/accuracy=0.988738, train/loss=0.038780, train/mean_average_precision=0.234253, validation/accuracy=0.985699, validation/loss=0.048581, validation/mean_average_precision=0.189652, validation/num_examples=43793
I0815 10:49:13.632843 140716550170432 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:51:20.945268 140669305681664 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.016785, loss=0.042490
I0815 10:51:20.953103 140716550170432 submission.py:120] 5500) loss = 0.042, grad_norm = 0.017
I0815 10:53:13.198011 140716550170432 spec.py:320] Evaluating on the training split.
I0815 10:54:23.272713 140716550170432 spec.py:332] Evaluating on the validation split.
I0815 10:54:26.708824 140716550170432 spec.py:348] Evaluating on the test split.
I0815 10:54:30.060551 140716550170432 submission_runner.py:365] Time since start: 2638.22s, 	Step: 5839, 	{'train/accuracy': 0.9890031729664593, 'train/loss': 0.03753492566841253, 'train/mean_average_precision': 0.2661559273421925, 'validation/accuracy': 0.9858501250703292, 'validation/loss': 0.048110816685705285, 'validation/mean_average_precision': 0.20659100644682404, 'validation/num_examples': 43793, 'test/accuracy': 0.9849709312488127, 'test/loss': 0.050835835844004654, 'test/mean_average_precision': 0.21010566520269675, 'test/num_examples': 43793, 'score': 1920.2511143684387, 'total_duration': 2638.2174022197723, 'accumulated_submission_time': 1920.2511143684387, 'accumulated_eval_time': 711.9266004562378, 'accumulated_logging_time': 4.34058690071106}
I0815 10:54:30.079946 140669314074368 logging_writer.py:48] [5839] accumulated_eval_time=711.926600, accumulated_logging_time=4.340587, accumulated_submission_time=1920.251114, global_step=5839, preemption_count=0, score=1920.251114, test/accuracy=0.984971, test/loss=0.050836, test/mean_average_precision=0.210106, test/num_examples=43793, total_duration=2638.217402, train/accuracy=0.989003, train/loss=0.037535, train/mean_average_precision=0.266156, validation/accuracy=0.985850, validation/loss=0.048111, validation/mean_average_precision=0.206591, validation/num_examples=43793
I0815 10:54:30.718879 140716550170432 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:55:23.705656 140716550170432 spec.py:320] Evaluating on the training split.
I0815 10:56:31.872853 140716550170432 spec.py:332] Evaluating on the validation split.
I0815 10:56:35.341266 140716550170432 spec.py:348] Evaluating on the test split.
I0815 10:56:38.714613 140716550170432 submission_runner.py:365] Time since start: 2766.87s, 	Step: 6000, 	{'train/accuracy': 0.9890625535237371, 'train/loss': 0.03728416153134094, 'train/mean_average_precision': 0.2521811715786305, 'validation/accuracy': 0.9858586498249989, 'validation/loss': 0.047389307836604, 'validation/mean_average_precision': 0.20992669670802233, 'validation/num_examples': 43793, 'test/accuracy': 0.9850357952303072, 'test/loss': 0.0500170235045588, 'test/mean_average_precision': 0.2091164857202246, 'test/num_examples': 43793, 'score': 1973.1864566802979, 'total_duration': 2766.8714973926544, 'accumulated_submission_time': 1973.1864566802979, 'accumulated_eval_time': 786.9353442192078, 'accumulated_logging_time': 5.003198146820068}
I0815 10:56:38.730134 140669305681664 logging_writer.py:48] [6000] accumulated_eval_time=786.935344, accumulated_logging_time=5.003198, accumulated_submission_time=1973.186457, global_step=6000, preemption_count=0, score=1973.186457, test/accuracy=0.985036, test/loss=0.050017, test/mean_average_precision=0.209116, test/num_examples=43793, total_duration=2766.871497, train/accuracy=0.989063, train/loss=0.037284, train/mean_average_precision=0.252181, validation/accuracy=0.985859, validation/loss=0.047389, validation/mean_average_precision=0.209927, validation/num_examples=43793
I0815 10:56:39.364362 140716550170432 submission_runner.py:396] Released all unoccupied cached memory.
I0815 10:56:39.474476 140669314074368 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1973.186457
I0815 10:56:39.562281 140716550170432 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_after_pytorch_fixes/adamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0815 10:56:39.711480 140716550170432 submission_runner.py:534] Tuning trial 1/1
I0815 10:56:39.711689 140716550170432 submission_runner.py:535] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0815 10:56:39.712738 140716550170432 submission_runner.py:536] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5292460965126498, 'train/loss': 0.7383571329336538, 'train/mean_average_precision': 0.02007834088388548, 'validation/accuracy': 0.52937346300703, 'validation/loss': 0.7380277432051616, 'validation/mean_average_precision': 0.02458107616068408, 'validation/num_examples': 43793, 'test/accuracy': 0.5318092544053805, 'test/loss': 0.7373300953289931, 'test/mean_average_precision': 0.026619820273342486, 'test/num_examples': 43793, 'score': 5.553447961807251, 'total_duration': 148.52094650268555, 'accumulated_submission_time': 5.553447961807251, 'accumulated_eval_time': 142.96720123291016, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (742, {'train/accuracy': 0.9865441226687447, 'train/loss': 0.056026674597563254, 'train/mean_average_precision': 0.043239768584178356, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.0648548175925768, 'validation/mean_average_precision': 0.04394149729542692, 'validation/num_examples': 43793, 'test/accuracy': 0.983142946315789, 'test/loss': 0.06796844956972857, 'test/mean_average_precision': 0.04510807904753009, 'test/num_examples': 43793, 'score': 244.9157691001892, 'total_duration': 453.40493154525757, 'accumulated_submission_time': 244.9157691001892, 'accumulated_eval_time': 207.839834690094, 'accumulated_logging_time': 0.41968321800231934, 'global_step': 742, 'preemption_count': 0}), (1470, {'train/accuracy': 0.987422733752759, 'train/loss': 0.04699933147642716, 'train/mean_average_precision': 0.09648427342743895, 'validation/accuracy': 0.9845576098861174, 'validation/loss': 0.05626363453321285, 'validation/mean_average_precision': 0.10119329357499648, 'validation/num_examples': 43793, 'test/accuracy': 0.9835363421516057, 'test/loss': 0.059322757380489165, 'test/mean_average_precision': 0.09967765649738207, 'test/num_examples': 43793, 'score': 484.3297505378723, 'total_duration': 759.8919787406921, 'accumulated_submission_time': 484.3297505378723, 'accumulated_eval_time': 274.21895694732666, 'accumulated_logging_time': 0.8928680419921875, 'global_step': 1470, 'preemption_count': 0}), (2201, {'train/accuracy': 0.9876022411620945, 'train/loss': 0.04519536901686107, 'train/mean_average_precision': 0.12802668235930142, 'validation/accuracy': 0.9847776297447364, 'validation/loss': 0.05456630133018648, 'validation/mean_average_precision': 0.1211232772568052, 'validation/num_examples': 43793, 'test/accuracy': 0.9837380944057343, 'test/loss': 0.05774535037922263, 'test/mean_average_precision': 0.12046976275097207, 'test/num_examples': 43793, 'score': 723.6202735900879, 'total_duration': 1068.7129406929016, 'accumulated_submission_time': 723.6202735900879, 'accumulated_eval_time': 343.03108191490173, 'accumulated_logging_time': 1.3973357677459717, 'global_step': 2201, 'preemption_count': 0}), (2928, {'train/accuracy': 0.9879824780174351, 'train/loss': 0.04290051126745865, 'train/mean_average_precision': 0.1489318079461532, 'validation/accuracy': 0.9850849268535455, 'validation/loss': 0.05235314435575993, 'validation/mean_average_precision': 0.14214066882866266, 'validation/num_examples': 43793, 'test/accuracy': 0.9841529711704874, 'test/loss': 0.055126098581037224, 'test/mean_average_precision': 0.14018081859847706, 'test/num_examples': 43793, 'score': 963.0413632392883, 'total_duration': 1379.2086899280548, 'accumulated_submission_time': 963.0413632392883, 'accumulated_eval_time': 413.3406472206116, 'accumulated_logging_time': 1.9510643482208252, 'global_step': 2928, 'preemption_count': 0}), (3661, {'train/accuracy': 0.9880609959858837, 'train/loss': 0.041563473411400466, 'train/mean_average_precision': 0.18850878295474716, 'validation/accuracy': 0.9851527189502048, 'validation/loss': 0.051247961289697955, 'validation/mean_average_precision': 0.16583446269761343, 'validation/num_examples': 43793, 'test/accuracy': 0.9842544790895793, 'test/loss': 0.054176738923316876, 'test/mean_average_precision': 0.1604271478267737, 'test/num_examples': 43793, 'score': 1202.3320770263672, 'total_duration': 1690.417057275772, 'accumulated_submission_time': 1202.3320770263672, 'accumulated_eval_time': 484.49133110046387, 'accumulated_logging_time': 2.5069472789764404, 'global_step': 3661, 'preemption_count': 0}), (4392, {'train/accuracy': 0.9882576922190592, 'train/loss': 0.04012929609476728, 'train/mean_average_precision': 0.2116166372203883, 'validation/accuracy': 0.985425105158938, 'validation/loss': 0.04989776446935026, 'validation/mean_average_precision': 0.17673697525280202, 'validation/num_examples': 43793, 'test/accuracy': 0.9845371007232334, 'test/loss': 0.05251873237036996, 'test/mean_average_precision': 0.17763728210740976, 'test/num_examples': 43793, 'score': 1441.5880365371704, 'total_duration': 2004.6734187602997, 'accumulated_submission_time': 1441.5880365371704, 'accumulated_eval_time': 558.700742483139, 'accumulated_logging_time': 3.094485282897949, 'global_step': 4392, 'preemption_count': 0}), (5119, {'train/accuracy': 0.9887375690455343, 'train/loss': 0.038780078496566735, 'train/mean_average_precision': 0.2342534807622136, 'validation/accuracy': 0.9856987091897668, 'validation/loss': 0.04858058521628927, 'validation/mean_average_precision': 0.18965199300299176, 'validation/num_examples': 43793, 'test/accuracy': 0.9848226707196828, 'test/loss': 0.05121507229912067, 'test/mean_average_precision': 0.19153576693915272, 'test/num_examples': 43793, 'score': 1680.8944771289825, 'total_duration': 2321.155102491379, 'accumulated_submission_time': 1680.8944771289825, 'accumulated_eval_time': 635.0642800331116, 'accumulated_logging_time': 3.700626850128174, 'global_step': 5119, 'preemption_count': 0}), (5839, {'train/accuracy': 0.9890031729664593, 'train/loss': 0.03753492566841253, 'train/mean_average_precision': 0.2661559273421925, 'validation/accuracy': 0.9858501250703292, 'validation/loss': 0.048110816685705285, 'validation/mean_average_precision': 0.20659100644682404, 'validation/num_examples': 43793, 'test/accuracy': 0.9849709312488127, 'test/loss': 0.050835835844004654, 'test/mean_average_precision': 0.21010566520269675, 'test/num_examples': 43793, 'score': 1920.2511143684387, 'total_duration': 2638.2174022197723, 'accumulated_submission_time': 1920.2511143684387, 'accumulated_eval_time': 711.9266004562378, 'accumulated_logging_time': 4.34058690071106, 'global_step': 5839, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9890625535237371, 'train/loss': 0.03728416153134094, 'train/mean_average_precision': 0.2521811715786305, 'validation/accuracy': 0.9858586498249989, 'validation/loss': 0.047389307836604, 'validation/mean_average_precision': 0.20992669670802233, 'validation/num_examples': 43793, 'test/accuracy': 0.9850357952303072, 'test/loss': 0.0500170235045588, 'test/mean_average_precision': 0.2091164857202246, 'test/num_examples': 43793, 'score': 1973.1864566802979, 'total_duration': 2766.8714973926544, 'accumulated_submission_time': 1973.1864566802979, 'accumulated_eval_time': 786.9353442192078, 'accumulated_logging_time': 5.003198146820068, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0815 10:56:39.712863 140716550170432 submission_runner.py:537] Timing: 1973.1864566802979
I0815 10:56:39.712920 140716550170432 submission_runner.py:539] Total number of evals: 10
I0815 10:56:39.712970 140716550170432 submission_runner.py:540] ====================
I0815 10:56:39.713080 140716550170432 submission_runner.py:608] Final ogbg score: 1973.1864566802979
