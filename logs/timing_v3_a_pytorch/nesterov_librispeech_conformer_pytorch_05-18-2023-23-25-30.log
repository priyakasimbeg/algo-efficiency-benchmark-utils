torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_conformer --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_pytorch/timing_nesterov --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_pytorch_05-18-2023-23-25-30.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0518 23:25:53.938440 140100653373248 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0518 23:25:53.938463 140564812482368 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0518 23:25:53.939198 140220819904320 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0518 23:25:53.939430 140651494410048 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0518 23:25:53.939642 139788200011584 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0518 23:25:53.939712 139992531593024 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0518 23:25:53.940115 139750598297408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0518 23:25:53.949916 139951642003264 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0518 23:25:53.950075 140651494410048 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:25:53.950286 139951642003264 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:25:53.950340 139788200011584 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:25:53.950385 139992531593024 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:25:53.950688 139750598297408 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:25:53.959470 140100653373248 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:25:53.959496 140564812482368 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:25:53.960150 140220819904320 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:25:54.328066 139951642003264 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_pytorch/timing_nesterov/librispeech_conformer_pytorch.
W0518 23:25:54.660552 140100653373248 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 23:25:54.660812 140564812482368 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 23:25:54.661541 140651494410048 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 23:25:54.663381 139951642003264 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 23:25:54.664561 139750598297408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0518 23:25:54.669169 139951642003264 submission_runner.py:544] Using RNG seed 3547993155
W0518 23:25:54.670229 139992531593024 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0518 23:25:54.670594 139951642003264 submission_runner.py:553] --- Tuning run 1/1 ---
I0518 23:25:54.670721 139951642003264 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_pytorch/timing_nesterov/librispeech_conformer_pytorch/trial_1.
I0518 23:25:54.670970 139951642003264 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_pytorch/timing_nesterov/librispeech_conformer_pytorch/trial_1/hparams.json.
I0518 23:25:54.671928 139951642003264 submission_runner.py:241] Initializing dataset.
I0518 23:25:54.672068 139951642003264 input_pipeline.py:20] Loading split = train-clean-100
W0518 23:25:54.696988 139788200011584 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 23:25:54.698930 140220819904320 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0518 23:25:54.709610 139951642003264 input_pipeline.py:20] Loading split = train-clean-360
I0518 23:25:55.048604 139951642003264 input_pipeline.py:20] Loading split = train-other-500
I0518 23:25:55.500028 139951642003264 submission_runner.py:248] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0518 23:26:02.533867 139951642003264 submission_runner.py:258] Initializing optimizer.
I0518 23:26:03.012032 139951642003264 submission_runner.py:265] Initializing metrics bundle.
I0518 23:26:03.012225 139951642003264 submission_runner.py:283] Initializing checkpoint and logger.
I0518 23:26:03.013499 139951642003264 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0518 23:26:03.013639 139951642003264 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0518 23:26:03.555363 139951642003264 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_pytorch/timing_nesterov/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0518 23:26:03.556343 139951642003264 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_pytorch/timing_nesterov/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0518 23:26:03.563935 139951642003264 submission_runner.py:319] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0518 23:26:11.421495 139925026498304 logging_writer.py:48] [0] global_step=0, grad_norm=54.722347, loss=32.326878
I0518 23:26:11.445219 139951642003264 submission.py:139] 0) loss = 32.327, grad_norm = 54.722
I0518 23:26:11.446269 139951642003264 spec.py:298] Evaluating on the training split.
I0518 23:26:11.447329 139951642003264 input_pipeline.py:20] Loading split = train-clean-100
I0518 23:26:11.480379 139951642003264 input_pipeline.py:20] Loading split = train-clean-360
I0518 23:26:11.914031 139951642003264 input_pipeline.py:20] Loading split = train-other-500
I0518 23:26:29.094162 139951642003264 spec.py:310] Evaluating on the validation split.
I0518 23:26:29.095408 139951642003264 input_pipeline.py:20] Loading split = dev-clean
I0518 23:26:29.099551 139951642003264 input_pipeline.py:20] Loading split = dev-other
I0518 23:26:40.060399 139951642003264 spec.py:326] Evaluating on the test split.
I0518 23:26:40.061832 139951642003264 input_pipeline.py:20] Loading split = test-clean
I0518 23:26:45.902590 139951642003264 submission_runner.py:421] Time since start: 42.34s, 	Step: 1, 	{'train/ctc_loss': 31.299088759326455, 'train/wer': 2.432714938767523, 'validation/ctc_loss': 30.347134317862167, 'validation/wer': 2.1297446048375415, 'validation/num_examples': 5348, 'test/ctc_loss': 30.396996580424542, 'test/wer': 2.203461093169216, 'test/num_examples': 2472, 'score': 7.881624937057495, 'total_duration': 42.33900046348572, 'accumulated_submission_time': 7.881624937057495, 'accumulated_eval_time': 34.456048250198364, 'accumulated_logging_time': 0}
I0518 23:26:45.926836 139910820497152 logging_writer.py:48] [1] accumulated_eval_time=34.456048, accumulated_logging_time=0, accumulated_submission_time=7.881625, global_step=1, preemption_count=0, score=7.881625, test/ctc_loss=30.396997, test/num_examples=2472, test/wer=2.203461, total_duration=42.339000, train/ctc_loss=31.299089, train/wer=2.432715, validation/ctc_loss=30.347134, validation/num_examples=5348, validation/wer=2.129745
I0518 23:26:45.972471 139951642003264 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:26:45.972544 139788200011584 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:26:45.972539 140564812482368 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:26:45.972564 140651494410048 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:26:45.972582 139992531593024 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:26:45.973466 140100653373248 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:26:45.973502 140220819904320 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:26:45.973630 139750598297408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:26:47.021579 139910812104448 logging_writer.py:48] [1] global_step=1, grad_norm=56.243847, loss=31.753084
I0518 23:26:47.024956 139951642003264 submission.py:139] 1) loss = 31.753, grad_norm = 56.244
I0518 23:26:47.871903 139910820497152 logging_writer.py:48] [2] global_step=2, grad_norm=66.117165, loss=31.406675
I0518 23:26:47.875429 139951642003264 submission.py:139] 2) loss = 31.407, grad_norm = 66.117
I0518 23:26:48.826134 139910812104448 logging_writer.py:48] [3] global_step=3, grad_norm=65.948586, loss=29.898985
I0518 23:26:48.829298 139951642003264 submission.py:139] 3) loss = 29.899, grad_norm = 65.949
I0518 23:26:49.614651 139910820497152 logging_writer.py:48] [4] global_step=4, grad_norm=66.654091, loss=25.721806
I0518 23:26:49.617791 139951642003264 submission.py:139] 4) loss = 25.722, grad_norm = 66.654
I0518 23:26:50.403887 139910812104448 logging_writer.py:48] [5] global_step=5, grad_norm=51.275112, loss=21.650543
I0518 23:26:50.407043 139951642003264 submission.py:139] 5) loss = 21.651, grad_norm = 51.275
I0518 23:26:51.194811 139910820497152 logging_writer.py:48] [6] global_step=6, grad_norm=62.400173, loss=20.073288
I0518 23:26:51.197986 139951642003264 submission.py:139] 6) loss = 20.073, grad_norm = 62.400
I0518 23:26:51.985120 139910812104448 logging_writer.py:48] [7] global_step=7, grad_norm=37.350536, loss=19.264324
I0518 23:26:51.988416 139951642003264 submission.py:139] 7) loss = 19.264, grad_norm = 37.351
I0518 23:26:52.778133 139910820497152 logging_writer.py:48] [8] global_step=8, grad_norm=58.602673, loss=18.472675
I0518 23:26:52.781352 139951642003264 submission.py:139] 8) loss = 18.473, grad_norm = 58.603
I0518 23:26:53.572333 139910812104448 logging_writer.py:48] [9] global_step=9, grad_norm=27.808300, loss=17.614109
I0518 23:26:53.575690 139951642003264 submission.py:139] 9) loss = 17.614, grad_norm = 27.808
I0518 23:26:54.365531 139910820497152 logging_writer.py:48] [10] global_step=10, grad_norm=26.734200, loss=17.678303
I0518 23:26:54.368802 139951642003264 submission.py:139] 10) loss = 17.678, grad_norm = 26.734
I0518 23:26:55.157111 139910812104448 logging_writer.py:48] [11] global_step=11, grad_norm=22.180435, loss=16.434980
I0518 23:26:55.160195 139951642003264 submission.py:139] 11) loss = 16.435, grad_norm = 22.180
I0518 23:26:55.947613 139910820497152 logging_writer.py:48] [12] global_step=12, grad_norm=58.805431, loss=17.156796
I0518 23:26:55.951006 139951642003264 submission.py:139] 12) loss = 17.157, grad_norm = 58.805
I0518 23:26:56.739701 139910812104448 logging_writer.py:48] [13] global_step=13, grad_norm=13.985455, loss=15.603718
I0518 23:26:56.742957 139951642003264 submission.py:139] 13) loss = 15.604, grad_norm = 13.985
I0518 23:26:57.529924 139910820497152 logging_writer.py:48] [14] global_step=14, grad_norm=10.291460, loss=15.266778
I0518 23:26:57.533296 139951642003264 submission.py:139] 14) loss = 15.267, grad_norm = 10.291
I0518 23:26:58.320696 139910812104448 logging_writer.py:48] [15] global_step=15, grad_norm=24.078104, loss=15.092823
I0518 23:26:58.323947 139951642003264 submission.py:139] 15) loss = 15.093, grad_norm = 24.078
I0518 23:26:59.115078 139910820497152 logging_writer.py:48] [16] global_step=16, grad_norm=28.459963, loss=14.655760
I0518 23:26:59.118515 139951642003264 submission.py:139] 16) loss = 14.656, grad_norm = 28.460
I0518 23:26:59.907836 139910812104448 logging_writer.py:48] [17] global_step=17, grad_norm=47.082565, loss=12.370715
I0518 23:26:59.911185 139951642003264 submission.py:139] 17) loss = 12.371, grad_norm = 47.083
I0518 23:27:00.701018 139910820497152 logging_writer.py:48] [18] global_step=18, grad_norm=18.221531, loss=7.784082
I0518 23:27:00.704380 139951642003264 submission.py:139] 18) loss = 7.784, grad_norm = 18.222
I0518 23:27:01.491255 139910812104448 logging_writer.py:48] [19] global_step=19, grad_norm=25.387001, loss=8.847794
I0518 23:27:01.494448 139951642003264 submission.py:139] 19) loss = 8.848, grad_norm = 25.387
I0518 23:27:02.284692 139910820497152 logging_writer.py:48] [20] global_step=20, grad_norm=25.552185, loss=9.004796
I0518 23:27:02.288052 139951642003264 submission.py:139] 20) loss = 9.005, grad_norm = 25.552
I0518 23:27:03.075954 139910812104448 logging_writer.py:48] [21] global_step=21, grad_norm=23.776834, loss=8.244905
I0518 23:27:03.079605 139951642003264 submission.py:139] 21) loss = 8.245, grad_norm = 23.777
I0518 23:27:03.867333 139910820497152 logging_writer.py:48] [22] global_step=22, grad_norm=13.175283, loss=7.029384
I0518 23:27:03.870994 139951642003264 submission.py:139] 22) loss = 7.029, grad_norm = 13.175
I0518 23:27:04.660515 139910812104448 logging_writer.py:48] [23] global_step=23, grad_norm=23.757940, loss=7.107949
I0518 23:27:04.663868 139951642003264 submission.py:139] 23) loss = 7.108, grad_norm = 23.758
I0518 23:27:05.453881 139910820497152 logging_writer.py:48] [24] global_step=24, grad_norm=5.022226, loss=6.702653
I0518 23:27:05.457146 139951642003264 submission.py:139] 24) loss = 6.703, grad_norm = 5.022
I0518 23:27:06.244817 139910812104448 logging_writer.py:48] [25] global_step=25, grad_norm=3.444919, loss=6.663373
I0518 23:27:06.248256 139951642003264 submission.py:139] 25) loss = 6.663, grad_norm = 3.445
I0518 23:27:07.036675 139910820497152 logging_writer.py:48] [26] global_step=26, grad_norm=1.901013, loss=6.625906
I0518 23:27:07.040282 139951642003264 submission.py:139] 26) loss = 6.626, grad_norm = 1.901
I0518 23:27:07.830314 139910812104448 logging_writer.py:48] [27] global_step=27, grad_norm=1.911276, loss=6.640022
I0518 23:27:07.833513 139951642003264 submission.py:139] 27) loss = 6.640, grad_norm = 1.911
I0518 23:27:08.622236 139910820497152 logging_writer.py:48] [28] global_step=28, grad_norm=1.948242, loss=6.587995
I0518 23:27:08.625511 139951642003264 submission.py:139] 28) loss = 6.588, grad_norm = 1.948
I0518 23:27:09.415377 139910812104448 logging_writer.py:48] [29] global_step=29, grad_norm=1.992041, loss=6.592243
I0518 23:27:09.418729 139951642003264 submission.py:139] 29) loss = 6.592, grad_norm = 1.992
I0518 23:27:10.208645 139910820497152 logging_writer.py:48] [30] global_step=30, grad_norm=1.900792, loss=6.547296
I0518 23:27:10.211881 139951642003264 submission.py:139] 30) loss = 6.547, grad_norm = 1.901
I0518 23:27:11.002987 139910812104448 logging_writer.py:48] [31] global_step=31, grad_norm=1.807886, loss=6.529216
I0518 23:27:11.006097 139951642003264 submission.py:139] 31) loss = 6.529, grad_norm = 1.808
I0518 23:27:11.795261 139910820497152 logging_writer.py:48] [32] global_step=32, grad_norm=1.678339, loss=6.494266
I0518 23:27:11.798369 139951642003264 submission.py:139] 32) loss = 6.494, grad_norm = 1.678
I0518 23:27:12.585726 139910812104448 logging_writer.py:48] [33] global_step=33, grad_norm=1.618746, loss=6.476244
I0518 23:27:12.589138 139951642003264 submission.py:139] 33) loss = 6.476, grad_norm = 1.619
I0518 23:27:13.379400 139910820497152 logging_writer.py:48] [34] global_step=34, grad_norm=1.494646, loss=6.442467
I0518 23:27:13.382594 139951642003264 submission.py:139] 34) loss = 6.442, grad_norm = 1.495
I0518 23:27:14.174146 139910812104448 logging_writer.py:48] [35] global_step=35, grad_norm=1.390538, loss=6.404982
I0518 23:27:14.177427 139951642003264 submission.py:139] 35) loss = 6.405, grad_norm = 1.391
I0518 23:27:14.966523 139910820497152 logging_writer.py:48] [36] global_step=36, grad_norm=1.352308, loss=6.384350
I0518 23:27:14.969603 139951642003264 submission.py:139] 36) loss = 6.384, grad_norm = 1.352
I0518 23:27:15.756222 139910812104448 logging_writer.py:48] [37] global_step=37, grad_norm=1.312022, loss=6.366129
I0518 23:27:15.759284 139951642003264 submission.py:139] 37) loss = 6.366, grad_norm = 1.312
I0518 23:27:16.549636 139910820497152 logging_writer.py:48] [38] global_step=38, grad_norm=1.501385, loss=6.364709
I0518 23:27:16.552879 139951642003264 submission.py:139] 38) loss = 6.365, grad_norm = 1.501
I0518 23:27:17.341573 139910812104448 logging_writer.py:48] [39] global_step=39, grad_norm=1.458323, loss=6.339456
I0518 23:27:17.344827 139951642003264 submission.py:139] 39) loss = 6.339, grad_norm = 1.458
I0518 23:27:18.135082 139910820497152 logging_writer.py:48] [40] global_step=40, grad_norm=1.461730, loss=6.316230
I0518 23:27:18.138440 139951642003264 submission.py:139] 40) loss = 6.316, grad_norm = 1.462
I0518 23:27:18.928463 139910812104448 logging_writer.py:48] [41] global_step=41, grad_norm=1.507181, loss=6.276378
I0518 23:27:18.931746 139951642003264 submission.py:139] 41) loss = 6.276, grad_norm = 1.507
I0518 23:27:19.721648 139910820497152 logging_writer.py:48] [42] global_step=42, grad_norm=1.607506, loss=6.259930
I0518 23:27:19.725048 139951642003264 submission.py:139] 42) loss = 6.260, grad_norm = 1.608
I0518 23:27:20.511749 139910812104448 logging_writer.py:48] [43] global_step=43, grad_norm=1.681633, loss=6.234223
I0518 23:27:20.515061 139951642003264 submission.py:139] 43) loss = 6.234, grad_norm = 1.682
I0518 23:27:21.301912 139910820497152 logging_writer.py:48] [44] global_step=44, grad_norm=1.962239, loss=6.201501
I0518 23:27:21.305238 139951642003264 submission.py:139] 44) loss = 6.202, grad_norm = 1.962
I0518 23:27:22.095201 139910812104448 logging_writer.py:48] [45] global_step=45, grad_norm=2.242837, loss=6.182336
I0518 23:27:22.098275 139951642003264 submission.py:139] 45) loss = 6.182, grad_norm = 2.243
I0518 23:27:22.889291 139910820497152 logging_writer.py:48] [46] global_step=46, grad_norm=2.666564, loss=6.163083
I0518 23:27:22.892520 139951642003264 submission.py:139] 46) loss = 6.163, grad_norm = 2.667
I0518 23:27:23.682514 139910812104448 logging_writer.py:48] [47] global_step=47, grad_norm=2.809453, loss=6.145498
I0518 23:27:23.685822 139951642003264 submission.py:139] 47) loss = 6.145, grad_norm = 2.809
I0518 23:27:24.475032 139910820497152 logging_writer.py:48] [48] global_step=48, grad_norm=4.465128, loss=6.167215
I0518 23:27:24.478502 139951642003264 submission.py:139] 48) loss = 6.167, grad_norm = 4.465
I0518 23:27:25.268008 139910812104448 logging_writer.py:48] [49] global_step=49, grad_norm=6.174095, loss=6.169743
I0518 23:27:25.271509 139951642003264 submission.py:139] 49) loss = 6.170, grad_norm = 6.174
I0518 23:27:26.059468 139910820497152 logging_writer.py:48] [50] global_step=50, grad_norm=8.525650, loss=6.189791
I0518 23:27:26.062943 139951642003264 submission.py:139] 50) loss = 6.190, grad_norm = 8.526
I0518 23:27:26.850151 139910812104448 logging_writer.py:48] [51] global_step=51, grad_norm=11.015606, loss=6.279082
I0518 23:27:26.853425 139951642003264 submission.py:139] 51) loss = 6.279, grad_norm = 11.016
I0518 23:27:27.640084 139910820497152 logging_writer.py:48] [52] global_step=52, grad_norm=15.870131, loss=6.301898
I0518 23:27:27.643288 139951642003264 submission.py:139] 52) loss = 6.302, grad_norm = 15.870
I0518 23:27:28.430796 139910812104448 logging_writer.py:48] [53] global_step=53, grad_norm=17.436043, loss=6.698470
I0518 23:27:28.433926 139951642003264 submission.py:139] 53) loss = 6.698, grad_norm = 17.436
I0518 23:27:29.220352 139910820497152 logging_writer.py:48] [54] global_step=54, grad_norm=17.803888, loss=6.344193
I0518 23:27:29.223614 139951642003264 submission.py:139] 54) loss = 6.344, grad_norm = 17.804
I0518 23:27:30.011160 139910812104448 logging_writer.py:48] [55] global_step=55, grad_norm=18.592291, loss=6.832246
I0518 23:27:30.014382 139951642003264 submission.py:139] 55) loss = 6.832, grad_norm = 18.592
I0518 23:27:30.801400 139910820497152 logging_writer.py:48] [56] global_step=56, grad_norm=17.871290, loss=6.286270
I0518 23:27:30.804723 139951642003264 submission.py:139] 56) loss = 6.286, grad_norm = 17.871
I0518 23:27:31.591667 139910812104448 logging_writer.py:48] [57] global_step=57, grad_norm=18.535395, loss=6.798986
I0518 23:27:31.595238 139951642003264 submission.py:139] 57) loss = 6.799, grad_norm = 18.535
I0518 23:27:32.381739 139910820497152 logging_writer.py:48] [58] global_step=58, grad_norm=21.622347, loss=6.401903
I0518 23:27:32.384887 139951642003264 submission.py:139] 58) loss = 6.402, grad_norm = 21.622
I0518 23:27:33.174160 139910812104448 logging_writer.py:48] [59] global_step=59, grad_norm=20.722837, loss=7.248285
I0518 23:27:33.177619 139951642003264 submission.py:139] 59) loss = 7.248, grad_norm = 20.723
I0518 23:27:33.966785 139910820497152 logging_writer.py:48] [60] global_step=60, grad_norm=12.763120, loss=6.174158
I0518 23:27:33.969849 139951642003264 submission.py:139] 60) loss = 6.174, grad_norm = 12.763
I0518 23:27:34.758878 139910812104448 logging_writer.py:48] [61] global_step=61, grad_norm=14.318789, loss=6.362052
I0518 23:27:34.761914 139951642003264 submission.py:139] 61) loss = 6.362, grad_norm = 14.319
I0518 23:27:35.550222 139910820497152 logging_writer.py:48] [62] global_step=62, grad_norm=34.617222, loss=6.875869
I0518 23:27:35.553370 139951642003264 submission.py:139] 62) loss = 6.876, grad_norm = 34.617
I0518 23:27:36.341576 139910812104448 logging_writer.py:48] [63] global_step=63, grad_norm=23.313738, loss=9.245583
I0518 23:27:36.345009 139951642003264 submission.py:139] 63) loss = 9.246, grad_norm = 23.314
I0518 23:27:37.133558 139910820497152 logging_writer.py:48] [64] global_step=64, grad_norm=19.928463, loss=7.065948
I0518 23:27:37.137221 139951642003264 submission.py:139] 64) loss = 7.066, grad_norm = 19.928
I0518 23:27:37.925047 139910812104448 logging_writer.py:48] [65] global_step=65, grad_norm=52.078552, loss=7.887184
I0518 23:27:37.928184 139951642003264 submission.py:139] 65) loss = 7.887, grad_norm = 52.079
I0518 23:27:38.717375 139910820497152 logging_writer.py:48] [66] global_step=66, grad_norm=23.315727, loss=11.398440
I0518 23:27:38.720604 139951642003264 submission.py:139] 66) loss = 11.398, grad_norm = 23.316
I0518 23:27:39.508796 139910812104448 logging_writer.py:48] [67] global_step=67, grad_norm=23.058210, loss=9.728109
I0518 23:27:39.512051 139951642003264 submission.py:139] 67) loss = 9.728, grad_norm = 23.058
I0518 23:27:40.300183 139910820497152 logging_writer.py:48] [68] global_step=68, grad_norm=12.941639, loss=6.276694
I0518 23:27:40.303384 139951642003264 submission.py:139] 68) loss = 6.277, grad_norm = 12.942
I0518 23:27:41.093109 139910812104448 logging_writer.py:48] [69] global_step=69, grad_norm=84.324806, loss=11.829480
I0518 23:27:41.096813 139951642003264 submission.py:139] 69) loss = 11.829, grad_norm = 84.325
I0518 23:27:41.886494 139910820497152 logging_writer.py:48] [70] global_step=70, grad_norm=22.940668, loss=16.200237
I0518 23:27:41.889892 139951642003264 submission.py:139] 70) loss = 16.200, grad_norm = 22.941
I0518 23:27:42.677653 139910812104448 logging_writer.py:48] [71] global_step=71, grad_norm=22.817198, loss=16.725431
I0518 23:27:42.680871 139951642003264 submission.py:139] 71) loss = 16.725, grad_norm = 22.817
I0518 23:27:43.467737 139910820497152 logging_writer.py:48] [72] global_step=72, grad_norm=22.690023, loss=14.784016
I0518 23:27:43.471484 139951642003264 submission.py:139] 72) loss = 14.784, grad_norm = 22.690
I0518 23:27:44.259040 139910812104448 logging_writer.py:48] [73] global_step=73, grad_norm=22.513891, loss=10.530096
I0518 23:27:44.262233 139951642003264 submission.py:139] 73) loss = 10.530, grad_norm = 22.514
I0518 23:27:45.052815 139910820497152 logging_writer.py:48] [74] global_step=74, grad_norm=17.509340, loss=6.272701
I0518 23:27:45.056201 139951642003264 submission.py:139] 74) loss = 6.273, grad_norm = 17.509
I0518 23:27:45.845824 139910812104448 logging_writer.py:48] [75] global_step=75, grad_norm=18.945145, loss=6.299335
I0518 23:27:45.848919 139951642003264 submission.py:139] 75) loss = 6.299, grad_norm = 18.945
I0518 23:27:46.638553 139910820497152 logging_writer.py:48] [76] global_step=76, grad_norm=15.998206, loss=6.549135
I0518 23:27:46.641733 139951642003264 submission.py:139] 76) loss = 6.549, grad_norm = 15.998
I0518 23:27:47.429044 139910812104448 logging_writer.py:48] [77] global_step=77, grad_norm=52.478237, loss=8.093855
I0518 23:27:47.432719 139951642003264 submission.py:139] 77) loss = 8.094, grad_norm = 52.478
I0518 23:27:48.219510 139910820497152 logging_writer.py:48] [78] global_step=78, grad_norm=22.277683, loss=12.880946
I0518 23:27:48.222636 139951642003264 submission.py:139] 78) loss = 12.881, grad_norm = 22.278
I0518 23:27:49.011642 139910812104448 logging_writer.py:48] [79] global_step=79, grad_norm=22.219707, loss=11.717288
I0518 23:27:49.015078 139951642003264 submission.py:139] 79) loss = 11.717, grad_norm = 22.220
I0518 23:27:49.802843 139910820497152 logging_writer.py:48] [80] global_step=80, grad_norm=21.069233, loss=8.110054
I0518 23:27:49.806150 139951642003264 submission.py:139] 80) loss = 8.110, grad_norm = 21.069
I0518 23:27:50.595606 139910812104448 logging_writer.py:48] [81] global_step=81, grad_norm=67.001732, loss=9.623656
I0518 23:27:50.598984 139951642003264 submission.py:139] 81) loss = 9.624, grad_norm = 67.002
I0518 23:27:51.388583 139910820497152 logging_writer.py:48] [82] global_step=82, grad_norm=22.040506, loss=14.455996
I0518 23:27:51.392079 139951642003264 submission.py:139] 82) loss = 14.456, grad_norm = 22.041
I0518 23:27:52.179267 139910812104448 logging_writer.py:48] [83] global_step=83, grad_norm=21.979746, loss=13.356736
I0518 23:27:52.182890 139951642003264 submission.py:139] 83) loss = 13.357, grad_norm = 21.980
I0518 23:27:52.973077 139910820497152 logging_writer.py:48] [84] global_step=84, grad_norm=21.651459, loss=9.728869
I0518 23:27:52.976466 139951642003264 submission.py:139] 84) loss = 9.729, grad_norm = 21.651
I0518 23:27:53.763687 139910812104448 logging_writer.py:48] [85] global_step=85, grad_norm=27.366714, loss=6.597217
I0518 23:27:53.766947 139951642003264 submission.py:139] 85) loss = 6.597, grad_norm = 27.367
I0518 23:27:54.555257 139910820497152 logging_writer.py:48] [86] global_step=86, grad_norm=18.704660, loss=7.127574
I0518 23:27:54.558718 139951642003264 submission.py:139] 86) loss = 7.128, grad_norm = 18.705
I0518 23:27:55.348645 139910812104448 logging_writer.py:48] [87] global_step=87, grad_norm=59.574379, loss=8.954000
I0518 23:27:55.352288 139951642003264 submission.py:139] 87) loss = 8.954, grad_norm = 59.574
I0518 23:27:56.140568 139910820497152 logging_writer.py:48] [88] global_step=88, grad_norm=21.600315, loss=14.669111
I0518 23:27:56.144103 139951642003264 submission.py:139] 88) loss = 14.669, grad_norm = 21.600
I0518 23:27:56.931864 139910812104448 logging_writer.py:48] [89] global_step=89, grad_norm=21.483006, loss=13.933451
I0518 23:27:56.935535 139951642003264 submission.py:139] 89) loss = 13.933, grad_norm = 21.483
I0518 23:27:57.724688 139910820497152 logging_writer.py:48] [90] global_step=90, grad_norm=21.315413, loss=10.553789
I0518 23:27:57.727968 139951642003264 submission.py:139] 90) loss = 10.554, grad_norm = 21.315
I0518 23:27:58.517058 139910812104448 logging_writer.py:48] [91] global_step=91, grad_norm=2.167128, loss=5.929457
I0518 23:27:58.520373 139951642003264 submission.py:139] 91) loss = 5.929, grad_norm = 2.167
I0518 23:27:59.305707 139910820497152 logging_writer.py:48] [92] global_step=92, grad_norm=63.545631, loss=9.481256
I0518 23:27:59.309038 139951642003264 submission.py:139] 92) loss = 9.481, grad_norm = 63.546
I0518 23:28:00.095037 139910812104448 logging_writer.py:48] [93] global_step=93, grad_norm=21.187294, loss=15.537487
I0518 23:28:00.098408 139951642003264 submission.py:139] 93) loss = 15.537, grad_norm = 21.187
I0518 23:28:00.888624 139910820497152 logging_writer.py:48] [94] global_step=94, grad_norm=21.060740, loss=14.975700
I0518 23:28:00.892016 139951642003264 submission.py:139] 94) loss = 14.976, grad_norm = 21.061
I0518 23:28:01.680231 139910812104448 logging_writer.py:48] [95] global_step=95, grad_norm=20.902481, loss=11.711897
I0518 23:28:01.683662 139951642003264 submission.py:139] 95) loss = 11.712, grad_norm = 20.902
I0518 23:28:02.472598 139910820497152 logging_writer.py:48] [96] global_step=96, grad_norm=13.362566, loss=6.351184
I0518 23:28:02.475756 139951642003264 submission.py:139] 96) loss = 6.351, grad_norm = 13.363
I0518 23:28:03.265652 139910812104448 logging_writer.py:48] [97] global_step=97, grad_norm=73.046814, loss=18.501875
I0518 23:28:03.269166 139951642003264 submission.py:139] 97) loss = 18.502, grad_norm = 73.047
I0518 23:28:04.056479 139910820497152 logging_writer.py:48] [98] global_step=98, grad_norm=20.527948, loss=14.088911
I0518 23:28:04.059715 139951642003264 submission.py:139] 98) loss = 14.089, grad_norm = 20.528
I0518 23:28:04.844854 139910812104448 logging_writer.py:48] [99] global_step=99, grad_norm=20.371143, loss=13.206993
I0518 23:28:04.848226 139951642003264 submission.py:139] 99) loss = 13.207, grad_norm = 20.371
I0518 23:28:05.636371 139910820497152 logging_writer.py:48] [100] global_step=100, grad_norm=19.778837, loss=9.760865
I0518 23:28:05.640141 139951642003264 submission.py:139] 100) loss = 9.761, grad_norm = 19.779
I0518 23:33:18.310089 139910812104448 logging_writer.py:48] [500] global_step=500, grad_norm=2.217248, loss=5.874295
I0518 23:33:18.314408 139951642003264 submission.py:139] 500) loss = 5.874, grad_norm = 2.217
I0518 23:39:49.182600 139910820497152 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.600923, loss=5.878197
I0518 23:39:49.188129 139951642003264 submission.py:139] 1000) loss = 5.878, grad_norm = 1.601
I0518 23:46:21.937568 139917015381760 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.939933, loss=5.812323
I0518 23:46:21.945396 139951642003264 submission.py:139] 1500) loss = 5.812, grad_norm = 0.940
I0518 23:52:52.736600 139917006989056 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.084112, loss=5.786186
I0518 23:52:52.741569 139951642003264 submission.py:139] 2000) loss = 5.786, grad_norm = 0.084
I0518 23:59:25.495095 139917015381760 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.564102, loss=5.807277
I0518 23:59:25.502633 139951642003264 submission.py:139] 2500) loss = 5.807, grad_norm = 0.564
I0519 00:05:49.293120 139917006989056 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0519 00:05:49.298571 139951642003264 submission.py:139] 3000) loss = nan, grad_norm = nan
I0519 00:06:46.769260 139951642003264 spec.py:298] Evaluating on the training split.
I0519 00:06:56.588976 139951642003264 spec.py:310] Evaluating on the validation split.
I0519 00:07:05.858647 139951642003264 spec.py:326] Evaluating on the test split.
I0519 00:07:10.927425 139951642003264 submission_runner.py:421] Time since start: 2467.36s, 	Step: 3078, 	{'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1390.1706454753876, 'total_duration': 2467.3637969493866, 'accumulated_submission_time': 1390.1706454753876, 'accumulated_eval_time': 58.61391472816467, 'accumulated_logging_time': 0.033032894134521484}
I0519 00:07:10.947802 139917015381760 logging_writer.py:48] [3078] accumulated_eval_time=58.613915, accumulated_logging_time=0.033033, accumulated_submission_time=1390.170645, global_step=3078, preemption_count=0, score=1390.170645, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2467.363797, train/ctc_loss=nan, train/wer=0.941556, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 00:12:28.302674 139917015381760 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0519 00:12:28.310710 139951642003264 submission.py:139] 3500) loss = nan, grad_norm = nan
I0519 00:18:41.561096 139917006989056 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0519 00:18:41.566227 139951642003264 submission.py:139] 4000) loss = nan, grad_norm = nan
I0519 00:24:56.498536 139917015381760 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0519 00:24:56.505004 139951642003264 submission.py:139] 4500) loss = nan, grad_norm = nan
I0519 00:31:09.912291 139917006989056 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0519 00:31:09.917671 139951642003264 submission.py:139] 5000) loss = nan, grad_norm = nan
I0519 00:37:24.808182 139917015381760 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0519 00:37:24.815273 139951642003264 submission.py:139] 5500) loss = nan, grad_norm = nan
I0519 00:43:38.142681 139917006989056 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0519 00:43:38.147760 139951642003264 submission.py:139] 6000) loss = nan, grad_norm = nan
I0519 00:47:12.009154 139951642003264 spec.py:298] Evaluating on the training split.
I0519 00:47:21.691903 139951642003264 spec.py:310] Evaluating on the validation split.
I0519 00:47:31.071433 139951642003264 spec.py:326] Evaluating on the test split.
I0519 00:47:37.330645 139951642003264 submission_runner.py:421] Time since start: 4893.77s, 	Step: 6285, 	{'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2763.820147037506, 'total_duration': 4893.7668833732605, 'accumulated_submission_time': 2763.820147037506, 'accumulated_eval_time': 83.93518161773682, 'accumulated_logging_time': 0.06403517723083496}
I0519 00:47:37.352688 139917006989056 logging_writer.py:48] [6285] accumulated_eval_time=83.935182, accumulated_logging_time=0.064035, accumulated_submission_time=2763.820147, global_step=6285, preemption_count=0, score=2763.820147, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=4893.766883, train/ctc_loss=nan, train/wer=0.941556, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 00:50:18.648572 139916998596352 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0519 00:50:18.654077 139951642003264 submission.py:139] 6500) loss = nan, grad_norm = nan
I0519 00:56:32.152454 139917006989056 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0519 00:56:32.158425 139951642003264 submission.py:139] 7000) loss = nan, grad_norm = nan
I0519 01:02:47.009556 139917006989056 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0519 01:02:47.016510 139951642003264 submission.py:139] 7500) loss = nan, grad_norm = nan
I0519 01:09:00.307968 139916998596352 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0519 01:09:00.312789 139951642003264 submission.py:139] 8000) loss = nan, grad_norm = nan
I0519 01:15:15.220287 139917006989056 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0519 01:15:15.227970 139951642003264 submission.py:139] 8500) loss = nan, grad_norm = nan
I0519 01:21:28.460057 139916998596352 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0519 01:21:28.464637 139951642003264 submission.py:139] 9000) loss = nan, grad_norm = nan
I0519 01:27:38.107410 139951642003264 spec.py:298] Evaluating on the training split.
I0519 01:27:48.030816 139951642003264 spec.py:310] Evaluating on the validation split.
I0519 01:27:57.484947 139951642003264 spec.py:326] Evaluating on the test split.
I0519 01:28:02.500848 139951642003264 submission_runner.py:421] Time since start: 7318.94s, 	Step: 9494, 	{'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4136.319772720337, 'total_duration': 7318.937154531479, 'accumulated_submission_time': 4136.319772720337, 'accumulated_eval_time': 108.32850933074951, 'accumulated_logging_time': 0.09587931632995605}
I0519 01:28:02.521925 139917006989056 logging_writer.py:48] [9494] accumulated_eval_time=108.328509, accumulated_logging_time=0.095879, accumulated_submission_time=4136.319773, global_step=9494, preemption_count=0, score=4136.319773, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7318.937155, train/ctc_loss=nan, train/wer=0.941556, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 01:28:07.767425 139916998596352 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0519 01:28:07.770973 139951642003264 submission.py:139] 9500) loss = nan, grad_norm = nan
I0519 01:34:21.154083 139917006989056 logging_writer.py:48] [10000] global_step=10000, grad_norm=nan, loss=nan
I0519 01:34:21.159123 139951642003264 submission.py:139] 10000) loss = nan, grad_norm = nan
I0519 01:40:36.160256 139917006989056 logging_writer.py:48] [10500] global_step=10500, grad_norm=nan, loss=nan
I0519 01:40:36.168114 139951642003264 submission.py:139] 10500) loss = nan, grad_norm = nan
I0519 01:46:49.298981 139916998596352 logging_writer.py:48] [11000] global_step=11000, grad_norm=nan, loss=nan
I0519 01:46:49.306084 139951642003264 submission.py:139] 11000) loss = nan, grad_norm = nan
I0519 01:53:04.073672 139917006989056 logging_writer.py:48] [11500] global_step=11500, grad_norm=nan, loss=nan
I0519 01:53:04.080210 139951642003264 submission.py:139] 11500) loss = nan, grad_norm = nan
I0519 01:59:17.183709 139916998596352 logging_writer.py:48] [12000] global_step=12000, grad_norm=nan, loss=nan
I0519 01:59:17.188059 139951642003264 submission.py:139] 12000) loss = nan, grad_norm = nan
I0519 02:05:31.811414 139917006989056 logging_writer.py:48] [12500] global_step=12500, grad_norm=nan, loss=nan
I0519 02:05:31.818677 139951642003264 submission.py:139] 12500) loss = nan, grad_norm = nan
I0519 02:08:03.284310 139951642003264 spec.py:298] Evaluating on the training split.
I0519 02:08:13.005554 139951642003264 spec.py:310] Evaluating on the validation split.
I0519 02:08:22.538488 139951642003264 spec.py:326] Evaluating on the test split.
I0519 02:08:27.513067 139951642003264 submission_runner.py:421] Time since start: 9743.95s, 	Step: 12704, 	{'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5508.7393345832825, 'total_duration': 9743.949453830719, 'accumulated_submission_time': 5508.7393345832825, 'accumulated_eval_time': 132.55700993537903, 'accumulated_logging_time': 0.12626910209655762}
I0519 02:08:27.534358 139917006989056 logging_writer.py:48] [12704] accumulated_eval_time=132.557010, accumulated_logging_time=0.126269, accumulated_submission_time=5508.739335, global_step=12704, preemption_count=0, score=5508.739335, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=9743.949454, train/ctc_loss=nan, train/wer=0.941556, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 02:12:09.166621 139916998596352 logging_writer.py:48] [13000] global_step=13000, grad_norm=nan, loss=nan
I0519 02:12:09.170663 139951642003264 submission.py:139] 13000) loss = nan, grad_norm = nan
I0519 02:18:23.886298 139917006989056 logging_writer.py:48] [13500] global_step=13500, grad_norm=nan, loss=nan
I0519 02:18:23.896338 139951642003264 submission.py:139] 13500) loss = nan, grad_norm = nan
I0519 02:24:37.071158 139916998596352 logging_writer.py:48] [14000] global_step=14000, grad_norm=nan, loss=nan
I0519 02:24:37.075523 139951642003264 submission.py:139] 14000) loss = nan, grad_norm = nan
I0519 02:30:51.744038 139917006989056 logging_writer.py:48] [14500] global_step=14500, grad_norm=nan, loss=nan
I0519 02:30:51.751897 139951642003264 submission.py:139] 14500) loss = nan, grad_norm = nan
I0519 02:37:04.892585 139916998596352 logging_writer.py:48] [15000] global_step=15000, grad_norm=nan, loss=nan
I0519 02:37:04.932812 139951642003264 submission.py:139] 15000) loss = nan, grad_norm = nan
I0519 02:43:19.562701 139917006989056 logging_writer.py:48] [15500] global_step=15500, grad_norm=nan, loss=nan
I0519 02:43:19.570878 139951642003264 submission.py:139] 15500) loss = nan, grad_norm = nan
I0519 02:48:28.440147 139951642003264 spec.py:298] Evaluating on the training split.
I0519 02:48:38.233459 139951642003264 spec.py:310] Evaluating on the validation split.
I0519 02:48:49.167102 139951642003264 spec.py:326] Evaluating on the test split.
I0519 02:48:54.102470 139951642003264 submission_runner.py:421] Time since start: 12170.54s, 	Step: 15915, 	{'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 6880.748918056488, 'total_duration': 12170.538833618164, 'accumulated_submission_time': 6880.748918056488, 'accumulated_eval_time': 158.2189817428589, 'accumulated_logging_time': 0.15880298614501953}
I0519 02:48:54.122807 139917006989056 logging_writer.py:48] [15915] accumulated_eval_time=158.218982, accumulated_logging_time=0.158803, accumulated_submission_time=6880.748918, global_step=15915, preemption_count=0, score=6880.748918, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=12170.538834, train/ctc_loss=nan, train/wer=0.941556, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 02:49:58.301803 139916998596352 logging_writer.py:48] [16000] global_step=16000, grad_norm=nan, loss=nan
I0519 02:49:58.305916 139951642003264 submission.py:139] 16000) loss = nan, grad_norm = nan
I0519 02:56:12.915798 139917006989056 logging_writer.py:48] [16500] global_step=16500, grad_norm=nan, loss=nan
I0519 02:56:12.922383 139951642003264 submission.py:139] 16500) loss = nan, grad_norm = nan
I0519 03:02:26.089346 139916998596352 logging_writer.py:48] [17000] global_step=17000, grad_norm=nan, loss=nan
I0519 03:02:26.117691 139951642003264 submission.py:139] 17000) loss = nan, grad_norm = nan
I0519 03:08:39.180853 139917006989056 logging_writer.py:48] [17500] global_step=17500, grad_norm=nan, loss=nan
I0519 03:08:39.190231 139951642003264 submission.py:139] 17500) loss = nan, grad_norm = nan
I0519 03:14:53.947192 139917006989056 logging_writer.py:48] [18000] global_step=18000, grad_norm=nan, loss=nan
I0519 03:14:53.956882 139951642003264 submission.py:139] 18000) loss = nan, grad_norm = nan
I0519 03:21:07.094739 139916998596352 logging_writer.py:48] [18500] global_step=18500, grad_norm=nan, loss=nan
I0519 03:21:07.124760 139951642003264 submission.py:139] 18500) loss = nan, grad_norm = nan
I0519 03:27:21.802290 139917006989056 logging_writer.py:48] [19000] global_step=19000, grad_norm=nan, loss=nan
I0519 03:27:21.810569 139951642003264 submission.py:139] 19000) loss = nan, grad_norm = nan
I0519 03:28:55.093630 139951642003264 spec.py:298] Evaluating on the training split.
I0519 03:29:04.924248 139951642003264 spec.py:310] Evaluating on the validation split.
I0519 03:29:14.161521 139951642003264 spec.py:326] Evaluating on the test split.
I0519 03:29:19.185724 139951642003264 submission_runner.py:421] Time since start: 14595.62s, 	Step: 19126, 	{'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8253.104804754257, 'total_duration': 14595.622115850449, 'accumulated_submission_time': 8253.104804754257, 'accumulated_eval_time': 182.31075286865234, 'accumulated_logging_time': 0.18806219100952148}
I0519 03:29:19.208456 139917006989056 logging_writer.py:48] [19126] accumulated_eval_time=182.310753, accumulated_logging_time=0.188062, accumulated_submission_time=8253.104805, global_step=19126, preemption_count=0, score=8253.104805, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=14595.622116, train/ctc_loss=nan, train/wer=0.941556, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 03:33:59.074983 139916998596352 logging_writer.py:48] [19500] global_step=19500, grad_norm=nan, loss=nan
I0519 03:33:59.079192 139951642003264 submission.py:139] 19500) loss = nan, grad_norm = nan
I0519 03:40:12.997492 139951642003264 spec.py:298] Evaluating on the training split.
I0519 03:40:22.340060 139951642003264 spec.py:310] Evaluating on the validation split.
I0519 03:40:31.912621 139951642003264 spec.py:326] Evaluating on the test split.
I0519 03:40:36.825968 139951642003264 submission_runner.py:421] Time since start: 15273.26s, 	Step: 20000, 	{'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8626.667088508606, 'total_duration': 15273.26236319542, 'accumulated_submission_time': 8626.667088508606, 'accumulated_eval_time': 206.13908863067627, 'accumulated_logging_time': 0.2197558879852295}
I0519 03:40:36.842771 139917006989056 logging_writer.py:48] [20000] accumulated_eval_time=206.139089, accumulated_logging_time=0.219756, accumulated_submission_time=8626.667089, global_step=20000, preemption_count=0, score=8626.667089, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=15273.262363, train/ctc_loss=nan, train/wer=0.941556, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 03:40:36.861972 139916998596352 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=8626.667089
I0519 03:40:37.287669 139951642003264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_pytorch/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_20000.
I0519 03:40:37.391649 139951642003264 submission_runner.py:584] Tuning trial 1/1
I0519 03:40:37.391948 139951642003264 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0519 03:40:37.392574 139951642003264 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.299088759326455, 'train/wer': 2.432714938767523, 'validation/ctc_loss': 30.347134317862167, 'validation/wer': 2.1297446048375415, 'validation/num_examples': 5348, 'test/ctc_loss': 30.396996580424542, 'test/wer': 2.203461093169216, 'test/num_examples': 2472, 'score': 7.881624937057495, 'total_duration': 42.33900046348572, 'accumulated_submission_time': 7.881624937057495, 'accumulated_eval_time': 34.456048250198364, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3078, {'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1390.1706454753876, 'total_duration': 2467.3637969493866, 'accumulated_submission_time': 1390.1706454753876, 'accumulated_eval_time': 58.61391472816467, 'accumulated_logging_time': 0.033032894134521484, 'global_step': 3078, 'preemption_count': 0}), (6285, {'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2763.820147037506, 'total_duration': 4893.7668833732605, 'accumulated_submission_time': 2763.820147037506, 'accumulated_eval_time': 83.93518161773682, 'accumulated_logging_time': 0.06403517723083496, 'global_step': 6285, 'preemption_count': 0}), (9494, {'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4136.319772720337, 'total_duration': 7318.937154531479, 'accumulated_submission_time': 4136.319772720337, 'accumulated_eval_time': 108.32850933074951, 'accumulated_logging_time': 0.09587931632995605, 'global_step': 9494, 'preemption_count': 0}), (12704, {'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5508.7393345832825, 'total_duration': 9743.949453830719, 'accumulated_submission_time': 5508.7393345832825, 'accumulated_eval_time': 132.55700993537903, 'accumulated_logging_time': 0.12626910209655762, 'global_step': 12704, 'preemption_count': 0}), (15915, {'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 6880.748918056488, 'total_duration': 12170.538833618164, 'accumulated_submission_time': 6880.748918056488, 'accumulated_eval_time': 158.2189817428589, 'accumulated_logging_time': 0.15880298614501953, 'global_step': 15915, 'preemption_count': 0}), (19126, {'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8253.104804754257, 'total_duration': 14595.622115850449, 'accumulated_submission_time': 8253.104804754257, 'accumulated_eval_time': 182.31075286865234, 'accumulated_logging_time': 0.18806219100952148, 'global_step': 19126, 'preemption_count': 0}), (20000, {'train/ctc_loss': nan, 'train/wer': 0.9415560061096586, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8626.667088508606, 'total_duration': 15273.26236319542, 'accumulated_submission_time': 8626.667088508606, 'accumulated_eval_time': 206.13908863067627, 'accumulated_logging_time': 0.2197558879852295, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0519 03:40:37.392745 139951642003264 submission_runner.py:587] Timing: 8626.667088508606
I0519 03:40:37.392854 139951642003264 submission_runner.py:588] ====================
I0519 03:40:37.393088 139951642003264 submission_runner.py:651] Final librispeech_conformer score: 8626.667088508606
