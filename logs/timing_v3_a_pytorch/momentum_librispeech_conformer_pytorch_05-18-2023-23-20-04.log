torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_conformer --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_pytorch/timing_momentum --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_pytorch_05-18-2023-23-20-04.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0518 23:20:28.076007 140190824081216 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0518 23:20:28.076050 140218047719232 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0518 23:20:28.076027 140365394257728 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0518 23:20:28.076600 139926308042560 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0518 23:20:28.076940 140065664665408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0518 23:20:29.055000 140580528817984 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0518 23:20:29.055079 140157149701952 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0518 23:20:29.062877 140179873642304 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0518 23:20:29.063297 140179873642304 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:20:29.065536 140190824081216 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:20:29.065563 140218047719232 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:20:29.065595 139926308042560 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:20:29.065658 140580528817984 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:20:29.065698 140157149701952 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:20:29.067105 140365394257728 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:20:29.071778 140065664665408 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 23:20:29.445797 140179873642304 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_pytorch/timing_momentum/librispeech_conformer_pytorch.
W0518 23:20:29.773183 140179873642304 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 23:20:29.773970 140190824081216 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 23:20:29.774493 139926308042560 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 23:20:29.774767 140218047719232 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 23:20:29.775863 140157149701952 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 23:20:29.775939 140580528817984 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0518 23:20:29.779995 140179873642304 submission_runner.py:544] Using RNG seed 3371909343
I0518 23:20:29.781464 140179873642304 submission_runner.py:553] --- Tuning run 1/1 ---
I0518 23:20:29.781608 140179873642304 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_pytorch/timing_momentum/librispeech_conformer_pytorch/trial_1.
I0518 23:20:29.782016 140179873642304 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_pytorch/timing_momentum/librispeech_conformer_pytorch/trial_1/hparams.json.
I0518 23:20:29.783147 140179873642304 submission_runner.py:241] Initializing dataset.
I0518 23:20:29.783287 140179873642304 input_pipeline.py:20] Loading split = train-clean-100
W0518 23:20:29.784349 140065664665408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 23:20:29.786063 140365394257728 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0518 23:20:29.821268 140179873642304 input_pipeline.py:20] Loading split = train-clean-360
I0518 23:20:30.160239 140179873642304 input_pipeline.py:20] Loading split = train-other-500
I0518 23:20:30.611852 140179873642304 submission_runner.py:248] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0518 23:20:37.464389 140179873642304 submission_runner.py:258] Initializing optimizer.
I0518 23:20:37.959785 140179873642304 submission_runner.py:265] Initializing metrics bundle.
I0518 23:20:37.959981 140179873642304 submission_runner.py:283] Initializing checkpoint and logger.
I0518 23:20:37.961931 140179873642304 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0518 23:20:37.962047 140179873642304 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0518 23:20:38.503727 140179873642304 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_pytorch/timing_momentum/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0518 23:20:38.504870 140179873642304 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_pytorch/timing_momentum/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0518 23:20:38.513948 140179873642304 submission_runner.py:319] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0518 23:20:46.538057 140153540572928 logging_writer.py:48] [0] global_step=0, grad_norm=27.940323, loss=33.177578
I0518 23:20:46.559647 140179873642304 submission.py:139] 0) loss = 33.178, grad_norm = 27.940
I0518 23:20:46.561030 140179873642304 spec.py:298] Evaluating on the training split.
I0518 23:20:46.562244 140179873642304 input_pipeline.py:20] Loading split = train-clean-100
I0518 23:20:46.595932 140179873642304 input_pipeline.py:20] Loading split = train-clean-360
I0518 23:20:47.030362 140179873642304 input_pipeline.py:20] Loading split = train-other-500
I0518 23:21:00.800947 140179873642304 spec.py:310] Evaluating on the validation split.
I0518 23:21:00.802271 140179873642304 input_pipeline.py:20] Loading split = dev-clean
I0518 23:21:00.806817 140179873642304 input_pipeline.py:20] Loading split = dev-other
I0518 23:21:10.905951 140179873642304 spec.py:326] Evaluating on the test split.
I0518 23:21:10.907346 140179873642304 input_pipeline.py:20] Loading split = test-clean
I0518 23:21:16.202016 140179873642304 submission_runner.py:421] Time since start: 37.69s, 	Step: 1, 	{'train/ctc_loss': 32.40272822593823, 'train/wer': 1.0461815589405343, 'validation/ctc_loss': 31.013116586296967, 'validation/wer': 1.0537054024042871, 'validation/num_examples': 5348, 'test/ctc_loss': 31.12825343570553, 'test/wer': 1.0560396482034409, 'test/num_examples': 2472, 'score': 8.046057224273682, 'total_duration': 37.68837118148804, 'accumulated_submission_time': 8.046057224273682, 'accumulated_eval_time': 29.640853881835938, 'accumulated_logging_time': 0}
I0518 23:21:16.225588 140151250478848 logging_writer.py:48] [1] accumulated_eval_time=29.640854, accumulated_logging_time=0, accumulated_submission_time=8.046057, global_step=1, preemption_count=0, score=8.046057, test/ctc_loss=31.128253, test/num_examples=2472, test/wer=1.056040, total_duration=37.688371, train/ctc_loss=32.402728, train/wer=1.046182, validation/ctc_loss=31.013117, validation/num_examples=5348, validation/wer=1.053705
I0518 23:21:16.271734 140179873642304 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:21:16.271781 139926308042560 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:21:16.271847 140065664665408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:21:16.271885 140218047719232 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:21:16.271846 140580528817984 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:21:16.272564 140365394257728 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:21:16.273125 140157149701952 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:21:16.273523 140190824081216 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 23:21:17.373374 140151242086144 logging_writer.py:48] [1] global_step=1, grad_norm=26.287708, loss=32.578251
I0518 23:21:17.376750 140179873642304 submission.py:139] 1) loss = 32.578, grad_norm = 26.288
I0518 23:21:18.232491 140151250478848 logging_writer.py:48] [2] global_step=2, grad_norm=28.521887, loss=32.980911
I0518 23:21:18.235567 140179873642304 submission.py:139] 2) loss = 32.981, grad_norm = 28.522
I0518 23:21:19.228576 140151242086144 logging_writer.py:48] [3] global_step=3, grad_norm=30.697445, loss=32.857201
I0518 23:21:19.231948 140179873642304 submission.py:139] 3) loss = 32.857, grad_norm = 30.697
I0518 23:21:20.027967 140151250478848 logging_writer.py:48] [4] global_step=4, grad_norm=40.696136, loss=31.653244
I0518 23:21:20.031389 140179873642304 submission.py:139] 4) loss = 31.653, grad_norm = 40.696
I0518 23:21:20.827104 140151242086144 logging_writer.py:48] [5] global_step=5, grad_norm=57.094559, loss=30.286026
I0518 23:21:20.830161 140179873642304 submission.py:139] 5) loss = 30.286, grad_norm = 57.095
I0518 23:21:21.623830 140151250478848 logging_writer.py:48] [6] global_step=6, grad_norm=77.057434, loss=26.634653
I0518 23:21:21.626918 140179873642304 submission.py:139] 6) loss = 26.635, grad_norm = 77.057
I0518 23:21:22.422977 140151242086144 logging_writer.py:48] [7] global_step=7, grad_norm=56.676102, loss=21.500757
I0518 23:21:22.426372 140179873642304 submission.py:139] 7) loss = 21.501, grad_norm = 56.676
I0518 23:21:23.221987 140151250478848 logging_writer.py:48] [8] global_step=8, grad_norm=46.674644, loss=19.507912
I0518 23:21:23.225176 140179873642304 submission.py:139] 8) loss = 19.508, grad_norm = 46.675
I0518 23:21:24.015750 140151242086144 logging_writer.py:48] [9] global_step=9, grad_norm=95.848808, loss=21.707745
I0518 23:21:24.019005 140179873642304 submission.py:139] 9) loss = 21.708, grad_norm = 95.849
I0518 23:21:24.809955 140151250478848 logging_writer.py:48] [10] global_step=10, grad_norm=66.341309, loss=17.086411
I0518 23:21:24.813172 140179873642304 submission.py:139] 10) loss = 17.086, grad_norm = 66.341
I0518 23:21:25.607266 140151242086144 logging_writer.py:48] [11] global_step=11, grad_norm=74.621780, loss=13.166773
I0518 23:21:25.610448 140179873642304 submission.py:139] 11) loss = 13.167, grad_norm = 74.622
I0518 23:21:26.406247 140151250478848 logging_writer.py:48] [12] global_step=12, grad_norm=18.461460, loss=8.597200
I0518 23:21:26.409540 140179873642304 submission.py:139] 12) loss = 8.597, grad_norm = 18.461
I0518 23:21:27.202538 140151242086144 logging_writer.py:48] [13] global_step=13, grad_norm=27.844419, loss=10.619026
I0518 23:21:27.206039 140179873642304 submission.py:139] 13) loss = 10.619, grad_norm = 27.844
I0518 23:21:27.999069 140151250478848 logging_writer.py:48] [14] global_step=14, grad_norm=28.058722, loss=11.749526
I0518 23:21:28.002479 140179873642304 submission.py:139] 14) loss = 11.750, grad_norm = 28.059
I0518 23:21:28.795340 140151242086144 logging_writer.py:48] [15] global_step=15, grad_norm=28.470135, loss=11.823719
I0518 23:21:28.798826 140179873642304 submission.py:139] 15) loss = 11.824, grad_norm = 28.470
I0518 23:21:29.592963 140151250478848 logging_writer.py:48] [16] global_step=16, grad_norm=29.063301, loss=10.780519
I0518 23:21:29.596190 140179873642304 submission.py:139] 16) loss = 10.781, grad_norm = 29.063
I0518 23:21:30.388194 140151242086144 logging_writer.py:48] [17] global_step=17, grad_norm=27.699909, loss=8.782475
I0518 23:21:30.391516 140179873642304 submission.py:139] 17) loss = 8.782, grad_norm = 27.700
I0518 23:21:31.184292 140151250478848 logging_writer.py:48] [18] global_step=18, grad_norm=16.638491, loss=7.293470
I0518 23:21:31.187613 140179873642304 submission.py:139] 18) loss = 7.293, grad_norm = 16.638
I0518 23:21:31.982566 140151242086144 logging_writer.py:48] [19] global_step=19, grad_norm=112.077019, loss=12.460858
I0518 23:21:31.985781 140179873642304 submission.py:139] 19) loss = 12.461, grad_norm = 112.077
I0518 23:21:32.781878 140151250478848 logging_writer.py:48] [20] global_step=20, grad_norm=33.907860, loss=7.632505
I0518 23:21:32.785345 140179873642304 submission.py:139] 20) loss = 7.633, grad_norm = 33.908
I0518 23:21:33.579259 140151242086144 logging_writer.py:48] [21] global_step=21, grad_norm=25.006031, loss=8.238338
I0518 23:21:33.582525 140179873642304 submission.py:139] 21) loss = 8.238, grad_norm = 25.006
I0518 23:21:34.374280 140151250478848 logging_writer.py:48] [22] global_step=22, grad_norm=27.367184, loss=9.951748
I0518 23:21:34.377661 140179873642304 submission.py:139] 22) loss = 9.952, grad_norm = 27.367
I0518 23:21:35.171750 140151242086144 logging_writer.py:48] [23] global_step=23, grad_norm=27.011820, loss=10.580094
I0518 23:21:35.175050 140179873642304 submission.py:139] 23) loss = 10.580, grad_norm = 27.012
I0518 23:21:35.969877 140151250478848 logging_writer.py:48] [24] global_step=24, grad_norm=26.456335, loss=10.003573
I0518 23:21:35.973343 140179873642304 submission.py:139] 24) loss = 10.004, grad_norm = 26.456
I0518 23:21:36.766803 140151242086144 logging_writer.py:48] [25] global_step=25, grad_norm=24.128906, loss=8.389819
I0518 23:21:36.770126 140179873642304 submission.py:139] 25) loss = 8.390, grad_norm = 24.129
I0518 23:21:37.561925 140151250478848 logging_writer.py:48] [26] global_step=26, grad_norm=4.457510, loss=6.896590
I0518 23:21:37.565304 140179873642304 submission.py:139] 26) loss = 6.897, grad_norm = 4.458
I0518 23:21:38.363309 140151242086144 logging_writer.py:48] [27] global_step=27, grad_norm=82.133057, loss=10.559522
I0518 23:21:38.366837 140179873642304 submission.py:139] 27) loss = 10.560, grad_norm = 82.133
I0518 23:21:39.162889 140151250478848 logging_writer.py:48] [28] global_step=28, grad_norm=24.112793, loss=7.194541
I0518 23:21:39.166035 140179873642304 submission.py:139] 28) loss = 7.195, grad_norm = 24.113
I0518 23:21:39.957993 140151242086144 logging_writer.py:48] [29] global_step=29, grad_norm=20.692665, loss=7.644346
I0518 23:21:39.961558 140179873642304 submission.py:139] 29) loss = 7.644, grad_norm = 20.693
I0518 23:21:40.753921 140151250478848 logging_writer.py:48] [30] global_step=30, grad_norm=24.189293, loss=8.936613
I0518 23:21:40.757021 140179873642304 submission.py:139] 30) loss = 8.937, grad_norm = 24.189
I0518 23:21:41.551514 140151242086144 logging_writer.py:48] [31] global_step=31, grad_norm=24.127750, loss=9.021825
I0518 23:21:41.554576 140179873642304 submission.py:139] 31) loss = 9.022, grad_norm = 24.128
I0518 23:21:42.350385 140151250478848 logging_writer.py:48] [32] global_step=32, grad_norm=21.969530, loss=8.013012
I0518 23:21:42.353571 140179873642304 submission.py:139] 32) loss = 8.013, grad_norm = 21.970
I0518 23:21:43.145587 140151242086144 logging_writer.py:48] [33] global_step=33, grad_norm=2.494585, loss=6.729656
I0518 23:21:43.148960 140179873642304 submission.py:139] 33) loss = 6.730, grad_norm = 2.495
I0518 23:21:43.939077 140151250478848 logging_writer.py:48] [34] global_step=34, grad_norm=65.071487, loss=9.206922
I0518 23:21:43.942415 140179873642304 submission.py:139] 34) loss = 9.207, grad_norm = 65.071
I0518 23:21:44.734472 140151242086144 logging_writer.py:48] [35] global_step=35, grad_norm=13.437522, loss=6.731090
I0518 23:21:44.737793 140179873642304 submission.py:139] 35) loss = 6.731, grad_norm = 13.438
I0518 23:21:45.533703 140151250478848 logging_writer.py:48] [36] global_step=36, grad_norm=19.072052, loss=7.369344
I0518 23:21:45.537253 140179873642304 submission.py:139] 36) loss = 7.369, grad_norm = 19.072
I0518 23:21:46.330576 140151242086144 logging_writer.py:48] [37] global_step=37, grad_norm=22.260853, loss=8.133678
I0518 23:21:46.334020 140179873642304 submission.py:139] 37) loss = 8.134, grad_norm = 22.261
I0518 23:21:47.124836 140151250478848 logging_writer.py:48] [38] global_step=38, grad_norm=21.028625, loss=7.770087
I0518 23:21:47.128035 140179873642304 submission.py:139] 38) loss = 7.770, grad_norm = 21.029
I0518 23:21:47.922436 140151242086144 logging_writer.py:48] [39] global_step=39, grad_norm=8.256638, loss=6.612497
I0518 23:21:47.925932 140179873642304 submission.py:139] 39) loss = 6.612, grad_norm = 8.257
I0518 23:21:48.720923 140151250478848 logging_writer.py:48] [40] global_step=40, grad_norm=47.225819, loss=7.949223
I0518 23:21:48.724092 140179873642304 submission.py:139] 40) loss = 7.949, grad_norm = 47.226
I0518 23:21:49.515714 140151242086144 logging_writer.py:48] [41] global_step=41, grad_norm=15.784369, loss=6.698298
I0518 23:21:49.518770 140179873642304 submission.py:139] 41) loss = 6.698, grad_norm = 15.784
I0518 23:21:50.311174 140151250478848 logging_writer.py:48] [42] global_step=42, grad_norm=16.220959, loss=6.985343
I0518 23:21:50.314252 140179873642304 submission.py:139] 42) loss = 6.985, grad_norm = 16.221
I0518 23:21:51.106727 140151242086144 logging_writer.py:48] [43] global_step=43, grad_norm=20.139744, loss=7.549314
I0518 23:21:51.110087 140179873642304 submission.py:139] 43) loss = 7.549, grad_norm = 20.140
I0518 23:21:51.902959 140151250478848 logging_writer.py:48] [44] global_step=44, grad_norm=16.823250, loss=7.017975
I0518 23:21:51.906309 140179873642304 submission.py:139] 44) loss = 7.018, grad_norm = 16.823
I0518 23:21:52.697477 140151242086144 logging_writer.py:48] [45] global_step=45, grad_norm=9.015448, loss=6.522747
I0518 23:21:52.701293 140179873642304 submission.py:139] 45) loss = 6.523, grad_norm = 9.015
I0518 23:21:53.495517 140151250478848 logging_writer.py:48] [46] global_step=46, grad_norm=39.516426, loss=7.536267
I0518 23:21:53.498649 140179873642304 submission.py:139] 46) loss = 7.536, grad_norm = 39.516
I0518 23:21:54.293728 140151242086144 logging_writer.py:48] [47] global_step=47, grad_norm=6.377920, loss=6.444239
I0518 23:21:54.296966 140179873642304 submission.py:139] 47) loss = 6.444, grad_norm = 6.378
I0518 23:21:55.092742 140151250478848 logging_writer.py:48] [48] global_step=48, grad_norm=18.062174, loss=7.139456
I0518 23:21:55.095914 140179873642304 submission.py:139] 48) loss = 7.139, grad_norm = 18.062
I0518 23:21:55.889107 140151242086144 logging_writer.py:48] [49] global_step=49, grad_norm=17.164402, loss=6.976909
I0518 23:21:55.892394 140179873642304 submission.py:139] 49) loss = 6.977, grad_norm = 17.164
I0518 23:21:56.685544 140151250478848 logging_writer.py:48] [50] global_step=50, grad_norm=1.041836, loss=6.340807
I0518 23:21:56.688605 140179873642304 submission.py:139] 50) loss = 6.341, grad_norm = 1.042
I0518 23:21:57.482861 140151242086144 logging_writer.py:48] [51] global_step=51, grad_norm=36.604084, loss=7.266884
I0518 23:21:57.485934 140179873642304 submission.py:139] 51) loss = 7.267, grad_norm = 36.604
I0518 23:21:58.277245 140151250478848 logging_writer.py:48] [52] global_step=52, grad_norm=3.345163, loss=6.274367
I0518 23:21:58.280511 140179873642304 submission.py:139] 52) loss = 6.274, grad_norm = 3.345
I0518 23:21:59.074502 140151242086144 logging_writer.py:48] [53] global_step=53, grad_norm=16.416346, loss=6.825673
I0518 23:21:59.077752 140179873642304 submission.py:139] 53) loss = 6.826, grad_norm = 16.416
I0518 23:21:59.871370 140151250478848 logging_writer.py:48] [54] global_step=54, grad_norm=14.582225, loss=6.651113
I0518 23:21:59.874946 140179873642304 submission.py:139] 54) loss = 6.651, grad_norm = 14.582
I0518 23:22:00.671720 140151242086144 logging_writer.py:48] [55] global_step=55, grad_norm=6.923675, loss=6.289557
I0518 23:22:00.674841 140179873642304 submission.py:139] 55) loss = 6.290, grad_norm = 6.924
I0518 23:22:01.471645 140151250478848 logging_writer.py:48] [56] global_step=56, grad_norm=27.287874, loss=6.783250
I0518 23:22:01.475417 140179873642304 submission.py:139] 56) loss = 6.783, grad_norm = 27.288
I0518 23:22:02.268598 140151242086144 logging_writer.py:48] [57] global_step=57, grad_norm=8.422167, loss=6.358773
I0518 23:22:02.271844 140179873642304 submission.py:139] 57) loss = 6.359, grad_norm = 8.422
I0518 23:22:03.064554 140151250478848 logging_writer.py:48] [58] global_step=58, grad_norm=16.167183, loss=6.754140
I0518 23:22:03.067876 140179873642304 submission.py:139] 58) loss = 6.754, grad_norm = 16.167
I0518 23:22:03.863812 140151242086144 logging_writer.py:48] [59] global_step=59, grad_norm=9.732207, loss=6.373901
I0518 23:22:03.866903 140179873642304 submission.py:139] 59) loss = 6.374, grad_norm = 9.732
I0518 23:22:04.659441 140151250478848 logging_writer.py:48] [60] global_step=60, grad_norm=21.544743, loss=6.600845
I0518 23:22:04.662683 140179873642304 submission.py:139] 60) loss = 6.601, grad_norm = 21.545
I0518 23:22:05.455720 140151242086144 logging_writer.py:48] [61] global_step=61, grad_norm=7.230740, loss=6.250448
I0518 23:22:05.459171 140179873642304 submission.py:139] 61) loss = 6.250, grad_norm = 7.231
I0518 23:22:06.253223 140151250478848 logging_writer.py:48] [62] global_step=62, grad_norm=11.694763, loss=6.406112
I0518 23:22:06.256379 140179873642304 submission.py:139] 62) loss = 6.406, grad_norm = 11.695
I0518 23:22:07.049599 140151242086144 logging_writer.py:48] [63] global_step=63, grad_norm=11.557063, loss=6.374307
I0518 23:22:07.052791 140179873642304 submission.py:139] 63) loss = 6.374, grad_norm = 11.557
I0518 23:22:07.847774 140151250478848 logging_writer.py:48] [64] global_step=64, grad_norm=6.328485, loss=6.191133
I0518 23:22:07.851338 140179873642304 submission.py:139] 64) loss = 6.191, grad_norm = 6.328
I0518 23:22:08.644416 140151242086144 logging_writer.py:48] [65] global_step=65, grad_norm=17.461416, loss=6.446557
I0518 23:22:08.647607 140179873642304 submission.py:139] 65) loss = 6.447, grad_norm = 17.461
I0518 23:22:09.439394 140151250478848 logging_writer.py:48] [66] global_step=66, grad_norm=8.864749, loss=6.215778
I0518 23:22:09.442685 140179873642304 submission.py:139] 66) loss = 6.216, grad_norm = 8.865
I0518 23:22:10.239651 140151242086144 logging_writer.py:48] [67] global_step=67, grad_norm=12.716307, loss=6.395023
I0518 23:22:10.242941 140179873642304 submission.py:139] 67) loss = 6.395, grad_norm = 12.716
I0518 23:22:11.033711 140151250478848 logging_writer.py:48] [68] global_step=68, grad_norm=1.059966, loss=6.108724
I0518 23:22:11.036892 140179873642304 submission.py:139] 68) loss = 6.109, grad_norm = 1.060
I0518 23:22:11.829296 140151242086144 logging_writer.py:48] [69] global_step=69, grad_norm=22.360075, loss=6.538385
I0518 23:22:11.832734 140179873642304 submission.py:139] 69) loss = 6.538, grad_norm = 22.360
I0518 23:22:12.624801 140151250478848 logging_writer.py:48] [70] global_step=70, grad_norm=9.934147, loss=6.227281
I0518 23:22:12.627960 140179873642304 submission.py:139] 70) loss = 6.227, grad_norm = 9.934
I0518 23:22:13.424460 140151242086144 logging_writer.py:48] [71] global_step=71, grad_norm=13.860396, loss=6.438865
I0518 23:22:13.427507 140179873642304 submission.py:139] 71) loss = 6.439, grad_norm = 13.860
I0518 23:22:14.220899 140151250478848 logging_writer.py:48] [72] global_step=72, grad_norm=0.733243, loss=6.051737
I0518 23:22:14.224233 140179873642304 submission.py:139] 72) loss = 6.052, grad_norm = 0.733
I0518 23:22:15.015738 140151242086144 logging_writer.py:48] [73] global_step=73, grad_norm=25.336994, loss=6.572026
I0518 23:22:15.019066 140179873642304 submission.py:139] 73) loss = 6.572, grad_norm = 25.337
I0518 23:22:15.814125 140151250478848 logging_writer.py:48] [74] global_step=74, grad_norm=11.758383, loss=6.294543
I0518 23:22:15.817485 140179873642304 submission.py:139] 74) loss = 6.295, grad_norm = 11.758
I0518 23:22:16.614622 140151242086144 logging_writer.py:48] [75] global_step=75, grad_norm=15.318960, loss=6.521652
I0518 23:22:16.617670 140179873642304 submission.py:139] 75) loss = 6.522, grad_norm = 15.319
I0518 23:22:17.411365 140151250478848 logging_writer.py:48] [76] global_step=76, grad_norm=1.305156, loss=6.027821
I0518 23:22:17.414511 140179873642304 submission.py:139] 76) loss = 6.028, grad_norm = 1.305
I0518 23:22:18.208391 140151242086144 logging_writer.py:48] [77] global_step=77, grad_norm=29.498388, loss=6.731513
I0518 23:22:18.211567 140179873642304 submission.py:139] 77) loss = 6.732, grad_norm = 29.498
I0518 23:22:19.005290 140151250478848 logging_writer.py:48] [78] global_step=78, grad_norm=14.901748, loss=6.485793
I0518 23:22:19.008463 140179873642304 submission.py:139] 78) loss = 6.486, grad_norm = 14.902
I0518 23:22:19.805404 140151242086144 logging_writer.py:48] [79] global_step=79, grad_norm=17.793098, loss=6.818643
I0518 23:22:19.808597 140179873642304 submission.py:139] 79) loss = 6.819, grad_norm = 17.793
I0518 23:22:20.604028 140151250478848 logging_writer.py:48] [80] global_step=80, grad_norm=0.656640, loss=6.008243
I0518 23:22:20.607263 140179873642304 submission.py:139] 80) loss = 6.008, grad_norm = 0.657
I0518 23:22:21.401643 140151242086144 logging_writer.py:48] [81] global_step=81, grad_norm=41.470863, loss=7.288933
I0518 23:22:21.405247 140179873642304 submission.py:139] 81) loss = 7.289, grad_norm = 41.471
I0518 23:22:22.198092 140151250478848 logging_writer.py:48] [82] global_step=82, grad_norm=19.082561, loss=7.042330
I0518 23:22:22.201299 140179873642304 submission.py:139] 82) loss = 7.042, grad_norm = 19.083
I0518 23:22:22.992852 140151242086144 logging_writer.py:48] [83] global_step=83, grad_norm=21.309437, loss=7.924029
I0518 23:22:22.996377 140179873642304 submission.py:139] 83) loss = 7.924, grad_norm = 21.309
I0518 23:22:23.789321 140151250478848 logging_writer.py:48] [84] global_step=84, grad_norm=15.494226, loss=6.494511
I0518 23:22:23.792491 140179873642304 submission.py:139] 84) loss = 6.495, grad_norm = 15.494
I0518 23:22:24.586623 140151242086144 logging_writer.py:48] [85] global_step=85, grad_norm=62.870438, loss=8.921201
I0518 23:22:24.590066 140179873642304 submission.py:139] 85) loss = 8.921, grad_norm = 62.870
I0518 23:22:25.382620 140151250478848 logging_writer.py:48] [86] global_step=86, grad_norm=20.089308, loss=7.333080
I0518 23:22:25.385925 140179873642304 submission.py:139] 86) loss = 7.333, grad_norm = 20.089
I0518 23:22:26.181649 140151242086144 logging_writer.py:48] [87] global_step=87, grad_norm=22.132933, loss=9.017818
I0518 23:22:26.184963 140179873642304 submission.py:139] 87) loss = 9.018, grad_norm = 22.133
I0518 23:22:26.978067 140151250478848 logging_writer.py:48] [88] global_step=88, grad_norm=21.052578, loss=7.854158
I0518 23:22:26.981927 140179873642304 submission.py:139] 88) loss = 7.854, grad_norm = 21.053
I0518 23:22:27.777022 140151242086144 logging_writer.py:48] [89] global_step=89, grad_norm=23.021481, loss=6.443686
I0518 23:22:27.780132 140179873642304 submission.py:139] 89) loss = 6.444, grad_norm = 23.021
I0518 23:22:28.573859 140151250478848 logging_writer.py:48] [90] global_step=90, grad_norm=40.332748, loss=7.243649
I0518 23:22:28.576943 140179873642304 submission.py:139] 90) loss = 7.244, grad_norm = 40.333
I0518 23:22:29.375259 140151242086144 logging_writer.py:48] [91] global_step=91, grad_norm=21.296955, loss=8.132955
I0518 23:22:29.378328 140179873642304 submission.py:139] 91) loss = 8.133, grad_norm = 21.297
I0518 23:22:30.171145 140151250478848 logging_writer.py:48] [92] global_step=92, grad_norm=22.120058, loss=9.706876
I0518 23:22:30.174538 140179873642304 submission.py:139] 92) loss = 9.707, grad_norm = 22.120
I0518 23:22:30.968613 140151242086144 logging_writer.py:48] [93] global_step=93, grad_norm=21.350946, loss=8.266113
I0518 23:22:30.971738 140179873642304 submission.py:139] 93) loss = 8.266, grad_norm = 21.351
I0518 23:22:31.767130 140151250478848 logging_writer.py:48] [94] global_step=94, grad_norm=20.798721, loss=6.344044
I0518 23:22:31.770379 140179873642304 submission.py:139] 94) loss = 6.344, grad_norm = 20.799
I0518 23:22:32.563076 140151242086144 logging_writer.py:48] [95] global_step=95, grad_norm=54.714066, loss=8.308043
I0518 23:22:32.566282 140179873642304 submission.py:139] 95) loss = 8.308, grad_norm = 54.714
I0518 23:22:33.361205 140151250478848 logging_writer.py:48] [96] global_step=96, grad_norm=21.817698, loss=9.295088
I0518 23:22:33.364435 140179873642304 submission.py:139] 96) loss = 9.295, grad_norm = 21.818
I0518 23:22:34.160229 140151242086144 logging_writer.py:48] [97] global_step=97, grad_norm=22.057627, loss=12.297264
I0518 23:22:34.163506 140179873642304 submission.py:139] 97) loss = 12.297, grad_norm = 22.058
I0518 23:22:34.959240 140151250478848 logging_writer.py:48] [98] global_step=98, grad_norm=22.009024, loss=12.003534
I0518 23:22:34.962672 140179873642304 submission.py:139] 98) loss = 12.004, grad_norm = 22.009
I0518 23:22:35.755101 140151242086144 logging_writer.py:48] [99] global_step=99, grad_norm=21.293135, loss=8.640014
I0518 23:22:35.758932 140179873642304 submission.py:139] 99) loss = 8.640, grad_norm = 21.293
I0518 23:22:36.553812 140151250478848 logging_writer.py:48] [100] global_step=100, grad_norm=63.542820, loss=9.299159
I0518 23:22:36.557196 140179873642304 submission.py:139] 100) loss = 9.299, grad_norm = 63.543
I0518 23:27:42.366773 140151242086144 logging_writer.py:48] [500] global_step=500, grad_norm=nan, loss=nan
I0518 23:27:42.370958 140179873642304 submission.py:139] 500) loss = nan, grad_norm = nan
I0518 23:33:56.461630 140151250478848 logging_writer.py:48] [1000] global_step=1000, grad_norm=nan, loss=nan
I0518 23:33:56.466277 140179873642304 submission.py:139] 1000) loss = nan, grad_norm = nan
I0518 23:40:12.255013 140151250478848 logging_writer.py:48] [1500] global_step=1500, grad_norm=nan, loss=nan
I0518 23:40:12.262356 140179873642304 submission.py:139] 1500) loss = nan, grad_norm = nan
I0518 23:46:26.467286 140151242086144 logging_writer.py:48] [2000] global_step=2000, grad_norm=nan, loss=nan
I0518 23:46:26.472506 140179873642304 submission.py:139] 2000) loss = nan, grad_norm = nan
I0518 23:52:42.217232 140151242086144 logging_writer.py:48] [2500] global_step=2500, grad_norm=nan, loss=nan
I0518 23:52:42.225587 140179873642304 submission.py:139] 2500) loss = nan, grad_norm = nan
I0518 23:58:56.267746 140150990436096 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0518 23:58:56.274230 140179873642304 submission.py:139] 3000) loss = nan, grad_norm = nan
I0519 00:01:17.095956 140179873642304 spec.py:298] Evaluating on the training split.
I0519 00:01:26.928731 140179873642304 spec.py:310] Evaluating on the validation split.
I0519 00:01:36.162025 140179873642304 spec.py:326] Evaluating on the test split.
I0519 00:01:41.210125 140179873642304 submission_runner.py:421] Time since start: 2462.70s, 	Step: 3187, 	{'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1304.2159271240234, 'total_duration': 2462.6964650154114, 'accumulated_submission_time': 1304.2159271240234, 'accumulated_eval_time': 53.75495266914368, 'accumulated_logging_time': 0.03196883201599121}
I0519 00:01:41.230821 140151242086144 logging_writer.py:48] [3187] accumulated_eval_time=53.754953, accumulated_logging_time=0.031969, accumulated_submission_time=1304.215927, global_step=3187, preemption_count=0, score=1304.215927, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2462.696465, train/ctc_loss=nan, train/wer=0.941487, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 00:05:36.162838 140150990436096 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0519 00:05:36.171123 140179873642304 submission.py:139] 3500) loss = nan, grad_norm = nan
I0519 00:11:50.224730 140151242086144 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0519 00:11:50.235097 140179873642304 submission.py:139] 4000) loss = nan, grad_norm = nan
I0519 00:18:05.813126 140150990436096 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0519 00:18:05.819086 140179873642304 submission.py:139] 4500) loss = nan, grad_norm = nan
I0519 00:24:19.878500 140150982043392 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0519 00:24:19.883204 140179873642304 submission.py:139] 5000) loss = nan, grad_norm = nan
I0519 00:30:35.429875 140150990436096 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0519 00:30:35.436068 140179873642304 submission.py:139] 5500) loss = nan, grad_norm = nan
I0519 00:36:49.400394 140150982043392 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0519 00:36:49.404483 140179873642304 submission.py:139] 6000) loss = nan, grad_norm = nan
I0519 00:41:41.884376 140179873642304 spec.py:298] Evaluating on the training split.
I0519 00:41:51.669656 140179873642304 spec.py:310] Evaluating on the validation split.
I0519 00:42:00.898682 140179873642304 spec.py:326] Evaluating on the test split.
I0519 00:42:06.208067 140179873642304 submission_runner.py:421] Time since start: 4887.69s, 	Step: 6390, 	{'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2562.563961982727, 'total_duration': 4887.694297552109, 'accumulated_submission_time': 2562.563961982727, 'accumulated_eval_time': 78.0784318447113, 'accumulated_logging_time': 0.0629429817199707}
I0519 00:42:06.227668 140150982043392 logging_writer.py:48] [6390] accumulated_eval_time=78.078432, accumulated_logging_time=0.062943, accumulated_submission_time=2562.563962, global_step=6390, preemption_count=0, score=2562.563962, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=4887.694298, train/ctc_loss=nan, train/wer=0.941487, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 00:43:29.273211 140150973650688 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0519 00:43:29.277379 140179873642304 submission.py:139] 6500) loss = nan, grad_norm = nan
I0519 00:49:43.280856 140150982043392 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0519 00:49:43.285810 140179873642304 submission.py:139] 7000) loss = nan, grad_norm = nan
I0519 00:55:58.851426 140150982043392 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0519 00:55:58.858291 140179873642304 submission.py:139] 7500) loss = nan, grad_norm = nan
I0519 01:02:12.854684 140150973650688 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0519 01:02:12.859195 140179873642304 submission.py:139] 8000) loss = nan, grad_norm = nan
I0519 01:08:28.508142 140150982043392 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0519 01:08:28.514281 140179873642304 submission.py:139] 8500) loss = nan, grad_norm = nan
I0519 01:14:42.513839 140150973650688 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0519 01:14:42.518444 140179873642304 submission.py:139] 9000) loss = nan, grad_norm = nan
I0519 01:20:58.123208 140150982043392 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0519 01:20:58.129462 140179873642304 submission.py:139] 9500) loss = nan, grad_norm = nan
I0519 01:22:06.943168 140179873642304 spec.py:298] Evaluating on the training split.
I0519 01:22:16.733179 140179873642304 spec.py:310] Evaluating on the validation split.
I0519 01:22:26.726253 140179873642304 spec.py:326] Evaluating on the test split.
I0519 01:22:31.691875 140179873642304 submission_runner.py:421] Time since start: 7313.18s, 	Step: 9593, 	{'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 3821.5638999938965, 'total_duration': 7313.178191423416, 'accumulated_submission_time': 3821.5638999938965, 'accumulated_eval_time': 102.82679390907288, 'accumulated_logging_time': 0.09114360809326172}
I0519 01:22:31.710346 140150982043392 logging_writer.py:48] [9593] accumulated_eval_time=102.826794, accumulated_logging_time=0.091144, accumulated_submission_time=3821.563900, global_step=9593, preemption_count=0, score=3821.563900, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7313.178191, train/ctc_loss=nan, train/wer=0.941487, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 01:27:36.970122 140150973650688 logging_writer.py:48] [10000] global_step=10000, grad_norm=nan, loss=nan
I0519 01:27:36.975024 140179873642304 submission.py:139] 10000) loss = nan, grad_norm = nan
I0519 01:33:52.763332 140150982043392 logging_writer.py:48] [10500] global_step=10500, grad_norm=nan, loss=nan
I0519 01:33:52.770679 140179873642304 submission.py:139] 10500) loss = nan, grad_norm = nan
I0519 01:40:06.818948 140150973650688 logging_writer.py:48] [11000] global_step=11000, grad_norm=nan, loss=nan
I0519 01:40:06.823360 140179873642304 submission.py:139] 11000) loss = nan, grad_norm = nan
I0519 01:46:22.393946 140150982043392 logging_writer.py:48] [11500] global_step=11500, grad_norm=nan, loss=nan
I0519 01:46:22.400781 140179873642304 submission.py:139] 11500) loss = nan, grad_norm = nan
I0519 01:52:36.549843 140150973650688 logging_writer.py:48] [12000] global_step=12000, grad_norm=nan, loss=nan
I0519 01:52:36.554330 140179873642304 submission.py:139] 12000) loss = nan, grad_norm = nan
I0519 01:58:52.254071 140150982043392 logging_writer.py:48] [12500] global_step=12500, grad_norm=nan, loss=nan
I0519 01:58:52.260319 140179873642304 submission.py:139] 12500) loss = nan, grad_norm = nan
I0519 02:02:32.201684 140179873642304 spec.py:298] Evaluating on the training split.
I0519 02:02:41.974083 140179873642304 spec.py:310] Evaluating on the validation split.
I0519 02:02:51.559703 140179873642304 spec.py:326] Evaluating on the test split.
I0519 02:02:56.495720 140179873642304 submission_runner.py:421] Time since start: 9737.98s, 	Step: 12795, 	{'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5080.045091867447, 'total_duration': 9737.982015371323, 'accumulated_submission_time': 5080.045091867447, 'accumulated_eval_time': 127.12053227424622, 'accumulated_logging_time': 0.11907339096069336}
I0519 02:02:56.516435 140150982043392 logging_writer.py:48] [12795] accumulated_eval_time=127.120532, accumulated_logging_time=0.119073, accumulated_submission_time=5080.045092, global_step=12795, preemption_count=0, score=5080.045092, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=9737.982015, train/ctc_loss=nan, train/wer=0.941487, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 02:05:30.677618 140150973650688 logging_writer.py:48] [13000] global_step=13000, grad_norm=nan, loss=nan
I0519 02:05:30.681491 140179873642304 submission.py:139] 13000) loss = nan, grad_norm = nan
I0519 02:11:46.345559 140150982043392 logging_writer.py:48] [13500] global_step=13500, grad_norm=nan, loss=nan
I0519 02:11:46.352902 140179873642304 submission.py:139] 13500) loss = nan, grad_norm = nan
I0519 02:18:00.526921 140150973650688 logging_writer.py:48] [14000] global_step=14000, grad_norm=nan, loss=nan
I0519 02:18:00.531144 140179873642304 submission.py:139] 14000) loss = nan, grad_norm = nan
I0519 02:24:16.196227 140150982043392 logging_writer.py:48] [14500] global_step=14500, grad_norm=nan, loss=nan
I0519 02:24:16.203060 140179873642304 submission.py:139] 14500) loss = nan, grad_norm = nan
I0519 02:30:30.284408 140150973650688 logging_writer.py:48] [15000] global_step=15000, grad_norm=nan, loss=nan
I0519 02:30:30.329196 140179873642304 submission.py:139] 15000) loss = nan, grad_norm = nan
I0519 02:36:45.959159 140150973650688 logging_writer.py:48] [15500] global_step=15500, grad_norm=nan, loss=nan
I0519 02:36:45.967163 140179873642304 submission.py:139] 15500) loss = nan, grad_norm = nan
I0519 02:42:57.060180 140179873642304 spec.py:298] Evaluating on the training split.
I0519 02:43:06.858407 140179873642304 spec.py:310] Evaluating on the validation split.
I0519 02:43:16.073923 140179873642304 spec.py:326] Evaluating on the test split.
I0519 02:43:21.002482 140179873642304 submission_runner.py:421] Time since start: 12162.49s, 	Step: 15997, 	{'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 6338.512364149094, 'total_duration': 12162.487787008286, 'accumulated_submission_time': 6338.512364149094, 'accumulated_eval_time': 151.0614950656891, 'accumulated_logging_time': 0.15022540092468262}
I0519 02:43:21.028470 140150973650688 logging_writer.py:48] [15997] accumulated_eval_time=151.061495, accumulated_logging_time=0.150225, accumulated_submission_time=6338.512364, global_step=15997, preemption_count=0, score=6338.512364, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=12162.487787, train/ctc_loss=nan, train/wer=0.941487, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 02:43:24.047894 140150965257984 logging_writer.py:48] [16000] global_step=16000, grad_norm=nan, loss=nan
I0519 02:43:24.051232 140179873642304 submission.py:139] 16000) loss = nan, grad_norm = nan
I0519 02:49:39.639127 140150973650688 logging_writer.py:48] [16500] global_step=16500, grad_norm=nan, loss=nan
I0519 02:49:39.646378 140179873642304 submission.py:139] 16500) loss = nan, grad_norm = nan
I0519 02:55:53.704667 140150965257984 logging_writer.py:48] [17000] global_step=17000, grad_norm=nan, loss=nan
I0519 02:55:53.728711 140179873642304 submission.py:139] 17000) loss = nan, grad_norm = nan
I0519 03:02:07.693677 140150973650688 logging_writer.py:48] [17500] global_step=17500, grad_norm=nan, loss=nan
I0519 03:02:07.706458 140179873642304 submission.py:139] 17500) loss = nan, grad_norm = nan
I0519 03:08:23.314524 140150973650688 logging_writer.py:48] [18000] global_step=18000, grad_norm=nan, loss=nan
I0519 03:08:23.321732 140179873642304 submission.py:139] 18000) loss = nan, grad_norm = nan
I0519 03:14:37.369751 140150965257984 logging_writer.py:48] [18500] global_step=18500, grad_norm=nan, loss=nan
I0519 03:14:37.399471 140179873642304 submission.py:139] 18500) loss = nan, grad_norm = nan
I0519 03:20:52.986535 140150973650688 logging_writer.py:48] [19000] global_step=19000, grad_norm=nan, loss=nan
I0519 03:20:52.993904 140179873642304 submission.py:139] 19000) loss = nan, grad_norm = nan
I0519 03:23:21.835329 140179873642304 spec.py:298] Evaluating on the training split.
I0519 03:23:31.528666 140179873642304 spec.py:310] Evaluating on the validation split.
I0519 03:23:40.888521 140179873642304 spec.py:326] Evaluating on the test split.
I0519 03:23:45.964749 140179873642304 submission_runner.py:421] Time since start: 14587.45s, 	Step: 19200, 	{'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7597.731959104538, 'total_duration': 14587.451088190079, 'accumulated_submission_time': 7597.731959104538, 'accumulated_eval_time': 175.19060373306274, 'accumulated_logging_time': 0.19259238243103027}
I0519 03:23:45.984356 140150973650688 logging_writer.py:48] [19200] accumulated_eval_time=175.190604, accumulated_logging_time=0.192592, accumulated_submission_time=7597.731959, global_step=19200, preemption_count=0, score=7597.731959, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=14587.451088, train/ctc_loss=nan, train/wer=0.941487, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 03:27:31.162119 140150965257984 logging_writer.py:48] [19500] global_step=19500, grad_norm=nan, loss=nan
I0519 03:27:31.166561 140179873642304 submission.py:139] 19500) loss = nan, grad_norm = nan
I0519 03:33:45.998116 140179873642304 spec.py:298] Evaluating on the training split.
I0519 03:33:55.254663 140179873642304 spec.py:310] Evaluating on the validation split.
I0519 03:34:04.671577 140179873642304 spec.py:326] Evaluating on the test split.
I0519 03:34:09.675291 140179873642304 submission_runner.py:421] Time since start: 15211.16s, 	Step: 20000, 	{'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7912.4125893116, 'total_duration': 15211.161646842957, 'accumulated_submission_time': 7912.4125893116, 'accumulated_eval_time': 198.86769342422485, 'accumulated_logging_time': 0.22081327438354492}
I0519 03:34:09.692736 140150973650688 logging_writer.py:48] [20000] accumulated_eval_time=198.867693, accumulated_logging_time=0.220813, accumulated_submission_time=7912.412589, global_step=20000, preemption_count=0, score=7912.412589, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=15211.161647, train/ctc_loss=nan, train/wer=0.941487, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0519 03:34:09.711456 140150965257984 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7912.412589
I0519 03:34:10.158126 140179873642304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_pytorch/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_20000.
I0519 03:34:10.284533 140179873642304 submission_runner.py:584] Tuning trial 1/1
I0519 03:34:10.284777 140179873642304 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0519 03:34:10.285285 140179873642304 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/ctc_loss': 32.40272822593823, 'train/wer': 1.0461815589405343, 'validation/ctc_loss': 31.013116586296967, 'validation/wer': 1.0537054024042871, 'validation/num_examples': 5348, 'test/ctc_loss': 31.12825343570553, 'test/wer': 1.0560396482034409, 'test/num_examples': 2472, 'score': 8.046057224273682, 'total_duration': 37.68837118148804, 'accumulated_submission_time': 8.046057224273682, 'accumulated_eval_time': 29.640853881835938, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3187, {'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1304.2159271240234, 'total_duration': 2462.6964650154114, 'accumulated_submission_time': 1304.2159271240234, 'accumulated_eval_time': 53.75495266914368, 'accumulated_logging_time': 0.03196883201599121, 'global_step': 3187, 'preemption_count': 0}), (6390, {'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2562.563961982727, 'total_duration': 4887.694297552109, 'accumulated_submission_time': 2562.563961982727, 'accumulated_eval_time': 78.0784318447113, 'accumulated_logging_time': 0.0629429817199707, 'global_step': 6390, 'preemption_count': 0}), (9593, {'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 3821.5638999938965, 'total_duration': 7313.178191423416, 'accumulated_submission_time': 3821.5638999938965, 'accumulated_eval_time': 102.82679390907288, 'accumulated_logging_time': 0.09114360809326172, 'global_step': 9593, 'preemption_count': 0}), (12795, {'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5080.045091867447, 'total_duration': 9737.982015371323, 'accumulated_submission_time': 5080.045091867447, 'accumulated_eval_time': 127.12053227424622, 'accumulated_logging_time': 0.11907339096069336, 'global_step': 12795, 'preemption_count': 0}), (15997, {'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 6338.512364149094, 'total_duration': 12162.487787008286, 'accumulated_submission_time': 6338.512364149094, 'accumulated_eval_time': 151.0614950656891, 'accumulated_logging_time': 0.15022540092468262, 'global_step': 15997, 'preemption_count': 0}), (19200, {'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7597.731959104538, 'total_duration': 14587.451088190079, 'accumulated_submission_time': 7597.731959104538, 'accumulated_eval_time': 175.19060373306274, 'accumulated_logging_time': 0.19259238243103027, 'global_step': 19200, 'preemption_count': 0}), (20000, {'train/ctc_loss': nan, 'train/wer': 0.9414866696053942, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7912.4125893116, 'total_duration': 15211.161646842957, 'accumulated_submission_time': 7912.4125893116, 'accumulated_eval_time': 198.86769342422485, 'accumulated_logging_time': 0.22081327438354492, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0519 03:34:10.285431 140179873642304 submission_runner.py:587] Timing: 7912.4125893116
I0519 03:34:10.285498 140179873642304 submission_runner.py:588] ====================
I0519 03:34:10.285648 140179873642304 submission_runner.py:651] Final librispeech_conformer score: 7912.4125893116
