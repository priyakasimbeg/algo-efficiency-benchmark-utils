torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_pytorch/timing_nadamw --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_resnet_pytorch_05-18-2023-19-22-08.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0518 19:22:31.899562 139901548439360 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0518 19:22:31.899588 140254004447040 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0518 19:22:31.899602 140700543428416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0518 19:22:31.899619 139913632900928 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0518 19:22:31.900476 140665774430016 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0518 19:22:31.901152 139776768866112 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0518 19:22:31.901178 139703618299712 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0518 19:22:31.901276 140382513522496 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0518 19:22:31.901491 139776768866112 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:22:31.901519 139703618299712 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:22:31.901609 140382513522496 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:22:31.910115 139901548439360 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:22:31.910142 140254004447040 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:22:31.910167 140700543428416 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:22:31.910188 139913632900928 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:22:31.911198 140665774430016 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:22:34.151303 140382513522496 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v3_pytorch/timing_nadamw/imagenet_resnet_pytorch because --overwrite was set.
I0518 19:22:34.159647 140382513522496 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_pytorch/timing_nadamw/imagenet_resnet_pytorch.
W0518 19:22:34.183286 140700543428416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:22:34.183700 140254004447040 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:22:34.184154 140665774430016 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:22:34.184306 139913632900928 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:22:34.185540 139776768866112 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:22:34.187695 139901548439360 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:22:34.187718 139703618299712 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:22:34.191839 140382513522496 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0518 19:22:34.196729 140382513522496 submission_runner.py:544] Using RNG seed 1102835059
I0518 19:22:34.198044 140382513522496 submission_runner.py:553] --- Tuning run 1/1 ---
I0518 19:22:34.198151 140382513522496 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_pytorch/timing_nadamw/imagenet_resnet_pytorch/trial_1.
I0518 19:22:34.198358 140382513522496 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_pytorch/timing_nadamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0518 19:22:34.199253 140382513522496 submission_runner.py:241] Initializing dataset.
I0518 19:22:40.500384 140382513522496 submission_runner.py:248] Initializing model.
I0518 19:22:44.919183 140382513522496 submission_runner.py:258] Initializing optimizer.
I0518 19:22:44.920411 140382513522496 submission_runner.py:265] Initializing metrics bundle.
I0518 19:22:44.920514 140382513522496 submission_runner.py:283] Initializing checkpoint and logger.
I0518 19:22:45.378843 140382513522496 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_pytorch/timing_nadamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0518 19:22:45.379796 140382513522496 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_pytorch/timing_nadamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0518 19:22:45.428940 140382513522496 submission_runner.py:319] Starting training loop.
I0518 19:22:53.382668 140354179290880 logging_writer.py:48] [0] global_step=0, grad_norm=0.599565, loss=6.930942
I0518 19:22:53.402327 140382513522496 submission.py:296] 0) loss = 6.931, grad_norm = 0.600
I0518 19:22:53.403173 140382513522496 spec.py:298] Evaluating on the training split.
I0518 19:23:51.165719 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 19:24:46.623395 140382513522496 spec.py:326] Evaluating on the test split.
I0518 19:24:46.641781 140382513522496 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0518 19:24:46.647718 140382513522496 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0518 19:24:46.722185 140382513522496 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0518 19:24:58.437586 140382513522496 submission_runner.py:421] Time since start: 133.01s, 	Step: 1, 	{'train/accuracy': 0.0012555803571428572, 'train/loss': 6.927241111288265, 'validation/accuracy': 0.0014, 'validation/loss': 6.926903125, 'validation/num_examples': 50000, 'test/accuracy': 0.0012, 'test/loss': 6.93054921875, 'test/num_examples': 10000, 'score': 7.973525285720825, 'total_duration': 133.00902700424194, 'accumulated_submission_time': 7.973525285720825, 'accumulated_eval_time': 125.03431963920593, 'accumulated_logging_time': 0}
I0518 19:24:58.457319 140330271766272 logging_writer.py:48] [1] accumulated_eval_time=125.034320, accumulated_logging_time=0, accumulated_submission_time=7.973525, global_step=1, preemption_count=0, score=7.973525, test/accuracy=0.001200, test/loss=6.930549, test/num_examples=10000, total_duration=133.009027, train/accuracy=0.001256, train/loss=6.927241, validation/accuracy=0.001400, validation/loss=6.926903, validation/num_examples=50000
I0518 19:24:58.476793 140382513522496 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:24:58.476827 140700543428416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:24:58.476835 140254004447040 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:24:58.476831 139913632900928 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:24:58.476857 139901548439360 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:24:58.476861 139776768866112 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:24:58.476865 140665774430016 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:24:58.476895 139703618299712 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:24:58.866244 140330254980864 logging_writer.py:48] [1] global_step=1, grad_norm=0.631396, loss=6.926857
I0518 19:24:58.869524 140382513522496 submission.py:296] 1) loss = 6.927, grad_norm = 0.631
I0518 19:24:59.263648 140330271766272 logging_writer.py:48] [2] global_step=2, grad_norm=0.616782, loss=6.930663
I0518 19:24:59.267384 140382513522496 submission.py:296] 2) loss = 6.931, grad_norm = 0.617
I0518 19:24:59.658648 140330254980864 logging_writer.py:48] [3] global_step=3, grad_norm=0.620632, loss=6.932471
I0518 19:24:59.662070 140382513522496 submission.py:296] 3) loss = 6.932, grad_norm = 0.621
I0518 19:25:00.048143 140330271766272 logging_writer.py:48] [4] global_step=4, grad_norm=0.615700, loss=6.929145
I0518 19:25:00.051465 140382513522496 submission.py:296] 4) loss = 6.929, grad_norm = 0.616
I0518 19:25:00.443336 140330254980864 logging_writer.py:48] [5] global_step=5, grad_norm=0.610201, loss=6.932950
I0518 19:25:00.447495 140382513522496 submission.py:296] 5) loss = 6.933, grad_norm = 0.610
I0518 19:25:00.840780 140330271766272 logging_writer.py:48] [6] global_step=6, grad_norm=0.622786, loss=6.919762
I0518 19:25:00.848948 140382513522496 submission.py:296] 6) loss = 6.920, grad_norm = 0.623
I0518 19:25:01.240532 140330254980864 logging_writer.py:48] [7] global_step=7, grad_norm=0.633944, loss=6.933766
I0518 19:25:01.243991 140382513522496 submission.py:296] 7) loss = 6.934, grad_norm = 0.634
I0518 19:25:01.637652 140330271766272 logging_writer.py:48] [8] global_step=8, grad_norm=0.618249, loss=6.925078
I0518 19:25:01.641377 140382513522496 submission.py:296] 8) loss = 6.925, grad_norm = 0.618
I0518 19:25:02.032639 140330254980864 logging_writer.py:48] [9] global_step=9, grad_norm=0.631862, loss=6.919888
I0518 19:25:02.036128 140382513522496 submission.py:296] 9) loss = 6.920, grad_norm = 0.632
I0518 19:25:02.428085 140330271766272 logging_writer.py:48] [10] global_step=10, grad_norm=0.621189, loss=6.926186
I0518 19:25:02.432217 140382513522496 submission.py:296] 10) loss = 6.926, grad_norm = 0.621
I0518 19:25:02.829408 140330254980864 logging_writer.py:48] [11] global_step=11, grad_norm=0.609848, loss=6.925552
I0518 19:25:02.833618 140382513522496 submission.py:296] 11) loss = 6.926, grad_norm = 0.610
I0518 19:25:03.225887 140330271766272 logging_writer.py:48] [12] global_step=12, grad_norm=0.601072, loss=6.923507
I0518 19:25:03.229447 140382513522496 submission.py:296] 12) loss = 6.924, grad_norm = 0.601
I0518 19:25:03.621011 140330254980864 logging_writer.py:48] [13] global_step=13, grad_norm=0.618010, loss=6.926631
I0518 19:25:03.630980 140382513522496 submission.py:296] 13) loss = 6.927, grad_norm = 0.618
I0518 19:25:04.023513 140330271766272 logging_writer.py:48] [14] global_step=14, grad_norm=0.625849, loss=6.921529
I0518 19:25:04.027460 140382513522496 submission.py:296] 14) loss = 6.922, grad_norm = 0.626
I0518 19:25:04.421526 140330254980864 logging_writer.py:48] [15] global_step=15, grad_norm=0.608322, loss=6.927038
I0518 19:25:04.425310 140382513522496 submission.py:296] 15) loss = 6.927, grad_norm = 0.608
I0518 19:25:04.816037 140330271766272 logging_writer.py:48] [16] global_step=16, grad_norm=0.601376, loss=6.922520
I0518 19:25:04.820230 140382513522496 submission.py:296] 16) loss = 6.923, grad_norm = 0.601
I0518 19:25:05.214478 140330254980864 logging_writer.py:48] [17] global_step=17, grad_norm=0.615712, loss=6.928703
I0518 19:25:05.220339 140382513522496 submission.py:296] 17) loss = 6.929, grad_norm = 0.616
I0518 19:25:05.623121 140330271766272 logging_writer.py:48] [18] global_step=18, grad_norm=0.615387, loss=6.922352
I0518 19:25:05.626609 140382513522496 submission.py:296] 18) loss = 6.922, grad_norm = 0.615
I0518 19:25:06.020160 140330254980864 logging_writer.py:48] [19] global_step=19, grad_norm=0.615667, loss=6.920632
I0518 19:25:06.027679 140382513522496 submission.py:296] 19) loss = 6.921, grad_norm = 0.616
I0518 19:25:06.418070 140330271766272 logging_writer.py:48] [20] global_step=20, grad_norm=0.625629, loss=6.928395
I0518 19:25:06.421581 140382513522496 submission.py:296] 20) loss = 6.928, grad_norm = 0.626
I0518 19:25:06.810772 140330254980864 logging_writer.py:48] [21] global_step=21, grad_norm=0.607320, loss=6.923551
I0518 19:25:06.814365 140382513522496 submission.py:296] 21) loss = 6.924, grad_norm = 0.607
I0518 19:25:07.209552 140330271766272 logging_writer.py:48] [22] global_step=22, grad_norm=0.613926, loss=6.927194
I0518 19:25:07.213025 140382513522496 submission.py:296] 22) loss = 6.927, grad_norm = 0.614
I0518 19:25:07.604134 140330254980864 logging_writer.py:48] [23] global_step=23, grad_norm=0.604573, loss=6.915575
I0518 19:25:07.607503 140382513522496 submission.py:296] 23) loss = 6.916, grad_norm = 0.605
I0518 19:25:07.996414 140330271766272 logging_writer.py:48] [24] global_step=24, grad_norm=0.619515, loss=6.927777
I0518 19:25:07.999842 140382513522496 submission.py:296] 24) loss = 6.928, grad_norm = 0.620
I0518 19:25:08.389667 140330254980864 logging_writer.py:48] [25] global_step=25, grad_norm=0.607738, loss=6.925023
I0518 19:25:08.393486 140382513522496 submission.py:296] 25) loss = 6.925, grad_norm = 0.608
I0518 19:25:08.786923 140330271766272 logging_writer.py:48] [26] global_step=26, grad_norm=0.622356, loss=6.912713
I0518 19:25:08.790356 140382513522496 submission.py:296] 26) loss = 6.913, grad_norm = 0.622
I0518 19:25:09.183777 140330254980864 logging_writer.py:48] [27] global_step=27, grad_norm=0.604285, loss=6.924524
I0518 19:25:09.187128 140382513522496 submission.py:296] 27) loss = 6.925, grad_norm = 0.604
I0518 19:25:09.577266 140330271766272 logging_writer.py:48] [28] global_step=28, grad_norm=0.615914, loss=6.919935
I0518 19:25:09.580690 140382513522496 submission.py:296] 28) loss = 6.920, grad_norm = 0.616
I0518 19:25:09.971487 140330254980864 logging_writer.py:48] [29] global_step=29, grad_norm=0.628342, loss=6.926934
I0518 19:25:09.975509 140382513522496 submission.py:296] 29) loss = 6.927, grad_norm = 0.628
I0518 19:25:10.367418 140330271766272 logging_writer.py:48] [30] global_step=30, grad_norm=0.614132, loss=6.921585
I0518 19:25:10.371174 140382513522496 submission.py:296] 30) loss = 6.922, grad_norm = 0.614
I0518 19:25:10.765430 140330254980864 logging_writer.py:48] [31] global_step=31, grad_norm=0.597357, loss=6.918987
I0518 19:25:10.769249 140382513522496 submission.py:296] 31) loss = 6.919, grad_norm = 0.597
I0518 19:25:11.166921 140330271766272 logging_writer.py:48] [32] global_step=32, grad_norm=0.597474, loss=6.922636
I0518 19:25:11.170718 140382513522496 submission.py:296] 32) loss = 6.923, grad_norm = 0.597
I0518 19:25:11.559806 140330254980864 logging_writer.py:48] [33] global_step=33, grad_norm=0.631569, loss=6.916977
I0518 19:25:11.563376 140382513522496 submission.py:296] 33) loss = 6.917, grad_norm = 0.632
I0518 19:25:11.956852 140330271766272 logging_writer.py:48] [34] global_step=34, grad_norm=0.600141, loss=6.908576
I0518 19:25:11.960383 140382513522496 submission.py:296] 34) loss = 6.909, grad_norm = 0.600
I0518 19:25:12.357145 140330254980864 logging_writer.py:48] [35] global_step=35, grad_norm=0.613248, loss=6.923532
I0518 19:25:12.361501 140382513522496 submission.py:296] 35) loss = 6.924, grad_norm = 0.613
I0518 19:25:12.759002 140330271766272 logging_writer.py:48] [36] global_step=36, grad_norm=0.628487, loss=6.916151
I0518 19:25:12.762469 140382513522496 submission.py:296] 36) loss = 6.916, grad_norm = 0.628
I0518 19:25:13.164770 140330254980864 logging_writer.py:48] [37] global_step=37, grad_norm=0.614817, loss=6.917169
I0518 19:25:13.169026 140382513522496 submission.py:296] 37) loss = 6.917, grad_norm = 0.615
I0518 19:25:13.560853 140330271766272 logging_writer.py:48] [38] global_step=38, grad_norm=0.601543, loss=6.921168
I0518 19:25:13.566815 140382513522496 submission.py:296] 38) loss = 6.921, grad_norm = 0.602
I0518 19:25:13.968260 140330254980864 logging_writer.py:48] [39] global_step=39, grad_norm=0.608922, loss=6.914940
I0518 19:25:13.971859 140382513522496 submission.py:296] 39) loss = 6.915, grad_norm = 0.609
I0518 19:25:14.363220 140330271766272 logging_writer.py:48] [40] global_step=40, grad_norm=0.619098, loss=6.917079
I0518 19:25:14.366631 140382513522496 submission.py:296] 40) loss = 6.917, grad_norm = 0.619
I0518 19:25:14.757910 140330254980864 logging_writer.py:48] [41] global_step=41, grad_norm=0.590378, loss=6.911037
I0518 19:25:14.761513 140382513522496 submission.py:296] 41) loss = 6.911, grad_norm = 0.590
I0518 19:25:15.150183 140330271766272 logging_writer.py:48] [42] global_step=42, grad_norm=0.602579, loss=6.916906
I0518 19:25:15.154121 140382513522496 submission.py:296] 42) loss = 6.917, grad_norm = 0.603
I0518 19:25:15.543842 140330254980864 logging_writer.py:48] [43] global_step=43, grad_norm=0.618666, loss=6.913689
I0518 19:25:15.547550 140382513522496 submission.py:296] 43) loss = 6.914, grad_norm = 0.619
I0518 19:25:15.939477 140330271766272 logging_writer.py:48] [44] global_step=44, grad_norm=0.604519, loss=6.911724
I0518 19:25:15.943751 140382513522496 submission.py:296] 44) loss = 6.912, grad_norm = 0.605
I0518 19:25:16.338986 140330254980864 logging_writer.py:48] [45] global_step=45, grad_norm=0.598198, loss=6.918717
I0518 19:25:16.342507 140382513522496 submission.py:296] 45) loss = 6.919, grad_norm = 0.598
I0518 19:25:16.732883 140330271766272 logging_writer.py:48] [46] global_step=46, grad_norm=0.622027, loss=6.914692
I0518 19:25:16.736488 140382513522496 submission.py:296] 46) loss = 6.915, grad_norm = 0.622
I0518 19:25:17.126725 140330254980864 logging_writer.py:48] [47] global_step=47, grad_norm=0.617139, loss=6.922264
I0518 19:25:17.130243 140382513522496 submission.py:296] 47) loss = 6.922, grad_norm = 0.617
I0518 19:25:17.522118 140330271766272 logging_writer.py:48] [48] global_step=48, grad_norm=0.602310, loss=6.918748
I0518 19:25:17.526110 140382513522496 submission.py:296] 48) loss = 6.919, grad_norm = 0.602
I0518 19:25:17.923958 140330254980864 logging_writer.py:48] [49] global_step=49, grad_norm=0.612533, loss=6.913252
I0518 19:25:17.927401 140382513522496 submission.py:296] 49) loss = 6.913, grad_norm = 0.613
I0518 19:25:18.319404 140330271766272 logging_writer.py:48] [50] global_step=50, grad_norm=0.601570, loss=6.911331
I0518 19:25:18.322810 140382513522496 submission.py:296] 50) loss = 6.911, grad_norm = 0.602
I0518 19:25:18.725463 140330254980864 logging_writer.py:48] [51] global_step=51, grad_norm=0.607859, loss=6.912484
I0518 19:25:18.732210 140382513522496 submission.py:296] 51) loss = 6.912, grad_norm = 0.608
I0518 19:25:19.122517 140330271766272 logging_writer.py:48] [52] global_step=52, grad_norm=0.600687, loss=6.909638
I0518 19:25:19.126292 140382513522496 submission.py:296] 52) loss = 6.910, grad_norm = 0.601
I0518 19:25:19.516974 140330254980864 logging_writer.py:48] [53] global_step=53, grad_norm=0.597604, loss=6.904119
I0518 19:25:19.521202 140382513522496 submission.py:296] 53) loss = 6.904, grad_norm = 0.598
I0518 19:25:19.913197 140330271766272 logging_writer.py:48] [54] global_step=54, grad_norm=0.612094, loss=6.903380
I0518 19:25:19.917038 140382513522496 submission.py:296] 54) loss = 6.903, grad_norm = 0.612
I0518 19:25:20.314588 140330254980864 logging_writer.py:48] [55] global_step=55, grad_norm=0.597567, loss=6.911294
I0518 19:25:20.321174 140382513522496 submission.py:296] 55) loss = 6.911, grad_norm = 0.598
I0518 19:25:20.715677 140330271766272 logging_writer.py:48] [56] global_step=56, grad_norm=0.610119, loss=6.906126
I0518 19:25:20.719833 140382513522496 submission.py:296] 56) loss = 6.906, grad_norm = 0.610
I0518 19:25:21.109799 140330254980864 logging_writer.py:48] [57] global_step=57, grad_norm=0.611724, loss=6.910768
I0518 19:25:21.116569 140382513522496 submission.py:296] 57) loss = 6.911, grad_norm = 0.612
I0518 19:25:21.515192 140330271766272 logging_writer.py:48] [58] global_step=58, grad_norm=0.626610, loss=6.907390
I0518 19:25:21.518575 140382513522496 submission.py:296] 58) loss = 6.907, grad_norm = 0.627
I0518 19:25:21.914414 140330254980864 logging_writer.py:48] [59] global_step=59, grad_norm=0.613176, loss=6.906431
I0518 19:25:21.918028 140382513522496 submission.py:296] 59) loss = 6.906, grad_norm = 0.613
I0518 19:25:22.318210 140330271766272 logging_writer.py:48] [60] global_step=60, grad_norm=0.613954, loss=6.912874
I0518 19:25:22.321829 140382513522496 submission.py:296] 60) loss = 6.913, grad_norm = 0.614
I0518 19:25:22.715042 140330254980864 logging_writer.py:48] [61] global_step=61, grad_norm=0.581774, loss=6.897278
I0518 19:25:22.718483 140382513522496 submission.py:296] 61) loss = 6.897, grad_norm = 0.582
I0518 19:25:23.114495 140330271766272 logging_writer.py:48] [62] global_step=62, grad_norm=0.590422, loss=6.904296
I0518 19:25:23.118552 140382513522496 submission.py:296] 62) loss = 6.904, grad_norm = 0.590
I0518 19:25:23.510874 140330254980864 logging_writer.py:48] [63] global_step=63, grad_norm=0.619375, loss=6.899779
I0518 19:25:23.514307 140382513522496 submission.py:296] 63) loss = 6.900, grad_norm = 0.619
I0518 19:25:23.905682 140330271766272 logging_writer.py:48] [64] global_step=64, grad_norm=0.643245, loss=6.897011
I0518 19:25:23.909094 140382513522496 submission.py:296] 64) loss = 6.897, grad_norm = 0.643
I0518 19:25:24.300471 140330254980864 logging_writer.py:48] [65] global_step=65, grad_norm=0.602843, loss=6.891972
I0518 19:25:24.305502 140382513522496 submission.py:296] 65) loss = 6.892, grad_norm = 0.603
I0518 19:25:24.697440 140330271766272 logging_writer.py:48] [66] global_step=66, grad_norm=0.605484, loss=6.902027
I0518 19:25:24.700777 140382513522496 submission.py:296] 66) loss = 6.902, grad_norm = 0.605
I0518 19:25:25.091582 140330254980864 logging_writer.py:48] [67] global_step=67, grad_norm=0.608271, loss=6.901767
I0518 19:25:25.096688 140382513522496 submission.py:296] 67) loss = 6.902, grad_norm = 0.608
I0518 19:25:25.489317 140330271766272 logging_writer.py:48] [68] global_step=68, grad_norm=0.625388, loss=6.895496
I0518 19:25:25.493331 140382513522496 submission.py:296] 68) loss = 6.895, grad_norm = 0.625
I0518 19:25:25.890655 140330254980864 logging_writer.py:48] [69] global_step=69, grad_norm=0.583544, loss=6.901027
I0518 19:25:25.894361 140382513522496 submission.py:296] 69) loss = 6.901, grad_norm = 0.584
I0518 19:25:26.288280 140330271766272 logging_writer.py:48] [70] global_step=70, grad_norm=0.599920, loss=6.891779
I0518 19:25:26.291857 140382513522496 submission.py:296] 70) loss = 6.892, grad_norm = 0.600
I0518 19:25:26.684260 140330254980864 logging_writer.py:48] [71] global_step=71, grad_norm=0.609413, loss=6.902492
I0518 19:25:26.688922 140382513522496 submission.py:296] 71) loss = 6.902, grad_norm = 0.609
I0518 19:25:27.081183 140330271766272 logging_writer.py:48] [72] global_step=72, grad_norm=0.607828, loss=6.892690
I0518 19:25:27.085489 140382513522496 submission.py:296] 72) loss = 6.893, grad_norm = 0.608
I0518 19:25:27.478357 140330254980864 logging_writer.py:48] [73] global_step=73, grad_norm=0.627334, loss=6.889528
I0518 19:25:27.482006 140382513522496 submission.py:296] 73) loss = 6.890, grad_norm = 0.627
I0518 19:25:27.887754 140330271766272 logging_writer.py:48] [74] global_step=74, grad_norm=0.581034, loss=6.884732
I0518 19:25:27.892630 140382513522496 submission.py:296] 74) loss = 6.885, grad_norm = 0.581
I0518 19:25:28.283743 140330254980864 logging_writer.py:48] [75] global_step=75, grad_norm=0.596111, loss=6.884905
I0518 19:25:28.287257 140382513522496 submission.py:296] 75) loss = 6.885, grad_norm = 0.596
I0518 19:25:28.678244 140330271766272 logging_writer.py:48] [76] global_step=76, grad_norm=0.593595, loss=6.891963
I0518 19:25:28.681801 140382513522496 submission.py:296] 76) loss = 6.892, grad_norm = 0.594
I0518 19:25:29.075458 140330254980864 logging_writer.py:48] [77] global_step=77, grad_norm=0.599484, loss=6.883430
I0518 19:25:29.078907 140382513522496 submission.py:296] 77) loss = 6.883, grad_norm = 0.599
I0518 19:25:29.469003 140330271766272 logging_writer.py:48] [78] global_step=78, grad_norm=0.610034, loss=6.883783
I0518 19:25:29.473225 140382513522496 submission.py:296] 78) loss = 6.884, grad_norm = 0.610
I0518 19:25:29.865039 140330254980864 logging_writer.py:48] [79] global_step=79, grad_norm=0.623876, loss=6.894493
I0518 19:25:29.868799 140382513522496 submission.py:296] 79) loss = 6.894, grad_norm = 0.624
I0518 19:25:30.261728 140330271766272 logging_writer.py:48] [80] global_step=80, grad_norm=0.589340, loss=6.885440
I0518 19:25:30.265310 140382513522496 submission.py:296] 80) loss = 6.885, grad_norm = 0.589
I0518 19:25:30.658600 140330254980864 logging_writer.py:48] [81] global_step=81, grad_norm=0.591480, loss=6.891770
I0518 19:25:30.662520 140382513522496 submission.py:296] 81) loss = 6.892, grad_norm = 0.591
I0518 19:25:31.079383 140330271766272 logging_writer.py:48] [82] global_step=82, grad_norm=0.592944, loss=6.884348
I0518 19:25:31.082772 140382513522496 submission.py:296] 82) loss = 6.884, grad_norm = 0.593
I0518 19:25:31.473629 140330254980864 logging_writer.py:48] [83] global_step=83, grad_norm=0.607974, loss=6.879880
I0518 19:25:31.477293 140382513522496 submission.py:296] 83) loss = 6.880, grad_norm = 0.608
I0518 19:25:31.873407 140330271766272 logging_writer.py:48] [84] global_step=84, grad_norm=0.599389, loss=6.884420
I0518 19:25:31.876898 140382513522496 submission.py:296] 84) loss = 6.884, grad_norm = 0.599
I0518 19:25:32.266945 140330254980864 logging_writer.py:48] [85] global_step=85, grad_norm=0.601380, loss=6.883865
I0518 19:25:32.270608 140382513522496 submission.py:296] 85) loss = 6.884, grad_norm = 0.601
I0518 19:25:32.662342 140330271766272 logging_writer.py:48] [86] global_step=86, grad_norm=0.611834, loss=6.878115
I0518 19:25:32.666022 140382513522496 submission.py:296] 86) loss = 6.878, grad_norm = 0.612
I0518 19:25:33.056754 140330254980864 logging_writer.py:48] [87] global_step=87, grad_norm=0.609916, loss=6.880209
I0518 19:25:33.061250 140382513522496 submission.py:296] 87) loss = 6.880, grad_norm = 0.610
I0518 19:25:33.463170 140330271766272 logging_writer.py:48] [88] global_step=88, grad_norm=0.596816, loss=6.877183
I0518 19:25:33.468105 140382513522496 submission.py:296] 88) loss = 6.877, grad_norm = 0.597
I0518 19:25:33.861031 140330254980864 logging_writer.py:48] [89] global_step=89, grad_norm=0.597398, loss=6.870274
I0518 19:25:33.864706 140382513522496 submission.py:296] 89) loss = 6.870, grad_norm = 0.597
I0518 19:25:34.261190 140330271766272 logging_writer.py:48] [90] global_step=90, grad_norm=0.616872, loss=6.888024
I0518 19:25:34.264613 140382513522496 submission.py:296] 90) loss = 6.888, grad_norm = 0.617
I0518 19:25:34.656132 140330254980864 logging_writer.py:48] [91] global_step=91, grad_norm=0.594568, loss=6.873693
I0518 19:25:34.659537 140382513522496 submission.py:296] 91) loss = 6.874, grad_norm = 0.595
I0518 19:25:35.066510 140330271766272 logging_writer.py:48] [92] global_step=92, grad_norm=0.617310, loss=6.875296
I0518 19:25:35.070217 140382513522496 submission.py:296] 92) loss = 6.875, grad_norm = 0.617
I0518 19:25:35.465272 140330254980864 logging_writer.py:48] [93] global_step=93, grad_norm=0.606735, loss=6.879754
I0518 19:25:35.468956 140382513522496 submission.py:296] 93) loss = 6.880, grad_norm = 0.607
I0518 19:25:35.859088 140330271766272 logging_writer.py:48] [94] global_step=94, grad_norm=0.591583, loss=6.875074
I0518 19:25:35.863390 140382513522496 submission.py:296] 94) loss = 6.875, grad_norm = 0.592
I0518 19:25:36.253748 140330254980864 logging_writer.py:48] [95] global_step=95, grad_norm=0.601212, loss=6.876326
I0518 19:25:36.257457 140382513522496 submission.py:296] 95) loss = 6.876, grad_norm = 0.601
I0518 19:25:36.647312 140330271766272 logging_writer.py:48] [96] global_step=96, grad_norm=0.599343, loss=6.873667
I0518 19:25:36.650817 140382513522496 submission.py:296] 96) loss = 6.874, grad_norm = 0.599
I0518 19:25:37.044870 140330254980864 logging_writer.py:48] [97] global_step=97, grad_norm=0.609305, loss=6.870027
I0518 19:25:37.049407 140382513522496 submission.py:296] 97) loss = 6.870, grad_norm = 0.609
I0518 19:25:37.447377 140330271766272 logging_writer.py:48] [98] global_step=98, grad_norm=0.613273, loss=6.869332
I0518 19:25:37.451840 140382513522496 submission.py:296] 98) loss = 6.869, grad_norm = 0.613
I0518 19:25:37.852169 140330254980864 logging_writer.py:48] [99] global_step=99, grad_norm=0.613770, loss=6.873576
I0518 19:25:37.856392 140382513522496 submission.py:296] 99) loss = 6.874, grad_norm = 0.614
I0518 19:25:38.249159 140330271766272 logging_writer.py:48] [100] global_step=100, grad_norm=0.600541, loss=6.871138
I0518 19:25:38.253402 140382513522496 submission.py:296] 100) loss = 6.871, grad_norm = 0.601
I0518 19:28:12.504612 140330254980864 logging_writer.py:48] [500] global_step=500, grad_norm=0.916870, loss=6.319915
I0518 19:28:12.509404 140382513522496 submission.py:296] 500) loss = 6.320, grad_norm = 0.917
I0518 19:31:25.215212 140330271766272 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.849715, loss=5.635782
I0518 19:31:25.219212 140382513522496 submission.py:296] 1000) loss = 5.636, grad_norm = 2.850
I0518 19:33:28.692843 140382513522496 spec.py:298] Evaluating on the training split.
I0518 19:34:09.506443 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 19:35:03.560899 140382513522496 spec.py:326] Evaluating on the test split.
I0518 19:35:04.962674 140382513522496 submission_runner.py:421] Time since start: 739.53s, 	Step: 1318, 	{'train/accuracy': 0.11870216836734694, 'train/loss': 4.773458986866231, 'validation/accuracy': 0.10706, 'validation/loss': 4.855113125, 'validation/num_examples': 50000, 'test/accuracy': 0.0734, 'test/loss': 5.261117578125, 'test/num_examples': 10000, 'score': 517.6791594028473, 'total_duration': 739.5341739654541, 'accumulated_submission_time': 517.6791594028473, 'accumulated_eval_time': 221.30411386489868, 'accumulated_logging_time': 0.027118444442749023}
I0518 19:35:04.972706 140330355627776 logging_writer.py:48] [1318] accumulated_eval_time=221.304114, accumulated_logging_time=0.027118, accumulated_submission_time=517.679159, global_step=1318, preemption_count=0, score=517.679159, test/accuracy=0.073400, test/loss=5.261118, test/num_examples=10000, total_duration=739.534174, train/accuracy=0.118702, train/loss=4.773459, validation/accuracy=0.107060, validation/loss=4.855113, validation/num_examples=50000
I0518 19:36:14.852444 140330364020480 logging_writer.py:48] [1500] global_step=1500, grad_norm=5.754792, loss=5.203674
I0518 19:36:14.856298 140382513522496 submission.py:296] 1500) loss = 5.204, grad_norm = 5.755
I0518 19:39:25.541458 140330355627776 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.111491, loss=4.864266
I0518 19:39:25.545663 140382513522496 submission.py:296] 2000) loss = 4.864, grad_norm = 3.111
I0518 19:42:37.479676 140330364020480 logging_writer.py:48] [2500] global_step=2500, grad_norm=5.517213, loss=4.554340
I0518 19:42:37.483587 140382513522496 submission.py:296] 2500) loss = 4.554, grad_norm = 5.517
I0518 19:43:35.029516 140382513522496 spec.py:298] Evaluating on the training split.
I0518 19:44:20.380836 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 19:45:15.046390 140382513522496 spec.py:326] Evaluating on the test split.
I0518 19:45:16.418102 140382513522496 submission_runner.py:421] Time since start: 1350.99s, 	Step: 2648, 	{'train/accuracy': 0.2749123086734694, 'train/loss': 3.5815887451171875, 'validation/accuracy': 0.25062, 'validation/loss': 3.71039375, 'validation/num_examples': 50000, 'test/accuracy': 0.1818, 'test/loss': 4.290884765625, 'test/num_examples': 10000, 'score': 1027.258706331253, 'total_duration': 1350.9885404109955, 'accumulated_submission_time': 1027.258706331253, 'accumulated_eval_time': 322.69152760505676, 'accumulated_logging_time': 0.04552316665649414}
I0518 19:45:16.428572 140330355627776 logging_writer.py:48] [2648] accumulated_eval_time=322.691528, accumulated_logging_time=0.045523, accumulated_submission_time=1027.258706, global_step=2648, preemption_count=0, score=1027.258706, test/accuracy=0.181800, test/loss=4.290885, test/num_examples=10000, total_duration=1350.988540, train/accuracy=0.274912, train/loss=3.581589, validation/accuracy=0.250620, validation/loss=3.710394, validation/num_examples=50000
I0518 19:47:31.103158 140330364020480 logging_writer.py:48] [3000] global_step=3000, grad_norm=5.996862, loss=4.277403
I0518 19:47:31.106873 140382513522496 submission.py:296] 3000) loss = 4.277, grad_norm = 5.997
I0518 19:50:41.892964 140330355627776 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.445536, loss=4.026092
I0518 19:50:41.897081 140382513522496 submission.py:296] 3500) loss = 4.026, grad_norm = 4.446
I0518 19:53:46.773239 140382513522496 spec.py:298] Evaluating on the training split.
I0518 19:54:27.457175 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 19:55:10.711423 140382513522496 spec.py:326] Evaluating on the test split.
I0518 19:55:12.072358 140382513522496 submission_runner.py:421] Time since start: 1946.64s, 	Step: 3981, 	{'train/accuracy': 0.37412308673469385, 'train/loss': 2.8935515734614157, 'validation/accuracy': 0.34858, 'validation/loss': 3.054421875, 'validation/num_examples': 50000, 'test/accuracy': 0.2474, 'test/loss': 3.787951171875, 'test/num_examples': 10000, 'score': 1537.1315760612488, 'total_duration': 1946.6438481807709, 'accumulated_submission_time': 1537.1315760612488, 'accumulated_eval_time': 407.9905982017517, 'accumulated_logging_time': 0.06451606750488281}
I0518 19:55:12.082193 140330364020480 logging_writer.py:48] [3981] accumulated_eval_time=407.990598, accumulated_logging_time=0.064516, accumulated_submission_time=1537.131576, global_step=3981, preemption_count=0, score=1537.131576, test/accuracy=0.247400, test/loss=3.787951, test/num_examples=10000, total_duration=1946.643848, train/accuracy=0.374123, train/loss=2.893552, validation/accuracy=0.348580, validation/loss=3.054422, validation/num_examples=50000
I0518 19:55:19.695973 140330355627776 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.372219, loss=3.976987
I0518 19:55:19.699430 140382513522496 submission.py:296] 4000) loss = 3.977, grad_norm = 4.372
I0518 19:58:30.632097 140330364020480 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.200029, loss=3.575613
I0518 19:58:30.638812 140382513522496 submission.py:296] 4500) loss = 3.576, grad_norm = 3.200
I0518 20:01:42.289717 140330355627776 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.656337, loss=3.511216
I0518 20:01:42.301355 140382513522496 submission.py:296] 5000) loss = 3.511, grad_norm = 2.656
I0518 20:03:42.180371 140382513522496 spec.py:298] Evaluating on the training split.
I0518 20:04:27.209317 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 20:05:19.510716 140382513522496 spec.py:326] Evaluating on the test split.
I0518 20:05:20.875399 140382513522496 submission_runner.py:421] Time since start: 2555.45s, 	Step: 5311, 	{'train/accuracy': 0.4767219387755102, 'train/loss': 2.374318025550064, 'validation/accuracy': 0.435, 'validation/loss': 2.59030234375, 'validation/num_examples': 50000, 'test/accuracy': 0.3204, 'test/loss': 3.33454296875, 'test/num_examples': 10000, 'score': 2046.7515587806702, 'total_duration': 2555.4468989372253, 'accumulated_submission_time': 2046.7515587806702, 'accumulated_eval_time': 506.6855614185333, 'accumulated_logging_time': 0.08177065849304199}
I0518 20:05:20.886232 140330364020480 logging_writer.py:48] [5311] accumulated_eval_time=506.685561, accumulated_logging_time=0.081771, accumulated_submission_time=2046.751559, global_step=5311, preemption_count=0, score=2046.751559, test/accuracy=0.320400, test/loss=3.334543, test/num_examples=10000, total_duration=2555.446899, train/accuracy=0.476722, train/loss=2.374318, validation/accuracy=0.435000, validation/loss=2.590302, validation/num_examples=50000
I0518 20:06:33.295055 140330355627776 logging_writer.py:48] [5500] global_step=5500, grad_norm=4.537986, loss=3.523825
I0518 20:06:33.299413 140382513522496 submission.py:296] 5500) loss = 3.524, grad_norm = 4.538
I0518 20:09:44.086597 140330364020480 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.855254, loss=3.423074
I0518 20:09:44.092209 140382513522496 submission.py:296] 6000) loss = 3.423, grad_norm = 4.855
I0518 20:12:56.985207 140330355627776 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.716788, loss=3.259675
I0518 20:12:56.989845 140382513522496 submission.py:296] 6500) loss = 3.260, grad_norm = 2.717
I0518 20:13:51.133332 140382513522496 spec.py:298] Evaluating on the training split.
I0518 20:14:32.793026 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 20:15:16.357412 140382513522496 spec.py:326] Evaluating on the test split.
I0518 20:15:17.727482 140382513522496 submission_runner.py:421] Time since start: 3152.30s, 	Step: 6643, 	{'train/accuracy': 0.5316286670918368, 'train/loss': 2.0954923045878506, 'validation/accuracy': 0.49002, 'validation/loss': 2.3066215625, 'validation/num_examples': 50000, 'test/accuracy': 0.3546, 'test/loss': 3.0969744140625, 'test/num_examples': 10000, 'score': 2556.523392677307, 'total_duration': 3152.2989926338196, 'accumulated_submission_time': 2556.523392677307, 'accumulated_eval_time': 593.2796285152435, 'accumulated_logging_time': 0.10029315948486328}
I0518 20:15:17.737145 140330364020480 logging_writer.py:48] [6643] accumulated_eval_time=593.279629, accumulated_logging_time=0.100293, accumulated_submission_time=2556.523393, global_step=6643, preemption_count=0, score=2556.523393, test/accuracy=0.354600, test/loss=3.096974, test/num_examples=10000, total_duration=3152.298993, train/accuracy=0.531629, train/loss=2.095492, validation/accuracy=0.490020, validation/loss=2.306622, validation/num_examples=50000
I0518 20:17:34.144568 140330355627776 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.525990, loss=3.210536
I0518 20:17:34.148675 140382513522496 submission.py:296] 7000) loss = 3.211, grad_norm = 1.526
I0518 20:20:45.866303 140330364020480 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.584532, loss=3.170677
I0518 20:20:45.870520 140382513522496 submission.py:296] 7500) loss = 3.171, grad_norm = 1.585
I0518 20:23:47.746353 140382513522496 spec.py:298] Evaluating on the training split.
I0518 20:24:31.017183 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 20:25:15.564396 140382513522496 spec.py:326] Evaluating on the test split.
I0518 20:25:16.926489 140382513522496 submission_runner.py:421] Time since start: 3751.50s, 	Step: 7973, 	{'train/accuracy': 0.5887276785714286, 'train/loss': 1.837281051947146, 'validation/accuracy': 0.53776, 'validation/loss': 2.08126859375, 'validation/num_examples': 50000, 'test/accuracy': 0.4057, 'test/loss': 2.818893359375, 'test/num_examples': 10000, 'score': 3066.049071788788, 'total_duration': 3751.498022079468, 'accumulated_submission_time': 3066.049071788788, 'accumulated_eval_time': 682.4597043991089, 'accumulated_logging_time': 0.11752724647521973}
I0518 20:25:16.936238 140330355627776 logging_writer.py:48] [7973] accumulated_eval_time=682.459704, accumulated_logging_time=0.117527, accumulated_submission_time=3066.049072, global_step=7973, preemption_count=0, score=3066.049072, test/accuracy=0.405700, test/loss=2.818893, test/num_examples=10000, total_duration=3751.498022, train/accuracy=0.588728, train/loss=1.837281, validation/accuracy=0.537760, validation/loss=2.081269, validation/num_examples=50000
I0518 20:25:27.573725 140330364020480 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.056429, loss=3.073678
I0518 20:25:27.577275 140382513522496 submission.py:296] 8000) loss = 3.074, grad_norm = 3.056
I0518 20:28:38.427073 140330355627776 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.082708, loss=3.043781
I0518 20:28:38.431221 140382513522496 submission.py:296] 8500) loss = 3.044, grad_norm = 2.083
I0518 20:31:51.332478 140330364020480 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.171342, loss=3.050852
I0518 20:31:51.336858 140382513522496 submission.py:296] 9000) loss = 3.051, grad_norm = 2.171
I0518 20:33:47.228760 140382513522496 spec.py:298] Evaluating on the training split.
I0518 20:34:29.574267 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 20:35:13.364949 140382513522496 spec.py:326] Evaluating on the test split.
I0518 20:35:14.728620 140382513522496 submission_runner.py:421] Time since start: 4349.30s, 	Step: 9305, 	{'train/accuracy': 0.6146763392857143, 'train/loss': 1.680441408741231, 'validation/accuracy': 0.56254, 'validation/loss': 1.947511875, 'validation/num_examples': 50000, 'test/accuracy': 0.4284, 'test/loss': 2.714372265625, 'test/num_examples': 10000, 'score': 3575.861747980118, 'total_duration': 4349.29998922348, 'accumulated_submission_time': 3575.861747980118, 'accumulated_eval_time': 769.9593527317047, 'accumulated_logging_time': 0.13484525680541992}
I0518 20:35:14.738413 140330355627776 logging_writer.py:48] [9305] accumulated_eval_time=769.959353, accumulated_logging_time=0.134845, accumulated_submission_time=3575.861748, global_step=9305, preemption_count=0, score=3575.861748, test/accuracy=0.428400, test/loss=2.714372, test/num_examples=10000, total_duration=4349.299989, train/accuracy=0.614676, train/loss=1.680441, validation/accuracy=0.562540, validation/loss=1.947512, validation/num_examples=50000
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0518 20:36:29.457431 140330364020480 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.712279, loss=3.005004
I0518 20:36:29.461446 140382513522496 submission.py:296] 9500) loss = 3.005, grad_norm = 1.712
I0518 20:39:41.140032 140330355627776 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.117675, loss=2.893403
I0518 20:39:41.144189 140382513522496 submission.py:296] 10000) loss = 2.893, grad_norm = 2.118
I0518 20:42:53.537816 140330364020480 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.792602, loss=2.968847
I0518 20:42:53.541815 140382513522496 submission.py:296] 10500) loss = 2.969, grad_norm = 1.793
I0518 20:43:44.961268 140382513522496 spec.py:298] Evaluating on the training split.
I0518 20:44:28.230076 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 20:45:13.594076 140382513522496 spec.py:326] Evaluating on the test split.
I0518 20:45:14.963781 140382513522496 submission_runner.py:421] Time since start: 4949.54s, 	Step: 10636, 	{'train/accuracy': 0.6358816964285714, 'train/loss': 1.6063190382354113, 'validation/accuracy': 0.57314, 'validation/loss': 1.9015153125, 'validation/num_examples': 50000, 'test/accuracy': 0.4472, 'test/loss': 2.6186091796875, 'test/num_examples': 10000, 'score': 4085.601354122162, 'total_duration': 4949.5352947711945, 'accumulated_submission_time': 4085.601354122162, 'accumulated_eval_time': 859.9618237018585, 'accumulated_logging_time': 0.15256786346435547}
I0518 20:45:14.973768 140330355627776 logging_writer.py:48] [10636] accumulated_eval_time=859.961824, accumulated_logging_time=0.152568, accumulated_submission_time=4085.601354, global_step=10636, preemption_count=0, score=4085.601354, test/accuracy=0.447200, test/loss=2.618609, test/num_examples=10000, total_duration=4949.535295, train/accuracy=0.635882, train/loss=1.606319, validation/accuracy=0.573140, validation/loss=1.901515, validation/num_examples=50000
I0518 20:47:34.267766 140330364020480 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.860740, loss=2.823886
I0518 20:47:34.271704 140382513522496 submission.py:296] 11000) loss = 2.824, grad_norm = 1.861
I0518 20:50:47.418979 140330355627776 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.696078, loss=2.662268
I0518 20:50:47.424047 140382513522496 submission.py:296] 11500) loss = 2.662, grad_norm = 1.696
I0518 20:53:45.302585 140382513522496 spec.py:298] Evaluating on the training split.
I0518 20:54:28.077408 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 20:55:12.360493 140382513522496 spec.py:326] Evaluating on the test split.
I0518 20:55:13.726146 140382513522496 submission_runner.py:421] Time since start: 5548.30s, 	Step: 11968, 	{'train/accuracy': 0.6757015306122449, 'train/loss': 1.4233513656927614, 'validation/accuracy': 0.6088, 'validation/loss': 1.73782859375, 'validation/num_examples': 50000, 'test/accuracy': 0.476, 'test/loss': 2.4551328125, 'test/num_examples': 10000, 'score': 4595.443540096283, 'total_duration': 5548.297689437866, 'accumulated_submission_time': 4595.443540096283, 'accumulated_eval_time': 948.3853883743286, 'accumulated_logging_time': 0.1709423065185547}
I0518 20:55:13.736685 140330364020480 logging_writer.py:48] [11968] accumulated_eval_time=948.385388, accumulated_logging_time=0.170942, accumulated_submission_time=4595.443540, global_step=11968, preemption_count=0, score=4595.443540, test/accuracy=0.476000, test/loss=2.455133, test/num_examples=10000, total_duration=5548.297689, train/accuracy=0.675702, train/loss=1.423351, validation/accuracy=0.608800, validation/loss=1.737829, validation/num_examples=50000
I0518 20:55:26.358740 140330355627776 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.047554, loss=2.719142
I0518 20:55:26.362966 140382513522496 submission.py:296] 12000) loss = 2.719, grad_norm = 1.048
I0518 20:58:37.775670 140330364020480 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.324684, loss=2.824089
I0518 20:58:37.781669 140382513522496 submission.py:296] 12500) loss = 2.824, grad_norm = 1.325
I0518 21:01:50.359356 140330355627776 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.238618, loss=2.679690
I0518 21:01:50.363804 140382513522496 submission.py:296] 13000) loss = 2.680, grad_norm = 1.239
I0518 21:03:43.911593 140382513522496 spec.py:298] Evaluating on the training split.
I0518 21:04:26.293332 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 21:05:10.794428 140382513522496 spec.py:326] Evaluating on the test split.
I0518 21:05:12.158926 140382513522496 submission_runner.py:421] Time since start: 6146.73s, 	Step: 13299, 	{'train/accuracy': 0.6989795918367347, 'train/loss': 1.3432129840461575, 'validation/accuracy': 0.62694, 'validation/loss': 1.67171625, 'validation/num_examples': 50000, 'test/accuracy': 0.4838, 'test/loss': 2.3989142578125, 'test/num_examples': 10000, 'score': 5105.136167764664, 'total_duration': 6146.730411529541, 'accumulated_submission_time': 5105.136167764664, 'accumulated_eval_time': 1036.6326570510864, 'accumulated_logging_time': 0.18971586227416992}
I0518 21:05:12.170263 140330364020480 logging_writer.py:48] [13299] accumulated_eval_time=1036.632657, accumulated_logging_time=0.189716, accumulated_submission_time=5105.136168, global_step=13299, preemption_count=0, score=5105.136168, test/accuracy=0.483800, test/loss=2.398914, test/num_examples=10000, total_duration=6146.730412, train/accuracy=0.698980, train/loss=1.343213, validation/accuracy=0.626940, validation/loss=1.671716, validation/num_examples=50000
I0518 21:06:29.294117 140330355627776 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.130994, loss=2.634950
I0518 21:06:29.298748 140382513522496 submission.py:296] 13500) loss = 2.635, grad_norm = 1.131
I0518 21:09:42.768751 140330364020480 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.349220, loss=2.568380
I0518 21:09:42.773727 140382513522496 submission.py:296] 14000) loss = 2.568, grad_norm = 1.349
I0518 21:12:53.117363 140330355627776 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.336283, loss=2.595670
I0518 21:12:53.121253 140382513522496 submission.py:296] 14500) loss = 2.596, grad_norm = 1.336
I0518 21:13:42.298972 140382513522496 spec.py:298] Evaluating on the training split.
I0518 21:14:25.135127 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 21:15:09.525609 140382513522496 spec.py:326] Evaluating on the test split.
I0518 21:15:10.883964 140382513522496 submission_runner.py:421] Time since start: 6745.46s, 	Step: 14630, 	{'train/accuracy': 0.7124521683673469, 'train/loss': 1.2556779044015067, 'validation/accuracy': 0.63666, 'validation/loss': 1.599663125, 'validation/num_examples': 50000, 'test/accuracy': 0.4981, 'test/loss': 2.339590234375, 'test/num_examples': 10000, 'score': 5614.786230802536, 'total_duration': 6745.455413341522, 'accumulated_submission_time': 5614.786230802536, 'accumulated_eval_time': 1125.2175080776215, 'accumulated_logging_time': 0.20892977714538574}
I0518 21:15:10.896559 140330364020480 logging_writer.py:48] [14630] accumulated_eval_time=1125.217508, accumulated_logging_time=0.208930, accumulated_submission_time=5614.786231, global_step=14630, preemption_count=0, score=5614.786231, test/accuracy=0.498100, test/loss=2.339590, test/num_examples=10000, total_duration=6745.455413, train/accuracy=0.712452, train/loss=1.255678, validation/accuracy=0.636660, validation/loss=1.599663, validation/num_examples=50000
I0518 21:17:33.289134 140330355627776 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.116336, loss=2.557165
I0518 21:17:33.296941 140382513522496 submission.py:296] 15000) loss = 2.557, grad_norm = 1.116
I0518 21:20:45.213633 140330364020480 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.894948, loss=2.509161
I0518 21:20:45.217648 140382513522496 submission.py:296] 15500) loss = 2.509, grad_norm = 0.895
I0518 21:23:41.118573 140382513522496 spec.py:298] Evaluating on the training split.
I0518 21:24:24.548385 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 21:25:11.197548 140382513522496 spec.py:326] Evaluating on the test split.
I0518 21:25:12.556634 140382513522496 submission_runner.py:421] Time since start: 7347.13s, 	Step: 15962, 	{'train/accuracy': 0.7258848852040817, 'train/loss': 1.2229691330267458, 'validation/accuracy': 0.64488, 'validation/loss': 1.5794928125, 'validation/num_examples': 50000, 'test/accuracy': 0.5138, 'test/loss': 2.27263828125, 'test/num_examples': 10000, 'score': 6124.526127815247, 'total_duration': 7347.12804889679, 'accumulated_submission_time': 6124.526127815247, 'accumulated_eval_time': 1216.655401468277, 'accumulated_logging_time': 0.2294330596923828}
I0518 21:25:12.567245 140330355627776 logging_writer.py:48] [15962] accumulated_eval_time=1216.655401, accumulated_logging_time=0.229433, accumulated_submission_time=6124.526128, global_step=15962, preemption_count=0, score=6124.526128, test/accuracy=0.513800, test/loss=2.272638, test/num_examples=10000, total_duration=7347.128049, train/accuracy=0.725885, train/loss=1.222969, validation/accuracy=0.644880, validation/loss=1.579493, validation/num_examples=50000
I0518 21:25:27.496500 140330364020480 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.935176, loss=2.502096
I0518 21:25:27.500223 140382513522496 submission.py:296] 16000) loss = 2.502, grad_norm = 0.935
I0518 21:28:40.527844 140330355627776 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.197666, loss=2.574663
I0518 21:28:40.532063 140382513522496 submission.py:296] 16500) loss = 2.575, grad_norm = 1.198
I0518 21:31:50.978812 140330364020480 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.941714, loss=2.590511
I0518 21:31:50.982599 140382513522496 submission.py:296] 17000) loss = 2.591, grad_norm = 0.942
I0518 21:33:42.903253 140382513522496 spec.py:298] Evaluating on the training split.
I0518 21:34:25.615299 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 21:35:10.301097 140382513522496 spec.py:326] Evaluating on the test split.
I0518 21:35:11.662950 140382513522496 submission_runner.py:421] Time since start: 7946.23s, 	Step: 17294, 	{'train/accuracy': 0.7338169642857143, 'train/loss': 1.1805511786013234, 'validation/accuracy': 0.6457, 'validation/loss': 1.56375359375, 'validation/num_examples': 50000, 'test/accuracy': 0.504, 'test/loss': 2.3082041015625, 'test/num_examples': 10000, 'score': 6634.378789424896, 'total_duration': 7946.234441041946, 'accumulated_submission_time': 6634.378789424896, 'accumulated_eval_time': 1305.4150018692017, 'accumulated_logging_time': 0.24859380722045898}
I0518 21:35:11.673278 140330355627776 logging_writer.py:48] [17294] accumulated_eval_time=1305.415002, accumulated_logging_time=0.248594, accumulated_submission_time=6634.378789, global_step=17294, preemption_count=0, score=6634.378789, test/accuracy=0.504000, test/loss=2.308204, test/num_examples=10000, total_duration=7946.234441, train/accuracy=0.733817, train/loss=1.180551, validation/accuracy=0.645700, validation/loss=1.563754, validation/num_examples=50000
I0518 21:36:31.358960 140330364020480 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.880777, loss=2.470695
I0518 21:36:31.363094 140382513522496 submission.py:296] 17500) loss = 2.471, grad_norm = 0.881
I0518 21:39:43.180583 140330355627776 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.140589, loss=2.511031
I0518 21:39:43.184608 140382513522496 submission.py:296] 18000) loss = 2.511, grad_norm = 1.141
I0518 21:42:53.891705 140330364020480 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.165945, loss=2.477717
I0518 21:42:53.899736 140382513522496 submission.py:296] 18500) loss = 2.478, grad_norm = 1.166
I0518 21:43:41.789427 140382513522496 spec.py:298] Evaluating on the training split.
I0518 21:44:24.946785 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 21:45:21.112396 140382513522496 spec.py:326] Evaluating on the test split.
I0518 21:45:22.478516 140382513522496 submission_runner.py:421] Time since start: 8557.05s, 	Step: 18626, 	{'train/accuracy': 0.7410514987244898, 'train/loss': 1.1649209625866948, 'validation/accuracy': 0.65286, 'validation/loss': 1.55810890625, 'validation/num_examples': 50000, 'test/accuracy': 0.5072, 'test/loss': 2.3147654296875, 'test/num_examples': 10000, 'score': 7144.013987064362, 'total_duration': 8557.04890036583, 'accumulated_submission_time': 7144.013987064362, 'accumulated_eval_time': 1406.1028895378113, 'accumulated_logging_time': 0.2682991027832031}
I0518 21:45:22.488413 140330355627776 logging_writer.py:48] [18626] accumulated_eval_time=1406.102890, accumulated_logging_time=0.268299, accumulated_submission_time=7144.013987, global_step=18626, preemption_count=0, score=7144.013987, test/accuracy=0.507200, test/loss=2.314765, test/num_examples=10000, total_duration=8557.048900, train/accuracy=0.741051, train/loss=1.164921, validation/accuracy=0.652860, validation/loss=1.558109, validation/num_examples=50000
I0518 21:47:47.695891 140330364020480 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.955010, loss=2.453778
I0518 21:47:47.700114 140382513522496 submission.py:296] 19000) loss = 2.454, grad_norm = 0.955
I0518 21:50:58.190890 140330355627776 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.031552, loss=2.446184
I0518 21:50:58.195322 140382513522496 submission.py:296] 19500) loss = 2.446, grad_norm = 1.032
I0518 21:53:52.549981 140382513522496 spec.py:298] Evaluating on the training split.
I0518 21:54:35.079025 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 21:55:20.606130 140382513522496 spec.py:326] Evaluating on the test split.
I0518 21:55:21.968281 140382513522496 submission_runner.py:421] Time since start: 9156.54s, 	Step: 19956, 	{'train/accuracy': 0.7547034438775511, 'train/loss': 1.082745532600247, 'validation/accuracy': 0.66292, 'validation/loss': 1.4848253125, 'validation/num_examples': 50000, 'test/accuracy': 0.5273, 'test/loss': 2.1948068359375, 'test/num_examples': 10000, 'score': 7653.589478492737, 'total_duration': 9156.539736747742, 'accumulated_submission_time': 7653.589478492737, 'accumulated_eval_time': 1495.5210893154144, 'accumulated_logging_time': 0.285844087600708}
I0518 21:55:21.980067 140330364020480 logging_writer.py:48] [19956] accumulated_eval_time=1495.521089, accumulated_logging_time=0.285844, accumulated_submission_time=7653.589478, global_step=19956, preemption_count=0, score=7653.589478, test/accuracy=0.527300, test/loss=2.194807, test/num_examples=10000, total_duration=9156.539737, train/accuracy=0.754703, train/loss=1.082746, validation/accuracy=0.662920, validation/loss=1.484825, validation/num_examples=50000
I0518 21:55:39.419974 140330355627776 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.826929, loss=2.375529
I0518 21:55:39.423742 140382513522496 submission.py:296] 20000) loss = 2.376, grad_norm = 0.827
I0518 21:58:51.742378 140330364020480 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.666628, loss=2.420109
I0518 21:58:51.746585 140382513522496 submission.py:296] 20500) loss = 2.420, grad_norm = 0.667
I0518 22:02:02.501689 140330355627776 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.157962, loss=2.380440
I0518 22:02:02.506050 140382513522496 submission.py:296] 21000) loss = 2.380, grad_norm = 1.158
I0518 22:03:52.311667 140382513522496 spec.py:298] Evaluating on the training split.
I0518 22:04:35.446668 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 22:05:33.430896 140382513522496 spec.py:326] Evaluating on the test split.
I0518 22:05:34.794330 140382513522496 submission_runner.py:421] Time since start: 9769.36s, 	Step: 21283, 	{'train/accuracy': 0.7664620535714286, 'train/loss': 0.9988330918915418, 'validation/accuracy': 0.6709, 'validation/loss': 1.42804265625, 'validation/num_examples': 50000, 'test/accuracy': 0.5355, 'test/loss': 2.1586837890625, 'test/num_examples': 10000, 'score': 8163.439563751221, 'total_duration': 9769.364540815353, 'accumulated_submission_time': 8163.439563751221, 'accumulated_eval_time': 1598.0024600028992, 'accumulated_logging_time': 0.3056151866912842}
I0518 22:05:34.805146 140330364020480 logging_writer.py:48] [21283] accumulated_eval_time=1598.002460, accumulated_logging_time=0.305615, accumulated_submission_time=8163.439564, global_step=21283, preemption_count=0, score=8163.439564, test/accuracy=0.535500, test/loss=2.158684, test/num_examples=10000, total_duration=9769.364541, train/accuracy=0.766462, train/loss=0.998833, validation/accuracy=0.670900, validation/loss=1.428043, validation/num_examples=50000
I0518 22:06:57.855870 140330355627776 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.868990, loss=2.451061
I0518 22:06:57.859745 140382513522496 submission.py:296] 21500) loss = 2.451, grad_norm = 0.869
I0518 22:10:08.262302 140330364020480 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.688269, loss=2.433995
I0518 22:10:08.266331 140382513522496 submission.py:296] 22000) loss = 2.434, grad_norm = 0.688
I0518 22:13:19.869220 140330355627776 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.695988, loss=2.427372
I0518 22:13:19.873926 140382513522496 submission.py:296] 22500) loss = 2.427, grad_norm = 0.696
I0518 22:14:05.024637 140382513522496 spec.py:298] Evaluating on the training split.
I0518 22:14:47.355276 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 22:15:44.072861 140382513522496 spec.py:326] Evaluating on the test split.
I0518 22:15:45.438414 140382513522496 submission_runner.py:421] Time since start: 10380.01s, 	Step: 22615, 	{'train/accuracy': 0.7702088647959183, 'train/loss': 1.0254446535694355, 'validation/accuracy': 0.67062, 'validation/loss': 1.47192546875, 'validation/num_examples': 50000, 'test/accuracy': 0.5322, 'test/loss': 2.1863013671875, 'test/num_examples': 10000, 'score': 8673.175226211548, 'total_duration': 10380.009854793549, 'accumulated_submission_time': 8673.175226211548, 'accumulated_eval_time': 1698.4160785675049, 'accumulated_logging_time': 0.3265230655670166}
I0518 22:15:45.449688 140330364020480 logging_writer.py:48] [22615] accumulated_eval_time=1698.416079, accumulated_logging_time=0.326523, accumulated_submission_time=8673.175226, global_step=22615, preemption_count=0, score=8673.175226, test/accuracy=0.532200, test/loss=2.186301, test/num_examples=10000, total_duration=10380.009855, train/accuracy=0.770209, train/loss=1.025445, validation/accuracy=0.670620, validation/loss=1.471925, validation/num_examples=50000
I0518 22:18:12.412736 140330355627776 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.656087, loss=2.416844
I0518 22:18:12.416793 140382513522496 submission.py:296] 23000) loss = 2.417, grad_norm = 0.656
I0518 22:21:22.928746 140330364020480 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.751078, loss=2.355133
I0518 22:21:22.933240 140382513522496 submission.py:296] 23500) loss = 2.355, grad_norm = 0.751
I0518 22:24:15.515433 140382513522496 spec.py:298] Evaluating on the training split.
I0518 22:24:58.522304 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 22:25:53.746348 140382513522496 spec.py:326] Evaluating on the test split.
I0518 22:25:55.105133 140382513522496 submission_runner.py:421] Time since start: 10989.68s, 	Step: 23947, 	{'train/accuracy': 0.7843590561224489, 'train/loss': 0.9425930101044324, 'validation/accuracy': 0.67904, 'validation/loss': 1.4034834375, 'validation/num_examples': 50000, 'test/accuracy': 0.5343, 'test/loss': 2.1527544921875, 'test/num_examples': 10000, 'score': 9182.76004934311, 'total_duration': 10989.67537021637, 'accumulated_submission_time': 9182.76004934311, 'accumulated_eval_time': 1798.004454612732, 'accumulated_logging_time': 0.34564995765686035}
I0518 22:25:55.115882 140330355627776 logging_writer.py:48] [23947] accumulated_eval_time=1798.004455, accumulated_logging_time=0.345650, accumulated_submission_time=9182.760049, global_step=23947, preemption_count=0, score=9182.760049, test/accuracy=0.534300, test/loss=2.152754, test/num_examples=10000, total_duration=10989.675370, train/accuracy=0.784359, train/loss=0.942593, validation/accuracy=0.679040, validation/loss=1.403483, validation/num_examples=50000
I0518 22:26:15.733803 140330364020480 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.798409, loss=2.348090
I0518 22:26:15.737421 140382513522496 submission.py:296] 24000) loss = 2.348, grad_norm = 0.798
I0518 22:29:26.186451 140330355627776 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.675096, loss=2.351142
I0518 22:29:26.190731 140382513522496 submission.py:296] 24500) loss = 2.351, grad_norm = 0.675
I0518 22:32:38.006221 140330364020480 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.670253, loss=2.384678
I0518 22:32:38.015653 140382513522496 submission.py:296] 25000) loss = 2.385, grad_norm = 0.670
I0518 22:34:25.435127 140382513522496 spec.py:298] Evaluating on the training split.
I0518 22:35:08.615861 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 22:36:03.953735 140382513522496 spec.py:326] Evaluating on the test split.
I0518 22:36:05.330810 140382513522496 submission_runner.py:421] Time since start: 11599.90s, 	Step: 25278, 	{'train/accuracy': 0.7897401147959183, 'train/loss': 0.9350654446348852, 'validation/accuracy': 0.68388, 'validation/loss': 1.40380625, 'validation/num_examples': 50000, 'test/accuracy': 0.5404, 'test/loss': 2.1344083984375, 'test/num_examples': 10000, 'score': 9692.599747657776, 'total_duration': 11599.902292013168, 'accumulated_submission_time': 9692.599747657776, 'accumulated_eval_time': 1897.9000186920166, 'accumulated_logging_time': 0.36431455612182617}
I0518 22:36:05.340968 140330355627776 logging_writer.py:48] [25278] accumulated_eval_time=1897.900019, accumulated_logging_time=0.364315, accumulated_submission_time=9692.599748, global_step=25278, preemption_count=0, score=9692.599748, test/accuracy=0.540400, test/loss=2.134408, test/num_examples=10000, total_duration=11599.902292, train/accuracy=0.789740, train/loss=0.935065, validation/accuracy=0.683880, validation/loss=1.403806, validation/num_examples=50000
I0518 22:37:30.288917 140330364020480 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.733513, loss=2.348078
I0518 22:37:30.293352 140382513522496 submission.py:296] 25500) loss = 2.348, grad_norm = 0.734
I0518 22:40:40.843719 140330355627776 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.664180, loss=2.292814
I0518 22:40:40.848732 140382513522496 submission.py:296] 26000) loss = 2.293, grad_norm = 0.664
I0518 22:43:54.248217 140330364020480 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.614626, loss=2.333603
I0518 22:43:54.253844 140382513522496 submission.py:296] 26500) loss = 2.334, grad_norm = 0.615
I0518 22:44:35.408360 140382513522496 spec.py:298] Evaluating on the training split.
I0518 22:45:19.148166 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 22:46:05.321445 140382513522496 spec.py:326] Evaluating on the test split.
I0518 22:46:06.687445 140382513522496 submission_runner.py:421] Time since start: 12201.26s, 	Step: 26609, 	{'train/accuracy': 0.7974131058673469, 'train/loss': 0.9110342531788106, 'validation/accuracy': 0.6829, 'validation/loss': 1.40137203125, 'validation/num_examples': 50000, 'test/accuracy': 0.5456, 'test/loss': 2.1525875, 'test/num_examples': 10000, 'score': 10202.183466434479, 'total_duration': 12201.257165908813, 'accumulated_submission_time': 10202.183466434479, 'accumulated_eval_time': 1989.1772458553314, 'accumulated_logging_time': 0.3827633857727051}
I0518 22:46:06.698485 140330355627776 logging_writer.py:48] [26609] accumulated_eval_time=1989.177246, accumulated_logging_time=0.382763, accumulated_submission_time=10202.183466, global_step=26609, preemption_count=0, score=10202.183466, test/accuracy=0.545600, test/loss=2.152588, test/num_examples=10000, total_duration=12201.257166, train/accuracy=0.797413, train/loss=0.911034, validation/accuracy=0.682900, validation/loss=1.401372, validation/num_examples=50000
I0518 22:48:35.966840 140330364020480 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.923570, loss=2.357096
I0518 22:48:35.971191 140382513522496 submission.py:296] 27000) loss = 2.357, grad_norm = 0.924
I0518 22:51:47.444501 140330355627776 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.839793, loss=2.299429
I0518 22:51:47.451473 140382513522496 submission.py:296] 27500) loss = 2.299, grad_norm = 0.840
I0518 22:54:37.051624 140382513522496 spec.py:298] Evaluating on the training split.
I0518 22:55:20.656229 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 22:56:12.755361 140382513522496 spec.py:326] Evaluating on the test split.
I0518 22:56:14.119797 140382513522496 submission_runner.py:421] Time since start: 12808.69s, 	Step: 27941, 	{'train/accuracy': 0.7901785714285714, 'train/loss': 0.9217141599071269, 'validation/accuracy': 0.67502, 'validation/loss': 1.42231078125, 'validation/num_examples': 50000, 'test/accuracy': 0.5354, 'test/loss': 2.1812041015625, 'test/num_examples': 10000, 'score': 10712.057082176208, 'total_duration': 12808.691358566284, 'accumulated_submission_time': 10712.057082176208, 'accumulated_eval_time': 2086.2454018592834, 'accumulated_logging_time': 0.4024844169616699}
I0518 22:56:14.131156 140330364020480 logging_writer.py:48] [27941] accumulated_eval_time=2086.245402, accumulated_logging_time=0.402484, accumulated_submission_time=10712.057082, global_step=27941, preemption_count=0, score=10712.057082, test/accuracy=0.535400, test/loss=2.181204, test/num_examples=10000, total_duration=12808.691359, train/accuracy=0.790179, train/loss=0.921714, validation/accuracy=0.675020, validation/loss=1.422311, validation/num_examples=50000
I0518 22:56:36.589337 140382513522496 spec.py:298] Evaluating on the training split.
I0518 22:57:18.784576 140382513522496 spec.py:310] Evaluating on the validation split.
I0518 22:58:03.249609 140382513522496 spec.py:326] Evaluating on the test split.
I0518 22:58:04.608903 140382513522496 submission_runner.py:421] Time since start: 12919.18s, 	Step: 28000, 	{'train/accuracy': 0.7942641900510204, 'train/loss': 0.9071872477628746, 'validation/accuracy': 0.6798, 'validation/loss': 1.40614171875, 'validation/num_examples': 50000, 'test/accuracy': 0.5371, 'test/loss': 2.1783234375, 'test/num_examples': 10000, 'score': 10734.486578702927, 'total_duration': 12919.18037056923, 'accumulated_submission_time': 10734.486578702927, 'accumulated_eval_time': 2174.264847278595, 'accumulated_logging_time': 0.4214935302734375}
I0518 22:58:04.620019 140330355627776 logging_writer.py:48] [28000] accumulated_eval_time=2174.264847, accumulated_logging_time=0.421494, accumulated_submission_time=10734.486579, global_step=28000, preemption_count=0, score=10734.486579, test/accuracy=0.537100, test/loss=2.178323, test/num_examples=10000, total_duration=12919.180371, train/accuracy=0.794264, train/loss=0.907187, validation/accuracy=0.679800, validation/loss=1.406142, validation/num_examples=50000
I0518 22:58:04.636445 140330364020480 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10734.486579
I0518 22:58:05.320965 140382513522496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_pytorch/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_28000.
I0518 22:58:05.583333 140382513522496 submission_runner.py:584] Tuning trial 1/1
I0518 22:58:05.583535 140382513522496 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0518 22:58:05.584559 140382513522496 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0012555803571428572, 'train/loss': 6.927241111288265, 'validation/accuracy': 0.0014, 'validation/loss': 6.926903125, 'validation/num_examples': 50000, 'test/accuracy': 0.0012, 'test/loss': 6.93054921875, 'test/num_examples': 10000, 'score': 7.973525285720825, 'total_duration': 133.00902700424194, 'accumulated_submission_time': 7.973525285720825, 'accumulated_eval_time': 125.03431963920593, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1318, {'train/accuracy': 0.11870216836734694, 'train/loss': 4.773458986866231, 'validation/accuracy': 0.10706, 'validation/loss': 4.855113125, 'validation/num_examples': 50000, 'test/accuracy': 0.0734, 'test/loss': 5.261117578125, 'test/num_examples': 10000, 'score': 517.6791594028473, 'total_duration': 739.5341739654541, 'accumulated_submission_time': 517.6791594028473, 'accumulated_eval_time': 221.30411386489868, 'accumulated_logging_time': 0.027118444442749023, 'global_step': 1318, 'preemption_count': 0}), (2648, {'train/accuracy': 0.2749123086734694, 'train/loss': 3.5815887451171875, 'validation/accuracy': 0.25062, 'validation/loss': 3.71039375, 'validation/num_examples': 50000, 'test/accuracy': 0.1818, 'test/loss': 4.290884765625, 'test/num_examples': 10000, 'score': 1027.258706331253, 'total_duration': 1350.9885404109955, 'accumulated_submission_time': 1027.258706331253, 'accumulated_eval_time': 322.69152760505676, 'accumulated_logging_time': 0.04552316665649414, 'global_step': 2648, 'preemption_count': 0}), (3981, {'train/accuracy': 0.37412308673469385, 'train/loss': 2.8935515734614157, 'validation/accuracy': 0.34858, 'validation/loss': 3.054421875, 'validation/num_examples': 50000, 'test/accuracy': 0.2474, 'test/loss': 3.787951171875, 'test/num_examples': 10000, 'score': 1537.1315760612488, 'total_duration': 1946.6438481807709, 'accumulated_submission_time': 1537.1315760612488, 'accumulated_eval_time': 407.9905982017517, 'accumulated_logging_time': 0.06451606750488281, 'global_step': 3981, 'preemption_count': 0}), (5311, {'train/accuracy': 0.4767219387755102, 'train/loss': 2.374318025550064, 'validation/accuracy': 0.435, 'validation/loss': 2.59030234375, 'validation/num_examples': 50000, 'test/accuracy': 0.3204, 'test/loss': 3.33454296875, 'test/num_examples': 10000, 'score': 2046.7515587806702, 'total_duration': 2555.4468989372253, 'accumulated_submission_time': 2046.7515587806702, 'accumulated_eval_time': 506.6855614185333, 'accumulated_logging_time': 0.08177065849304199, 'global_step': 5311, 'preemption_count': 0}), (6643, {'train/accuracy': 0.5316286670918368, 'train/loss': 2.0954923045878506, 'validation/accuracy': 0.49002, 'validation/loss': 2.3066215625, 'validation/num_examples': 50000, 'test/accuracy': 0.3546, 'test/loss': 3.0969744140625, 'test/num_examples': 10000, 'score': 2556.523392677307, 'total_duration': 3152.2989926338196, 'accumulated_submission_time': 2556.523392677307, 'accumulated_eval_time': 593.2796285152435, 'accumulated_logging_time': 0.10029315948486328, 'global_step': 6643, 'preemption_count': 0}), (7973, {'train/accuracy': 0.5887276785714286, 'train/loss': 1.837281051947146, 'validation/accuracy': 0.53776, 'validation/loss': 2.08126859375, 'validation/num_examples': 50000, 'test/accuracy': 0.4057, 'test/loss': 2.818893359375, 'test/num_examples': 10000, 'score': 3066.049071788788, 'total_duration': 3751.498022079468, 'accumulated_submission_time': 3066.049071788788, 'accumulated_eval_time': 682.4597043991089, 'accumulated_logging_time': 0.11752724647521973, 'global_step': 7973, 'preemption_count': 0}), (9305, {'train/accuracy': 0.6146763392857143, 'train/loss': 1.680441408741231, 'validation/accuracy': 0.56254, 'validation/loss': 1.947511875, 'validation/num_examples': 50000, 'test/accuracy': 0.4284, 'test/loss': 2.714372265625, 'test/num_examples': 10000, 'score': 3575.861747980118, 'total_duration': 4349.29998922348, 'accumulated_submission_time': 3575.861747980118, 'accumulated_eval_time': 769.9593527317047, 'accumulated_logging_time': 0.13484525680541992, 'global_step': 9305, 'preemption_count': 0}), (10636, {'train/accuracy': 0.6358816964285714, 'train/loss': 1.6063190382354113, 'validation/accuracy': 0.57314, 'validation/loss': 1.9015153125, 'validation/num_examples': 50000, 'test/accuracy': 0.4472, 'test/loss': 2.6186091796875, 'test/num_examples': 10000, 'score': 4085.601354122162, 'total_duration': 4949.5352947711945, 'accumulated_submission_time': 4085.601354122162, 'accumulated_eval_time': 859.9618237018585, 'accumulated_logging_time': 0.15256786346435547, 'global_step': 10636, 'preemption_count': 0}), (11968, {'train/accuracy': 0.6757015306122449, 'train/loss': 1.4233513656927614, 'validation/accuracy': 0.6088, 'validation/loss': 1.73782859375, 'validation/num_examples': 50000, 'test/accuracy': 0.476, 'test/loss': 2.4551328125, 'test/num_examples': 10000, 'score': 4595.443540096283, 'total_duration': 5548.297689437866, 'accumulated_submission_time': 4595.443540096283, 'accumulated_eval_time': 948.3853883743286, 'accumulated_logging_time': 0.1709423065185547, 'global_step': 11968, 'preemption_count': 0}), (13299, {'train/accuracy': 0.6989795918367347, 'train/loss': 1.3432129840461575, 'validation/accuracy': 0.62694, 'validation/loss': 1.67171625, 'validation/num_examples': 50000, 'test/accuracy': 0.4838, 'test/loss': 2.3989142578125, 'test/num_examples': 10000, 'score': 5105.136167764664, 'total_duration': 6146.730411529541, 'accumulated_submission_time': 5105.136167764664, 'accumulated_eval_time': 1036.6326570510864, 'accumulated_logging_time': 0.18971586227416992, 'global_step': 13299, 'preemption_count': 0}), (14630, {'train/accuracy': 0.7124521683673469, 'train/loss': 1.2556779044015067, 'validation/accuracy': 0.63666, 'validation/loss': 1.599663125, 'validation/num_examples': 50000, 'test/accuracy': 0.4981, 'test/loss': 2.339590234375, 'test/num_examples': 10000, 'score': 5614.786230802536, 'total_duration': 6745.455413341522, 'accumulated_submission_time': 5614.786230802536, 'accumulated_eval_time': 1125.2175080776215, 'accumulated_logging_time': 0.20892977714538574, 'global_step': 14630, 'preemption_count': 0}), (15962, {'train/accuracy': 0.7258848852040817, 'train/loss': 1.2229691330267458, 'validation/accuracy': 0.64488, 'validation/loss': 1.5794928125, 'validation/num_examples': 50000, 'test/accuracy': 0.5138, 'test/loss': 2.27263828125, 'test/num_examples': 10000, 'score': 6124.526127815247, 'total_duration': 7347.12804889679, 'accumulated_submission_time': 6124.526127815247, 'accumulated_eval_time': 1216.655401468277, 'accumulated_logging_time': 0.2294330596923828, 'global_step': 15962, 'preemption_count': 0}), (17294, {'train/accuracy': 0.7338169642857143, 'train/loss': 1.1805511786013234, 'validation/accuracy': 0.6457, 'validation/loss': 1.56375359375, 'validation/num_examples': 50000, 'test/accuracy': 0.504, 'test/loss': 2.3082041015625, 'test/num_examples': 10000, 'score': 6634.378789424896, 'total_duration': 7946.234441041946, 'accumulated_submission_time': 6634.378789424896, 'accumulated_eval_time': 1305.4150018692017, 'accumulated_logging_time': 0.24859380722045898, 'global_step': 17294, 'preemption_count': 0}), (18626, {'train/accuracy': 0.7410514987244898, 'train/loss': 1.1649209625866948, 'validation/accuracy': 0.65286, 'validation/loss': 1.55810890625, 'validation/num_examples': 50000, 'test/accuracy': 0.5072, 'test/loss': 2.3147654296875, 'test/num_examples': 10000, 'score': 7144.013987064362, 'total_duration': 8557.04890036583, 'accumulated_submission_time': 7144.013987064362, 'accumulated_eval_time': 1406.1028895378113, 'accumulated_logging_time': 0.2682991027832031, 'global_step': 18626, 'preemption_count': 0}), (19956, {'train/accuracy': 0.7547034438775511, 'train/loss': 1.082745532600247, 'validation/accuracy': 0.66292, 'validation/loss': 1.4848253125, 'validation/num_examples': 50000, 'test/accuracy': 0.5273, 'test/loss': 2.1948068359375, 'test/num_examples': 10000, 'score': 7653.589478492737, 'total_duration': 9156.539736747742, 'accumulated_submission_time': 7653.589478492737, 'accumulated_eval_time': 1495.5210893154144, 'accumulated_logging_time': 0.285844087600708, 'global_step': 19956, 'preemption_count': 0}), (21283, {'train/accuracy': 0.7664620535714286, 'train/loss': 0.9988330918915418, 'validation/accuracy': 0.6709, 'validation/loss': 1.42804265625, 'validation/num_examples': 50000, 'test/accuracy': 0.5355, 'test/loss': 2.1586837890625, 'test/num_examples': 10000, 'score': 8163.439563751221, 'total_duration': 9769.364540815353, 'accumulated_submission_time': 8163.439563751221, 'accumulated_eval_time': 1598.0024600028992, 'accumulated_logging_time': 0.3056151866912842, 'global_step': 21283, 'preemption_count': 0}), (22615, {'train/accuracy': 0.7702088647959183, 'train/loss': 1.0254446535694355, 'validation/accuracy': 0.67062, 'validation/loss': 1.47192546875, 'validation/num_examples': 50000, 'test/accuracy': 0.5322, 'test/loss': 2.1863013671875, 'test/num_examples': 10000, 'score': 8673.175226211548, 'total_duration': 10380.009854793549, 'accumulated_submission_time': 8673.175226211548, 'accumulated_eval_time': 1698.4160785675049, 'accumulated_logging_time': 0.3265230655670166, 'global_step': 22615, 'preemption_count': 0}), (23947, {'train/accuracy': 0.7843590561224489, 'train/loss': 0.9425930101044324, 'validation/accuracy': 0.67904, 'validation/loss': 1.4034834375, 'validation/num_examples': 50000, 'test/accuracy': 0.5343, 'test/loss': 2.1527544921875, 'test/num_examples': 10000, 'score': 9182.76004934311, 'total_duration': 10989.67537021637, 'accumulated_submission_time': 9182.76004934311, 'accumulated_eval_time': 1798.004454612732, 'accumulated_logging_time': 0.34564995765686035, 'global_step': 23947, 'preemption_count': 0}), (25278, {'train/accuracy': 0.7897401147959183, 'train/loss': 0.9350654446348852, 'validation/accuracy': 0.68388, 'validation/loss': 1.40380625, 'validation/num_examples': 50000, 'test/accuracy': 0.5404, 'test/loss': 2.1344083984375, 'test/num_examples': 10000, 'score': 9692.599747657776, 'total_duration': 11599.902292013168, 'accumulated_submission_time': 9692.599747657776, 'accumulated_eval_time': 1897.9000186920166, 'accumulated_logging_time': 0.36431455612182617, 'global_step': 25278, 'preemption_count': 0}), (26609, {'train/accuracy': 0.7974131058673469, 'train/loss': 0.9110342531788106, 'validation/accuracy': 0.6829, 'validation/loss': 1.40137203125, 'validation/num_examples': 50000, 'test/accuracy': 0.5456, 'test/loss': 2.1525875, 'test/num_examples': 10000, 'score': 10202.183466434479, 'total_duration': 12201.257165908813, 'accumulated_submission_time': 10202.183466434479, 'accumulated_eval_time': 1989.1772458553314, 'accumulated_logging_time': 0.3827633857727051, 'global_step': 26609, 'preemption_count': 0}), (27941, {'train/accuracy': 0.7901785714285714, 'train/loss': 0.9217141599071269, 'validation/accuracy': 0.67502, 'validation/loss': 1.42231078125, 'validation/num_examples': 50000, 'test/accuracy': 0.5354, 'test/loss': 2.1812041015625, 'test/num_examples': 10000, 'score': 10712.057082176208, 'total_duration': 12808.691358566284, 'accumulated_submission_time': 10712.057082176208, 'accumulated_eval_time': 2086.2454018592834, 'accumulated_logging_time': 0.4024844169616699, 'global_step': 27941, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7942641900510204, 'train/loss': 0.9071872477628746, 'validation/accuracy': 0.6798, 'validation/loss': 1.40614171875, 'validation/num_examples': 50000, 'test/accuracy': 0.5371, 'test/loss': 2.1783234375, 'test/num_examples': 10000, 'score': 10734.486578702927, 'total_duration': 12919.18037056923, 'accumulated_submission_time': 10734.486578702927, 'accumulated_eval_time': 2174.264847278595, 'accumulated_logging_time': 0.4214935302734375, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0518 22:58:05.584672 140382513522496 submission_runner.py:587] Timing: 10734.486578702927
I0518 22:58:05.584723 140382513522496 submission_runner.py:588] ====================
I0518 22:58:05.584837 140382513522496 submission_runner.py:651] Final imagenet_resnet score: 10734.486578702927
