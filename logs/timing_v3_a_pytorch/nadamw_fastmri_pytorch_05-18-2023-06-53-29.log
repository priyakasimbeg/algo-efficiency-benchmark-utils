torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_pytorch/timing_nadamw --overwrite=True --save_checkpoints=False --max_global_steps=5428 2>&1 | tee -a /logs/fastmri_pytorch_05-18-2023-06-53-29.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0518 06:53:52.461177 140583914477376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0518 06:53:52.461197 140139122177856 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0518 06:53:52.461195 140075628922688 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0518 06:53:53.449327 139981063649088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0518 06:53:53.449314 139662168328000 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0518 06:53:53.449365 139873958627136 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0518 06:53:53.449396 140219364472640 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0518 06:53:53.450494 140354255357760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0518 06:53:53.450934 140354255357760 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 06:53:53.452308 140075628922688 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 06:53:53.453457 140139122177856 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 06:53:53.454134 140583914477376 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 06:53:53.460046 139981063649088 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 06:53:53.460025 139662168328000 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 06:53:53.460094 139873958627136 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 06:53:53.460134 140219364472640 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 06:53:54.031014 140354255357760 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_pytorch/timing_nadamw/fastmri_pytorch.
W0518 06:53:54.040577 140583914477376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 06:53:54.041214 140139122177856 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 06:53:54.042593 139981063649088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 06:53:54.043419 139662168328000 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 06:53:54.043502 140219364472640 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 06:53:54.045061 139873958627136 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 06:53:54.062806 140354255357760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 06:53:54.062974 140075628922688 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0518 06:53:54.067513 140354255357760 submission_runner.py:544] Using RNG seed 764241455
I0518 06:53:54.068911 140354255357760 submission_runner.py:553] --- Tuning run 1/1 ---
I0518 06:53:54.069047 140354255357760 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_pytorch/timing_nadamw/fastmri_pytorch/trial_1.
I0518 06:53:54.069271 140354255357760 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_pytorch/timing_nadamw/fastmri_pytorch/trial_1/hparams.json.
I0518 06:53:54.070211 140354255357760 submission_runner.py:241] Initializing dataset.
I0518 06:53:54.070335 140354255357760 submission_runner.py:248] Initializing model.
I0518 06:53:58.271829 140354255357760 submission_runner.py:258] Initializing optimizer.
I0518 06:53:58.272706 140354255357760 submission_runner.py:265] Initializing metrics bundle.
I0518 06:53:58.272819 140354255357760 submission_runner.py:283] Initializing checkpoint and logger.
I0518 06:53:58.275815 140354255357760 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0518 06:53:58.275922 140354255357760 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0518 06:53:58.726736 140354255357760 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_pytorch/timing_nadamw/fastmri_pytorch/trial_1/meta_data_0.json.
I0518 06:53:58.727591 140354255357760 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_pytorch/timing_nadamw/fastmri_pytorch/trial_1/flags_0.json.
I0518 06:53:58.776862 140354255357760 submission_runner.py:319] Starting training loop.
I0518 06:54:45.015834 140312244643584 logging_writer.py:48] [0] global_step=0, grad_norm=4.278861, loss=1.165950
I0518 06:54:45.023007 140354255357760 submission.py:296] 0) loss = 1.166, grad_norm = 4.279
I0518 06:54:45.024796 140354255357760 spec.py:298] Evaluating on the training split.
I0518 06:56:22.523438 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 06:57:27.128484 140354255357760 spec.py:326] Evaluating on the test split.
I0518 06:58:28.300271 140354255357760 submission_runner.py:421] Time since start: 269.52s, 	Step: 1, 	{'train/ssim': 0.19099199771881104, 'train/loss': 1.1586253302437919, 'validation/ssim': 0.18754581930131542, 'validation/loss': 1.1574455608996905, 'validation/num_examples': 3554, 'test/ssim': 0.21058086788344735, 'test/loss': 1.1514768155717676, 'test/num_examples': 3581, 'score': 46.24677062034607, 'total_duration': 269.52392745018005, 'accumulated_submission_time': 46.24677062034607, 'accumulated_eval_time': 223.27545285224915, 'accumulated_logging_time': 0}
I0518 06:58:28.316738 140288588769024 logging_writer.py:48] [1] accumulated_eval_time=223.275453, accumulated_logging_time=0, accumulated_submission_time=46.246771, global_step=1, preemption_count=0, score=46.246771, test/loss=1.151477, test/num_examples=3581, test/ssim=0.210581, total_duration=269.523927, train/loss=1.158625, train/ssim=0.190992, validation/loss=1.157446, validation/num_examples=3554, validation/ssim=0.187546
I0518 06:58:28.341591 140075628922688 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 06:58:28.341591 140583914477376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 06:58:28.341616 140219364472640 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 06:58:28.341622 140139122177856 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 06:58:28.341682 139981063649088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 06:58:28.341665 139662168328000 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 06:58:28.341748 140354255357760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 06:58:28.341701 139873958627136 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 06:58:28.401698 140288580376320 logging_writer.py:48] [1] global_step=1, grad_norm=4.539653, loss=1.148177
I0518 06:58:28.408152 140354255357760 submission.py:296] 1) loss = 1.148, grad_norm = 4.540
I0518 06:58:28.485571 140288588769024 logging_writer.py:48] [2] global_step=2, grad_norm=4.453014, loss=1.153569
I0518 06:58:28.491803 140354255357760 submission.py:296] 2) loss = 1.154, grad_norm = 4.453
I0518 06:58:28.566998 140288580376320 logging_writer.py:48] [3] global_step=3, grad_norm=4.710705, loss=1.126581
I0518 06:58:28.570527 140354255357760 submission.py:296] 3) loss = 1.127, grad_norm = 4.711
I0518 06:58:28.640045 140288588769024 logging_writer.py:48] [4] global_step=4, grad_norm=4.341722, loss=1.134570
I0518 06:58:28.643314 140354255357760 submission.py:296] 4) loss = 1.135, grad_norm = 4.342
I0518 06:58:28.720000 140288580376320 logging_writer.py:48] [5] global_step=5, grad_norm=4.698216, loss=1.122279
I0518 06:58:28.725337 140354255357760 submission.py:296] 5) loss = 1.122, grad_norm = 4.698
I0518 06:58:28.801552 140288588769024 logging_writer.py:48] [6] global_step=6, grad_norm=4.385350, loss=1.109339
I0518 06:58:28.807243 140354255357760 submission.py:296] 6) loss = 1.109, grad_norm = 4.385
I0518 06:58:28.885706 140288580376320 logging_writer.py:48] [7] global_step=7, grad_norm=4.477614, loss=1.090088
I0518 06:58:28.889025 140354255357760 submission.py:296] 7) loss = 1.090, grad_norm = 4.478
I0518 06:58:28.959524 140288588769024 logging_writer.py:48] [8] global_step=8, grad_norm=4.603953, loss=1.110737
I0518 06:58:28.964989 140354255357760 submission.py:296] 8) loss = 1.111, grad_norm = 4.604
I0518 06:58:29.040843 140288580376320 logging_writer.py:48] [9] global_step=9, grad_norm=4.526439, loss=1.073610
I0518 06:58:29.045816 140354255357760 submission.py:296] 9) loss = 1.074, grad_norm = 4.526
I0518 06:58:29.123558 140288588769024 logging_writer.py:48] [10] global_step=10, grad_norm=4.785537, loss=1.075471
I0518 06:58:29.126761 140354255357760 submission.py:296] 10) loss = 1.075, grad_norm = 4.786
I0518 06:58:29.194431 140288580376320 logging_writer.py:48] [11] global_step=11, grad_norm=3.926973, loss=1.069142
I0518 06:58:29.197671 140354255357760 submission.py:296] 11) loss = 1.069, grad_norm = 3.927
I0518 06:58:29.269658 140288588769024 logging_writer.py:48] [12] global_step=12, grad_norm=5.000561, loss=1.059843
I0518 06:58:29.275108 140354255357760 submission.py:296] 12) loss = 1.060, grad_norm = 5.001
I0518 06:58:29.351361 140288580376320 logging_writer.py:48] [13] global_step=13, grad_norm=4.172801, loss=1.031479
I0518 06:58:29.357684 140354255357760 submission.py:296] 13) loss = 1.031, grad_norm = 4.173
I0518 06:58:29.428222 140288588769024 logging_writer.py:48] [14] global_step=14, grad_norm=4.481201, loss=0.945104
I0518 06:58:29.431499 140354255357760 submission.py:296] 14) loss = 0.945, grad_norm = 4.481
I0518 06:58:29.748536 140288580376320 logging_writer.py:48] [15] global_step=15, grad_norm=4.508169, loss=0.948626
I0518 06:58:29.752028 140354255357760 submission.py:296] 15) loss = 0.949, grad_norm = 4.508
I0518 06:58:30.023327 140288588769024 logging_writer.py:48] [16] global_step=16, grad_norm=4.462422, loss=0.936554
I0518 06:58:30.026990 140354255357760 submission.py:296] 16) loss = 0.937, grad_norm = 4.462
I0518 06:58:30.274554 140288580376320 logging_writer.py:48] [17] global_step=17, grad_norm=4.287515, loss=0.908839
I0518 06:58:30.280583 140354255357760 submission.py:296] 17) loss = 0.909, grad_norm = 4.288
I0518 06:58:30.546321 140288588769024 logging_writer.py:48] [18] global_step=18, grad_norm=3.690485, loss=0.877342
I0518 06:58:30.552662 140354255357760 submission.py:296] 18) loss = 0.877, grad_norm = 3.690
I0518 06:58:30.793687 140288580376320 logging_writer.py:48] [19] global_step=19, grad_norm=4.075157, loss=0.921975
I0518 06:58:30.799758 140354255357760 submission.py:296] 19) loss = 0.922, grad_norm = 4.075
I0518 06:58:31.060369 140288588769024 logging_writer.py:48] [20] global_step=20, grad_norm=4.108899, loss=0.858499
I0518 06:58:31.065390 140354255357760 submission.py:296] 20) loss = 0.858, grad_norm = 4.109
I0518 06:58:31.332265 140288580376320 logging_writer.py:48] [21] global_step=21, grad_norm=3.591617, loss=0.827985
I0518 06:58:31.335641 140354255357760 submission.py:296] 21) loss = 0.828, grad_norm = 3.592
I0518 06:58:31.557551 140288588769024 logging_writer.py:48] [22] global_step=22, grad_norm=3.487762, loss=0.815880
I0518 06:58:31.563079 140354255357760 submission.py:296] 22) loss = 0.816, grad_norm = 3.488
I0518 06:58:31.851671 140288580376320 logging_writer.py:48] [23] global_step=23, grad_norm=3.352364, loss=0.786475
I0518 06:58:31.856404 140354255357760 submission.py:296] 23) loss = 0.786, grad_norm = 3.352
I0518 06:58:32.174304 140288588769024 logging_writer.py:48] [24] global_step=24, grad_norm=3.122867, loss=0.744023
I0518 06:58:32.180444 140354255357760 submission.py:296] 24) loss = 0.744, grad_norm = 3.123
I0518 06:58:32.464094 140288580376320 logging_writer.py:48] [25] global_step=25, grad_norm=3.150495, loss=0.774673
I0518 06:58:32.469363 140354255357760 submission.py:296] 25) loss = 0.775, grad_norm = 3.150
I0518 06:58:32.660280 140288588769024 logging_writer.py:48] [26] global_step=26, grad_norm=2.684964, loss=0.802455
I0518 06:58:32.663379 140354255357760 submission.py:296] 26) loss = 0.802, grad_norm = 2.685
I0518 06:58:32.939292 140288580376320 logging_writer.py:48] [27] global_step=27, grad_norm=2.800947, loss=0.769342
I0518 06:58:32.942815 140354255357760 submission.py:296] 27) loss = 0.769, grad_norm = 2.801
I0518 06:58:33.189632 140288588769024 logging_writer.py:48] [28] global_step=28, grad_norm=2.557003, loss=0.727546
I0518 06:58:33.192901 140354255357760 submission.py:296] 28) loss = 0.728, grad_norm = 2.557
I0518 06:58:33.524575 140288580376320 logging_writer.py:48] [29] global_step=29, grad_norm=2.186260, loss=0.804209
I0518 06:58:33.527888 140354255357760 submission.py:296] 29) loss = 0.804, grad_norm = 2.186
I0518 06:58:33.802655 140288588769024 logging_writer.py:48] [30] global_step=30, grad_norm=2.368278, loss=0.674201
I0518 06:58:33.805960 140354255357760 submission.py:296] 30) loss = 0.674, grad_norm = 2.368
I0518 06:58:34.021777 140288580376320 logging_writer.py:48] [31] global_step=31, grad_norm=1.862970, loss=0.694803
I0518 06:58:34.025066 140354255357760 submission.py:296] 31) loss = 0.695, grad_norm = 1.863
I0518 06:58:34.353245 140288588769024 logging_writer.py:48] [32] global_step=32, grad_norm=2.022308, loss=0.677260
I0518 06:58:34.356558 140354255357760 submission.py:296] 32) loss = 0.677, grad_norm = 2.022
I0518 06:58:34.677115 140288580376320 logging_writer.py:48] [33] global_step=33, grad_norm=1.901227, loss=0.671429
I0518 06:58:34.680409 140354255357760 submission.py:296] 33) loss = 0.671, grad_norm = 1.901
I0518 06:58:34.985850 140288588769024 logging_writer.py:48] [34] global_step=34, grad_norm=1.931908, loss=0.626677
I0518 06:58:34.989418 140354255357760 submission.py:296] 34) loss = 0.627, grad_norm = 1.932
I0518 06:58:35.247795 140288580376320 logging_writer.py:48] [35] global_step=35, grad_norm=1.722811, loss=0.642276
I0518 06:58:35.253466 140354255357760 submission.py:296] 35) loss = 0.642, grad_norm = 1.723
I0518 06:58:35.464807 140288588769024 logging_writer.py:48] [36] global_step=36, grad_norm=1.553199, loss=0.647437
I0518 06:58:35.469380 140354255357760 submission.py:296] 36) loss = 0.647, grad_norm = 1.553
I0518 06:58:35.731708 140288580376320 logging_writer.py:48] [37] global_step=37, grad_norm=1.469513, loss=0.628051
I0518 06:58:35.738295 140354255357760 submission.py:296] 37) loss = 0.628, grad_norm = 1.470
I0518 06:58:36.004890 140288588769024 logging_writer.py:48] [38] global_step=38, grad_norm=1.443349, loss=0.623409
I0518 06:58:36.011415 140354255357760 submission.py:296] 38) loss = 0.623, grad_norm = 1.443
I0518 06:58:36.291324 140288580376320 logging_writer.py:48] [39] global_step=39, grad_norm=1.371840, loss=0.606119
I0518 06:58:36.296715 140354255357760 submission.py:296] 39) loss = 0.606, grad_norm = 1.372
I0518 06:58:36.552385 140288588769024 logging_writer.py:48] [40] global_step=40, grad_norm=1.310800, loss=0.582379
I0518 06:58:36.558055 140354255357760 submission.py:296] 40) loss = 0.582, grad_norm = 1.311
I0518 06:58:36.798908 140288580376320 logging_writer.py:48] [41] global_step=41, grad_norm=1.255658, loss=0.604411
I0518 06:58:36.806080 140354255357760 submission.py:296] 41) loss = 0.604, grad_norm = 1.256
I0518 06:58:37.040143 140288588769024 logging_writer.py:48] [42] global_step=42, grad_norm=1.315251, loss=0.580452
I0518 06:58:37.044084 140354255357760 submission.py:296] 42) loss = 0.580, grad_norm = 1.315
I0518 06:58:37.354007 140288580376320 logging_writer.py:48] [43] global_step=43, grad_norm=1.161737, loss=0.575700
I0518 06:58:37.360075 140354255357760 submission.py:296] 43) loss = 0.576, grad_norm = 1.162
I0518 06:58:37.602414 140288588769024 logging_writer.py:48] [44] global_step=44, grad_norm=1.168584, loss=0.565361
I0518 06:58:37.606013 140354255357760 submission.py:296] 44) loss = 0.565, grad_norm = 1.169
I0518 06:58:37.847425 140288580376320 logging_writer.py:48] [45] global_step=45, grad_norm=1.046206, loss=0.617810
I0518 06:58:37.850813 140354255357760 submission.py:296] 45) loss = 0.618, grad_norm = 1.046
I0518 06:58:38.117425 140288588769024 logging_writer.py:48] [46] global_step=46, grad_norm=0.986931, loss=0.580423
I0518 06:58:38.120575 140354255357760 submission.py:296] 46) loss = 0.580, grad_norm = 0.987
I0518 06:58:38.398956 140288580376320 logging_writer.py:48] [47] global_step=47, grad_norm=0.993428, loss=0.594335
I0518 06:58:38.402271 140354255357760 submission.py:296] 47) loss = 0.594, grad_norm = 0.993
I0518 06:58:38.654905 140288588769024 logging_writer.py:48] [48] global_step=48, grad_norm=0.971309, loss=0.614501
I0518 06:58:38.658307 140354255357760 submission.py:296] 48) loss = 0.615, grad_norm = 0.971
I0518 06:58:38.982860 140288580376320 logging_writer.py:48] [49] global_step=49, grad_norm=1.187581, loss=0.537365
I0518 06:58:38.986456 140354255357760 submission.py:296] 49) loss = 0.537, grad_norm = 1.188
I0518 06:58:39.162853 140288588769024 logging_writer.py:48] [50] global_step=50, grad_norm=1.053867, loss=0.528880
I0518 06:58:39.168468 140354255357760 submission.py:296] 50) loss = 0.529, grad_norm = 1.054
I0518 06:58:39.513051 140288580376320 logging_writer.py:48] [51] global_step=51, grad_norm=1.007246, loss=0.572893
I0518 06:58:39.519989 140354255357760 submission.py:296] 51) loss = 0.573, grad_norm = 1.007
I0518 06:58:39.736246 140288588769024 logging_writer.py:48] [52] global_step=52, grad_norm=1.181414, loss=0.515301
I0518 06:58:39.742265 140354255357760 submission.py:296] 52) loss = 0.515, grad_norm = 1.181
I0518 06:58:40.010747 140288580376320 logging_writer.py:48] [53] global_step=53, grad_norm=1.019847, loss=0.593431
I0518 06:58:40.016688 140354255357760 submission.py:296] 53) loss = 0.593, grad_norm = 1.020
I0518 06:58:40.254292 140288588769024 logging_writer.py:48] [54] global_step=54, grad_norm=1.076749, loss=0.549485
I0518 06:58:40.259623 140354255357760 submission.py:296] 54) loss = 0.549, grad_norm = 1.077
I0518 06:58:40.543403 140288580376320 logging_writer.py:48] [55] global_step=55, grad_norm=1.058403, loss=0.538227
I0518 06:58:40.548604 140354255357760 submission.py:296] 55) loss = 0.538, grad_norm = 1.058
I0518 06:58:40.804724 140288588769024 logging_writer.py:48] [56] global_step=56, grad_norm=1.106088, loss=0.519488
I0518 06:58:40.807973 140354255357760 submission.py:296] 56) loss = 0.519, grad_norm = 1.106
I0518 06:58:41.056320 140288580376320 logging_writer.py:48] [57] global_step=57, grad_norm=1.048468, loss=0.542116
I0518 06:58:41.061944 140354255357760 submission.py:296] 57) loss = 0.542, grad_norm = 1.048
I0518 06:58:41.301056 140288588769024 logging_writer.py:48] [58] global_step=58, grad_norm=1.085322, loss=0.515150
I0518 06:58:41.306171 140354255357760 submission.py:296] 58) loss = 0.515, grad_norm = 1.085
I0518 06:58:41.581751 140288580376320 logging_writer.py:48] [59] global_step=59, grad_norm=1.057456, loss=0.520851
I0518 06:58:41.588028 140354255357760 submission.py:296] 59) loss = 0.521, grad_norm = 1.057
I0518 06:58:41.855734 140288588769024 logging_writer.py:48] [60] global_step=60, grad_norm=0.999789, loss=0.548067
I0518 06:58:41.860974 140354255357760 submission.py:296] 60) loss = 0.548, grad_norm = 1.000
I0518 06:58:42.123070 140288580376320 logging_writer.py:48] [61] global_step=61, grad_norm=0.924521, loss=0.564115
I0518 06:58:42.127954 140354255357760 submission.py:296] 61) loss = 0.564, grad_norm = 0.925
I0518 06:58:42.331245 140288588769024 logging_writer.py:48] [62] global_step=62, grad_norm=0.930997, loss=0.581412
I0518 06:58:42.335578 140354255357760 submission.py:296] 62) loss = 0.581, grad_norm = 0.931
I0518 06:58:42.606325 140288580376320 logging_writer.py:48] [63] global_step=63, grad_norm=1.025903, loss=0.531605
I0518 06:58:42.610844 140354255357760 submission.py:296] 63) loss = 0.532, grad_norm = 1.026
I0518 06:58:42.922626 140288588769024 logging_writer.py:48] [64] global_step=64, grad_norm=0.971402, loss=0.548347
I0518 06:58:42.926403 140354255357760 submission.py:296] 64) loss = 0.548, grad_norm = 0.971
I0518 06:58:43.212893 140288580376320 logging_writer.py:48] [65] global_step=65, grad_norm=0.889117, loss=0.509997
I0518 06:58:43.216197 140354255357760 submission.py:296] 65) loss = 0.510, grad_norm = 0.889
I0518 06:58:43.412793 140288588769024 logging_writer.py:48] [66] global_step=66, grad_norm=0.957951, loss=0.510548
I0518 06:58:43.419852 140354255357760 submission.py:296] 66) loss = 0.511, grad_norm = 0.958
I0518 06:58:43.685722 140288580376320 logging_writer.py:48] [67] global_step=67, grad_norm=0.995519, loss=0.453881
I0518 06:58:43.691462 140354255357760 submission.py:296] 67) loss = 0.454, grad_norm = 0.996
I0518 06:58:43.967899 140288588769024 logging_writer.py:48] [68] global_step=68, grad_norm=0.908906, loss=0.513970
I0518 06:58:43.973779 140354255357760 submission.py:296] 68) loss = 0.514, grad_norm = 0.909
I0518 06:58:44.173622 140288580376320 logging_writer.py:48] [69] global_step=69, grad_norm=0.984094, loss=0.467070
I0518 06:58:44.177348 140354255357760 submission.py:296] 69) loss = 0.467, grad_norm = 0.984
I0518 06:58:44.432327 140288588769024 logging_writer.py:48] [70] global_step=70, grad_norm=0.964298, loss=0.466675
I0518 06:58:44.436363 140354255357760 submission.py:296] 70) loss = 0.467, grad_norm = 0.964
I0518 06:58:44.710245 140288580376320 logging_writer.py:48] [71] global_step=71, grad_norm=0.877617, loss=0.466504
I0518 06:58:44.714356 140354255357760 submission.py:296] 71) loss = 0.467, grad_norm = 0.878
I0518 06:58:44.941123 140288588769024 logging_writer.py:48] [72] global_step=72, grad_norm=0.902343, loss=0.417792
I0518 06:58:44.945062 140354255357760 submission.py:296] 72) loss = 0.418, grad_norm = 0.902
I0518 06:58:45.244401 140288580376320 logging_writer.py:48] [73] global_step=73, grad_norm=0.970465, loss=0.443306
I0518 06:58:45.247919 140354255357760 submission.py:296] 73) loss = 0.443, grad_norm = 0.970
I0518 06:58:45.507296 140288588769024 logging_writer.py:48] [74] global_step=74, grad_norm=0.880680, loss=0.464892
I0518 06:58:45.512706 140354255357760 submission.py:296] 74) loss = 0.465, grad_norm = 0.881
I0518 06:58:45.722476 140288580376320 logging_writer.py:48] [75] global_step=75, grad_norm=0.864611, loss=0.450566
I0518 06:58:45.727264 140354255357760 submission.py:296] 75) loss = 0.451, grad_norm = 0.865
I0518 06:58:46.008710 140288588769024 logging_writer.py:48] [76] global_step=76, grad_norm=0.897893, loss=0.410545
I0518 06:58:46.014422 140354255357760 submission.py:296] 76) loss = 0.411, grad_norm = 0.898
I0518 06:58:46.253517 140288580376320 logging_writer.py:48] [77] global_step=77, grad_norm=1.026727, loss=0.409247
I0518 06:58:46.258789 140354255357760 submission.py:296] 77) loss = 0.409, grad_norm = 1.027
I0518 06:58:46.513717 140288588769024 logging_writer.py:48] [78] global_step=78, grad_norm=0.849279, loss=0.484502
I0518 06:58:46.520120 140354255357760 submission.py:296] 78) loss = 0.485, grad_norm = 0.849
I0518 06:58:46.776677 140288580376320 logging_writer.py:48] [79] global_step=79, grad_norm=0.885593, loss=0.496740
I0518 06:58:46.782616 140354255357760 submission.py:296] 79) loss = 0.497, grad_norm = 0.886
I0518 06:58:47.038454 140288588769024 logging_writer.py:48] [80] global_step=80, grad_norm=0.975082, loss=0.354851
I0518 06:58:47.044186 140354255357760 submission.py:296] 80) loss = 0.355, grad_norm = 0.975
I0518 06:58:47.339738 140288580376320 logging_writer.py:48] [81] global_step=81, grad_norm=0.946810, loss=0.364318
I0518 06:58:47.342983 140354255357760 submission.py:296] 81) loss = 0.364, grad_norm = 0.947
I0518 06:58:47.562876 140288588769024 logging_writer.py:48] [82] global_step=82, grad_norm=0.791704, loss=0.440461
I0518 06:58:47.566082 140354255357760 submission.py:296] 82) loss = 0.440, grad_norm = 0.792
I0518 06:58:47.808648 140288580376320 logging_writer.py:48] [83] global_step=83, grad_norm=0.925194, loss=0.361595
I0518 06:58:47.813639 140354255357760 submission.py:296] 83) loss = 0.362, grad_norm = 0.925
I0518 06:58:48.054536 140288588769024 logging_writer.py:48] [84] global_step=84, grad_norm=0.804194, loss=0.378429
I0518 06:58:48.060441 140354255357760 submission.py:296] 84) loss = 0.378, grad_norm = 0.804
I0518 06:58:48.336720 140288580376320 logging_writer.py:48] [85] global_step=85, grad_norm=0.717306, loss=0.422955
I0518 06:58:48.341782 140354255357760 submission.py:296] 85) loss = 0.423, grad_norm = 0.717
I0518 06:58:48.608386 140288588769024 logging_writer.py:48] [86] global_step=86, grad_norm=0.751846, loss=0.420292
I0518 06:58:48.612262 140354255357760 submission.py:296] 86) loss = 0.420, grad_norm = 0.752
I0518 06:58:48.879292 140288580376320 logging_writer.py:48] [87] global_step=87, grad_norm=0.673860, loss=0.429447
I0518 06:58:48.882709 140354255357760 submission.py:296] 87) loss = 0.429, grad_norm = 0.674
I0518 06:58:49.163434 140288588769024 logging_writer.py:48] [88] global_step=88, grad_norm=0.775955, loss=0.379403
I0518 06:58:49.167135 140354255357760 submission.py:296] 88) loss = 0.379, grad_norm = 0.776
I0518 06:58:49.397959 140288580376320 logging_writer.py:48] [89] global_step=89, grad_norm=0.872649, loss=0.339952
I0518 06:58:49.401450 140354255357760 submission.py:296] 89) loss = 0.340, grad_norm = 0.873
I0518 06:58:49.704733 140288588769024 logging_writer.py:48] [90] global_step=90, grad_norm=0.693315, loss=0.377656
I0518 06:58:49.708156 140354255357760 submission.py:296] 90) loss = 0.378, grad_norm = 0.693
I0518 06:58:49.988553 140288580376320 logging_writer.py:48] [91] global_step=91, grad_norm=0.782325, loss=0.364807
I0518 06:58:49.993513 140354255357760 submission.py:296] 91) loss = 0.365, grad_norm = 0.782
I0518 06:58:50.256414 140288588769024 logging_writer.py:48] [92] global_step=92, grad_norm=0.679907, loss=0.401494
I0518 06:58:50.262603 140354255357760 submission.py:296] 92) loss = 0.401, grad_norm = 0.680
I0518 06:58:50.512642 140288580376320 logging_writer.py:48] [93] global_step=93, grad_norm=0.731500, loss=0.328665
I0518 06:58:50.518772 140354255357760 submission.py:296] 93) loss = 0.329, grad_norm = 0.732
I0518 06:58:50.755430 140288588769024 logging_writer.py:48] [94] global_step=94, grad_norm=0.545000, loss=0.420126
I0518 06:58:50.759631 140354255357760 submission.py:296] 94) loss = 0.420, grad_norm = 0.545
I0518 06:58:51.050858 140288580376320 logging_writer.py:48] [95] global_step=95, grad_norm=0.644728, loss=0.331851
I0518 06:58:51.054661 140354255357760 submission.py:296] 95) loss = 0.332, grad_norm = 0.645
I0518 06:58:51.338811 140288588769024 logging_writer.py:48] [96] global_step=96, grad_norm=0.594765, loss=0.419860
I0518 06:58:51.342205 140354255357760 submission.py:296] 96) loss = 0.420, grad_norm = 0.595
I0518 06:58:51.644956 140288580376320 logging_writer.py:48] [97] global_step=97, grad_norm=0.569525, loss=0.338616
I0518 06:58:51.648867 140354255357760 submission.py:296] 97) loss = 0.339, grad_norm = 0.570
I0518 06:58:51.916873 140288588769024 logging_writer.py:48] [98] global_step=98, grad_norm=0.527533, loss=0.378664
I0518 06:58:51.920740 140354255357760 submission.py:296] 98) loss = 0.379, grad_norm = 0.528
I0518 06:58:52.162615 140288580376320 logging_writer.py:48] [99] global_step=99, grad_norm=0.644627, loss=0.330498
I0518 06:58:52.168917 140354255357760 submission.py:296] 99) loss = 0.330, grad_norm = 0.645
I0518 06:58:52.415661 140288588769024 logging_writer.py:48] [100] global_step=100, grad_norm=0.513280, loss=0.329636
I0518 06:58:52.421343 140354255357760 submission.py:296] 100) loss = 0.330, grad_norm = 0.513
I0518 06:59:48.465737 140354255357760 spec.py:298] Evaluating on the training split.
I0518 06:59:50.665784 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 06:59:52.912238 140354255357760 spec.py:326] Evaluating on the test split.
I0518 06:59:55.084217 140354255357760 submission_runner.py:421] Time since start: 356.31s, 	Step: 312, 	{'train/ssim': 0.6984287670680455, 'train/loss': 0.3045938696180071, 'validation/ssim': 0.6775531586328785, 'validation/loss': 0.32576688595816333, 'validation/num_examples': 3554, 'test/ssim': 0.695585547529496, 'test/loss': 0.3273083799479894, 'test/num_examples': 3581, 'score': 123.38214993476868, 'total_duration': 356.307909488678, 'accumulated_submission_time': 123.38214993476868, 'accumulated_eval_time': 229.89392042160034, 'accumulated_logging_time': 0.02452850341796875}
I0518 06:59:55.095887 140288580376320 logging_writer.py:48] [312] accumulated_eval_time=229.893920, accumulated_logging_time=0.024529, accumulated_submission_time=123.382150, global_step=312, preemption_count=0, score=123.382150, test/loss=0.327308, test/num_examples=3581, test/ssim=0.695586, total_duration=356.307909, train/loss=0.304594, train/ssim=0.698429, validation/loss=0.325767, validation/num_examples=3554, validation/ssim=0.677553
I0518 07:00:57.209027 140288588769024 logging_writer.py:48] [500] global_step=500, grad_norm=0.395464, loss=0.248554
I0518 07:00:57.214945 140354255357760 submission.py:296] 500) loss = 0.249, grad_norm = 0.395
I0518 07:01:15.135937 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:01:17.267036 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:01:19.474080 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:01:21.573562 140354255357760 submission_runner.py:421] Time since start: 442.80s, 	Step: 549, 	{'train/ssim': 0.7160925183977399, 'train/loss': 0.28954625129699707, 'validation/ssim': 0.6950789383441193, 'validation/loss': 0.3103443289581282, 'validation/num_examples': 3554, 'test/ssim': 0.7127689978358001, 'test/loss': 0.31212485791983735, 'test/num_examples': 3581, 'score': 199.73591589927673, 'total_duration': 442.7972002029419, 'accumulated_submission_time': 199.73591589927673, 'accumulated_eval_time': 236.33151841163635, 'accumulated_logging_time': 0.05028986930847168}
I0518 07:01:21.585415 140288580376320 logging_writer.py:48] [549] accumulated_eval_time=236.331518, accumulated_logging_time=0.050290, accumulated_submission_time=199.735916, global_step=549, preemption_count=0, score=199.735916, test/loss=0.312125, test/num_examples=3581, test/ssim=0.712769, total_duration=442.797200, train/loss=0.289546, train/ssim=0.716093, validation/loss=0.310344, validation/num_examples=3554, validation/ssim=0.695079
I0518 07:02:41.878016 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:02:44.000008 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:02:46.181101 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:02:48.262333 140354255357760 submission_runner.py:421] Time since start: 529.49s, 	Step: 781, 	{'train/ssim': 0.7256426130022321, 'train/loss': 0.28176913942609516, 'validation/ssim': 0.7044693534134074, 'validation/loss': 0.3023123840435073, 'validation/num_examples': 3554, 'test/ssim': 0.7218885807909802, 'test/loss': 0.30414665454394724, 'test/num_examples': 3581, 'score': 276.37985038757324, 'total_duration': 529.4859795570374, 'accumulated_submission_time': 276.37985038757324, 'accumulated_eval_time': 242.71579313278198, 'accumulated_logging_time': 0.08527612686157227}
I0518 07:02:48.275895 140288588769024 logging_writer.py:48] [781] accumulated_eval_time=242.715793, accumulated_logging_time=0.085276, accumulated_submission_time=276.379850, global_step=781, preemption_count=0, score=276.379850, test/loss=0.304147, test/num_examples=3581, test/ssim=0.721889, total_duration=529.485980, train/loss=0.281769, train/ssim=0.725643, validation/loss=0.302312, validation/num_examples=3554, validation/ssim=0.704469
I0518 07:04:01.821511 140288580376320 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.161056, loss=0.217791
I0518 07:04:01.825007 140354255357760 submission.py:296] 1000) loss = 0.218, grad_norm = 0.161
I0518 07:04:08.425930 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:04:10.471002 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:04:12.640802 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:04:14.799642 140354255357760 submission_runner.py:421] Time since start: 616.02s, 	Step: 1026, 	{'train/ssim': 0.7299469539097377, 'train/loss': 0.27783966064453125, 'validation/ssim': 0.7085153282393079, 'validation/loss': 0.29852538787721583, 'validation/num_examples': 3554, 'test/ssim': 0.7258561897863726, 'test/loss': 0.30026092571907287, 'test/num_examples': 3581, 'score': 352.6790351867676, 'total_duration': 616.023339509964, 'accumulated_submission_time': 352.6790351867676, 'accumulated_eval_time': 249.08948922157288, 'accumulated_logging_time': 0.11387157440185547}
I0518 07:04:14.809379 140288588769024 logging_writer.py:48] [1026] accumulated_eval_time=249.089489, accumulated_logging_time=0.113872, accumulated_submission_time=352.679035, global_step=1026, preemption_count=0, score=352.679035, test/loss=0.300261, test/num_examples=3581, test/ssim=0.725856, total_duration=616.023340, train/loss=0.277840, train/ssim=0.729947, validation/loss=0.298525, validation/num_examples=3554, validation/ssim=0.708515
I0518 07:05:35.065075 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:05:37.133901 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:05:39.273606 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:05:41.363962 140354255357760 submission_runner.py:421] Time since start: 702.59s, 	Step: 1340, 	{'train/ssim': 0.7228820664542062, 'train/loss': 0.2828022241592407, 'validation/ssim': 0.7031820852164814, 'validation/loss': 0.30253406153277995, 'validation/num_examples': 3554, 'test/ssim': 0.7203130863192893, 'test/loss': 0.30441111181408825, 'test/num_examples': 3581, 'score': 426.5305826663971, 'total_duration': 702.5876288414001, 'accumulated_submission_time': 426.5305826663971, 'accumulated_eval_time': 255.3883798122406, 'accumulated_logging_time': 0.13139033317565918}
I0518 07:05:41.374991 140288580376320 logging_writer.py:48] [1340] accumulated_eval_time=255.388380, accumulated_logging_time=0.131390, accumulated_submission_time=426.530583, global_step=1340, preemption_count=0, score=426.530583, test/loss=0.304411, test/num_examples=3581, test/ssim=0.720313, total_duration=702.587629, train/loss=0.282802, train/ssim=0.722882, validation/loss=0.302534, validation/num_examples=3554, validation/ssim=0.703182
I0518 07:06:21.467229 140288588769024 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.131278, loss=0.240682
I0518 07:06:21.471245 140354255357760 submission.py:296] 1500) loss = 0.241, grad_norm = 0.131
I0518 07:07:01.411728 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:07:03.484146 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:07:05.618555 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:07:07.701479 140354255357760 submission_runner.py:421] Time since start: 788.93s, 	Step: 1653, 	{'train/ssim': 0.7306544440133231, 'train/loss': 0.27712152685437885, 'validation/ssim': 0.7099125078037071, 'validation/loss': 0.29755899217870707, 'validation/num_examples': 3554, 'test/ssim': 0.7269785821654217, 'test/loss': 0.2993100999088069, 'test/num_examples': 3581, 'score': 500.1528968811035, 'total_duration': 788.9251790046692, 'accumulated_submission_time': 500.1528968811035, 'accumulated_eval_time': 261.67811846733093, 'accumulated_logging_time': 0.15016841888427734}
I0518 07:07:07.712059 140288580376320 logging_writer.py:48] [1653] accumulated_eval_time=261.678118, accumulated_logging_time=0.150168, accumulated_submission_time=500.152897, global_step=1653, preemption_count=0, score=500.152897, test/loss=0.299310, test/num_examples=3581, test/ssim=0.726979, total_duration=788.925179, train/loss=0.277122, train/ssim=0.730654, validation/loss=0.297559, validation/num_examples=3554, validation/ssim=0.709913
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0518 07:08:28.009305 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:08:30.075694 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:08:32.198692 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:08:34.279332 140354255357760 submission_runner.py:421] Time since start: 875.50s, 	Step: 1967, 	{'train/ssim': 0.7328243255615234, 'train/loss': 0.27542240279061453, 'validation/ssim': 0.711359216178074, 'validation/loss': 0.29613409434132315, 'validation/num_examples': 3554, 'test/ssim': 0.7283117767819743, 'test/loss': 0.2979045016776913, 'test/num_examples': 3581, 'score': 574.0339956283569, 'total_duration': 875.5030364990234, 'accumulated_submission_time': 574.0339956283569, 'accumulated_eval_time': 267.9482283592224, 'accumulated_logging_time': 0.16950154304504395}
I0518 07:08:34.289328 140288588769024 logging_writer.py:48] [1967] accumulated_eval_time=267.948228, accumulated_logging_time=0.169502, accumulated_submission_time=574.033996, global_step=1967, preemption_count=0, score=574.033996, test/loss=0.297905, test/num_examples=3581, test/ssim=0.728312, total_duration=875.503036, train/loss=0.275422, train/ssim=0.732824, validation/loss=0.296134, validation/num_examples=3554, validation/ssim=0.711359
I0518 07:08:40.694248 140288580376320 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.082081, loss=0.247111
I0518 07:08:40.698105 140354255357760 submission.py:296] 2000) loss = 0.247, grad_norm = 0.082
I0518 07:09:54.550823 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:09:56.582997 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:09:58.690493 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:10:00.764546 140354255357760 submission_runner.py:421] Time since start: 961.99s, 	Step: 2279, 	{'train/ssim': 0.7363900457109723, 'train/loss': 0.2729040895189558, 'validation/ssim': 0.7146682352850662, 'validation/loss': 0.29376615697101854, 'validation/num_examples': 3554, 'test/ssim': 0.731668522889905, 'test/loss': 0.29556457638840405, 'test/num_examples': 3581, 'score': 647.9737515449524, 'total_duration': 961.9882388114929, 'accumulated_submission_time': 647.9737515449524, 'accumulated_eval_time': 274.1620273590088, 'accumulated_logging_time': 0.18760347366333008}
I0518 07:10:00.775079 140288588769024 logging_writer.py:48] [2279] accumulated_eval_time=274.162027, accumulated_logging_time=0.187603, accumulated_submission_time=647.973752, global_step=2279, preemption_count=0, score=647.973752, test/loss=0.295565, test/num_examples=3581, test/ssim=0.731669, total_duration=961.988239, train/loss=0.272904, train/ssim=0.736390, validation/loss=0.293766, validation/num_examples=3554, validation/ssim=0.714668
I0518 07:10:56.823465 140288580376320 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.372499, loss=0.301018
I0518 07:10:56.826849 140354255357760 submission.py:296] 2500) loss = 0.301, grad_norm = 0.372
I0518 07:11:21.070644 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:11:23.162155 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:11:25.280014 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:11:27.360436 140354255357760 submission_runner.py:421] Time since start: 1048.58s, 	Step: 2592, 	{'train/ssim': 0.7371725354875837, 'train/loss': 0.2724770818437849, 'validation/ssim': 0.7154361722706809, 'validation/loss': 0.29336865564109105, 'validation/num_examples': 3554, 'test/ssim': 0.7325335483716141, 'test/loss': 0.2950339915199141, 'test/num_examples': 3581, 'score': 721.9225435256958, 'total_duration': 1048.5841200351715, 'accumulated_submission_time': 721.9225435256958, 'accumulated_eval_time': 280.45179200172424, 'accumulated_logging_time': 0.20676159858703613}
I0518 07:11:27.370642 140288588769024 logging_writer.py:48] [2592] accumulated_eval_time=280.451792, accumulated_logging_time=0.206762, accumulated_submission_time=721.922544, global_step=2592, preemption_count=0, score=721.922544, test/loss=0.295034, test/num_examples=3581, test/ssim=0.732534, total_duration=1048.584120, train/loss=0.272477, train/ssim=0.737173, validation/loss=0.293369, validation/num_examples=3554, validation/ssim=0.715436
I0518 07:12:47.573705 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:12:49.653982 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:12:51.790390 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:12:53.878940 140354255357760 submission_runner.py:421] Time since start: 1135.10s, 	Step: 2906, 	{'train/ssim': 0.7377610206604004, 'train/loss': 0.27240024294172016, 'validation/ssim': 0.7168406334631753, 'validation/loss': 0.2930736810086346, 'validation/num_examples': 3554, 'test/ssim': 0.7338023160430047, 'test/loss': 0.29470316426975707, 'test/num_examples': 3581, 'score': 795.6775770187378, 'total_duration': 1135.1026275157928, 'accumulated_submission_time': 795.6775770187378, 'accumulated_eval_time': 286.7570538520813, 'accumulated_logging_time': 0.22512245178222656}
I0518 07:12:53.889264 140288580376320 logging_writer.py:48] [2906] accumulated_eval_time=286.757054, accumulated_logging_time=0.225122, accumulated_submission_time=795.677577, global_step=2906, preemption_count=0, score=795.677577, test/loss=0.294703, test/num_examples=3581, test/ssim=0.733802, total_duration=1135.102628, train/loss=0.272400, train/ssim=0.737761, validation/loss=0.293074, validation/num_examples=3554, validation/ssim=0.716841
I0518 07:13:16.405824 140288588769024 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.128045, loss=0.255094
I0518 07:13:16.409448 140354255357760 submission.py:296] 3000) loss = 0.255, grad_norm = 0.128
I0518 07:14:14.078663 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:14:16.135918 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:14:18.267911 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:14:20.359013 140354255357760 submission_runner.py:421] Time since start: 1221.58s, 	Step: 3215, 	{'train/ssim': 0.7363804408482143, 'train/loss': 0.2715491907937186, 'validation/ssim': 0.7155424428241066, 'validation/loss': 0.29208372306072383, 'validation/num_examples': 3554, 'test/ssim': 0.7325128226665037, 'test/loss': 0.2937909605382575, 'test/num_examples': 3581, 'score': 869.5577476024628, 'total_duration': 1221.5826992988586, 'accumulated_submission_time': 869.5577476024628, 'accumulated_eval_time': 293.0373899936676, 'accumulated_logging_time': 0.2441103458404541}
I0518 07:14:20.369882 140288580376320 logging_writer.py:48] [3215] accumulated_eval_time=293.037390, accumulated_logging_time=0.244110, accumulated_submission_time=869.557748, global_step=3215, preemption_count=0, score=869.557748, test/loss=0.293791, test/num_examples=3581, test/ssim=0.732513, total_duration=1221.582699, train/loss=0.271549, train/ssim=0.736380, validation/loss=0.292084, validation/num_examples=3554, validation/ssim=0.715542
I0518 07:15:33.947575 140288588769024 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.118474, loss=0.309542
I0518 07:15:33.951832 140354255357760 submission.py:296] 3500) loss = 0.310, grad_norm = 0.118
I0518 07:15:40.544837 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:15:42.602145 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:15:44.716475 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:15:46.802073 140354255357760 submission_runner.py:421] Time since start: 1308.03s, 	Step: 3526, 	{'train/ssim': 0.738837718963623, 'train/loss': 0.27030175072806223, 'validation/ssim': 0.717191456809229, 'validation/loss': 0.2911291428342185, 'validation/num_examples': 3554, 'test/ssim': 0.7343238675038397, 'test/loss': 0.29271151945598295, 'test/num_examples': 3581, 'score': 943.4523708820343, 'total_duration': 1308.0257515907288, 'accumulated_submission_time': 943.4523708820343, 'accumulated_eval_time': 299.2945947647095, 'accumulated_logging_time': 0.2629363536834717}
I0518 07:15:46.811954 140288580376320 logging_writer.py:48] [3526] accumulated_eval_time=299.294595, accumulated_logging_time=0.262936, accumulated_submission_time=943.452371, global_step=3526, preemption_count=0, score=943.452371, test/loss=0.292712, test/num_examples=3581, test/ssim=0.734324, total_duration=1308.025752, train/loss=0.270302, train/ssim=0.738838, validation/loss=0.291129, validation/num_examples=3554, validation/ssim=0.717191
I0518 07:17:06.928356 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:17:08.983540 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:17:11.097723 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:17:13.175905 140354255357760 submission_runner.py:421] Time since start: 1394.40s, 	Step: 3838, 	{'train/ssim': 0.7352480888366699, 'train/loss': 0.27318859100341797, 'validation/ssim': 0.7143940064232203, 'validation/loss': 0.293695745001231, 'validation/num_examples': 3554, 'test/ssim': 0.7315268517872103, 'test/loss': 0.2953292987294052, 'test/num_examples': 3581, 'score': 1017.170697927475, 'total_duration': 1394.399603843689, 'accumulated_submission_time': 1017.170697927475, 'accumulated_eval_time': 305.54215383529663, 'accumulated_logging_time': 0.28020405769348145}
I0518 07:17:13.186286 140288588769024 logging_writer.py:48] [3838] accumulated_eval_time=305.542154, accumulated_logging_time=0.280204, accumulated_submission_time=1017.170698, global_step=3838, preemption_count=0, score=1017.170698, test/loss=0.295329, test/num_examples=3581, test/ssim=0.731527, total_duration=1394.399604, train/loss=0.273189, train/ssim=0.735248, validation/loss=0.293696, validation/num_examples=3554, validation/ssim=0.714394
I0518 07:17:53.743771 140288580376320 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.141604, loss=0.228796
I0518 07:17:53.747124 140354255357760 submission.py:296] 4000) loss = 0.229, grad_norm = 0.142
I0518 07:18:33.390509 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:18:35.460602 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:18:37.593830 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:18:39.673270 140354255357760 submission_runner.py:421] Time since start: 1480.90s, 	Step: 4152, 	{'train/ssim': 0.7373875209263393, 'train/loss': 0.2717865875789097, 'validation/ssim': 0.7153894599395048, 'validation/loss': 0.2930638233328644, 'validation/num_examples': 3554, 'test/ssim': 0.732325405023911, 'test/loss': 0.29480375893386973, 'test/num_examples': 3581, 'score': 1091.0104115009308, 'total_duration': 1480.8969621658325, 'accumulated_submission_time': 1091.0104115009308, 'accumulated_eval_time': 311.82493138313293, 'accumulated_logging_time': 0.29851222038269043}
I0518 07:18:39.683423 140288588769024 logging_writer.py:48] [4152] accumulated_eval_time=311.824931, accumulated_logging_time=0.298512, accumulated_submission_time=1091.010412, global_step=4152, preemption_count=0, score=1091.010412, test/loss=0.294804, test/num_examples=3581, test/ssim=0.732325, total_duration=1480.896962, train/loss=0.271787, train/ssim=0.737388, validation/loss=0.293064, validation/num_examples=3554, validation/ssim=0.715389
I0518 07:19:59.819451 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:20:01.882906 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:20:04.013185 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:20:06.091769 140354255357760 submission_runner.py:421] Time since start: 1567.32s, 	Step: 4463, 	{'train/ssim': 0.7396188463483538, 'train/loss': 0.2698056016649519, 'validation/ssim': 0.7174928200399198, 'validation/loss': 0.2909324358337437, 'validation/num_examples': 3554, 'test/ssim': 0.7346704776511449, 'test/loss': 0.29260778866543913, 'test/num_examples': 3581, 'score': 1164.8067255020142, 'total_duration': 1567.3154621124268, 'accumulated_submission_time': 1164.8067255020142, 'accumulated_eval_time': 318.0972514152527, 'accumulated_logging_time': 0.31746888160705566}
I0518 07:20:06.101807 140288580376320 logging_writer.py:48] [4463] accumulated_eval_time=318.097251, accumulated_logging_time=0.317469, accumulated_submission_time=1164.806726, global_step=4463, preemption_count=0, score=1164.806726, test/loss=0.292608, test/num_examples=3581, test/ssim=0.734670, total_duration=1567.315462, train/loss=0.269806, train/ssim=0.739619, validation/loss=0.290932, validation/num_examples=3554, validation/ssim=0.717493
I0518 07:20:13.454102 140288588769024 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.191863, loss=0.246471
I0518 07:20:13.457375 140354255357760 submission.py:296] 4500) loss = 0.246, grad_norm = 0.192
I0518 07:21:26.131418 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:21:28.186274 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:21:30.317792 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:21:32.403510 140354255357760 submission_runner.py:421] Time since start: 1653.63s, 	Step: 4777, 	{'train/ssim': 0.7407131195068359, 'train/loss': 0.2696591785975865, 'validation/ssim': 0.71891692788935, 'validation/loss': 0.2907350075399198, 'validation/num_examples': 3554, 'test/ssim': 0.7360867796181234, 'test/loss': 0.2922359190650307, 'test/num_examples': 3581, 'score': 1238.4797484874725, 'total_duration': 1653.6271953582764, 'accumulated_submission_time': 1238.4797484874725, 'accumulated_eval_time': 324.3693993091583, 'accumulated_logging_time': 0.3361482620239258}
I0518 07:21:32.414182 140288580376320 logging_writer.py:48] [4777] accumulated_eval_time=324.369399, accumulated_logging_time=0.336148, accumulated_submission_time=1238.479748, global_step=4777, preemption_count=0, score=1238.479748, test/loss=0.292236, test/num_examples=3581, test/ssim=0.736087, total_duration=1653.627195, train/loss=0.269659, train/ssim=0.740713, validation/loss=0.290735, validation/num_examples=3554, validation/ssim=0.718917
I0518 07:22:29.479971 140288588769024 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.164714, loss=0.264322
I0518 07:22:29.484239 140354255357760 submission.py:296] 5000) loss = 0.264, grad_norm = 0.165
I0518 07:22:52.618969 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:22:54.680438 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:22:56.810942 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:22:58.889221 140354255357760 submission_runner.py:421] Time since start: 1740.11s, 	Step: 5089, 	{'train/ssim': 0.7409616197858538, 'train/loss': 0.2700993163245065, 'validation/ssim': 0.719583196860052, 'validation/loss': 0.2909596732444956, 'validation/num_examples': 3554, 'test/ssim': 0.7366542139721796, 'test/loss': 0.292513432165858, 'test/num_examples': 3581, 'score': 1312.3807258605957, 'total_duration': 1740.1128952503204, 'accumulated_submission_time': 1312.3807258605957, 'accumulated_eval_time': 330.6396472454071, 'accumulated_logging_time': 0.35548949241638184}
I0518 07:22:58.899294 140288580376320 logging_writer.py:48] [5089] accumulated_eval_time=330.639647, accumulated_logging_time=0.355489, accumulated_submission_time=1312.380726, global_step=5089, preemption_count=0, score=1312.380726, test/loss=0.292513, test/num_examples=3581, test/ssim=0.736654, total_duration=1740.112895, train/loss=0.270099, train/ssim=0.740962, validation/loss=0.290960, validation/num_examples=3554, validation/ssim=0.719583
I0518 07:24:18.955317 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:24:21.017385 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:24:23.151098 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:24:25.241174 140354255357760 submission_runner.py:421] Time since start: 1826.46s, 	Step: 5400, 	{'train/ssim': 0.7399881907871791, 'train/loss': 0.26944717339106966, 'validation/ssim': 0.7179953210730866, 'validation/loss': 0.2906179862808631, 'validation/num_examples': 3554, 'test/ssim': 0.7351683036337615, 'test/loss': 0.292177628019408, 'test/num_examples': 3581, 'score': 1386.1993930339813, 'total_duration': 1826.46484375, 'accumulated_submission_time': 1386.1993930339813, 'accumulated_eval_time': 336.9255814552307, 'accumulated_logging_time': 0.37308359146118164}
I0518 07:24:25.251393 140288588769024 logging_writer.py:48] [5400] accumulated_eval_time=336.925581, accumulated_logging_time=0.373084, accumulated_submission_time=1386.199393, global_step=5400, preemption_count=0, score=1386.199393, test/loss=0.292178, test/num_examples=3581, test/ssim=0.735168, total_duration=1826.464844, train/loss=0.269447, train/ssim=0.739988, validation/loss=0.290618, validation/num_examples=3554, validation/ssim=0.717995
I0518 07:24:30.216067 140354255357760 spec.py:298] Evaluating on the training split.
I0518 07:24:32.271369 140354255357760 spec.py:310] Evaluating on the validation split.
I0518 07:24:34.369886 140354255357760 spec.py:326] Evaluating on the test split.
I0518 07:24:36.419028 140354255357760 submission_runner.py:421] Time since start: 1837.64s, 	Step: 5428, 	{'train/ssim': 0.7409659113202777, 'train/loss': 0.2690762792314802, 'validation/ssim': 0.7189479778506612, 'validation/loss': 0.2901924918995322, 'validation/num_examples': 3554, 'test/ssim': 0.7360884158580006, 'test/loss': 0.2917983612512217, 'test/num_examples': 3581, 'score': 1390.6437215805054, 'total_duration': 1837.6427149772644, 'accumulated_submission_time': 1390.6437215805054, 'accumulated_eval_time': 343.1285808086395, 'accumulated_logging_time': 0.39130282402038574}
I0518 07:24:36.429332 140288580376320 logging_writer.py:48] [5428] accumulated_eval_time=343.128581, accumulated_logging_time=0.391303, accumulated_submission_time=1390.643722, global_step=5428, preemption_count=0, score=1390.643722, test/loss=0.291798, test/num_examples=3581, test/ssim=0.736088, total_duration=1837.642715, train/loss=0.269076, train/ssim=0.740966, validation/loss=0.290192, validation/num_examples=3554, validation/ssim=0.718948
I0518 07:24:36.445342 140288588769024 logging_writer.py:48] [5428] global_step=5428, preemption_count=0, score=1390.643722
I0518 07:24:36.582795 140354255357760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_pytorch/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_5428.
I0518 07:24:37.510346 140354255357760 submission_runner.py:584] Tuning trial 1/1
I0518 07:24:37.510575 140354255357760 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0518 07:24:37.517297 140354255357760 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/ssim': 0.19099199771881104, 'train/loss': 1.1586253302437919, 'validation/ssim': 0.18754581930131542, 'validation/loss': 1.1574455608996905, 'validation/num_examples': 3554, 'test/ssim': 0.21058086788344735, 'test/loss': 1.1514768155717676, 'test/num_examples': 3581, 'score': 46.24677062034607, 'total_duration': 269.52392745018005, 'accumulated_submission_time': 46.24677062034607, 'accumulated_eval_time': 223.27545285224915, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (312, {'train/ssim': 0.6984287670680455, 'train/loss': 0.3045938696180071, 'validation/ssim': 0.6775531586328785, 'validation/loss': 0.32576688595816333, 'validation/num_examples': 3554, 'test/ssim': 0.695585547529496, 'test/loss': 0.3273083799479894, 'test/num_examples': 3581, 'score': 123.38214993476868, 'total_duration': 356.307909488678, 'accumulated_submission_time': 123.38214993476868, 'accumulated_eval_time': 229.89392042160034, 'accumulated_logging_time': 0.02452850341796875, 'global_step': 312, 'preemption_count': 0}), (549, {'train/ssim': 0.7160925183977399, 'train/loss': 0.28954625129699707, 'validation/ssim': 0.6950789383441193, 'validation/loss': 0.3103443289581282, 'validation/num_examples': 3554, 'test/ssim': 0.7127689978358001, 'test/loss': 0.31212485791983735, 'test/num_examples': 3581, 'score': 199.73591589927673, 'total_duration': 442.7972002029419, 'accumulated_submission_time': 199.73591589927673, 'accumulated_eval_time': 236.33151841163635, 'accumulated_logging_time': 0.05028986930847168, 'global_step': 549, 'preemption_count': 0}), (781, {'train/ssim': 0.7256426130022321, 'train/loss': 0.28176913942609516, 'validation/ssim': 0.7044693534134074, 'validation/loss': 0.3023123840435073, 'validation/num_examples': 3554, 'test/ssim': 0.7218885807909802, 'test/loss': 0.30414665454394724, 'test/num_examples': 3581, 'score': 276.37985038757324, 'total_duration': 529.4859795570374, 'accumulated_submission_time': 276.37985038757324, 'accumulated_eval_time': 242.71579313278198, 'accumulated_logging_time': 0.08527612686157227, 'global_step': 781, 'preemption_count': 0}), (1026, {'train/ssim': 0.7299469539097377, 'train/loss': 0.27783966064453125, 'validation/ssim': 0.7085153282393079, 'validation/loss': 0.29852538787721583, 'validation/num_examples': 3554, 'test/ssim': 0.7258561897863726, 'test/loss': 0.30026092571907287, 'test/num_examples': 3581, 'score': 352.6790351867676, 'total_duration': 616.023339509964, 'accumulated_submission_time': 352.6790351867676, 'accumulated_eval_time': 249.08948922157288, 'accumulated_logging_time': 0.11387157440185547, 'global_step': 1026, 'preemption_count': 0}), (1340, {'train/ssim': 0.7228820664542062, 'train/loss': 0.2828022241592407, 'validation/ssim': 0.7031820852164814, 'validation/loss': 0.30253406153277995, 'validation/num_examples': 3554, 'test/ssim': 0.7203130863192893, 'test/loss': 0.30441111181408825, 'test/num_examples': 3581, 'score': 426.5305826663971, 'total_duration': 702.5876288414001, 'accumulated_submission_time': 426.5305826663971, 'accumulated_eval_time': 255.3883798122406, 'accumulated_logging_time': 0.13139033317565918, 'global_step': 1340, 'preemption_count': 0}), (1653, {'train/ssim': 0.7306544440133231, 'train/loss': 0.27712152685437885, 'validation/ssim': 0.7099125078037071, 'validation/loss': 0.29755899217870707, 'validation/num_examples': 3554, 'test/ssim': 0.7269785821654217, 'test/loss': 0.2993100999088069, 'test/num_examples': 3581, 'score': 500.1528968811035, 'total_duration': 788.9251790046692, 'accumulated_submission_time': 500.1528968811035, 'accumulated_eval_time': 261.67811846733093, 'accumulated_logging_time': 0.15016841888427734, 'global_step': 1653, 'preemption_count': 0}), (1967, {'train/ssim': 0.7328243255615234, 'train/loss': 0.27542240279061453, 'validation/ssim': 0.711359216178074, 'validation/loss': 0.29613409434132315, 'validation/num_examples': 3554, 'test/ssim': 0.7283117767819743, 'test/loss': 0.2979045016776913, 'test/num_examples': 3581, 'score': 574.0339956283569, 'total_duration': 875.5030364990234, 'accumulated_submission_time': 574.0339956283569, 'accumulated_eval_time': 267.9482283592224, 'accumulated_logging_time': 0.16950154304504395, 'global_step': 1967, 'preemption_count': 0}), (2279, {'train/ssim': 0.7363900457109723, 'train/loss': 0.2729040895189558, 'validation/ssim': 0.7146682352850662, 'validation/loss': 0.29376615697101854, 'validation/num_examples': 3554, 'test/ssim': 0.731668522889905, 'test/loss': 0.29556457638840405, 'test/num_examples': 3581, 'score': 647.9737515449524, 'total_duration': 961.9882388114929, 'accumulated_submission_time': 647.9737515449524, 'accumulated_eval_time': 274.1620273590088, 'accumulated_logging_time': 0.18760347366333008, 'global_step': 2279, 'preemption_count': 0}), (2592, {'train/ssim': 0.7371725354875837, 'train/loss': 0.2724770818437849, 'validation/ssim': 0.7154361722706809, 'validation/loss': 0.29336865564109105, 'validation/num_examples': 3554, 'test/ssim': 0.7325335483716141, 'test/loss': 0.2950339915199141, 'test/num_examples': 3581, 'score': 721.9225435256958, 'total_duration': 1048.5841200351715, 'accumulated_submission_time': 721.9225435256958, 'accumulated_eval_time': 280.45179200172424, 'accumulated_logging_time': 0.20676159858703613, 'global_step': 2592, 'preemption_count': 0}), (2906, {'train/ssim': 0.7377610206604004, 'train/loss': 0.27240024294172016, 'validation/ssim': 0.7168406334631753, 'validation/loss': 0.2930736810086346, 'validation/num_examples': 3554, 'test/ssim': 0.7338023160430047, 'test/loss': 0.29470316426975707, 'test/num_examples': 3581, 'score': 795.6775770187378, 'total_duration': 1135.1026275157928, 'accumulated_submission_time': 795.6775770187378, 'accumulated_eval_time': 286.7570538520813, 'accumulated_logging_time': 0.22512245178222656, 'global_step': 2906, 'preemption_count': 0}), (3215, {'train/ssim': 0.7363804408482143, 'train/loss': 0.2715491907937186, 'validation/ssim': 0.7155424428241066, 'validation/loss': 0.29208372306072383, 'validation/num_examples': 3554, 'test/ssim': 0.7325128226665037, 'test/loss': 0.2937909605382575, 'test/num_examples': 3581, 'score': 869.5577476024628, 'total_duration': 1221.5826992988586, 'accumulated_submission_time': 869.5577476024628, 'accumulated_eval_time': 293.0373899936676, 'accumulated_logging_time': 0.2441103458404541, 'global_step': 3215, 'preemption_count': 0}), (3526, {'train/ssim': 0.738837718963623, 'train/loss': 0.27030175072806223, 'validation/ssim': 0.717191456809229, 'validation/loss': 0.2911291428342185, 'validation/num_examples': 3554, 'test/ssim': 0.7343238675038397, 'test/loss': 0.29271151945598295, 'test/num_examples': 3581, 'score': 943.4523708820343, 'total_duration': 1308.0257515907288, 'accumulated_submission_time': 943.4523708820343, 'accumulated_eval_time': 299.2945947647095, 'accumulated_logging_time': 0.2629363536834717, 'global_step': 3526, 'preemption_count': 0}), (3838, {'train/ssim': 0.7352480888366699, 'train/loss': 0.27318859100341797, 'validation/ssim': 0.7143940064232203, 'validation/loss': 0.293695745001231, 'validation/num_examples': 3554, 'test/ssim': 0.7315268517872103, 'test/loss': 0.2953292987294052, 'test/num_examples': 3581, 'score': 1017.170697927475, 'total_duration': 1394.399603843689, 'accumulated_submission_time': 1017.170697927475, 'accumulated_eval_time': 305.54215383529663, 'accumulated_logging_time': 0.28020405769348145, 'global_step': 3838, 'preemption_count': 0}), (4152, {'train/ssim': 0.7373875209263393, 'train/loss': 0.2717865875789097, 'validation/ssim': 0.7153894599395048, 'validation/loss': 0.2930638233328644, 'validation/num_examples': 3554, 'test/ssim': 0.732325405023911, 'test/loss': 0.29480375893386973, 'test/num_examples': 3581, 'score': 1091.0104115009308, 'total_duration': 1480.8969621658325, 'accumulated_submission_time': 1091.0104115009308, 'accumulated_eval_time': 311.82493138313293, 'accumulated_logging_time': 0.29851222038269043, 'global_step': 4152, 'preemption_count': 0}), (4463, {'train/ssim': 0.7396188463483538, 'train/loss': 0.2698056016649519, 'validation/ssim': 0.7174928200399198, 'validation/loss': 0.2909324358337437, 'validation/num_examples': 3554, 'test/ssim': 0.7346704776511449, 'test/loss': 0.29260778866543913, 'test/num_examples': 3581, 'score': 1164.8067255020142, 'total_duration': 1567.3154621124268, 'accumulated_submission_time': 1164.8067255020142, 'accumulated_eval_time': 318.0972514152527, 'accumulated_logging_time': 0.31746888160705566, 'global_step': 4463, 'preemption_count': 0}), (4777, {'train/ssim': 0.7407131195068359, 'train/loss': 0.2696591785975865, 'validation/ssim': 0.71891692788935, 'validation/loss': 0.2907350075399198, 'validation/num_examples': 3554, 'test/ssim': 0.7360867796181234, 'test/loss': 0.2922359190650307, 'test/num_examples': 3581, 'score': 1238.4797484874725, 'total_duration': 1653.6271953582764, 'accumulated_submission_time': 1238.4797484874725, 'accumulated_eval_time': 324.3693993091583, 'accumulated_logging_time': 0.3361482620239258, 'global_step': 4777, 'preemption_count': 0}), (5089, {'train/ssim': 0.7409616197858538, 'train/loss': 0.2700993163245065, 'validation/ssim': 0.719583196860052, 'validation/loss': 0.2909596732444956, 'validation/num_examples': 3554, 'test/ssim': 0.7366542139721796, 'test/loss': 0.292513432165858, 'test/num_examples': 3581, 'score': 1312.3807258605957, 'total_duration': 1740.1128952503204, 'accumulated_submission_time': 1312.3807258605957, 'accumulated_eval_time': 330.6396472454071, 'accumulated_logging_time': 0.35548949241638184, 'global_step': 5089, 'preemption_count': 0}), (5400, {'train/ssim': 0.7399881907871791, 'train/loss': 0.26944717339106966, 'validation/ssim': 0.7179953210730866, 'validation/loss': 0.2906179862808631, 'validation/num_examples': 3554, 'test/ssim': 0.7351683036337615, 'test/loss': 0.292177628019408, 'test/num_examples': 3581, 'score': 1386.1993930339813, 'total_duration': 1826.46484375, 'accumulated_submission_time': 1386.1993930339813, 'accumulated_eval_time': 336.9255814552307, 'accumulated_logging_time': 0.37308359146118164, 'global_step': 5400, 'preemption_count': 0}), (5428, {'train/ssim': 0.7409659113202777, 'train/loss': 0.2690762792314802, 'validation/ssim': 0.7189479778506612, 'validation/loss': 0.2901924918995322, 'validation/num_examples': 3554, 'test/ssim': 0.7360884158580006, 'test/loss': 0.2917983612512217, 'test/num_examples': 3581, 'score': 1390.6437215805054, 'total_duration': 1837.6427149772644, 'accumulated_submission_time': 1390.6437215805054, 'accumulated_eval_time': 343.1285808086395, 'accumulated_logging_time': 0.39130282402038574, 'global_step': 5428, 'preemption_count': 0})], 'global_step': 5428}
I0518 07:24:37.517480 140354255357760 submission_runner.py:587] Timing: 1390.6437215805054
I0518 07:24:37.517555 140354255357760 submission_runner.py:588] ====================
I0518 07:24:37.517703 140354255357760 submission_runner.py:651] Final fastmri score: 1390.6437215805054
