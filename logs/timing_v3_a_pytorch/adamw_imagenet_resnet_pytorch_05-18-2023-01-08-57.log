torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_pytorch/timing_adamw --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_resnet_pytorch_05-18-2023-01-08-57.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0518 01:09:20.666330 140057885620032 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0518 01:09:20.666364 139646380013376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0518 01:09:20.666388 140257282512704 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0518 01:09:20.666949 140299993294656 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0518 01:09:20.667072 139944377976640 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0518 01:09:20.667265 140367884277568 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0518 01:09:21.658829 140258406643520 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0518 01:09:21.666407 139639360366400 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0518 01:09:21.666740 139639360366400 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 01:09:21.669733 140258406643520 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 01:09:21.675930 140057885620032 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 01:09:21.675956 139646380013376 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 01:09:21.675980 140257282512704 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 01:09:21.675999 140299993294656 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 01:09:21.676027 139944377976640 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 01:09:21.676135 140367884277568 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 01:09:23.877609 139639360366400 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_pytorch/timing_adamw/imagenet_resnet_pytorch.
W0518 01:09:23.926920 139639360366400 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 01:09:23.928509 140367884277568 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 01:09:23.930175 140299993294656 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 01:09:23.931068 140258406643520 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 01:09:23.931322 140057885620032 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 01:09:23.931368 140257282512704 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 01:09:23.931552 139646380013376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 01:09:23.931752 139944377976640 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0518 01:09:23.932814 139639360366400 submission_runner.py:544] Using RNG seed 4141270517
I0518 01:09:23.934213 139639360366400 submission_runner.py:553] --- Tuning run 1/1 ---
I0518 01:09:23.934337 139639360366400 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_pytorch/timing_adamw/imagenet_resnet_pytorch/trial_1.
I0518 01:09:23.934757 139639360366400 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_pytorch/timing_adamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0518 01:09:23.936216 139639360366400 submission_runner.py:241] Initializing dataset.
I0518 01:09:30.521793 139639360366400 submission_runner.py:248] Initializing model.
I0518 01:09:34.750591 139639360366400 submission_runner.py:258] Initializing optimizer.
I0518 01:09:34.751984 139639360366400 submission_runner.py:265] Initializing metrics bundle.
I0518 01:09:34.752095 139639360366400 submission_runner.py:283] Initializing checkpoint and logger.
I0518 01:09:35.216739 139639360366400 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_pytorch/timing_adamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0518 01:09:35.217787 139639360366400 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_pytorch/timing_adamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0518 01:09:35.267568 139639360366400 submission_runner.py:319] Starting training loop.
I0518 01:09:43.195960 139612441802496 logging_writer.py:48] [0] global_step=0, grad_norm=0.591043, loss=6.922792
I0518 01:09:43.216439 139639360366400 submission.py:119] 0) loss = 6.923, grad_norm = 0.591
I0518 01:09:43.217369 139639360366400 spec.py:298] Evaluating on the training split.
I0518 01:10:45.183946 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 01:11:40.251299 139639360366400 spec.py:326] Evaluating on the test split.
I0518 01:11:40.270545 139639360366400 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0518 01:11:40.276569 139639360366400 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0518 01:11:40.353408 139639360366400 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0518 01:11:52.114108 139639360366400 submission_runner.py:421] Time since start: 136.85s, 	Step: 1, 	{'train/accuracy': 0.0011957908163265306, 'train/loss': 6.917802460339605, 'validation/accuracy': 0.00112, 'validation/loss': 6.918008125, 'validation/num_examples': 50000, 'test/accuracy': 0.0014, 'test/loss': 6.92026796875, 'test/num_examples': 10000, 'score': 7.949028968811035, 'total_duration': 136.84689927101135, 'accumulated_submission_time': 7.949028968811035, 'accumulated_eval_time': 128.8966143131256, 'accumulated_logging_time': 0}
I0518 01:11:52.130480 139586697160448 logging_writer.py:48] [1] accumulated_eval_time=128.896614, accumulated_logging_time=0, accumulated_submission_time=7.949029, global_step=1, preemption_count=0, score=7.949029, test/accuracy=0.001400, test/loss=6.920268, test/num_examples=10000, total_duration=136.846899, train/accuracy=0.001196, train/loss=6.917802, validation/accuracy=0.001120, validation/loss=6.918008, validation/num_examples=50000
I0518 01:11:52.150262 139639360366400 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 01:11:52.150357 139944377976640 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 01:11:52.150386 140299993294656 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 01:11:52.150378 140367884277568 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 01:11:52.150370 139646380013376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 01:11:52.150406 140257282512704 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 01:11:52.150408 140057885620032 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 01:11:52.150421 140258406643520 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 01:11:52.525126 139586688767744 logging_writer.py:48] [1] global_step=1, grad_norm=0.604351, loss=6.915729
I0518 01:11:52.528460 139639360366400 submission.py:119] 1) loss = 6.916, grad_norm = 0.604
I0518 01:11:52.908374 139586697160448 logging_writer.py:48] [2] global_step=2, grad_norm=0.610673, loss=6.936293
I0518 01:11:52.912971 139639360366400 submission.py:119] 2) loss = 6.936, grad_norm = 0.611
I0518 01:11:53.295048 139586688767744 logging_writer.py:48] [3] global_step=3, grad_norm=0.603996, loss=6.937574
I0518 01:11:53.298568 139639360366400 submission.py:119] 3) loss = 6.938, grad_norm = 0.604
I0518 01:11:53.682941 139586697160448 logging_writer.py:48] [4] global_step=4, grad_norm=0.602039, loss=6.922173
I0518 01:11:53.686536 139639360366400 submission.py:119] 4) loss = 6.922, grad_norm = 0.602
I0518 01:11:54.071801 139586688767744 logging_writer.py:48] [5] global_step=5, grad_norm=0.592171, loss=6.920939
I0518 01:11:54.075973 139639360366400 submission.py:119] 5) loss = 6.921, grad_norm = 0.592
I0518 01:11:54.459300 139586697160448 logging_writer.py:48] [6] global_step=6, grad_norm=0.610283, loss=6.924027
I0518 01:11:54.465322 139639360366400 submission.py:119] 6) loss = 6.924, grad_norm = 0.610
I0518 01:11:54.849430 139586688767744 logging_writer.py:48] [7] global_step=7, grad_norm=0.606311, loss=6.922856
I0518 01:11:54.853365 139639360366400 submission.py:119] 7) loss = 6.923, grad_norm = 0.606
I0518 01:11:55.238973 139586697160448 logging_writer.py:48] [8] global_step=8, grad_norm=0.606422, loss=6.936651
I0518 01:11:55.243450 139639360366400 submission.py:119] 8) loss = 6.937, grad_norm = 0.606
I0518 01:11:55.627424 139586688767744 logging_writer.py:48] [9] global_step=9, grad_norm=0.614605, loss=6.925906
I0518 01:11:55.632275 139639360366400 submission.py:119] 9) loss = 6.926, grad_norm = 0.615
I0518 01:11:56.015784 139586697160448 logging_writer.py:48] [10] global_step=10, grad_norm=0.608123, loss=6.930352
I0518 01:11:56.019472 139639360366400 submission.py:119] 10) loss = 6.930, grad_norm = 0.608
I0518 01:11:56.403837 139586688767744 logging_writer.py:48] [11] global_step=11, grad_norm=0.595596, loss=6.918069
I0518 01:11:56.407896 139639360366400 submission.py:119] 11) loss = 6.918, grad_norm = 0.596
I0518 01:11:56.795537 139586697160448 logging_writer.py:48] [12] global_step=12, grad_norm=0.592531, loss=6.923568
I0518 01:11:56.799532 139639360366400 submission.py:119] 12) loss = 6.924, grad_norm = 0.593
I0518 01:11:57.185478 139586688767744 logging_writer.py:48] [13] global_step=13, grad_norm=0.596166, loss=6.919500
I0518 01:11:57.188944 139639360366400 submission.py:119] 13) loss = 6.919, grad_norm = 0.596
I0518 01:11:57.592265 139586697160448 logging_writer.py:48] [14] global_step=14, grad_norm=0.611111, loss=6.932511
I0518 01:11:57.596879 139639360366400 submission.py:119] 14) loss = 6.933, grad_norm = 0.611
I0518 01:11:57.977622 139586688767744 logging_writer.py:48] [15] global_step=15, grad_norm=0.586641, loss=6.923665
I0518 01:11:57.982729 139639360366400 submission.py:119] 15) loss = 6.924, grad_norm = 0.587
I0518 01:11:58.366053 139586697160448 logging_writer.py:48] [16] global_step=16, grad_norm=0.580193, loss=6.920053
I0518 01:11:58.372138 139639360366400 submission.py:119] 16) loss = 6.920, grad_norm = 0.580
I0518 01:11:58.757432 139586688767744 logging_writer.py:48] [17] global_step=17, grad_norm=0.597237, loss=6.923092
I0518 01:11:58.767593 139639360366400 submission.py:119] 17) loss = 6.923, grad_norm = 0.597
I0518 01:11:59.155671 139586697160448 logging_writer.py:48] [18] global_step=18, grad_norm=0.605812, loss=6.922885
I0518 01:11:59.159637 139639360366400 submission.py:119] 18) loss = 6.923, grad_norm = 0.606
I0518 01:11:59.540756 139586688767744 logging_writer.py:48] [19] global_step=19, grad_norm=0.597115, loss=6.923394
I0518 01:11:59.544610 139639360366400 submission.py:119] 19) loss = 6.923, grad_norm = 0.597
I0518 01:11:59.940104 139586697160448 logging_writer.py:48] [20] global_step=20, grad_norm=0.602247, loss=6.917137
I0518 01:11:59.944107 139639360366400 submission.py:119] 20) loss = 6.917, grad_norm = 0.602
I0518 01:12:00.329182 139586688767744 logging_writer.py:48] [21] global_step=21, grad_norm=0.600108, loss=6.925397
I0518 01:12:00.333085 139639360366400 submission.py:119] 21) loss = 6.925, grad_norm = 0.600
I0518 01:12:00.729617 139586697160448 logging_writer.py:48] [22] global_step=22, grad_norm=0.597601, loss=6.916574
I0518 01:12:00.733230 139639360366400 submission.py:119] 22) loss = 6.917, grad_norm = 0.598
I0518 01:12:01.119608 139586688767744 logging_writer.py:48] [23] global_step=23, grad_norm=0.588506, loss=6.920691
I0518 01:12:01.123948 139639360366400 submission.py:119] 23) loss = 6.921, grad_norm = 0.589
I0518 01:12:01.515318 139586697160448 logging_writer.py:48] [24] global_step=24, grad_norm=0.602729, loss=6.919448
I0518 01:12:01.520941 139639360366400 submission.py:119] 24) loss = 6.919, grad_norm = 0.603
I0518 01:12:01.915050 139586688767744 logging_writer.py:48] [25] global_step=25, grad_norm=0.599935, loss=6.921493
I0518 01:12:01.919142 139639360366400 submission.py:119] 25) loss = 6.921, grad_norm = 0.600
I0518 01:12:02.307532 139586697160448 logging_writer.py:48] [26] global_step=26, grad_norm=0.599804, loss=6.911849
I0518 01:12:02.312202 139639360366400 submission.py:119] 26) loss = 6.912, grad_norm = 0.600
I0518 01:12:02.694337 139586688767744 logging_writer.py:48] [27] global_step=27, grad_norm=0.589570, loss=6.925468
I0518 01:12:02.698117 139639360366400 submission.py:119] 27) loss = 6.925, grad_norm = 0.590
I0518 01:12:03.102495 139586697160448 logging_writer.py:48] [28] global_step=28, grad_norm=0.606759, loss=6.923801
I0518 01:12:03.106032 139639360366400 submission.py:119] 28) loss = 6.924, grad_norm = 0.607
I0518 01:12:03.490566 139586688767744 logging_writer.py:48] [29] global_step=29, grad_norm=0.594014, loss=6.904684
I0518 01:12:03.494765 139639360366400 submission.py:119] 29) loss = 6.905, grad_norm = 0.594
I0518 01:12:03.882916 139586697160448 logging_writer.py:48] [30] global_step=30, grad_norm=0.592748, loss=6.910682
I0518 01:12:03.886556 139639360366400 submission.py:119] 30) loss = 6.911, grad_norm = 0.593
I0518 01:12:04.267785 139586688767744 logging_writer.py:48] [31] global_step=31, grad_norm=0.587064, loss=6.918227
I0518 01:12:04.272044 139639360366400 submission.py:119] 31) loss = 6.918, grad_norm = 0.587
I0518 01:12:04.654350 139586697160448 logging_writer.py:48] [32] global_step=32, grad_norm=0.577514, loss=6.929766
I0518 01:12:04.661507 139639360366400 submission.py:119] 32) loss = 6.930, grad_norm = 0.578
I0518 01:12:05.053421 139586688767744 logging_writer.py:48] [33] global_step=33, grad_norm=0.611037, loss=6.913518
I0518 01:12:05.058001 139639360366400 submission.py:119] 33) loss = 6.914, grad_norm = 0.611
I0518 01:12:05.442332 139586697160448 logging_writer.py:48] [34] global_step=34, grad_norm=0.588505, loss=6.915730
I0518 01:12:05.446434 139639360366400 submission.py:119] 34) loss = 6.916, grad_norm = 0.589
I0518 01:12:05.830399 139586688767744 logging_writer.py:48] [35] global_step=35, grad_norm=0.589978, loss=6.919104
I0518 01:12:05.835239 139639360366400 submission.py:119] 35) loss = 6.919, grad_norm = 0.590
I0518 01:12:06.215299 139586697160448 logging_writer.py:48] [36] global_step=36, grad_norm=0.606562, loss=6.924537
I0518 01:12:06.219684 139639360366400 submission.py:119] 36) loss = 6.925, grad_norm = 0.607
I0518 01:12:06.609979 139586688767744 logging_writer.py:48] [37] global_step=37, grad_norm=0.600951, loss=6.918559
I0518 01:12:06.614045 139639360366400 submission.py:119] 37) loss = 6.919, grad_norm = 0.601
I0518 01:12:06.999490 139586697160448 logging_writer.py:48] [38] global_step=38, grad_norm=0.584075, loss=6.911346
I0518 01:12:07.004716 139639360366400 submission.py:119] 38) loss = 6.911, grad_norm = 0.584
I0518 01:12:07.391170 139586688767744 logging_writer.py:48] [39] global_step=39, grad_norm=0.599430, loss=6.919598
I0518 01:12:07.396103 139639360366400 submission.py:119] 39) loss = 6.920, grad_norm = 0.599
I0518 01:12:07.779054 139586697160448 logging_writer.py:48] [40] global_step=40, grad_norm=0.604213, loss=6.922106
I0518 01:12:07.787967 139639360366400 submission.py:119] 40) loss = 6.922, grad_norm = 0.604
I0518 01:12:08.172754 139586688767744 logging_writer.py:48] [41] global_step=41, grad_norm=0.581124, loss=6.912173
I0518 01:12:08.177852 139639360366400 submission.py:119] 41) loss = 6.912, grad_norm = 0.581
I0518 01:12:08.562134 139586697160448 logging_writer.py:48] [42] global_step=42, grad_norm=0.594073, loss=6.919326
I0518 01:12:08.565717 139639360366400 submission.py:119] 42) loss = 6.919, grad_norm = 0.594
I0518 01:12:08.949875 139586688767744 logging_writer.py:48] [43] global_step=43, grad_norm=0.601536, loss=6.912813
I0518 01:12:08.953406 139639360366400 submission.py:119] 43) loss = 6.913, grad_norm = 0.602
I0518 01:12:09.343963 139586697160448 logging_writer.py:48] [44] global_step=44, grad_norm=0.591289, loss=6.912834
I0518 01:12:09.347775 139639360366400 submission.py:119] 44) loss = 6.913, grad_norm = 0.591
I0518 01:12:09.730214 139586688767744 logging_writer.py:48] [45] global_step=45, grad_norm=0.583427, loss=6.910247
I0518 01:12:09.734890 139639360366400 submission.py:119] 45) loss = 6.910, grad_norm = 0.583
I0518 01:12:10.129197 139586697160448 logging_writer.py:48] [46] global_step=46, grad_norm=0.612251, loss=6.913146
I0518 01:12:10.132905 139639360366400 submission.py:119] 46) loss = 6.913, grad_norm = 0.612
I0518 01:12:10.517491 139586688767744 logging_writer.py:48] [47] global_step=47, grad_norm=0.591086, loss=6.918203
I0518 01:12:10.521965 139639360366400 submission.py:119] 47) loss = 6.918, grad_norm = 0.591
I0518 01:12:10.909063 139586697160448 logging_writer.py:48] [48] global_step=48, grad_norm=0.589247, loss=6.916762
I0518 01:12:10.913186 139639360366400 submission.py:119] 48) loss = 6.917, grad_norm = 0.589
I0518 01:12:11.296340 139586688767744 logging_writer.py:48] [49] global_step=49, grad_norm=0.601311, loss=6.916392
I0518 01:12:11.300194 139639360366400 submission.py:119] 49) loss = 6.916, grad_norm = 0.601
I0518 01:12:11.683462 139586697160448 logging_writer.py:48] [50] global_step=50, grad_norm=0.584982, loss=6.903061
I0518 01:12:11.688573 139639360366400 submission.py:119] 50) loss = 6.903, grad_norm = 0.585
I0518 01:12:12.076379 139586688767744 logging_writer.py:48] [51] global_step=51, grad_norm=0.592237, loss=6.904791
I0518 01:12:12.080627 139639360366400 submission.py:119] 51) loss = 6.905, grad_norm = 0.592
I0518 01:12:12.462324 139586697160448 logging_writer.py:48] [52] global_step=52, grad_norm=0.581180, loss=6.910920
I0518 01:12:12.466590 139639360366400 submission.py:119] 52) loss = 6.911, grad_norm = 0.581
I0518 01:12:12.851926 139586688767744 logging_writer.py:48] [53] global_step=53, grad_norm=0.578456, loss=6.911260
I0518 01:12:12.855988 139639360366400 submission.py:119] 53) loss = 6.911, grad_norm = 0.578
I0518 01:12:13.241156 139586697160448 logging_writer.py:48] [54] global_step=54, grad_norm=0.598939, loss=6.903226
I0518 01:12:13.246186 139639360366400 submission.py:119] 54) loss = 6.903, grad_norm = 0.599
I0518 01:12:13.631993 139586688767744 logging_writer.py:48] [55] global_step=55, grad_norm=0.581536, loss=6.912943
I0518 01:12:13.636714 139639360366400 submission.py:119] 55) loss = 6.913, grad_norm = 0.582
I0518 01:12:14.018485 139586697160448 logging_writer.py:48] [56] global_step=56, grad_norm=0.594183, loss=6.903083
I0518 01:12:14.023168 139639360366400 submission.py:119] 56) loss = 6.903, grad_norm = 0.594
I0518 01:12:14.406801 139586688767744 logging_writer.py:48] [57] global_step=57, grad_norm=0.595268, loss=6.907751
I0518 01:12:14.410544 139639360366400 submission.py:119] 57) loss = 6.908, grad_norm = 0.595
I0518 01:12:14.797466 139586697160448 logging_writer.py:48] [58] global_step=58, grad_norm=0.608389, loss=6.903447
I0518 01:12:14.802711 139639360366400 submission.py:119] 58) loss = 6.903, grad_norm = 0.608
I0518 01:12:15.191501 139586688767744 logging_writer.py:48] [59] global_step=59, grad_norm=0.598799, loss=6.901592
I0518 01:12:15.196392 139639360366400 submission.py:119] 59) loss = 6.902, grad_norm = 0.599
I0518 01:12:15.580312 139586697160448 logging_writer.py:48] [60] global_step=60, grad_norm=0.592831, loss=6.897782
I0518 01:12:15.584775 139639360366400 submission.py:119] 60) loss = 6.898, grad_norm = 0.593
I0518 01:12:15.969585 139586688767744 logging_writer.py:48] [61] global_step=61, grad_norm=0.568421, loss=6.896025
I0518 01:12:15.973881 139639360366400 submission.py:119] 61) loss = 6.896, grad_norm = 0.568
I0518 01:12:16.360377 139586697160448 logging_writer.py:48] [62] global_step=62, grad_norm=0.579824, loss=6.903221
I0518 01:12:16.371774 139639360366400 submission.py:119] 62) loss = 6.903, grad_norm = 0.580
I0518 01:12:16.755048 139586688767744 logging_writer.py:48] [63] global_step=63, grad_norm=0.598923, loss=6.901057
I0518 01:12:16.758612 139639360366400 submission.py:119] 63) loss = 6.901, grad_norm = 0.599
I0518 01:12:17.146399 139586697160448 logging_writer.py:48] [64] global_step=64, grad_norm=0.613251, loss=6.898795
I0518 01:12:17.150885 139639360366400 submission.py:119] 64) loss = 6.899, grad_norm = 0.613
I0518 01:12:17.540415 139586688767744 logging_writer.py:48] [65] global_step=65, grad_norm=0.588969, loss=6.901937
I0518 01:12:17.545235 139639360366400 submission.py:119] 65) loss = 6.902, grad_norm = 0.589
I0518 01:12:17.943704 139586697160448 logging_writer.py:48] [66] global_step=66, grad_norm=0.589340, loss=6.889279
I0518 01:12:17.948832 139639360366400 submission.py:119] 66) loss = 6.889, grad_norm = 0.589
I0518 01:12:18.338082 139586688767744 logging_writer.py:48] [67] global_step=67, grad_norm=0.599961, loss=6.897575
I0518 01:12:18.341542 139639360366400 submission.py:119] 67) loss = 6.898, grad_norm = 0.600
I0518 01:12:18.728714 139586697160448 logging_writer.py:48] [68] global_step=68, grad_norm=0.598380, loss=6.897996
I0518 01:12:18.733013 139639360366400 submission.py:119] 68) loss = 6.898, grad_norm = 0.598
I0518 01:12:19.139520 139586688767744 logging_writer.py:48] [69] global_step=69, grad_norm=0.570679, loss=6.903608
I0518 01:12:19.143269 139639360366400 submission.py:119] 69) loss = 6.904, grad_norm = 0.571
I0518 01:12:19.531527 139586697160448 logging_writer.py:48] [70] global_step=70, grad_norm=0.591679, loss=6.893980
I0518 01:12:19.535632 139639360366400 submission.py:119] 70) loss = 6.894, grad_norm = 0.592
I0518 01:12:19.936830 139586688767744 logging_writer.py:48] [71] global_step=71, grad_norm=0.585382, loss=6.905711
I0518 01:12:19.940381 139639360366400 submission.py:119] 71) loss = 6.906, grad_norm = 0.585
I0518 01:12:20.323279 139586697160448 logging_writer.py:48] [72] global_step=72, grad_norm=0.594957, loss=6.898534
I0518 01:12:20.327052 139639360366400 submission.py:119] 72) loss = 6.899, grad_norm = 0.595
I0518 01:12:20.711462 139586688767744 logging_writer.py:48] [73] global_step=73, grad_norm=0.606587, loss=6.899393
I0518 01:12:20.714878 139639360366400 submission.py:119] 73) loss = 6.899, grad_norm = 0.607
I0518 01:12:21.099991 139586697160448 logging_writer.py:48] [74] global_step=74, grad_norm=0.573388, loss=6.896223
I0518 01:12:21.103607 139639360366400 submission.py:119] 74) loss = 6.896, grad_norm = 0.573
I0518 01:12:21.491817 139586688767744 logging_writer.py:48] [75] global_step=75, grad_norm=0.578850, loss=6.889970
I0518 01:12:21.498108 139639360366400 submission.py:119] 75) loss = 6.890, grad_norm = 0.579
I0518 01:12:21.887789 139586697160448 logging_writer.py:48] [76] global_step=76, grad_norm=0.580260, loss=6.887785
I0518 01:12:21.891971 139639360366400 submission.py:119] 76) loss = 6.888, grad_norm = 0.580
I0518 01:12:22.281648 139586688767744 logging_writer.py:48] [77] global_step=77, grad_norm=0.590819, loss=6.888846
I0518 01:12:22.285669 139639360366400 submission.py:119] 77) loss = 6.889, grad_norm = 0.591
I0518 01:12:22.671480 139586697160448 logging_writer.py:48] [78] global_step=78, grad_norm=0.601634, loss=6.889206
I0518 01:12:22.675341 139639360366400 submission.py:119] 78) loss = 6.889, grad_norm = 0.602
I0518 01:12:23.067318 139586688767744 logging_writer.py:48] [79] global_step=79, grad_norm=0.605549, loss=6.894651
I0518 01:12:23.071063 139639360366400 submission.py:119] 79) loss = 6.895, grad_norm = 0.606
I0518 01:12:23.463875 139586697160448 logging_writer.py:48] [80] global_step=80, grad_norm=0.574305, loss=6.882861
I0518 01:12:23.467582 139639360366400 submission.py:119] 80) loss = 6.883, grad_norm = 0.574
I0518 01:12:23.863236 139586688767744 logging_writer.py:48] [81] global_step=81, grad_norm=0.585807, loss=6.885338
I0518 01:12:23.867750 139639360366400 submission.py:119] 81) loss = 6.885, grad_norm = 0.586
I0518 01:12:24.259157 139586697160448 logging_writer.py:48] [82] global_step=82, grad_norm=0.585282, loss=6.890253
I0518 01:12:24.265281 139639360366400 submission.py:119] 82) loss = 6.890, grad_norm = 0.585
I0518 01:12:24.662441 139586688767744 logging_writer.py:48] [83] global_step=83, grad_norm=0.598565, loss=6.896639
I0518 01:12:24.666714 139639360366400 submission.py:119] 83) loss = 6.897, grad_norm = 0.599
I0518 01:12:25.049432 139586697160448 logging_writer.py:48] [84] global_step=84, grad_norm=0.583757, loss=6.897913
I0518 01:12:25.053086 139639360366400 submission.py:119] 84) loss = 6.898, grad_norm = 0.584
I0518 01:12:25.439446 139586688767744 logging_writer.py:48] [85] global_step=85, grad_norm=0.580754, loss=6.887725
I0518 01:12:25.442916 139639360366400 submission.py:119] 85) loss = 6.888, grad_norm = 0.581
I0518 01:12:25.847434 139586697160448 logging_writer.py:48] [86] global_step=86, grad_norm=0.604705, loss=6.885225
I0518 01:12:25.851763 139639360366400 submission.py:119] 86) loss = 6.885, grad_norm = 0.605
I0518 01:12:26.242371 139586688767744 logging_writer.py:48] [87] global_step=87, grad_norm=0.591667, loss=6.883069
I0518 01:12:26.246225 139639360366400 submission.py:119] 87) loss = 6.883, grad_norm = 0.592
I0518 01:12:26.635621 139586697160448 logging_writer.py:48] [88] global_step=88, grad_norm=0.594301, loss=6.886920
I0518 01:12:26.639583 139639360366400 submission.py:119] 88) loss = 6.887, grad_norm = 0.594
I0518 01:12:27.025691 139586688767744 logging_writer.py:48] [89] global_step=89, grad_norm=0.588639, loss=6.880328
I0518 01:12:27.029404 139639360366400 submission.py:119] 89) loss = 6.880, grad_norm = 0.589
I0518 01:12:27.415926 139586697160448 logging_writer.py:48] [90] global_step=90, grad_norm=0.601260, loss=6.887429
I0518 01:12:27.420045 139639360366400 submission.py:119] 90) loss = 6.887, grad_norm = 0.601
I0518 01:12:27.807982 139586688767744 logging_writer.py:48] [91] global_step=91, grad_norm=0.583209, loss=6.881202
I0518 01:12:27.812292 139639360366400 submission.py:119] 91) loss = 6.881, grad_norm = 0.583
I0518 01:12:28.205787 139586697160448 logging_writer.py:48] [92] global_step=92, grad_norm=0.605679, loss=6.885426
I0518 01:12:28.209770 139639360366400 submission.py:119] 92) loss = 6.885, grad_norm = 0.606
I0518 01:12:28.612590 139586688767744 logging_writer.py:48] [93] global_step=93, grad_norm=0.587819, loss=6.878226
I0518 01:12:28.617082 139639360366400 submission.py:119] 93) loss = 6.878, grad_norm = 0.588
I0518 01:12:29.010480 139586697160448 logging_writer.py:48] [94] global_step=94, grad_norm=0.578584, loss=6.880212
I0518 01:12:29.014604 139639360366400 submission.py:119] 94) loss = 6.880, grad_norm = 0.579
I0518 01:12:29.414544 139586688767744 logging_writer.py:48] [95] global_step=95, grad_norm=0.588684, loss=6.885705
I0518 01:12:29.421416 139639360366400 submission.py:119] 95) loss = 6.886, grad_norm = 0.589
I0518 01:12:29.812312 139586697160448 logging_writer.py:48] [96] global_step=96, grad_norm=0.588772, loss=6.875442
I0518 01:12:29.816195 139639360366400 submission.py:119] 96) loss = 6.875, grad_norm = 0.589
I0518 01:12:30.202280 139586688767744 logging_writer.py:48] [97] global_step=97, grad_norm=0.597095, loss=6.866893
I0518 01:12:30.206281 139639360366400 submission.py:119] 97) loss = 6.867, grad_norm = 0.597
I0518 01:12:30.590196 139586697160448 logging_writer.py:48] [98] global_step=98, grad_norm=0.606270, loss=6.878205
I0518 01:12:30.593748 139639360366400 submission.py:119] 98) loss = 6.878, grad_norm = 0.606
I0518 01:12:30.980612 139586688767744 logging_writer.py:48] [99] global_step=99, grad_norm=0.598938, loss=6.871655
I0518 01:12:30.984534 139639360366400 submission.py:119] 99) loss = 6.872, grad_norm = 0.599
I0518 01:12:31.371639 139586697160448 logging_writer.py:48] [100] global_step=100, grad_norm=0.591615, loss=6.876391
I0518 01:12:31.375810 139639360366400 submission.py:119] 100) loss = 6.876, grad_norm = 0.592
I0518 01:15:02.026396 139586688767744 logging_writer.py:48] [500] global_step=500, grad_norm=0.924249, loss=6.297516
I0518 01:15:02.031961 139639360366400 submission.py:119] 500) loss = 6.298, grad_norm = 0.924
I0518 01:18:10.327482 139586697160448 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.949362, loss=5.696654
I0518 01:18:10.333127 139639360366400 submission.py:119] 1000) loss = 5.697, grad_norm = 2.949
I0518 01:20:22.287645 139639360366400 spec.py:298] Evaluating on the training split.
I0518 01:21:05.019323 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 01:22:00.310937 139639360366400 spec.py:326] Evaluating on the test split.
I0518 01:22:01.712406 139639360366400 submission_runner.py:421] Time since start: 746.45s, 	Step: 1348, 	{'train/accuracy': 0.1126235650510204, 'train/loss': 4.839840947365274, 'validation/accuracy': 0.10368, 'validation/loss': 4.9202246875, 'validation/num_examples': 50000, 'test/accuracy': 0.0695, 'test/loss': 5.323053125, 'test/num_examples': 10000, 'score': 517.5543377399445, 'total_duration': 746.4450426101685, 'accumulated_submission_time': 517.5543377399445, 'accumulated_eval_time': 228.32104563713074, 'accumulated_logging_time': 0.0238802433013916}
I0518 01:22:01.725179 139586705553152 logging_writer.py:48] [1348] accumulated_eval_time=228.321046, accumulated_logging_time=0.023880, accumulated_submission_time=517.554338, global_step=1348, preemption_count=0, score=517.554338, test/accuracy=0.069500, test/loss=5.323053, test/num_examples=10000, total_duration=746.445043, train/accuracy=0.112624, train/loss=4.839841, validation/accuracy=0.103680, validation/loss=4.920225, validation/num_examples=50000
I0518 01:22:58.878030 139587192067840 logging_writer.py:48] [1500] global_step=1500, grad_norm=5.177838, loss=5.317966
I0518 01:22:58.882203 139639360366400 submission.py:119] 1500) loss = 5.318, grad_norm = 5.178
I0518 01:26:06.106284 139586705553152 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.428908, loss=4.940953
I0518 01:26:06.110522 139639360366400 submission.py:119] 2000) loss = 4.941, grad_norm = 3.429
I0518 01:29:14.251441 139587192067840 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.216517, loss=4.626143
I0518 01:29:14.255267 139639360366400 submission.py:119] 2500) loss = 4.626, grad_norm = 4.217
I0518 01:30:32.034046 139639360366400 spec.py:298] Evaluating on the training split.
I0518 01:31:17.182467 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 01:32:11.421150 139639360366400 spec.py:326] Evaluating on the test split.
I0518 01:32:12.783723 139639360366400 submission_runner.py:421] Time since start: 1357.52s, 	Step: 2705, 	{'train/accuracy': 0.2530891262755102, 'train/loss': 3.702829477738361, 'validation/accuracy': 0.22836, 'validation/loss': 3.8375884375, 'validation/num_examples': 50000, 'test/accuracy': 0.1623, 'test/loss': 4.391807421875, 'test/num_examples': 10000, 'score': 1027.3640894889832, 'total_duration': 1357.515206336975, 'accumulated_submission_time': 1027.3640894889832, 'accumulated_eval_time': 329.0693280696869, 'accumulated_logging_time': 0.04634690284729004}
I0518 01:32:12.792931 139586705553152 logging_writer.py:48] [2705] accumulated_eval_time=329.069328, accumulated_logging_time=0.046347, accumulated_submission_time=1027.364089, global_step=2705, preemption_count=0, score=1027.364089, test/accuracy=0.162300, test/loss=4.391807, test/num_examples=10000, total_duration=1357.515206, train/accuracy=0.253089, train/loss=3.702829, validation/accuracy=0.228360, validation/loss=3.837588, validation/num_examples=50000
I0518 01:34:03.391180 139587192067840 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.986992, loss=4.369642
I0518 01:34:03.395762 139639360366400 submission.py:119] 3000) loss = 4.370, grad_norm = 4.987
I0518 01:37:10.629050 139586705553152 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.758381, loss=4.071679
I0518 01:37:10.633110 139639360366400 submission.py:119] 3500) loss = 4.072, grad_norm = 2.758
I0518 01:40:19.269557 139587192067840 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.403253, loss=4.013976
I0518 01:40:19.273371 139639360366400 submission.py:119] 4000) loss = 4.014, grad_norm = 3.403
I0518 01:40:43.175540 139639360366400 spec.py:298] Evaluating on the training split.
I0518 01:41:24.601401 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 01:42:08.295594 139639360366400 spec.py:326] Evaluating on the test split.
I0518 01:42:09.661012 139639360366400 submission_runner.py:421] Time since start: 1954.39s, 	Step: 4065, 	{'train/accuracy': 0.36738679846938777, 'train/loss': 3.006744384765625, 'validation/accuracy': 0.33548, 'validation/loss': 3.1731546875, 'validation/num_examples': 50000, 'test/accuracy': 0.2488, 'test/loss': 3.7751296875, 'test/num_examples': 10000, 'score': 1537.2584671974182, 'total_duration': 1954.3939156532288, 'accumulated_submission_time': 1537.2584671974182, 'accumulated_eval_time': 415.5547707080841, 'accumulated_logging_time': 0.06373000144958496}
I0518 01:42:09.671364 139586705553152 logging_writer.py:48] [4065] accumulated_eval_time=415.554771, accumulated_logging_time=0.063730, accumulated_submission_time=1537.258467, global_step=4065, preemption_count=0, score=1537.258467, test/accuracy=0.248800, test/loss=3.775130, test/num_examples=10000, total_duration=1954.393916, train/accuracy=0.367387, train/loss=3.006744, validation/accuracy=0.335480, validation/loss=3.173155, validation/num_examples=50000
I0518 01:44:52.680739 139587192067840 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.687997, loss=3.632088
I0518 01:44:52.685109 139639360366400 submission.py:119] 4500) loss = 3.632, grad_norm = 1.688
I0518 01:48:00.677793 139586705553152 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.155231, loss=3.640983
I0518 01:48:00.683105 139639360366400 submission.py:119] 5000) loss = 3.641, grad_norm = 3.155
I0518 01:50:39.888626 139639360366400 spec.py:298] Evaluating on the training split.
I0518 01:51:24.302715 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 01:52:19.986770 139639360366400 spec.py:326] Evaluating on the test split.
I0518 01:52:21.343132 139639360366400 submission_runner.py:421] Time since start: 2566.08s, 	Step: 5423, 	{'train/accuracy': 0.4384167729591837, 'train/loss': 2.5883337526905295, 'validation/accuracy': 0.39776, 'validation/loss': 2.8067809375, 'validation/num_examples': 50000, 'test/accuracy': 0.2953, 'test/loss': 3.4852171875, 'test/num_examples': 10000, 'score': 2046.9737541675568, 'total_duration': 2566.0760085582733, 'accumulated_submission_time': 2046.9737541675568, 'accumulated_eval_time': 517.0092177391052, 'accumulated_logging_time': 0.08228635787963867}
I0518 01:52:21.353325 139587192067840 logging_writer.py:48] [5423] accumulated_eval_time=517.009218, accumulated_logging_time=0.082286, accumulated_submission_time=2046.973754, global_step=5423, preemption_count=0, score=2046.973754, test/accuracy=0.295300, test/loss=3.485217, test/num_examples=10000, total_duration=2566.076009, train/accuracy=0.438417, train/loss=2.588334, validation/accuracy=0.397760, validation/loss=2.806781, validation/num_examples=50000
I0518 01:52:50.519007 139586705553152 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.159639, loss=3.568808
I0518 01:52:50.522437 139639360366400 submission.py:119] 5500) loss = 3.569, grad_norm = 2.160
I0518 01:55:57.788331 139587192067840 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.633070, loss=3.420749
I0518 01:55:57.793460 139639360366400 submission.py:119] 6000) loss = 3.421, grad_norm = 1.633
I0518 01:59:06.807408 139586705553152 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.382820, loss=3.324137
I0518 01:59:06.812132 139639360366400 submission.py:119] 6500) loss = 3.324, grad_norm = 1.383
I0518 02:00:51.414539 139639360366400 spec.py:298] Evaluating on the training split.
I0518 02:01:34.796772 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 02:02:19.780421 139639360366400 spec.py:326] Evaluating on the test split.
I0518 02:02:21.136438 139639360366400 submission_runner.py:421] Time since start: 3165.87s, 	Step: 6781, 	{'train/accuracy': 0.5190130739795918, 'train/loss': 2.186302340760523, 'validation/accuracy': 0.47544, 'validation/loss': 2.4079078125, 'validation/num_examples': 50000, 'test/accuracy': 0.354, 'test/loss': 3.08278671875, 'test/num_examples': 10000, 'score': 2556.5354743003845, 'total_duration': 3165.8692643642426, 'accumulated_submission_time': 2556.5354743003845, 'accumulated_eval_time': 606.7310247421265, 'accumulated_logging_time': 0.10019302368164062}
I0518 02:02:21.148582 139587192067840 logging_writer.py:48] [6781] accumulated_eval_time=606.731025, accumulated_logging_time=0.100193, accumulated_submission_time=2556.535474, global_step=6781, preemption_count=0, score=2556.535474, test/accuracy=0.354000, test/loss=3.082787, test/num_examples=10000, total_duration=3165.869264, train/accuracy=0.519013, train/loss=2.186302, validation/accuracy=0.475440, validation/loss=2.407908, validation/num_examples=50000
I0518 02:03:43.280174 139586705553152 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.861346, loss=3.258494
I0518 02:03:43.284168 139639360366400 submission.py:119] 7000) loss = 3.258, grad_norm = 1.861
I0518 02:06:51.072691 139587192067840 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.128098, loss=3.227593
I0518 02:06:51.076916 139639360366400 submission.py:119] 7500) loss = 3.228, grad_norm = 1.128
I0518 02:09:59.401833 139586705553152 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.391678, loss=3.157069
I0518 02:09:59.406221 139639360366400 submission.py:119] 8000) loss = 3.157, grad_norm = 1.392
I0518 02:10:51.427825 139639360366400 spec.py:298] Evaluating on the training split.
I0518 02:11:35.595908 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 02:12:20.676207 139639360366400 spec.py:326] Evaluating on the test split.
I0518 02:12:22.042529 139639360366400 submission_runner.py:421] Time since start: 3766.78s, 	Step: 8140, 	{'train/accuracy': 0.5419523278061225, 'train/loss': 2.0230683307258452, 'validation/accuracy': 0.49062, 'validation/loss': 2.2821115625, 'validation/num_examples': 50000, 'test/accuracy': 0.3761, 'test/loss': 3.0166126953125, 'test/num_examples': 10000, 'score': 3066.315442800522, 'total_duration': 3766.775370121002, 'accumulated_submission_time': 3066.315442800522, 'accumulated_eval_time': 697.3457424640656, 'accumulated_logging_time': 0.12054681777954102}
I0518 02:12:22.052248 139587192067840 logging_writer.py:48] [8140] accumulated_eval_time=697.345742, accumulated_logging_time=0.120547, accumulated_submission_time=3066.315443, global_step=8140, preemption_count=0, score=3066.315443, test/accuracy=0.376100, test/loss=3.016613, test/num_examples=10000, total_duration=3766.775370, train/accuracy=0.541952, train/loss=2.023068, validation/accuracy=0.490620, validation/loss=2.282112, validation/num_examples=50000
I0518 02:14:37.110557 139586705553152 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.962557, loss=3.104717
I0518 02:14:37.114638 139639360366400 submission.py:119] 8500) loss = 3.105, grad_norm = 0.963
I0518 02:17:45.989467 139587192067840 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.262328, loss=3.102813
I0518 02:17:45.994055 139639360366400 submission.py:119] 9000) loss = 3.103, grad_norm = 1.262
I0518 02:20:52.099383 139639360366400 spec.py:298] Evaluating on the training split.
I0518 02:21:35.032719 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 02:22:19.459507 139639360366400 spec.py:326] Evaluating on the test split.
I0518 02:22:20.819429 139639360366400 submission_runner.py:421] Time since start: 4365.55s, 	Step: 9499, 	{'train/accuracy': 0.5697544642857143, 'train/loss': 1.9426499970105229, 'validation/accuracy': 0.51542, 'validation/loss': 2.2234978125, 'validation/num_examples': 50000, 'test/accuracy': 0.3945, 'test/loss': 2.9274208984375, 'test/num_examples': 10000, 'score': 3575.8637721538544, 'total_duration': 4365.552325963974, 'accumulated_submission_time': 3575.8637721538544, 'accumulated_eval_time': 786.0657477378845, 'accumulated_logging_time': 0.1381826400756836}
I0518 02:22:20.829588 139586705553152 logging_writer.py:48] [9499] accumulated_eval_time=786.065748, accumulated_logging_time=0.138183, accumulated_submission_time=3575.863772, global_step=9499, preemption_count=0, score=3575.863772, test/accuracy=0.394500, test/loss=2.927421, test/num_examples=10000, total_duration=4365.552326, train/accuracy=0.569754, train/loss=1.942650, validation/accuracy=0.515420, validation/loss=2.223498, validation/num_examples=50000
I0518 02:22:21.624888 139587192067840 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.920601, loss=3.092120
I0518 02:22:21.628442 139639360366400 submission.py:119] 9500) loss = 3.092, grad_norm = 0.921
I0518 02:25:29.273043 139586705553152 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.843955, loss=2.941257
I0518 02:25:29.277166 139639360366400 submission.py:119] 10000) loss = 2.941, grad_norm = 0.844
I0518 02:28:37.461781 139587192067840 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.944910, loss=2.901101
I0518 02:28:37.465864 139639360366400 submission.py:119] 10500) loss = 2.901, grad_norm = 0.945
I0518 02:30:51.042683 139639360366400 spec.py:298] Evaluating on the training split.
I0518 02:31:33.842203 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 02:32:22.483314 139639360366400 spec.py:326] Evaluating on the test split.
I0518 02:32:23.841669 139639360366400 submission_runner.py:421] Time since start: 4968.57s, 	Step: 10858, 	{'train/accuracy': 0.611328125, 'train/loss': 1.7097759636080996, 'validation/accuracy': 0.55438, 'validation/loss': 1.99458125, 'validation/num_examples': 50000, 'test/accuracy': 0.43, 'test/loss': 2.7279087890625, 'test/num_examples': 10000, 'score': 4085.571762561798, 'total_duration': 4968.574539422989, 'accumulated_submission_time': 4085.571762561798, 'accumulated_eval_time': 878.8646557331085, 'accumulated_logging_time': 0.15598678588867188}
I0518 02:32:23.854037 139586705553152 logging_writer.py:48] [10858] accumulated_eval_time=878.864656, accumulated_logging_time=0.155987, accumulated_submission_time=4085.571763, global_step=10858, preemption_count=0, score=4085.571763, test/accuracy=0.430000, test/loss=2.727909, test/num_examples=10000, total_duration=4968.574539, train/accuracy=0.611328, train/loss=1.709776, validation/accuracy=0.554380, validation/loss=1.994581, validation/num_examples=50000
I0518 02:33:17.359810 139587192067840 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.661292, loss=2.795405
I0518 02:33:17.363631 139639360366400 submission.py:119] 11000) loss = 2.795, grad_norm = 0.661
I0518 02:36:26.605090 139586705553152 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.743559, loss=2.750468
I0518 02:36:26.612571 139639360366400 submission.py:119] 11500) loss = 2.750, grad_norm = 0.744
I0518 02:39:33.420601 139587192067840 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.650359, loss=2.665901
I0518 02:39:33.424938 139639360366400 submission.py:119] 12000) loss = 2.666, grad_norm = 0.650
I0518 02:40:53.892286 139639360366400 spec.py:298] Evaluating on the training split.
I0518 02:41:36.701537 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 02:42:20.664846 139639360366400 spec.py:326] Evaluating on the test split.
I0518 02:42:22.030835 139639360366400 submission_runner.py:421] Time since start: 5566.76s, 	Step: 12216, 	{'train/accuracy': 0.6395487882653061, 'train/loss': 1.6292986188616072, 'validation/accuracy': 0.5733, 'validation/loss': 1.94100625, 'validation/num_examples': 50000, 'test/accuracy': 0.4506, 'test/loss': 2.602783984375, 'test/num_examples': 10000, 'score': 4595.106580018997, 'total_duration': 5566.763742208481, 'accumulated_submission_time': 4595.106580018997, 'accumulated_eval_time': 967.0031452178955, 'accumulated_logging_time': 0.17780470848083496}
I0518 02:42:22.041053 139586705553152 logging_writer.py:48] [12216] accumulated_eval_time=967.003145, accumulated_logging_time=0.177805, accumulated_submission_time=4595.106580, global_step=12216, preemption_count=0, score=4595.106580, test/accuracy=0.450600, test/loss=2.602784, test/num_examples=10000, total_duration=5566.763742, train/accuracy=0.639549, train/loss=1.629299, validation/accuracy=0.573300, validation/loss=1.941006, validation/num_examples=50000
I0518 02:44:09.060913 139587192067840 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.636879, loss=2.807683
I0518 02:44:09.066564 139639360366400 submission.py:119] 12500) loss = 2.808, grad_norm = 0.637
I0518 02:47:17.240140 139586705553152 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.523181, loss=2.672281
I0518 02:47:17.244882 139639360366400 submission.py:119] 13000) loss = 2.672, grad_norm = 0.523
I0518 02:50:24.268698 139587192067840 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.570176, loss=2.681030
I0518 02:50:24.272690 139639360366400 submission.py:119] 13500) loss = 2.681, grad_norm = 0.570
I0518 02:50:52.120569 139639360366400 spec.py:298] Evaluating on the training split.
I0518 02:51:34.213563 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 02:52:31.112871 139639360366400 spec.py:326] Evaluating on the test split.
I0518 02:52:32.476994 139639360366400 submission_runner.py:421] Time since start: 6177.21s, 	Step: 13575, 	{'train/accuracy': 0.6668526785714286, 'train/loss': 1.4395765966298628, 'validation/accuracy': 0.60108, 'validation/loss': 1.764566875, 'validation/num_examples': 50000, 'test/accuracy': 0.4557, 'test/loss': 2.5206498046875, 'test/num_examples': 10000, 'score': 5104.679742097855, 'total_duration': 6177.2078285217285, 'accumulated_submission_time': 5104.679742097855, 'accumulated_eval_time': 1067.3574419021606, 'accumulated_logging_time': 0.1956162452697754}
I0518 02:52:32.487371 139586705553152 logging_writer.py:48] [13575] accumulated_eval_time=1067.357442, accumulated_logging_time=0.195616, accumulated_submission_time=5104.679742, global_step=13575, preemption_count=0, score=5104.679742, test/accuracy=0.455700, test/loss=2.520650, test/num_examples=10000, total_duration=6177.207829, train/accuracy=0.666853, train/loss=1.439577, validation/accuracy=0.601080, validation/loss=1.764567, validation/num_examples=50000
I0518 02:55:13.813105 139587192067840 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.549277, loss=2.632146
I0518 02:55:13.817006 139639360366400 submission.py:119] 14000) loss = 2.632, grad_norm = 0.549
I0518 02:58:20.740289 139586705553152 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.601071, loss=2.607303
I0518 02:58:20.744463 139639360366400 submission.py:119] 14500) loss = 2.607, grad_norm = 0.601
I0518 03:01:02.668516 139639360366400 spec.py:298] Evaluating on the training split.
I0518 03:01:47.917754 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 03:02:33.760939 139639360366400 spec.py:326] Evaluating on the test split.
I0518 03:02:35.120694 139639360366400 submission_runner.py:421] Time since start: 6779.85s, 	Step: 14932, 	{'train/accuracy': 0.670280612244898, 'train/loss': 1.4543492842693717, 'validation/accuracy': 0.60336, 'validation/loss': 1.7722246875, 'validation/num_examples': 50000, 'test/accuracy': 0.4733, 'test/loss': 2.4677029296875, 'test/num_examples': 10000, 'score': 5614.3503568172455, 'total_duration': 6779.853564500809, 'accumulated_submission_time': 5614.3503568172455, 'accumulated_eval_time': 1159.8096103668213, 'accumulated_logging_time': 0.21430301666259766}
I0518 03:02:35.130920 139587192067840 logging_writer.py:48] [14932] accumulated_eval_time=1159.809610, accumulated_logging_time=0.214303, accumulated_submission_time=5614.350357, global_step=14932, preemption_count=0, score=5614.350357, test/accuracy=0.473300, test/loss=2.467703, test/num_examples=10000, total_duration=6779.853565, train/accuracy=0.670281, train/loss=1.454349, validation/accuracy=0.603360, validation/loss=1.772225, validation/num_examples=50000
I0518 03:03:01.048794 139586705553152 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.479363, loss=2.662442
I0518 03:03:01.053304 139639360366400 submission.py:119] 15000) loss = 2.662, grad_norm = 0.479
I0518 03:06:09.434935 139587192067840 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.550144, loss=2.571881
I0518 03:06:09.438911 139639360366400 submission.py:119] 15500) loss = 2.572, grad_norm = 0.550
I0518 03:09:16.692654 139586705553152 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.557929, loss=2.563874
I0518 03:09:16.697195 139639360366400 submission.py:119] 16000) loss = 2.564, grad_norm = 0.558
I0518 03:11:05.191412 139639360366400 spec.py:298] Evaluating on the training split.
I0518 03:11:47.274394 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 03:12:43.702405 139639360366400 spec.py:326] Evaluating on the test split.
I0518 03:12:45.060461 139639360366400 submission_runner.py:421] Time since start: 7389.79s, 	Step: 16285, 	{'train/accuracy': 0.7034239477040817, 'train/loss': 1.3633992720623405, 'validation/accuracy': 0.62562, 'validation/loss': 1.71865015625, 'validation/num_examples': 50000, 'test/accuracy': 0.492, 'test/loss': 2.4218541015625, 'test/num_examples': 10000, 'score': 6123.900628089905, 'total_duration': 7389.791800260544, 'accumulated_submission_time': 6123.900628089905, 'accumulated_eval_time': 1259.6770253181458, 'accumulated_logging_time': 0.23334455490112305}
I0518 03:12:45.071637 139587192067840 logging_writer.py:48] [16285] accumulated_eval_time=1259.677025, accumulated_logging_time=0.233345, accumulated_submission_time=6123.900628, global_step=16285, preemption_count=0, score=6123.900628, test/accuracy=0.492000, test/loss=2.421854, test/num_examples=10000, total_duration=7389.791800, train/accuracy=0.703424, train/loss=1.363399, validation/accuracy=0.625620, validation/loss=1.718650, validation/num_examples=50000
I0518 03:14:05.606454 139586705553152 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.468342, loss=2.567870
I0518 03:14:05.610554 139639360366400 submission.py:119] 16500) loss = 2.568, grad_norm = 0.468
I0518 03:17:12.457847 139587192067840 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.458645, loss=2.511276
I0518 03:17:12.461782 139639360366400 submission.py:119] 17000) loss = 2.511, grad_norm = 0.459
I0518 03:20:20.476774 139586705553152 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.528971, loss=2.541290
I0518 03:20:20.481891 139639360366400 submission.py:119] 17500) loss = 2.541, grad_norm = 0.529
I0518 03:21:15.380853 139639360366400 spec.py:298] Evaluating on the training split.
I0518 03:21:58.175560 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 03:22:54.689149 139639360366400 spec.py:326] Evaluating on the test split.
I0518 03:22:56.039438 139639360366400 submission_runner.py:421] Time since start: 8000.77s, 	Step: 17644, 	{'train/accuracy': 0.7098014987244898, 'train/loss': 1.305204119001116, 'validation/accuracy': 0.62548, 'validation/loss': 1.694680625, 'validation/num_examples': 50000, 'test/accuracy': 0.4798, 'test/loss': 2.4767146484375, 'test/num_examples': 10000, 'score': 6633.701737880707, 'total_duration': 8000.77228307724, 'accumulated_submission_time': 6633.701737880707, 'accumulated_eval_time': 1360.335521221161, 'accumulated_logging_time': 0.25318455696105957}
I0518 03:22:56.051944 139587192067840 logging_writer.py:48] [17644] accumulated_eval_time=1360.335521, accumulated_logging_time=0.253185, accumulated_submission_time=6633.701738, global_step=17644, preemption_count=0, score=6633.701738, test/accuracy=0.479800, test/loss=2.476715, test/num_examples=10000, total_duration=8000.772283, train/accuracy=0.709801, train/loss=1.305204, validation/accuracy=0.625480, validation/loss=1.694681, validation/num_examples=50000
I0518 03:25:09.210940 139586705553152 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.491891, loss=2.578090
I0518 03:25:09.214812 139639360366400 submission.py:119] 18000) loss = 2.578, grad_norm = 0.492
I0518 03:28:16.264813 139587192067840 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.524032, loss=2.543117
I0518 03:28:16.271261 139639360366400 submission.py:119] 18500) loss = 2.543, grad_norm = 0.524
I0518 03:31:26.129929 139639360366400 spec.py:298] Evaluating on the training split.
I0518 03:32:09.560643 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 03:33:06.760333 139639360366400 spec.py:326] Evaluating on the test split.
I0518 03:33:08.121371 139639360366400 submission_runner.py:421] Time since start: 8612.85s, 	Step: 19000, 	{'train/accuracy': 0.7402144451530612, 'train/loss': 1.0985377564722179, 'validation/accuracy': 0.65132, 'validation/loss': 1.51972328125, 'validation/num_examples': 50000, 'test/accuracy': 0.5079, 'test/loss': 2.2774552734375, 'test/num_examples': 10000, 'score': 7143.277175664902, 'total_duration': 8612.854277133942, 'accumulated_submission_time': 7143.277175664902, 'accumulated_eval_time': 1462.326945066452, 'accumulated_logging_time': 0.2739591598510742}
I0518 03:33:08.132326 139586705553152 logging_writer.py:48] [19000] accumulated_eval_time=1462.326945, accumulated_logging_time=0.273959, accumulated_submission_time=7143.277176, global_step=19000, preemption_count=0, score=7143.277176, test/accuracy=0.507900, test/loss=2.277455, test/num_examples=10000, total_duration=8612.854277, train/accuracy=0.740214, train/loss=1.098538, validation/accuracy=0.651320, validation/loss=1.519723, validation/num_examples=50000
I0518 03:33:08.581838 139587192067840 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.514427, loss=2.418253
I0518 03:33:08.585105 139639360366400 submission.py:119] 19000) loss = 2.418, grad_norm = 0.514
I0518 03:36:15.357765 139586705553152 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.457695, loss=2.453576
I0518 03:36:15.361765 139639360366400 submission.py:119] 19500) loss = 2.454, grad_norm = 0.458
I0518 03:39:23.047320 139587192067840 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.460852, loss=2.452148
I0518 03:39:23.053171 139639360366400 submission.py:119] 20000) loss = 2.452, grad_norm = 0.461
I0518 03:41:38.271631 139639360366400 spec.py:298] Evaluating on the training split.
I0518 03:42:22.007486 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 03:43:19.658185 139639360366400 spec.py:326] Evaluating on the test split.
I0518 03:43:21.020709 139639360366400 submission_runner.py:421] Time since start: 9225.75s, 	Step: 20358, 	{'train/accuracy': 0.7404536033163265, 'train/loss': 1.1267143560915578, 'validation/accuracy': 0.64696, 'validation/loss': 1.5564703125, 'validation/num_examples': 50000, 'test/accuracy': 0.5077, 'test/loss': 2.3165359375, 'test/num_examples': 10000, 'score': 7652.910516023636, 'total_duration': 9225.753448486328, 'accumulated_submission_time': 7652.910516023636, 'accumulated_eval_time': 1565.0758826732635, 'accumulated_logging_time': 0.29488468170166016}
I0518 03:43:21.031941 139586705553152 logging_writer.py:48] [20358] accumulated_eval_time=1565.075883, accumulated_logging_time=0.294885, accumulated_submission_time=7652.910516, global_step=20358, preemption_count=0, score=7652.910516, test/accuracy=0.507700, test/loss=2.316536, test/num_examples=10000, total_duration=9225.753448, train/accuracy=0.740454, train/loss=1.126714, validation/accuracy=0.646960, validation/loss=1.556470, validation/num_examples=50000
I0518 03:44:14.394618 139587192067840 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.437955, loss=2.478910
I0518 03:44:14.398267 139639360366400 submission.py:119] 20500) loss = 2.479, grad_norm = 0.438
I0518 03:47:21.521051 139586705553152 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.496077, loss=2.444947
I0518 03:47:21.524874 139639360366400 submission.py:119] 21000) loss = 2.445, grad_norm = 0.496
I0518 03:50:30.626789 139587192067840 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.432208, loss=2.378656
I0518 03:50:30.631163 139639360366400 submission.py:119] 21500) loss = 2.379, grad_norm = 0.432
I0518 03:51:51.353044 139639360366400 spec.py:298] Evaluating on the training split.
I0518 03:52:35.031528 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 03:53:19.927530 139639360366400 spec.py:326] Evaluating on the test split.
I0518 03:53:21.280048 139639360366400 submission_runner.py:421] Time since start: 9826.01s, 	Step: 21717, 	{'train/accuracy': 0.7489437181122449, 'train/loss': 1.0732035734215561, 'validation/accuracy': 0.65376, 'validation/loss': 1.50570390625, 'validation/num_examples': 50000, 'test/accuracy': 0.5102, 'test/loss': 2.2662509765625, 'test/num_examples': 10000, 'score': 8162.723639011383, 'total_duration': 9826.01291847229, 'accumulated_submission_time': 8162.723639011383, 'accumulated_eval_time': 1655.0027854442596, 'accumulated_logging_time': 0.31523609161376953}
I0518 03:53:21.292472 139586705553152 logging_writer.py:48] [21717] accumulated_eval_time=1655.002785, accumulated_logging_time=0.315236, accumulated_submission_time=8162.723639, global_step=21717, preemption_count=0, score=8162.723639, test/accuracy=0.510200, test/loss=2.266251, test/num_examples=10000, total_duration=9826.012918, train/accuracy=0.748944, train/loss=1.073204, validation/accuracy=0.653760, validation/loss=1.505704, validation/num_examples=50000
I0518 03:55:07.323612 139587192067840 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.541629, loss=2.484012
I0518 03:55:07.328865 139639360366400 submission.py:119] 22000) loss = 2.484, grad_norm = 0.542
I0518 03:58:15.019840 139586705553152 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.541333, loss=2.440521
I0518 03:58:15.025567 139639360366400 submission.py:119] 22500) loss = 2.441, grad_norm = 0.541
I0518 04:01:23.427165 139587192067840 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.395543, loss=2.398464
I0518 04:01:23.432030 139639360366400 submission.py:119] 23000) loss = 2.398, grad_norm = 0.396
I0518 04:01:51.448719 139639360366400 spec.py:298] Evaluating on the training split.
I0518 04:02:36.768532 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 04:03:28.546437 139639360366400 spec.py:326] Evaluating on the test split.
I0518 04:03:29.911749 139639360366400 submission_runner.py:421] Time since start: 10434.64s, 	Step: 23076, 	{'train/accuracy': 0.7582310267857143, 'train/loss': 1.04302978515625, 'validation/accuracy': 0.66294, 'validation/loss': 1.477304375, 'validation/num_examples': 50000, 'test/accuracy': 0.5232, 'test/loss': 2.2106513671875, 'test/num_examples': 10000, 'score': 8672.372595787048, 'total_duration': 10434.644580841064, 'accumulated_submission_time': 8672.372595787048, 'accumulated_eval_time': 1753.4656674861908, 'accumulated_logging_time': 0.3359816074371338}
I0518 04:03:29.923661 139586705553152 logging_writer.py:48] [23076] accumulated_eval_time=1753.465667, accumulated_logging_time=0.335982, accumulated_submission_time=8672.372596, global_step=23076, preemption_count=0, score=8672.372596, test/accuracy=0.523200, test/loss=2.210651, test/num_examples=10000, total_duration=10434.644581, train/accuracy=0.758231, train/loss=1.043030, validation/accuracy=0.662940, validation/loss=1.477304, validation/num_examples=50000
I0518 04:06:08.832667 139587192067840 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.446409, loss=2.358812
I0518 04:06:08.836748 139639360366400 submission.py:119] 23500) loss = 2.359, grad_norm = 0.446
I0518 04:09:18.650102 139586705553152 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.438208, loss=2.342220
I0518 04:09:18.655275 139639360366400 submission.py:119] 24000) loss = 2.342, grad_norm = 0.438
I0518 04:12:00.295632 139639360366400 spec.py:298] Evaluating on the training split.
I0518 04:12:43.163851 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 04:13:28.267320 139639360366400 spec.py:326] Evaluating on the test split.
I0518 04:13:29.631908 139639360366400 submission_runner.py:421] Time since start: 11034.36s, 	Step: 24433, 	{'train/accuracy': 0.7538265306122449, 'train/loss': 1.052773923290019, 'validation/accuracy': 0.6554, 'validation/loss': 1.5150028125, 'validation/num_examples': 50000, 'test/accuracy': 0.5096, 'test/loss': 2.27986328125, 'test/num_examples': 10000, 'score': 9182.239196538925, 'total_duration': 11034.364786863327, 'accumulated_submission_time': 9182.239196538925, 'accumulated_eval_time': 1842.8019831180573, 'accumulated_logging_time': 0.3557455539703369}
I0518 04:13:29.643231 139587192067840 logging_writer.py:48] [24433] accumulated_eval_time=1842.801983, accumulated_logging_time=0.355746, accumulated_submission_time=9182.239197, global_step=24433, preemption_count=0, score=9182.239197, test/accuracy=0.509600, test/loss=2.279863, test/num_examples=10000, total_duration=11034.364787, train/accuracy=0.753827, train/loss=1.052774, validation/accuracy=0.655400, validation/loss=1.515003, validation/num_examples=50000
I0518 04:13:55.053833 139586705553152 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.535424, loss=2.345848
I0518 04:13:55.057764 139639360366400 submission.py:119] 24500) loss = 2.346, grad_norm = 0.535
I0518 04:17:02.672892 139587192067840 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.455561, loss=2.370560
I0518 04:17:02.681595 139639360366400 submission.py:119] 25000) loss = 2.371, grad_norm = 0.456
I0518 04:20:11.030679 139586705553152 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.446269, loss=2.329656
I0518 04:20:11.034820 139639360366400 submission.py:119] 25500) loss = 2.330, grad_norm = 0.446
I0518 04:21:59.814990 139639360366400 spec.py:298] Evaluating on the training split.
I0518 04:22:44.214931 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 04:23:30.723466 139639360366400 spec.py:326] Evaluating on the test split.
I0518 04:23:32.082714 139639360366400 submission_runner.py:421] Time since start: 11636.82s, 	Step: 25792, 	{'train/accuracy': 0.7760881696428571, 'train/loss': 1.0133994744748485, 'validation/accuracy': 0.66946, 'validation/loss': 1.48030140625, 'validation/num_examples': 50000, 'test/accuracy': 0.5397, 'test/loss': 2.162750390625, 'test/num_examples': 10000, 'score': 9691.907193183899, 'total_duration': 11636.815573692322, 'accumulated_submission_time': 9691.907193183899, 'accumulated_eval_time': 1935.0696351528168, 'accumulated_logging_time': 0.37462902069091797}
I0518 04:23:32.092536 139587192067840 logging_writer.py:48] [25792] accumulated_eval_time=1935.069635, accumulated_logging_time=0.374629, accumulated_submission_time=9691.907193, global_step=25792, preemption_count=0, score=9691.907193, test/accuracy=0.539700, test/loss=2.162750, test/num_examples=10000, total_duration=11636.815574, train/accuracy=0.776088, train/loss=1.013399, validation/accuracy=0.669460, validation/loss=1.480301, validation/num_examples=50000
I0518 04:24:50.302574 139586705553152 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.454621, loss=2.295360
I0518 04:24:50.306444 139639360366400 submission.py:119] 26000) loss = 2.295, grad_norm = 0.455
I0518 04:27:59.291501 139587192067840 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.412494, loss=2.376185
I0518 04:27:59.296414 139639360366400 submission.py:119] 26500) loss = 2.376, grad_norm = 0.412
I0518 04:31:06.273263 139586705553152 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.419697, loss=2.348612
I0518 04:31:06.277284 139639360366400 submission.py:119] 27000) loss = 2.349, grad_norm = 0.420
I0518 04:32:02.426823 139639360366400 spec.py:298] Evaluating on the training split.
I0518 04:32:45.549145 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 04:33:30.553184 139639360366400 spec.py:326] Evaluating on the test split.
I0518 04:33:31.912152 139639360366400 submission_runner.py:421] Time since start: 12236.65s, 	Step: 27151, 	{'train/accuracy': 0.7716039540816326, 'train/loss': 0.9775821140834263, 'validation/accuracy': 0.66342, 'validation/loss': 1.476531875, 'validation/num_examples': 50000, 'test/accuracy': 0.5099, 'test/loss': 2.285048828125, 'test/num_examples': 10000, 'score': 10201.731195688248, 'total_duration': 12236.64501285553, 'accumulated_submission_time': 10201.731195688248, 'accumulated_eval_time': 2024.5548632144928, 'accumulated_logging_time': 0.3922746181488037}
I0518 04:33:31.922874 139587192067840 logging_writer.py:48] [27151] accumulated_eval_time=2024.554863, accumulated_logging_time=0.392275, accumulated_submission_time=10201.731196, global_step=27151, preemption_count=0, score=10201.731196, test/accuracy=0.509900, test/loss=2.285049, test/num_examples=10000, total_duration=12236.645013, train/accuracy=0.771604, train/loss=0.977582, validation/accuracy=0.663420, validation/loss=1.476532, validation/num_examples=50000
I0518 04:35:43.323410 139586705553152 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.390966, loss=2.252127
I0518 04:35:43.328130 139639360366400 submission.py:119] 27500) loss = 2.252, grad_norm = 0.391
I0518 04:38:52.533532 139639360366400 spec.py:298] Evaluating on the training split.
I0518 04:39:35.118804 139639360366400 spec.py:310] Evaluating on the validation split.
I0518 04:40:19.927623 139639360366400 spec.py:326] Evaluating on the test split.
I0518 04:40:21.286457 139639360366400 submission_runner.py:421] Time since start: 12646.02s, 	Step: 28000, 	{'train/accuracy': 0.7797951211734694, 'train/loss': 0.9638631392498406, 'validation/accuracy': 0.6721, 'validation/loss': 1.44817609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5275, 'test/loss': 2.1878125, 'test/num_examples': 10000, 'score': 10522.018436193466, 'total_duration': 12646.01926779747, 'accumulated_submission_time': 10522.018436193466, 'accumulated_eval_time': 2113.307656764984, 'accumulated_logging_time': 0.4105837345123291}
I0518 04:40:21.297844 139587192067840 logging_writer.py:48] [28000] accumulated_eval_time=2113.307657, accumulated_logging_time=0.410584, accumulated_submission_time=10522.018436, global_step=28000, preemption_count=0, score=10522.018436, test/accuracy=0.527500, test/loss=2.187813, test/num_examples=10000, total_duration=12646.019268, train/accuracy=0.779795, train/loss=0.963863, validation/accuracy=0.672100, validation/loss=1.448176, validation/num_examples=50000
I0518 04:40:21.315265 139586705553152 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10522.018436
I0518 04:40:22.010551 139639360366400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_pytorch/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_28000.
I0518 04:40:22.256473 139639360366400 submission_runner.py:584] Tuning trial 1/1
I0518 04:40:22.256659 139639360366400 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0518 04:40:22.257579 139639360366400 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011957908163265306, 'train/loss': 6.917802460339605, 'validation/accuracy': 0.00112, 'validation/loss': 6.918008125, 'validation/num_examples': 50000, 'test/accuracy': 0.0014, 'test/loss': 6.92026796875, 'test/num_examples': 10000, 'score': 7.949028968811035, 'total_duration': 136.84689927101135, 'accumulated_submission_time': 7.949028968811035, 'accumulated_eval_time': 128.8966143131256, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1348, {'train/accuracy': 0.1126235650510204, 'train/loss': 4.839840947365274, 'validation/accuracy': 0.10368, 'validation/loss': 4.9202246875, 'validation/num_examples': 50000, 'test/accuracy': 0.0695, 'test/loss': 5.323053125, 'test/num_examples': 10000, 'score': 517.5543377399445, 'total_duration': 746.4450426101685, 'accumulated_submission_time': 517.5543377399445, 'accumulated_eval_time': 228.32104563713074, 'accumulated_logging_time': 0.0238802433013916, 'global_step': 1348, 'preemption_count': 0}), (2705, {'train/accuracy': 0.2530891262755102, 'train/loss': 3.702829477738361, 'validation/accuracy': 0.22836, 'validation/loss': 3.8375884375, 'validation/num_examples': 50000, 'test/accuracy': 0.1623, 'test/loss': 4.391807421875, 'test/num_examples': 10000, 'score': 1027.3640894889832, 'total_duration': 1357.515206336975, 'accumulated_submission_time': 1027.3640894889832, 'accumulated_eval_time': 329.0693280696869, 'accumulated_logging_time': 0.04634690284729004, 'global_step': 2705, 'preemption_count': 0}), (4065, {'train/accuracy': 0.36738679846938777, 'train/loss': 3.006744384765625, 'validation/accuracy': 0.33548, 'validation/loss': 3.1731546875, 'validation/num_examples': 50000, 'test/accuracy': 0.2488, 'test/loss': 3.7751296875, 'test/num_examples': 10000, 'score': 1537.2584671974182, 'total_duration': 1954.3939156532288, 'accumulated_submission_time': 1537.2584671974182, 'accumulated_eval_time': 415.5547707080841, 'accumulated_logging_time': 0.06373000144958496, 'global_step': 4065, 'preemption_count': 0}), (5423, {'train/accuracy': 0.4384167729591837, 'train/loss': 2.5883337526905295, 'validation/accuracy': 0.39776, 'validation/loss': 2.8067809375, 'validation/num_examples': 50000, 'test/accuracy': 0.2953, 'test/loss': 3.4852171875, 'test/num_examples': 10000, 'score': 2046.9737541675568, 'total_duration': 2566.0760085582733, 'accumulated_submission_time': 2046.9737541675568, 'accumulated_eval_time': 517.0092177391052, 'accumulated_logging_time': 0.08228635787963867, 'global_step': 5423, 'preemption_count': 0}), (6781, {'train/accuracy': 0.5190130739795918, 'train/loss': 2.186302340760523, 'validation/accuracy': 0.47544, 'validation/loss': 2.4079078125, 'validation/num_examples': 50000, 'test/accuracy': 0.354, 'test/loss': 3.08278671875, 'test/num_examples': 10000, 'score': 2556.5354743003845, 'total_duration': 3165.8692643642426, 'accumulated_submission_time': 2556.5354743003845, 'accumulated_eval_time': 606.7310247421265, 'accumulated_logging_time': 0.10019302368164062, 'global_step': 6781, 'preemption_count': 0}), (8140, {'train/accuracy': 0.5419523278061225, 'train/loss': 2.0230683307258452, 'validation/accuracy': 0.49062, 'validation/loss': 2.2821115625, 'validation/num_examples': 50000, 'test/accuracy': 0.3761, 'test/loss': 3.0166126953125, 'test/num_examples': 10000, 'score': 3066.315442800522, 'total_duration': 3766.775370121002, 'accumulated_submission_time': 3066.315442800522, 'accumulated_eval_time': 697.3457424640656, 'accumulated_logging_time': 0.12054681777954102, 'global_step': 8140, 'preemption_count': 0}), (9499, {'train/accuracy': 0.5697544642857143, 'train/loss': 1.9426499970105229, 'validation/accuracy': 0.51542, 'validation/loss': 2.2234978125, 'validation/num_examples': 50000, 'test/accuracy': 0.3945, 'test/loss': 2.9274208984375, 'test/num_examples': 10000, 'score': 3575.8637721538544, 'total_duration': 4365.552325963974, 'accumulated_submission_time': 3575.8637721538544, 'accumulated_eval_time': 786.0657477378845, 'accumulated_logging_time': 0.1381826400756836, 'global_step': 9499, 'preemption_count': 0}), (10858, {'train/accuracy': 0.611328125, 'train/loss': 1.7097759636080996, 'validation/accuracy': 0.55438, 'validation/loss': 1.99458125, 'validation/num_examples': 50000, 'test/accuracy': 0.43, 'test/loss': 2.7279087890625, 'test/num_examples': 10000, 'score': 4085.571762561798, 'total_duration': 4968.574539422989, 'accumulated_submission_time': 4085.571762561798, 'accumulated_eval_time': 878.8646557331085, 'accumulated_logging_time': 0.15598678588867188, 'global_step': 10858, 'preemption_count': 0}), (12216, {'train/accuracy': 0.6395487882653061, 'train/loss': 1.6292986188616072, 'validation/accuracy': 0.5733, 'validation/loss': 1.94100625, 'validation/num_examples': 50000, 'test/accuracy': 0.4506, 'test/loss': 2.602783984375, 'test/num_examples': 10000, 'score': 4595.106580018997, 'total_duration': 5566.763742208481, 'accumulated_submission_time': 4595.106580018997, 'accumulated_eval_time': 967.0031452178955, 'accumulated_logging_time': 0.17780470848083496, 'global_step': 12216, 'preemption_count': 0}), (13575, {'train/accuracy': 0.6668526785714286, 'train/loss': 1.4395765966298628, 'validation/accuracy': 0.60108, 'validation/loss': 1.764566875, 'validation/num_examples': 50000, 'test/accuracy': 0.4557, 'test/loss': 2.5206498046875, 'test/num_examples': 10000, 'score': 5104.679742097855, 'total_duration': 6177.2078285217285, 'accumulated_submission_time': 5104.679742097855, 'accumulated_eval_time': 1067.3574419021606, 'accumulated_logging_time': 0.1956162452697754, 'global_step': 13575, 'preemption_count': 0}), (14932, {'train/accuracy': 0.670280612244898, 'train/loss': 1.4543492842693717, 'validation/accuracy': 0.60336, 'validation/loss': 1.7722246875, 'validation/num_examples': 50000, 'test/accuracy': 0.4733, 'test/loss': 2.4677029296875, 'test/num_examples': 10000, 'score': 5614.3503568172455, 'total_duration': 6779.853564500809, 'accumulated_submission_time': 5614.3503568172455, 'accumulated_eval_time': 1159.8096103668213, 'accumulated_logging_time': 0.21430301666259766, 'global_step': 14932, 'preemption_count': 0}), (16285, {'train/accuracy': 0.7034239477040817, 'train/loss': 1.3633992720623405, 'validation/accuracy': 0.62562, 'validation/loss': 1.71865015625, 'validation/num_examples': 50000, 'test/accuracy': 0.492, 'test/loss': 2.4218541015625, 'test/num_examples': 10000, 'score': 6123.900628089905, 'total_duration': 7389.791800260544, 'accumulated_submission_time': 6123.900628089905, 'accumulated_eval_time': 1259.6770253181458, 'accumulated_logging_time': 0.23334455490112305, 'global_step': 16285, 'preemption_count': 0}), (17644, {'train/accuracy': 0.7098014987244898, 'train/loss': 1.305204119001116, 'validation/accuracy': 0.62548, 'validation/loss': 1.694680625, 'validation/num_examples': 50000, 'test/accuracy': 0.4798, 'test/loss': 2.4767146484375, 'test/num_examples': 10000, 'score': 6633.701737880707, 'total_duration': 8000.77228307724, 'accumulated_submission_time': 6633.701737880707, 'accumulated_eval_time': 1360.335521221161, 'accumulated_logging_time': 0.25318455696105957, 'global_step': 17644, 'preemption_count': 0}), (19000, {'train/accuracy': 0.7402144451530612, 'train/loss': 1.0985377564722179, 'validation/accuracy': 0.65132, 'validation/loss': 1.51972328125, 'validation/num_examples': 50000, 'test/accuracy': 0.5079, 'test/loss': 2.2774552734375, 'test/num_examples': 10000, 'score': 7143.277175664902, 'total_duration': 8612.854277133942, 'accumulated_submission_time': 7143.277175664902, 'accumulated_eval_time': 1462.326945066452, 'accumulated_logging_time': 0.2739591598510742, 'global_step': 19000, 'preemption_count': 0}), (20358, {'train/accuracy': 0.7404536033163265, 'train/loss': 1.1267143560915578, 'validation/accuracy': 0.64696, 'validation/loss': 1.5564703125, 'validation/num_examples': 50000, 'test/accuracy': 0.5077, 'test/loss': 2.3165359375, 'test/num_examples': 10000, 'score': 7652.910516023636, 'total_duration': 9225.753448486328, 'accumulated_submission_time': 7652.910516023636, 'accumulated_eval_time': 1565.0758826732635, 'accumulated_logging_time': 0.29488468170166016, 'global_step': 20358, 'preemption_count': 0}), (21717, {'train/accuracy': 0.7489437181122449, 'train/loss': 1.0732035734215561, 'validation/accuracy': 0.65376, 'validation/loss': 1.50570390625, 'validation/num_examples': 50000, 'test/accuracy': 0.5102, 'test/loss': 2.2662509765625, 'test/num_examples': 10000, 'score': 8162.723639011383, 'total_duration': 9826.01291847229, 'accumulated_submission_time': 8162.723639011383, 'accumulated_eval_time': 1655.0027854442596, 'accumulated_logging_time': 0.31523609161376953, 'global_step': 21717, 'preemption_count': 0}), (23076, {'train/accuracy': 0.7582310267857143, 'train/loss': 1.04302978515625, 'validation/accuracy': 0.66294, 'validation/loss': 1.477304375, 'validation/num_examples': 50000, 'test/accuracy': 0.5232, 'test/loss': 2.2106513671875, 'test/num_examples': 10000, 'score': 8672.372595787048, 'total_duration': 10434.644580841064, 'accumulated_submission_time': 8672.372595787048, 'accumulated_eval_time': 1753.4656674861908, 'accumulated_logging_time': 0.3359816074371338, 'global_step': 23076, 'preemption_count': 0}), (24433, {'train/accuracy': 0.7538265306122449, 'train/loss': 1.052773923290019, 'validation/accuracy': 0.6554, 'validation/loss': 1.5150028125, 'validation/num_examples': 50000, 'test/accuracy': 0.5096, 'test/loss': 2.27986328125, 'test/num_examples': 10000, 'score': 9182.239196538925, 'total_duration': 11034.364786863327, 'accumulated_submission_time': 9182.239196538925, 'accumulated_eval_time': 1842.8019831180573, 'accumulated_logging_time': 0.3557455539703369, 'global_step': 24433, 'preemption_count': 0}), (25792, {'train/accuracy': 0.7760881696428571, 'train/loss': 1.0133994744748485, 'validation/accuracy': 0.66946, 'validation/loss': 1.48030140625, 'validation/num_examples': 50000, 'test/accuracy': 0.5397, 'test/loss': 2.162750390625, 'test/num_examples': 10000, 'score': 9691.907193183899, 'total_duration': 11636.815573692322, 'accumulated_submission_time': 9691.907193183899, 'accumulated_eval_time': 1935.0696351528168, 'accumulated_logging_time': 0.37462902069091797, 'global_step': 25792, 'preemption_count': 0}), (27151, {'train/accuracy': 0.7716039540816326, 'train/loss': 0.9775821140834263, 'validation/accuracy': 0.66342, 'validation/loss': 1.476531875, 'validation/num_examples': 50000, 'test/accuracy': 0.5099, 'test/loss': 2.285048828125, 'test/num_examples': 10000, 'score': 10201.731195688248, 'total_duration': 12236.64501285553, 'accumulated_submission_time': 10201.731195688248, 'accumulated_eval_time': 2024.5548632144928, 'accumulated_logging_time': 0.3922746181488037, 'global_step': 27151, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7797951211734694, 'train/loss': 0.9638631392498406, 'validation/accuracy': 0.6721, 'validation/loss': 1.44817609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5275, 'test/loss': 2.1878125, 'test/num_examples': 10000, 'score': 10522.018436193466, 'total_duration': 12646.01926779747, 'accumulated_submission_time': 10522.018436193466, 'accumulated_eval_time': 2113.307656764984, 'accumulated_logging_time': 0.4105837345123291, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0518 04:40:22.257697 139639360366400 submission_runner.py:587] Timing: 10522.018436193466
I0518 04:40:22.257750 139639360366400 submission_runner.py:588] ====================
I0518 04:40:22.257858 139639360366400 submission_runner.py:651] Final imagenet_resnet score: 10522.018436193466
