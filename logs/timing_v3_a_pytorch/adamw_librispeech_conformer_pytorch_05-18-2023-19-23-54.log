torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_conformer --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_pytorch/timing_adamw --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_pytorch_05-18-2023-19-23-54.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0518 19:24:17.770347 140643777775424 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0518 19:24:17.770383 140593595483968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0518 19:24:17.770407 140479122429760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0518 19:24:17.770420 140273820202816 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0518 19:24:17.771153 140404476753728 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0518 19:24:17.771360 139873529071424 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0518 19:24:17.772166 140377621149504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0518 19:24:17.772294 139782008821568 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0518 19:24:17.772604 140377621149504 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:24:17.772727 139782008821568 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:24:17.781130 140643777775424 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:24:17.781163 140593595483968 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:24:17.781201 140273820202816 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:24:17.781185 140479122429760 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:24:17.781980 140404476753728 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:24:17.782034 139873529071424 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 19:24:18.142949 139873529071424 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_pytorch/timing_adamw/librispeech_conformer_pytorch.
W0518 19:24:18.175622 140404476753728 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:24:18.176523 139873529071424 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:24:18.177418 140593595483968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:24:18.177612 140377621149504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:24:18.177870 140643777775424 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:24:18.178050 140273820202816 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:24:18.178470 139782008821568 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 19:24:18.178505 140479122429760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0518 19:24:18.181695 139873529071424 submission_runner.py:544] Using RNG seed 3932062448
I0518 19:24:18.183487 139873529071424 submission_runner.py:553] --- Tuning run 1/1 ---
I0518 19:24:18.183606 139873529071424 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_pytorch/timing_adamw/librispeech_conformer_pytorch/trial_1.
I0518 19:24:18.183897 139873529071424 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_pytorch/timing_adamw/librispeech_conformer_pytorch/trial_1/hparams.json.
I0518 19:24:18.184916 139873529071424 submission_runner.py:241] Initializing dataset.
I0518 19:24:18.185049 139873529071424 input_pipeline.py:20] Loading split = train-clean-100
I0518 19:24:18.433468 139873529071424 input_pipeline.py:20] Loading split = train-clean-360
I0518 19:24:18.774064 139873529071424 input_pipeline.py:20] Loading split = train-other-500
I0518 19:24:19.224448 139873529071424 submission_runner.py:248] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0518 19:24:26.191890 139873529071424 submission_runner.py:258] Initializing optimizer.
I0518 19:24:26.193216 139873529071424 submission_runner.py:265] Initializing metrics bundle.
I0518 19:24:26.193346 139873529071424 submission_runner.py:283] Initializing checkpoint and logger.
I0518 19:24:26.195066 139873529071424 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0518 19:24:26.195194 139873529071424 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0518 19:24:26.762847 139873529071424 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_pytorch/timing_adamw/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0518 19:24:26.763768 139873529071424 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_pytorch/timing_adamw/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0518 19:24:26.770675 139873529071424 submission_runner.py:319] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0518 19:24:34.790277 139847170053888 logging_writer.py:48] [0] global_step=0, grad_norm=41.435272, loss=32.780716
I0518 19:24:34.810563 139873529071424 submission.py:119] 0) loss = 32.781, grad_norm = 41.435
I0518 19:24:34.813187 139873529071424 spec.py:298] Evaluating on the training split.
I0518 19:24:34.814403 139873529071424 input_pipeline.py:20] Loading split = train-clean-100
I0518 19:24:34.849250 139873529071424 input_pipeline.py:20] Loading split = train-clean-360
I0518 19:24:35.285229 139873529071424 input_pipeline.py:20] Loading split = train-other-500
I0518 19:24:51.291966 139873529071424 spec.py:310] Evaluating on the validation split.
I0518 19:24:51.293296 139873529071424 input_pipeline.py:20] Loading split = dev-clean
I0518 19:24:51.297422 139873529071424 input_pipeline.py:20] Loading split = dev-other
I0518 19:25:02.144025 139873529071424 spec.py:326] Evaluating on the test split.
I0518 19:25:02.145442 139873529071424 input_pipeline.py:20] Loading split = test-clean
I0518 19:25:07.858325 139873529071424 submission_runner.py:421] Time since start: 41.09s, 	Step: 1, 	{'train/ctc_loss': 31.65093310345292, 'train/wer': 1.9712180942571011, 'validation/ctc_loss': 30.465419052642154, 'validation/wer': 1.8997924009076426, 'validation/num_examples': 5348, 'test/ctc_loss': 30.54029614813859, 'test/wer': 1.9449962423577682, 'test/num_examples': 2472, 'score': 8.041781663894653, 'total_duration': 41.08777189254761, 'accumulated_submission_time': 8.041781663894653, 'accumulated_eval_time': 33.04480743408203, 'accumulated_logging_time': 0}
I0518 19:25:07.884317 139832290424576 logging_writer.py:48] [1] accumulated_eval_time=33.044807, accumulated_logging_time=0, accumulated_submission_time=8.041782, global_step=1, preemption_count=0, score=8.041782, test/ctc_loss=30.540296, test/num_examples=2472, test/wer=1.944996, total_duration=41.087772, train/ctc_loss=31.650933, train/wer=1.971218, validation/ctc_loss=30.465419, validation/num_examples=5348, validation/wer=1.899792
I0518 19:25:07.930796 139873529071424 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:25:07.930891 140404476753728 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:25:07.930882 140643777775424 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:25:07.930919 139782008821568 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:25:07.930894 140479122429760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:25:07.930912 140273820202816 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:25:07.930914 140593595483968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:25:07.930942 140377621149504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 19:25:09.007212 139832282031872 logging_writer.py:48] [1] global_step=1, grad_norm=40.014763, loss=32.204742
I0518 19:25:09.011107 139873529071424 submission.py:119] 1) loss = 32.205, grad_norm = 40.015
I0518 19:25:09.877420 139832290424576 logging_writer.py:48] [2] global_step=2, grad_norm=41.461861, loss=32.687241
I0518 19:25:09.881503 139873529071424 submission.py:119] 2) loss = 32.687, grad_norm = 41.462
I0518 19:25:10.840278 139832282031872 logging_writer.py:48] [3] global_step=3, grad_norm=44.689987, loss=32.761032
I0518 19:25:10.844096 139873529071424 submission.py:119] 3) loss = 32.761, grad_norm = 44.690
I0518 19:25:11.634838 139832290424576 logging_writer.py:48] [4] global_step=4, grad_norm=42.627892, loss=32.207794
I0518 19:25:11.638748 139873529071424 submission.py:119] 4) loss = 32.208, grad_norm = 42.628
I0518 19:25:12.430541 139832282031872 logging_writer.py:48] [5] global_step=5, grad_norm=48.471199, loss=32.315582
I0518 19:25:12.434046 139873529071424 submission.py:119] 5) loss = 32.316, grad_norm = 48.471
I0518 19:25:13.227743 139832290424576 logging_writer.py:48] [6] global_step=6, grad_norm=53.683735, loss=32.373642
I0518 19:25:13.231134 139873529071424 submission.py:119] 6) loss = 32.374, grad_norm = 53.684
I0518 19:25:14.025182 139832282031872 logging_writer.py:48] [7] global_step=7, grad_norm=57.345585, loss=31.202715
I0518 19:25:14.028404 139873529071424 submission.py:119] 7) loss = 31.203, grad_norm = 57.346
I0518 19:25:14.821437 139832290424576 logging_writer.py:48] [8] global_step=8, grad_norm=69.105133, loss=31.436041
I0518 19:25:14.824797 139873529071424 submission.py:119] 8) loss = 31.436, grad_norm = 69.105
I0518 19:25:15.613533 139832282031872 logging_writer.py:48] [9] global_step=9, grad_norm=78.376534, loss=30.805586
I0518 19:25:15.616695 139873529071424 submission.py:119] 9) loss = 30.806, grad_norm = 78.377
I0518 19:25:16.408423 139832290424576 logging_writer.py:48] [10] global_step=10, grad_norm=90.698830, loss=30.287939
I0518 19:25:16.411939 139873529071424 submission.py:119] 10) loss = 30.288, grad_norm = 90.699
I0518 19:25:17.203604 139832282031872 logging_writer.py:48] [11] global_step=11, grad_norm=104.560776, loss=29.891066
I0518 19:25:17.206977 139873529071424 submission.py:119] 11) loss = 29.891, grad_norm = 104.561
I0518 19:25:17.996306 139832290424576 logging_writer.py:48] [12] global_step=12, grad_norm=117.961891, loss=29.237852
I0518 19:25:17.999914 139873529071424 submission.py:119] 12) loss = 29.238, grad_norm = 117.962
I0518 19:25:18.793354 139832282031872 logging_writer.py:48] [13] global_step=13, grad_norm=134.340668, loss=27.891527
I0518 19:25:18.797058 139873529071424 submission.py:119] 13) loss = 27.892, grad_norm = 134.341
I0518 19:25:19.590453 139832290424576 logging_writer.py:48] [14] global_step=14, grad_norm=146.846863, loss=26.510824
I0518 19:25:19.593679 139873529071424 submission.py:119] 14) loss = 26.511, grad_norm = 146.847
I0518 19:25:20.386432 139832282031872 logging_writer.py:48] [15] global_step=15, grad_norm=155.045120, loss=24.289370
I0518 19:25:20.389560 139873529071424 submission.py:119] 15) loss = 24.289, grad_norm = 155.045
I0518 19:25:21.181372 139832290424576 logging_writer.py:48] [16] global_step=16, grad_norm=168.073074, loss=22.895193
I0518 19:25:21.184670 139873529071424 submission.py:119] 16) loss = 22.895, grad_norm = 168.073
I0518 19:25:21.975934 139832282031872 logging_writer.py:48] [17] global_step=17, grad_norm=165.419800, loss=20.681690
I0518 19:25:21.979179 139873529071424 submission.py:119] 17) loss = 20.682, grad_norm = 165.420
I0518 19:25:22.773335 139832290424576 logging_writer.py:48] [18] global_step=18, grad_norm=159.485550, loss=18.424339
I0518 19:25:22.776856 139873529071424 submission.py:119] 18) loss = 18.424, grad_norm = 159.486
I0518 19:25:23.568126 139832282031872 logging_writer.py:48] [19] global_step=19, grad_norm=148.210876, loss=16.021376
I0518 19:25:23.572205 139873529071424 submission.py:119] 19) loss = 16.021, grad_norm = 148.211
I0518 19:25:24.361701 139832290424576 logging_writer.py:48] [20] global_step=20, grad_norm=133.439697, loss=13.806611
I0518 19:25:24.365239 139873529071424 submission.py:119] 20) loss = 13.807, grad_norm = 133.440
I0518 19:25:25.158190 139832282031872 logging_writer.py:48] [21] global_step=21, grad_norm=117.253029, loss=11.958076
I0518 19:25:25.161510 139873529071424 submission.py:119] 21) loss = 11.958, grad_norm = 117.253
I0518 19:25:25.954267 139832290424576 logging_writer.py:48] [22] global_step=22, grad_norm=98.814766, loss=10.453268
I0518 19:25:25.957360 139873529071424 submission.py:119] 22) loss = 10.453, grad_norm = 98.815
I0518 19:25:26.750141 139832282031872 logging_writer.py:48] [23] global_step=23, grad_norm=73.085449, loss=9.083018
I0518 19:25:26.753428 139873529071424 submission.py:119] 23) loss = 9.083, grad_norm = 73.085
I0518 19:25:27.547452 139832290424576 logging_writer.py:48] [24] global_step=24, grad_norm=52.470787, loss=8.235564
I0518 19:25:27.551557 139873529071424 submission.py:119] 24) loss = 8.236, grad_norm = 52.471
I0518 19:25:28.345888 139832282031872 logging_writer.py:48] [25] global_step=25, grad_norm=34.225224, loss=7.709776
I0518 19:25:28.349750 139873529071424 submission.py:119] 25) loss = 7.710, grad_norm = 34.225
I0518 19:25:29.144768 139832290424576 logging_writer.py:48] [26] global_step=26, grad_norm=19.731915, loss=7.392333
I0518 19:25:29.148556 139873529071424 submission.py:119] 26) loss = 7.392, grad_norm = 19.732
I0518 19:25:29.943447 139832282031872 logging_writer.py:48] [27] global_step=27, grad_norm=9.466609, loss=7.257325
I0518 19:25:29.947494 139873529071424 submission.py:119] 27) loss = 7.257, grad_norm = 9.467
I0518 19:25:30.739474 139832290424576 logging_writer.py:48] [28] global_step=28, grad_norm=3.343790, loss=7.213039
I0518 19:25:30.743344 139873529071424 submission.py:119] 28) loss = 7.213, grad_norm = 3.344
I0518 19:25:31.536133 139832282031872 logging_writer.py:48] [29] global_step=29, grad_norm=5.079801, loss=7.220284
I0518 19:25:31.540337 139873529071424 submission.py:119] 29) loss = 7.220, grad_norm = 5.080
I0518 19:25:32.335540 139832290424576 logging_writer.py:48] [30] global_step=30, grad_norm=7.941815, loss=7.235936
I0518 19:25:32.339470 139873529071424 submission.py:119] 30) loss = 7.236, grad_norm = 7.942
I0518 19:25:33.134454 139832282031872 logging_writer.py:48] [31] global_step=31, grad_norm=9.853397, loss=7.265290
I0518 19:25:33.138070 139873529071424 submission.py:119] 31) loss = 7.265, grad_norm = 9.853
I0518 19:25:33.933433 139832290424576 logging_writer.py:48] [32] global_step=32, grad_norm=11.365509, loss=7.280397
I0518 19:25:33.937613 139873529071424 submission.py:119] 32) loss = 7.280, grad_norm = 11.366
I0518 19:25:34.731922 139832282031872 logging_writer.py:48] [33] global_step=33, grad_norm=11.768879, loss=7.292219
I0518 19:25:34.735660 139873529071424 submission.py:119] 33) loss = 7.292, grad_norm = 11.769
I0518 19:25:35.530878 139832290424576 logging_writer.py:48] [34] global_step=34, grad_norm=12.127273, loss=7.288898
I0518 19:25:35.534727 139873529071424 submission.py:119] 34) loss = 7.289, grad_norm = 12.127
I0518 19:25:36.331600 139832282031872 logging_writer.py:48] [35] global_step=35, grad_norm=11.941694, loss=7.294588
I0518 19:25:36.335329 139873529071424 submission.py:119] 35) loss = 7.295, grad_norm = 11.942
I0518 19:25:37.129199 139832290424576 logging_writer.py:48] [36] global_step=36, grad_norm=11.725698, loss=7.270137
I0518 19:25:37.133104 139873529071424 submission.py:119] 36) loss = 7.270, grad_norm = 11.726
I0518 19:25:37.928271 139832282031872 logging_writer.py:48] [37] global_step=37, grad_norm=11.248401, loss=7.240320
I0518 19:25:37.931799 139873529071424 submission.py:119] 37) loss = 7.240, grad_norm = 11.248
I0518 19:25:38.727319 139832290424576 logging_writer.py:48] [38] global_step=38, grad_norm=9.908567, loss=7.227520
I0518 19:25:38.731021 139873529071424 submission.py:119] 38) loss = 7.228, grad_norm = 9.909
I0518 19:25:39.528612 139832282031872 logging_writer.py:48] [39] global_step=39, grad_norm=9.077999, loss=7.183857
I0518 19:25:39.532532 139873529071424 submission.py:119] 39) loss = 7.184, grad_norm = 9.078
I0518 19:25:40.325249 139832290424576 logging_writer.py:48] [40] global_step=40, grad_norm=7.748922, loss=7.159090
I0518 19:25:40.328988 139873529071424 submission.py:119] 40) loss = 7.159, grad_norm = 7.749
I0518 19:25:41.122919 139832282031872 logging_writer.py:48] [41] global_step=41, grad_norm=6.800091, loss=7.129163
I0518 19:25:41.126687 139873529071424 submission.py:119] 41) loss = 7.129, grad_norm = 6.800
I0518 19:25:41.924282 139832290424576 logging_writer.py:48] [42] global_step=42, grad_norm=5.262233, loss=7.101929
I0518 19:25:41.927944 139873529071424 submission.py:119] 42) loss = 7.102, grad_norm = 5.262
I0518 19:25:42.723959 139832282031872 logging_writer.py:48] [43] global_step=43, grad_norm=4.342996, loss=7.082548
I0518 19:25:42.727605 139873529071424 submission.py:119] 43) loss = 7.083, grad_norm = 4.343
I0518 19:25:43.522064 139832290424576 logging_writer.py:48] [44] global_step=44, grad_norm=3.322618, loss=7.069724
I0518 19:25:43.525916 139873529071424 submission.py:119] 44) loss = 7.070, grad_norm = 3.323
I0518 19:25:44.321534 139832282031872 logging_writer.py:48] [45] global_step=45, grad_norm=2.903925, loss=7.060841
I0518 19:25:44.325149 139873529071424 submission.py:119] 45) loss = 7.061, grad_norm = 2.904
I0518 19:25:45.120947 139832290424576 logging_writer.py:48] [46] global_step=46, grad_norm=2.869616, loss=7.040462
I0518 19:25:45.124641 139873529071424 submission.py:119] 46) loss = 7.040, grad_norm = 2.870
I0518 19:25:45.924283 139832282031872 logging_writer.py:48] [47] global_step=47, grad_norm=3.394282, loss=7.040454
I0518 19:25:45.928302 139873529071424 submission.py:119] 47) loss = 7.040, grad_norm = 3.394
I0518 19:25:46.723478 139832290424576 logging_writer.py:48] [48] global_step=48, grad_norm=4.214168, loss=7.033709
I0518 19:25:46.727104 139873529071424 submission.py:119] 48) loss = 7.034, grad_norm = 4.214
I0518 19:25:47.521292 139832282031872 logging_writer.py:48] [49] global_step=49, grad_norm=4.455783, loss=7.030035
I0518 19:25:47.525007 139873529071424 submission.py:119] 49) loss = 7.030, grad_norm = 4.456
I0518 19:25:48.318932 139832290424576 logging_writer.py:48] [50] global_step=50, grad_norm=3.840207, loss=7.013589
I0518 19:25:48.322493 139873529071424 submission.py:119] 50) loss = 7.014, grad_norm = 3.840
I0518 19:25:49.118748 139832282031872 logging_writer.py:48] [51] global_step=51, grad_norm=3.857617, loss=7.008387
I0518 19:25:49.122393 139873529071424 submission.py:119] 51) loss = 7.008, grad_norm = 3.858
I0518 19:25:49.916120 139832290424576 logging_writer.py:48] [52] global_step=52, grad_norm=3.380441, loss=6.990688
I0518 19:25:49.919740 139873529071424 submission.py:119] 52) loss = 6.991, grad_norm = 3.380
I0518 19:25:50.713272 139832282031872 logging_writer.py:48] [53] global_step=53, grad_norm=3.429928, loss=6.973581
I0518 19:25:50.716906 139873529071424 submission.py:119] 53) loss = 6.974, grad_norm = 3.430
I0518 19:25:51.512293 139832290424576 logging_writer.py:48] [54] global_step=54, grad_norm=3.721117, loss=6.972273
I0518 19:25:51.516003 139873529071424 submission.py:119] 54) loss = 6.972, grad_norm = 3.721
I0518 19:25:52.312839 139832282031872 logging_writer.py:48] [55] global_step=55, grad_norm=3.631247, loss=6.960793
I0518 19:25:52.316485 139873529071424 submission.py:119] 55) loss = 6.961, grad_norm = 3.631
I0518 19:25:53.114649 139832290424576 logging_writer.py:48] [56] global_step=56, grad_norm=3.031347, loss=6.938316
I0518 19:25:53.118253 139873529071424 submission.py:119] 56) loss = 6.938, grad_norm = 3.031
I0518 19:25:53.912863 139832282031872 logging_writer.py:48] [57] global_step=57, grad_norm=2.998195, loss=6.928562
I0518 19:25:53.916919 139873529071424 submission.py:119] 57) loss = 6.929, grad_norm = 2.998
I0518 19:25:54.710801 139832290424576 logging_writer.py:48] [58] global_step=58, grad_norm=2.727814, loss=6.912570
I0518 19:25:54.714695 139873529071424 submission.py:119] 58) loss = 6.913, grad_norm = 2.728
I0518 19:25:55.512008 139832282031872 logging_writer.py:48] [59] global_step=59, grad_norm=2.690727, loss=6.905497
I0518 19:25:55.515864 139873529071424 submission.py:119] 59) loss = 6.905, grad_norm = 2.691
I0518 19:25:56.313766 139832290424576 logging_writer.py:48] [60] global_step=60, grad_norm=2.637340, loss=6.889434
I0518 19:25:56.317690 139873529071424 submission.py:119] 60) loss = 6.889, grad_norm = 2.637
I0518 19:25:57.115960 139832282031872 logging_writer.py:48] [61] global_step=61, grad_norm=2.622869, loss=6.885066
I0518 19:25:57.119694 139873529071424 submission.py:119] 61) loss = 6.885, grad_norm = 2.623
I0518 19:25:57.914126 139832290424576 logging_writer.py:48] [62] global_step=62, grad_norm=2.636417, loss=6.875162
I0518 19:25:57.918369 139873529071424 submission.py:119] 62) loss = 6.875, grad_norm = 2.636
I0518 19:25:58.714090 139832282031872 logging_writer.py:48] [63] global_step=63, grad_norm=2.744971, loss=6.857920
I0518 19:25:58.717705 139873529071424 submission.py:119] 63) loss = 6.858, grad_norm = 2.745
I0518 19:25:59.517292 139832290424576 logging_writer.py:48] [64] global_step=64, grad_norm=2.663615, loss=6.835492
I0518 19:25:59.520921 139873529071424 submission.py:119] 64) loss = 6.835, grad_norm = 2.664
I0518 19:26:00.319082 139832282031872 logging_writer.py:48] [65] global_step=65, grad_norm=2.571964, loss=6.826590
I0518 19:26:00.322961 139873529071424 submission.py:119] 65) loss = 6.827, grad_norm = 2.572
I0518 19:26:01.118304 139832290424576 logging_writer.py:48] [66] global_step=66, grad_norm=2.574021, loss=6.809686
I0518 19:26:01.122003 139873529071424 submission.py:119] 66) loss = 6.810, grad_norm = 2.574
I0518 19:26:01.919469 139832282031872 logging_writer.py:48] [67] global_step=67, grad_norm=2.555832, loss=6.781079
I0518 19:26:01.923291 139873529071424 submission.py:119] 67) loss = 6.781, grad_norm = 2.556
I0518 19:26:02.719059 139832290424576 logging_writer.py:48] [68] global_step=68, grad_norm=2.514826, loss=6.784499
I0518 19:26:02.722545 139873529071424 submission.py:119] 68) loss = 6.784, grad_norm = 2.515
I0518 19:26:03.518245 139832282031872 logging_writer.py:48] [69] global_step=69, grad_norm=3.130512, loss=6.775722
I0518 19:26:03.521752 139873529071424 submission.py:119] 69) loss = 6.776, grad_norm = 3.131
I0518 19:26:04.319588 139832290424576 logging_writer.py:48] [70] global_step=70, grad_norm=2.815838, loss=6.745529
I0518 19:26:04.323169 139873529071424 submission.py:119] 70) loss = 6.746, grad_norm = 2.816
I0518 19:26:05.120980 139832282031872 logging_writer.py:48] [71] global_step=71, grad_norm=2.963959, loss=6.745132
I0518 19:26:05.124625 139873529071424 submission.py:119] 71) loss = 6.745, grad_norm = 2.964
I0518 19:26:05.923172 139832290424576 logging_writer.py:48] [72] global_step=72, grad_norm=2.567612, loss=6.721804
I0518 19:26:05.926986 139873529071424 submission.py:119] 72) loss = 6.722, grad_norm = 2.568
I0518 19:26:06.723489 139832282031872 logging_writer.py:48] [73] global_step=73, grad_norm=2.751533, loss=6.713984
I0518 19:26:06.727409 139873529071424 submission.py:119] 73) loss = 6.714, grad_norm = 2.752
I0518 19:26:07.526582 139832290424576 logging_writer.py:48] [74] global_step=74, grad_norm=2.443781, loss=6.698286
I0518 19:26:07.530249 139873529071424 submission.py:119] 74) loss = 6.698, grad_norm = 2.444
I0518 19:26:08.327666 139832282031872 logging_writer.py:48] [75] global_step=75, grad_norm=2.629756, loss=6.691642
I0518 19:26:08.331573 139873529071424 submission.py:119] 75) loss = 6.692, grad_norm = 2.630
I0518 19:26:09.126968 139832290424576 logging_writer.py:48] [76] global_step=76, grad_norm=2.377402, loss=6.657705
I0518 19:26:09.130766 139873529071424 submission.py:119] 76) loss = 6.658, grad_norm = 2.377
I0518 19:26:09.928128 139832282031872 logging_writer.py:48] [77] global_step=77, grad_norm=2.283762, loss=6.659328
I0518 19:26:09.931653 139873529071424 submission.py:119] 77) loss = 6.659, grad_norm = 2.284
I0518 19:26:10.731481 139832290424576 logging_writer.py:48] [78] global_step=78, grad_norm=2.254418, loss=6.655470
I0518 19:26:10.735556 139873529071424 submission.py:119] 78) loss = 6.655, grad_norm = 2.254
I0518 19:26:11.533595 139832282031872 logging_writer.py:48] [79] global_step=79, grad_norm=2.242783, loss=6.642344
I0518 19:26:11.537286 139873529071424 submission.py:119] 79) loss = 6.642, grad_norm = 2.243
I0518 19:26:12.333581 139832290424576 logging_writer.py:48] [80] global_step=80, grad_norm=2.490361, loss=6.631182
I0518 19:26:12.337222 139873529071424 submission.py:119] 80) loss = 6.631, grad_norm = 2.490
I0518 19:26:13.134550 139832282031872 logging_writer.py:48] [81] global_step=81, grad_norm=2.457007, loss=6.608063
I0518 19:26:13.138250 139873529071424 submission.py:119] 81) loss = 6.608, grad_norm = 2.457
I0518 19:26:13.932213 139832290424576 logging_writer.py:48] [82] global_step=82, grad_norm=2.292769, loss=6.587707
I0518 19:26:13.935768 139873529071424 submission.py:119] 82) loss = 6.588, grad_norm = 2.293
I0518 19:26:14.730844 139832282031872 logging_writer.py:48] [83] global_step=83, grad_norm=2.407776, loss=6.586347
I0518 19:26:14.734466 139873529071424 submission.py:119] 83) loss = 6.586, grad_norm = 2.408
I0518 19:26:15.532439 139832290424576 logging_writer.py:48] [84] global_step=84, grad_norm=2.096411, loss=6.562331
I0518 19:26:15.536353 139873529071424 submission.py:119] 84) loss = 6.562, grad_norm = 2.096
I0518 19:26:16.332954 139832282031872 logging_writer.py:48] [85] global_step=85, grad_norm=2.286127, loss=6.544514
I0518 19:26:16.336971 139873529071424 submission.py:119] 85) loss = 6.545, grad_norm = 2.286
I0518 19:26:17.132757 139832290424576 logging_writer.py:48] [86] global_step=86, grad_norm=1.973254, loss=6.525579
I0518 19:26:17.136616 139873529071424 submission.py:119] 86) loss = 6.526, grad_norm = 1.973
I0518 19:26:17.931006 139832282031872 logging_writer.py:48] [87] global_step=87, grad_norm=1.960816, loss=6.514694
I0518 19:26:17.935223 139873529071424 submission.py:119] 87) loss = 6.515, grad_norm = 1.961
I0518 19:26:18.730698 139832290424576 logging_writer.py:48] [88] global_step=88, grad_norm=2.159735, loss=6.514533
I0518 19:26:18.734723 139873529071424 submission.py:119] 88) loss = 6.515, grad_norm = 2.160
I0518 19:26:19.534410 139832282031872 logging_writer.py:48] [89] global_step=89, grad_norm=2.328049, loss=6.511110
I0518 19:26:19.538156 139873529071424 submission.py:119] 89) loss = 6.511, grad_norm = 2.328
I0518 19:26:20.335895 139832290424576 logging_writer.py:48] [90] global_step=90, grad_norm=1.924684, loss=6.475245
I0518 19:26:20.339695 139873529071424 submission.py:119] 90) loss = 6.475, grad_norm = 1.925
I0518 19:26:21.136285 139832282031872 logging_writer.py:48] [91] global_step=91, grad_norm=2.328227, loss=6.477947
I0518 19:26:21.139980 139873529071424 submission.py:119] 91) loss = 6.478, grad_norm = 2.328
I0518 19:26:21.936434 139832290424576 logging_writer.py:48] [92] global_step=92, grad_norm=1.989991, loss=6.457272
I0518 19:26:21.940164 139873529071424 submission.py:119] 92) loss = 6.457, grad_norm = 1.990
I0518 19:26:22.737574 139832282031872 logging_writer.py:48] [93] global_step=93, grad_norm=1.883968, loss=6.449022
I0518 19:26:22.741197 139873529071424 submission.py:119] 93) loss = 6.449, grad_norm = 1.884
I0518 19:26:23.539231 139832290424576 logging_writer.py:48] [94] global_step=94, grad_norm=1.714493, loss=6.419188
I0518 19:26:23.542842 139873529071424 submission.py:119] 94) loss = 6.419, grad_norm = 1.714
I0518 19:26:24.339629 139832282031872 logging_writer.py:48] [95] global_step=95, grad_norm=1.725515, loss=6.420915
I0518 19:26:24.343371 139873529071424 submission.py:119] 95) loss = 6.421, grad_norm = 1.726
I0518 19:26:25.140783 139832290424576 logging_writer.py:48] [96] global_step=96, grad_norm=1.873811, loss=6.420430
I0518 19:26:25.144442 139873529071424 submission.py:119] 96) loss = 6.420, grad_norm = 1.874
I0518 19:26:25.944966 139832282031872 logging_writer.py:48] [97] global_step=97, grad_norm=1.715300, loss=6.394763
I0518 19:26:25.948649 139873529071424 submission.py:119] 97) loss = 6.395, grad_norm = 1.715
I0518 19:26:26.747524 139832290424576 logging_writer.py:48] [98] global_step=98, grad_norm=1.922304, loss=6.380426
I0518 19:26:26.751347 139873529071424 submission.py:119] 98) loss = 6.380, grad_norm = 1.922
I0518 19:26:27.547454 139832282031872 logging_writer.py:48] [99] global_step=99, grad_norm=1.878156, loss=6.379622
I0518 19:26:27.551225 139873529071424 submission.py:119] 99) loss = 6.380, grad_norm = 1.878
I0518 19:26:28.346024 139832290424576 logging_writer.py:48] [100] global_step=100, grad_norm=1.660124, loss=6.369855
I0518 19:26:28.349892 139873529071424 submission.py:119] 100) loss = 6.370, grad_norm = 1.660
I0518 19:31:43.789112 139832282031872 logging_writer.py:48] [500] global_step=500, grad_norm=0.505789, loss=5.791275
I0518 19:31:43.792750 139873529071424 submission.py:119] 500) loss = 5.791, grad_norm = 0.506
I0518 19:38:18.617080 139832290424576 logging_writer.py:48] [1000] global_step=1000, grad_norm=4.129536, loss=5.523479
I0518 19:38:18.621403 139873529071424 submission.py:119] 1000) loss = 5.523, grad_norm = 4.130
I0518 19:44:54.404510 139839001327360 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.535712, loss=3.604387
I0518 19:44:54.411945 139873529071424 submission.py:119] 1500) loss = 3.604, grad_norm = 1.536
I0518 19:51:28.236854 139838992934656 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.957104, loss=2.919003
I0518 19:51:28.242267 139873529071424 submission.py:119] 2000) loss = 2.919, grad_norm = 0.957
I0518 19:58:03.138550 139839001327360 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.284224, loss=2.731105
I0518 19:58:03.147074 139873529071424 submission.py:119] 2500) loss = 2.731, grad_norm = 1.284
I0518 20:04:36.373000 139838992934656 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.018664, loss=2.459142
I0518 20:04:36.377853 139873529071424 submission.py:119] 3000) loss = 2.459, grad_norm = 1.019
I0518 20:05:08.606703 139873529071424 spec.py:298] Evaluating on the training split.
I0518 20:05:20.090510 139873529071424 spec.py:310] Evaluating on the validation split.
I0518 20:05:30.150888 139873529071424 spec.py:326] Evaluating on the test split.
I0518 20:05:35.375580 139873529071424 submission_runner.py:421] Time since start: 2468.61s, 	Step: 3042, 	{'train/ctc_loss': 2.907006399722527, 'train/wer': 0.6160902555695171, 'validation/ctc_loss': 3.115166484579064, 'validation/wer': 0.6289576594409308, 'validation/num_examples': 5348, 'test/ctc_loss': 2.7629439802567908, 'test/wer': 0.5666118254016614, 'test/num_examples': 2472, 'score': 2403.651233434677, 'total_duration': 2468.605101108551, 'accumulated_submission_time': 2403.651233434677, 'accumulated_eval_time': 59.81337094306946, 'accumulated_logging_time': 0.035954952239990234}
I0518 20:05:35.396876 139838992934656 logging_writer.py:48] [3042] accumulated_eval_time=59.813371, accumulated_logging_time=0.035955, accumulated_submission_time=2403.651233, global_step=3042, preemption_count=0, score=2403.651233, test/ctc_loss=2.762944, test/num_examples=2472, test/wer=0.566612, total_duration=2468.605101, train/ctc_loss=2.907006, train/wer=0.616090, validation/ctc_loss=3.115166, validation/num_examples=5348, validation/wer=0.628958
I0518 20:11:37.503253 139838992934656 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.794557, loss=2.276966
I0518 20:11:37.510238 139873529071424 submission.py:119] 3500) loss = 2.277, grad_norm = 0.795
I0518 20:18:10.210607 139838984541952 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.111467, loss=2.125075
I0518 20:18:10.215634 139873529071424 submission.py:119] 4000) loss = 2.125, grad_norm = 1.111
I0518 20:24:44.341826 139838992934656 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.860627, loss=2.006951
I0518 20:24:44.348563 139873529071424 submission.py:119] 4500) loss = 2.007, grad_norm = 0.861
I0518 20:31:17.008853 139838984541952 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.680519, loss=1.883090
I0518 20:31:17.013567 139873529071424 submission.py:119] 5000) loss = 1.883, grad_norm = 0.681
I0518 20:37:50.857615 139838992934656 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.607096, loss=1.932233
I0518 20:37:50.864622 139873529071424 submission.py:119] 5500) loss = 1.932, grad_norm = 0.607
I0518 20:44:23.164076 139838984541952 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.571086, loss=1.848507
I0518 20:44:23.169032 139873529071424 submission.py:119] 6000) loss = 1.849, grad_norm = 0.571
I0518 20:45:36.161323 139873529071424 spec.py:298] Evaluating on the training split.
I0518 20:45:48.362900 139873529071424 spec.py:310] Evaluating on the validation split.
I0518 20:45:58.607716 139873529071424 spec.py:326] Evaluating on the test split.
I0518 20:46:04.218080 139873529071424 submission_runner.py:421] Time since start: 4897.45s, 	Step: 6094, 	{'train/ctc_loss': 0.6632329121852333, 'train/wer': 0.22279147274698133, 'validation/ctc_loss': 0.8867883082491963, 'validation/wer': 0.2650992130546034, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6095865055810052, 'test/wer': 0.2036032742266366, 'test/num_examples': 2472, 'score': 4799.40868973732, 'total_duration': 4897.447292327881, 'accumulated_submission_time': 4799.40868973732, 'accumulated_eval_time': 87.86950182914734, 'accumulated_logging_time': 0.06655120849609375}
I0518 20:46:04.239317 139838984541952 logging_writer.py:48] [6094] accumulated_eval_time=87.869502, accumulated_logging_time=0.066551, accumulated_submission_time=4799.408690, global_step=6094, preemption_count=0, score=4799.408690, test/ctc_loss=0.609587, test/num_examples=2472, test/wer=0.203603, total_duration=4897.447292, train/ctc_loss=0.663233, train/wer=0.222791, validation/ctc_loss=0.886788, validation/num_examples=5348, validation/wer=0.265099
I0518 20:51:25.230051 139838984541952 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.533158, loss=1.759608
I0518 20:51:25.237456 139873529071424 submission.py:119] 6500) loss = 1.760, grad_norm = 0.533
I0518 20:57:57.414922 139838976149248 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.618510, loss=1.740623
I0518 20:57:57.420298 139873529071424 submission.py:119] 7000) loss = 1.741, grad_norm = 0.619
I0518 21:04:31.298724 139838984541952 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.524263, loss=1.718062
I0518 21:04:31.305120 139873529071424 submission.py:119] 7500) loss = 1.718, grad_norm = 0.524
I0518 21:11:03.442529 139838976149248 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.453699, loss=1.629918
I0518 21:11:03.447878 139873529071424 submission.py:119] 8000) loss = 1.630, grad_norm = 0.454
I0518 21:17:37.098155 139838984541952 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.404486, loss=1.605707
I0518 21:17:37.105580 139873529071424 submission.py:119] 8500) loss = 1.606, grad_norm = 0.404
I0518 21:24:09.224027 139838976149248 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.593129, loss=1.621635
I0518 21:24:09.228767 139873529071424 submission.py:119] 9000) loss = 1.622, grad_norm = 0.593
I0518 21:26:04.542213 139873529071424 spec.py:298] Evaluating on the training split.
I0518 21:26:16.768810 139873529071424 spec.py:310] Evaluating on the validation split.
I0518 21:26:26.919345 139873529071424 spec.py:326] Evaluating on the test split.
I0518 21:26:32.351982 139873529071424 submission_runner.py:421] Time since start: 7325.58s, 	Step: 9148, 	{'train/ctc_loss': 0.47320905423516496, 'train/wer': 0.1663493844004983, 'validation/ctc_loss': 0.7079987866058871, 'validation/wer': 0.21375947472601747, 'validation/num_examples': 5348, 'test/ctc_loss': 0.44748648098264404, 'test/wer': 0.15195092722361017, 'test/num_examples': 2472, 'score': 7194.719589233398, 'total_duration': 7325.581441402435, 'accumulated_submission_time': 7194.719589233398, 'accumulated_eval_time': 115.67890501022339, 'accumulated_logging_time': 0.0969688892364502}
I0518 21:26:32.370860 139838984541952 logging_writer.py:48] [9148] accumulated_eval_time=115.678905, accumulated_logging_time=0.096969, accumulated_submission_time=7194.719589, global_step=9148, preemption_count=0, score=7194.719589, test/ctc_loss=0.447486, test/num_examples=2472, test/wer=0.151951, total_duration=7325.581441, train/ctc_loss=0.473209, train/wer=0.166349, validation/ctc_loss=0.707999, validation/num_examples=5348, validation/wer=0.213759
I0518 21:31:10.564321 139838984541952 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.364233, loss=1.606984
I0518 21:31:10.572518 139873529071424 submission.py:119] 9500) loss = 1.607, grad_norm = 0.364
I0518 21:37:42.603310 139838976149248 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.473691, loss=1.586310
I0518 21:37:42.608039 139873529071424 submission.py:119] 10000) loss = 1.586, grad_norm = 0.474
I0518 21:44:16.263723 139838984541952 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.375826, loss=1.557787
I0518 21:44:16.270932 139873529071424 submission.py:119] 10500) loss = 1.558, grad_norm = 0.376
I0518 21:50:48.317580 139838976149248 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.410692, loss=1.503211
I0518 21:50:48.326672 139873529071424 submission.py:119] 11000) loss = 1.503, grad_norm = 0.411
I0518 21:57:21.954675 139838984541952 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.394174, loss=1.494558
I0518 21:57:21.961524 139873529071424 submission.py:119] 11500) loss = 1.495, grad_norm = 0.394
I0518 22:03:54.006495 139838976149248 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.485358, loss=1.503810
I0518 22:03:54.011394 139873529071424 submission.py:119] 12000) loss = 1.504, grad_norm = 0.485
I0518 22:06:33.136947 139873529071424 spec.py:298] Evaluating on the training split.
I0518 22:06:45.244634 139873529071424 spec.py:310] Evaluating on the validation split.
I0518 22:06:55.365520 139873529071424 spec.py:326] Evaluating on the test split.
I0518 22:07:00.915407 139873529071424 submission_runner.py:421] Time since start: 9754.14s, 	Step: 12204, 	{'train/ctc_loss': 0.39378184813244616, 'train/wer': 0.1416121534387825, 'validation/ctc_loss': 0.6310629481552642, 'validation/wer': 0.19162844590353884, 'validation/num_examples': 5348, 'test/ctc_loss': 0.396121322343377, 'test/wer': 0.13553917088131945, 'test/num_examples': 2472, 'score': 9590.48723435402, 'total_duration': 9754.144926309586, 'accumulated_submission_time': 9590.48723435402, 'accumulated_eval_time': 143.4570655822754, 'accumulated_logging_time': 0.12621736526489258}
I0518 22:07:00.935673 139838976149248 logging_writer.py:48] [12204] accumulated_eval_time=143.457066, accumulated_logging_time=0.126217, accumulated_submission_time=9590.487234, global_step=12204, preemption_count=0, score=9590.487234, test/ctc_loss=0.396121, test/num_examples=2472, test/wer=0.135539, total_duration=9754.144926, train/ctc_loss=0.393782, train/wer=0.141612, validation/ctc_loss=0.631063, validation/num_examples=5348, validation/wer=0.191628
I0518 22:10:55.200301 139838976149248 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.449880, loss=1.488779
I0518 22:10:55.207784 139873529071424 submission.py:119] 12500) loss = 1.489, grad_norm = 0.450
I0518 22:17:27.396091 139838967756544 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.360686, loss=1.399398
I0518 22:17:27.401847 139873529071424 submission.py:119] 13000) loss = 1.399, grad_norm = 0.361
I0518 22:24:01.023161 139838976149248 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.368356, loss=1.365587
I0518 22:24:01.030085 139873529071424 submission.py:119] 13500) loss = 1.366, grad_norm = 0.368
I0518 22:30:33.137465 139838967756544 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.386812, loss=1.403909
I0518 22:30:33.142441 139873529071424 submission.py:119] 14000) loss = 1.404, grad_norm = 0.387
I0518 22:37:06.477258 139838976149248 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.317927, loss=1.380283
I0518 22:37:06.484877 139873529071424 submission.py:119] 14500) loss = 1.380, grad_norm = 0.318
I0518 22:43:38.626705 139838967756544 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.338106, loss=1.386788
I0518 22:43:38.631816 139873529071424 submission.py:119] 15000) loss = 1.387, grad_norm = 0.338
I0518 22:47:01.674620 139873529071424 spec.py:298] Evaluating on the training split.
I0518 22:47:13.698042 139873529071424 spec.py:310] Evaluating on the validation split.
I0518 22:47:23.991649 139873529071424 spec.py:326] Evaluating on the test split.
I0518 22:47:29.564371 139873529071424 submission_runner.py:421] Time since start: 12182.79s, 	Step: 15260, 	{'train/ctc_loss': 0.32865696851659787, 'train/wer': 0.12173931801390289, 'validation/ctc_loss': 0.5785123285864979, 'validation/wer': 0.17507845314536766, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3471346185641009, 'test/wer': 0.11888367558345013, 'test/num_examples': 2472, 'score': 11986.280050039291, 'total_duration': 12182.793868303299, 'accumulated_submission_time': 11986.280050039291, 'accumulated_eval_time': 171.34649014472961, 'accumulated_logging_time': 0.1560661792755127}
I0518 22:47:29.591362 139838967756544 logging_writer.py:48] [15260] accumulated_eval_time=171.346490, accumulated_logging_time=0.156066, accumulated_submission_time=11986.280050, global_step=15260, preemption_count=0, score=11986.280050, test/ctc_loss=0.347135, test/num_examples=2472, test/wer=0.118884, total_duration=12182.793868, train/ctc_loss=0.328657, train/wer=0.121739, validation/ctc_loss=0.578512, validation/num_examples=5348, validation/wer=0.175078
I0518 22:50:40.060793 139838967756544 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.321168, loss=1.375973
I0518 22:50:40.071162 139873529071424 submission.py:119] 15500) loss = 1.376, grad_norm = 0.321
I0518 22:57:12.210655 139838959363840 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.346179, loss=1.386001
I0518 22:57:12.217615 139873529071424 submission.py:119] 16000) loss = 1.386, grad_norm = 0.346
I0518 23:03:45.728736 139838967756544 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.450372, loss=1.317707
I0518 23:03:45.736241 139873529071424 submission.py:119] 16500) loss = 1.318, grad_norm = 0.450
I0518 23:10:17.610341 139838959363840 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.386208, loss=1.375949
I0518 23:10:17.655550 139873529071424 submission.py:119] 17000) loss = 1.376, grad_norm = 0.386
I0518 23:16:49.570027 139838967756544 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.309222, loss=1.385817
I0518 23:16:49.574755 139873529071424 submission.py:119] 17500) loss = 1.386, grad_norm = 0.309
I0518 23:23:23.258403 139838967756544 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.286602, loss=1.354702
I0518 23:23:23.294732 139873529071424 submission.py:119] 18000) loss = 1.355, grad_norm = 0.287
I0518 23:27:30.237726 139873529071424 spec.py:298] Evaluating on the training split.
I0518 23:27:42.341067 139873529071424 spec.py:310] Evaluating on the validation split.
I0518 23:27:52.591893 139873529071424 spec.py:326] Evaluating on the test split.
I0518 23:27:58.154124 139873529071424 submission_runner.py:421] Time since start: 14611.38s, 	Step: 18316, 	{'train/ctc_loss': 0.28403932920772074, 'train/wer': 0.10501054311892961, 'validation/ctc_loss': 0.5236440654353778, 'validation/wer': 0.15832568918070777, 'validation/num_examples': 5348, 'test/ctc_loss': 0.31345694177850186, 'test/wer': 0.1072654520342047, 'test/num_examples': 2472, 'score': 14381.938515901566, 'total_duration': 14611.383615255356, 'accumulated_submission_time': 14381.938515901566, 'accumulated_eval_time': 199.26259684562683, 'accumulated_logging_time': 0.1923685073852539}
I0518 23:27:58.173900 139838967756544 logging_writer.py:48] [18316] accumulated_eval_time=199.262597, accumulated_logging_time=0.192369, accumulated_submission_time=14381.938516, global_step=18316, preemption_count=0, score=14381.938516, test/ctc_loss=0.313457, test/num_examples=2472, test/wer=0.107265, total_duration=14611.383615, train/ctc_loss=0.284039, train/wer=0.105011, validation/ctc_loss=0.523644, validation/num_examples=5348, validation/wer=0.158326
I0518 23:30:23.210631 139838959363840 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.484216, loss=1.371536
I0518 23:30:23.215868 139873529071424 submission.py:119] 18500) loss = 1.372, grad_norm = 0.484
I0518 23:36:56.863537 139838967756544 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.291673, loss=1.248935
I0518 23:36:56.871916 139873529071424 submission.py:119] 19000) loss = 1.249, grad_norm = 0.292
I0518 23:43:28.857808 139838959363840 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.414717, loss=1.328317
I0518 23:43:28.892691 139873529071424 submission.py:119] 19500) loss = 1.328, grad_norm = 0.415
I0518 23:50:01.738217 139873529071424 spec.py:298] Evaluating on the training split.
I0518 23:50:13.712528 139873529071424 spec.py:310] Evaluating on the validation split.
I0518 23:50:24.023059 139873529071424 spec.py:326] Evaluating on the test split.
I0518 23:50:29.572688 139873529071424 submission_runner.py:421] Time since start: 15962.80s, 	Step: 20000, 	{'train/ctc_loss': 0.2782280578837308, 'train/wer': 0.10544736799529739, 'validation/ctc_loss': 0.5343950826144013, 'validation/wer': 0.1610486168106986, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3123757480321311, 'test/wer': 0.10785448784351959, 'test/num_examples': 2472, 'score': 15702.683103084564, 'total_duration': 15962.802151203156, 'accumulated_submission_time': 15702.683103084564, 'accumulated_eval_time': 227.09707951545715, 'accumulated_logging_time': 0.22132468223571777}
I0518 23:50:29.592370 139838967756544 logging_writer.py:48] [20000] accumulated_eval_time=227.097080, accumulated_logging_time=0.221325, accumulated_submission_time=15702.683103, global_step=20000, preemption_count=0, score=15702.683103, test/ctc_loss=0.312376, test/num_examples=2472, test/wer=0.107854, total_duration=15962.802151, train/ctc_loss=0.278228, train/wer=0.105447, validation/ctc_loss=0.534395, validation/num_examples=5348, validation/wer=0.161049
I0518 23:50:29.612529 139838959363840 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15702.683103
I0518 23:50:30.280788 139873529071424 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_pytorch/timing_adamw/librispeech_conformer_pytorch/trial_1/checkpoint_20000.
I0518 23:50:30.402353 139873529071424 submission_runner.py:584] Tuning trial 1/1
I0518 23:50:30.402608 139873529071424 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0518 23:50:30.403199 139873529071424 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.65093310345292, 'train/wer': 1.9712180942571011, 'validation/ctc_loss': 30.465419052642154, 'validation/wer': 1.8997924009076426, 'validation/num_examples': 5348, 'test/ctc_loss': 30.54029614813859, 'test/wer': 1.9449962423577682, 'test/num_examples': 2472, 'score': 8.041781663894653, 'total_duration': 41.08777189254761, 'accumulated_submission_time': 8.041781663894653, 'accumulated_eval_time': 33.04480743408203, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3042, {'train/ctc_loss': 2.907006399722527, 'train/wer': 0.6160902555695171, 'validation/ctc_loss': 3.115166484579064, 'validation/wer': 0.6289576594409308, 'validation/num_examples': 5348, 'test/ctc_loss': 2.7629439802567908, 'test/wer': 0.5666118254016614, 'test/num_examples': 2472, 'score': 2403.651233434677, 'total_duration': 2468.605101108551, 'accumulated_submission_time': 2403.651233434677, 'accumulated_eval_time': 59.81337094306946, 'accumulated_logging_time': 0.035954952239990234, 'global_step': 3042, 'preemption_count': 0}), (6094, {'train/ctc_loss': 0.6632329121852333, 'train/wer': 0.22279147274698133, 'validation/ctc_loss': 0.8867883082491963, 'validation/wer': 0.2650992130546034, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6095865055810052, 'test/wer': 0.2036032742266366, 'test/num_examples': 2472, 'score': 4799.40868973732, 'total_duration': 4897.447292327881, 'accumulated_submission_time': 4799.40868973732, 'accumulated_eval_time': 87.86950182914734, 'accumulated_logging_time': 0.06655120849609375, 'global_step': 6094, 'preemption_count': 0}), (9148, {'train/ctc_loss': 0.47320905423516496, 'train/wer': 0.1663493844004983, 'validation/ctc_loss': 0.7079987866058871, 'validation/wer': 0.21375947472601747, 'validation/num_examples': 5348, 'test/ctc_loss': 0.44748648098264404, 'test/wer': 0.15195092722361017, 'test/num_examples': 2472, 'score': 7194.719589233398, 'total_duration': 7325.581441402435, 'accumulated_submission_time': 7194.719589233398, 'accumulated_eval_time': 115.67890501022339, 'accumulated_logging_time': 0.0969688892364502, 'global_step': 9148, 'preemption_count': 0}), (12204, {'train/ctc_loss': 0.39378184813244616, 'train/wer': 0.1416121534387825, 'validation/ctc_loss': 0.6310629481552642, 'validation/wer': 0.19162844590353884, 'validation/num_examples': 5348, 'test/ctc_loss': 0.396121322343377, 'test/wer': 0.13553917088131945, 'test/num_examples': 2472, 'score': 9590.48723435402, 'total_duration': 9754.144926309586, 'accumulated_submission_time': 9590.48723435402, 'accumulated_eval_time': 143.4570655822754, 'accumulated_logging_time': 0.12621736526489258, 'global_step': 12204, 'preemption_count': 0}), (15260, {'train/ctc_loss': 0.32865696851659787, 'train/wer': 0.12173931801390289, 'validation/ctc_loss': 0.5785123285864979, 'validation/wer': 0.17507845314536766, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3471346185641009, 'test/wer': 0.11888367558345013, 'test/num_examples': 2472, 'score': 11986.280050039291, 'total_duration': 12182.793868303299, 'accumulated_submission_time': 11986.280050039291, 'accumulated_eval_time': 171.34649014472961, 'accumulated_logging_time': 0.1560661792755127, 'global_step': 15260, 'preemption_count': 0}), (18316, {'train/ctc_loss': 0.28403932920772074, 'train/wer': 0.10501054311892961, 'validation/ctc_loss': 0.5236440654353778, 'validation/wer': 0.15832568918070777, 'validation/num_examples': 5348, 'test/ctc_loss': 0.31345694177850186, 'test/wer': 0.1072654520342047, 'test/num_examples': 2472, 'score': 14381.938515901566, 'total_duration': 14611.383615255356, 'accumulated_submission_time': 14381.938515901566, 'accumulated_eval_time': 199.26259684562683, 'accumulated_logging_time': 0.1923685073852539, 'global_step': 18316, 'preemption_count': 0}), (20000, {'train/ctc_loss': 0.2782280578837308, 'train/wer': 0.10544736799529739, 'validation/ctc_loss': 0.5343950826144013, 'validation/wer': 0.1610486168106986, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3123757480321311, 'test/wer': 0.10785448784351959, 'test/num_examples': 2472, 'score': 15702.683103084564, 'total_duration': 15962.802151203156, 'accumulated_submission_time': 15702.683103084564, 'accumulated_eval_time': 227.09707951545715, 'accumulated_logging_time': 0.22132468223571777, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0518 23:50:30.403330 139873529071424 submission_runner.py:587] Timing: 15702.683103084564
I0518 23:50:30.403384 139873529071424 submission_runner.py:588] ====================
I0518 23:50:30.403574 139873529071424 submission_runner.py:651] Final librispeech_conformer score: 15702.683103084564
