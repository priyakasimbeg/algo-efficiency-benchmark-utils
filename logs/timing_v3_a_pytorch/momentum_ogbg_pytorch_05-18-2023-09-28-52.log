torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3_pytorch/timing_momentum --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_pytorch_05-18-2023-09-28-52.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0518 09:29:16.132088 140687654790976 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0518 09:29:16.132227 140348783191872 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0518 09:29:16.132677 139700605507392 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0518 09:29:16.133048 140649446995776 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0518 09:29:16.133050 139872280180544 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0518 09:29:16.133245 139756087269184 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0518 09:29:16.133378 140618201737024 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0518 09:29:16.133919 140256999294784 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0518 09:29:16.134262 140256999294784 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 09:29:16.142862 140687654790976 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 09:29:16.142885 140348783191872 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 09:29:16.143364 139700605507392 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 09:29:16.143693 139872280180544 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 09:29:16.143720 140649446995776 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 09:29:16.143925 139756087269184 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 09:29:16.144048 140618201737024 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0518 09:29:17.303719 139872280180544 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3_pytorch/timing_momentum/ogbg_pytorch.
W0518 09:29:17.424464 140618201737024 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 09:29:17.424661 140649446995776 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 09:29:17.426167 140687654790976 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 09:29:17.426675 139872280180544 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 09:29:17.427010 140348783191872 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 09:29:17.427393 139700605507392 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 09:29:17.427695 140256999294784 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0518 09:29:17.429528 139756087269184 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0518 09:29:17.432306 139872280180544 submission_runner.py:544] Using RNG seed 177938639
I0518 09:29:17.433607 139872280180544 submission_runner.py:553] --- Tuning run 1/1 ---
I0518 09:29:17.433716 139872280180544 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_v3_pytorch/timing_momentum/ogbg_pytorch/trial_1.
I0518 09:29:17.434035 139872280180544 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3_pytorch/timing_momentum/ogbg_pytorch/trial_1/hparams.json.
I0518 09:29:17.435008 139872280180544 submission_runner.py:241] Initializing dataset.
I0518 09:29:17.435123 139872280180544 submission_runner.py:248] Initializing model.
I0518 09:29:21.557301 139872280180544 submission_runner.py:258] Initializing optimizer.
I0518 09:29:22.067585 139872280180544 submission_runner.py:265] Initializing metrics bundle.
I0518 09:29:22.067815 139872280180544 submission_runner.py:283] Initializing checkpoint and logger.
I0518 09:29:22.071123 139872280180544 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0518 09:29:22.071264 139872280180544 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0518 09:29:22.558784 139872280180544 submission_runner.py:304] Saving meta data to /experiment_runs/timing_v3_pytorch/timing_momentum/ogbg_pytorch/trial_1/meta_data_0.json.
I0518 09:29:22.559895 139872280180544 submission_runner.py:307] Saving flags to /experiment_runs/timing_v3_pytorch/timing_momentum/ogbg_pytorch/trial_1/flags_0.json.
I0518 09:29:22.616827 139872280180544 submission_runner.py:319] Starting training loop.
I0518 09:29:22.856733 139872280180544 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0518 09:29:22.862415 139872280180544 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0518 09:29:23.002033 139872280180544 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0518 09:29:27.638007 139834093987584 logging_writer.py:48] [0] global_step=0, grad_norm=2.882780, loss=0.748250
I0518 09:29:27.646805 139872280180544 submission.py:139] 0) loss = 0.748, grad_norm = 2.883
I0518 09:29:27.647649 139872280180544 spec.py:298] Evaluating on the training split.
I0518 09:29:27.653176 139872280180544 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0518 09:29:27.657402 139872280180544 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0518 09:29:27.710270 139872280180544 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0518 09:30:23.266636 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 09:30:23.269861 139872280180544 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0518 09:30:23.274478 139872280180544 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0518 09:30:23.328977 139872280180544 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0518 09:31:06.948614 139872280180544 spec.py:326] Evaluating on the test split.
I0518 09:31:06.951869 139872280180544 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0518 09:31:06.955917 139872280180544 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0518 09:31:07.010056 139872280180544 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0518 09:31:51.489774 139872280180544 submission_runner.py:421] Time since start: 148.87s, 	Step: 1, 	{'train/accuracy': 0.48072310275931274, 'train/loss': 0.7473357462523087, 'train/mean_average_precision': 0.02194968160100208, 'validation/accuracy': 0.4807768811900882, 'validation/loss': 0.7464825745895737, 'validation/mean_average_precision': 0.026760981165532393, 'validation/num_examples': 43793, 'test/accuracy': 0.4792677446161842, 'test/loss': 0.7473922784063173, 'test/mean_average_precision': 0.02790392316762357, 'test/num_examples': 43793, 'score': 5.030078649520874, 'total_duration': 148.87319612503052, 'accumulated_submission_time': 5.030078649520874, 'accumulated_eval_time': 143.84181356430054, 'accumulated_logging_time': 0}
I0518 09:31:51.509519 139820319442688 logging_writer.py:48] [1] accumulated_eval_time=143.841814, accumulated_logging_time=0, accumulated_submission_time=5.030079, global_step=1, preemption_count=0, score=5.030079, test/accuracy=0.479268, test/loss=0.747392, test/mean_average_precision=0.027904, test/num_examples=43793, total_duration=148.873196, train/accuracy=0.480723, train/loss=0.747336, train/mean_average_precision=0.021950, validation/accuracy=0.480777, validation/loss=0.746483, validation/mean_average_precision=0.026761, validation/num_examples=43793
I0518 09:31:51.801914 139700605507392 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 09:31:51.801928 140687654790976 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 09:31:51.801928 140348783191872 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 09:31:51.801933 140649446995776 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 09:31:51.801956 139872280180544 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 09:31:51.801949 140618201737024 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 09:31:51.801970 139756087269184 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 09:31:51.802002 140256999294784 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0518 09:31:51.835008 139820327835392 logging_writer.py:48] [1] global_step=1, grad_norm=2.886296, loss=0.746638
I0518 09:31:51.838811 139872280180544 submission.py:139] 1) loss = 0.747, grad_norm = 2.886
I0518 09:31:52.151588 139820319442688 logging_writer.py:48] [2] global_step=2, grad_norm=2.866119, loss=0.744556
I0518 09:31:52.155420 139872280180544 submission.py:139] 2) loss = 0.745, grad_norm = 2.866
I0518 09:31:52.468652 139820327835392 logging_writer.py:48] [3] global_step=3, grad_norm=2.859492, loss=0.740087
I0518 09:31:52.472795 139872280180544 submission.py:139] 3) loss = 0.740, grad_norm = 2.859
I0518 09:31:52.794970 139820319442688 logging_writer.py:48] [4] global_step=4, grad_norm=2.807546, loss=0.732036
I0518 09:31:52.798837 139872280180544 submission.py:139] 4) loss = 0.732, grad_norm = 2.808
I0518 09:31:53.117877 139820327835392 logging_writer.py:48] [5] global_step=5, grad_norm=2.720193, loss=0.717950
I0518 09:31:53.121808 139872280180544 submission.py:139] 5) loss = 0.718, grad_norm = 2.720
I0518 09:31:53.422524 139820319442688 logging_writer.py:48] [6] global_step=6, grad_norm=2.604124, loss=0.695931
I0518 09:31:53.426225 139872280180544 submission.py:139] 6) loss = 0.696, grad_norm = 2.604
I0518 09:31:53.718125 139820327835392 logging_writer.py:48] [7] global_step=7, grad_norm=2.344553, loss=0.672080
I0518 09:31:53.722218 139872280180544 submission.py:139] 7) loss = 0.672, grad_norm = 2.345
I0518 09:31:54.011949 139820319442688 logging_writer.py:48] [8] global_step=8, grad_norm=2.007805, loss=0.647995
I0518 09:31:54.015638 139872280180544 submission.py:139] 8) loss = 0.648, grad_norm = 2.008
I0518 09:31:54.306410 139820327835392 logging_writer.py:48] [9] global_step=9, grad_norm=1.726207, loss=0.626281
I0518 09:31:54.310175 139872280180544 submission.py:139] 9) loss = 0.626, grad_norm = 1.726
I0518 09:31:54.603435 139820319442688 logging_writer.py:48] [10] global_step=10, grad_norm=1.585141, loss=0.607052
I0518 09:31:54.607118 139872280180544 submission.py:139] 10) loss = 0.607, grad_norm = 1.585
I0518 09:31:54.900074 139820327835392 logging_writer.py:48] [11] global_step=11, grad_norm=1.583091, loss=0.590680
I0518 09:31:54.904044 139872280180544 submission.py:139] 11) loss = 0.591, grad_norm = 1.583
I0518 09:31:55.191009 139820319442688 logging_writer.py:48] [12] global_step=12, grad_norm=1.470062, loss=0.575728
I0518 09:31:55.194814 139872280180544 submission.py:139] 12) loss = 0.576, grad_norm = 1.470
I0518 09:31:55.487966 139820327835392 logging_writer.py:48] [13] global_step=13, grad_norm=1.355256, loss=0.558704
I0518 09:31:55.491803 139872280180544 submission.py:139] 13) loss = 0.559, grad_norm = 1.355
I0518 09:31:55.779756 139820319442688 logging_writer.py:48] [14] global_step=14, grad_norm=1.239545, loss=0.545380
I0518 09:31:55.783584 139872280180544 submission.py:139] 14) loss = 0.545, grad_norm = 1.240
I0518 09:31:56.075624 139820327835392 logging_writer.py:48] [15] global_step=15, grad_norm=1.109986, loss=0.530283
I0518 09:31:56.079371 139872280180544 submission.py:139] 15) loss = 0.530, grad_norm = 1.110
I0518 09:31:56.367418 139820319442688 logging_writer.py:48] [16] global_step=16, grad_norm=1.078973, loss=0.517759
I0518 09:31:56.371147 139872280180544 submission.py:139] 16) loss = 0.518, grad_norm = 1.079
I0518 09:31:56.660151 139820327835392 logging_writer.py:48] [17] global_step=17, grad_norm=1.033933, loss=0.506053
I0518 09:31:56.663871 139872280180544 submission.py:139] 17) loss = 0.506, grad_norm = 1.034
I0518 09:31:57.156190 139820319442688 logging_writer.py:48] [18] global_step=18, grad_norm=0.946219, loss=0.496276
I0518 09:31:57.159966 139872280180544 submission.py:139] 18) loss = 0.496, grad_norm = 0.946
I0518 09:31:57.445686 139820327835392 logging_writer.py:48] [19] global_step=19, grad_norm=0.872697, loss=0.485667
I0518 09:31:57.449549 139872280180544 submission.py:139] 19) loss = 0.486, grad_norm = 0.873
I0518 09:31:57.740734 139820319442688 logging_writer.py:48] [20] global_step=20, grad_norm=0.808842, loss=0.473251
I0518 09:31:57.744517 139872280180544 submission.py:139] 20) loss = 0.473, grad_norm = 0.809
I0518 09:31:58.032222 139820327835392 logging_writer.py:48] [21] global_step=21, grad_norm=0.768831, loss=0.465895
I0518 09:31:58.036083 139872280180544 submission.py:139] 21) loss = 0.466, grad_norm = 0.769
I0518 09:31:58.332441 139820319442688 logging_writer.py:48] [22] global_step=22, grad_norm=0.755217, loss=0.454619
I0518 09:31:58.336187 139872280180544 submission.py:139] 22) loss = 0.455, grad_norm = 0.755
I0518 09:31:58.625099 139820327835392 logging_writer.py:48] [23] global_step=23, grad_norm=0.724865, loss=0.444776
I0518 09:31:58.629157 139872280180544 submission.py:139] 23) loss = 0.445, grad_norm = 0.725
I0518 09:31:58.917601 139820319442688 logging_writer.py:48] [24] global_step=24, grad_norm=0.669084, loss=0.435568
I0518 09:31:58.921359 139872280180544 submission.py:139] 24) loss = 0.436, grad_norm = 0.669
I0518 09:31:59.205894 139820327835392 logging_writer.py:48] [25] global_step=25, grad_norm=0.614954, loss=0.427208
I0518 09:31:59.209604 139872280180544 submission.py:139] 25) loss = 0.427, grad_norm = 0.615
I0518 09:31:59.501790 139820319442688 logging_writer.py:48] [26] global_step=26, grad_norm=0.583541, loss=0.417882
I0518 09:31:59.505555 139872280180544 submission.py:139] 26) loss = 0.418, grad_norm = 0.584
I0518 09:31:59.796879 139820327835392 logging_writer.py:48] [27] global_step=27, grad_norm=0.557388, loss=0.410497
I0518 09:31:59.800529 139872280180544 submission.py:139] 27) loss = 0.410, grad_norm = 0.557
I0518 09:32:00.092559 139820319442688 logging_writer.py:48] [28] global_step=28, grad_norm=0.553772, loss=0.400710
I0518 09:32:00.096562 139872280180544 submission.py:139] 28) loss = 0.401, grad_norm = 0.554
I0518 09:32:00.391429 139820327835392 logging_writer.py:48] [29] global_step=29, grad_norm=0.545059, loss=0.391569
I0518 09:32:00.395242 139872280180544 submission.py:139] 29) loss = 0.392, grad_norm = 0.545
I0518 09:32:00.688634 139820319442688 logging_writer.py:48] [30] global_step=30, grad_norm=0.509460, loss=0.381711
I0518 09:32:00.692430 139872280180544 submission.py:139] 30) loss = 0.382, grad_norm = 0.509
I0518 09:32:00.989259 139820327835392 logging_writer.py:48] [31] global_step=31, grad_norm=0.462617, loss=0.376428
I0518 09:32:00.993243 139872280180544 submission.py:139] 31) loss = 0.376, grad_norm = 0.463
I0518 09:32:01.286644 139820319442688 logging_writer.py:48] [32] global_step=32, grad_norm=0.441817, loss=0.367220
I0518 09:32:01.290343 139872280180544 submission.py:139] 32) loss = 0.367, grad_norm = 0.442
I0518 09:32:01.587588 139820327835392 logging_writer.py:48] [33] global_step=33, grad_norm=0.426484, loss=0.360379
I0518 09:32:01.591089 139872280180544 submission.py:139] 33) loss = 0.360, grad_norm = 0.426
I0518 09:32:01.936201 139820319442688 logging_writer.py:48] [34] global_step=34, grad_norm=0.413111, loss=0.353020
I0518 09:32:01.940434 139872280180544 submission.py:139] 34) loss = 0.353, grad_norm = 0.413
I0518 09:32:02.246208 139820327835392 logging_writer.py:48] [35] global_step=35, grad_norm=0.399946, loss=0.343193
I0518 09:32:02.250235 139872280180544 submission.py:139] 35) loss = 0.343, grad_norm = 0.400
I0518 09:32:02.544077 139820319442688 logging_writer.py:48] [36] global_step=36, grad_norm=0.390052, loss=0.335142
I0518 09:32:02.547787 139872280180544 submission.py:139] 36) loss = 0.335, grad_norm = 0.390
I0518 09:32:02.845060 139820327835392 logging_writer.py:48] [37] global_step=37, grad_norm=0.380369, loss=0.325632
I0518 09:32:02.849183 139872280180544 submission.py:139] 37) loss = 0.326, grad_norm = 0.380
I0518 09:32:03.145905 139820319442688 logging_writer.py:48] [38] global_step=38, grad_norm=0.376713, loss=0.318990
I0518 09:32:03.149743 139872280180544 submission.py:139] 38) loss = 0.319, grad_norm = 0.377
I0518 09:32:03.443380 139820327835392 logging_writer.py:48] [39] global_step=39, grad_norm=0.364880, loss=0.313387
I0518 09:32:03.447114 139872280180544 submission.py:139] 39) loss = 0.313, grad_norm = 0.365
I0518 09:32:03.744200 139820319442688 logging_writer.py:48] [40] global_step=40, grad_norm=0.356259, loss=0.305040
I0518 09:32:03.748124 139872280180544 submission.py:139] 40) loss = 0.305, grad_norm = 0.356
I0518 09:32:04.043974 139820327835392 logging_writer.py:48] [41] global_step=41, grad_norm=0.345689, loss=0.298627
I0518 09:32:04.047856 139872280180544 submission.py:139] 41) loss = 0.299, grad_norm = 0.346
I0518 09:32:04.339786 139820319442688 logging_writer.py:48] [42] global_step=42, grad_norm=0.336745, loss=0.291408
I0518 09:32:04.343750 139872280180544 submission.py:139] 42) loss = 0.291, grad_norm = 0.337
I0518 09:32:04.638534 139820327835392 logging_writer.py:48] [43] global_step=43, grad_norm=0.331743, loss=0.282397
I0518 09:32:04.642456 139872280180544 submission.py:139] 43) loss = 0.282, grad_norm = 0.332
I0518 09:32:04.934925 139820319442688 logging_writer.py:48] [44] global_step=44, grad_norm=0.321135, loss=0.275545
I0518 09:32:04.938704 139872280180544 submission.py:139] 44) loss = 0.276, grad_norm = 0.321
I0518 09:32:05.234778 139820327835392 logging_writer.py:48] [45] global_step=45, grad_norm=0.314469, loss=0.269937
I0518 09:32:05.238591 139872280180544 submission.py:139] 45) loss = 0.270, grad_norm = 0.314
I0518 09:32:05.533958 139820319442688 logging_writer.py:48] [46] global_step=46, grad_norm=0.304812, loss=0.263267
I0518 09:32:05.537867 139872280180544 submission.py:139] 46) loss = 0.263, grad_norm = 0.305
I0518 09:32:05.835217 139820327835392 logging_writer.py:48] [47] global_step=47, grad_norm=0.294985, loss=0.256378
I0518 09:32:05.839289 139872280180544 submission.py:139] 47) loss = 0.256, grad_norm = 0.295
I0518 09:32:06.137749 139820319442688 logging_writer.py:48] [48] global_step=48, grad_norm=0.286740, loss=0.249726
I0518 09:32:06.141563 139872280180544 submission.py:139] 48) loss = 0.250, grad_norm = 0.287
I0518 09:32:06.438807 139820327835392 logging_writer.py:48] [49] global_step=49, grad_norm=0.280220, loss=0.241911
I0518 09:32:06.442892 139872280180544 submission.py:139] 49) loss = 0.242, grad_norm = 0.280
I0518 09:32:06.735739 139820319442688 logging_writer.py:48] [50] global_step=50, grad_norm=0.270466, loss=0.235928
I0518 09:32:06.739593 139872280180544 submission.py:139] 50) loss = 0.236, grad_norm = 0.270
I0518 09:32:07.036756 139820327835392 logging_writer.py:48] [51] global_step=51, grad_norm=0.262762, loss=0.229470
I0518 09:32:07.040381 139872280180544 submission.py:139] 51) loss = 0.229, grad_norm = 0.263
I0518 09:32:07.347370 139820319442688 logging_writer.py:48] [52] global_step=52, grad_norm=0.254631, loss=0.221070
I0518 09:32:07.351379 139872280180544 submission.py:139] 52) loss = 0.221, grad_norm = 0.255
I0518 09:32:07.653762 139820327835392 logging_writer.py:48] [53] global_step=53, grad_norm=0.244783, loss=0.214913
I0518 09:32:07.657750 139872280180544 submission.py:139] 53) loss = 0.215, grad_norm = 0.245
I0518 09:32:07.954814 139820319442688 logging_writer.py:48] [54] global_step=54, grad_norm=0.239607, loss=0.212366
I0518 09:32:07.958588 139872280180544 submission.py:139] 54) loss = 0.212, grad_norm = 0.240
I0518 09:32:08.255445 139820327835392 logging_writer.py:48] [55] global_step=55, grad_norm=0.230739, loss=0.205293
I0518 09:32:08.259014 139872280180544 submission.py:139] 55) loss = 0.205, grad_norm = 0.231
I0518 09:32:08.562537 139820319442688 logging_writer.py:48] [56] global_step=56, grad_norm=0.225453, loss=0.202976
I0518 09:32:08.566305 139872280180544 submission.py:139] 56) loss = 0.203, grad_norm = 0.225
I0518 09:32:08.860280 139820327835392 logging_writer.py:48] [57] global_step=57, grad_norm=0.217623, loss=0.192780
I0518 09:32:08.864213 139872280180544 submission.py:139] 57) loss = 0.193, grad_norm = 0.218
I0518 09:32:09.152729 139820319442688 logging_writer.py:48] [58] global_step=58, grad_norm=0.210608, loss=0.191106
I0518 09:32:09.156464 139872280180544 submission.py:139] 58) loss = 0.191, grad_norm = 0.211
I0518 09:32:09.444975 139820327835392 logging_writer.py:48] [59] global_step=59, grad_norm=0.204572, loss=0.187246
I0518 09:32:09.448741 139872280180544 submission.py:139] 59) loss = 0.187, grad_norm = 0.205
I0518 09:32:09.740481 139820319442688 logging_writer.py:48] [60] global_step=60, grad_norm=0.199608, loss=0.179777
I0518 09:32:09.744410 139872280180544 submission.py:139] 60) loss = 0.180, grad_norm = 0.200
I0518 09:32:10.038245 139820327835392 logging_writer.py:48] [61] global_step=61, grad_norm=0.190215, loss=0.176863
I0518 09:32:10.041893 139872280180544 submission.py:139] 61) loss = 0.177, grad_norm = 0.190
I0518 09:32:10.334656 139820319442688 logging_writer.py:48] [62] global_step=62, grad_norm=0.189211, loss=0.173903
I0518 09:32:10.338207 139872280180544 submission.py:139] 62) loss = 0.174, grad_norm = 0.189
I0518 09:32:10.634206 139820327835392 logging_writer.py:48] [63] global_step=63, grad_norm=0.181527, loss=0.169499
I0518 09:32:10.638089 139872280180544 submission.py:139] 63) loss = 0.169, grad_norm = 0.182
I0518 09:32:10.932714 139820319442688 logging_writer.py:48] [64] global_step=64, grad_norm=0.173766, loss=0.163557
I0518 09:32:10.936532 139872280180544 submission.py:139] 64) loss = 0.164, grad_norm = 0.174
I0518 09:32:11.225875 139820327835392 logging_writer.py:48] [65] global_step=65, grad_norm=0.169796, loss=0.161352
I0518 09:32:11.229489 139872280180544 submission.py:139] 65) loss = 0.161, grad_norm = 0.170
I0518 09:32:11.523248 139820319442688 logging_writer.py:48] [66] global_step=66, grad_norm=0.165903, loss=0.157405
I0518 09:32:11.526837 139872280180544 submission.py:139] 66) loss = 0.157, grad_norm = 0.166
I0518 09:32:11.819414 139820327835392 logging_writer.py:48] [67] global_step=67, grad_norm=0.160767, loss=0.150792
I0518 09:32:11.823148 139872280180544 submission.py:139] 67) loss = 0.151, grad_norm = 0.161
I0518 09:32:12.115688 139820319442688 logging_writer.py:48] [68] global_step=68, grad_norm=0.154860, loss=0.154625
I0518 09:32:12.119378 139872280180544 submission.py:139] 68) loss = 0.155, grad_norm = 0.155
I0518 09:32:12.412516 139820327835392 logging_writer.py:48] [69] global_step=69, grad_norm=0.148219, loss=0.146767
I0518 09:32:12.416316 139872280180544 submission.py:139] 69) loss = 0.147, grad_norm = 0.148
I0518 09:32:12.711800 139820319442688 logging_writer.py:48] [70] global_step=70, grad_norm=0.146782, loss=0.140926
I0518 09:32:12.715604 139872280180544 submission.py:139] 70) loss = 0.141, grad_norm = 0.147
I0518 09:32:13.008124 139820327835392 logging_writer.py:48] [71] global_step=71, grad_norm=0.141546, loss=0.141306
I0518 09:32:13.011833 139872280180544 submission.py:139] 71) loss = 0.141, grad_norm = 0.142
I0518 09:32:13.304407 139820319442688 logging_writer.py:48] [72] global_step=72, grad_norm=0.134826, loss=0.138242
I0518 09:32:13.308140 139872280180544 submission.py:139] 72) loss = 0.138, grad_norm = 0.135
I0518 09:32:13.599150 139820327835392 logging_writer.py:48] [73] global_step=73, grad_norm=0.133910, loss=0.134455
I0518 09:32:13.602993 139872280180544 submission.py:139] 73) loss = 0.134, grad_norm = 0.134
I0518 09:32:13.893768 139820319442688 logging_writer.py:48] [74] global_step=74, grad_norm=0.131419, loss=0.128967
I0518 09:32:13.897573 139872280180544 submission.py:139] 74) loss = 0.129, grad_norm = 0.131
I0518 09:32:14.189635 139820327835392 logging_writer.py:48] [75] global_step=75, grad_norm=0.126215, loss=0.127259
I0518 09:32:14.193360 139872280180544 submission.py:139] 75) loss = 0.127, grad_norm = 0.126
I0518 09:32:14.487305 139820319442688 logging_writer.py:48] [76] global_step=76, grad_norm=0.120230, loss=0.129667
I0518 09:32:14.491305 139872280180544 submission.py:139] 76) loss = 0.130, grad_norm = 0.120
I0518 09:32:14.787583 139820327835392 logging_writer.py:48] [77] global_step=77, grad_norm=0.116916, loss=0.128910
I0518 09:32:14.791302 139872280180544 submission.py:139] 77) loss = 0.129, grad_norm = 0.117
I0518 09:32:15.091648 139820319442688 logging_writer.py:48] [78] global_step=78, grad_norm=0.115273, loss=0.124487
I0518 09:32:15.095415 139872280180544 submission.py:139] 78) loss = 0.124, grad_norm = 0.115
I0518 09:32:15.391255 139820327835392 logging_writer.py:48] [79] global_step=79, grad_norm=0.112019, loss=0.119921
I0518 09:32:15.395101 139872280180544 submission.py:139] 79) loss = 0.120, grad_norm = 0.112
I0518 09:32:15.694235 139820319442688 logging_writer.py:48] [80] global_step=80, grad_norm=0.110120, loss=0.118030
I0518 09:32:15.698161 139872280180544 submission.py:139] 80) loss = 0.118, grad_norm = 0.110
I0518 09:32:15.998527 139820327835392 logging_writer.py:48] [81] global_step=81, grad_norm=0.107366, loss=0.116411
I0518 09:32:16.002380 139872280180544 submission.py:139] 81) loss = 0.116, grad_norm = 0.107
I0518 09:32:16.295203 139820319442688 logging_writer.py:48] [82] global_step=82, grad_norm=0.103134, loss=0.114089
I0518 09:32:16.298873 139872280180544 submission.py:139] 82) loss = 0.114, grad_norm = 0.103
I0518 09:32:16.593825 139820327835392 logging_writer.py:48] [83] global_step=83, grad_norm=0.100710, loss=0.110191
I0518 09:32:16.597596 139872280180544 submission.py:139] 83) loss = 0.110, grad_norm = 0.101
I0518 09:32:16.895700 139820319442688 logging_writer.py:48] [84] global_step=84, grad_norm=0.098862, loss=0.112764
I0518 09:32:16.899456 139872280180544 submission.py:139] 84) loss = 0.113, grad_norm = 0.099
I0518 09:32:17.187240 139820327835392 logging_writer.py:48] [85] global_step=85, grad_norm=0.096969, loss=0.112523
I0518 09:32:17.190857 139872280180544 submission.py:139] 85) loss = 0.113, grad_norm = 0.097
I0518 09:32:17.488277 139820319442688 logging_writer.py:48] [86] global_step=86, grad_norm=0.094537, loss=0.106097
I0518 09:32:17.491833 139872280180544 submission.py:139] 86) loss = 0.106, grad_norm = 0.095
I0518 09:32:17.789676 139820327835392 logging_writer.py:48] [87] global_step=87, grad_norm=0.091833, loss=0.105675
I0518 09:32:17.793475 139872280180544 submission.py:139] 87) loss = 0.106, grad_norm = 0.092
I0518 09:32:18.082780 139820319442688 logging_writer.py:48] [88] global_step=88, grad_norm=0.089517, loss=0.103363
I0518 09:32:18.086629 139872280180544 submission.py:139] 88) loss = 0.103, grad_norm = 0.090
I0518 09:32:18.377259 139820327835392 logging_writer.py:48] [89] global_step=89, grad_norm=0.085298, loss=0.100663
I0518 09:32:18.381195 139872280180544 submission.py:139] 89) loss = 0.101, grad_norm = 0.085
I0518 09:32:18.672854 139820319442688 logging_writer.py:48] [90] global_step=90, grad_norm=0.083561, loss=0.104048
I0518 09:32:18.676628 139872280180544 submission.py:139] 90) loss = 0.104, grad_norm = 0.084
I0518 09:32:18.968378 139820327835392 logging_writer.py:48] [91] global_step=91, grad_norm=0.082944, loss=0.101024
I0518 09:32:18.971982 139872280180544 submission.py:139] 91) loss = 0.101, grad_norm = 0.083
I0518 09:32:19.260967 139820319442688 logging_writer.py:48] [92] global_step=92, grad_norm=0.080552, loss=0.099681
I0518 09:32:19.264977 139872280180544 submission.py:139] 92) loss = 0.100, grad_norm = 0.081
I0518 09:32:19.558870 139820327835392 logging_writer.py:48] [93] global_step=93, grad_norm=0.079229, loss=0.093125
I0518 09:32:19.562766 139872280180544 submission.py:139] 93) loss = 0.093, grad_norm = 0.079
I0518 09:32:19.858318 139820319442688 logging_writer.py:48] [94] global_step=94, grad_norm=0.076113, loss=0.096571
I0518 09:32:19.862308 139872280180544 submission.py:139] 94) loss = 0.097, grad_norm = 0.076
I0518 09:32:20.150404 139820327835392 logging_writer.py:48] [95] global_step=95, grad_norm=0.076037, loss=0.095770
I0518 09:32:20.154238 139872280180544 submission.py:139] 95) loss = 0.096, grad_norm = 0.076
I0518 09:32:20.440300 139820319442688 logging_writer.py:48] [96] global_step=96, grad_norm=0.074683, loss=0.092136
I0518 09:32:20.444183 139872280180544 submission.py:139] 96) loss = 0.092, grad_norm = 0.075
I0518 09:32:20.733410 139820327835392 logging_writer.py:48] [97] global_step=97, grad_norm=0.072274, loss=0.092390
I0518 09:32:20.737278 139872280180544 submission.py:139] 97) loss = 0.092, grad_norm = 0.072
I0518 09:32:21.026686 139820319442688 logging_writer.py:48] [98] global_step=98, grad_norm=0.069067, loss=0.091562
I0518 09:32:21.030216 139872280180544 submission.py:139] 98) loss = 0.092, grad_norm = 0.069
I0518 09:32:21.320848 139820327835392 logging_writer.py:48] [99] global_step=99, grad_norm=0.069508, loss=0.088304
I0518 09:32:21.325091 139872280180544 submission.py:139] 99) loss = 0.088, grad_norm = 0.070
I0518 09:32:21.616729 139820319442688 logging_writer.py:48] [100] global_step=100, grad_norm=0.068154, loss=0.084183
I0518 09:32:21.620394 139872280180544 submission.py:139] 100) loss = 0.084, grad_norm = 0.068
I0518 09:34:16.261320 139820327835392 logging_writer.py:48] [500] global_step=500, grad_norm=0.013684, loss=0.058439
I0518 09:34:16.265793 139872280180544 submission.py:139] 500) loss = 0.058, grad_norm = 0.014
I0518 09:35:51.557169 139872280180544 spec.py:298] Evaluating on the training split.
I0518 09:36:47.239936 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 09:36:50.468195 139872280180544 spec.py:326] Evaluating on the test split.
I0518 09:36:53.568783 139872280180544 submission_runner.py:421] Time since start: 450.95s, 	Step: 834, 	{'train/accuracy': 0.9868241790536274, 'train/loss': 0.055029201383793636, 'train/mean_average_precision': 0.031096511553329195, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06524455872013393, 'validation/mean_average_precision': 0.03395175465061206, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06858320953719549, 'test/mean_average_precision': 0.034527050956562104, 'test/num_examples': 43793, 'score': 244.87157011032104, 'total_duration': 450.95229601860046, 'accumulated_submission_time': 244.87157011032104, 'accumulated_eval_time': 205.85320496559143, 'accumulated_logging_time': 0.029907703399658203}
I0518 09:36:53.578573 139820319442688 logging_writer.py:48] [834] accumulated_eval_time=205.853205, accumulated_logging_time=0.029908, accumulated_submission_time=244.871570, global_step=834, preemption_count=0, score=244.871570, test/accuracy=0.983142, test/loss=0.068583, test/mean_average_precision=0.034527, test/num_examples=43793, total_duration=450.952296, train/accuracy=0.986824, train/loss=0.055029, train/mean_average_precision=0.031097, validation/accuracy=0.984118, validation/loss=0.065245, validation/mean_average_precision=0.033952, validation/num_examples=43793
I0518 09:37:41.522316 139820327835392 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.019995, loss=0.052850
I0518 09:37:41.526645 139872280180544 submission.py:139] 1000) loss = 0.053, grad_norm = 0.020
I0518 09:40:04.334742 139820319442688 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.041987, loss=0.053743
I0518 09:40:04.339834 139872280180544 submission.py:139] 1500) loss = 0.054, grad_norm = 0.042
I0518 09:40:53.619891 139872280180544 spec.py:298] Evaluating on the training split.
I0518 09:41:51.214246 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 09:41:54.430905 139872280180544 spec.py:326] Evaluating on the test split.
I0518 09:41:57.629301 139872280180544 submission_runner.py:421] Time since start: 755.01s, 	Step: 1673, 	{'train/accuracy': 0.9867694011058111, 'train/loss': 0.05269909591136991, 'train/mean_average_precision': 0.046822941243029645, 'validation/accuracy': 0.9841273127456449, 'validation/loss': 0.06242290298139087, 'validation/mean_average_precision': 0.047886895898442305, 'validation/num_examples': 43793, 'test/accuracy': 0.9831442098998441, 'test/loss': 0.06562146064841237, 'test/mean_average_precision': 0.048392161488527076, 'test/num_examples': 43793, 'score': 484.717246055603, 'total_duration': 755.0127830505371, 'accumulated_submission_time': 484.717246055603, 'accumulated_eval_time': 269.8623342514038, 'accumulated_logging_time': 0.05006265640258789}
I0518 09:41:57.639590 139820327835392 logging_writer.py:48] [1673] accumulated_eval_time=269.862334, accumulated_logging_time=0.050063, accumulated_submission_time=484.717246, global_step=1673, preemption_count=0, score=484.717246, test/accuracy=0.983144, test/loss=0.065621, test/mean_average_precision=0.048392, test/num_examples=43793, total_duration=755.012783, train/accuracy=0.986769, train/loss=0.052699, train/mean_average_precision=0.046823, validation/accuracy=0.984127, validation/loss=0.062423, validation/mean_average_precision=0.047887, validation/num_examples=43793
I0518 09:43:32.519991 139820319442688 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.025599, loss=0.053555
I0518 09:43:32.525428 139872280180544 submission.py:139] 2000) loss = 0.054, grad_norm = 0.026
I0518 09:45:55.543450 139820327835392 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.061311, loss=0.053601
I0518 09:45:55.549489 139872280180544 submission.py:139] 2500) loss = 0.054, grad_norm = 0.061
I0518 09:45:57.796371 139872280180544 spec.py:298] Evaluating on the training split.
I0518 09:46:55.577531 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 09:46:58.788800 139872280180544 spec.py:326] Evaluating on the test split.
I0518 09:47:01.933856 139872280180544 submission_runner.py:421] Time since start: 1059.32s, 	Step: 2509, 	{'train/accuracy': 0.9867582735667144, 'train/loss': 0.05243701844998382, 'train/mean_average_precision': 0.06413406124014626, 'validation/accuracy': 0.9841183820502766, 'validation/loss': 0.062242157885357474, 'validation/mean_average_precision': 0.060360948102757925, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06571811824745946, 'test/mean_average_precision': 0.060778327217722014, 'test/num_examples': 43793, 'score': 724.6784598827362, 'total_duration': 1059.317280292511, 'accumulated_submission_time': 724.6784598827362, 'accumulated_eval_time': 333.9994766712189, 'accumulated_logging_time': 0.07034087181091309}
I0518 09:47:01.946951 139820319442688 logging_writer.py:48] [2509] accumulated_eval_time=333.999477, accumulated_logging_time=0.070341, accumulated_submission_time=724.678460, global_step=2509, preemption_count=0, score=724.678460, test/accuracy=0.983142, test/loss=0.065718, test/mean_average_precision=0.060778, test/num_examples=43793, total_duration=1059.317280, train/accuracy=0.986758, train/loss=0.052437, train/mean_average_precision=0.064134, validation/accuracy=0.984118, validation/loss=0.062242, validation/mean_average_precision=0.060361, validation/num_examples=43793
I0518 09:49:23.870913 139820327835392 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.050859, loss=0.048750
I0518 09:49:23.875817 139872280180544 submission.py:139] 3000) loss = 0.049, grad_norm = 0.051
I0518 09:51:02.184305 139872280180544 spec.py:298] Evaluating on the training split.
I0518 09:52:00.891323 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 09:52:04.198477 139872280180544 spec.py:326] Evaluating on the test split.
I0518 09:52:07.403947 139872280180544 submission_runner.py:421] Time since start: 1364.79s, 	Step: 3342, 	{'train/accuracy': 0.9870400245228556, 'train/loss': 0.04879446486924091, 'train/mean_average_precision': 0.0860268404232141, 'validation/accuracy': 0.9842994316018339, 'validation/loss': 0.057942256407165015, 'validation/mean_average_precision': 0.08894718267820631, 'validation/num_examples': 43793, 'test/accuracy': 0.983319848083501, 'test/loss': 0.06095792070294023, 'test/mean_average_precision': 0.08789856903213467, 'test/num_examples': 43793, 'score': 964.7144048213959, 'total_duration': 1364.7873787879944, 'accumulated_submission_time': 964.7144048213959, 'accumulated_eval_time': 399.2188060283661, 'accumulated_logging_time': 0.09545230865478516}
I0518 09:52:07.414059 139820319442688 logging_writer.py:48] [3342] accumulated_eval_time=399.218806, accumulated_logging_time=0.095452, accumulated_submission_time=964.714405, global_step=3342, preemption_count=0, score=964.714405, test/accuracy=0.983320, test/loss=0.060958, test/mean_average_precision=0.087899, test/num_examples=43793, total_duration=1364.787379, train/accuracy=0.987040, train/loss=0.048794, train/mean_average_precision=0.086027, validation/accuracy=0.984299, validation/loss=0.057942, validation/mean_average_precision=0.088947, validation/num_examples=43793
I0518 09:52:53.530443 139820327835392 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.050771, loss=0.050712
I0518 09:52:53.536619 139872280180544 submission.py:139] 3500) loss = 0.051, grad_norm = 0.051
I0518 09:55:17.440604 139820319442688 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.050568, loss=0.052861
I0518 09:55:17.446321 139872280180544 submission.py:139] 4000) loss = 0.053, grad_norm = 0.051
I0518 09:56:07.463036 139872280180544 spec.py:298] Evaluating on the training split.
I0518 09:57:06.632245 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 09:57:09.919300 139872280180544 spec.py:326] Evaluating on the test split.
I0518 09:57:13.139825 139872280180544 submission_runner.py:421] Time since start: 1670.52s, 	Step: 4171, 	{'train/accuracy': 0.9872188114887477, 'train/loss': 0.04737489157590859, 'train/mean_average_precision': 0.105489043893314, 'validation/accuracy': 0.9845722237512655, 'validation/loss': 0.055665506285179835, 'validation/mean_average_precision': 0.10254631663519151, 'validation/num_examples': 43793, 'test/accuracy': 0.983557401885857, 'test/loss': 0.05845996639287608, 'test/mean_average_precision': 0.10525694450887044, 'test/num_examples': 43793, 'score': 1204.5502247810364, 'total_duration': 1670.5232634544373, 'accumulated_submission_time': 1204.5502247810364, 'accumulated_eval_time': 464.89529633522034, 'accumulated_logging_time': 0.12102150917053223}
I0518 09:57:13.150155 139820327835392 logging_writer.py:48] [4171] accumulated_eval_time=464.895296, accumulated_logging_time=0.121022, accumulated_submission_time=1204.550225, global_step=4171, preemption_count=0, score=1204.550225, test/accuracy=0.983557, test/loss=0.058460, test/mean_average_precision=0.105257, test/num_examples=43793, total_duration=1670.523263, train/accuracy=0.987219, train/loss=0.047375, train/mean_average_precision=0.105489, validation/accuracy=0.984572, validation/loss=0.055666, validation/mean_average_precision=0.102546, validation/num_examples=43793
I0518 09:58:50.973550 139820319442688 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.053986, loss=0.043759
I0518 09:58:50.980856 139872280180544 submission.py:139] 4500) loss = 0.044, grad_norm = 0.054
I0518 10:01:13.170355 139872280180544 spec.py:298] Evaluating on the training split.
I0518 10:02:12.033809 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 10:02:15.272460 139872280180544 spec.py:326] Evaluating on the test split.
I0518 10:02:18.511124 139872280180544 submission_runner.py:421] Time since start: 1975.89s, 	Step: 4982, 	{'train/accuracy': 0.9874633320020383, 'train/loss': 0.0447098358768939, 'train/mean_average_precision': 0.1294173515708741, 'validation/accuracy': 0.9847406891411675, 'validation/loss': 0.0540827308158515, 'validation/mean_average_precision': 0.12215870315918433, 'validation/num_examples': 43793, 'test/accuracy': 0.9837852682104575, 'test/loss': 0.05700822019342102, 'test/mean_average_precision': 0.1257197884592373, 'test/num_examples': 43793, 'score': 1444.3751084804535, 'total_duration': 1975.8945398330688, 'accumulated_submission_time': 1444.3751084804535, 'accumulated_eval_time': 530.2357382774353, 'accumulated_logging_time': 0.14200258255004883}
I0518 10:02:18.522870 139820327835392 logging_writer.py:48] [4982] accumulated_eval_time=530.235738, accumulated_logging_time=0.142003, accumulated_submission_time=1444.375108, global_step=4982, preemption_count=0, score=1444.375108, test/accuracy=0.983785, test/loss=0.057008, test/mean_average_precision=0.125720, test/num_examples=43793, total_duration=1975.894540, train/accuracy=0.987463, train/loss=0.044710, train/mean_average_precision=0.129417, validation/accuracy=0.984741, validation/loss=0.054083, validation/mean_average_precision=0.122159, validation/num_examples=43793
I0518 10:02:24.104004 139820319442688 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.131475, loss=0.046013
I0518 10:02:24.109059 139872280180544 submission.py:139] 5000) loss = 0.046, grad_norm = 0.131
I0518 10:04:48.216636 139820327835392 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.095029, loss=0.049792
I0518 10:04:48.226741 139872280180544 submission.py:139] 5500) loss = 0.050, grad_norm = 0.095
I0518 10:06:18.596804 139872280180544 spec.py:298] Evaluating on the training split.
I0518 10:07:19.171077 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 10:07:22.387806 139872280180544 spec.py:326] Evaluating on the test split.
I0518 10:07:25.556539 139872280180544 submission_runner.py:421] Time since start: 2282.94s, 	Step: 5812, 	{'train/accuracy': 0.9877121649429381, 'train/loss': 0.0438410793018044, 'train/mean_average_precision': 0.14547120857986157, 'validation/accuracy': 0.984961114940485, 'validation/loss': 0.053366448453244154, 'validation/mean_average_precision': 0.13602719649624892, 'validation/num_examples': 43793, 'test/accuracy': 0.9840278763490339, 'test/loss': 0.056377059957905803, 'test/mean_average_precision': 0.1402140081599344, 'test/num_examples': 43793, 'score': 1684.2420589923859, 'total_duration': 2282.9400720596313, 'accumulated_submission_time': 1684.2420589923859, 'accumulated_eval_time': 597.1952545642853, 'accumulated_logging_time': 0.17023658752441406}
I0518 10:07:25.567495 139820319442688 logging_writer.py:48] [5812] accumulated_eval_time=597.195255, accumulated_logging_time=0.170237, accumulated_submission_time=1684.242059, global_step=5812, preemption_count=0, score=1684.242059, test/accuracy=0.984028, test/loss=0.056377, test/mean_average_precision=0.140214, test/num_examples=43793, total_duration=2282.940072, train/accuracy=0.987712, train/loss=0.043841, train/mean_average_precision=0.145471, validation/accuracy=0.984961, validation/loss=0.053366, validation/mean_average_precision=0.136027, validation/num_examples=43793
I0518 10:08:20.830940 139820327835392 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.065308, loss=0.048186
I0518 10:08:20.836457 139872280180544 submission.py:139] 6000) loss = 0.048, grad_norm = 0.065
I0518 10:10:43.979186 139820319442688 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.056523, loss=0.045888
I0518 10:10:43.986002 139872280180544 submission.py:139] 6500) loss = 0.046, grad_norm = 0.057
I0518 10:11:25.841318 139872280180544 spec.py:298] Evaluating on the training split.
I0518 10:12:24.250518 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 10:12:27.501503 139872280180544 spec.py:326] Evaluating on the test split.
I0518 10:12:30.648468 139872280180544 submission_runner.py:421] Time since start: 2588.03s, 	Step: 6649, 	{'train/accuracy': 0.9876046404070751, 'train/loss': 0.043292003316096644, 'train/mean_average_precision': 0.1599105029099629, 'validation/accuracy': 0.9848754614530891, 'validation/loss': 0.05346877088057468, 'validation/mean_average_precision': 0.14742932466460335, 'validation/num_examples': 43793, 'test/accuracy': 0.9838551865281723, 'test/loss': 0.05655993742521162, 'test/mean_average_precision': 0.15023145167926424, 'test/num_examples': 43793, 'score': 1924.3082604408264, 'total_duration': 2588.032000541687, 'accumulated_submission_time': 1924.3082604408264, 'accumulated_eval_time': 662.0021715164185, 'accumulated_logging_time': 0.19210124015808105}
I0518 10:12:30.658626 139820327835392 logging_writer.py:48] [6649] accumulated_eval_time=662.002172, accumulated_logging_time=0.192101, accumulated_submission_time=1924.308260, global_step=6649, preemption_count=0, score=1924.308260, test/accuracy=0.983855, test/loss=0.056560, test/mean_average_precision=0.150231, test/num_examples=43793, total_duration=2588.032001, train/accuracy=0.987605, train/loss=0.043292, train/mean_average_precision=0.159911, validation/accuracy=0.984875, validation/loss=0.053469, validation/mean_average_precision=0.147429, validation/num_examples=43793
I0518 10:14:09.267833 139820319442688 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.057822, loss=0.043299
I0518 10:14:09.273043 139872280180544 submission.py:139] 7000) loss = 0.043, grad_norm = 0.058
I0518 10:16:29.648566 139820327835392 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.068420, loss=0.043937
I0518 10:16:29.654093 139872280180544 submission.py:139] 7500) loss = 0.044, grad_norm = 0.068
I0518 10:16:30.765686 139872280180544 spec.py:298] Evaluating on the training split.
I0518 10:17:29.805705 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 10:17:33.023352 139872280180544 spec.py:326] Evaluating on the test split.
I0518 10:17:36.195662 139872280180544 submission_runner.py:421] Time since start: 2893.58s, 	Step: 7505, 	{'train/accuracy': 0.9882859295424726, 'train/loss': 0.041287008495897615, 'train/mean_average_precision': 0.18750820825094477, 'validation/accuracy': 0.9854023724798187, 'validation/loss': 0.0509675418951098, 'validation/mean_average_precision': 0.16209579829971324, 'validation/num_examples': 43793, 'test/accuracy': 0.9844212721848505, 'test/loss': 0.05368438208001941, 'test/mean_average_precision': 0.1670139071696798, 'test/num_examples': 43793, 'score': 2164.20805478096, 'total_duration': 2893.579170227051, 'accumulated_submission_time': 2164.20805478096, 'accumulated_eval_time': 727.431880235672, 'accumulated_logging_time': 0.21345162391662598}
I0518 10:17:36.205903 139820319442688 logging_writer.py:48] [7505] accumulated_eval_time=727.431880, accumulated_logging_time=0.213452, accumulated_submission_time=2164.208055, global_step=7505, preemption_count=0, score=2164.208055, test/accuracy=0.984421, test/loss=0.053684, test/mean_average_precision=0.167014, test/num_examples=43793, total_duration=2893.579170, train/accuracy=0.988286, train/loss=0.041287, train/mean_average_precision=0.187508, validation/accuracy=0.985402, validation/loss=0.050968, validation/mean_average_precision=0.162096, validation/num_examples=43793
I0518 10:19:56.262380 139820327835392 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.047815, loss=0.040371
I0518 10:19:56.268397 139872280180544 submission.py:139] 8000) loss = 0.040, grad_norm = 0.048
I0518 10:21:36.337678 139872280180544 spec.py:298] Evaluating on the training split.
I0518 10:22:36.457551 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 10:22:39.725616 139872280180544 spec.py:326] Evaluating on the test split.
I0518 10:22:42.948349 139872280180544 submission_runner.py:421] Time since start: 3200.33s, 	Step: 8359, 	{'train/accuracy': 0.988340576368241, 'train/loss': 0.040314692782254066, 'train/mean_average_precision': 0.19501798174955387, 'validation/accuracy': 0.9852814021516481, 'validation/loss': 0.05104111864672361, 'validation/mean_average_precision': 0.16927262747116706, 'validation/num_examples': 43793, 'test/accuracy': 0.9843239762126089, 'test/loss': 0.05379884173567591, 'test/mean_average_precision': 0.17588592502928443, 'test/num_examples': 43793, 'score': 2404.132672071457, 'total_duration': 3200.3318355083466, 'accumulated_submission_time': 2404.132672071457, 'accumulated_eval_time': 794.0422923564911, 'accumulated_logging_time': 0.23519563674926758}
I0518 10:22:42.958466 139820319442688 logging_writer.py:48] [8359] accumulated_eval_time=794.042292, accumulated_logging_time=0.235196, accumulated_submission_time=2404.132672, global_step=8359, preemption_count=0, score=2404.132672, test/accuracy=0.984324, test/loss=0.053799, test/mean_average_precision=0.175886, test/num_examples=43793, total_duration=3200.331836, train/accuracy=0.988341, train/loss=0.040315, train/mean_average_precision=0.195018, validation/accuracy=0.985281, validation/loss=0.051041, validation/mean_average_precision=0.169273, validation/num_examples=43793
I0518 10:23:22.764934 139820327835392 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.135372, loss=0.049222
I0518 10:23:22.770750 139872280180544 submission.py:139] 8500) loss = 0.049, grad_norm = 0.135
I0518 10:25:43.926288 139820319442688 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.048917, loss=0.044673
I0518 10:25:43.931726 139872280180544 submission.py:139] 9000) loss = 0.045, grad_norm = 0.049
I0518 10:26:43.076691 139872280180544 spec.py:298] Evaluating on the training split.
I0518 10:27:41.808988 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 10:27:45.107956 139872280180544 spec.py:326] Evaluating on the test split.
I0518 10:27:48.369489 139872280180544 submission_runner.py:421] Time since start: 3505.75s, 	Step: 9212, 	{'train/accuracy': 0.9882355306884585, 'train/loss': 0.03986225565931595, 'train/mean_average_precision': 0.21314866681569972, 'validation/accuracy': 0.9853954714879432, 'validation/loss': 0.049948430942789154, 'validation/mean_average_precision': 0.1810416731479826, 'validation/num_examples': 43793, 'test/accuracy': 0.9844065303708746, 'test/loss': 0.05277847128652653, 'test/mean_average_precision': 0.182624764729027, 'test/num_examples': 43793, 'score': 2644.0491335392, 'total_duration': 3505.7529530525208, 'accumulated_submission_time': 2644.0491335392, 'accumulated_eval_time': 859.3347923755646, 'accumulated_logging_time': 0.2570321559906006}
I0518 10:27:48.380122 139820327835392 logging_writer.py:48] [9212] accumulated_eval_time=859.334792, accumulated_logging_time=0.257032, accumulated_submission_time=2644.049134, global_step=9212, preemption_count=0, score=2644.049134, test/accuracy=0.984407, test/loss=0.052778, test/mean_average_precision=0.182625, test/num_examples=43793, total_duration=3505.752953, train/accuracy=0.988236, train/loss=0.039862, train/mean_average_precision=0.213149, validation/accuracy=0.985395, validation/loss=0.049948, validation/mean_average_precision=0.181042, validation/num_examples=43793
I0518 10:29:11.044698 139820319442688 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.060146, loss=0.041776
I0518 10:29:11.050384 139872280180544 submission.py:139] 9500) loss = 0.042, grad_norm = 0.060
I0518 10:31:33.469145 139820327835392 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.040839, loss=0.038683
I0518 10:31:33.474894 139872280180544 submission.py:139] 10000) loss = 0.039, grad_norm = 0.041
I0518 10:31:48.436697 139872280180544 spec.py:298] Evaluating on the training split.
I0518 10:32:47.239188 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 10:32:50.511759 139872280180544 spec.py:326] Evaluating on the test split.
I0518 10:32:53.720021 139872280180544 submission_runner.py:421] Time since start: 3811.10s, 	Step: 10054, 	{'train/accuracy': 0.9882225377879974, 'train/loss': 0.040023435891446246, 'train/mean_average_precision': 0.2104194816731944, 'validation/accuracy': 0.9853179368145184, 'validation/loss': 0.05071805645031651, 'validation/mean_average_precision': 0.18403249518551285, 'validation/num_examples': 43793, 'test/accuracy': 0.984384207052568, 'test/loss': 0.05367819907366653, 'test/mean_average_precision': 0.18678753101795395, 'test/num_examples': 43793, 'score': 2883.907247543335, 'total_duration': 3811.1034688949585, 'accumulated_submission_time': 2883.907247543335, 'accumulated_eval_time': 924.6177878379822, 'accumulated_logging_time': 0.279191255569458}
I0518 10:32:53.730402 139820319442688 logging_writer.py:48] [10054] accumulated_eval_time=924.617788, accumulated_logging_time=0.279191, accumulated_submission_time=2883.907248, global_step=10054, preemption_count=0, score=2883.907248, test/accuracy=0.984384, test/loss=0.053678, test/mean_average_precision=0.186788, test/num_examples=43793, total_duration=3811.103469, train/accuracy=0.988223, train/loss=0.040023, train/mean_average_precision=0.210419, validation/accuracy=0.985318, validation/loss=0.050718, validation/mean_average_precision=0.184032, validation/num_examples=43793
I0518 10:34:59.889419 139820327835392 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.043403, loss=0.042133
I0518 10:34:59.896094 139872280180544 submission.py:139] 10500) loss = 0.042, grad_norm = 0.043
I0518 10:36:53.977831 139872280180544 spec.py:298] Evaluating on the training split.
I0518 10:37:53.774043 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 10:37:57.022412 139872280180544 spec.py:326] Evaluating on the test split.
I0518 10:38:00.230949 139872280180544 submission_runner.py:421] Time since start: 4117.61s, 	Step: 10903, 	{'train/accuracy': 0.988607276459881, 'train/loss': 0.03866420316492104, 'train/mean_average_precision': 0.23385145488917208, 'validation/accuracy': 0.9856337586779973, 'validation/loss': 0.048998390166045985, 'validation/mean_average_precision': 0.1923604083600845, 'validation/num_examples': 43793, 'test/accuracy': 0.9847207416059058, 'test/loss': 0.05184103999390952, 'test/mean_average_precision': 0.18887786769263512, 'test/num_examples': 43793, 'score': 3123.951404094696, 'total_duration': 4117.614449262619, 'accumulated_submission_time': 3123.951404094696, 'accumulated_eval_time': 990.8706452846527, 'accumulated_logging_time': 0.30504512786865234}
I0518 10:38:00.241686 139820319442688 logging_writer.py:48] [10903] accumulated_eval_time=990.870645, accumulated_logging_time=0.305045, accumulated_submission_time=3123.951404, global_step=10903, preemption_count=0, score=3123.951404, test/accuracy=0.984721, test/loss=0.051841, test/mean_average_precision=0.188878, test/num_examples=43793, total_duration=4117.614449, train/accuracy=0.988607, train/loss=0.038664, train/mean_average_precision=0.233851, validation/accuracy=0.985634, validation/loss=0.048998, validation/mean_average_precision=0.192360, validation/num_examples=43793
I0518 10:38:28.260502 139820327835392 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.041621, loss=0.038950
I0518 10:38:28.265595 139872280180544 submission.py:139] 11000) loss = 0.039, grad_norm = 0.042
I0518 10:40:52.289616 139820319442688 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.051584, loss=0.043687
I0518 10:40:52.295820 139872280180544 submission.py:139] 11500) loss = 0.044, grad_norm = 0.052
I0518 10:42:00.386159 139872280180544 spec.py:298] Evaluating on the training split.
I0518 10:43:00.038238 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 10:43:03.397578 139872280180544 spec.py:326] Evaluating on the test split.
I0518 10:43:06.616579 139872280180544 submission_runner.py:421] Time since start: 4424.00s, 	Step: 11739, 	{'train/accuracy': 0.9887759048427873, 'train/loss': 0.0384131367899694, 'train/mean_average_precision': 0.23754286781334444, 'validation/accuracy': 0.9857295606828572, 'validation/loss': 0.04883898549939231, 'validation/mean_average_precision': 0.2004123086723081, 'validation/num_examples': 43793, 'test/accuracy': 0.9847658094372038, 'test/loss': 0.05166002170626809, 'test/mean_average_precision': 0.19918170093638066, 'test/num_examples': 43793, 'score': 3363.8976991176605, 'total_duration': 4424.000096082687, 'accumulated_submission_time': 3363.8976991176605, 'accumulated_eval_time': 1057.100821018219, 'accumulated_logging_time': 0.326582670211792}
I0518 10:43:06.626968 139820327835392 logging_writer.py:48] [11739] accumulated_eval_time=1057.100821, accumulated_logging_time=0.326583, accumulated_submission_time=3363.897699, global_step=11739, preemption_count=0, score=3363.897699, test/accuracy=0.984766, test/loss=0.051660, test/mean_average_precision=0.199182, test/num_examples=43793, total_duration=4424.000096, train/accuracy=0.988776, train/loss=0.038413, train/mean_average_precision=0.237543, validation/accuracy=0.985730, validation/loss=0.048839, validation/mean_average_precision=0.200412, validation/num_examples=43793
I0518 10:44:19.503309 139872280180544 spec.py:298] Evaluating on the training split.
I0518 10:45:18.473589 139872280180544 spec.py:310] Evaluating on the validation split.
I0518 10:45:21.893260 139872280180544 spec.py:326] Evaluating on the test split.
I0518 10:45:25.228548 139872280180544 submission_runner.py:421] Time since start: 4562.61s, 	Step: 12000, 	{'train/accuracy': 0.9886014460097885, 'train/loss': 0.039079544350801694, 'train/mean_average_precision': 0.24008290336649407, 'validation/accuracy': 0.985628481448916, 'validation/loss': 0.04982659481922243, 'validation/mean_average_precision': 0.20241154066827546, 'validation/num_examples': 43793, 'test/accuracy': 0.9846693558543324, 'test/loss': 0.052698779935885745, 'test/mean_average_precision': 0.19988009992418548, 'test/num_examples': 43793, 'score': 3436.704211473465, 'total_duration': 4562.611914157867, 'accumulated_submission_time': 3436.704211473465, 'accumulated_eval_time': 1122.825663805008, 'accumulated_logging_time': 0.34885191917419434}
I0518 10:45:25.241231 139820319442688 logging_writer.py:48] [12000] accumulated_eval_time=1122.825664, accumulated_logging_time=0.348852, accumulated_submission_time=3436.704211, global_step=12000, preemption_count=0, score=3436.704211, test/accuracy=0.984669, test/loss=0.052699, test/mean_average_precision=0.199880, test/num_examples=43793, total_duration=4562.611914, train/accuracy=0.988601, train/loss=0.039080, train/mean_average_precision=0.240083, validation/accuracy=0.985628, validation/loss=0.049827, validation/mean_average_precision=0.202412, validation/num_examples=43793
I0518 10:45:25.262226 139820327835392 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3436.704211
I0518 10:45:25.327291 139872280180544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_pytorch/timing_momentum/ogbg_pytorch/trial_1/checkpoint_12000.
I0518 10:45:25.523779 139872280180544 submission_runner.py:584] Tuning trial 1/1
I0518 10:45:25.524059 139872280180544 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0518 10:45:25.525637 139872280180544 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/accuracy': 0.48072310275931274, 'train/loss': 0.7473357462523087, 'train/mean_average_precision': 0.02194968160100208, 'validation/accuracy': 0.4807768811900882, 'validation/loss': 0.7464825745895737, 'validation/mean_average_precision': 0.026760981165532393, 'validation/num_examples': 43793, 'test/accuracy': 0.4792677446161842, 'test/loss': 0.7473922784063173, 'test/mean_average_precision': 0.02790392316762357, 'test/num_examples': 43793, 'score': 5.030078649520874, 'total_duration': 148.87319612503052, 'accumulated_submission_time': 5.030078649520874, 'accumulated_eval_time': 143.84181356430054, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (834, {'train/accuracy': 0.9868241790536274, 'train/loss': 0.055029201383793636, 'train/mean_average_precision': 0.031096511553329195, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06524455872013393, 'validation/mean_average_precision': 0.03395175465061206, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06858320953719549, 'test/mean_average_precision': 0.034527050956562104, 'test/num_examples': 43793, 'score': 244.87157011032104, 'total_duration': 450.95229601860046, 'accumulated_submission_time': 244.87157011032104, 'accumulated_eval_time': 205.85320496559143, 'accumulated_logging_time': 0.029907703399658203, 'global_step': 834, 'preemption_count': 0}), (1673, {'train/accuracy': 0.9867694011058111, 'train/loss': 0.05269909591136991, 'train/mean_average_precision': 0.046822941243029645, 'validation/accuracy': 0.9841273127456449, 'validation/loss': 0.06242290298139087, 'validation/mean_average_precision': 0.047886895898442305, 'validation/num_examples': 43793, 'test/accuracy': 0.9831442098998441, 'test/loss': 0.06562146064841237, 'test/mean_average_precision': 0.048392161488527076, 'test/num_examples': 43793, 'score': 484.717246055603, 'total_duration': 755.0127830505371, 'accumulated_submission_time': 484.717246055603, 'accumulated_eval_time': 269.8623342514038, 'accumulated_logging_time': 0.05006265640258789, 'global_step': 1673, 'preemption_count': 0}), (2509, {'train/accuracy': 0.9867582735667144, 'train/loss': 0.05243701844998382, 'train/mean_average_precision': 0.06413406124014626, 'validation/accuracy': 0.9841183820502766, 'validation/loss': 0.062242157885357474, 'validation/mean_average_precision': 0.060360948102757925, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06571811824745946, 'test/mean_average_precision': 0.060778327217722014, 'test/num_examples': 43793, 'score': 724.6784598827362, 'total_duration': 1059.317280292511, 'accumulated_submission_time': 724.6784598827362, 'accumulated_eval_time': 333.9994766712189, 'accumulated_logging_time': 0.07034087181091309, 'global_step': 2509, 'preemption_count': 0}), (3342, {'train/accuracy': 0.9870400245228556, 'train/loss': 0.04879446486924091, 'train/mean_average_precision': 0.0860268404232141, 'validation/accuracy': 0.9842994316018339, 'validation/loss': 0.057942256407165015, 'validation/mean_average_precision': 0.08894718267820631, 'validation/num_examples': 43793, 'test/accuracy': 0.983319848083501, 'test/loss': 0.06095792070294023, 'test/mean_average_precision': 0.08789856903213467, 'test/num_examples': 43793, 'score': 964.7144048213959, 'total_duration': 1364.7873787879944, 'accumulated_submission_time': 964.7144048213959, 'accumulated_eval_time': 399.2188060283661, 'accumulated_logging_time': 0.09545230865478516, 'global_step': 3342, 'preemption_count': 0}), (4171, {'train/accuracy': 0.9872188114887477, 'train/loss': 0.04737489157590859, 'train/mean_average_precision': 0.105489043893314, 'validation/accuracy': 0.9845722237512655, 'validation/loss': 0.055665506285179835, 'validation/mean_average_precision': 0.10254631663519151, 'validation/num_examples': 43793, 'test/accuracy': 0.983557401885857, 'test/loss': 0.05845996639287608, 'test/mean_average_precision': 0.10525694450887044, 'test/num_examples': 43793, 'score': 1204.5502247810364, 'total_duration': 1670.5232634544373, 'accumulated_submission_time': 1204.5502247810364, 'accumulated_eval_time': 464.89529633522034, 'accumulated_logging_time': 0.12102150917053223, 'global_step': 4171, 'preemption_count': 0}), (4982, {'train/accuracy': 0.9874633320020383, 'train/loss': 0.0447098358768939, 'train/mean_average_precision': 0.1294173515708741, 'validation/accuracy': 0.9847406891411675, 'validation/loss': 0.0540827308158515, 'validation/mean_average_precision': 0.12215870315918433, 'validation/num_examples': 43793, 'test/accuracy': 0.9837852682104575, 'test/loss': 0.05700822019342102, 'test/mean_average_precision': 0.1257197884592373, 'test/num_examples': 43793, 'score': 1444.3751084804535, 'total_duration': 1975.8945398330688, 'accumulated_submission_time': 1444.3751084804535, 'accumulated_eval_time': 530.2357382774353, 'accumulated_logging_time': 0.14200258255004883, 'global_step': 4982, 'preemption_count': 0}), (5812, {'train/accuracy': 0.9877121649429381, 'train/loss': 0.0438410793018044, 'train/mean_average_precision': 0.14547120857986157, 'validation/accuracy': 0.984961114940485, 'validation/loss': 0.053366448453244154, 'validation/mean_average_precision': 0.13602719649624892, 'validation/num_examples': 43793, 'test/accuracy': 0.9840278763490339, 'test/loss': 0.056377059957905803, 'test/mean_average_precision': 0.1402140081599344, 'test/num_examples': 43793, 'score': 1684.2420589923859, 'total_duration': 2282.9400720596313, 'accumulated_submission_time': 1684.2420589923859, 'accumulated_eval_time': 597.1952545642853, 'accumulated_logging_time': 0.17023658752441406, 'global_step': 5812, 'preemption_count': 0}), (6649, {'train/accuracy': 0.9876046404070751, 'train/loss': 0.043292003316096644, 'train/mean_average_precision': 0.1599105029099629, 'validation/accuracy': 0.9848754614530891, 'validation/loss': 0.05346877088057468, 'validation/mean_average_precision': 0.14742932466460335, 'validation/num_examples': 43793, 'test/accuracy': 0.9838551865281723, 'test/loss': 0.05655993742521162, 'test/mean_average_precision': 0.15023145167926424, 'test/num_examples': 43793, 'score': 1924.3082604408264, 'total_duration': 2588.032000541687, 'accumulated_submission_time': 1924.3082604408264, 'accumulated_eval_time': 662.0021715164185, 'accumulated_logging_time': 0.19210124015808105, 'global_step': 6649, 'preemption_count': 0}), (7505, {'train/accuracy': 0.9882859295424726, 'train/loss': 0.041287008495897615, 'train/mean_average_precision': 0.18750820825094477, 'validation/accuracy': 0.9854023724798187, 'validation/loss': 0.0509675418951098, 'validation/mean_average_precision': 0.16209579829971324, 'validation/num_examples': 43793, 'test/accuracy': 0.9844212721848505, 'test/loss': 0.05368438208001941, 'test/mean_average_precision': 0.1670139071696798, 'test/num_examples': 43793, 'score': 2164.20805478096, 'total_duration': 2893.579170227051, 'accumulated_submission_time': 2164.20805478096, 'accumulated_eval_time': 727.431880235672, 'accumulated_logging_time': 0.21345162391662598, 'global_step': 7505, 'preemption_count': 0}), (8359, {'train/accuracy': 0.988340576368241, 'train/loss': 0.040314692782254066, 'train/mean_average_precision': 0.19501798174955387, 'validation/accuracy': 0.9852814021516481, 'validation/loss': 0.05104111864672361, 'validation/mean_average_precision': 0.16927262747116706, 'validation/num_examples': 43793, 'test/accuracy': 0.9843239762126089, 'test/loss': 0.05379884173567591, 'test/mean_average_precision': 0.17588592502928443, 'test/num_examples': 43793, 'score': 2404.132672071457, 'total_duration': 3200.3318355083466, 'accumulated_submission_time': 2404.132672071457, 'accumulated_eval_time': 794.0422923564911, 'accumulated_logging_time': 0.23519563674926758, 'global_step': 8359, 'preemption_count': 0}), (9212, {'train/accuracy': 0.9882355306884585, 'train/loss': 0.03986225565931595, 'train/mean_average_precision': 0.21314866681569972, 'validation/accuracy': 0.9853954714879432, 'validation/loss': 0.049948430942789154, 'validation/mean_average_precision': 0.1810416731479826, 'validation/num_examples': 43793, 'test/accuracy': 0.9844065303708746, 'test/loss': 0.05277847128652653, 'test/mean_average_precision': 0.182624764729027, 'test/num_examples': 43793, 'score': 2644.0491335392, 'total_duration': 3505.7529530525208, 'accumulated_submission_time': 2644.0491335392, 'accumulated_eval_time': 859.3347923755646, 'accumulated_logging_time': 0.2570321559906006, 'global_step': 9212, 'preemption_count': 0}), (10054, {'train/accuracy': 0.9882225377879974, 'train/loss': 0.040023435891446246, 'train/mean_average_precision': 0.2104194816731944, 'validation/accuracy': 0.9853179368145184, 'validation/loss': 0.05071805645031651, 'validation/mean_average_precision': 0.18403249518551285, 'validation/num_examples': 43793, 'test/accuracy': 0.984384207052568, 'test/loss': 0.05367819907366653, 'test/mean_average_precision': 0.18678753101795395, 'test/num_examples': 43793, 'score': 2883.907247543335, 'total_duration': 3811.1034688949585, 'accumulated_submission_time': 2883.907247543335, 'accumulated_eval_time': 924.6177878379822, 'accumulated_logging_time': 0.279191255569458, 'global_step': 10054, 'preemption_count': 0}), (10903, {'train/accuracy': 0.988607276459881, 'train/loss': 0.03866420316492104, 'train/mean_average_precision': 0.23385145488917208, 'validation/accuracy': 0.9856337586779973, 'validation/loss': 0.048998390166045985, 'validation/mean_average_precision': 0.1923604083600845, 'validation/num_examples': 43793, 'test/accuracy': 0.9847207416059058, 'test/loss': 0.05184103999390952, 'test/mean_average_precision': 0.18887786769263512, 'test/num_examples': 43793, 'score': 3123.951404094696, 'total_duration': 4117.614449262619, 'accumulated_submission_time': 3123.951404094696, 'accumulated_eval_time': 990.8706452846527, 'accumulated_logging_time': 0.30504512786865234, 'global_step': 10903, 'preemption_count': 0}), (11739, {'train/accuracy': 0.9887759048427873, 'train/loss': 0.0384131367899694, 'train/mean_average_precision': 0.23754286781334444, 'validation/accuracy': 0.9857295606828572, 'validation/loss': 0.04883898549939231, 'validation/mean_average_precision': 0.2004123086723081, 'validation/num_examples': 43793, 'test/accuracy': 0.9847658094372038, 'test/loss': 0.05166002170626809, 'test/mean_average_precision': 0.19918170093638066, 'test/num_examples': 43793, 'score': 3363.8976991176605, 'total_duration': 4424.000096082687, 'accumulated_submission_time': 3363.8976991176605, 'accumulated_eval_time': 1057.100821018219, 'accumulated_logging_time': 0.326582670211792, 'global_step': 11739, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9886014460097885, 'train/loss': 0.039079544350801694, 'train/mean_average_precision': 0.24008290336649407, 'validation/accuracy': 0.985628481448916, 'validation/loss': 0.04982659481922243, 'validation/mean_average_precision': 0.20241154066827546, 'validation/num_examples': 43793, 'test/accuracy': 0.9846693558543324, 'test/loss': 0.052698779935885745, 'test/mean_average_precision': 0.19988009992418548, 'test/num_examples': 43793, 'score': 3436.704211473465, 'total_duration': 4562.611914157867, 'accumulated_submission_time': 3436.704211473465, 'accumulated_eval_time': 1122.825663805008, 'accumulated_logging_time': 0.34885191917419434, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0518 10:45:25.525805 139872280180544 submission_runner.py:587] Timing: 3436.704211473465
I0518 10:45:25.525857 139872280180544 submission_runner.py:588] ====================
I0518 10:45:25.525992 139872280180544 submission_runner.py:651] Final ogbg score: 3436.704211473465
