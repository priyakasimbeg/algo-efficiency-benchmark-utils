python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_prelaunch_jax/adamw --overwrite=true --save_checkpoints=false --max_global_steps=14000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_09-26-2023-19-19-14.log
2023-09-26 19:19:19.217857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0926 19:19:38.235800 140074027902784 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_prelaunch_jax/adamw/imagenet_resnet_jax.
I0926 19:19:39.167155 140074027902784 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0926 19:19:39.167867 140074027902784 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0926 19:19:39.168008 140074027902784 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0926 19:19:39.174272 140074027902784 submission_runner.py:507] Using RNG seed 2288850919
I0926 19:19:44.586556 140074027902784 submission_runner.py:516] --- Tuning run 1/1 ---
I0926 19:19:44.586763 140074027902784 submission_runner.py:521] Creating tuning directory at /experiment_runs/timing_prelaunch_jax/adamw/imagenet_resnet_jax/trial_1.
I0926 19:19:44.586940 140074027902784 logger_utils.py:92] Saving hparams to /experiment_runs/timing_prelaunch_jax/adamw/imagenet_resnet_jax/trial_1/hparams.json.
I0926 19:19:44.769460 140074027902784 submission_runner.py:191] Initializing dataset.
I0926 19:19:44.787925 140074027902784 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0926 19:19:44.798300 140074027902784 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0926 19:19:45.178975 140074027902784 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0926 19:19:46.351970 140074027902784 submission_runner.py:198] Initializing model.
I0926 19:19:57.294655 140074027902784 submission_runner.py:232] Initializing optimizer.
I0926 19:19:59.030186 140074027902784 submission_runner.py:239] Initializing metrics bundle.
I0926 19:19:59.030399 140074027902784 submission_runner.py:257] Initializing checkpoint and logger.
I0926 19:19:59.031611 140074027902784 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_prelaunch_jax/adamw/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0926 19:19:59.031758 140074027902784 submission_runner.py:277] Saving meta data to /experiment_runs/timing_prelaunch_jax/adamw/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0926 19:20:00.048717 140074027902784 submission_runner.py:280] Saving flags to /experiment_runs/timing_prelaunch_jax/adamw/imagenet_resnet_jax/trial_1/flags_0.json.
I0926 19:20:00.058162 140074027902784 submission_runner.py:290] Starting training loop.
2023-09-26 19:21:04.104145: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-09-26 19:21:07.023453: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
I0926 19:21:08.701751 139900800198400 logging_writer.py:48] [0] global_step=0, grad_norm=0.6000758409500122, loss=6.927995204925537
I0926 19:21:08.719149 140074027902784 spec.py:321] Evaluating on the training split.
I0926 19:21:09.699371 140074027902784 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0926 19:21:09.708846 140074027902784 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0926 19:21:09.791446 140074027902784 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0926 19:21:23.458930 140074027902784 spec.py:333] Evaluating on the validation split.
I0926 19:21:24.983341 140074027902784 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0926 19:21:25.007067 140074027902784 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0926 19:21:25.086514 140074027902784 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0926 19:21:41.589864 140074027902784 spec.py:349] Evaluating on the test split.
I0926 19:21:42.376838 140074027902784 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0926 19:21:42.382022 140074027902784 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0926 19:21:42.419894 140074027902784 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0926 19:21:46.501539 140074027902784 submission_runner.py:381] Time since start: 106.44s, 	Step: 1, 	{'train/accuracy': 0.0011559311533346772, 'train/loss': 6.912101745605469, 'validation/accuracy': 0.0011999999405816197, 'validation/loss': 6.912053108215332, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.91219425201416, 'test/num_examples': 10000, 'score': 68.6608498096466, 'total_duration': 106.44332361221313, 'accumulated_submission_time': 68.6608498096466, 'accumulated_eval_time': 37.78234362602234, 'accumulated_logging_time': 0}
I0926 19:21:46.522474 139879996434176 logging_writer.py:48] [1] accumulated_eval_time=37.782344, accumulated_logging_time=0, accumulated_submission_time=68.660850, global_step=1, preemption_count=0, score=68.660850, test/accuracy=0.001000, test/loss=6.912194, test/num_examples=10000, total_duration=106.443324, train/accuracy=0.001156, train/loss=6.912102, validation/accuracy=0.001200, validation/loss=6.912053, validation/num_examples=50000
I0926 19:22:20.160966 139879988041472 logging_writer.py:48] [100] global_step=100, grad_norm=0.6061220765113831, loss=6.881713390350342
I0926 19:22:53.795974 139879996434176 logging_writer.py:48] [200] global_step=200, grad_norm=0.6889243721961975, loss=6.723714828491211
I0926 19:23:27.445055 139879988041472 logging_writer.py:48] [300] global_step=300, grad_norm=0.7179092764854431, loss=6.599028587341309
I0926 19:24:01.082903 139879996434176 logging_writer.py:48] [400] global_step=400, grad_norm=0.8985034227371216, loss=6.398254871368408
I0926 19:24:34.727801 139879988041472 logging_writer.py:48] [500] global_step=500, grad_norm=1.3651247024536133, loss=6.257554054260254
I0926 19:25:08.346013 139879996434176 logging_writer.py:48] [600] global_step=600, grad_norm=1.665329933166504, loss=6.120144844055176
I0926 19:25:41.988922 139879988041472 logging_writer.py:48] [700] global_step=700, grad_norm=2.5724937915802, loss=5.923855781555176
I0926 19:26:15.615706 139879996434176 logging_writer.py:48] [800] global_step=800, grad_norm=2.6312057971954346, loss=5.88265323638916
I0926 19:26:49.273923 139879988041472 logging_writer.py:48] [900] global_step=900, grad_norm=3.5580532550811768, loss=5.838164806365967
I0926 19:27:22.905079 139879996434176 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.7982633113861084, loss=5.724635124206543
I0926 19:27:56.555963 139879988041472 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.461113214492798, loss=5.585536956787109
I0926 19:28:30.247491 139879996434176 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.7360808849334717, loss=5.534854412078857
I0926 19:29:03.905801 139879988041472 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.445990562438965, loss=5.4390411376953125
I0926 19:29:37.521947 139879996434176 logging_writer.py:48] [1400] global_step=1400, grad_norm=4.470651626586914, loss=5.412250995635986
I0926 19:30:11.162538 139879988041472 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.4717719554901123, loss=5.262395858764648
I0926 19:30:16.644658 140074027902784 spec.py:321] Evaluating on the training split.
I0926 19:30:24.003437 140074027902784 spec.py:333] Evaluating on the validation split.
I0926 19:30:32.197626 140074027902784 spec.py:349] Evaluating on the test split.
I0926 19:30:34.560572 140074027902784 submission_runner.py:381] Time since start: 634.50s, 	Step: 1518, 	{'train/accuracy': 0.12342553585767746, 'train/loss': 4.697055816650391, 'validation/accuracy': 0.11245999485254288, 'validation/loss': 4.804331302642822, 'validation/num_examples': 50000, 'test/accuracy': 0.0836000069975853, 'test/loss': 5.133493423461914, 'test/num_examples': 10000, 'score': 578.751136302948, 'total_duration': 634.5023300647736, 'accumulated_submission_time': 578.751136302948, 'accumulated_eval_time': 55.69820427894592, 'accumulated_logging_time': 0.029627084732055664}
I0926 19:30:34.577898 139880004826880 logging_writer.py:48] [1518] accumulated_eval_time=55.698204, accumulated_logging_time=0.029627, accumulated_submission_time=578.751136, global_step=1518, preemption_count=0, score=578.751136, test/accuracy=0.083600, test/loss=5.133493, test/num_examples=10000, total_duration=634.502330, train/accuracy=0.123426, train/loss=4.697056, validation/accuracy=0.112460, validation/loss=4.804331, validation/num_examples=50000
I0926 19:31:02.537039 139880013219584 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.8322083950042725, loss=5.24055290222168
I0926 19:31:36.178919 139880004826880 logging_writer.py:48] [1700] global_step=1700, grad_norm=4.325596809387207, loss=5.1699323654174805
I0926 19:32:09.828946 139880013219584 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.22579026222229, loss=5.081339359283447
I0926 19:32:43.435776 139880004826880 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.095374584197998, loss=5.027392387390137
I0926 19:33:17.083866 139880013219584 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.719310283660889, loss=5.020378589630127
I0926 19:33:50.766406 139880004826880 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.0102667808532715, loss=4.836311340332031
I0926 19:34:24.379369 139880013219584 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.0530951023101807, loss=4.824269771575928
I0926 19:34:58.041424 139880004826880 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.221988201141357, loss=4.755710124969482
I0926 19:35:31.703270 139880013219584 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.9812891483306885, loss=4.700382709503174
I0926 19:36:05.317060 139880004826880 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.921204090118408, loss=4.65154504776001
I0926 19:36:38.957271 139880013219584 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.5513393878936768, loss=4.607823848724365
I0926 19:37:12.576497 139880004826880 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.446781635284424, loss=4.476657390594482
I0926 19:37:46.219011 139880013219584 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.362998962402344, loss=4.415390968322754
I0926 19:38:19.827473 139880004826880 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.450596570968628, loss=4.375748634338379
I0926 19:38:53.473862 139880013219584 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.455462694168091, loss=4.384130477905273
I0926 19:39:04.663119 140074027902784 spec.py:321] Evaluating on the training split.
I0926 19:39:12.117824 140074027902784 spec.py:333] Evaluating on the validation split.
I0926 19:39:20.598600 140074027902784 spec.py:349] Evaluating on the test split.
I0926 19:39:22.952005 140074027902784 submission_runner.py:381] Time since start: 1162.89s, 	Step: 3035, 	{'train/accuracy': 0.27459341287612915, 'train/loss': 3.5488719940185547, 'validation/accuracy': 0.24917998909950256, 'validation/loss': 3.6985020637512207, 'validation/num_examples': 50000, 'test/accuracy': 0.18740001320838928, 'test/loss': 4.249171733856201, 'test/num_examples': 10000, 'score': 1088.8022935390472, 'total_duration': 1162.893762588501, 'accumulated_submission_time': 1088.8022935390472, 'accumulated_eval_time': 73.98703527450562, 'accumulated_logging_time': 0.057549476623535156}
I0926 19:39:22.968821 139905233565440 logging_writer.py:48] [3035] accumulated_eval_time=73.987035, accumulated_logging_time=0.057549, accumulated_submission_time=1088.802294, global_step=3035, preemption_count=0, score=1088.802294, test/accuracy=0.187400, test/loss=4.249172, test/num_examples=10000, total_duration=1162.893763, train/accuracy=0.274593, train/loss=3.548872, validation/accuracy=0.249180, validation/loss=3.698502, validation/num_examples=50000
I0926 19:39:45.198170 139906437314304 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.567336320877075, loss=4.305100440979004
I0926 19:40:18.860593 139905233565440 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.4096920490264893, loss=4.266351699829102
I0926 19:40:52.499608 139906437314304 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.381927967071533, loss=4.257087707519531
I0926 19:41:26.132345 139905233565440 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.3156309127807617, loss=4.192869663238525
I0926 19:41:59.773607 139906437314304 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.1361517906188965, loss=4.111026287078857
I0926 19:42:33.402616 139905233565440 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.5724759101867676, loss=4.206305980682373
I0926 19:43:07.065771 139906437314304 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.0920114517211914, loss=4.012624263763428
I0926 19:43:40.689410 139905233565440 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.7734241485595703, loss=4.022583961486816
I0926 19:44:14.336258 139906437314304 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.498412847518921, loss=4.063860893249512
I0926 19:44:47.960968 139905233565440 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.9443906545639038, loss=4.045018196105957
I0926 19:45:21.590212 139906437314304 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.131143093109131, loss=3.818180561065674
I0926 19:45:55.268925 139905233565440 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.5865087509155273, loss=3.8798627853393555
I0926 19:46:28.880224 139906437314304 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.97396719455719, loss=3.912053346633911
I0926 19:47:02.503093 139905233565440 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.9867457151412964, loss=3.8249361515045166
I0926 19:47:36.138125 139906437314304 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.6511082649230957, loss=3.824760913848877
I0926 19:47:53.058931 140074027902784 spec.py:321] Evaluating on the training split.
I0926 19:48:00.589728 140074027902784 spec.py:333] Evaluating on the validation split.
I0926 19:48:09.416369 140074027902784 spec.py:349] Evaluating on the test split.
I0926 19:48:11.711136 140074027902784 submission_runner.py:381] Time since start: 1691.65s, 	Step: 4552, 	{'train/accuracy': 0.39275747537612915, 'train/loss': 2.8285820484161377, 'validation/accuracy': 0.3679800033569336, 'validation/loss': 2.966596841812134, 'validation/num_examples': 50000, 'test/accuracy': 0.280100017786026, 'test/loss': 3.5681755542755127, 'test/num_examples': 10000, 'score': 1598.8594815731049, 'total_duration': 1691.6529033184052, 'accumulated_submission_time': 1598.8594815731049, 'accumulated_eval_time': 92.63919639587402, 'accumulated_logging_time': 0.08409523963928223}
I0926 19:48:11.728549 139907670447872 logging_writer.py:48] [4552] accumulated_eval_time=92.639196, accumulated_logging_time=0.084095, accumulated_submission_time=1598.859482, global_step=4552, preemption_count=0, score=1598.859482, test/accuracy=0.280100, test/loss=3.568176, test/num_examples=10000, total_duration=1691.652903, train/accuracy=0.392757, train/loss=2.828582, validation/accuracy=0.367980, validation/loss=2.966597, validation/num_examples=50000
I0926 19:48:28.248500 139907678840576 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.0276877880096436, loss=3.806884765625
I0926 19:49:01.905493 139907670447872 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.214385986328125, loss=3.844764232635498
I0926 19:49:35.521764 139907678840576 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.816687822341919, loss=3.839462995529175
I0926 19:50:09.169287 139907670447872 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.8662160634994507, loss=3.7710750102996826
I0926 19:50:42.787871 139907678840576 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.5320504903793335, loss=3.6645727157592773
I0926 19:51:16.408792 139907670447872 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.942237138748169, loss=3.5739974975585938
I0926 19:51:50.037708 139907678840576 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.111088752746582, loss=3.6197621822357178
I0926 19:52:23.694803 139907670447872 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.4665815830230713, loss=3.6860804557800293
I0926 19:52:57.276350 139907678840576 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.0482256412506104, loss=3.58135986328125
I0926 19:53:30.912800 139907670447872 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.8492709398269653, loss=3.5945379734039307
I0926 19:54:04.544676 139907678840576 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.8798673152923584, loss=3.4480206966400146
I0926 19:54:38.142471 139907670447872 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.94943106174469, loss=3.4431543350219727
I0926 19:55:11.788817 139907678840576 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.2538630962371826, loss=3.592172622680664
I0926 19:55:45.429644 139907670447872 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.0360190868377686, loss=3.4653055667877197
I0926 19:56:19.044327 139907678840576 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.445802927017212, loss=3.386282205581665
I0926 19:56:42.033224 140074027902784 spec.py:321] Evaluating on the training split.
I0926 19:56:49.486313 140074027902784 spec.py:333] Evaluating on the validation split.
I0926 19:56:57.911699 140074027902784 spec.py:349] Evaluating on the test split.
I0926 19:57:00.249286 140074027902784 submission_runner.py:381] Time since start: 2220.19s, 	Step: 6070, 	{'train/accuracy': 0.46326929330825806, 'train/loss': 2.431932210922241, 'validation/accuracy': 0.43337997794151306, 'validation/loss': 2.6036412715911865, 'validation/num_examples': 50000, 'test/accuracy': 0.3297000229358673, 'test/loss': 3.2746663093566895, 'test/num_examples': 10000, 'score': 2109.130287885666, 'total_duration': 2220.191058397293, 'accumulated_submission_time': 2109.130287885666, 'accumulated_eval_time': 110.85522246360779, 'accumulated_logging_time': 0.11183333396911621}
I0926 19:57:00.267219 139907645269760 logging_writer.py:48] [6070] accumulated_eval_time=110.855222, accumulated_logging_time=0.111833, accumulated_submission_time=2109.130288, global_step=6070, preemption_count=0, score=2109.130288, test/accuracy=0.329700, test/loss=3.274666, test/num_examples=10000, total_duration=2220.191058, train/accuracy=0.463269, train/loss=2.431932, validation/accuracy=0.433380, validation/loss=2.603641, validation/num_examples=50000
I0926 19:57:10.666984 139907653662464 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.7358472347259521, loss=3.5238211154937744
I0926 19:57:44.276972 139907645269760 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.0719399452209473, loss=3.368131637573242
I0926 19:58:17.954216 139907653662464 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.159291386604309, loss=3.4032135009765625
I0926 19:58:51.489561 139907645269760 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.082434058189392, loss=3.492180347442627
I0926 19:59:25.104401 139907653662464 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.4524199962615967, loss=3.3492414951324463
I0926 19:59:58.709782 139907645269760 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.269885540008545, loss=3.3822901248931885
I0926 20:00:32.345209 139907653662464 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.2643100023269653, loss=3.3093008995056152
I0926 20:01:05.938965 139907645269760 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.2514314651489258, loss=3.255723714828491
I0926 20:01:39.581528 139907653662464 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.198907732963562, loss=3.262608051300049
I0926 20:02:13.171344 139907645269760 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.2725276947021484, loss=3.3063700199127197
I0926 20:02:46.709221 139907653662464 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.119615912437439, loss=3.240809440612793
I0926 20:03:20.233510 139907645269760 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.1046195030212402, loss=3.2327914237976074
I0926 20:03:53.791639 139907653662464 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.091770052909851, loss=3.2942886352539062
I0926 20:04:27.568571 139907645269760 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.0415757894515991, loss=3.1978490352630615
I0926 20:05:01.197183 139907653662464 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.1219068765640259, loss=3.268711566925049
I0926 20:05:30.551819 140074027902784 spec.py:321] Evaluating on the training split.
I0926 20:05:38.020842 140074027902784 spec.py:333] Evaluating on the validation split.
I0926 20:05:46.574978 140074027902784 spec.py:349] Evaluating on the test split.
I0926 20:05:48.989786 140074027902784 submission_runner.py:381] Time since start: 2748.93s, 	Step: 7589, 	{'train/accuracy': 0.5162627696990967, 'train/loss': 2.1689934730529785, 'validation/accuracy': 0.4816799759864807, 'validation/loss': 2.3434340953826904, 'validation/num_examples': 50000, 'test/accuracy': 0.3700000047683716, 'test/loss': 3.029193639755249, 'test/num_examples': 10000, 'score': 2619.3815405368805, 'total_duration': 2748.9315712451935, 'accumulated_submission_time': 2619.3815405368805, 'accumulated_eval_time': 129.29316759109497, 'accumulated_logging_time': 0.13973069190979004}
I0926 20:05:49.004828 139907636877056 logging_writer.py:48] [7589] accumulated_eval_time=129.293168, accumulated_logging_time=0.139731, accumulated_submission_time=2619.381541, global_step=7589, preemption_count=0, score=2619.381541, test/accuracy=0.370000, test/loss=3.029194, test/num_examples=10000, total_duration=2748.931571, train/accuracy=0.516263, train/loss=2.168993, validation/accuracy=0.481680, validation/loss=2.343434, validation/num_examples=50000
I0926 20:05:53.034390 139907645269760 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9027708768844604, loss=3.2627735137939453
I0926 20:06:26.557556 139907636877056 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.1794763803482056, loss=3.2449302673339844
I0926 20:07:00.167394 139907645269760 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8493175506591797, loss=3.1726744174957275
I0926 20:07:33.800641 139907636877056 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.1423829793930054, loss=3.125620126724243
I0926 20:08:07.449487 139907645269760 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.2993340492248535, loss=3.2164907455444336
I0926 20:08:41.080320 139907636877056 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8650836944580078, loss=3.279146909713745
I0926 20:09:14.683156 139907645269760 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.2102428674697876, loss=3.1908302307128906
I0926 20:09:48.322546 139907636877056 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.9672752022743225, loss=3.153888702392578
I0926 20:10:21.970122 139907645269760 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.9255402088165283, loss=3.1005725860595703
I0926 20:10:55.531038 139907636877056 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.3235594034194946, loss=3.1329519748687744
I0926 20:11:29.171013 139907645269760 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.9708731174468994, loss=3.1808598041534424
I0926 20:12:02.801211 139907636877056 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.2169947624206543, loss=3.19352126121521
I0926 20:12:36.429195 139907645269760 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.0012753009796143, loss=3.1466422080993652
I0926 20:13:10.052131 139907636877056 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.2274925708770752, loss=3.1174068450927734
I0926 20:13:43.595046 139907645269760 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8157676458358765, loss=3.1614911556243896
I0926 20:14:17.138128 139907636877056 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.1698126792907715, loss=3.1249630451202393
I0926 20:14:19.262062 140074027902784 spec.py:321] Evaluating on the training split.
I0926 20:14:26.872115 140074027902784 spec.py:333] Evaluating on the validation split.
I0926 20:14:35.633762 140074027902784 spec.py:349] Evaluating on the test split.
I0926 20:14:37.945646 140074027902784 submission_runner.py:381] Time since start: 3277.89s, 	Step: 9108, 	{'train/accuracy': 0.6137197017669678, 'train/loss': 1.6966463327407837, 'validation/accuracy': 0.5277599692344666, 'validation/loss': 2.1154229640960693, 'validation/num_examples': 50000, 'test/accuracy': 0.4140000343322754, 'test/loss': 2.7832822799682617, 'test/num_examples': 10000, 'score': 3129.6065108776093, 'total_duration': 3277.887415409088, 'accumulated_submission_time': 3129.6065108776093, 'accumulated_eval_time': 147.97671103477478, 'accumulated_logging_time': 0.16365265846252441}
I0926 20:14:37.963234 139907678840576 logging_writer.py:48] [9108] accumulated_eval_time=147.976711, accumulated_logging_time=0.163653, accumulated_submission_time=3129.606511, global_step=9108, preemption_count=0, score=3129.606511, test/accuracy=0.414000, test/loss=2.783282, test/num_examples=10000, total_duration=3277.887415, train/accuracy=0.613720, train/loss=1.696646, validation/accuracy=0.527760, validation/loss=2.115423, validation/num_examples=50000
I0926 20:15:09.159124 139907687233280 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.0421520471572876, loss=3.184189796447754
I0926 20:15:42.699789 139907678840576 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.1829538345336914, loss=3.1059484481811523
I0926 20:16:16.242877 139907687233280 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.968024492263794, loss=2.9601211547851562
I0926 20:16:49.872542 139907678840576 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9571950435638428, loss=3.0161375999450684
I0926 20:17:23.421614 139907687233280 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.690935492515564, loss=2.982733964920044
I0926 20:17:57.004903 139907678840576 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.1370142698287964, loss=3.025184392929077
I0926 20:18:30.637811 139907687233280 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9512460827827454, loss=3.0216774940490723
I0926 20:19:04.208143 139907678840576 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8844611048698425, loss=3.0647542476654053
I0926 20:19:37.718431 139907687233280 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7158780694007874, loss=2.9397943019866943
I0926 20:20:11.266233 139907678840576 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7663899660110474, loss=3.017643690109253
I0926 20:20:44.885042 139907687233280 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.189299464225769, loss=3.0277535915374756
I0926 20:21:18.472087 139907678840576 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.9725680351257324, loss=2.9802639484405518
I0926 20:21:51.999408 139907687233280 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.894666314125061, loss=2.911651134490967
I0926 20:22:25.560805 139907678840576 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6677479147911072, loss=2.9552855491638184
I0926 20:22:59.109927 139907687233280 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8540744185447693, loss=3.006842851638794
I0926 20:23:07.978188 140074027902784 spec.py:321] Evaluating on the training split.
I0926 20:23:15.445802 140074027902784 spec.py:333] Evaluating on the validation split.
I0926 20:23:26.737976 140074027902784 spec.py:349] Evaluating on the test split.
I0926 20:23:29.033197 140074027902784 submission_runner.py:381] Time since start: 3808.97s, 	Step: 10628, 	{'train/accuracy': 0.5992107391357422, 'train/loss': 1.780951976776123, 'validation/accuracy': 0.5349999666213989, 'validation/loss': 2.1016712188720703, 'validation/num_examples': 50000, 'test/accuracy': 0.4173000156879425, 'test/loss': 2.787350654602051, 'test/num_examples': 10000, 'score': 3639.5884165763855, 'total_duration': 3808.9749813079834, 'accumulated_submission_time': 3639.5884165763855, 'accumulated_eval_time': 169.0317144393921, 'accumulated_logging_time': 0.19072842597961426}
I0926 20:23:29.054491 139907636877056 logging_writer.py:48] [10628] accumulated_eval_time=169.031714, accumulated_logging_time=0.190728, accumulated_submission_time=3639.588417, global_step=10628, preemption_count=0, score=3639.588417, test/accuracy=0.417300, test/loss=2.787351, test/num_examples=10000, total_duration=3808.974981, train/accuracy=0.599211, train/loss=1.780952, validation/accuracy=0.535000, validation/loss=2.101671, validation/num_examples=50000
I0926 20:23:53.542773 139907645269760 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.7519497275352478, loss=2.9122891426086426
I0926 20:24:27.124258 139907636877056 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7091625332832336, loss=2.899428367614746
I0926 20:25:00.751222 139907645269760 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7225024700164795, loss=2.9210941791534424
I0926 20:25:34.357745 139907636877056 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6635868549346924, loss=2.9898130893707275
I0926 20:26:07.963230 139907645269760 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.8078672885894775, loss=2.9099814891815186
I0926 20:26:41.576233 139907636877056 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9106274843215942, loss=2.77508544921875
I0926 20:27:15.095570 139907645269760 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6633013486862183, loss=2.8172812461853027
I0926 20:27:48.612279 139907636877056 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7020080089569092, loss=2.869140386581421
I0926 20:28:22.180598 139907645269760 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7769902348518372, loss=2.958481550216675
I0926 20:28:55.800530 139907636877056 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5983554720878601, loss=2.7515177726745605
I0926 20:29:29.441124 139907645269760 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5994840264320374, loss=2.928163528442383
I0926 20:30:03.047132 139907636877056 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.7045270204544067, loss=2.807003974914551
I0926 20:30:36.647903 139907645269760 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7256301045417786, loss=2.8113021850585938
I0926 20:31:10.274766 139907636877056 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5839006304740906, loss=2.8050012588500977
I0926 20:31:43.863409 139907645269760 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6085530519485474, loss=2.7674171924591064
I0926 20:31:59.097863 140074027902784 spec.py:321] Evaluating on the training split.
I0926 20:32:06.928165 140074027902784 spec.py:333] Evaluating on the validation split.
I0926 20:32:21.204307 140074027902784 spec.py:349] Evaluating on the test split.
I0926 20:32:23.432558 140074027902784 submission_runner.py:381] Time since start: 4343.37s, 	Step: 12147, 	{'train/accuracy': 0.6424983739852905, 'train/loss': 1.5501998662948608, 'validation/accuracy': 0.5827000141143799, 'validation/loss': 1.8350743055343628, 'validation/num_examples': 50000, 'test/accuracy': 0.4603000283241272, 'test/loss': 2.512204885482788, 'test/num_examples': 10000, 'score': 4149.600026845932, 'total_duration': 4343.374324798584, 'accumulated_submission_time': 4149.600026845932, 'accumulated_eval_time': 193.36636447906494, 'accumulated_logging_time': 0.22093653678894043}
I0926 20:32:23.453549 139908844820224 logging_writer.py:48] [12147] accumulated_eval_time=193.366364, accumulated_logging_time=0.220937, accumulated_submission_time=4149.600027, global_step=12147, preemption_count=0, score=4149.600027, test/accuracy=0.460300, test/loss=2.512205, test/num_examples=10000, total_duration=4343.374325, train/accuracy=0.642498, train/loss=1.550200, validation/accuracy=0.582700, validation/loss=1.835074, validation/num_examples=50000
I0926 20:32:41.563131 139908886783744 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.736000120639801, loss=2.880326271057129
I0926 20:33:15.072532 139908844820224 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7527269721031189, loss=2.8266615867614746
I0926 20:33:48.619137 139908886783744 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.740167498588562, loss=2.832427978515625
I0926 20:34:22.236366 139908844820224 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6537104249000549, loss=2.813180685043335
I0926 20:34:55.890774 139908886783744 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7173507809638977, loss=2.802342414855957
I0926 20:35:29.447478 139908844820224 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5845328569412231, loss=2.819282293319702
I0926 20:36:03.012969 139908886783744 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5724474191665649, loss=2.777956247329712
I0926 20:36:36.528761 139908844820224 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5509979724884033, loss=2.7658238410949707
I0926 20:37:10.072318 139908886783744 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6950492858886719, loss=2.7551522254943848
I0926 20:37:43.678160 139908844820224 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.5043278336524963, loss=2.84102201461792
I0926 20:38:17.243366 139908886783744 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5485534071922302, loss=2.7494757175445557
I0926 20:38:50.874161 139908844820224 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6273432374000549, loss=2.8191990852355957
I0926 20:39:24.439846 139908886783744 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6578951478004456, loss=2.6808125972747803
I0926 20:39:58.066615 139908844820224 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5043419599533081, loss=2.725860595703125
I0926 20:40:31.628188 139908886783744 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6430864930152893, loss=2.763058662414551
I0926 20:40:53.605699 140074027902784 spec.py:321] Evaluating on the training split.
I0926 20:41:01.126194 140074027902784 spec.py:333] Evaluating on the validation split.
I0926 20:41:15.971957 140074027902784 spec.py:349] Evaluating on the test split.
I0926 20:41:18.148817 140074027902784 submission_runner.py:381] Time since start: 4878.09s, 	Step: 13667, 	{'train/accuracy': 0.6538185477256775, 'train/loss': 1.4993382692337036, 'validation/accuracy': 0.591219961643219, 'validation/loss': 1.805548906326294, 'validation/num_examples': 50000, 'test/accuracy': 0.46230003237724304, 'test/loss': 2.514923572540283, 'test/num_examples': 10000, 'score': 4659.71684551239, 'total_duration': 4878.0905148983, 'accumulated_submission_time': 4659.71684551239, 'accumulated_eval_time': 217.9093770980835, 'accumulated_logging_time': 0.25406503677368164}
I0926 20:41:18.166761 139908869998336 logging_writer.py:48] [13667] accumulated_eval_time=217.909377, accumulated_logging_time=0.254065, accumulated_submission_time=4659.716846, global_step=13667, preemption_count=0, score=4659.716846, test/accuracy=0.462300, test/loss=2.514924, test/num_examples=10000, total_duration=4878.090515, train/accuracy=0.653819, train/loss=1.499338, validation/accuracy=0.591220, validation/loss=1.805549, validation/num_examples=50000
I0926 20:41:29.597530 139908878391040 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5443903803825378, loss=2.6951687335968018
I0926 20:42:03.204003 139908869998336 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.585436999797821, loss=2.6387853622436523
I0926 20:42:36.770185 139908878391040 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7152417898178101, loss=2.8401477336883545
I0926 20:43:09.710141 140074027902784 spec.py:321] Evaluating on the training split.
I0926 20:43:17.295380 140074027902784 spec.py:333] Evaluating on the validation split.
I0926 20:43:32.210052 140074027902784 spec.py:349] Evaluating on the test split.
I0926 20:43:34.459432 140074027902784 submission_runner.py:381] Time since start: 5014.40s, 	Step: 14000, 	{'train/accuracy': 0.6437340378761292, 'train/loss': 1.5619592666625977, 'validation/accuracy': 0.5853999853134155, 'validation/loss': 1.8347046375274658, 'validation/num_examples': 50000, 'test/accuracy': 0.46410003304481506, 'test/loss': 2.556713104248047, 'test/num_examples': 10000, 'score': 4771.244979858398, 'total_duration': 5014.401158571243, 'accumulated_submission_time': 4771.244979858398, 'accumulated_eval_time': 242.65860247612, 'accumulated_logging_time': 0.2817392349243164}
I0926 20:43:34.477524 139908844820224 logging_writer.py:48] [14000] accumulated_eval_time=242.658602, accumulated_logging_time=0.281739, accumulated_submission_time=4771.244980, global_step=14000, preemption_count=0, score=4771.244980, test/accuracy=0.464100, test/loss=2.556713, test/num_examples=10000, total_duration=5014.401159, train/accuracy=0.643734, train/loss=1.561959, validation/accuracy=0.585400, validation/loss=1.834705, validation/num_examples=50000
I0926 20:43:34.493800 139908853212928 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4771.244980
I0926 20:43:34.801673 140074027902784 checkpoints.py:490] Saving checkpoint at step: 14000
I0926 20:43:35.941807 140074027902784 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_prelaunch_jax/adamw/imagenet_resnet_jax/trial_1/checkpoint_14000
I0926 20:43:35.961927 140074027902784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_prelaunch_jax/adamw/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0926 20:43:36.149176 140074027902784 submission_runner.py:549] Tuning trial 1/1
I0926 20:43:36.149436 140074027902784 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0926 20:43:36.151686 140074027902784 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011559311533346772, 'train/loss': 6.912101745605469, 'validation/accuracy': 0.0011999999405816197, 'validation/loss': 6.912053108215332, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.91219425201416, 'test/num_examples': 10000, 'score': 68.6608498096466, 'total_duration': 106.44332361221313, 'accumulated_submission_time': 68.6608498096466, 'accumulated_eval_time': 37.78234362602234, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1518, {'train/accuracy': 0.12342553585767746, 'train/loss': 4.697055816650391, 'validation/accuracy': 0.11245999485254288, 'validation/loss': 4.804331302642822, 'validation/num_examples': 50000, 'test/accuracy': 0.0836000069975853, 'test/loss': 5.133493423461914, 'test/num_examples': 10000, 'score': 578.751136302948, 'total_duration': 634.5023300647736, 'accumulated_submission_time': 578.751136302948, 'accumulated_eval_time': 55.69820427894592, 'accumulated_logging_time': 0.029627084732055664, 'global_step': 1518, 'preemption_count': 0}), (3035, {'train/accuracy': 0.27459341287612915, 'train/loss': 3.5488719940185547, 'validation/accuracy': 0.24917998909950256, 'validation/loss': 3.6985020637512207, 'validation/num_examples': 50000, 'test/accuracy': 0.18740001320838928, 'test/loss': 4.249171733856201, 'test/num_examples': 10000, 'score': 1088.8022935390472, 'total_duration': 1162.893762588501, 'accumulated_submission_time': 1088.8022935390472, 'accumulated_eval_time': 73.98703527450562, 'accumulated_logging_time': 0.057549476623535156, 'global_step': 3035, 'preemption_count': 0}), (4552, {'train/accuracy': 0.39275747537612915, 'train/loss': 2.8285820484161377, 'validation/accuracy': 0.3679800033569336, 'validation/loss': 2.966596841812134, 'validation/num_examples': 50000, 'test/accuracy': 0.280100017786026, 'test/loss': 3.5681755542755127, 'test/num_examples': 10000, 'score': 1598.8594815731049, 'total_duration': 1691.6529033184052, 'accumulated_submission_time': 1598.8594815731049, 'accumulated_eval_time': 92.63919639587402, 'accumulated_logging_time': 0.08409523963928223, 'global_step': 4552, 'preemption_count': 0}), (6070, {'train/accuracy': 0.46326929330825806, 'train/loss': 2.431932210922241, 'validation/accuracy': 0.43337997794151306, 'validation/loss': 2.6036412715911865, 'validation/num_examples': 50000, 'test/accuracy': 0.3297000229358673, 'test/loss': 3.2746663093566895, 'test/num_examples': 10000, 'score': 2109.130287885666, 'total_duration': 2220.191058397293, 'accumulated_submission_time': 2109.130287885666, 'accumulated_eval_time': 110.85522246360779, 'accumulated_logging_time': 0.11183333396911621, 'global_step': 6070, 'preemption_count': 0}), (7589, {'train/accuracy': 0.5162627696990967, 'train/loss': 2.1689934730529785, 'validation/accuracy': 0.4816799759864807, 'validation/loss': 2.3434340953826904, 'validation/num_examples': 50000, 'test/accuracy': 0.3700000047683716, 'test/loss': 3.029193639755249, 'test/num_examples': 10000, 'score': 2619.3815405368805, 'total_duration': 2748.9315712451935, 'accumulated_submission_time': 2619.3815405368805, 'accumulated_eval_time': 129.29316759109497, 'accumulated_logging_time': 0.13973069190979004, 'global_step': 7589, 'preemption_count': 0}), (9108, {'train/accuracy': 0.6137197017669678, 'train/loss': 1.6966463327407837, 'validation/accuracy': 0.5277599692344666, 'validation/loss': 2.1154229640960693, 'validation/num_examples': 50000, 'test/accuracy': 0.4140000343322754, 'test/loss': 2.7832822799682617, 'test/num_examples': 10000, 'score': 3129.6065108776093, 'total_duration': 3277.887415409088, 'accumulated_submission_time': 3129.6065108776093, 'accumulated_eval_time': 147.97671103477478, 'accumulated_logging_time': 0.16365265846252441, 'global_step': 9108, 'preemption_count': 0}), (10628, {'train/accuracy': 0.5992107391357422, 'train/loss': 1.780951976776123, 'validation/accuracy': 0.5349999666213989, 'validation/loss': 2.1016712188720703, 'validation/num_examples': 50000, 'test/accuracy': 0.4173000156879425, 'test/loss': 2.787350654602051, 'test/num_examples': 10000, 'score': 3639.5884165763855, 'total_duration': 3808.9749813079834, 'accumulated_submission_time': 3639.5884165763855, 'accumulated_eval_time': 169.0317144393921, 'accumulated_logging_time': 0.19072842597961426, 'global_step': 10628, 'preemption_count': 0}), (12147, {'train/accuracy': 0.6424983739852905, 'train/loss': 1.5501998662948608, 'validation/accuracy': 0.5827000141143799, 'validation/loss': 1.8350743055343628, 'validation/num_examples': 50000, 'test/accuracy': 0.4603000283241272, 'test/loss': 2.512204885482788, 'test/num_examples': 10000, 'score': 4149.600026845932, 'total_duration': 4343.374324798584, 'accumulated_submission_time': 4149.600026845932, 'accumulated_eval_time': 193.36636447906494, 'accumulated_logging_time': 0.22093653678894043, 'global_step': 12147, 'preemption_count': 0}), (13667, {'train/accuracy': 0.6538185477256775, 'train/loss': 1.4993382692337036, 'validation/accuracy': 0.591219961643219, 'validation/loss': 1.805548906326294, 'validation/num_examples': 50000, 'test/accuracy': 0.46230003237724304, 'test/loss': 2.514923572540283, 'test/num_examples': 10000, 'score': 4659.71684551239, 'total_duration': 4878.0905148983, 'accumulated_submission_time': 4659.71684551239, 'accumulated_eval_time': 217.9093770980835, 'accumulated_logging_time': 0.25406503677368164, 'global_step': 13667, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6437340378761292, 'train/loss': 1.5619592666625977, 'validation/accuracy': 0.5853999853134155, 'validation/loss': 1.8347046375274658, 'validation/num_examples': 50000, 'test/accuracy': 0.46410003304481506, 'test/loss': 2.556713104248047, 'test/num_examples': 10000, 'score': 4771.244979858398, 'total_duration': 5014.401158571243, 'accumulated_submission_time': 4771.244979858398, 'accumulated_eval_time': 242.65860247612, 'accumulated_logging_time': 0.2817392349243164, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0926 20:43:36.151836 140074027902784 submission_runner.py:552] Timing: 4771.244979858398
I0926 20:43:36.151888 140074027902784 submission_runner.py:554] Total number of evals: 11
I0926 20:43:36.151938 140074027902784 submission_runner.py:555] ====================
I0926 20:43:36.152059 140074027902784 submission_runner.py:625] Final imagenet_resnet score: 4771.244979858398
