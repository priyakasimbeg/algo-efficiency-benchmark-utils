python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_prelaunch_jax/adamw --overwrite=true --save_checkpoints=false --max_global_steps=6000 2>&1 | tee -a /logs/ogbg_jax_09-26-2023-22-55-23.log
2023-09-26 22:55:28.647062: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0926 22:55:47.168214 139754842453824 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_prelaunch_jax/adamw/ogbg_jax.
I0926 22:55:48.191114 139754842453824 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0926 22:55:48.191864 139754842453824 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0926 22:55:48.192034 139754842453824 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0926 22:55:48.197876 139754842453824 submission_runner.py:507] Using RNG seed 2062692001
I0926 22:55:53.513605 139754842453824 submission_runner.py:516] --- Tuning run 1/1 ---
I0926 22:55:53.513811 139754842453824 submission_runner.py:521] Creating tuning directory at /experiment_runs/timing_prelaunch_jax/adamw/ogbg_jax/trial_1.
I0926 22:55:53.513982 139754842453824 logger_utils.py:92] Saving hparams to /experiment_runs/timing_prelaunch_jax/adamw/ogbg_jax/trial_1/hparams.json.
I0926 22:55:53.693983 139754842453824 submission_runner.py:191] Initializing dataset.
I0926 22:55:53.805536 139754842453824 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0926 22:55:53.814159 139754842453824 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0926 22:55:54.059454 139754842453824 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0926 22:55:54.119453 139754842453824 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0926 22:55:54.196941 139754842453824 submission_runner.py:198] Initializing model.
I0926 22:55:58.873901 139754842453824 submission_runner.py:232] Initializing optimizer.
I0926 22:55:59.506812 139754842453824 submission_runner.py:239] Initializing metrics bundle.
I0926 22:55:59.507026 139754842453824 submission_runner.py:257] Initializing checkpoint and logger.
I0926 22:55:59.507985 139754842453824 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_prelaunch_jax/adamw/ogbg_jax/trial_1 with prefix checkpoint_
I0926 22:55:59.508134 139754842453824 submission_runner.py:277] Saving meta data to /experiment_runs/timing_prelaunch_jax/adamw/ogbg_jax/trial_1/meta_data_0.json.
I0926 22:55:59.508327 139754842453824 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0926 22:55:59.508387 139754842453824 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0926 22:56:00.442961 139754842453824 submission_runner.py:280] Saving flags to /experiment_runs/timing_prelaunch_jax/adamw/ogbg_jax/trial_1/flags_0.json.
I0926 22:56:00.455877 139754842453824 submission_runner.py:290] Starting training loop.
I0926 22:56:24.785637 139590488811264 logging_writer.py:48] [0] global_step=0, grad_norm=2.5695559978485107, loss=0.7298126816749573
I0926 22:56:24.801679 139754842453824 spec.py:321] Evaluating on the training split.
I0926 22:56:24.808361 139754842453824 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0926 22:56:24.813024 139754842453824 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0926 22:56:24.881581 139754842453824 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0926 22:58:17.472773 139754842453824 spec.py:333] Evaluating on the validation split.
I0926 22:58:17.476484 139754842453824 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0926 22:58:17.480465 139754842453824 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0926 22:58:17.545221 139754842453824 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0926 22:59:48.924763 139754842453824 spec.py:349] Evaluating on the test split.
I0926 22:59:48.928226 139754842453824 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0926 22:59:48.932256 139754842453824 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0926 22:59:48.996513 139754842453824 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0926 23:01:24.424772 139754842453824 submission_runner.py:381] Time since start: 323.97s, 	Step: 1, 	{'train/accuracy': 0.532181978225708, 'train/loss': 0.728960394859314, 'train/mean_average_precision': 0.02313895077271053, 'validation/accuracy': 0.5307276844978333, 'validation/loss': 0.7279101610183716, 'validation/mean_average_precision': 0.026399944647783197, 'validation/num_examples': 43793, 'test/accuracy': 0.5276063680648804, 'test/loss': 0.7299690246582031, 'test/mean_average_precision': 0.027237141320772127, 'test/num_examples': 43793, 'score': 24.34574866294861, 'total_duration': 323.96882033348083, 'accumulated_submission_time': 24.34574866294861, 'accumulated_eval_time': 299.62303018569946, 'accumulated_logging_time': 0}
I0926 23:01:24.445127 139579499251456 logging_writer.py:48] [1] accumulated_eval_time=299.623030, accumulated_logging_time=0, accumulated_submission_time=24.345749, global_step=1, preemption_count=0, score=24.345749, test/accuracy=0.527606, test/loss=0.729969, test/mean_average_precision=0.027237, test/num_examples=43793, total_duration=323.968820, train/accuracy=0.532182, train/loss=0.728960, train/mean_average_precision=0.023139, validation/accuracy=0.530728, validation/loss=0.727910, validation/mean_average_precision=0.026400, validation/num_examples=43793
I0926 23:01:56.057363 139580615415552 logging_writer.py:48] [100] global_step=100, grad_norm=0.38974058628082275, loss=0.34233179688453674
I0926 23:02:28.388298 139579499251456 logging_writer.py:48] [200] global_step=200, grad_norm=0.2683711051940918, loss=0.20089441537857056
I0926 23:03:00.083580 139580615415552 logging_writer.py:48] [300] global_step=300, grad_norm=0.13207805156707764, loss=0.11225121468305588
I0926 23:03:32.051175 139579499251456 logging_writer.py:48] [400] global_step=400, grad_norm=0.06211748719215393, loss=0.08144965022802353
I0926 23:04:03.418323 139580615415552 logging_writer.py:48] [500] global_step=500, grad_norm=0.04102734476327896, loss=0.06672749668359756
I0926 23:04:33.905125 139579499251456 logging_writer.py:48] [600] global_step=600, grad_norm=0.03587188944220543, loss=0.056052226573228836
I0926 23:05:04.444735 139580615415552 logging_writer.py:48] [700] global_step=700, grad_norm=0.027261918410658836, loss=0.05582275986671448
I0926 23:05:24.489973 139754842453824 spec.py:321] Evaluating on the training split.
I0926 23:07:13.646888 139754842453824 spec.py:333] Evaluating on the validation split.
I0926 23:07:16.717517 139754842453824 spec.py:349] Evaluating on the test split.
I0926 23:07:19.783651 139754842453824 submission_runner.py:381] Time since start: 679.33s, 	Step: 766, 	{'train/accuracy': 0.9867082238197327, 'train/loss': 0.05706782266497612, 'train/mean_average_precision': 0.03911227886309096, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.066437728703022, 'validation/mean_average_precision': 0.039811373190403836, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06965232640504837, 'test/mean_average_precision': 0.04082655100716385, 'test/num_examples': 43793, 'score': 264.36876606941223, 'total_duration': 679.3277101516724, 'accumulated_submission_time': 264.36876606941223, 'accumulated_eval_time': 414.91666889190674, 'accumulated_logging_time': 0.031746864318847656}
I0926 23:07:19.798343 139580607022848 logging_writer.py:48] [766] accumulated_eval_time=414.916669, accumulated_logging_time=0.031747, accumulated_submission_time=264.368766, global_step=766, preemption_count=0, score=264.368766, test/accuracy=0.983142, test/loss=0.069652, test/mean_average_precision=0.040827, test/num_examples=43793, total_duration=679.327710, train/accuracy=0.986708, train/loss=0.057068, train/mean_average_precision=0.039112, validation/accuracy=0.984118, validation/loss=0.066438, validation/mean_average_precision=0.039811, validation/num_examples=43793
I0926 23:07:30.800703 139580632200960 logging_writer.py:48] [800] global_step=800, grad_norm=0.0355832464993, loss=0.05575890839099884
I0926 23:08:01.621571 139580607022848 logging_writer.py:48] [900] global_step=900, grad_norm=0.03941675275564194, loss=0.048383597284555435
I0926 23:08:32.446630 139580632200960 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.027896113693714142, loss=0.0547170527279377
I0926 23:09:03.111257 139580607022848 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.02497977949678898, loss=0.04769640415906906
I0926 23:09:33.387811 139580632200960 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.01650872826576233, loss=0.050949208438396454
I0926 23:10:03.642759 139580607022848 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.03058645687997341, loss=0.055138617753982544
I0926 23:10:34.268146 139580632200960 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.04248339310288429, loss=0.04833943769335747
I0926 23:11:05.188145 139580607022848 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.04416652396321297, loss=0.043239884078502655
I0926 23:11:19.921963 139754842453824 spec.py:321] Evaluating on the training split.
I0926 23:13:13.165488 139754842453824 spec.py:333] Evaluating on the validation split.
I0926 23:13:16.175655 139754842453824 spec.py:349] Evaluating on the test split.
I0926 23:13:19.091454 139754842453824 submission_runner.py:381] Time since start: 1038.64s, 	Step: 1549, 	{'train/accuracy': 0.9871712327003479, 'train/loss': 0.04783337935805321, 'train/mean_average_precision': 0.09123253410612571, 'validation/accuracy': 0.9844455122947693, 'validation/loss': 0.0578041635453701, 'validation/mean_average_precision': 0.08916381907961687, 'validation/num_examples': 43793, 'test/accuracy': 0.98348069190979, 'test/loss': 0.06123698502779007, 'test/mean_average_precision': 0.0873548839429084, 'test/num_examples': 43793, 'score': 504.47212624549866, 'total_duration': 1038.6355051994324, 'accumulated_submission_time': 504.47212624549866, 'accumulated_eval_time': 534.0861167907715, 'accumulated_logging_time': 0.05720090866088867}
I0926 23:13:19.111341 139580615415552 logging_writer.py:48] [1549] accumulated_eval_time=534.086117, accumulated_logging_time=0.057201, accumulated_submission_time=504.472126, global_step=1549, preemption_count=0, score=504.472126, test/accuracy=0.983481, test/loss=0.061237, test/mean_average_precision=0.087355, test/num_examples=43793, total_duration=1038.635505, train/accuracy=0.987171, train/loss=0.047833, train/mean_average_precision=0.091233, validation/accuracy=0.984446, validation/loss=0.057804, validation/mean_average_precision=0.089164, validation/num_examples=43793
I0926 23:13:35.395017 139580623808256 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.019516069442033768, loss=0.04705115035176277
I0926 23:14:06.098747 139580615415552 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.03179563581943512, loss=0.051102206110954285
I0926 23:14:36.979635 139580623808256 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.03844686225056648, loss=0.04988216236233711
I0926 23:15:07.949747 139580615415552 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.03738568723201752, loss=0.0459408201277256
I0926 23:15:39.270925 139580623808256 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.033419325947761536, loss=0.05044988915324211
I0926 23:16:10.736193 139580615415552 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.05167238414287567, loss=0.044477201998233795
I0926 23:16:42.213776 139580623808256 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.014048891142010689, loss=0.050587065517902374
I0926 23:17:13.434976 139580615415552 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.01843343861401081, loss=0.04420638084411621
I0926 23:17:19.254095 139754842453824 spec.py:321] Evaluating on the training split.
I0926 23:19:12.885536 139754842453824 spec.py:333] Evaluating on the validation split.
I0926 23:19:15.894787 139754842453824 spec.py:349] Evaluating on the test split.
I0926 23:19:18.748316 139754842453824 submission_runner.py:381] Time since start: 1398.29s, 	Step: 2320, 	{'train/accuracy': 0.9873344302177429, 'train/loss': 0.046253133565187454, 'train/mean_average_precision': 0.12206369197194639, 'validation/accuracy': 0.9847337603569031, 'validation/loss': 0.055462438613176346, 'validation/mean_average_precision': 0.11607175657001434, 'validation/num_examples': 43793, 'test/accuracy': 0.9837245941162109, 'test/loss': 0.0587640255689621, 'test/mean_average_precision': 0.11630518397026482, 'test/num_examples': 43793, 'score': 744.5920267105103, 'total_duration': 1398.2923798561096, 'accumulated_submission_time': 744.5920267105103, 'accumulated_eval_time': 653.5802991390228, 'accumulated_logging_time': 0.09009647369384766}
I0926 23:19:18.764046 139580607022848 logging_writer.py:48] [2320] accumulated_eval_time=653.580299, accumulated_logging_time=0.090096, accumulated_submission_time=744.592027, global_step=2320, preemption_count=0, score=744.592027, test/accuracy=0.983725, test/loss=0.058764, test/mean_average_precision=0.116305, test/num_examples=43793, total_duration=1398.292380, train/accuracy=0.987334, train/loss=0.046253, train/mean_average_precision=0.122064, validation/accuracy=0.984734, validation/loss=0.055462, validation/mean_average_precision=0.116072, validation/num_examples=43793
I0926 23:19:43.288596 139580715800320 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.031655021011829376, loss=0.04564206302165985
I0926 23:20:14.047651 139580607022848 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.02122589573264122, loss=0.05069665238261223
I0926 23:20:44.831333 139580715800320 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.023996593430638313, loss=0.04635639861226082
I0926 23:21:15.883059 139580607022848 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.04192278906702995, loss=0.042257316410541534
I0926 23:21:46.651204 139580715800320 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.043158940970897675, loss=0.043092112988233566
I0926 23:22:17.498017 139580607022848 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.013270589523017406, loss=0.03898989036679268
I0926 23:22:48.303762 139580715800320 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.021241113543510437, loss=0.04710927605628967
I0926 23:23:18.852730 139754842453824 spec.py:321] Evaluating on the training split.
I0926 23:25:13.260154 139754842453824 spec.py:333] Evaluating on the validation split.
I0926 23:25:16.689728 139754842453824 spec.py:349] Evaluating on the test split.
I0926 23:25:19.970062 139754842453824 submission_runner.py:381] Time since start: 1759.51s, 	Step: 3099, 	{'train/accuracy': 0.9877336621284485, 'train/loss': 0.04378492385149002, 'train/mean_average_precision': 0.14498747340365853, 'validation/accuracy': 0.9849082827568054, 'validation/loss': 0.05349517986178398, 'validation/mean_average_precision': 0.13723413532440942, 'validation/num_examples': 43793, 'test/accuracy': 0.9839280247688293, 'test/loss': 0.05659135431051254, 'test/mean_average_precision': 0.13605511715025898, 'test/num_examples': 43793, 'score': 984.6586670875549, 'total_duration': 1759.5141053199768, 'accumulated_submission_time': 984.6586670875549, 'accumulated_eval_time': 774.6975815296173, 'accumulated_logging_time': 0.11857318878173828}
I0926 23:25:19.987861 139580615415552 logging_writer.py:48] [3099] accumulated_eval_time=774.697582, accumulated_logging_time=0.118573, accumulated_submission_time=984.658667, global_step=3099, preemption_count=0, score=984.658667, test/accuracy=0.983928, test/loss=0.056591, test/mean_average_precision=0.136055, test/num_examples=43793, total_duration=1759.514105, train/accuracy=0.987734, train/loss=0.043785, train/mean_average_precision=0.144987, validation/accuracy=0.984908, validation/loss=0.053495, validation/mean_average_precision=0.137234, validation/num_examples=43793
I0926 23:25:20.666026 139580623808256 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.010503851808607578, loss=0.04038761928677559
I0926 23:25:52.087103 139580615415552 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.01356294471770525, loss=0.04740304499864578
I0926 23:26:23.454527 139580623808256 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.03428909182548523, loss=0.047572266310453415
I0926 23:26:54.696997 139580615415552 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.017029142007231712, loss=0.04029496759176254
I0926 23:27:26.099611 139580623808256 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.01764669083058834, loss=0.041920255869627
I0926 23:27:57.670577 139580615415552 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.01378562394529581, loss=0.0476023368537426
I0926 23:28:29.042473 139580623808256 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.012929779477417469, loss=0.04405143857002258
I0926 23:29:00.265980 139580615415552 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.014490337111055851, loss=0.04209720343351364
I0926 23:29:20.039891 139754842453824 spec.py:321] Evaluating on the training split.
I0926 23:31:16.359570 139754842453824 spec.py:333] Evaluating on the validation split.
I0926 23:31:19.335970 139754842453824 spec.py:349] Evaluating on the test split.
I0926 23:31:22.240815 139754842453824 submission_runner.py:381] Time since start: 2121.78s, 	Step: 3865, 	{'train/accuracy': 0.9881292581558228, 'train/loss': 0.04198610782623291, 'train/mean_average_precision': 0.1687162983919301, 'validation/accuracy': 0.9851985573768616, 'validation/loss': 0.05164087563753128, 'validation/mean_average_precision': 0.1501886949932247, 'validation/num_examples': 43793, 'test/accuracy': 0.984246015548706, 'test/loss': 0.054451651871204376, 'test/mean_average_precision': 0.14995864213649565, 'test/num_examples': 43793, 'score': 1224.687399148941, 'total_duration': 2121.7848269939423, 'accumulated_submission_time': 1224.687399148941, 'accumulated_eval_time': 896.8984174728394, 'accumulated_logging_time': 0.14932966232299805}
I0926 23:31:22.255860 139580607022848 logging_writer.py:48] [3865] accumulated_eval_time=896.898417, accumulated_logging_time=0.149330, accumulated_submission_time=1224.687399, global_step=3865, preemption_count=0, score=1224.687399, test/accuracy=0.984246, test/loss=0.054452, test/mean_average_precision=0.149959, test/num_examples=43793, total_duration=2121.784827, train/accuracy=0.988129, train/loss=0.041986, train/mean_average_precision=0.168716, validation/accuracy=0.985199, validation/loss=0.051641, validation/mean_average_precision=0.150189, validation/num_examples=43793
I0926 23:31:33.601500 139580632200960 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.013658984564244747, loss=0.04478941485285759
I0926 23:32:04.659300 139580607022848 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.015623011626303196, loss=0.04258821904659271
I0926 23:32:35.336810 139580632200960 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.014142267405986786, loss=0.04485456645488739
I0926 23:33:06.341082 139580607022848 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.017552820965647697, loss=0.04398126155138016
I0926 23:33:37.171725 139580632200960 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.013780436478555202, loss=0.042146895080804825
I0926 23:34:08.134994 139580607022848 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.014650743454694748, loss=0.04165487736463547
I0926 23:34:38.945714 139580632200960 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.011997994035482407, loss=0.041572440415620804
I0926 23:35:09.921023 139580607022848 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.01216210052371025, loss=0.04141847789287567
I0926 23:35:22.355345 139754842453824 spec.py:321] Evaluating on the training split.
I0926 23:37:22.238457 139754842453824 spec.py:333] Evaluating on the validation split.
I0926 23:37:25.619981 139754842453824 spec.py:349] Evaluating on the test split.
I0926 23:37:28.964495 139754842453824 submission_runner.py:381] Time since start: 2488.51s, 	Step: 4641, 	{'train/accuracy': 0.9881665110588074, 'train/loss': 0.040915895253419876, 'train/mean_average_precision': 0.20220748666171945, 'validation/accuracy': 0.9853734970092773, 'validation/loss': 0.05096198990941048, 'validation/mean_average_precision': 0.1701832627763418, 'validation/num_examples': 43793, 'test/accuracy': 0.9844178557395935, 'test/loss': 0.05386662483215332, 'test/mean_average_precision': 0.17234884858570346, 'test/num_examples': 43793, 'score': 1464.7664771080017, 'total_duration': 2488.5085406303406, 'accumulated_submission_time': 1464.7664771080017, 'accumulated_eval_time': 1023.5075256824493, 'accumulated_logging_time': 0.1750955581665039}
I0926 23:37:28.981462 139580732585728 logging_writer.py:48] [4641] accumulated_eval_time=1023.507526, accumulated_logging_time=0.175096, accumulated_submission_time=1464.766477, global_step=4641, preemption_count=0, score=1464.766477, test/accuracy=0.984418, test/loss=0.053867, test/mean_average_precision=0.172349, test/num_examples=43793, total_duration=2488.508541, train/accuracy=0.988167, train/loss=0.040916, train/mean_average_precision=0.202207, validation/accuracy=0.985373, validation/loss=0.050962, validation/mean_average_precision=0.170183, validation/num_examples=43793
I0926 23:37:47.804538 139689918985984 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.01128342468291521, loss=0.042501144111156464
I0926 23:38:19.070177 139580732585728 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.011712420731782913, loss=0.041277650743722916
I0926 23:38:49.963950 139689918985984 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.020471151918172836, loss=0.04615136235952377
I0926 23:39:20.816348 139580732585728 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.019247474148869514, loss=0.04106181114912033
I0926 23:39:51.526435 139689918985984 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.010972347110509872, loss=0.04606378823518753
I0926 23:40:22.383007 139580732585728 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.019205201417207718, loss=0.03927928954362869
I0926 23:40:53.056530 139689918985984 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.015402822755277157, loss=0.04658544063568115
I0926 23:41:24.118846 139580732585728 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.015383477322757244, loss=0.03971670940518379
I0926 23:41:29.113089 139754842453824 spec.py:321] Evaluating on the training split.
I0926 23:43:25.704033 139754842453824 spec.py:333] Evaluating on the validation split.
I0926 23:43:28.758113 139754842453824 spec.py:349] Evaluating on the test split.
I0926 23:43:31.699302 139754842453824 submission_runner.py:381] Time since start: 2851.24s, 	Step: 5417, 	{'train/accuracy': 0.9883888363838196, 'train/loss': 0.03952179104089737, 'train/mean_average_precision': 0.23850696321104237, 'validation/accuracy': 0.9857218265533447, 'validation/loss': 0.04876665771007538, 'validation/mean_average_precision': 0.19427855926041857, 'validation/num_examples': 43793, 'test/accuracy': 0.9847329258918762, 'test/loss': 0.05159736052155495, 'test/mean_average_precision': 0.1924794153798867, 'test/num_examples': 43793, 'score': 1704.877164363861, 'total_duration': 2851.2433495521545, 'accumulated_submission_time': 1704.877164363861, 'accumulated_eval_time': 1146.0936813354492, 'accumulated_logging_time': 0.20308685302734375}
I0926 23:43:31.715219 139591609804544 logging_writer.py:48] [5417] accumulated_eval_time=1146.093681, accumulated_logging_time=0.203087, accumulated_submission_time=1704.877164, global_step=5417, preemption_count=0, score=1704.877164, test/accuracy=0.984733, test/loss=0.051597, test/mean_average_precision=0.192479, test/num_examples=43793, total_duration=2851.243350, train/accuracy=0.988389, train/loss=0.039522, train/mean_average_precision=0.238507, validation/accuracy=0.985722, validation/loss=0.048767, validation/mean_average_precision=0.194279, validation/num_examples=43793
I0926 23:43:57.846884 139591618197248 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.027636682614684105, loss=0.04063034802675247
I0926 23:44:28.853425 139591609804544 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.009766311384737492, loss=0.039018165320158005
I0926 23:44:59.340234 139591618197248 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.015056929551064968, loss=0.038358770310878754
I0926 23:45:30.114279 139591609804544 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.016629135236144066, loss=0.04379785433411598
I0926 23:46:00.592461 139591618197248 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.01664132811129093, loss=0.038931235671043396
I0926 23:46:31.221871 139754842453824 spec.py:321] Evaluating on the training split.
I0926 23:48:29.004082 139754842453824 spec.py:333] Evaluating on the validation split.
I0926 23:48:32.074165 139754842453824 spec.py:349] Evaluating on the test split.
I0926 23:48:34.989508 139754842453824 submission_runner.py:381] Time since start: 3154.53s, 	Step: 6000, 	{'train/accuracy': 0.9893710613250732, 'train/loss': 0.0366615355014801, 'train/mean_average_precision': 0.2625566074233984, 'validation/accuracy': 0.9859856963157654, 'validation/loss': 0.047480687499046326, 'validation/mean_average_precision': 0.20522401545556054, 'validation/num_examples': 43793, 'test/accuracy': 0.9851077795028687, 'test/loss': 0.050244107842445374, 'test/mean_average_precision': 0.20725213837448525, 'test/num_examples': 43793, 'score': 1884.3642363548279, 'total_duration': 3154.5335721969604, 'accumulated_submission_time': 1884.3642363548279, 'accumulated_eval_time': 1269.8612875938416, 'accumulated_logging_time': 0.23130059242248535}
I0926 23:48:35.005334 139580732585728 logging_writer.py:48] [6000] accumulated_eval_time=1269.861288, accumulated_logging_time=0.231301, accumulated_submission_time=1884.364236, global_step=6000, preemption_count=0, score=1884.364236, test/accuracy=0.985108, test/loss=0.050244, test/mean_average_precision=0.207252, test/num_examples=43793, total_duration=3154.533572, train/accuracy=0.989371, train/loss=0.036662, train/mean_average_precision=0.262557, validation/accuracy=0.985986, validation/loss=0.047481, validation/mean_average_precision=0.205224, validation/num_examples=43793
I0926 23:48:35.022438 139689918985984 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1884.364236
I0926 23:48:35.072406 139754842453824 checkpoints.py:490] Saving checkpoint at step: 6000
I0926 23:48:35.172572 139754842453824 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_prelaunch_jax/adamw/ogbg_jax/trial_1/checkpoint_6000
I0926 23:48:35.173536 139754842453824 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_prelaunch_jax/adamw/ogbg_jax/trial_1/checkpoint_6000.
I0926 23:48:35.337216 139754842453824 submission_runner.py:549] Tuning trial 1/1
I0926 23:48:35.337432 139754842453824 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0926 23:48:35.340221 139754842453824 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/accuracy': 0.532181978225708, 'train/loss': 0.728960394859314, 'train/mean_average_precision': 0.02313895077271053, 'validation/accuracy': 0.5307276844978333, 'validation/loss': 0.7279101610183716, 'validation/mean_average_precision': 0.026399944647783197, 'validation/num_examples': 43793, 'test/accuracy': 0.5276063680648804, 'test/loss': 0.7299690246582031, 'test/mean_average_precision': 0.027237141320772127, 'test/num_examples': 43793, 'score': 24.34574866294861, 'total_duration': 323.96882033348083, 'accumulated_submission_time': 24.34574866294861, 'accumulated_eval_time': 299.62303018569946, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (766, {'train/accuracy': 0.9867082238197327, 'train/loss': 0.05706782266497612, 'train/mean_average_precision': 0.03911227886309096, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.066437728703022, 'validation/mean_average_precision': 0.039811373190403836, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06965232640504837, 'test/mean_average_precision': 0.04082655100716385, 'test/num_examples': 43793, 'score': 264.36876606941223, 'total_duration': 679.3277101516724, 'accumulated_submission_time': 264.36876606941223, 'accumulated_eval_time': 414.91666889190674, 'accumulated_logging_time': 0.031746864318847656, 'global_step': 766, 'preemption_count': 0}), (1549, {'train/accuracy': 0.9871712327003479, 'train/loss': 0.04783337935805321, 'train/mean_average_precision': 0.09123253410612571, 'validation/accuracy': 0.9844455122947693, 'validation/loss': 0.0578041635453701, 'validation/mean_average_precision': 0.08916381907961687, 'validation/num_examples': 43793, 'test/accuracy': 0.98348069190979, 'test/loss': 0.06123698502779007, 'test/mean_average_precision': 0.0873548839429084, 'test/num_examples': 43793, 'score': 504.47212624549866, 'total_duration': 1038.6355051994324, 'accumulated_submission_time': 504.47212624549866, 'accumulated_eval_time': 534.0861167907715, 'accumulated_logging_time': 0.05720090866088867, 'global_step': 1549, 'preemption_count': 0}), (2320, {'train/accuracy': 0.9873344302177429, 'train/loss': 0.046253133565187454, 'train/mean_average_precision': 0.12206369197194639, 'validation/accuracy': 0.9847337603569031, 'validation/loss': 0.055462438613176346, 'validation/mean_average_precision': 0.11607175657001434, 'validation/num_examples': 43793, 'test/accuracy': 0.9837245941162109, 'test/loss': 0.0587640255689621, 'test/mean_average_precision': 0.11630518397026482, 'test/num_examples': 43793, 'score': 744.5920267105103, 'total_duration': 1398.2923798561096, 'accumulated_submission_time': 744.5920267105103, 'accumulated_eval_time': 653.5802991390228, 'accumulated_logging_time': 0.09009647369384766, 'global_step': 2320, 'preemption_count': 0}), (3099, {'train/accuracy': 0.9877336621284485, 'train/loss': 0.04378492385149002, 'train/mean_average_precision': 0.14498747340365853, 'validation/accuracy': 0.9849082827568054, 'validation/loss': 0.05349517986178398, 'validation/mean_average_precision': 0.13723413532440942, 'validation/num_examples': 43793, 'test/accuracy': 0.9839280247688293, 'test/loss': 0.05659135431051254, 'test/mean_average_precision': 0.13605511715025898, 'test/num_examples': 43793, 'score': 984.6586670875549, 'total_duration': 1759.5141053199768, 'accumulated_submission_time': 984.6586670875549, 'accumulated_eval_time': 774.6975815296173, 'accumulated_logging_time': 0.11857318878173828, 'global_step': 3099, 'preemption_count': 0}), (3865, {'train/accuracy': 0.9881292581558228, 'train/loss': 0.04198610782623291, 'train/mean_average_precision': 0.1687162983919301, 'validation/accuracy': 0.9851985573768616, 'validation/loss': 0.05164087563753128, 'validation/mean_average_precision': 0.1501886949932247, 'validation/num_examples': 43793, 'test/accuracy': 0.984246015548706, 'test/loss': 0.054451651871204376, 'test/mean_average_precision': 0.14995864213649565, 'test/num_examples': 43793, 'score': 1224.687399148941, 'total_duration': 2121.7848269939423, 'accumulated_submission_time': 1224.687399148941, 'accumulated_eval_time': 896.8984174728394, 'accumulated_logging_time': 0.14932966232299805, 'global_step': 3865, 'preemption_count': 0}), (4641, {'train/accuracy': 0.9881665110588074, 'train/loss': 0.040915895253419876, 'train/mean_average_precision': 0.20220748666171945, 'validation/accuracy': 0.9853734970092773, 'validation/loss': 0.05096198990941048, 'validation/mean_average_precision': 0.1701832627763418, 'validation/num_examples': 43793, 'test/accuracy': 0.9844178557395935, 'test/loss': 0.05386662483215332, 'test/mean_average_precision': 0.17234884858570346, 'test/num_examples': 43793, 'score': 1464.7664771080017, 'total_duration': 2488.5085406303406, 'accumulated_submission_time': 1464.7664771080017, 'accumulated_eval_time': 1023.5075256824493, 'accumulated_logging_time': 0.1750955581665039, 'global_step': 4641, 'preemption_count': 0}), (5417, {'train/accuracy': 0.9883888363838196, 'train/loss': 0.03952179104089737, 'train/mean_average_precision': 0.23850696321104237, 'validation/accuracy': 0.9857218265533447, 'validation/loss': 0.04876665771007538, 'validation/mean_average_precision': 0.19427855926041857, 'validation/num_examples': 43793, 'test/accuracy': 0.9847329258918762, 'test/loss': 0.05159736052155495, 'test/mean_average_precision': 0.1924794153798867, 'test/num_examples': 43793, 'score': 1704.877164363861, 'total_duration': 2851.2433495521545, 'accumulated_submission_time': 1704.877164363861, 'accumulated_eval_time': 1146.0936813354492, 'accumulated_logging_time': 0.20308685302734375, 'global_step': 5417, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9893710613250732, 'train/loss': 0.0366615355014801, 'train/mean_average_precision': 0.2625566074233984, 'validation/accuracy': 0.9859856963157654, 'validation/loss': 0.047480687499046326, 'validation/mean_average_precision': 0.20522401545556054, 'validation/num_examples': 43793, 'test/accuracy': 0.9851077795028687, 'test/loss': 0.050244107842445374, 'test/mean_average_precision': 0.20725213837448525, 'test/num_examples': 43793, 'score': 1884.3642363548279, 'total_duration': 3154.5335721969604, 'accumulated_submission_time': 1884.3642363548279, 'accumulated_eval_time': 1269.8612875938416, 'accumulated_logging_time': 0.23130059242248535, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0926 23:48:35.340366 139754842453824 submission_runner.py:552] Timing: 1884.3642363548279
I0926 23:48:35.340432 139754842453824 submission_runner.py:554] Total number of evals: 9
I0926 23:48:35.340487 139754842453824 submission_runner.py:555] ====================
I0926 23:48:35.340612 139754842453824 submission_runner.py:625] Final ogbg score: 1884.3642363548279
