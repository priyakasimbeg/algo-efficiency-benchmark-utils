python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_prelaunch_jax/adamw --overwrite=true --save_checkpoints=false --max_global_steps=14000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_09-26-2023-20-49-40.log
2023-09-26 20:49:44.970713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0926 20:50:03.870085 140524574070592 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_prelaunch_jax/adamw/imagenet_vit_jax.
I0926 20:50:04.777506 140524574070592 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0926 20:50:04.778464 140524574070592 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0926 20:50:04.778635 140524574070592 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0926 20:50:04.784713 140524574070592 submission_runner.py:507] Using RNG seed 2736159583
I0926 20:50:10.231677 140524574070592 submission_runner.py:516] --- Tuning run 1/1 ---
I0926 20:50:10.231933 140524574070592 submission_runner.py:521] Creating tuning directory at /experiment_runs/timing_prelaunch_jax/adamw/imagenet_vit_jax/trial_1.
I0926 20:50:10.232138 140524574070592 logger_utils.py:92] Saving hparams to /experiment_runs/timing_prelaunch_jax/adamw/imagenet_vit_jax/trial_1/hparams.json.
I0926 20:50:10.421345 140524574070592 submission_runner.py:191] Initializing dataset.
I0926 20:50:10.437607 140524574070592 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0926 20:50:10.448140 140524574070592 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0926 20:50:10.832701 140524574070592 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0926 20:50:19.627281 140524574070592 submission_runner.py:198] Initializing model.
I0926 20:50:28.424137 140524574070592 submission_runner.py:232] Initializing optimizer.
I0926 20:50:29.388497 140524574070592 submission_runner.py:239] Initializing metrics bundle.
I0926 20:50:29.388710 140524574070592 submission_runner.py:257] Initializing checkpoint and logger.
I0926 20:50:29.389861 140524574070592 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_prelaunch_jax/adamw/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0926 20:50:29.390007 140524574070592 submission_runner.py:277] Saving meta data to /experiment_runs/timing_prelaunch_jax/adamw/imagenet_vit_jax/trial_1/meta_data_0.json.
I0926 20:50:30.335926 140524574070592 submission_runner.py:280] Saving flags to /experiment_runs/timing_prelaunch_jax/adamw/imagenet_vit_jax/trial_1/flags_0.json.
I0926 20:50:30.345492 140524574070592 submission_runner.py:290] Starting training loop.
2023-09-26 20:51:18.650780: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-09-26 20:51:21.682865: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
I0926 20:51:23.241374 140360428807936 logging_writer.py:48] [0] global_step=0, grad_norm=0.31769663095474243, loss=6.907756805419922
I0926 20:51:23.258370 140524574070592 spec.py:321] Evaluating on the training split.
I0926 20:51:23.266401 140524574070592 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0926 20:51:23.275255 140524574070592 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0926 20:51:23.356507 140524574070592 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0926 20:51:40.500935 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 20:51:40.509325 140524574070592 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0926 20:51:40.522459 140524574070592 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0926 20:51:40.590045 140524574070592 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0926 20:51:58.272636 140524574070592 spec.py:349] Evaluating on the test split.
I0926 20:51:58.281715 140524574070592 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0926 20:51:58.290142 140524574070592 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0926 20:51:58.352530 140524574070592 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0926 20:52:03.886597 140524574070592 submission_runner.py:381] Time since start: 93.54s, 	Step: 1, 	{'train/accuracy': 0.0010742187732830644, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 52.91277360916138, 'total_duration': 93.54104375839233, 'accumulated_submission_time': 52.91277360916138, 'accumulated_eval_time': 40.628175497055054, 'accumulated_logging_time': 0}
I0926 20:52:03.906614 140318292817664 logging_writer.py:48] [1] accumulated_eval_time=40.628175, accumulated_logging_time=0, accumulated_submission_time=52.912774, global_step=1, preemption_count=0, score=52.912774, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=93.541044, train/accuracy=0.001074, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0926 20:53:03.605138 140359086630656 logging_writer.py:48] [100] global_step=100, grad_norm=0.3933399021625519, loss=6.9018168449401855
I0926 20:53:42.945517 140359095023360 logging_writer.py:48] [200] global_step=200, grad_norm=0.5018706321716309, loss=6.851391792297363
I0926 20:54:22.064603 140359086630656 logging_writer.py:48] [300] global_step=300, grad_norm=0.5606316328048706, loss=6.775953769683838
I0926 20:55:01.155511 140359095023360 logging_writer.py:48] [400] global_step=400, grad_norm=0.7117374539375305, loss=6.722426414489746
I0926 20:55:40.263283 140359086630656 logging_writer.py:48] [500] global_step=500, grad_norm=0.6380473971366882, loss=6.649009704589844
I0926 20:56:19.380692 140359095023360 logging_writer.py:48] [600] global_step=600, grad_norm=0.9149178862571716, loss=6.7054901123046875
I0926 20:56:58.501017 140359086630656 logging_writer.py:48] [700] global_step=700, grad_norm=0.7364795804023743, loss=6.5453925132751465
I0926 20:57:37.628909 140359095023360 logging_writer.py:48] [800] global_step=800, grad_norm=1.5523499250411987, loss=6.534436225891113
I0926 20:58:17.525199 140359086630656 logging_writer.py:48] [900] global_step=900, grad_norm=0.7775407433509827, loss=6.519551753997803
I0926 20:58:57.348895 140359095023360 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8898226618766785, loss=6.757175445556641
I0926 20:59:04.142942 140524574070592 spec.py:321] Evaluating on the training split.
I0926 20:59:15.191496 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 20:59:23.328824 140524574070592 spec.py:349] Evaluating on the test split.
I0926 20:59:25.145139 140524574070592 submission_runner.py:381] Time since start: 534.80s, 	Step: 1019, 	{'train/accuracy': 0.026914061978459358, 'train/loss': 6.073877334594727, 'validation/accuracy': 0.025340000167489052, 'validation/loss': 6.097294330596924, 'validation/num_examples': 50000, 'test/accuracy': 0.020200001075863838, 'test/loss': 6.174903392791748, 'test/num_examples': 10000, 'score': 473.1151487827301, 'total_duration': 534.7995736598969, 'accumulated_submission_time': 473.1151487827301, 'accumulated_eval_time': 61.6303825378418, 'accumulated_logging_time': 0.028868675231933594}
I0926 20:59:25.161946 140318301210368 logging_writer.py:48] [1019] accumulated_eval_time=61.630383, accumulated_logging_time=0.028869, accumulated_submission_time=473.115149, global_step=1019, preemption_count=0, score=473.115149, test/accuracy=0.020200, test/loss=6.174903, test/num_examples=10000, total_duration=534.799574, train/accuracy=0.026914, train/loss=6.073877, validation/accuracy=0.025340, validation/loss=6.097294, validation/num_examples=50000
I0926 20:59:57.324334 140318309603072 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8175485134124756, loss=6.438508987426758
I0926 21:00:36.485128 140318301210368 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9718619585037231, loss=6.425168514251709
I0926 21:01:15.891096 140318309603072 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8929819464683533, loss=6.454217433929443
I0926 21:01:55.060775 140318301210368 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.474021553993225, loss=6.244319438934326
I0926 21:02:34.223673 140318309603072 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9186022281646729, loss=6.178899765014648
I0926 21:03:13.423992 140318301210368 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.859763503074646, loss=6.365049362182617
I0926 21:03:52.648797 140318309603072 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.2771464586257935, loss=6.145521640777588
I0926 21:04:32.326636 140318301210368 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7642711997032166, loss=6.417544364929199
I0926 21:05:12.077010 140318309603072 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.902113139629364, loss=6.2462286949157715
I0926 21:05:51.340662 140318301210368 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.783598005771637, loss=6.001117706298828
I0926 21:06:25.222728 140524574070592 spec.py:321] Evaluating on the training split.
I0926 21:06:36.500047 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 21:06:44.639736 140524574070592 spec.py:349] Evaluating on the test split.
I0926 21:06:46.295909 140524574070592 submission_runner.py:381] Time since start: 975.95s, 	Step: 2087, 	{'train/accuracy': 0.06640625, 'train/loss': 5.4670515060424805, 'validation/accuracy': 0.06325999647378922, 'validation/loss': 5.494157314300537, 'validation/num_examples': 50000, 'test/accuracy': 0.048500001430511475, 'test/loss': 5.681517124176025, 'test/num_examples': 10000, 'score': 893.1392204761505, 'total_duration': 975.9503483772278, 'accumulated_submission_time': 893.1392204761505, 'accumulated_eval_time': 82.70357537269592, 'accumulated_logging_time': 0.056253910064697266}
I0926 21:06:46.312675 140318309603072 logging_writer.py:48] [2087] accumulated_eval_time=82.703575, accumulated_logging_time=0.056254, accumulated_submission_time=893.139220, global_step=2087, preemption_count=0, score=893.139220, test/accuracy=0.048500, test/loss=5.681517, test/num_examples=10000, total_duration=975.950348, train/accuracy=0.066406, train/loss=5.467052, validation/accuracy=0.063260, validation/loss=5.494157, validation/num_examples=50000
I0926 21:06:51.811094 140318301210368 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8133463263511658, loss=6.6245927810668945
I0926 21:07:31.038935 140318309603072 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8830204010009766, loss=6.003752708435059
I0926 21:08:10.232937 140318301210368 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8354890942573547, loss=5.871001720428467
I0926 21:08:49.692441 140318309603072 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8346001505851746, loss=5.945188999176025
I0926 21:09:28.913094 140318301210368 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.1772674322128296, loss=5.913288593292236
I0926 21:10:08.259397 140318309603072 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9983723759651184, loss=5.922463417053223
I0926 21:10:47.948536 140318301210368 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8151991963386536, loss=5.860294342041016
I0926 21:11:27.544269 140318309603072 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8480753302574158, loss=6.364680290222168
I0926 21:12:07.371293 140318301210368 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6481878161430359, loss=6.562893390655518
I0926 21:12:46.788093 140318309603072 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9075787663459778, loss=5.739761829376221
I0926 21:13:26.512270 140318301210368 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8636813163757324, loss=5.686262607574463
I0926 21:13:46.615945 140524574070592 spec.py:321] Evaluating on the training split.
I0926 21:13:58.233552 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 21:14:07.567354 140524574070592 spec.py:349] Evaluating on the test split.
I0926 21:14:09.231697 140524574070592 submission_runner.py:381] Time since start: 1418.89s, 	Step: 3153, 	{'train/accuracy': 0.10681640356779099, 'train/loss': 4.961960792541504, 'validation/accuracy': 0.09945999830961227, 'validation/loss': 5.0167107582092285, 'validation/num_examples': 50000, 'test/accuracy': 0.07750000059604645, 'test/loss': 5.282081604003906, 'test/num_examples': 10000, 'score': 1313.388878583908, 'total_duration': 1418.8861346244812, 'accumulated_submission_time': 1313.388878583908, 'accumulated_eval_time': 105.31931924819946, 'accumulated_logging_time': 0.08569741249084473}
I0926 21:14:09.250458 140318309603072 logging_writer.py:48] [3153] accumulated_eval_time=105.319319, accumulated_logging_time=0.085697, accumulated_submission_time=1313.388879, global_step=3153, preemption_count=0, score=1313.388879, test/accuracy=0.077500, test/loss=5.282082, test/num_examples=10000, total_duration=1418.886135, train/accuracy=0.106816, train/loss=4.961961, validation/accuracy=0.099460, validation/loss=5.016711, validation/num_examples=50000
I0926 21:14:28.087012 140318301210368 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.1712404489517212, loss=5.670925140380859
I0926 21:15:07.330710 140318309603072 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.899375855922699, loss=5.6383891105651855
I0926 21:15:46.750983 140318301210368 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7752224802970886, loss=5.707655906677246
I0926 21:16:25.989425 140318309603072 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7738556265830994, loss=6.075296401977539
I0926 21:17:05.342981 140318301210368 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6663134098052979, loss=6.435029983520508
I0926 21:17:44.641539 140318309603072 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.0224796533584595, loss=5.581782341003418
I0926 21:18:24.429919 140318301210368 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9719488620758057, loss=5.52888822555542
I0926 21:19:03.728283 140318309603072 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9701781272888184, loss=5.741366863250732
I0926 21:19:43.305986 140318301210368 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8938045501708984, loss=5.566621780395508
I0926 21:20:22.778794 140318309603072 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7421289086341858, loss=5.7459893226623535
I0926 21:21:02.370235 140318301210368 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8212664127349854, loss=5.362105369567871
I0926 21:21:09.316990 140524574070592 spec.py:321] Evaluating on the training split.
I0926 21:21:20.980559 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 21:21:31.374656 140524574070592 spec.py:349] Evaluating on the test split.
I0926 21:21:33.033650 140524574070592 submission_runner.py:381] Time since start: 1862.69s, 	Step: 4219, 	{'train/accuracy': 0.15978515148162842, 'train/loss': 4.552663803100586, 'validation/accuracy': 0.15053999423980713, 'validation/loss': 4.633122444152832, 'validation/num_examples': 50000, 'test/accuracy': 0.11390000581741333, 'test/loss': 4.9347920417785645, 'test/num_examples': 10000, 'score': 1733.419811964035, 'total_duration': 1862.6880745887756, 'accumulated_submission_time': 1733.419811964035, 'accumulated_eval_time': 129.03595662117004, 'accumulated_logging_time': 0.11454367637634277}
I0926 21:21:33.054103 140318309603072 logging_writer.py:48] [4219] accumulated_eval_time=129.035957, accumulated_logging_time=0.114544, accumulated_submission_time=1733.419812, global_step=4219, preemption_count=0, score=1733.419812, test/accuracy=0.113900, test/loss=4.934792, test/num_examples=10000, total_duration=1862.688075, train/accuracy=0.159785, train/loss=4.552664, validation/accuracy=0.150540, validation/loss=4.633122, validation/num_examples=50000
I0926 21:22:05.242988 140318301210368 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8294707536697388, loss=5.742999076843262
I0926 21:22:44.494137 140318309603072 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8915421366691589, loss=5.353484630584717
I0926 21:23:23.928915 140318301210368 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6042093634605408, loss=6.501921653747559
I0926 21:24:03.176479 140318309603072 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7001276016235352, loss=5.645676612854004
I0926 21:24:42.416981 140318301210368 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7338058948516846, loss=6.505136966705322
I0926 21:25:21.678642 140318309603072 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8129563927650452, loss=5.349977970123291
I0926 21:26:01.111063 140318301210368 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8048409819602966, loss=5.206503868103027
I0926 21:26:41.442144 140318309603072 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7920309901237488, loss=5.219504356384277
I0926 21:27:21.212729 140318301210368 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6892300248146057, loss=6.267757415771484
I0926 21:28:00.961636 140318309603072 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6897600293159485, loss=6.306212902069092
I0926 21:28:33.042321 140524574070592 spec.py:321] Evaluating on the training split.
I0926 21:28:44.703779 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 21:28:52.785404 140524574070592 spec.py:349] Evaluating on the test split.
I0926 21:28:54.454563 140524574070592 submission_runner.py:381] Time since start: 2304.11s, 	Step: 5282, 	{'train/accuracy': 0.21640624105930328, 'train/loss': 4.0838942527771, 'validation/accuracy': 0.19467999041080475, 'validation/loss': 4.2257161140441895, 'validation/num_examples': 50000, 'test/accuracy': 0.14259999990463257, 'test/loss': 4.61821985244751, 'test/num_examples': 10000, 'score': 2153.3723678588867, 'total_duration': 2304.1089975833893, 'accumulated_submission_time': 2153.3723678588867, 'accumulated_eval_time': 150.448171377182, 'accumulated_logging_time': 0.14507079124450684}
I0926 21:28:54.473848 140318301210368 logging_writer.py:48] [5282] accumulated_eval_time=150.448171, accumulated_logging_time=0.145071, accumulated_submission_time=2153.372368, global_step=5282, preemption_count=0, score=2153.372368, test/accuracy=0.142600, test/loss=4.618220, test/num_examples=10000, total_duration=2304.108998, train/accuracy=0.216406, train/loss=4.083894, validation/accuracy=0.194680, validation/loss=4.225716, validation/num_examples=50000
I0926 21:29:01.979666 140318309603072 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8763331174850464, loss=5.162594318389893
I0926 21:29:41.272394 140318301210368 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8536463975906372, loss=5.198189735412598
I0926 21:30:20.561355 140318309603072 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.936169445514679, loss=5.18153715133667
I0926 21:31:00.021820 140318301210368 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6399869322776794, loss=6.3837890625
I0926 21:31:39.297167 140318309603072 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7161877751350403, loss=5.118251800537109
I0926 21:32:18.562694 140318301210368 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8703876733779907, loss=5.15880012512207
I0926 21:32:58.114722 140318309603072 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.665008544921875, loss=5.946462631225586
I0926 21:33:37.933758 140318301210368 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8665066957473755, loss=4.998237133026123
I0926 21:34:18.333599 140318309603072 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7772295475006104, loss=5.0952887535095215
I0926 21:34:58.548795 140318301210368 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8969766497612, loss=4.975836753845215
I0926 21:35:38.643414 140318309603072 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9287469387054443, loss=4.999325752258301
I0926 21:35:54.834430 140524574070592 spec.py:321] Evaluating on the training split.
I0926 21:36:06.918331 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 21:36:15.172368 140524574070592 spec.py:349] Evaluating on the test split.
I0926 21:36:16.826712 140524574070592 submission_runner.py:381] Time since start: 2746.48s, 	Step: 6343, 	{'train/accuracy': 0.25376951694488525, 'train/loss': 3.827409029006958, 'validation/accuracy': 0.23635999858379364, 'validation/loss': 3.924687623977661, 'validation/num_examples': 50000, 'test/accuracy': 0.1787000149488449, 'test/loss': 4.35499906539917, 'test/num_examples': 10000, 'score': 2573.6964948177338, 'total_duration': 2746.4811346530914, 'accumulated_submission_time': 2573.6964948177338, 'accumulated_eval_time': 172.44042682647705, 'accumulated_logging_time': 0.17470407485961914}
I0926 21:36:16.844549 140318301210368 logging_writer.py:48] [6343] accumulated_eval_time=172.440427, accumulated_logging_time=0.174704, accumulated_submission_time=2573.696495, global_step=6343, preemption_count=0, score=2573.696495, test/accuracy=0.178700, test/loss=4.354999, test/num_examples=10000, total_duration=2746.481135, train/accuracy=0.253770, train/loss=3.827409, validation/accuracy=0.236360, validation/loss=3.924688, validation/num_examples=50000
I0926 21:36:39.648546 140318309603072 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5945913195610046, loss=6.231532096862793
I0926 21:37:18.953216 140318301210368 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6234375834465027, loss=6.227248668670654
I0926 21:37:58.246195 140318309603072 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6531969308853149, loss=5.2920637130737305
I0926 21:38:37.618484 140318301210368 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.851108729839325, loss=4.832704067230225
I0926 21:39:16.939030 140318309603072 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9469054937362671, loss=4.999675750732422
I0926 21:39:56.547743 140318301210368 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7077972292900085, loss=4.853170871734619
I0926 21:40:36.721551 140318309603072 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5599070191383362, loss=5.748956680297852
I0926 21:41:16.492099 140318301210368 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6521833539009094, loss=4.886262893676758
I0926 21:41:56.200894 140318309603072 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7572869658470154, loss=5.032403945922852
I0926 21:42:36.171802 140318301210368 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5376803874969482, loss=6.283633708953857
I0926 21:43:15.977078 140318309603072 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.8646060824394226, loss=4.920705318450928
I0926 21:43:16.848565 140524574070592 spec.py:321] Evaluating on the training split.
I0926 21:43:28.599579 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 21:43:39.483850 140524574070592 spec.py:349] Evaluating on the test split.
I0926 21:43:41.127267 140524574070592 submission_runner.py:381] Time since start: 3190.78s, 	Step: 7404, 	{'train/accuracy': 0.2941601574420929, 'train/loss': 3.5217747688293457, 'validation/accuracy': 0.2714399993419647, 'validation/loss': 3.6460108757019043, 'validation/num_examples': 50000, 'test/accuracy': 0.21250000596046448, 'test/loss': 4.104587078094482, 'test/num_examples': 10000, 'score': 2993.6651196479797, 'total_duration': 3190.7816865444183, 'accumulated_submission_time': 2993.6651196479797, 'accumulated_eval_time': 196.71908116340637, 'accumulated_logging_time': 0.20231270790100098}
I0926 21:43:41.150681 140318301210368 logging_writer.py:48] [7404] accumulated_eval_time=196.719081, accumulated_logging_time=0.202313, accumulated_submission_time=2993.665120, global_step=7404, preemption_count=0, score=2993.665120, test/accuracy=0.212500, test/loss=4.104587, test/num_examples=10000, total_duration=3190.781687, train/accuracy=0.294160, train/loss=3.521775, validation/accuracy=0.271440, validation/loss=3.646011, validation/num_examples=50000
I0926 21:44:19.278558 140318309603072 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6051139235496521, loss=5.39523458480835
I0926 21:44:58.545587 140318301210368 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5555527806282043, loss=5.5254693031311035
I0926 21:45:38.002139 140318309603072 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7151342034339905, loss=4.643130302429199
I0926 21:46:17.302274 140318301210368 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7881758213043213, loss=4.662926197052002
I0926 21:46:56.772447 140318309603072 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7481876611709595, loss=4.694944381713867
I0926 21:47:36.965650 140318301210368 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6775375604629517, loss=4.679853439331055
I0926 21:48:17.335230 140318309603072 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6391351222991943, loss=4.722771167755127
I0926 21:48:57.459292 140318301210368 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6721842288970947, loss=4.501456260681152
I0926 21:49:37.845258 140318309603072 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6078556776046753, loss=4.8570027351379395
I0926 21:50:18.204778 140318301210368 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6584458351135254, loss=4.560687065124512
I0926 21:50:41.260515 140524574070592 spec.py:321] Evaluating on the training split.
I0926 21:50:53.397855 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 21:51:05.501399 140524574070592 spec.py:349] Evaluating on the test split.
I0926 21:51:07.185768 140524574070592 submission_runner.py:381] Time since start: 3636.84s, 	Step: 8459, 	{'train/accuracy': 0.3467382788658142, 'train/loss': 3.139017343521118, 'validation/accuracy': 0.3193399906158447, 'validation/loss': 3.2893295288085938, 'validation/num_examples': 50000, 'test/accuracy': 0.24800001084804535, 'test/loss': 3.8087751865386963, 'test/num_examples': 10000, 'score': 3413.7358548641205, 'total_duration': 3636.8401494026184, 'accumulated_submission_time': 3413.7358548641205, 'accumulated_eval_time': 222.64426279067993, 'accumulated_logging_time': 0.24019074440002441}
I0926 21:51:07.216745 140318309603072 logging_writer.py:48] [8459] accumulated_eval_time=222.644263, accumulated_logging_time=0.240191, accumulated_submission_time=3413.735855, global_step=8459, preemption_count=0, score=3413.735855, test/accuracy=0.248000, test/loss=3.808775, test/num_examples=10000, total_duration=3636.840149, train/accuracy=0.346738, train/loss=3.139017, validation/accuracy=0.319340, validation/loss=3.289330, validation/num_examples=50000
I0926 21:51:23.735251 140318301210368 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7102470993995667, loss=4.637377738952637
I0926 21:52:02.985292 140318309603072 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6061804890632629, loss=4.970368385314941
I0926 21:52:42.242909 140318301210368 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5545518398284912, loss=6.0309553146362305
I0926 21:53:21.718015 140318309603072 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5012663006782532, loss=5.5648393630981445
I0926 21:54:01.178032 140318301210368 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.7163287401199341, loss=5.28097677230835
I0926 21:54:41.058947 140318309603072 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7177556753158569, loss=4.4641194343566895
I0926 21:55:21.231364 140318301210368 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5620378851890564, loss=5.637514591217041
I0926 21:56:01.473123 140318309603072 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7206928133964539, loss=4.428444862365723
I0926 21:56:42.074458 140318301210368 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7073135375976562, loss=4.447015762329102
I0926 21:57:22.944307 140318309603072 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7016600370407104, loss=4.734007835388184
I0926 21:58:03.795624 140318301210368 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5705940127372742, loss=4.385329246520996
I0926 21:58:07.461395 140524574070592 spec.py:321] Evaluating on the training split.
I0926 21:58:20.345221 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 21:58:35.570135 140524574070592 spec.py:349] Evaluating on the test split.
I0926 21:58:37.212363 140524574070592 submission_runner.py:381] Time since start: 4086.87s, 	Step: 9511, 	{'train/accuracy': 0.38544920086860657, 'train/loss': 2.999854326248169, 'validation/accuracy': 0.3493399918079376, 'validation/loss': 3.1870996952056885, 'validation/num_examples': 50000, 'test/accuracy': 0.26500001549720764, 'test/loss': 3.714484453201294, 'test/num_examples': 10000, 'score': 3833.9398651123047, 'total_duration': 4086.866806268692, 'accumulated_submission_time': 3833.9398651123047, 'accumulated_eval_time': 252.3952465057373, 'accumulated_logging_time': 0.2862672805786133}
I0926 21:58:37.237655 140318309603072 logging_writer.py:48] [9511] accumulated_eval_time=252.395247, accumulated_logging_time=0.286267, accumulated_submission_time=3833.939865, global_step=9511, preemption_count=0, score=3833.939865, test/accuracy=0.265000, test/loss=3.714484, test/num_examples=10000, total_duration=4086.866806, train/accuracy=0.385449, train/loss=2.999854, validation/accuracy=0.349340, validation/loss=3.187100, validation/num_examples=50000
I0926 21:59:12.566581 140318301210368 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6877217888832092, loss=4.514389991760254
I0926 21:59:51.844074 140318309603072 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7148844003677368, loss=4.475598335266113
I0926 22:00:31.108046 140318301210368 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7637593746185303, loss=4.321510314941406
I0926 22:01:11.288700 140318309603072 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8504809141159058, loss=4.460417747497559
I0926 22:01:51.397557 140318301210368 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.730327308177948, loss=4.180413722991943
I0926 22:02:32.249727 140318309603072 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6800041198730469, loss=4.359038352966309
I0926 22:03:12.689424 140318301210368 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6918397545814514, loss=4.23247766494751
I0926 22:03:52.965065 140318309603072 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6049420833587646, loss=4.944277763366699
I0926 22:04:33.279349 140318301210368 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6595684289932251, loss=4.369869232177734
I0926 22:05:13.530615 140318309603072 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7375211119651794, loss=4.157933712005615
I0926 22:05:37.305234 140524574070592 spec.py:321] Evaluating on the training split.
I0926 22:05:51.213135 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 22:06:07.165364 140524574070592 spec.py:349] Evaluating on the test split.
I0926 22:06:08.834375 140524574070592 submission_runner.py:381] Time since start: 4538.49s, 	Step: 10561, 	{'train/accuracy': 0.4374414086341858, 'train/loss': 2.6394755840301514, 'validation/accuracy': 0.37911999225616455, 'validation/loss': 2.918146848678589, 'validation/num_examples': 50000, 'test/accuracy': 0.29520002007484436, 'test/loss': 3.4893696308135986, 'test/num_examples': 10000, 'score': 4253.970742702484, 'total_duration': 4538.488778829575, 'accumulated_submission_time': 4253.970742702484, 'accumulated_eval_time': 283.9243507385254, 'accumulated_logging_time': 0.32251834869384766}
I0926 22:06:08.865493 140318301210368 logging_writer.py:48] [10561] accumulated_eval_time=283.924351, accumulated_logging_time=0.322518, accumulated_submission_time=4253.970743, global_step=10561, preemption_count=0, score=4253.970743, test/accuracy=0.295200, test/loss=3.489370, test/num_examples=10000, total_duration=4538.488779, train/accuracy=0.437441, train/loss=2.639476, validation/accuracy=0.379120, validation/loss=2.918147, validation/num_examples=50000
I0926 22:06:24.565497 140318309603072 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7986209392547607, loss=4.313215255737305
I0926 22:07:03.803908 140318301210368 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.665949821472168, loss=4.244047164916992
I0926 22:07:43.051055 140318309603072 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.4951196014881134, loss=5.708282947540283
I0926 22:08:22.507883 140318301210368 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.8307800889015198, loss=4.308711528778076
I0926 22:09:01.791720 140318309603072 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6795588731765747, loss=4.077960014343262
I0926 22:09:41.254237 140318301210368 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5932480096817017, loss=4.372718334197998
I0926 22:10:21.153207 140318309603072 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.4683493673801422, loss=6.05102014541626
I0926 22:11:01.495505 140318301210368 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9352394938468933, loss=4.166022777557373
I0926 22:11:42.054383 140318309603072 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6534502506256104, loss=4.116347312927246
I0926 22:12:22.032575 140318301210368 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6296749711036682, loss=4.16030216217041
I0926 22:13:01.603387 140318309603072 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6575795412063599, loss=4.168579578399658
I0926 22:13:09.166589 140524574070592 spec.py:321] Evaluating on the training split.
I0926 22:13:23.133177 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 22:13:43.954568 140524574070592 spec.py:349] Evaluating on the test split.
I0926 22:13:45.596390 140524574070592 submission_runner.py:381] Time since start: 4995.25s, 	Step: 11620, 	{'train/accuracy': 0.4425976574420929, 'train/loss': 2.596738576889038, 'validation/accuracy': 0.4142199754714966, 'validation/loss': 2.7438042163848877, 'validation/num_examples': 50000, 'test/accuracy': 0.3189000189304352, 'test/loss': 3.3495848178863525, 'test/num_examples': 10000, 'score': 4674.231693506241, 'total_duration': 4995.250826120377, 'accumulated_submission_time': 4674.231693506241, 'accumulated_eval_time': 320.35413122177124, 'accumulated_logging_time': 0.3683288097381592}
I0926 22:13:45.616955 140318301210368 logging_writer.py:48] [11620] accumulated_eval_time=320.354131, accumulated_logging_time=0.368329, accumulated_submission_time=4674.231694, global_step=11620, preemption_count=0, score=4674.231694, test/accuracy=0.318900, test/loss=3.349585, test/num_examples=10000, total_duration=4995.250826, train/accuracy=0.442598, train/loss=2.596739, validation/accuracy=0.414220, validation/loss=2.743804, validation/num_examples=50000
I0926 22:14:17.400039 140318309603072 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5178676843643188, loss=5.94736909866333
I0926 22:14:56.687514 140318301210368 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.8610084652900696, loss=4.5458664894104
I0926 22:15:36.136504 140318309603072 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.47835054993629456, loss=5.899672985076904
I0926 22:16:16.434713 140318301210368 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5925487279891968, loss=5.448849678039551
I0926 22:16:56.693346 140318309603072 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5336986780166626, loss=5.643464088439941
I0926 22:17:37.368572 140318301210368 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8106100559234619, loss=4.1187520027160645
I0926 22:18:17.859306 140318309603072 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7003322839736938, loss=4.001818656921387
I0926 22:18:57.775270 140318301210368 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6130635142326355, loss=4.443030834197998
I0926 22:19:38.942314 140318309603072 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6386328935623169, loss=3.9452695846557617
I0926 22:20:19.741327 140318301210368 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.981248140335083, loss=4.005969047546387
I0926 22:20:45.680933 140524574070592 spec.py:321] Evaluating on the training split.
I0926 22:21:00.227176 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 22:21:16.760575 140524574070592 spec.py:349] Evaluating on the test split.
I0926 22:21:18.407875 140524574070592 submission_runner.py:381] Time since start: 5448.06s, 	Step: 12666, 	{'train/accuracy': 0.465156227350235, 'train/loss': 2.4574532508850098, 'validation/accuracy': 0.429099977016449, 'validation/loss': 2.6417055130004883, 'validation/num_examples': 50000, 'test/accuracy': 0.32990002632141113, 'test/loss': 3.258254051208496, 'test/num_examples': 10000, 'score': 5094.257934093475, 'total_duration': 5448.062312364578, 'accumulated_submission_time': 5094.257934093475, 'accumulated_eval_time': 353.0810589790344, 'accumulated_logging_time': 0.3997809886932373}
I0926 22:21:18.433863 140318309603072 logging_writer.py:48] [12666] accumulated_eval_time=353.081059, accumulated_logging_time=0.399781, accumulated_submission_time=5094.257934, global_step=12666, preemption_count=0, score=5094.257934, test/accuracy=0.329900, test/loss=3.258254, test/num_examples=10000, total_duration=5448.062312, train/accuracy=0.465156, train/loss=2.457453, validation/accuracy=0.429100, validation/loss=2.641706, validation/num_examples=50000
I0926 22:21:32.199004 140318301210368 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6696761250495911, loss=4.020781517028809
I0926 22:22:12.012109 140318309603072 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5550850629806519, loss=5.928424835205078
I0926 22:22:52.283418 140318301210368 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7714619636535645, loss=3.9577066898345947
I0926 22:23:33.257926 140318309603072 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8014957308769226, loss=3.9916739463806152
I0926 22:24:14.246364 140318301210368 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6156092882156372, loss=4.1371684074401855
I0926 22:24:55.002471 140318309603072 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6764422059059143, loss=4.023815155029297
I0926 22:25:35.980549 140318301210368 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.7830913662910461, loss=3.8629281520843506
I0926 22:26:17.106728 140318309603072 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6541947722434998, loss=4.832122325897217
I0926 22:26:57.835510 140318301210368 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6240592002868652, loss=3.8832008838653564
I0926 22:27:38.838172 140318309603072 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6174207329750061, loss=4.804290771484375
I0926 22:28:18.759114 140524574070592 spec.py:321] Evaluating on the training split.
I0926 22:28:33.089840 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 22:28:48.844641 140524574070592 spec.py:349] Evaluating on the test split.
I0926 22:28:50.500898 140524574070592 submission_runner.py:381] Time since start: 5900.15s, 	Step: 13700, 	{'train/accuracy': 0.4983789026737213, 'train/loss': 2.317952871322632, 'validation/accuracy': 0.4539399743080139, 'validation/loss': 2.5212182998657227, 'validation/num_examples': 50000, 'test/accuracy': 0.34860002994537354, 'test/loss': 3.1458396911621094, 'test/num_examples': 10000, 'score': 5514.548373222351, 'total_duration': 5900.154172897339, 'accumulated_submission_time': 5514.548373222351, 'accumulated_eval_time': 384.8216643333435, 'accumulated_logging_time': 0.4364280700683594}
I0926 22:28:50.523288 140318301210368 logging_writer.py:48] [13700] accumulated_eval_time=384.821664, accumulated_logging_time=0.436428, accumulated_submission_time=5514.548373, global_step=13700, preemption_count=0, score=5514.548373, test/accuracy=0.348600, test/loss=3.145840, test/num_examples=10000, total_duration=5900.154173, train/accuracy=0.498379, train/loss=2.317953, validation/accuracy=0.453940, validation/loss=2.521218, validation/num_examples=50000
I0926 22:28:50.948402 140318309603072 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6760018467903137, loss=3.8564226627349854
I0926 22:29:30.309825 140318301210368 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.655664324760437, loss=5.053983211517334
I0926 22:30:10.048991 140318309603072 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7521381974220276, loss=3.8812875747680664
I0926 22:30:49.654272 140524574070592 spec.py:321] Evaluating on the training split.
I0926 22:31:03.828764 140524574070592 spec.py:333] Evaluating on the validation split.
I0926 22:31:34.569437 140524574070592 spec.py:349] Evaluating on the test split.
I0926 22:31:36.214878 140524574070592 submission_runner.py:381] Time since start: 6065.87s, 	Step: 14000, 	{'train/accuracy': 0.4943945109844208, 'train/loss': 2.3559789657592773, 'validation/accuracy': 0.4576199948787689, 'validation/loss': 2.5390267372131348, 'validation/num_examples': 50000, 'test/accuracy': 0.3538000285625458, 'test/loss': 3.1604461669921875, 'test/num_examples': 10000, 'score': 5633.6579875946045, 'total_duration': 6065.869316339493, 'accumulated_submission_time': 5633.6579875946045, 'accumulated_eval_time': 431.38224148750305, 'accumulated_logging_time': 0.4725644588470459}
I0926 22:31:36.238595 140318301210368 logging_writer.py:48] [14000] accumulated_eval_time=431.382241, accumulated_logging_time=0.472564, accumulated_submission_time=5633.657988, global_step=14000, preemption_count=0, score=5633.657988, test/accuracy=0.353800, test/loss=3.160446, test/num_examples=10000, total_duration=6065.869316, train/accuracy=0.494395, train/loss=2.355979, validation/accuracy=0.457620, validation/loss=2.539027, validation/num_examples=50000
I0926 22:31:36.261815 140318309603072 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5633.657988
I0926 22:31:36.520479 140524574070592 checkpoints.py:490] Saving checkpoint at step: 14000
I0926 22:31:37.375856 140524574070592 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_prelaunch_jax/adamw/imagenet_vit_jax/trial_1/checkpoint_14000
I0926 22:31:37.398029 140524574070592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_prelaunch_jax/adamw/imagenet_vit_jax/trial_1/checkpoint_14000.
I0926 22:31:37.967026 140524574070592 submission_runner.py:549] Tuning trial 1/1
I0926 22:31:37.967322 140524574070592 submission_runner.py:550] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0926 22:31:37.973890 140524574070592 submission_runner.py:551] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010742187732830644, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 52.91277360916138, 'total_duration': 93.54104375839233, 'accumulated_submission_time': 52.91277360916138, 'accumulated_eval_time': 40.628175497055054, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1019, {'train/accuracy': 0.026914061978459358, 'train/loss': 6.073877334594727, 'validation/accuracy': 0.025340000167489052, 'validation/loss': 6.097294330596924, 'validation/num_examples': 50000, 'test/accuracy': 0.020200001075863838, 'test/loss': 6.174903392791748, 'test/num_examples': 10000, 'score': 473.1151487827301, 'total_duration': 534.7995736598969, 'accumulated_submission_time': 473.1151487827301, 'accumulated_eval_time': 61.6303825378418, 'accumulated_logging_time': 0.028868675231933594, 'global_step': 1019, 'preemption_count': 0}), (2087, {'train/accuracy': 0.06640625, 'train/loss': 5.4670515060424805, 'validation/accuracy': 0.06325999647378922, 'validation/loss': 5.494157314300537, 'validation/num_examples': 50000, 'test/accuracy': 0.048500001430511475, 'test/loss': 5.681517124176025, 'test/num_examples': 10000, 'score': 893.1392204761505, 'total_duration': 975.9503483772278, 'accumulated_submission_time': 893.1392204761505, 'accumulated_eval_time': 82.70357537269592, 'accumulated_logging_time': 0.056253910064697266, 'global_step': 2087, 'preemption_count': 0}), (3153, {'train/accuracy': 0.10681640356779099, 'train/loss': 4.961960792541504, 'validation/accuracy': 0.09945999830961227, 'validation/loss': 5.0167107582092285, 'validation/num_examples': 50000, 'test/accuracy': 0.07750000059604645, 'test/loss': 5.282081604003906, 'test/num_examples': 10000, 'score': 1313.388878583908, 'total_duration': 1418.8861346244812, 'accumulated_submission_time': 1313.388878583908, 'accumulated_eval_time': 105.31931924819946, 'accumulated_logging_time': 0.08569741249084473, 'global_step': 3153, 'preemption_count': 0}), (4219, {'train/accuracy': 0.15978515148162842, 'train/loss': 4.552663803100586, 'validation/accuracy': 0.15053999423980713, 'validation/loss': 4.633122444152832, 'validation/num_examples': 50000, 'test/accuracy': 0.11390000581741333, 'test/loss': 4.9347920417785645, 'test/num_examples': 10000, 'score': 1733.419811964035, 'total_duration': 1862.6880745887756, 'accumulated_submission_time': 1733.419811964035, 'accumulated_eval_time': 129.03595662117004, 'accumulated_logging_time': 0.11454367637634277, 'global_step': 4219, 'preemption_count': 0}), (5282, {'train/accuracy': 0.21640624105930328, 'train/loss': 4.0838942527771, 'validation/accuracy': 0.19467999041080475, 'validation/loss': 4.2257161140441895, 'validation/num_examples': 50000, 'test/accuracy': 0.14259999990463257, 'test/loss': 4.61821985244751, 'test/num_examples': 10000, 'score': 2153.3723678588867, 'total_duration': 2304.1089975833893, 'accumulated_submission_time': 2153.3723678588867, 'accumulated_eval_time': 150.448171377182, 'accumulated_logging_time': 0.14507079124450684, 'global_step': 5282, 'preemption_count': 0}), (6343, {'train/accuracy': 0.25376951694488525, 'train/loss': 3.827409029006958, 'validation/accuracy': 0.23635999858379364, 'validation/loss': 3.924687623977661, 'validation/num_examples': 50000, 'test/accuracy': 0.1787000149488449, 'test/loss': 4.35499906539917, 'test/num_examples': 10000, 'score': 2573.6964948177338, 'total_duration': 2746.4811346530914, 'accumulated_submission_time': 2573.6964948177338, 'accumulated_eval_time': 172.44042682647705, 'accumulated_logging_time': 0.17470407485961914, 'global_step': 6343, 'preemption_count': 0}), (7404, {'train/accuracy': 0.2941601574420929, 'train/loss': 3.5217747688293457, 'validation/accuracy': 0.2714399993419647, 'validation/loss': 3.6460108757019043, 'validation/num_examples': 50000, 'test/accuracy': 0.21250000596046448, 'test/loss': 4.104587078094482, 'test/num_examples': 10000, 'score': 2993.6651196479797, 'total_duration': 3190.7816865444183, 'accumulated_submission_time': 2993.6651196479797, 'accumulated_eval_time': 196.71908116340637, 'accumulated_logging_time': 0.20231270790100098, 'global_step': 7404, 'preemption_count': 0}), (8459, {'train/accuracy': 0.3467382788658142, 'train/loss': 3.139017343521118, 'validation/accuracy': 0.3193399906158447, 'validation/loss': 3.2893295288085938, 'validation/num_examples': 50000, 'test/accuracy': 0.24800001084804535, 'test/loss': 3.8087751865386963, 'test/num_examples': 10000, 'score': 3413.7358548641205, 'total_duration': 3636.8401494026184, 'accumulated_submission_time': 3413.7358548641205, 'accumulated_eval_time': 222.64426279067993, 'accumulated_logging_time': 0.24019074440002441, 'global_step': 8459, 'preemption_count': 0}), (9511, {'train/accuracy': 0.38544920086860657, 'train/loss': 2.999854326248169, 'validation/accuracy': 0.3493399918079376, 'validation/loss': 3.1870996952056885, 'validation/num_examples': 50000, 'test/accuracy': 0.26500001549720764, 'test/loss': 3.714484453201294, 'test/num_examples': 10000, 'score': 3833.9398651123047, 'total_duration': 4086.866806268692, 'accumulated_submission_time': 3833.9398651123047, 'accumulated_eval_time': 252.3952465057373, 'accumulated_logging_time': 0.2862672805786133, 'global_step': 9511, 'preemption_count': 0}), (10561, {'train/accuracy': 0.4374414086341858, 'train/loss': 2.6394755840301514, 'validation/accuracy': 0.37911999225616455, 'validation/loss': 2.918146848678589, 'validation/num_examples': 50000, 'test/accuracy': 0.29520002007484436, 'test/loss': 3.4893696308135986, 'test/num_examples': 10000, 'score': 4253.970742702484, 'total_duration': 4538.488778829575, 'accumulated_submission_time': 4253.970742702484, 'accumulated_eval_time': 283.9243507385254, 'accumulated_logging_time': 0.32251834869384766, 'global_step': 10561, 'preemption_count': 0}), (11620, {'train/accuracy': 0.4425976574420929, 'train/loss': 2.596738576889038, 'validation/accuracy': 0.4142199754714966, 'validation/loss': 2.7438042163848877, 'validation/num_examples': 50000, 'test/accuracy': 0.3189000189304352, 'test/loss': 3.3495848178863525, 'test/num_examples': 10000, 'score': 4674.231693506241, 'total_duration': 4995.250826120377, 'accumulated_submission_time': 4674.231693506241, 'accumulated_eval_time': 320.35413122177124, 'accumulated_logging_time': 0.3683288097381592, 'global_step': 11620, 'preemption_count': 0}), (12666, {'train/accuracy': 0.465156227350235, 'train/loss': 2.4574532508850098, 'validation/accuracy': 0.429099977016449, 'validation/loss': 2.6417055130004883, 'validation/num_examples': 50000, 'test/accuracy': 0.32990002632141113, 'test/loss': 3.258254051208496, 'test/num_examples': 10000, 'score': 5094.257934093475, 'total_duration': 5448.062312364578, 'accumulated_submission_time': 5094.257934093475, 'accumulated_eval_time': 353.0810589790344, 'accumulated_logging_time': 0.3997809886932373, 'global_step': 12666, 'preemption_count': 0}), (13700, {'train/accuracy': 0.4983789026737213, 'train/loss': 2.317952871322632, 'validation/accuracy': 0.4539399743080139, 'validation/loss': 2.5212182998657227, 'validation/num_examples': 50000, 'test/accuracy': 0.34860002994537354, 'test/loss': 3.1458396911621094, 'test/num_examples': 10000, 'score': 5514.548373222351, 'total_duration': 5900.154172897339, 'accumulated_submission_time': 5514.548373222351, 'accumulated_eval_time': 384.8216643333435, 'accumulated_logging_time': 0.4364280700683594, 'global_step': 13700, 'preemption_count': 0}), (14000, {'train/accuracy': 0.4943945109844208, 'train/loss': 2.3559789657592773, 'validation/accuracy': 0.4576199948787689, 'validation/loss': 2.5390267372131348, 'validation/num_examples': 50000, 'test/accuracy': 0.3538000285625458, 'test/loss': 3.1604461669921875, 'test/num_examples': 10000, 'score': 5633.6579875946045, 'total_duration': 6065.869316339493, 'accumulated_submission_time': 5633.6579875946045, 'accumulated_eval_time': 431.38224148750305, 'accumulated_logging_time': 0.4725644588470459, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0926 22:31:37.974032 140524574070592 submission_runner.py:552] Timing: 5633.6579875946045
I0926 22:31:37.974100 140524574070592 submission_runner.py:554] Total number of evals: 15
I0926 22:31:37.974144 140524574070592 submission_runner.py:555] ====================
I0926 22:31:37.974268 140524574070592 submission_runner.py:625] Final imagenet_vit score: 5633.6579875946045
