WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 19:32:03.676588 140123311793984 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 19:32:03.676628 140076198303552 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 19:32:03.676645 140258701961024 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 19:32:03.677435 140478843184960 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 19:32:03.677522 139933346314048 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 19:32:03.677543 140502104815424 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 19:32:03.677573 140396801075008 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 19:32:03.677741 140478843184960 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:03.677872 139933346314048 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:03.677898 140396801075008 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:03.677891 140502104815424 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:03.677691 140232903325504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 19:32:03.678136 140232903325504 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:03.687225 140123311793984 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:03.687255 140076198303552 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:03.687288 140258701961024 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:05.798827 140396801075008 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch.
W0405 19:32:05.908402 140076198303552 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:05.908403 140502104815424 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:05.908666 140258701961024 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:05.908957 140478843184960 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:05.909553 139933346314048 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:05.909736 140123311793984 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:05.911011 140396801075008 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:05.911849 140232903325504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 19:32:05.914718 140396801075008 submission_runner.py:511] Using RNG seed 2465485033
I0405 19:32:05.915695 140396801075008 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 19:32:05.915807 140396801075008 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1.
I0405 19:32:05.916018 140396801075008 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/hparams.json.
I0405 19:32:05.917050 140396801075008 submission_runner.py:230] Starting train once: RAM USED (GB) 5.727023104
I0405 19:32:05.917143 140396801075008 submission_runner.py:231] Initializing dataset.
I0405 19:32:10.209684 140396801075008 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 7.745052672
I0405 19:32:10.209861 140396801075008 submission_runner.py:240] Initializing model.
I0405 19:32:14.775985 140396801075008 submission_runner.py:251] After Initializing model: RAM USED (GB) 17.564311552
I0405 19:32:14.776197 140396801075008 submission_runner.py:252] Initializing optimizer.
I0405 19:32:15.026023 140396801075008 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 17.567985664
I0405 19:32:15.026206 140396801075008 submission_runner.py:261] Initializing metrics bundle.
I0405 19:32:15.026255 140396801075008 submission_runner.py:276] Initializing checkpoint and logger.
I0405 19:32:15.718163 140396801075008 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0405 19:32:15.720004 140396801075008 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0405 19:32:15.757791 140396801075008 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 17.616191488
I0405 19:32:15.758833 140396801075008 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 17.616191488
I0405 19:32:15.758951 140396801075008 submission_runner.py:313] Starting training loop.
I0405 19:32:18.268205 140396801075008 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 22.755581952
I0405 19:32:23.719631 140364765722368 logging_writer.py:48] [0] global_step=0, grad_norm=0.524861, loss=6.926535
I0405 19:32:23.729986 140396801075008 submission.py:139] 0) loss = 6.927, grad_norm = 0.525
I0405 19:32:23.730528 140396801075008 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.784534016
I0405 19:32:23.731108 140396801075008 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.784534016
I0405 19:32:23.731241 140396801075008 spec.py:298] Evaluating on the training split.
I0405 19:33:23.004493 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 19:34:20.154134 140396801075008 spec.py:326] Evaluating on the test split.
I0405 19:34:20.169335 140396801075008 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 19:34:20.175263 140396801075008 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0405 19:34:20.237772 140396801075008 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 19:34:32.516988 140396801075008 submission_runner.py:382] Time since start: 7.97s, 	Step: 1, 	{'train/accuracy': 0.0006178252551020409, 'train/loss': 6.920537209024235, 'validation/accuracy': 0.00068, 'validation/loss': 6.919533125, 'validation/num_examples': 50000, 'test/accuracy': 0.0006, 'test/loss': 6.92169453125, 'test/num_examples': 10000}
I0405 19:34:32.517505 140396801075008 submission_runner.py:396] After eval at step 1: RAM USED (GB) 91.818221568
I0405 19:34:32.527070 140344222017280 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=7.970796, test/accuracy=0.000600, test/loss=6.921695, test/num_examples=10000, total_duration=7.972716, train/accuracy=0.000618, train/loss=6.920537, validation/accuracy=0.000680, validation/loss=6.919533, validation/num_examples=50000
I0405 19:34:32.853121 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_1.
I0405 19:34:32.853834 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 91.818442752
I0405 19:34:32.882708 140396801075008 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 91.820535808
I0405 19:34:32.890465 140396801075008 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:32.890604 140123311793984 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:32.890589 140478843184960 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:32.890663 140076198303552 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:32.890671 140232903325504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:32.890615 140258701961024 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:32.891056 140502104815424 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:32.891077 139933346314048 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:33.259284 140344213624576 logging_writer.py:48] [1] global_step=1, grad_norm=0.543423, loss=6.931923
I0405 19:34:33.262696 140396801075008 submission.py:139] 1) loss = 6.932, grad_norm = 0.543
I0405 19:34:33.263309 140396801075008 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 91.825332224
I0405 19:34:33.635447 140344222017280 logging_writer.py:48] [2] global_step=2, grad_norm=0.535168, loss=6.927670
I0405 19:34:33.639299 140396801075008 submission.py:139] 2) loss = 6.928, grad_norm = 0.535
I0405 19:34:34.013604 140344213624576 logging_writer.py:48] [3] global_step=3, grad_norm=0.534455, loss=6.929128
I0405 19:34:34.017376 140396801075008 submission.py:139] 3) loss = 6.929, grad_norm = 0.534
I0405 19:34:34.391456 140344222017280 logging_writer.py:48] [4] global_step=4, grad_norm=0.532369, loss=6.928584
I0405 19:34:34.394970 140396801075008 submission.py:139] 4) loss = 6.929, grad_norm = 0.532
I0405 19:34:34.767689 140344213624576 logging_writer.py:48] [5] global_step=5, grad_norm=0.529249, loss=6.932233
I0405 19:34:34.771233 140396801075008 submission.py:139] 5) loss = 6.932, grad_norm = 0.529
I0405 19:34:35.146256 140344222017280 logging_writer.py:48] [6] global_step=6, grad_norm=0.546361, loss=6.934733
I0405 19:34:35.150161 140396801075008 submission.py:139] 6) loss = 6.935, grad_norm = 0.546
I0405 19:34:35.526466 140344213624576 logging_writer.py:48] [7] global_step=7, grad_norm=0.539497, loss=6.923668
I0405 19:34:35.530245 140396801075008 submission.py:139] 7) loss = 6.924, grad_norm = 0.539
I0405 19:34:35.907222 140344222017280 logging_writer.py:48] [8] global_step=8, grad_norm=0.541253, loss=6.925313
I0405 19:34:35.910922 140396801075008 submission.py:139] 8) loss = 6.925, grad_norm = 0.541
I0405 19:34:36.293174 140344213624576 logging_writer.py:48] [9] global_step=9, grad_norm=0.548851, loss=6.922249
I0405 19:34:36.298019 140396801075008 submission.py:139] 9) loss = 6.922, grad_norm = 0.549
I0405 19:34:36.677601 140344222017280 logging_writer.py:48] [10] global_step=10, grad_norm=0.538661, loss=6.933415
I0405 19:34:36.681257 140396801075008 submission.py:139] 10) loss = 6.933, grad_norm = 0.539
I0405 19:34:37.056312 140344213624576 logging_writer.py:48] [11] global_step=11, grad_norm=0.527038, loss=6.914575
I0405 19:34:37.060635 140396801075008 submission.py:139] 11) loss = 6.915, grad_norm = 0.527
I0405 19:34:37.434511 140344222017280 logging_writer.py:48] [12] global_step=12, grad_norm=0.524939, loss=6.919075
I0405 19:34:37.438340 140396801075008 submission.py:139] 12) loss = 6.919, grad_norm = 0.525
I0405 19:34:37.812159 140344213624576 logging_writer.py:48] [13] global_step=13, grad_norm=0.534204, loss=6.925673
I0405 19:34:37.815778 140396801075008 submission.py:139] 13) loss = 6.926, grad_norm = 0.534
I0405 19:34:38.201416 140344222017280 logging_writer.py:48] [14] global_step=14, grad_norm=0.545270, loss=6.935760
I0405 19:34:38.206427 140396801075008 submission.py:139] 14) loss = 6.936, grad_norm = 0.545
I0405 19:34:38.586680 140344213624576 logging_writer.py:48] [15] global_step=15, grad_norm=0.529867, loss=6.927535
I0405 19:34:38.590861 140396801075008 submission.py:139] 15) loss = 6.928, grad_norm = 0.530
I0405 19:34:38.965041 140344222017280 logging_writer.py:48] [16] global_step=16, grad_norm=0.522298, loss=6.929729
I0405 19:34:38.969544 140396801075008 submission.py:139] 16) loss = 6.930, grad_norm = 0.522
I0405 19:34:39.344272 140344213624576 logging_writer.py:48] [17] global_step=17, grad_norm=0.533870, loss=6.922772
I0405 19:34:39.347859 140396801075008 submission.py:139] 17) loss = 6.923, grad_norm = 0.534
I0405 19:34:39.726128 140344222017280 logging_writer.py:48] [18] global_step=18, grad_norm=0.534446, loss=6.914604
I0405 19:34:39.799840 140396801075008 submission.py:139] 18) loss = 6.915, grad_norm = 0.534
I0405 19:34:40.174643 140344213624576 logging_writer.py:48] [19] global_step=19, grad_norm=0.537232, loss=6.933651
I0405 19:34:40.178227 140396801075008 submission.py:139] 19) loss = 6.934, grad_norm = 0.537
I0405 19:34:40.556511 140344222017280 logging_writer.py:48] [20] global_step=20, grad_norm=0.533196, loss=6.920750
I0405 19:34:40.562724 140396801075008 submission.py:139] 20) loss = 6.921, grad_norm = 0.533
I0405 19:34:40.939836 140344213624576 logging_writer.py:48] [21] global_step=21, grad_norm=0.531319, loss=6.917318
I0405 19:34:40.943552 140396801075008 submission.py:139] 21) loss = 6.917, grad_norm = 0.531
I0405 19:34:41.320524 140344222017280 logging_writer.py:48] [22] global_step=22, grad_norm=0.532960, loss=6.925204
I0405 19:34:41.325083 140396801075008 submission.py:139] 22) loss = 6.925, grad_norm = 0.533
I0405 19:34:41.700773 140344213624576 logging_writer.py:48] [23] global_step=23, grad_norm=0.524942, loss=6.926757
I0405 19:34:41.705466 140396801075008 submission.py:139] 23) loss = 6.927, grad_norm = 0.525
I0405 19:34:42.080717 140344222017280 logging_writer.py:48] [24] global_step=24, grad_norm=0.537480, loss=6.925460
I0405 19:34:42.085100 140396801075008 submission.py:139] 24) loss = 6.925, grad_norm = 0.537
I0405 19:34:42.460855 140344213624576 logging_writer.py:48] [25] global_step=25, grad_norm=0.529174, loss=6.924426
I0405 19:34:42.464778 140396801075008 submission.py:139] 25) loss = 6.924, grad_norm = 0.529
I0405 19:34:42.841090 140344222017280 logging_writer.py:48] [26] global_step=26, grad_norm=0.543620, loss=6.925426
I0405 19:34:42.844688 140396801075008 submission.py:139] 26) loss = 6.925, grad_norm = 0.544
I0405 19:34:43.220752 140344213624576 logging_writer.py:48] [27] global_step=27, grad_norm=0.528298, loss=6.926312
I0405 19:34:43.225511 140396801075008 submission.py:139] 27) loss = 6.926, grad_norm = 0.528
I0405 19:34:43.600190 140344222017280 logging_writer.py:48] [28] global_step=28, grad_norm=0.537598, loss=6.924993
I0405 19:34:43.604325 140396801075008 submission.py:139] 28) loss = 6.925, grad_norm = 0.538
I0405 19:34:43.977575 140344213624576 logging_writer.py:48] [29] global_step=29, grad_norm=0.536481, loss=6.919328
I0405 19:34:43.982506 140396801075008 submission.py:139] 29) loss = 6.919, grad_norm = 0.536
I0405 19:34:44.358181 140344222017280 logging_writer.py:48] [30] global_step=30, grad_norm=0.531255, loss=6.918235
I0405 19:34:44.362705 140396801075008 submission.py:139] 30) loss = 6.918, grad_norm = 0.531
I0405 19:34:44.802619 140344213624576 logging_writer.py:48] [31] global_step=31, grad_norm=0.522769, loss=6.917041
I0405 19:34:44.806554 140396801075008 submission.py:139] 31) loss = 6.917, grad_norm = 0.523
I0405 19:34:45.186174 140344222017280 logging_writer.py:48] [32] global_step=32, grad_norm=0.512720, loss=6.914637
I0405 19:34:45.192939 140396801075008 submission.py:139] 32) loss = 6.915, grad_norm = 0.513
I0405 19:34:45.568361 140344213624576 logging_writer.py:48] [33] global_step=33, grad_norm=0.550525, loss=6.928745
I0405 19:34:45.572315 140396801075008 submission.py:139] 33) loss = 6.929, grad_norm = 0.551
I0405 19:34:45.945440 140344222017280 logging_writer.py:48] [34] global_step=34, grad_norm=0.531399, loss=6.915233
I0405 19:34:45.949369 140396801075008 submission.py:139] 34) loss = 6.915, grad_norm = 0.531
I0405 19:34:46.326463 140344213624576 logging_writer.py:48] [35] global_step=35, grad_norm=0.524133, loss=6.923026
I0405 19:34:46.331197 140396801075008 submission.py:139] 35) loss = 6.923, grad_norm = 0.524
I0405 19:34:46.706881 140344222017280 logging_writer.py:48] [36] global_step=36, grad_norm=0.542958, loss=6.924350
I0405 19:34:46.710638 140396801075008 submission.py:139] 36) loss = 6.924, grad_norm = 0.543
I0405 19:34:47.088195 140344213624576 logging_writer.py:48] [37] global_step=37, grad_norm=0.532673, loss=6.922172
I0405 19:34:47.091757 140396801075008 submission.py:139] 37) loss = 6.922, grad_norm = 0.533
I0405 19:34:47.470071 140344222017280 logging_writer.py:48] [38] global_step=38, grad_norm=0.525220, loss=6.923598
I0405 19:34:47.474786 140396801075008 submission.py:139] 38) loss = 6.924, grad_norm = 0.525
I0405 19:34:47.853281 140344213624576 logging_writer.py:48] [39] global_step=39, grad_norm=0.527052, loss=6.923233
I0405 19:34:47.858837 140396801075008 submission.py:139] 39) loss = 6.923, grad_norm = 0.527
I0405 19:34:48.238471 140344222017280 logging_writer.py:48] [40] global_step=40, grad_norm=0.530865, loss=6.917331
I0405 19:34:48.242978 140396801075008 submission.py:139] 40) loss = 6.917, grad_norm = 0.531
I0405 19:34:48.620559 140344213624576 logging_writer.py:48] [41] global_step=41, grad_norm=0.517325, loss=6.923145
I0405 19:34:48.625207 140396801075008 submission.py:139] 41) loss = 6.923, grad_norm = 0.517
I0405 19:34:49.005014 140344222017280 logging_writer.py:48] [42] global_step=42, grad_norm=0.525647, loss=6.912258
I0405 19:34:49.009871 140396801075008 submission.py:139] 42) loss = 6.912, grad_norm = 0.526
I0405 19:34:49.389618 140344213624576 logging_writer.py:48] [43] global_step=43, grad_norm=0.536929, loss=6.917791
I0405 19:34:49.394308 140396801075008 submission.py:139] 43) loss = 6.918, grad_norm = 0.537
I0405 19:34:49.773431 140344222017280 logging_writer.py:48] [44] global_step=44, grad_norm=0.523842, loss=6.924343
I0405 19:34:49.779030 140396801075008 submission.py:139] 44) loss = 6.924, grad_norm = 0.524
I0405 19:34:50.158485 140344213624576 logging_writer.py:48] [45] global_step=45, grad_norm=0.515429, loss=6.915824
I0405 19:34:50.162386 140396801075008 submission.py:139] 45) loss = 6.916, grad_norm = 0.515
I0405 19:34:50.538409 140344222017280 logging_writer.py:48] [46] global_step=46, grad_norm=0.547590, loss=6.921487
I0405 19:34:50.543725 140396801075008 submission.py:139] 46) loss = 6.921, grad_norm = 0.548
I0405 19:34:50.923844 140344213624576 logging_writer.py:48] [47] global_step=47, grad_norm=0.530752, loss=6.918232
I0405 19:34:50.928888 140396801075008 submission.py:139] 47) loss = 6.918, grad_norm = 0.531
I0405 19:34:51.310006 140344222017280 logging_writer.py:48] [48] global_step=48, grad_norm=0.519734, loss=6.913848
I0405 19:34:51.315272 140396801075008 submission.py:139] 48) loss = 6.914, grad_norm = 0.520
I0405 19:34:51.693624 140344213624576 logging_writer.py:48] [49] global_step=49, grad_norm=0.538004, loss=6.911445
I0405 19:34:51.698573 140396801075008 submission.py:139] 49) loss = 6.911, grad_norm = 0.538
I0405 19:34:52.078182 140344222017280 logging_writer.py:48] [50] global_step=50, grad_norm=0.522781, loss=6.910911
I0405 19:34:52.083219 140396801075008 submission.py:139] 50) loss = 6.911, grad_norm = 0.523
I0405 19:34:52.457366 140344213624576 logging_writer.py:48] [51] global_step=51, grad_norm=0.524407, loss=6.913306
I0405 19:34:52.461100 140396801075008 submission.py:139] 51) loss = 6.913, grad_norm = 0.524
I0405 19:34:52.836300 140344222017280 logging_writer.py:48] [52] global_step=52, grad_norm=0.515934, loss=6.915543
I0405 19:34:52.844434 140396801075008 submission.py:139] 52) loss = 6.916, grad_norm = 0.516
I0405 19:34:53.219263 140344213624576 logging_writer.py:48] [53] global_step=53, grad_norm=0.515989, loss=6.907874
I0405 19:34:53.224132 140396801075008 submission.py:139] 53) loss = 6.908, grad_norm = 0.516
I0405 19:34:53.604172 140344222017280 logging_writer.py:48] [54] global_step=54, grad_norm=0.530923, loss=6.911918
I0405 19:34:53.609328 140396801075008 submission.py:139] 54) loss = 6.912, grad_norm = 0.531
I0405 19:34:53.984516 140344213624576 logging_writer.py:48] [55] global_step=55, grad_norm=0.521648, loss=6.920219
I0405 19:34:53.990206 140396801075008 submission.py:139] 55) loss = 6.920, grad_norm = 0.522
I0405 19:34:54.371756 140344222017280 logging_writer.py:48] [56] global_step=56, grad_norm=0.531324, loss=6.919658
I0405 19:34:54.376533 140396801075008 submission.py:139] 56) loss = 6.920, grad_norm = 0.531
I0405 19:34:54.752840 140344213624576 logging_writer.py:48] [57] global_step=57, grad_norm=0.530498, loss=6.916193
I0405 19:34:54.757577 140396801075008 submission.py:139] 57) loss = 6.916, grad_norm = 0.530
I0405 19:34:55.132665 140344222017280 logging_writer.py:48] [58] global_step=58, grad_norm=0.540863, loss=6.909307
I0405 19:34:55.137649 140396801075008 submission.py:139] 58) loss = 6.909, grad_norm = 0.541
I0405 19:34:55.513271 140344213624576 logging_writer.py:48] [59] global_step=59, grad_norm=0.537923, loss=6.914275
I0405 19:34:55.517498 140396801075008 submission.py:139] 59) loss = 6.914, grad_norm = 0.538
I0405 19:34:55.894422 140344222017280 logging_writer.py:48] [60] global_step=60, grad_norm=0.527542, loss=6.907575
I0405 19:34:55.898089 140396801075008 submission.py:139] 60) loss = 6.908, grad_norm = 0.528
I0405 19:34:56.272506 140344213624576 logging_writer.py:48] [61] global_step=61, grad_norm=0.511125, loss=6.908328
I0405 19:34:56.277000 140396801075008 submission.py:139] 61) loss = 6.908, grad_norm = 0.511
I0405 19:34:56.652907 140344222017280 logging_writer.py:48] [62] global_step=62, grad_norm=0.513064, loss=6.905919
I0405 19:34:56.658267 140396801075008 submission.py:139] 62) loss = 6.906, grad_norm = 0.513
I0405 19:34:57.035078 140344213624576 logging_writer.py:48] [63] global_step=63, grad_norm=0.534774, loss=6.913297
I0405 19:34:57.038734 140396801075008 submission.py:139] 63) loss = 6.913, grad_norm = 0.535
I0405 19:34:57.415738 140344222017280 logging_writer.py:48] [64] global_step=64, grad_norm=0.545622, loss=6.902861
I0405 19:34:57.419579 140396801075008 submission.py:139] 64) loss = 6.903, grad_norm = 0.546
I0405 19:34:57.809310 140344213624576 logging_writer.py:48] [65] global_step=65, grad_norm=0.524707, loss=6.906404
I0405 19:34:57.813405 140396801075008 submission.py:139] 65) loss = 6.906, grad_norm = 0.525
I0405 19:34:58.188898 140344222017280 logging_writer.py:48] [66] global_step=66, grad_norm=0.529824, loss=6.910545
I0405 19:34:58.192485 140396801075008 submission.py:139] 66) loss = 6.911, grad_norm = 0.530
I0405 19:34:58.571484 140344213624576 logging_writer.py:48] [67] global_step=67, grad_norm=0.535627, loss=6.912315
I0405 19:34:58.575660 140396801075008 submission.py:139] 67) loss = 6.912, grad_norm = 0.536
I0405 19:34:58.951457 140344222017280 logging_writer.py:48] [68] global_step=68, grad_norm=0.532995, loss=6.903419
I0405 19:34:58.955160 140396801075008 submission.py:139] 68) loss = 6.903, grad_norm = 0.533
I0405 19:34:59.330889 140344213624576 logging_writer.py:48] [69] global_step=69, grad_norm=0.509343, loss=6.905210
I0405 19:34:59.335437 140396801075008 submission.py:139] 69) loss = 6.905, grad_norm = 0.509
I0405 19:34:59.711562 140344222017280 logging_writer.py:48] [70] global_step=70, grad_norm=0.521692, loss=6.898463
I0405 19:34:59.715707 140396801075008 submission.py:139] 70) loss = 6.898, grad_norm = 0.522
I0405 19:35:00.096377 140344213624576 logging_writer.py:48] [71] global_step=71, grad_norm=0.524027, loss=6.905341
I0405 19:35:00.101134 140396801075008 submission.py:139] 71) loss = 6.905, grad_norm = 0.524
I0405 19:35:00.476628 140344222017280 logging_writer.py:48] [72] global_step=72, grad_norm=0.529598, loss=6.907252
I0405 19:35:00.480289 140396801075008 submission.py:139] 72) loss = 6.907, grad_norm = 0.530
I0405 19:35:00.855906 140344213624576 logging_writer.py:48] [73] global_step=73, grad_norm=0.540491, loss=6.904308
I0405 19:35:00.859644 140396801075008 submission.py:139] 73) loss = 6.904, grad_norm = 0.540
I0405 19:35:01.235003 140344222017280 logging_writer.py:48] [74] global_step=74, grad_norm=0.509771, loss=6.897458
I0405 19:35:01.238973 140396801075008 submission.py:139] 74) loss = 6.897, grad_norm = 0.510
I0405 19:35:01.615005 140344213624576 logging_writer.py:48] [75] global_step=75, grad_norm=0.517237, loss=6.897991
I0405 19:35:01.619974 140396801075008 submission.py:139] 75) loss = 6.898, grad_norm = 0.517
I0405 19:35:02.005269 140344222017280 logging_writer.py:48] [76] global_step=76, grad_norm=0.520034, loss=6.906116
I0405 19:35:02.009288 140396801075008 submission.py:139] 76) loss = 6.906, grad_norm = 0.520
I0405 19:35:02.390829 140344213624576 logging_writer.py:48] [77] global_step=77, grad_norm=0.520471, loss=6.901983
I0405 19:35:02.394573 140396801075008 submission.py:139] 77) loss = 6.902, grad_norm = 0.520
I0405 19:35:02.773469 140344222017280 logging_writer.py:48] [78] global_step=78, grad_norm=0.532569, loss=6.902690
I0405 19:35:02.777905 140396801075008 submission.py:139] 78) loss = 6.903, grad_norm = 0.533
I0405 19:35:03.166294 140344213624576 logging_writer.py:48] [79] global_step=79, grad_norm=0.540793, loss=6.900840
I0405 19:35:03.171649 140396801075008 submission.py:139] 79) loss = 6.901, grad_norm = 0.541
I0405 19:35:03.548568 140344222017280 logging_writer.py:48] [80] global_step=80, grad_norm=0.506286, loss=6.894593
I0405 19:35:03.552598 140396801075008 submission.py:139] 80) loss = 6.895, grad_norm = 0.506
I0405 19:35:03.932685 140344213624576 logging_writer.py:48] [81] global_step=81, grad_norm=0.512555, loss=6.901589
I0405 19:35:03.936838 140396801075008 submission.py:139] 81) loss = 6.902, grad_norm = 0.513
I0405 19:35:04.320773 140344222017280 logging_writer.py:48] [82] global_step=82, grad_norm=0.519087, loss=6.898958
I0405 19:35:04.325174 140396801075008 submission.py:139] 82) loss = 6.899, grad_norm = 0.519
I0405 19:35:04.703025 140344213624576 logging_writer.py:48] [83] global_step=83, grad_norm=0.530362, loss=6.891533
I0405 19:35:04.707302 140396801075008 submission.py:139] 83) loss = 6.892, grad_norm = 0.530
I0405 19:35:05.082477 140344222017280 logging_writer.py:48] [84] global_step=84, grad_norm=0.518143, loss=6.901729
I0405 19:35:05.086161 140396801075008 submission.py:139] 84) loss = 6.902, grad_norm = 0.518
I0405 19:35:05.465327 140344213624576 logging_writer.py:48] [85] global_step=85, grad_norm=0.510120, loss=6.889908
I0405 19:35:05.469447 140396801075008 submission.py:139] 85) loss = 6.890, grad_norm = 0.510
I0405 19:35:05.846187 140344222017280 logging_writer.py:48] [86] global_step=86, grad_norm=0.530767, loss=6.893570
I0405 19:35:05.850896 140396801075008 submission.py:139] 86) loss = 6.894, grad_norm = 0.531
I0405 19:35:06.232306 140344213624576 logging_writer.py:48] [87] global_step=87, grad_norm=0.528290, loss=6.895904
I0405 19:35:06.236285 140396801075008 submission.py:139] 87) loss = 6.896, grad_norm = 0.528
I0405 19:35:06.620160 140344222017280 logging_writer.py:48] [88] global_step=88, grad_norm=0.528988, loss=6.896300
I0405 19:35:06.624226 140396801075008 submission.py:139] 88) loss = 6.896, grad_norm = 0.529
I0405 19:35:07.005928 140344213624576 logging_writer.py:48] [89] global_step=89, grad_norm=0.524394, loss=6.885473
I0405 19:35:07.010058 140396801075008 submission.py:139] 89) loss = 6.885, grad_norm = 0.524
I0405 19:35:07.387362 140344222017280 logging_writer.py:48] [90] global_step=90, grad_norm=0.534533, loss=6.900300
I0405 19:35:07.391146 140396801075008 submission.py:139] 90) loss = 6.900, grad_norm = 0.535
I0405 19:35:07.765991 140344213624576 logging_writer.py:48] [91] global_step=91, grad_norm=0.518677, loss=6.894808
I0405 19:35:07.770636 140396801075008 submission.py:139] 91) loss = 6.895, grad_norm = 0.519
I0405 19:35:08.145845 140344222017280 logging_writer.py:48] [92] global_step=92, grad_norm=0.535700, loss=6.889826
I0405 19:35:08.149313 140396801075008 submission.py:139] 92) loss = 6.890, grad_norm = 0.536
I0405 19:35:08.524383 140344213624576 logging_writer.py:48] [93] global_step=93, grad_norm=0.521752, loss=6.888183
I0405 19:35:08.529007 140396801075008 submission.py:139] 93) loss = 6.888, grad_norm = 0.522
I0405 19:35:08.905266 140344222017280 logging_writer.py:48] [94] global_step=94, grad_norm=0.514108, loss=6.896772
I0405 19:35:08.911083 140396801075008 submission.py:139] 94) loss = 6.897, grad_norm = 0.514
I0405 19:35:09.286917 140344213624576 logging_writer.py:48] [95] global_step=95, grad_norm=0.519987, loss=6.887435
I0405 19:35:09.290724 140396801075008 submission.py:139] 95) loss = 6.887, grad_norm = 0.520
I0405 19:35:09.673210 140344222017280 logging_writer.py:48] [96] global_step=96, grad_norm=0.522782, loss=6.894408
I0405 19:35:09.677644 140396801075008 submission.py:139] 96) loss = 6.894, grad_norm = 0.523
I0405 19:35:10.059035 140344213624576 logging_writer.py:48] [97] global_step=97, grad_norm=0.528393, loss=6.881316
I0405 19:35:10.066882 140396801075008 submission.py:139] 97) loss = 6.881, grad_norm = 0.528
I0405 19:35:10.449345 140344222017280 logging_writer.py:48] [98] global_step=98, grad_norm=0.526644, loss=6.890158
I0405 19:35:10.454163 140396801075008 submission.py:139] 98) loss = 6.890, grad_norm = 0.527
I0405 19:35:10.832834 140344213624576 logging_writer.py:48] [99] global_step=99, grad_norm=0.524214, loss=6.885629
I0405 19:35:10.836705 140396801075008 submission.py:139] 99) loss = 6.886, grad_norm = 0.524
I0405 19:35:11.215427 140344222017280 logging_writer.py:48] [100] global_step=100, grad_norm=0.525368, loss=6.893729
I0405 19:35:11.219740 140396801075008 submission.py:139] 100) loss = 6.894, grad_norm = 0.525
I0405 19:37:39.902268 140344213624576 logging_writer.py:48] [500] global_step=500, grad_norm=0.658139, loss=6.559497
I0405 19:37:39.907706 140396801075008 submission.py:139] 500) loss = 6.559, grad_norm = 0.658
I0405 19:40:45.983767 140344222017280 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.855071, loss=6.278923
I0405 19:40:45.989041 140396801075008 submission.py:139] 1000) loss = 6.279, grad_norm = 0.855
I0405 19:43:03.206151 140396801075008 submission_runner.py:373] Before eval at step 1367: RAM USED (GB) 98.340626432
I0405 19:43:03.206374 140396801075008 spec.py:298] Evaluating on the training split.
I0405 19:43:48.715137 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 19:44:46.467381 140396801075008 spec.py:326] Evaluating on the test split.
I0405 19:44:47.856989 140396801075008 submission_runner.py:382] Time since start: 647.34s, 	Step: 1367, 	{'train/accuracy': 0.062021683673469385, 'train/loss': 5.579645896444515, 'validation/accuracy': 0.05846, 'validation/loss': 5.64113625, 'validation/num_examples': 50000, 'test/accuracy': 0.0394, 'test/loss': 5.86901328125, 'test/num_examples': 10000}
I0405 19:44:47.857442 140396801075008 submission_runner.py:396] After eval at step 1367: RAM USED (GB) 98.186604544
I0405 19:44:47.867212 140344230409984 logging_writer.py:48] [1367] global_step=1367, preemption_count=0, score=379.610985, test/accuracy=0.039400, test/loss=5.869013, test/num_examples=10000, total_duration=647.339772, train/accuracy=0.062022, train/loss=5.579646, validation/accuracy=0.058460, validation/loss=5.641136, validation/num_examples=50000
I0405 19:44:48.202874 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_1367.
I0405 19:44:48.203716 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 1367: RAM USED (GB) 98.177609728
I0405 19:45:37.715569 140344716924672 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.787432, loss=6.012946
I0405 19:45:37.719255 140396801075008 submission.py:139] 1500) loss = 6.013, grad_norm = 0.787
I0405 19:48:43.038167 140344230409984 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.846711, loss=5.827664
I0405 19:48:43.042850 140396801075008 submission.py:139] 2000) loss = 5.828, grad_norm = 0.847
I0405 19:51:48.244162 140344716924672 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.861439, loss=5.536205
I0405 19:51:48.248037 140396801075008 submission.py:139] 2500) loss = 5.536, grad_norm = 0.861
I0405 19:53:18.492639 140396801075008 submission_runner.py:373] Before eval at step 2741: RAM USED (GB) 99.688648704
I0405 19:53:18.492857 140396801075008 spec.py:298] Evaluating on the training split.
I0405 19:54:00.955172 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 19:54:58.643318 140396801075008 spec.py:326] Evaluating on the test split.
I0405 19:55:00.003563 140396801075008 submission_runner.py:382] Time since start: 1262.63s, 	Step: 2741, 	{'train/accuracy': 0.14188058035714285, 'train/loss': 4.743875620316486, 'validation/accuracy': 0.12776, 'validation/loss': 4.83460125, 'validation/num_examples': 50000, 'test/accuracy': 0.0931, 'test/loss': 5.18133203125, 'test/num_examples': 10000}
I0405 19:55:00.003906 140396801075008 submission_runner.py:396] After eval at step 2741: RAM USED (GB) 99.671048192
I0405 19:55:00.011742 140344230409984 logging_writer.py:48] [2741] global_step=2741, preemption_count=0, score=739.714933, test/accuracy=0.093100, test/loss=5.181332, test/num_examples=10000, total_duration=1262.625998, train/accuracy=0.141881, train/loss=4.743876, validation/accuracy=0.127760, validation/loss=4.834601, validation/num_examples=50000
I0405 19:55:00.329793 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_2741.
I0405 19:55:00.330591 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 2741: RAM USED (GB) 99.668525056
I0405 19:56:36.355359 140344716924672 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.907658, loss=5.255172
I0405 19:56:36.359498 140396801075008 submission.py:139] 3000) loss = 5.255, grad_norm = 0.908
I0405 19:59:41.557004 140344230409984 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.904751, loss=5.016955
I0405 19:59:41.560955 140396801075008 submission.py:139] 3500) loss = 5.017, grad_norm = 0.905
I0405 20:02:48.158854 140344716924672 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.765752, loss=4.895144
I0405 20:02:48.163661 140396801075008 submission.py:139] 4000) loss = 4.895, grad_norm = 0.766
I0405 20:03:30.708196 140396801075008 submission_runner.py:373] Before eval at step 4116: RAM USED (GB) 99.83633408
I0405 20:03:30.708414 140396801075008 spec.py:298] Evaluating on the training split.
I0405 20:04:13.490750 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 20:04:58.243020 140396801075008 spec.py:326] Evaluating on the test split.
I0405 20:04:59.609466 140396801075008 submission_runner.py:382] Time since start: 1874.84s, 	Step: 4116, 	{'train/accuracy': 0.28535554846938777, 'train/loss': 3.57246056381537, 'validation/accuracy': 0.26176, 'validation/loss': 3.698636875, 'validation/num_examples': 50000, 'test/accuracy': 0.1925, 'test/loss': 4.239963671875, 'test/num_examples': 10000}
I0405 20:04:59.609804 140396801075008 submission_runner.py:396] After eval at step 4116: RAM USED (GB) 99.862577152
I0405 20:04:59.618543 140344230409984 logging_writer.py:48] [4116] global_step=4116, preemption_count=0, score=1099.995042, test/accuracy=0.192500, test/loss=4.239964, test/num_examples=10000, total_duration=1874.842099, train/accuracy=0.285356, train/loss=3.572461, validation/accuracy=0.261760, validation/loss=3.698637, validation/num_examples=50000
I0405 20:04:59.936059 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_4116.
I0405 20:04:59.936856 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 4116: RAM USED (GB) 99.861602304
I0405 20:07:22.187420 140344716924672 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.701875, loss=4.594164
I0405 20:07:22.191874 140396801075008 submission.py:139] 4500) loss = 4.594, grad_norm = 0.702
I0405 20:10:27.364942 140344230409984 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.696346, loss=4.532646
I0405 20:10:27.369571 140396801075008 submission.py:139] 5000) loss = 4.533, grad_norm = 0.696
I0405 20:13:30.077166 140396801075008 submission_runner.py:373] Before eval at step 5491: RAM USED (GB) 100.12614656
I0405 20:13:30.077399 140396801075008 spec.py:298] Evaluating on the training split.
I0405 20:14:16.529007 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 20:15:14.598401 140396801075008 spec.py:326] Evaluating on the test split.
I0405 20:15:15.965872 140396801075008 submission_runner.py:382] Time since start: 2474.21s, 	Step: 5491, 	{'train/accuracy': 0.35453204719387754, 'train/loss': 3.243650786730708, 'validation/accuracy': 0.32552, 'validation/loss': 3.3937271875, 'validation/num_examples': 50000, 'test/accuracy': 0.2324, 'test/loss': 3.995139453125, 'test/num_examples': 10000}
I0405 20:15:15.966243 140396801075008 submission_runner.py:396] After eval at step 5491: RAM USED (GB) 100.170358784
I0405 20:15:15.975386 140344716924672 logging_writer.py:48] [5491] global_step=5491, preemption_count=0, score=1459.975521, test/accuracy=0.232400, test/loss=3.995139, test/num_examples=10000, total_duration=2474.211200, train/accuracy=0.354532, train/loss=3.243651, validation/accuracy=0.325520, validation/loss=3.393727, validation/num_examples=50000
I0405 20:15:16.284960 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_5491.
I0405 20:15:16.285666 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 5491: RAM USED (GB) 100.169383936
I0405 20:15:19.979479 140344230409984 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.709417, loss=4.484592
I0405 20:15:19.982954 140396801075008 submission.py:139] 5500) loss = 4.485, grad_norm = 0.709
I0405 20:18:24.848269 140344716924672 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.704600, loss=4.267672
I0405 20:18:24.852470 140396801075008 submission.py:139] 6000) loss = 4.268, grad_norm = 0.705
I0405 20:21:31.134720 140344230409984 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.646626, loss=4.203146
I0405 20:21:31.139845 140396801075008 submission.py:139] 6500) loss = 4.203, grad_norm = 0.647
I0405 20:23:46.458993 140396801075008 submission_runner.py:373] Before eval at step 6867: RAM USED (GB) 99.831967744
I0405 20:23:46.459208 140396801075008 spec.py:298] Evaluating on the training split.
I0405 20:24:30.379757 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 20:25:16.479109 140396801075008 spec.py:326] Evaluating on the test split.
I0405 20:25:17.841039 140396801075008 submission_runner.py:382] Time since start: 3090.59s, 	Step: 6867, 	{'train/accuracy': 0.44778380102040816, 'train/loss': 2.587762015206473, 'validation/accuracy': 0.413, 'validation/loss': 2.784301875, 'validation/num_examples': 50000, 'test/accuracy': 0.3067, 'test/loss': 3.427705078125, 'test/num_examples': 10000}
I0405 20:25:17.841395 140396801075008 submission_runner.py:396] After eval at step 6867: RAM USED (GB) 99.809099776
I0405 20:25:17.849359 140344716924672 logging_writer.py:48] [6867] global_step=6867, preemption_count=0, score=1820.026870, test/accuracy=0.306700, test/loss=3.427705, test/num_examples=10000, total_duration=3090.592244, train/accuracy=0.447784, train/loss=2.587762, validation/accuracy=0.413000, validation/loss=2.784302, validation/num_examples=50000
I0405 20:25:18.153768 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_6867.
I0405 20:25:18.154560 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 6867: RAM USED (GB) 99.80786688
I0405 20:26:07.590588 140344230409984 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.627529, loss=4.171142
I0405 20:26:07.594312 140396801075008 submission.py:139] 7000) loss = 4.171, grad_norm = 0.628
I0405 20:29:12.647722 140344716924672 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.569850, loss=4.037644
I0405 20:29:12.651710 140396801075008 submission.py:139] 7500) loss = 4.038, grad_norm = 0.570
I0405 20:32:18.785001 140344230409984 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.560356, loss=3.941615
I0405 20:32:18.788869 140396801075008 submission.py:139] 8000) loss = 3.942, grad_norm = 0.560
I0405 20:33:48.287562 140396801075008 submission_runner.py:373] Before eval at step 8243: RAM USED (GB) 99.837677568
I0405 20:33:48.287780 140396801075008 spec.py:298] Evaluating on the training split.
I0405 20:34:32.674653 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 20:35:23.863592 140396801075008 spec.py:326] Evaluating on the test split.
I0405 20:35:25.224402 140396801075008 submission_runner.py:382] Time since start: 3692.42s, 	Step: 8243, 	{'train/accuracy': 0.5038663903061225, 'train/loss': 2.425346685915577, 'validation/accuracy': 0.46692, 'validation/loss': 2.6071434375, 'validation/num_examples': 50000, 'test/accuracy': 0.3535, 'test/loss': 3.275227734375, 'test/num_examples': 10000}
I0405 20:35:25.224729 140396801075008 submission_runner.py:396] After eval at step 8243: RAM USED (GB) 99.911819264
I0405 20:35:25.233694 140344716924672 logging_writer.py:48] [8243] global_step=8243, preemption_count=0, score=2179.980633, test/accuracy=0.353500, test/loss=3.275228, test/num_examples=10000, total_duration=3692.421645, train/accuracy=0.503866, train/loss=2.425347, validation/accuracy=0.466920, validation/loss=2.607143, validation/num_examples=50000
I0405 20:35:25.543814 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_8243.
I0405 20:35:25.544567 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 8243: RAM USED (GB) 99.91084032
I0405 20:37:00.824715 140344230409984 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.565863, loss=3.872584
I0405 20:37:00.828878 140396801075008 submission.py:139] 8500) loss = 3.873, grad_norm = 0.566
I0405 20:40:07.442688 140344716924672 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.533642, loss=3.890309
I0405 20:40:07.447749 140396801075008 submission.py:139] 9000) loss = 3.890, grad_norm = 0.534
I0405 20:43:12.272132 140344230409984 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.518684, loss=3.857120
I0405 20:43:12.276170 140396801075008 submission.py:139] 9500) loss = 3.857, grad_norm = 0.519
I0405 20:43:55.911964 140396801075008 submission_runner.py:373] Before eval at step 9619: RAM USED (GB) 100.141420544
I0405 20:43:55.912198 140396801075008 spec.py:298] Evaluating on the training split.
I0405 20:44:39.844845 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 20:45:24.915436 140396801075008 spec.py:326] Evaluating on the test split.
I0405 20:45:26.283794 140396801075008 submission_runner.py:382] Time since start: 4300.05s, 	Step: 9619, 	{'train/accuracy': 0.5586734693877551, 'train/loss': 2.1265775719467475, 'validation/accuracy': 0.51034, 'validation/loss': 2.35898734375, 'validation/num_examples': 50000, 'test/accuracy': 0.3834, 'test/loss': 3.0381693359375, 'test/num_examples': 10000}
I0405 20:45:26.284135 140396801075008 submission_runner.py:396] After eval at step 9619: RAM USED (GB) 100.254973952
I0405 20:45:26.292807 140344716924672 logging_writer.py:48] [9619] global_step=9619, preemption_count=0, score=2540.248160, test/accuracy=0.383400, test/loss=3.038169, test/num_examples=10000, total_duration=4300.045269, train/accuracy=0.558673, train/loss=2.126578, validation/accuracy=0.510340, validation/loss=2.358987, validation/num_examples=50000
I0405 20:45:26.604969 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_9619.
I0405 20:45:26.605703 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 9619: RAM USED (GB) 100.25398272
I0405 20:47:47.762413 140344230409984 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.511242, loss=3.701602
I0405 20:47:47.766906 140396801075008 submission.py:139] 10000) loss = 3.702, grad_norm = 0.511
I0405 20:50:54.065169 140344716924672 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.510369, loss=3.720494
I0405 20:50:54.069432 140396801075008 submission.py:139] 10500) loss = 3.720, grad_norm = 0.510
I0405 20:53:56.764815 140396801075008 submission_runner.py:373] Before eval at step 10995: RAM USED (GB) 100.049768448
I0405 20:53:56.766462 140396801075008 spec.py:298] Evaluating on the training split.
I0405 20:54:40.338458 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 20:55:35.986544 140396801075008 spec.py:326] Evaluating on the test split.
I0405 20:55:37.341008 140396801075008 submission_runner.py:382] Time since start: 4900.90s, 	Step: 10995, 	{'train/accuracy': 0.5871532206632653, 'train/loss': 1.9894748609893176, 'validation/accuracy': 0.53442, 'validation/loss': 2.229416875, 'validation/num_examples': 50000, 'test/accuracy': 0.4081, 'test/loss': 2.948050390625, 'test/num_examples': 10000}
I0405 20:55:37.341435 140396801075008 submission_runner.py:396] After eval at step 10995: RAM USED (GB) 99.934896128
I0405 20:55:37.358964 140344230409984 logging_writer.py:48] [10995] global_step=10995, preemption_count=0, score=2900.146804, test/accuracy=0.408100, test/loss=2.948050, test/num_examples=10000, total_duration=4900.899757, train/accuracy=0.587153, train/loss=1.989475, validation/accuracy=0.534420, validation/loss=2.229417, validation/num_examples=50000
I0405 20:55:37.711405 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_10995.
I0405 20:55:37.712306 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 10995: RAM USED (GB) 99.932008448
I0405 20:55:39.917887 140344716924672 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.499613, loss=3.612324
I0405 20:55:39.921855 140396801075008 submission.py:139] 11000) loss = 3.612, grad_norm = 0.500
I0405 20:58:46.142293 140344230409984 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.480875, loss=3.554864
I0405 20:58:46.147590 140396801075008 submission.py:139] 11500) loss = 3.555, grad_norm = 0.481
I0405 21:01:51.006394 140344716924672 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.472549, loss=3.562153
I0405 21:01:51.011017 140396801075008 submission.py:139] 12000) loss = 3.562, grad_norm = 0.473
I0405 21:04:07.961953 140396801075008 submission_runner.py:373] Before eval at step 12371: RAM USED (GB) 99.994505216
I0405 21:04:07.962182 140396801075008 spec.py:298] Evaluating on the training split.
I0405 21:04:52.557225 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 21:05:39.476059 140396801075008 spec.py:326] Evaluating on the test split.
I0405 21:05:40.843529 140396801075008 submission_runner.py:382] Time since start: 5512.09s, 	Step: 12371, 	{'train/accuracy': 0.6189612563775511, 'train/loss': 1.8444976806640625, 'validation/accuracy': 0.55988, 'validation/loss': 2.1160128125, 'validation/num_examples': 50000, 'test/accuracy': 0.4308, 'test/loss': 2.8257794921875, 'test/num_examples': 10000}
I0405 21:05:40.843854 140396801075008 submission_runner.py:396] After eval at step 12371: RAM USED (GB) 100.070375424
I0405 21:05:40.851836 140344230409984 logging_writer.py:48] [12371] global_step=12371, preemption_count=0, score=3260.282789, test/accuracy=0.430800, test/loss=2.825779, test/num_examples=10000, total_duration=5512.089146, train/accuracy=0.618961, train/loss=1.844498, validation/accuracy=0.559880, validation/loss=2.116013, validation/num_examples=50000
I0405 21:05:41.170089 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_12371.
I0405 21:05:41.170836 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 12371: RAM USED (GB) 100.069916672
I0405 21:06:29.086445 140344716924672 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.473666, loss=3.579910
I0405 21:06:29.090613 140396801075008 submission.py:139] 12500) loss = 3.580, grad_norm = 0.474
I0405 21:09:35.128320 140344230409984 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.484507, loss=3.542353
I0405 21:09:35.132262 140396801075008 submission.py:139] 13000) loss = 3.542, grad_norm = 0.485
I0405 21:12:39.971054 140344716924672 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.489707, loss=3.470644
I0405 21:12:39.975988 140396801075008 submission.py:139] 13500) loss = 3.471, grad_norm = 0.490
I0405 21:14:11.396178 140396801075008 submission_runner.py:373] Before eval at step 13748: RAM USED (GB) 100.105334784
I0405 21:14:11.396393 140396801075008 spec.py:298] Evaluating on the training split.
I0405 21:14:54.816844 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 21:15:50.962904 140396801075008 spec.py:326] Evaluating on the test split.
I0405 21:15:52.319191 140396801075008 submission_runner.py:382] Time since start: 6115.53s, 	Step: 13748, 	{'train/accuracy': 0.6356823979591837, 'train/loss': 1.793221142827248, 'validation/accuracy': 0.57704, 'validation/loss': 2.05962828125, 'validation/num_examples': 50000, 'test/accuracy': 0.4395, 'test/loss': 2.7734322265625, 'test/num_examples': 10000}
I0405 21:15:52.319537 140396801075008 submission_runner.py:396] After eval at step 13748: RAM USED (GB) 100.224704512
I0405 21:15:52.327344 140344230409984 logging_writer.py:48] [13748] global_step=13748, preemption_count=0, score=3620.342242, test/accuracy=0.439500, test/loss=2.773432, test/num_examples=10000, total_duration=6115.529973, train/accuracy=0.635682, train/loss=1.793221, validation/accuracy=0.577040, validation/loss=2.059628, validation/num_examples=50000
I0405 21:15:52.634199 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_13748.
I0405 21:15:52.634901 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 13748: RAM USED (GB) 100.223721472
I0405 21:17:27.095994 140396801075008 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 100.178944
I0405 21:17:27.096250 140396801075008 spec.py:298] Evaluating on the training split.
I0405 21:18:10.578747 140396801075008 spec.py:310] Evaluating on the validation split.
I0405 21:18:56.824996 140396801075008 spec.py:326] Evaluating on the test split.
I0405 21:18:58.186201 140396801075008 submission_runner.py:382] Time since start: 6311.23s, 	Step: 14000, 	{'train/accuracy': 0.6536989795918368, 'train/loss': 1.6515873500279017, 'validation/accuracy': 0.59248, 'validation/loss': 1.92927765625, 'validation/num_examples': 50000, 'test/accuracy': 0.4609, 'test/loss': 2.63236171875, 'test/num_examples': 10000}
I0405 21:18:58.186531 140396801075008 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 100.101779456
I0405 21:18:58.194409 140344716924672 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=3687.318044, test/accuracy=0.460900, test/loss=2.632362, test/num_examples=10000, total_duration=6311.230356, train/accuracy=0.653699, train/loss=1.651587, validation/accuracy=0.592480, validation/loss=1.929278, validation/num_examples=50000
I0405 21:18:58.508659 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0405 21:18:58.509373 140396801075008 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.1018368
I0405 21:18:58.519442 140344230409984 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=3687.318044
I0405 21:18:59.295260 140396801075008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0405 21:18:59.628969 140396801075008 submission_runner.py:550] Tuning trial 1/1
I0405 21:18:59.629182 140396801075008 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 21:18:59.629746 140396801075008 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006178252551020409, 'train/loss': 6.920537209024235, 'validation/accuracy': 0.00068, 'validation/loss': 6.919533125, 'validation/num_examples': 50000, 'test/accuracy': 0.0006, 'test/loss': 6.92169453125, 'test/num_examples': 10000, 'score': 7.9707958698272705, 'total_duration': 7.972715616226196, 'global_step': 1, 'preemption_count': 0}), (1367, {'train/accuracy': 0.062021683673469385, 'train/loss': 5.579645896444515, 'validation/accuracy': 0.05846, 'validation/loss': 5.64113625, 'validation/num_examples': 50000, 'test/accuracy': 0.0394, 'test/loss': 5.86901328125, 'test/num_examples': 10000, 'score': 379.61098527908325, 'total_duration': 647.3397715091705, 'global_step': 1367, 'preemption_count': 0}), (2741, {'train/accuracy': 0.14188058035714285, 'train/loss': 4.743875620316486, 'validation/accuracy': 0.12776, 'validation/loss': 4.83460125, 'validation/num_examples': 50000, 'test/accuracy': 0.0931, 'test/loss': 5.18133203125, 'test/num_examples': 10000, 'score': 739.7149333953857, 'total_duration': 1262.6259982585907, 'global_step': 2741, 'preemption_count': 0}), (4116, {'train/accuracy': 0.28535554846938777, 'train/loss': 3.57246056381537, 'validation/accuracy': 0.26176, 'validation/loss': 3.698636875, 'validation/num_examples': 50000, 'test/accuracy': 0.1925, 'test/loss': 4.239963671875, 'test/num_examples': 10000, 'score': 1099.9950420856476, 'total_duration': 1874.8420994281769, 'global_step': 4116, 'preemption_count': 0}), (5491, {'train/accuracy': 0.35453204719387754, 'train/loss': 3.243650786730708, 'validation/accuracy': 0.32552, 'validation/loss': 3.3937271875, 'validation/num_examples': 50000, 'test/accuracy': 0.2324, 'test/loss': 3.995139453125, 'test/num_examples': 10000, 'score': 1459.975521326065, 'total_duration': 2474.211200237274, 'global_step': 5491, 'preemption_count': 0}), (6867, {'train/accuracy': 0.44778380102040816, 'train/loss': 2.587762015206473, 'validation/accuracy': 0.413, 'validation/loss': 2.784301875, 'validation/num_examples': 50000, 'test/accuracy': 0.3067, 'test/loss': 3.427705078125, 'test/num_examples': 10000, 'score': 1820.0268704891205, 'total_duration': 3090.592243909836, 'global_step': 6867, 'preemption_count': 0}), (8243, {'train/accuracy': 0.5038663903061225, 'train/loss': 2.425346685915577, 'validation/accuracy': 0.46692, 'validation/loss': 2.6071434375, 'validation/num_examples': 50000, 'test/accuracy': 0.3535, 'test/loss': 3.275227734375, 'test/num_examples': 10000, 'score': 2179.9806332588196, 'total_duration': 3692.4216454029083, 'global_step': 8243, 'preemption_count': 0}), (9619, {'train/accuracy': 0.5586734693877551, 'train/loss': 2.1265775719467475, 'validation/accuracy': 0.51034, 'validation/loss': 2.35898734375, 'validation/num_examples': 50000, 'test/accuracy': 0.3834, 'test/loss': 3.0381693359375, 'test/num_examples': 10000, 'score': 2540.248159646988, 'total_duration': 4300.045268535614, 'global_step': 9619, 'preemption_count': 0}), (10995, {'train/accuracy': 0.5871532206632653, 'train/loss': 1.9894748609893176, 'validation/accuracy': 0.53442, 'validation/loss': 2.229416875, 'validation/num_examples': 50000, 'test/accuracy': 0.4081, 'test/loss': 2.948050390625, 'test/num_examples': 10000, 'score': 2900.146804332733, 'total_duration': 4900.899756908417, 'global_step': 10995, 'preemption_count': 0}), (12371, {'train/accuracy': 0.6189612563775511, 'train/loss': 1.8444976806640625, 'validation/accuracy': 0.55988, 'validation/loss': 2.1160128125, 'validation/num_examples': 50000, 'test/accuracy': 0.4308, 'test/loss': 2.8257794921875, 'test/num_examples': 10000, 'score': 3260.2827887535095, 'total_duration': 5512.089146137238, 'global_step': 12371, 'preemption_count': 0}), (13748, {'train/accuracy': 0.6356823979591837, 'train/loss': 1.793221142827248, 'validation/accuracy': 0.57704, 'validation/loss': 2.05962828125, 'validation/num_examples': 50000, 'test/accuracy': 0.4395, 'test/loss': 2.7734322265625, 'test/num_examples': 10000, 'score': 3620.3422422409058, 'total_duration': 6115.529973268509, 'global_step': 13748, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6536989795918368, 'train/loss': 1.6515873500279017, 'validation/accuracy': 0.59248, 'validation/loss': 1.92927765625, 'validation/num_examples': 50000, 'test/accuracy': 0.4609, 'test/loss': 2.63236171875, 'test/num_examples': 10000, 'score': 3687.31804394722, 'total_duration': 6311.230356216431, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0405 21:18:59.629837 140396801075008 submission_runner.py:553] Timing: 3687.31804394722
I0405 21:18:59.629879 140396801075008 submission_runner.py:554] ====================
I0405 21:18:59.629976 140396801075008 submission_runner.py:613] Final imagenet_resnet score: 3687.31804394722
