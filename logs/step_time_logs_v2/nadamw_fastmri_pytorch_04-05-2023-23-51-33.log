WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 23:51:55.478921 139675619575616 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 23:51:55.478949 140645935015744 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 23:51:55.478969 140280413165376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 23:51:55.479598 140680601597760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 23:51:55.479937 140653185439552 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 23:51:56.462694 139664068962112 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 23:51:56.462733 140422640453440 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 23:51:56.472584 139913939117888 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 23:51:56.472882 139913939117888 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:51:56.473231 139664068962112 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:51:56.473359 140422640453440 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:51:56.482103 139675619575616 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:51:56.482151 140645935015744 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:51:56.482160 140280413165376 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:51:56.482217 140680601597760 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:51:56.482224 140653185439552 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:51:57.041436 139913939117888 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch.
W0405 23:51:57.078462 140645935015744 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:51:57.078457 139675619575616 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:51:57.078457 140680601597760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:51:57.079327 139664068962112 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:51:57.079341 140280413165376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:51:57.080506 139913939117888 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:51:57.080648 140422640453440 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:51:57.082583 140653185439552 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 23:51:57.084894 139913939117888 submission_runner.py:511] Using RNG seed 2690231732
I0405 23:51:57.085911 139913939117888 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 23:51:57.086033 139913939117888 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1.
I0405 23:51:57.086217 139913939117888 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/hparams.json.
I0405 23:51:57.087215 139913939117888 submission_runner.py:230] Starting train once: RAM USED (GB) 5.57715456
I0405 23:51:57.087316 139913939117888 submission_runner.py:231] Initializing dataset.
I0405 23:51:57.087487 139913939117888 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.577703424
I0405 23:51:57.087558 139913939117888 submission_runner.py:240] Initializing model.
I0405 23:52:01.202941 139913939117888 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.206596608
I0405 23:52:01.203150 139913939117888 submission_runner.py:252] Initializing optimizer.
I0405 23:52:01.203999 139913939117888 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.206596608
I0405 23:52:01.204114 139913939117888 submission_runner.py:261] Initializing metrics bundle.
I0405 23:52:01.204226 139913939117888 submission_runner.py:276] Initializing checkpoint and logger.
I0405 23:52:01.208452 139913939117888 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 23:52:01.208559 139913939117888 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 23:52:01.811043 139913939117888 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/meta_data_0.json.
I0405 23:52:01.811930 139913939117888 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/flags_0.json.
I0405 23:52:01.853995 139913939117888 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.257686016
I0405 23:52:01.855202 139913939117888 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.257686016
I0405 23:52:01.855340 139913939117888 submission_runner.py:313] Starting training loop.
I0405 23:52:46.870537 139913939117888 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 23.891849216
I0405 23:52:50.942442 139871482017536 logging_writer.py:48] [0] global_step=0, grad_norm=4.772581, loss=0.686521
I0405 23:52:50.950359 139913939117888 submission.py:296] 0) loss = 0.687, grad_norm = 4.773
I0405 23:52:50.951374 139913939117888 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 32.563625984
I0405 23:52:50.952309 139913939117888 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 32.564658176
I0405 23:52:50.952525 139913939117888 spec.py:298] Evaluating on the training split.
I0405 23:54:28.730703 139913939117888 spec.py:310] Evaluating on the validation split.
I0405 23:55:31.455155 139913939117888 spec.py:326] Evaluating on the test split.
I0405 23:56:33.679503 139913939117888 submission_runner.py:382] Time since start: 49.10s, 	Step: 1, 	{'train/ssim': 0.3166915348597935, 'train/loss': 0.7199264253888812, 'validation/ssim': 0.3109065599500563, 'validation/loss': 0.7318522593380697, 'validation/num_examples': 3554, 'test/ssim': 0.33545116178485407, 'test/loss': 0.7291052167690589, 'test/num_examples': 3581}
I0405 23:56:33.680065 139913939117888 submission_runner.py:396] After eval at step 1: RAM USED (GB) 69.083389952
I0405 23:56:33.689426 139847322810112 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=49.095226, test/loss=0.729105, test/num_examples=3581, test/ssim=0.335451, total_duration=49.097391, train/loss=0.719926, train/ssim=0.316692, validation/loss=0.731852, validation/num_examples=3554, validation/ssim=0.310907
I0405 23:56:33.840043 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_1.
I0405 23:56:33.840584 139913939117888 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 69.08256256
I0405 23:56:33.851009 139913939117888 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 69.08991488
I0405 23:56:33.854707 140645935015744 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:56:33.854701 140422640453440 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:56:33.854718 140653185439552 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:56:33.854718 139664068962112 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:56:33.854780 139913939117888 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:56:33.854753 140680601597760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:56:33.854755 140280413165376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:56:33.854751 139675619575616 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:56:33.916301 139847314417408 logging_writer.py:48] [1] global_step=1, grad_norm=4.736614, loss=0.735289
I0405 23:56:33.921719 139913939117888 submission.py:296] 1) loss = 0.735, grad_norm = 4.737
I0405 23:56:33.922375 139913939117888 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 69.090168832
I0405 23:56:33.994411 139847322810112 logging_writer.py:48] [2] global_step=2, grad_norm=4.183683, loss=0.673512
I0405 23:56:34.000130 139913939117888 submission.py:296] 2) loss = 0.674, grad_norm = 4.184
I0405 23:56:34.071754 139847314417408 logging_writer.py:48] [3] global_step=3, grad_norm=4.637101, loss=0.725862
I0405 23:56:34.075673 139913939117888 submission.py:296] 3) loss = 0.726, grad_norm = 4.637
I0405 23:56:34.144593 139847322810112 logging_writer.py:48] [4] global_step=4, grad_norm=4.203299, loss=0.757603
I0405 23:56:34.149513 139913939117888 submission.py:296] 4) loss = 0.758, grad_norm = 4.203
I0405 23:56:34.219872 139847314417408 logging_writer.py:48] [5] global_step=5, grad_norm=4.277393, loss=0.682428
I0405 23:56:34.225707 139913939117888 submission.py:296] 5) loss = 0.682, grad_norm = 4.277
I0405 23:56:34.293912 139847322810112 logging_writer.py:48] [6] global_step=6, grad_norm=3.840665, loss=0.728315
I0405 23:56:34.297652 139913939117888 submission.py:296] 6) loss = 0.728, grad_norm = 3.841
I0405 23:56:34.368109 139847314417408 logging_writer.py:48] [7] global_step=7, grad_norm=3.519768, loss=0.670590
I0405 23:56:34.371353 139913939117888 submission.py:296] 7) loss = 0.671, grad_norm = 3.520
I0405 23:56:34.437028 139847322810112 logging_writer.py:48] [8] global_step=8, grad_norm=3.642913, loss=0.686124
I0405 23:56:34.442348 139913939117888 submission.py:296] 8) loss = 0.686, grad_norm = 3.643
I0405 23:56:34.513413 139847314417408 logging_writer.py:48] [9] global_step=9, grad_norm=3.840649, loss=0.672180
I0405 23:56:34.519274 139913939117888 submission.py:296] 9) loss = 0.672, grad_norm = 3.841
I0405 23:56:34.597507 139847322810112 logging_writer.py:48] [10] global_step=10, grad_norm=3.814667, loss=0.751946
I0405 23:56:34.601280 139913939117888 submission.py:296] 10) loss = 0.752, grad_norm = 3.815
I0405 23:56:34.667091 139847314417408 logging_writer.py:48] [11] global_step=11, grad_norm=3.775816, loss=0.623931
I0405 23:56:34.670258 139913939117888 submission.py:296] 11) loss = 0.624, grad_norm = 3.776
I0405 23:56:34.736027 139847322810112 logging_writer.py:48] [12] global_step=12, grad_norm=3.051023, loss=0.695571
I0405 23:56:34.740559 139913939117888 submission.py:296] 12) loss = 0.696, grad_norm = 3.051
I0405 23:56:34.813903 139847314417408 logging_writer.py:48] [13] global_step=13, grad_norm=3.894697, loss=0.599822
I0405 23:56:34.818045 139913939117888 submission.py:296] 13) loss = 0.600, grad_norm = 3.895
I0405 23:56:34.930285 139847322810112 logging_writer.py:48] [14] global_step=14, grad_norm=3.504402, loss=0.628683
I0405 23:56:34.933391 139913939117888 submission.py:296] 14) loss = 0.629, grad_norm = 3.504
I0405 23:56:35.207519 139847314417408 logging_writer.py:48] [15] global_step=15, grad_norm=3.411048, loss=0.583828
I0405 23:56:35.212271 139913939117888 submission.py:296] 15) loss = 0.584, grad_norm = 3.411
I0405 23:56:35.473066 139847322810112 logging_writer.py:48] [16] global_step=16, grad_norm=2.853709, loss=0.596229
I0405 23:56:35.476451 139913939117888 submission.py:296] 16) loss = 0.596, grad_norm = 2.854
I0405 23:56:35.710808 139847314417408 logging_writer.py:48] [17] global_step=17, grad_norm=3.390071, loss=0.536132
I0405 23:56:35.715734 139913939117888 submission.py:296] 17) loss = 0.536, grad_norm = 3.390
I0405 23:56:35.969034 139847322810112 logging_writer.py:48] [18] global_step=18, grad_norm=2.631266, loss=0.606767
I0405 23:56:35.975121 139913939117888 submission.py:296] 18) loss = 0.607, grad_norm = 2.631
I0405 23:56:36.255791 139847314417408 logging_writer.py:48] [19] global_step=19, grad_norm=2.726816, loss=0.587593
I0405 23:56:36.261861 139913939117888 submission.py:296] 19) loss = 0.588, grad_norm = 2.727
I0405 23:56:36.549022 139847322810112 logging_writer.py:48] [20] global_step=20, grad_norm=2.530345, loss=0.516509
I0405 23:56:36.554667 139913939117888 submission.py:296] 20) loss = 0.517, grad_norm = 2.530
I0405 23:56:36.790127 139847314417408 logging_writer.py:48] [21] global_step=21, grad_norm=2.046081, loss=0.581458
I0405 23:56:36.795431 139913939117888 submission.py:296] 21) loss = 0.581, grad_norm = 2.046
I0405 23:56:37.110316 139847322810112 logging_writer.py:48] [22] global_step=22, grad_norm=2.273429, loss=0.491256
I0405 23:56:37.115982 139913939117888 submission.py:296] 22) loss = 0.491, grad_norm = 2.273
I0405 23:56:37.337061 139847314417408 logging_writer.py:48] [23] global_step=23, grad_norm=1.786528, loss=0.524196
I0405 23:56:37.342837 139913939117888 submission.py:296] 23) loss = 0.524, grad_norm = 1.787
I0405 23:56:37.606842 139847322810112 logging_writer.py:48] [24] global_step=24, grad_norm=1.595739, loss=0.567806
I0405 23:56:37.612398 139913939117888 submission.py:296] 24) loss = 0.568, grad_norm = 1.596
I0405 23:56:37.869785 139847314417408 logging_writer.py:48] [25] global_step=25, grad_norm=1.364221, loss=0.488360
I0405 23:56:37.874184 139913939117888 submission.py:296] 25) loss = 0.488, grad_norm = 1.364
I0405 23:56:38.128416 139847322810112 logging_writer.py:48] [26] global_step=26, grad_norm=1.498290, loss=0.461689
I0405 23:56:38.131608 139913939117888 submission.py:296] 26) loss = 0.462, grad_norm = 1.498
I0405 23:56:38.381964 139847314417408 logging_writer.py:48] [27] global_step=27, grad_norm=1.566366, loss=0.426726
I0405 23:56:38.386489 139913939117888 submission.py:296] 27) loss = 0.427, grad_norm = 1.566
I0405 23:56:38.695001 139847322810112 logging_writer.py:48] [28] global_step=28, grad_norm=1.344852, loss=0.423475
I0405 23:56:38.700567 139913939117888 submission.py:296] 28) loss = 0.423, grad_norm = 1.345
I0405 23:56:38.918800 139847314417408 logging_writer.py:48] [29] global_step=29, grad_norm=0.917420, loss=0.486910
I0405 23:56:38.921984 139913939117888 submission.py:296] 29) loss = 0.487, grad_norm = 0.917
I0405 23:56:39.178695 139847322810112 logging_writer.py:48] [30] global_step=30, grad_norm=0.993929, loss=0.446182
I0405 23:56:39.182526 139913939117888 submission.py:296] 30) loss = 0.446, grad_norm = 0.994
I0405 23:56:39.460463 139847314417408 logging_writer.py:48] [31] global_step=31, grad_norm=0.708319, loss=0.559630
I0405 23:56:39.463574 139913939117888 submission.py:296] 31) loss = 0.560, grad_norm = 0.708
I0405 23:56:39.719551 139847322810112 logging_writer.py:48] [32] global_step=32, grad_norm=0.810598, loss=0.437377
I0405 23:56:39.722566 139913939117888 submission.py:296] 32) loss = 0.437, grad_norm = 0.811
I0405 23:56:40.043143 139847314417408 logging_writer.py:48] [33] global_step=33, grad_norm=0.705970, loss=0.478375
I0405 23:56:40.046627 139913939117888 submission.py:296] 33) loss = 0.478, grad_norm = 0.706
I0405 23:56:40.261918 139847322810112 logging_writer.py:48] [34] global_step=34, grad_norm=0.680492, loss=0.433209
I0405 23:56:40.266566 139913939117888 submission.py:296] 34) loss = 0.433, grad_norm = 0.680
I0405 23:56:40.514664 139847314417408 logging_writer.py:48] [35] global_step=35, grad_norm=0.612105, loss=0.549361
I0405 23:56:40.519333 139913939117888 submission.py:296] 35) loss = 0.549, grad_norm = 0.612
I0405 23:56:40.797405 139847322810112 logging_writer.py:48] [36] global_step=36, grad_norm=0.695248, loss=0.549063
I0405 23:56:40.801855 139913939117888 submission.py:296] 36) loss = 0.549, grad_norm = 0.695
I0405 23:56:41.051177 139847314417408 logging_writer.py:48] [37] global_step=37, grad_norm=0.742016, loss=0.414477
I0405 23:56:41.056052 139913939117888 submission.py:296] 37) loss = 0.414, grad_norm = 0.742
I0405 23:56:41.305568 139847322810112 logging_writer.py:48] [38] global_step=38, grad_norm=0.735302, loss=0.406491
I0405 23:56:41.313736 139913939117888 submission.py:296] 38) loss = 0.406, grad_norm = 0.735
I0405 23:56:41.568758 139847314417408 logging_writer.py:48] [39] global_step=39, grad_norm=0.749449, loss=0.410317
I0405 23:56:41.574020 139913939117888 submission.py:296] 39) loss = 0.410, grad_norm = 0.749
I0405 23:56:41.846268 139847322810112 logging_writer.py:48] [40] global_step=40, grad_norm=0.748924, loss=0.465078
I0405 23:56:41.852259 139913939117888 submission.py:296] 40) loss = 0.465, grad_norm = 0.749
I0405 23:56:42.087706 139847314417408 logging_writer.py:48] [41] global_step=41, grad_norm=0.784849, loss=0.463151
I0405 23:56:42.093320 139913939117888 submission.py:296] 41) loss = 0.463, grad_norm = 0.785
I0405 23:56:42.362656 139847322810112 logging_writer.py:48] [42] global_step=42, grad_norm=0.748154, loss=0.451428
I0405 23:56:42.368012 139913939117888 submission.py:296] 42) loss = 0.451, grad_norm = 0.748
I0405 23:56:42.689732 139847314417408 logging_writer.py:48] [43] global_step=43, grad_norm=0.872420, loss=0.435401
I0405 23:56:42.695574 139913939117888 submission.py:296] 43) loss = 0.435, grad_norm = 0.872
I0405 23:56:42.904192 139847322810112 logging_writer.py:48] [44] global_step=44, grad_norm=0.689040, loss=0.402144
I0405 23:56:42.907449 139913939117888 submission.py:296] 44) loss = 0.402, grad_norm = 0.689
I0405 23:56:43.156733 139847314417408 logging_writer.py:48] [45] global_step=45, grad_norm=0.743934, loss=0.401767
I0405 23:56:43.159990 139913939117888 submission.py:296] 45) loss = 0.402, grad_norm = 0.744
I0405 23:56:43.399879 139847322810112 logging_writer.py:48] [46] global_step=46, grad_norm=0.726658, loss=0.516331
I0405 23:56:43.403260 139913939117888 submission.py:296] 46) loss = 0.516, grad_norm = 0.727
I0405 23:56:43.625712 139847314417408 logging_writer.py:48] [47] global_step=47, grad_norm=0.717916, loss=0.422558
I0405 23:56:43.629204 139913939117888 submission.py:296] 47) loss = 0.423, grad_norm = 0.718
I0405 23:56:43.966845 139847322810112 logging_writer.py:48] [48] global_step=48, grad_norm=0.779705, loss=0.514995
I0405 23:56:43.970325 139913939117888 submission.py:296] 48) loss = 0.515, grad_norm = 0.780
I0405 23:56:44.216197 139847314417408 logging_writer.py:48] [49] global_step=49, grad_norm=0.639031, loss=0.401660
I0405 23:56:44.219942 139913939117888 submission.py:296] 49) loss = 0.402, grad_norm = 0.639
I0405 23:56:44.477981 139847322810112 logging_writer.py:48] [50] global_step=50, grad_norm=0.615756, loss=0.471591
I0405 23:56:44.481055 139913939117888 submission.py:296] 50) loss = 0.472, grad_norm = 0.616
I0405 23:56:44.737866 139847314417408 logging_writer.py:48] [51] global_step=51, grad_norm=0.552200, loss=0.426767
I0405 23:56:44.742479 139913939117888 submission.py:296] 51) loss = 0.427, grad_norm = 0.552
I0405 23:56:45.014735 139847322810112 logging_writer.py:48] [52] global_step=52, grad_norm=0.566766, loss=0.402706
I0405 23:56:45.019580 139913939117888 submission.py:296] 52) loss = 0.403, grad_norm = 0.567
I0405 23:56:45.302224 139847314417408 logging_writer.py:48] [53] global_step=53, grad_norm=0.526799, loss=0.379467
I0405 23:56:45.308413 139913939117888 submission.py:296] 53) loss = 0.379, grad_norm = 0.527
I0405 23:56:45.551010 139847322810112 logging_writer.py:48] [54] global_step=54, grad_norm=0.494607, loss=0.376521
I0405 23:56:45.555748 139913939117888 submission.py:296] 54) loss = 0.377, grad_norm = 0.495
I0405 23:56:45.830626 139847314417408 logging_writer.py:48] [55] global_step=55, grad_norm=0.472819, loss=0.436803
I0405 23:56:45.836851 139913939117888 submission.py:296] 55) loss = 0.437, grad_norm = 0.473
I0405 23:56:46.101455 139847322810112 logging_writer.py:48] [56] global_step=56, grad_norm=0.431983, loss=0.409171
I0405 23:56:46.106359 139913939117888 submission.py:296] 56) loss = 0.409, grad_norm = 0.432
I0405 23:56:46.320235 139847314417408 logging_writer.py:48] [57] global_step=57, grad_norm=0.463039, loss=0.391624
I0405 23:56:46.325258 139913939117888 submission.py:296] 57) loss = 0.392, grad_norm = 0.463
I0405 23:56:46.600927 139847322810112 logging_writer.py:48] [58] global_step=58, grad_norm=0.436008, loss=0.491311
I0405 23:56:46.605317 139913939117888 submission.py:296] 58) loss = 0.491, grad_norm = 0.436
I0405 23:56:46.867701 139847314417408 logging_writer.py:48] [59] global_step=59, grad_norm=0.384507, loss=0.451621
I0405 23:56:46.872574 139913939117888 submission.py:296] 59) loss = 0.452, grad_norm = 0.385
I0405 23:56:47.141953 139847322810112 logging_writer.py:48] [60] global_step=60, grad_norm=0.354566, loss=0.399053
I0405 23:56:47.146291 139913939117888 submission.py:296] 60) loss = 0.399, grad_norm = 0.355
I0405 23:56:47.457999 139847314417408 logging_writer.py:48] [61] global_step=61, grad_norm=0.416824, loss=0.366767
I0405 23:56:47.465342 139913939117888 submission.py:296] 61) loss = 0.367, grad_norm = 0.417
I0405 23:56:47.687236 139847322810112 logging_writer.py:48] [62] global_step=62, grad_norm=0.319895, loss=0.473274
I0405 23:56:47.690545 139913939117888 submission.py:296] 62) loss = 0.473, grad_norm = 0.320
I0405 23:56:47.976798 139847314417408 logging_writer.py:48] [63] global_step=63, grad_norm=0.348702, loss=0.401065
I0405 23:56:47.980221 139913939117888 submission.py:296] 63) loss = 0.401, grad_norm = 0.349
I0405 23:56:48.238422 139847322810112 logging_writer.py:48] [64] global_step=64, grad_norm=0.357215, loss=0.354479
I0405 23:56:48.243993 139913939117888 submission.py:296] 64) loss = 0.354, grad_norm = 0.357
I0405 23:56:48.485703 139847314417408 logging_writer.py:48] [65] global_step=65, grad_norm=0.639129, loss=0.367382
I0405 23:56:48.490843 139913939117888 submission.py:296] 65) loss = 0.367, grad_norm = 0.639
I0405 23:56:48.778195 139847322810112 logging_writer.py:48] [66] global_step=66, grad_norm=0.382504, loss=0.408784
I0405 23:56:48.784324 139913939117888 submission.py:296] 66) loss = 0.409, grad_norm = 0.383
I0405 23:56:49.086269 139847314417408 logging_writer.py:48] [67] global_step=67, grad_norm=0.591723, loss=0.306663
I0405 23:56:49.092512 139913939117888 submission.py:296] 67) loss = 0.307, grad_norm = 0.592
I0405 23:56:49.367986 139847322810112 logging_writer.py:48] [68] global_step=68, grad_norm=0.409154, loss=0.359555
I0405 23:56:49.372993 139913939117888 submission.py:296] 68) loss = 0.360, grad_norm = 0.409
I0405 23:56:49.614829 139847314417408 logging_writer.py:48] [69] global_step=69, grad_norm=0.393229, loss=0.366885
I0405 23:56:49.621454 139913939117888 submission.py:296] 69) loss = 0.367, grad_norm = 0.393
I0405 23:56:49.920986 139847322810112 logging_writer.py:48] [70] global_step=70, grad_norm=0.284547, loss=0.373330
I0405 23:56:49.926058 139913939117888 submission.py:296] 70) loss = 0.373, grad_norm = 0.285
I0405 23:56:50.126236 139847314417408 logging_writer.py:48] [71] global_step=71, grad_norm=0.404004, loss=0.341607
I0405 23:56:50.129482 139913939117888 submission.py:296] 71) loss = 0.342, grad_norm = 0.404
I0405 23:56:50.409735 139847322810112 logging_writer.py:48] [72] global_step=72, grad_norm=0.357725, loss=0.374979
I0405 23:56:50.413077 139913939117888 submission.py:296] 72) loss = 0.375, grad_norm = 0.358
I0405 23:56:50.674310 139847314417408 logging_writer.py:48] [73] global_step=73, grad_norm=0.270748, loss=0.354077
I0405 23:56:50.677819 139913939117888 submission.py:296] 73) loss = 0.354, grad_norm = 0.271
I0405 23:56:50.886624 139847322810112 logging_writer.py:48] [74] global_step=74, grad_norm=0.324840, loss=0.374049
I0405 23:56:50.892141 139913939117888 submission.py:296] 74) loss = 0.374, grad_norm = 0.325
I0405 23:56:51.166745 139847314417408 logging_writer.py:48] [75] global_step=75, grad_norm=0.423405, loss=0.322002
I0405 23:56:51.171716 139913939117888 submission.py:296] 75) loss = 0.322, grad_norm = 0.423
I0405 23:56:51.427811 139847322810112 logging_writer.py:48] [76] global_step=76, grad_norm=0.434815, loss=0.387886
I0405 23:56:51.433635 139913939117888 submission.py:296] 76) loss = 0.388, grad_norm = 0.435
I0405 23:56:51.756681 139847314417408 logging_writer.py:48] [77] global_step=77, grad_norm=0.349166, loss=0.407169
I0405 23:56:51.762959 139913939117888 submission.py:296] 77) loss = 0.407, grad_norm = 0.349
I0405 23:56:51.957153 139847322810112 logging_writer.py:48] [78] global_step=78, grad_norm=0.304023, loss=0.361609
I0405 23:56:51.963165 139913939117888 submission.py:296] 78) loss = 0.362, grad_norm = 0.304
I0405 23:56:52.245446 139847314417408 logging_writer.py:48] [79] global_step=79, grad_norm=0.253578, loss=0.378987
I0405 23:56:52.249088 139913939117888 submission.py:296] 79) loss = 0.379, grad_norm = 0.254
I0405 23:56:52.515886 139847322810112 logging_writer.py:48] [80] global_step=80, grad_norm=0.390015, loss=0.291473
I0405 23:56:52.519502 139913939117888 submission.py:296] 80) loss = 0.291, grad_norm = 0.390
I0405 23:56:52.797994 139847314417408 logging_writer.py:48] [81] global_step=81, grad_norm=0.283000, loss=0.330907
I0405 23:56:52.801694 139913939117888 submission.py:296] 81) loss = 0.331, grad_norm = 0.283
I0405 23:56:53.064214 139847322810112 logging_writer.py:48] [82] global_step=82, grad_norm=0.247852, loss=0.381887
I0405 23:56:53.067533 139913939117888 submission.py:296] 82) loss = 0.382, grad_norm = 0.248
I0405 23:56:53.363775 139847314417408 logging_writer.py:48] [83] global_step=83, grad_norm=0.320611, loss=0.283201
I0405 23:56:53.367584 139913939117888 submission.py:296] 83) loss = 0.283, grad_norm = 0.321
I0405 23:56:53.668495 139847322810112 logging_writer.py:48] [84] global_step=84, grad_norm=0.305528, loss=0.373721
I0405 23:56:53.673537 139913939117888 submission.py:296] 84) loss = 0.374, grad_norm = 0.306
I0405 23:56:53.975288 139847314417408 logging_writer.py:48] [85] global_step=85, grad_norm=0.202882, loss=0.420612
I0405 23:56:53.980910 139913939117888 submission.py:296] 85) loss = 0.421, grad_norm = 0.203
I0405 23:56:54.183271 139847322810112 logging_writer.py:48] [86] global_step=86, grad_norm=0.203281, loss=0.355615
I0405 23:56:54.188935 139913939117888 submission.py:296] 86) loss = 0.356, grad_norm = 0.203
I0405 23:56:54.486845 139847314417408 logging_writer.py:48] [87] global_step=87, grad_norm=0.169644, loss=0.317076
I0405 23:56:54.490299 139913939117888 submission.py:296] 87) loss = 0.317, grad_norm = 0.170
I0405 23:56:54.811513 139847322810112 logging_writer.py:48] [88] global_step=88, grad_norm=0.261422, loss=0.286216
I0405 23:56:54.817716 139913939117888 submission.py:296] 88) loss = 0.286, grad_norm = 0.261
I0405 23:56:55.008109 139847314417408 logging_writer.py:48] [89] global_step=89, grad_norm=0.236942, loss=0.325159
I0405 23:56:55.012178 139913939117888 submission.py:296] 89) loss = 0.325, grad_norm = 0.237
I0405 23:56:55.305582 139847322810112 logging_writer.py:48] [90] global_step=90, grad_norm=0.239946, loss=0.268453
I0405 23:56:55.311867 139913939117888 submission.py:296] 90) loss = 0.268, grad_norm = 0.240
I0405 23:56:55.578117 139847314417408 logging_writer.py:48] [91] global_step=91, grad_norm=0.207459, loss=0.401473
I0405 23:56:55.582500 139913939117888 submission.py:296] 91) loss = 0.401, grad_norm = 0.207
I0405 23:56:55.832233 139847322810112 logging_writer.py:48] [92] global_step=92, grad_norm=0.183436, loss=0.443051
I0405 23:56:55.837490 139913939117888 submission.py:296] 92) loss = 0.443, grad_norm = 0.183
I0405 23:56:56.132498 139847314417408 logging_writer.py:48] [93] global_step=93, grad_norm=0.230141, loss=0.292217
I0405 23:56:56.137075 139913939117888 submission.py:296] 93) loss = 0.292, grad_norm = 0.230
I0405 23:56:56.376421 139847322810112 logging_writer.py:48] [94] global_step=94, grad_norm=0.270112, loss=0.256055
I0405 23:56:56.379708 139913939117888 submission.py:296] 94) loss = 0.256, grad_norm = 0.270
I0405 23:56:56.609031 139847314417408 logging_writer.py:48] [95] global_step=95, grad_norm=0.302814, loss=0.336126
I0405 23:56:56.612585 139913939117888 submission.py:296] 95) loss = 0.336, grad_norm = 0.303
I0405 23:56:56.908148 139847322810112 logging_writer.py:48] [96] global_step=96, grad_norm=0.325881, loss=0.260394
I0405 23:56:56.911720 139913939117888 submission.py:296] 96) loss = 0.260, grad_norm = 0.326
I0405 23:56:57.209313 139847314417408 logging_writer.py:48] [97] global_step=97, grad_norm=0.359181, loss=0.263132
I0405 23:56:57.214420 139913939117888 submission.py:296] 97) loss = 0.263, grad_norm = 0.359
I0405 23:56:57.476145 139847322810112 logging_writer.py:48] [98] global_step=98, grad_norm=0.205436, loss=0.302321
I0405 23:56:57.479650 139913939117888 submission.py:296] 98) loss = 0.302, grad_norm = 0.205
I0405 23:56:57.725998 139847314417408 logging_writer.py:48] [99] global_step=99, grad_norm=0.239268, loss=0.414372
I0405 23:56:57.731773 139913939117888 submission.py:296] 99) loss = 0.414, grad_norm = 0.239
I0405 23:56:57.950168 139847322810112 logging_writer.py:48] [100] global_step=100, grad_norm=0.236295, loss=0.293968
I0405 23:56:57.957285 139913939117888 submission.py:296] 100) loss = 0.294, grad_norm = 0.236
I0405 23:57:54.081825 139913939117888 submission_runner.py:373] Before eval at step 309: RAM USED (GB) 87.213666304
I0405 23:57:54.082089 139913939117888 spec.py:298] Evaluating on the training split.
I0405 23:57:56.061696 139913939117888 spec.py:310] Evaluating on the validation split.
I0405 23:58:00.441774 139913939117888 spec.py:326] Evaluating on the test split.
I0405 23:58:03.213630 139913939117888 submission_runner.py:382] Time since start: 352.22s, 	Step: 309, 	{'train/ssim': 0.7047346660069057, 'train/loss': 0.30096585409981863, 'validation/ssim': 0.6827692086601365, 'validation/loss': 0.32552487486590814, 'validation/num_examples': 3554, 'test/ssim': 0.7008278555658336, 'test/loss': 0.3275121940776843, 'test/num_examples': 3581}
I0405 23:58:03.214065 139913939117888 submission_runner.py:396] After eval at step 309: RAM USED (GB) 88.308895744
I0405 23:58:03.226411 139847314417408 logging_writer.py:48] [309] global_step=309, preemption_count=0, score=125.767235, test/loss=0.327512, test/num_examples=3581, test/ssim=0.700828, total_duration=352.215210, train/loss=0.300966, train/ssim=0.704735, validation/loss=0.325525, validation/num_examples=3554, validation/ssim=0.682769
I0405 23:58:03.380826 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_309.
I0405 23:58:03.381535 139913939117888 submission_runner.py:416] After logging and checkpointing eval at step 309: RAM USED (GB) 88.3296256
I0405 23:59:07.391315 139847322810112 logging_writer.py:48] [500] global_step=500, grad_norm=0.087028, loss=0.224794
I0405 23:59:07.396439 139913939117888 submission.py:296] 500) loss = 0.225, grad_norm = 0.087
I0405 23:59:23.619470 139913939117888 submission_runner.py:373] Before eval at step 544: RAM USED (GB) 106.14345728
I0405 23:59:23.619686 139913939117888 spec.py:298] Evaluating on the training split.
I0405 23:59:25.661873 139913939117888 spec.py:310] Evaluating on the validation split.
I0405 23:59:30.004701 139913939117888 spec.py:326] Evaluating on the test split.
I0405 23:59:32.935938 139913939117888 submission_runner.py:382] Time since start: 441.75s, 	Step: 544, 	{'train/ssim': 0.7223765509469169, 'train/loss': 0.28391369751521517, 'validation/ssim': 0.7005658511975943, 'validation/loss': 0.3075902592149691, 'validation/num_examples': 3554, 'test/ssim': 0.7178988825572465, 'test/loss': 0.3096451704743787, 'test/num_examples': 3581}
I0405 23:59:32.936320 139913939117888 submission_runner.py:396] After eval at step 544: RAM USED (GB) 107.285508096
I0405 23:59:32.945743 139847314417408 logging_writer.py:48] [544] global_step=544, preemption_count=0, score=201.939497, test/loss=0.309645, test/num_examples=3581, test/ssim=0.717899, total_duration=441.746575, train/loss=0.283914, train/ssim=0.722377, validation/loss=0.307590, validation/num_examples=3554, validation/ssim=0.700566
I0405 23:59:33.102258 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_544.
I0405 23:59:33.102955 139913939117888 submission_runner.py:416] After logging and checkpointing eval at step 544: RAM USED (GB) 107.305848832
I0406 00:00:53.224205 139913939117888 submission_runner.py:373] Before eval at step 776: RAM USED (GB) 124.948824064
I0406 00:00:53.224488 139913939117888 spec.py:298] Evaluating on the training split.
I0406 00:00:55.261139 139913939117888 spec.py:310] Evaluating on the validation split.
I0406 00:00:58.285672 139913939117888 spec.py:326] Evaluating on the test split.
I0406 00:01:00.402726 139913939117888 submission_runner.py:382] Time since start: 531.35s, 	Step: 776, 	{'train/ssim': 0.7243336268833706, 'train/loss': 0.2821298326764788, 'validation/ssim': 0.7026007914717571, 'validation/loss': 0.3051090787888647, 'validation/num_examples': 3554, 'test/ssim': 0.7194615598165666, 'test/loss': 0.30752518309524224, 'test/num_examples': 3581}
I0406 00:01:00.403102 139913939117888 submission_runner.py:396] After eval at step 776: RAM USED (GB) 126.075904
I0406 00:01:00.412554 139847322810112 logging_writer.py:48] [776] global_step=776, preemption_count=0, score=278.083467, test/loss=0.307525, test/num_examples=3581, test/ssim=0.719462, total_duration=531.351877, train/loss=0.282130, train/ssim=0.724334, validation/loss=0.305109, validation/num_examples=3554, validation/ssim=0.702601
I0406 00:01:00.566608 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_776.
I0406 00:01:00.567859 139913939117888 submission_runner.py:416] After logging and checkpointing eval at step 776: RAM USED (GB) 126.095192064
I0406 00:02:15.514746 139847314417408 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.150663, loss=0.268418
I0406 00:02:15.519458 139913939117888 submission.py:296] 1000) loss = 0.268, grad_norm = 0.151
I0406 00:02:20.617798 139913939117888 submission_runner.py:373] Before eval at step 1020: RAM USED (GB) 143.363796992
I0406 00:02:20.618017 139913939117888 spec.py:298] Evaluating on the training split.
I0406 00:02:22.615285 139913939117888 spec.py:310] Evaluating on the validation split.
I0406 00:02:26.816827 139913939117888 spec.py:326] Evaluating on the test split.
I0406 00:02:29.403248 139913939117888 submission_runner.py:382] Time since start: 618.74s, 	Step: 1020, 	{'train/ssim': 0.7303130286080497, 'train/loss': 0.2749087129320417, 'validation/ssim': 0.7083624140493107, 'validation/loss': 0.29780804446794107, 'validation/num_examples': 3554, 'test/ssim': 0.7256392516493297, 'test/loss': 0.29954040067151283, 'test/num_examples': 3581}
I0406 00:02:29.403731 139913939117888 submission_runner.py:396] After eval at step 1020: RAM USED (GB) 143.381266432
I0406 00:02:29.413029 139847322810112 logging_writer.py:48] [1020] global_step=1020, preemption_count=0, score=353.862668, test/loss=0.299540, test/num_examples=3581, test/ssim=0.725639, total_duration=618.742380, train/loss=0.274909, train/ssim=0.730313, validation/loss=0.297808, validation/num_examples=3554, validation/ssim=0.708362
I0406 00:02:29.562484 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_1020.
I0406 00:02:29.563176 139913939117888 submission_runner.py:416] After logging and checkpointing eval at step 1020: RAM USED (GB) 143.37544192
I0406 00:03:49.597008 139913939117888 submission_runner.py:373] Before eval at step 1334: RAM USED (GB) 143.65845504
I0406 00:03:49.597257 139913939117888 spec.py:298] Evaluating on the training split.
I0406 00:03:51.581598 139913939117888 spec.py:310] Evaluating on the validation split.
I0406 00:03:53.705951 139913939117888 spec.py:326] Evaluating on the test split.
I0406 00:03:55.791797 139913939117888 submission_runner.py:382] Time since start: 707.72s, 	Step: 1334, 	{'train/ssim': 0.7299168450491769, 'train/loss': 0.2757055589130947, 'validation/ssim': 0.7080229253130276, 'validation/loss': 0.2984419239325408, 'validation/num_examples': 3554, 'test/ssim': 0.7250763851315973, 'test/loss': 0.30039506330066673, 'test/num_examples': 3581}
I0406 00:03:55.792219 139913939117888 submission_runner.py:396] After eval at step 1334: RAM USED (GB) 143.658221568
I0406 00:03:55.801493 139847314417408 logging_writer.py:48] [1334] global_step=1334, preemption_count=0, score=426.955840, test/loss=0.300395, test/num_examples=3581, test/ssim=0.725076, total_duration=707.720717, train/loss=0.275706, train/ssim=0.729917, validation/loss=0.298442, validation/num_examples=3554, validation/ssim=0.708023
I0406 00:03:55.959459 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_1334.
I0406 00:03:55.960121 139913939117888 submission_runner.py:416] After logging and checkpointing eval at step 1334: RAM USED (GB) 143.667789824
I0406 00:04:37.374497 139847322810112 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.373829, loss=0.257381
I0406 00:04:37.377895 139913939117888 submission.py:296] 1500) loss = 0.257, grad_norm = 0.374
I0406 00:05:16.002913 139913939117888 submission_runner.py:373] Before eval at step 1646: RAM USED (GB) 143.72079616
I0406 00:05:16.003123 139913939117888 spec.py:298] Evaluating on the training split.
I0406 00:05:18.025439 139913939117888 spec.py:310] Evaluating on the validation split.
I0406 00:05:20.473019 139913939117888 spec.py:326] Evaluating on the test split.
I0406 00:05:22.800392 139913939117888 submission_runner.py:382] Time since start: 794.13s, 	Step: 1646, 	{'train/ssim': 0.734715325491769, 'train/loss': 0.2714108739580427, 'validation/ssim': 0.7126450417883019, 'validation/loss': 0.2943933387116629, 'validation/num_examples': 3554, 'test/ssim': 0.7298800445275412, 'test/loss': 0.29610930791416507, 'test/num_examples': 3581}
I0406 00:05:22.800791 139913939117888 submission_runner.py:396] After eval at step 1646: RAM USED (GB) 143.848681472
I0406 00:05:22.809137 139847314417408 logging_writer.py:48] [1646] global_step=1646, preemption_count=0, score=500.110175, test/loss=0.296109, test/num_examples=3581, test/ssim=0.729880, total_duration=794.128793, train/loss=0.271411, train/ssim=0.734715, validation/loss=0.294393, validation/num_examples=3554, validation/ssim=0.712645
I0406 00:05:22.949314 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_1646.
I0406 00:05:22.949846 139913939117888 submission_runner.py:416] After logging and checkpointing eval at step 1646: RAM USED (GB) 143.877844992
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0406 00:06:43.012066 139913939117888 submission_runner.py:373] Before eval at step 1961: RAM USED (GB) 143.772434432
I0406 00:06:43.012280 139913939117888 spec.py:298] Evaluating on the training split.
I0406 00:06:44.940819 139913939117888 spec.py:310] Evaluating on the validation split.
I0406 00:06:46.996191 139913939117888 spec.py:326] Evaluating on the test split.
I0406 00:06:49.015519 139913939117888 submission_runner.py:382] Time since start: 881.14s, 	Step: 1961, 	{'train/ssim': 0.7378009387425014, 'train/loss': 0.27067882674080984, 'validation/ssim': 0.7157841791379431, 'validation/loss': 0.2936467657481007, 'validation/num_examples': 3554, 'test/ssim': 0.73292249622574, 'test/loss': 0.2952821204796146, 'test/num_examples': 3581}
I0406 00:06:49.015874 139913939117888 submission_runner.py:396] After eval at step 1961: RAM USED (GB) 143.772684288
I0406 00:06:49.024661 139847322810112 logging_writer.py:48] [1961] global_step=1961, preemption_count=0, score=573.208379, test/loss=0.295282, test/num_examples=3581, test/ssim=0.732922, total_duration=881.136259, train/loss=0.270679, train/ssim=0.737801, validation/loss=0.293647, validation/num_examples=3554, validation/ssim=0.715784
I0406 00:06:49.164053 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_1961.
I0406 00:06:49.164595 139913939117888 submission_runner.py:416] After logging and checkpointing eval at step 1961: RAM USED (GB) 143.773138944
I0406 00:06:57.222680 139847314417408 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.191882, loss=0.331621
I0406 00:06:57.226115 139913939117888 submission.py:296] 2000) loss = 0.332, grad_norm = 0.192
I0406 00:08:09.245936 139913939117888 submission_runner.py:373] Before eval at step 2275: RAM USED (GB) 143.9869952
I0406 00:08:09.246181 139913939117888 spec.py:298] Evaluating on the training split.
I0406 00:08:11.160075 139913939117888 spec.py:310] Evaluating on the validation split.
I0406 00:08:13.776641 139913939117888 spec.py:326] Evaluating on the test split.
I0406 00:08:15.957192 139913939117888 submission_runner.py:382] Time since start: 967.37s, 	Step: 2275, 	{'train/ssim': 0.7367238317217145, 'train/loss': 0.2716402326311384, 'validation/ssim': 0.7149139558859735, 'validation/loss': 0.29471754289840674, 'validation/num_examples': 3554, 'test/ssim': 0.7319612734745881, 'test/loss': 0.2964070694834718, 'test/num_examples': 3581}
I0406 00:08:15.957600 139913939117888 submission_runner.py:396] After eval at step 2275: RAM USED (GB) 143.948460032
I0406 00:08:15.965524 139847322810112 logging_writer.py:48] [2275] global_step=2275, preemption_count=0, score=646.317759, test/loss=0.296407, test/num_examples=3581, test/ssim=0.731961, total_duration=967.372474, train/loss=0.271640, train/ssim=0.736724, validation/loss=0.294718, validation/num_examples=3554, validation/ssim=0.714914
I0406 00:08:16.103988 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_2275.
I0406 00:08:16.104503 139913939117888 submission_runner.py:416] After logging and checkpointing eval at step 2275: RAM USED (GB) 143.948378112
I0406 00:09:12.682379 139847314417408 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.047752, loss=0.221450
I0406 00:09:12.685818 139913939117888 submission.py:296] 2500) loss = 0.221, grad_norm = 0.048
I0406 00:09:36.292961 139913939117888 submission_runner.py:373] Before eval at step 2591: RAM USED (GB) 144.00434176
I0406 00:09:36.293196 139913939117888 spec.py:298] Evaluating on the training split.
I0406 00:09:38.194433 139913939117888 spec.py:310] Evaluating on the validation split.
I0406 00:09:41.095877 139913939117888 spec.py:326] Evaluating on the test split.
I0406 00:09:43.150110 139913939117888 submission_runner.py:382] Time since start: 1054.42s, 	Step: 2591, 	{'train/ssim': 0.7377236230032784, 'train/loss': 0.2695936645780291, 'validation/ssim': 0.7161755323007527, 'validation/loss': 0.29251845686638295, 'validation/num_examples': 3554, 'test/ssim': 0.7333342150848227, 'test/loss': 0.29410430047472774, 'test/num_examples': 3581}
I0406 00:09:43.150468 139913939117888 submission_runner.py:396] After eval at step 2591: RAM USED (GB) 143.95168768
I0406 00:09:43.159109 139847322810112 logging_writer.py:48] [2591] global_step=2591, preemption_count=0, score=719.443033, test/loss=0.294104, test/num_examples=3581, test/ssim=0.733334, total_duration=1054.418396, train/loss=0.269594, train/ssim=0.737724, validation/loss=0.292518, validation/num_examples=3554, validation/ssim=0.716176
I0406 00:09:43.297171 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_2591.
I0406 00:09:43.297691 139913939117888 submission_runner.py:416] After logging and checkpointing eval at step 2591: RAM USED (GB) 143.950606336
I0406 00:10:12.927555 139913939117888 submission_runner.py:373] Before eval at step 2714: RAM USED (GB) 144.011022336
I0406 00:10:12.927774 139913939117888 spec.py:298] Evaluating on the training split.
I0406 00:10:14.822709 139913939117888 spec.py:310] Evaluating on the validation split.
I0406 00:10:16.811281 139913939117888 spec.py:326] Evaluating on the test split.
I0406 00:10:18.777713 139913939117888 submission_runner.py:382] Time since start: 1091.05s, 	Step: 2714, 	{'train/ssim': 0.7398375783647809, 'train/loss': 0.2682554381234305, 'validation/ssim': 0.7174233697945976, 'validation/loss': 0.29149968153181277, 'validation/num_examples': 3554, 'test/ssim': 0.7345875066540422, 'test/loss': 0.2931196249520036, 'test/num_examples': 3581}
I0406 00:10:18.778099 139913939117888 submission_runner.py:396] After eval at step 2714: RAM USED (GB) 144.016695296
I0406 00:10:18.786401 139847314417408 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=746.352739, test/loss=0.293120, test/num_examples=3581, test/ssim=0.734588, total_duration=1091.051684, train/loss=0.268255, train/ssim=0.739838, validation/loss=0.291500, validation/num_examples=3554, validation/ssim=0.717423
I0406 00:10:18.925252 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_2714.
I0406 00:10:18.925769 139913939117888 submission_runner.py:416] After logging and checkpointing eval at step 2714: RAM USED (GB) 144.016121856
I0406 00:10:18.933341 139847322810112 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=746.352739
I0406 00:10:19.166814 139913939117888 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/fastmri_pytorch/trial_1/checkpoint_2714.
I0406 00:10:20.469373 139913939117888 submission_runner.py:550] Tuning trial 1/1
I0406 00:10:20.469595 139913939117888 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0406 00:10:20.474725 139913939117888 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ssim': 0.3166915348597935, 'train/loss': 0.7199264253888812, 'validation/ssim': 0.3109065599500563, 'validation/loss': 0.7318522593380697, 'validation/num_examples': 3554, 'test/ssim': 0.33545116178485407, 'test/loss': 0.7291052167690589, 'test/num_examples': 3581, 'score': 49.09522581100464, 'total_duration': 49.09739065170288, 'global_step': 1, 'preemption_count': 0}), (309, {'train/ssim': 0.7047346660069057, 'train/loss': 0.30096585409981863, 'validation/ssim': 0.6827692086601365, 'validation/loss': 0.32552487486590814, 'validation/num_examples': 3554, 'test/ssim': 0.7008278555658336, 'test/loss': 0.3275121940776843, 'test/num_examples': 3581, 'score': 125.76723504066467, 'total_duration': 352.2152099609375, 'global_step': 309, 'preemption_count': 0}), (544, {'train/ssim': 0.7223765509469169, 'train/loss': 0.28391369751521517, 'validation/ssim': 0.7005658511975943, 'validation/loss': 0.3075902592149691, 'validation/num_examples': 3554, 'test/ssim': 0.7178988825572465, 'test/loss': 0.3096451704743787, 'test/num_examples': 3581, 'score': 201.93949699401855, 'total_duration': 441.7465748786926, 'global_step': 544, 'preemption_count': 0}), (776, {'train/ssim': 0.7243336268833706, 'train/loss': 0.2821298326764788, 'validation/ssim': 0.7026007914717571, 'validation/loss': 0.3051090787888647, 'validation/num_examples': 3554, 'test/ssim': 0.7194615598165666, 'test/loss': 0.30752518309524224, 'test/num_examples': 3581, 'score': 278.08346676826477, 'total_duration': 531.3518769741058, 'global_step': 776, 'preemption_count': 0}), (1020, {'train/ssim': 0.7303130286080497, 'train/loss': 0.2749087129320417, 'validation/ssim': 0.7083624140493107, 'validation/loss': 0.29780804446794107, 'validation/num_examples': 3554, 'test/ssim': 0.7256392516493297, 'test/loss': 0.29954040067151283, 'test/num_examples': 3581, 'score': 353.8626675605774, 'total_duration': 618.7423803806305, 'global_step': 1020, 'preemption_count': 0}), (1334, {'train/ssim': 0.7299168450491769, 'train/loss': 0.2757055589130947, 'validation/ssim': 0.7080229253130276, 'validation/loss': 0.2984419239325408, 'validation/num_examples': 3554, 'test/ssim': 0.7250763851315973, 'test/loss': 0.30039506330066673, 'test/num_examples': 3581, 'score': 426.9558403491974, 'total_duration': 707.720716714859, 'global_step': 1334, 'preemption_count': 0}), (1646, {'train/ssim': 0.734715325491769, 'train/loss': 0.2714108739580427, 'validation/ssim': 0.7126450417883019, 'validation/loss': 0.2943933387116629, 'validation/num_examples': 3554, 'test/ssim': 0.7298800445275412, 'test/loss': 0.29610930791416507, 'test/num_examples': 3581, 'score': 500.1101748943329, 'total_duration': 794.1287932395935, 'global_step': 1646, 'preemption_count': 0}), (1961, {'train/ssim': 0.7378009387425014, 'train/loss': 0.27067882674080984, 'validation/ssim': 0.7157841791379431, 'validation/loss': 0.2936467657481007, 'validation/num_examples': 3554, 'test/ssim': 0.73292249622574, 'test/loss': 0.2952821204796146, 'test/num_examples': 3581, 'score': 573.2083790302277, 'total_duration': 881.1362586021423, 'global_step': 1961, 'preemption_count': 0}), (2275, {'train/ssim': 0.7367238317217145, 'train/loss': 0.2716402326311384, 'validation/ssim': 0.7149139558859735, 'validation/loss': 0.29471754289840674, 'validation/num_examples': 3554, 'test/ssim': 0.7319612734745881, 'test/loss': 0.2964070694834718, 'test/num_examples': 3581, 'score': 646.3177592754364, 'total_duration': 967.3724739551544, 'global_step': 2275, 'preemption_count': 0}), (2591, {'train/ssim': 0.7377236230032784, 'train/loss': 0.2695936645780291, 'validation/ssim': 0.7161755323007527, 'validation/loss': 0.29251845686638295, 'validation/num_examples': 3554, 'test/ssim': 0.7333342150848227, 'test/loss': 0.29410430047472774, 'test/num_examples': 3581, 'score': 719.4430332183838, 'total_duration': 1054.4183962345123, 'global_step': 2591, 'preemption_count': 0}), (2714, {'train/ssim': 0.7398375783647809, 'train/loss': 0.2682554381234305, 'validation/ssim': 0.7174233697945976, 'validation/loss': 0.29149968153181277, 'validation/num_examples': 3554, 'test/ssim': 0.7345875066540422, 'test/loss': 0.2931196249520036, 'test/num_examples': 3581, 'score': 746.3527390956879, 'total_duration': 1091.0516843795776, 'global_step': 2714, 'preemption_count': 0})], 'global_step': 2714}
I0406 00:10:20.474862 139913939117888 submission_runner.py:553] Timing: 746.3527390956879
I0406 00:10:20.474920 139913939117888 submission_runner.py:554] ====================
I0406 00:10:20.475005 139913939117888 submission_runner.py:613] Final fastmri score: 746.3527390956879
