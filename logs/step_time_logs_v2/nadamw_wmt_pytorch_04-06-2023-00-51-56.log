WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 00:52:12.084227 139979490015040 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 00:52:12.107020 140565491390272 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 00:52:12.108170 140202988824384 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 00:52:12.108201 139862572357440 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 00:52:12.108537 139781140256576 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 00:52:13.023777 140132315952960 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 00:52:13.026870 139738860390208 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 00:52:13.030030 140322653767488 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 00:52:13.030373 140322653767488 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:52:13.034414 140132315952960 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:52:13.037538 139738860390208 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:52:13.038817 140565491390272 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:52:13.038849 140202988824384 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:52:13.038998 139979490015040 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:52:13.039001 139862572357440 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:52:13.039057 139781140256576 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:52:16.926964 140322653767488 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch.
W0406 00:52:16.959402 139781140256576 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:52:16.960268 139738860390208 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:52:16.963152 139979490015040 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:52:16.963677 140322653767488 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:52:16.963657 140132315952960 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 00:52:16.967169 140322653767488 submission_runner.py:511] Using RNG seed 2635561425
I0406 00:52:16.968174 140322653767488 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 00:52:16.968303 140322653767488 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1.
I0406 00:52:16.968505 140322653767488 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1/hparams.json.
I0406 00:52:16.969623 140322653767488 submission_runner.py:230] Starting train once: RAM USED (GB) 15.116177408
I0406 00:52:16.969723 140322653767488 submission_runner.py:231] Initializing dataset.
I0406 00:52:16.969902 140322653767488 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 15.118340096
I0406 00:52:16.969969 140322653767488 submission_runner.py:240] Initializing model.
W0406 00:52:16.970747 140202988824384 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:52:16.972453 140565491390272 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:52:16.978899 139862572357440 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 00:52:20.599217 140322653767488 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.31835392
I0406 00:52:20.599398 140322653767488 submission_runner.py:252] Initializing optimizer.
I0406 00:52:20.600553 140322653767488 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.318349824
I0406 00:52:20.600654 140322653767488 submission_runner.py:261] Initializing metrics bundle.
I0406 00:52:20.600726 140322653767488 submission_runner.py:276] Initializing checkpoint and logger.
I0406 00:52:20.602275 140322653767488 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 00:52:20.602378 140322653767488 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 00:52:21.245956 140322653767488 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1/meta_data_0.json.
I0406 00:52:21.246814 140322653767488 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1/flags_0.json.
I0406 00:52:21.283473 140322653767488 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 19.371782144
I0406 00:52:21.284753 140322653767488 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.371782144
I0406 00:52:21.284884 140322653767488 submission_runner.py:313] Starting training loop.
I0406 00:52:21.295724 140322653767488 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:52:21.299315 140322653767488 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:52:21.299433 140322653767488 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:52:21.352174 140322653767488 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:52:23.620954 140322653767488 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 19.660271616
I0406 00:52:25.228544 140275083101952 logging_writer.py:48] [0] global_step=0, grad_norm=5.815859, loss=11.182068
I0406 00:52:25.232810 140322653767488 submission.py:296] 0) loss = 11.182, grad_norm = 5.816
I0406 00:52:25.233656 140322653767488 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 19.79109376
I0406 00:52:25.234213 140322653767488 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 19.79109376
I0406 00:52:25.234324 140322653767488 spec.py:298] Evaluating on the training split.
I0406 00:52:25.236479 140322653767488 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:52:25.238814 140322653767488 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:52:25.238926 140322653767488 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:52:25.266894 140322653767488 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:52:29.320490 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 00:57:00.833356 140322653767488 spec.py:310] Evaluating on the validation split.
I0406 00:57:00.836721 140322653767488 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:57:00.839886 140322653767488 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:57:00.839999 140322653767488 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:57:00.868937 140322653767488 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:57:04.669222 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 01:01:30.432261 140322653767488 spec.py:326] Evaluating on the test split.
I0406 01:01:30.434948 140322653767488 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 01:01:30.437709 140322653767488 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 01:01:30.437823 140322653767488 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 01:01:30.465148 140322653767488 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 01:01:34.334513 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 01:06:05.621575 140322653767488 submission_runner.py:382] Time since start: 3.95s, 	Step: 1, 	{'train/accuracy': 0.000662925329462459, 'train/loss': 11.188470814140883, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.180824633296549, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.191381238742665, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0406 01:06:05.622005 140322653767488 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.192858112
I0406 01:06:05.629638 140264956778240 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=3.947803, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.191381, test/num_examples=3003, total_duration=3.950085, train/accuracy=0.000663, train/bleu=0.000000, train/loss=11.188471, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.180825, validation/num_examples=3000
I0406 01:06:07.868206 140322653767488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1/checkpoint_1.
I0406 01:06:07.868813 140322653767488 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.193046528
I0406 01:06:07.873312 140322653767488 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.193038336
I0406 01:06:07.876800 140322653767488 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:06:07.876880 139862572357440 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:06:07.876883 140132315952960 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:06:07.876907 139738860390208 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:06:07.876908 140202988824384 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:06:07.876904 139979490015040 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:06:07.877122 140565491390272 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:06:07.877259 139781140256576 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:06:08.309252 140264948385536 logging_writer.py:48] [1] global_step=1, grad_norm=5.800229, loss=11.164554
I0406 01:06:08.312402 140322653767488 submission.py:296] 1) loss = 11.165, grad_norm = 5.800
I0406 01:06:08.313159 140322653767488 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.189876224
I0406 01:06:08.760329 140264956778240 logging_writer.py:48] [2] global_step=2, grad_norm=5.828933, loss=11.171609
I0406 01:06:08.763319 140322653767488 submission.py:296] 2) loss = 11.172, grad_norm = 5.829
I0406 01:06:09.204743 140264948385536 logging_writer.py:48] [3] global_step=3, grad_norm=5.675778, loss=11.144094
I0406 01:06:09.207947 140322653767488 submission.py:296] 3) loss = 11.144, grad_norm = 5.676
I0406 01:06:09.651946 140264956778240 logging_writer.py:48] [4] global_step=4, grad_norm=5.658922, loss=11.118153
I0406 01:06:09.655009 140322653767488 submission.py:296] 4) loss = 11.118, grad_norm = 5.659
I0406 01:06:10.103901 140264948385536 logging_writer.py:48] [5] global_step=5, grad_norm=5.597483, loss=11.071627
I0406 01:06:10.106787 140322653767488 submission.py:296] 5) loss = 11.072, grad_norm = 5.597
I0406 01:06:10.566210 140264956778240 logging_writer.py:48] [6] global_step=6, grad_norm=5.632792, loss=11.032242
I0406 01:06:10.569309 140322653767488 submission.py:296] 6) loss = 11.032, grad_norm = 5.633
I0406 01:06:11.012412 140264948385536 logging_writer.py:48] [7] global_step=7, grad_norm=5.502352, loss=10.975732
I0406 01:06:11.015489 140322653767488 submission.py:296] 7) loss = 10.976, grad_norm = 5.502
I0406 01:06:11.454723 140264956778240 logging_writer.py:48] [8] global_step=8, grad_norm=5.383188, loss=10.907945
I0406 01:06:11.457666 140322653767488 submission.py:296] 8) loss = 10.908, grad_norm = 5.383
I0406 01:06:11.899349 140264948385536 logging_writer.py:48] [9] global_step=9, grad_norm=5.208842, loss=10.847907
I0406 01:06:11.902577 140322653767488 submission.py:296] 9) loss = 10.848, grad_norm = 5.209
I0406 01:06:12.344377 140264956778240 logging_writer.py:48] [10] global_step=10, grad_norm=4.990858, loss=10.779509
I0406 01:06:12.347327 140322653767488 submission.py:296] 10) loss = 10.780, grad_norm = 4.991
I0406 01:06:12.790799 140264948385536 logging_writer.py:48] [11] global_step=11, grad_norm=4.858022, loss=10.693899
I0406 01:06:12.793774 140322653767488 submission.py:296] 11) loss = 10.694, grad_norm = 4.858
I0406 01:06:13.235315 140264956778240 logging_writer.py:48] [12] global_step=12, grad_norm=4.575899, loss=10.624200
I0406 01:06:13.238369 140322653767488 submission.py:296] 12) loss = 10.624, grad_norm = 4.576
I0406 01:06:13.695831 140264948385536 logging_writer.py:48] [13] global_step=13, grad_norm=4.335516, loss=10.542921
I0406 01:06:13.699008 140322653767488 submission.py:296] 13) loss = 10.543, grad_norm = 4.336
I0406 01:06:14.141015 140264956778240 logging_writer.py:48] [14] global_step=14, grad_norm=4.071302, loss=10.462640
I0406 01:06:14.144002 140322653767488 submission.py:296] 14) loss = 10.463, grad_norm = 4.071
I0406 01:06:14.585671 140264948385536 logging_writer.py:48] [15] global_step=15, grad_norm=3.769953, loss=10.381397
I0406 01:06:14.588890 140322653767488 submission.py:296] 15) loss = 10.381, grad_norm = 3.770
I0406 01:06:15.031281 140264956778240 logging_writer.py:48] [16] global_step=16, grad_norm=3.488945, loss=10.323959
I0406 01:06:15.034303 140322653767488 submission.py:296] 16) loss = 10.324, grad_norm = 3.489
I0406 01:06:15.478214 140264948385536 logging_writer.py:48] [17] global_step=17, grad_norm=3.244651, loss=10.244908
I0406 01:06:15.481238 140322653767488 submission.py:296] 17) loss = 10.245, grad_norm = 3.245
I0406 01:06:15.923959 140264956778240 logging_writer.py:48] [18] global_step=18, grad_norm=3.017473, loss=10.163864
I0406 01:06:15.926970 140322653767488 submission.py:296] 18) loss = 10.164, grad_norm = 3.017
I0406 01:06:16.373044 140264948385536 logging_writer.py:48] [19] global_step=19, grad_norm=2.738274, loss=10.124238
I0406 01:06:16.376221 140322653767488 submission.py:296] 19) loss = 10.124, grad_norm = 2.738
I0406 01:06:16.827790 140264956778240 logging_writer.py:48] [20] global_step=20, grad_norm=2.568565, loss=10.048601
I0406 01:06:16.831156 140322653767488 submission.py:296] 20) loss = 10.049, grad_norm = 2.569
I0406 01:06:17.277210 140264948385536 logging_writer.py:48] [21] global_step=21, grad_norm=2.398005, loss=9.967832
I0406 01:06:17.280278 140322653767488 submission.py:296] 21) loss = 9.968, grad_norm = 2.398
I0406 01:06:17.723567 140264956778240 logging_writer.py:48] [22] global_step=22, grad_norm=2.257723, loss=9.923077
I0406 01:06:17.726521 140322653767488 submission.py:296] 22) loss = 9.923, grad_norm = 2.258
I0406 01:06:18.166014 140264948385536 logging_writer.py:48] [23] global_step=23, grad_norm=2.072417, loss=9.877012
I0406 01:06:18.169070 140322653767488 submission.py:296] 23) loss = 9.877, grad_norm = 2.072
I0406 01:06:18.615538 140264956778240 logging_writer.py:48] [24] global_step=24, grad_norm=1.974750, loss=9.822368
I0406 01:06:18.618495 140322653767488 submission.py:296] 24) loss = 9.822, grad_norm = 1.975
I0406 01:06:19.073175 140264948385536 logging_writer.py:48] [25] global_step=25, grad_norm=1.887369, loss=9.754624
I0406 01:06:19.076309 140322653767488 submission.py:296] 25) loss = 9.755, grad_norm = 1.887
I0406 01:06:19.516143 140264956778240 logging_writer.py:48] [26] global_step=26, grad_norm=1.764687, loss=9.716805
I0406 01:06:19.519215 140322653767488 submission.py:296] 26) loss = 9.717, grad_norm = 1.765
I0406 01:06:19.964861 140264948385536 logging_writer.py:48] [27] global_step=27, grad_norm=1.695171, loss=9.666261
I0406 01:06:19.967738 140322653767488 submission.py:296] 27) loss = 9.666, grad_norm = 1.695
I0406 01:06:20.413476 140264956778240 logging_writer.py:48] [28] global_step=28, grad_norm=1.591804, loss=9.643696
I0406 01:06:20.416521 140322653767488 submission.py:296] 28) loss = 9.644, grad_norm = 1.592
I0406 01:06:20.857774 140264948385536 logging_writer.py:48] [29] global_step=29, grad_norm=1.513523, loss=9.597549
I0406 01:06:20.860944 140322653767488 submission.py:296] 29) loss = 9.598, grad_norm = 1.514
I0406 01:06:21.303919 140264956778240 logging_writer.py:48] [30] global_step=30, grad_norm=1.462214, loss=9.515058
I0406 01:06:21.306885 140322653767488 submission.py:296] 30) loss = 9.515, grad_norm = 1.462
I0406 01:06:21.747270 140264948385536 logging_writer.py:48] [31] global_step=31, grad_norm=1.351427, loss=9.511629
I0406 01:06:21.750213 140322653767488 submission.py:296] 31) loss = 9.512, grad_norm = 1.351
I0406 01:06:22.197596 140264956778240 logging_writer.py:48] [32] global_step=32, grad_norm=1.284278, loss=9.431597
I0406 01:06:22.200675 140322653767488 submission.py:296] 32) loss = 9.432, grad_norm = 1.284
I0406 01:06:22.654916 140264948385536 logging_writer.py:48] [33] global_step=33, grad_norm=1.214226, loss=9.434043
I0406 01:06:22.658069 140322653767488 submission.py:296] 33) loss = 9.434, grad_norm = 1.214
I0406 01:06:23.100395 140264956778240 logging_writer.py:48] [34] global_step=34, grad_norm=1.125789, loss=9.412650
I0406 01:06:23.103501 140322653767488 submission.py:296] 34) loss = 9.413, grad_norm = 1.126
I0406 01:06:23.542947 140264948385536 logging_writer.py:48] [35] global_step=35, grad_norm=1.054197, loss=9.376818
I0406 01:06:23.546196 140322653767488 submission.py:296] 35) loss = 9.377, grad_norm = 1.054
I0406 01:06:23.989124 140264956778240 logging_writer.py:48] [36] global_step=36, grad_norm=1.001964, loss=9.344090
I0406 01:06:23.992228 140322653767488 submission.py:296] 36) loss = 9.344, grad_norm = 1.002
I0406 01:06:24.432049 140264948385536 logging_writer.py:48] [37] global_step=37, grad_norm=0.953643, loss=9.333190
I0406 01:06:24.435342 140322653767488 submission.py:296] 37) loss = 9.333, grad_norm = 0.954
I0406 01:06:24.890805 140264956778240 logging_writer.py:48] [38] global_step=38, grad_norm=0.908937, loss=9.329351
I0406 01:06:24.893903 140322653767488 submission.py:296] 38) loss = 9.329, grad_norm = 0.909
I0406 01:06:25.336280 140264948385536 logging_writer.py:48] [39] global_step=39, grad_norm=0.872277, loss=9.282262
I0406 01:06:25.339311 140322653767488 submission.py:296] 39) loss = 9.282, grad_norm = 0.872
I0406 01:06:25.793828 140264956778240 logging_writer.py:48] [40] global_step=40, grad_norm=0.855138, loss=9.220307
I0406 01:06:25.796898 140322653767488 submission.py:296] 40) loss = 9.220, grad_norm = 0.855
I0406 01:06:26.237500 140264948385536 logging_writer.py:48] [41] global_step=41, grad_norm=0.809775, loss=9.239492
I0406 01:06:26.240469 140322653767488 submission.py:296] 41) loss = 9.239, grad_norm = 0.810
I0406 01:06:26.680697 140264956778240 logging_writer.py:48] [42] global_step=42, grad_norm=0.781034, loss=9.203493
I0406 01:06:26.683939 140322653767488 submission.py:296] 42) loss = 9.203, grad_norm = 0.781
I0406 01:06:27.125734 140264948385536 logging_writer.py:48] [43] global_step=43, grad_norm=0.788801, loss=9.154242
I0406 01:06:27.129273 140322653767488 submission.py:296] 43) loss = 9.154, grad_norm = 0.789
I0406 01:06:27.572635 140264956778240 logging_writer.py:48] [44] global_step=44, grad_norm=0.742010, loss=9.174430
I0406 01:06:27.576312 140322653767488 submission.py:296] 44) loss = 9.174, grad_norm = 0.742
I0406 01:06:28.020095 140264948385536 logging_writer.py:48] [45] global_step=45, grad_norm=0.705241, loss=9.121097
I0406 01:06:28.023642 140322653767488 submission.py:296] 45) loss = 9.121, grad_norm = 0.705
I0406 01:06:28.468376 140264956778240 logging_writer.py:48] [46] global_step=46, grad_norm=0.678820, loss=9.114750
I0406 01:06:28.472247 140322653767488 submission.py:296] 46) loss = 9.115, grad_norm = 0.679
I0406 01:06:28.914015 140264948385536 logging_writer.py:48] [47] global_step=47, grad_norm=0.649120, loss=9.070472
I0406 01:06:28.917614 140322653767488 submission.py:296] 47) loss = 9.070, grad_norm = 0.649
I0406 01:06:29.365666 140264956778240 logging_writer.py:48] [48] global_step=48, grad_norm=0.610680, loss=9.080501
I0406 01:06:29.369201 140322653767488 submission.py:296] 48) loss = 9.081, grad_norm = 0.611
I0406 01:06:29.813047 140264948385536 logging_writer.py:48] [49] global_step=49, grad_norm=0.591311, loss=9.052784
I0406 01:06:29.816492 140322653767488 submission.py:296] 49) loss = 9.053, grad_norm = 0.591
I0406 01:06:30.260648 140264956778240 logging_writer.py:48] [50] global_step=50, grad_norm=0.567409, loss=9.028359
I0406 01:06:30.264130 140322653767488 submission.py:296] 50) loss = 9.028, grad_norm = 0.567
I0406 01:06:30.707079 140264948385536 logging_writer.py:48] [51] global_step=51, grad_norm=0.524705, loss=8.999051
I0406 01:06:30.710796 140322653767488 submission.py:296] 51) loss = 8.999, grad_norm = 0.525
I0406 01:06:31.158530 140264956778240 logging_writer.py:48] [52] global_step=52, grad_norm=0.507091, loss=9.015432
I0406 01:06:31.162055 140322653767488 submission.py:296] 52) loss = 9.015, grad_norm = 0.507
I0406 01:06:31.604561 140264948385536 logging_writer.py:48] [53] global_step=53, grad_norm=0.489233, loss=9.007122
I0406 01:06:31.608000 140322653767488 submission.py:296] 53) loss = 9.007, grad_norm = 0.489
I0406 01:06:32.053588 140264956778240 logging_writer.py:48] [54] global_step=54, grad_norm=0.472918, loss=8.992978
I0406 01:06:32.056952 140322653767488 submission.py:296] 54) loss = 8.993, grad_norm = 0.473
I0406 01:06:32.503489 140264948385536 logging_writer.py:48] [55] global_step=55, grad_norm=0.455373, loss=9.001522
I0406 01:06:32.507142 140322653767488 submission.py:296] 55) loss = 9.002, grad_norm = 0.455
I0406 01:06:32.949556 140264956778240 logging_writer.py:48] [56] global_step=56, grad_norm=0.445793, loss=8.975343
I0406 01:06:32.953077 140322653767488 submission.py:296] 56) loss = 8.975, grad_norm = 0.446
I0406 01:06:33.398180 140264948385536 logging_writer.py:48] [57] global_step=57, grad_norm=0.432035, loss=8.929245
I0406 01:06:33.401408 140322653767488 submission.py:296] 57) loss = 8.929, grad_norm = 0.432
I0406 01:06:33.845144 140264956778240 logging_writer.py:48] [58] global_step=58, grad_norm=0.411979, loss=8.927671
I0406 01:06:33.848727 140322653767488 submission.py:296] 58) loss = 8.928, grad_norm = 0.412
I0406 01:06:34.290674 140264948385536 logging_writer.py:48] [59] global_step=59, grad_norm=0.390354, loss=8.971984
I0406 01:06:34.294224 140322653767488 submission.py:296] 59) loss = 8.972, grad_norm = 0.390
I0406 01:06:34.739187 140264956778240 logging_writer.py:48] [60] global_step=60, grad_norm=0.375767, loss=8.894870
I0406 01:06:34.742509 140322653767488 submission.py:296] 60) loss = 8.895, grad_norm = 0.376
I0406 01:06:35.189141 140264948385536 logging_writer.py:48] [61] global_step=61, grad_norm=0.375021, loss=8.921952
I0406 01:06:35.192609 140322653767488 submission.py:296] 61) loss = 8.922, grad_norm = 0.375
I0406 01:06:35.636216 140264956778240 logging_writer.py:48] [62] global_step=62, grad_norm=0.350060, loss=8.897762
I0406 01:06:35.639448 140322653767488 submission.py:296] 62) loss = 8.898, grad_norm = 0.350
I0406 01:06:36.084672 140264948385536 logging_writer.py:48] [63] global_step=63, grad_norm=0.351860, loss=8.861216
I0406 01:06:36.088264 140322653767488 submission.py:296] 63) loss = 8.861, grad_norm = 0.352
I0406 01:06:36.533299 140264956778240 logging_writer.py:48] [64] global_step=64, grad_norm=0.333020, loss=8.885939
I0406 01:06:36.536824 140322653767488 submission.py:296] 64) loss = 8.886, grad_norm = 0.333
I0406 01:06:36.981387 140264948385536 logging_writer.py:48] [65] global_step=65, grad_norm=0.336590, loss=8.850213
I0406 01:06:36.984922 140322653767488 submission.py:296] 65) loss = 8.850, grad_norm = 0.337
I0406 01:06:37.428623 140264956778240 logging_writer.py:48] [66] global_step=66, grad_norm=0.315060, loss=8.850349
I0406 01:06:37.431964 140322653767488 submission.py:296] 66) loss = 8.850, grad_norm = 0.315
I0406 01:06:37.875076 140264948385536 logging_writer.py:48] [67] global_step=67, grad_norm=0.306139, loss=8.834196
I0406 01:06:37.878624 140322653767488 submission.py:296] 67) loss = 8.834, grad_norm = 0.306
I0406 01:06:38.325233 140264956778240 logging_writer.py:48] [68] global_step=68, grad_norm=0.302376, loss=8.816041
I0406 01:06:38.328659 140322653767488 submission.py:296] 68) loss = 8.816, grad_norm = 0.302
I0406 01:06:38.772879 140264948385536 logging_writer.py:48] [69] global_step=69, grad_norm=0.289651, loss=8.820353
I0406 01:06:38.776438 140322653767488 submission.py:296] 69) loss = 8.820, grad_norm = 0.290
I0406 01:06:39.223147 140264956778240 logging_writer.py:48] [70] global_step=70, grad_norm=0.291251, loss=8.808262
I0406 01:06:39.226548 140322653767488 submission.py:296] 70) loss = 8.808, grad_norm = 0.291
I0406 01:06:39.669689 140264948385536 logging_writer.py:48] [71] global_step=71, grad_norm=0.278195, loss=8.840854
I0406 01:06:39.673126 140322653767488 submission.py:296] 71) loss = 8.841, grad_norm = 0.278
I0406 01:06:40.131815 140264956778240 logging_writer.py:48] [72] global_step=72, grad_norm=0.279106, loss=8.781461
I0406 01:06:40.135721 140322653767488 submission.py:296] 72) loss = 8.781, grad_norm = 0.279
I0406 01:06:40.590522 140264948385536 logging_writer.py:48] [73] global_step=73, grad_norm=0.266934, loss=8.777787
I0406 01:06:40.594452 140322653767488 submission.py:296] 73) loss = 8.778, grad_norm = 0.267
I0406 01:06:41.057196 140264956778240 logging_writer.py:48] [74] global_step=74, grad_norm=0.264686, loss=8.785731
I0406 01:06:41.060950 140322653767488 submission.py:296] 74) loss = 8.786, grad_norm = 0.265
I0406 01:06:41.518114 140264948385536 logging_writer.py:48] [75] global_step=75, grad_norm=0.261723, loss=8.751589
I0406 01:06:41.521518 140322653767488 submission.py:296] 75) loss = 8.752, grad_norm = 0.262
I0406 01:06:41.964934 140264956778240 logging_writer.py:48] [76] global_step=76, grad_norm=0.266704, loss=8.776236
I0406 01:06:41.968263 140322653767488 submission.py:296] 76) loss = 8.776, grad_norm = 0.267
I0406 01:06:42.409278 140264948385536 logging_writer.py:48] [77] global_step=77, grad_norm=0.233092, loss=8.754277
I0406 01:06:42.412690 140322653767488 submission.py:296] 77) loss = 8.754, grad_norm = 0.233
I0406 01:06:42.857078 140264956778240 logging_writer.py:48] [78] global_step=78, grad_norm=0.225448, loss=8.761054
I0406 01:06:42.860450 140322653767488 submission.py:296] 78) loss = 8.761, grad_norm = 0.225
I0406 01:06:43.304558 140264948385536 logging_writer.py:48] [79] global_step=79, grad_norm=0.228441, loss=8.765346
I0406 01:06:43.308292 140322653767488 submission.py:296] 79) loss = 8.765, grad_norm = 0.228
I0406 01:06:43.753671 140264956778240 logging_writer.py:48] [80] global_step=80, grad_norm=0.217292, loss=8.736784
I0406 01:06:43.757507 140322653767488 submission.py:296] 80) loss = 8.737, grad_norm = 0.217
I0406 01:06:44.203953 140264948385536 logging_writer.py:48] [81] global_step=81, grad_norm=0.207064, loss=8.757394
I0406 01:06:44.207284 140322653767488 submission.py:296] 81) loss = 8.757, grad_norm = 0.207
I0406 01:06:44.653641 140264956778240 logging_writer.py:48] [82] global_step=82, grad_norm=0.209695, loss=8.689239
I0406 01:06:44.657365 140322653767488 submission.py:296] 82) loss = 8.689, grad_norm = 0.210
I0406 01:06:45.103876 140264948385536 logging_writer.py:48] [83] global_step=83, grad_norm=0.203189, loss=8.731421
I0406 01:06:45.107409 140322653767488 submission.py:296] 83) loss = 8.731, grad_norm = 0.203
I0406 01:06:45.553934 140264956778240 logging_writer.py:48] [84] global_step=84, grad_norm=0.194584, loss=8.726866
I0406 01:06:45.557572 140322653767488 submission.py:296] 84) loss = 8.727, grad_norm = 0.195
I0406 01:06:46.001815 140264948385536 logging_writer.py:48] [85] global_step=85, grad_norm=0.196753, loss=8.729630
I0406 01:06:46.005756 140322653767488 submission.py:296] 85) loss = 8.730, grad_norm = 0.197
I0406 01:06:46.448974 140264956778240 logging_writer.py:48] [86] global_step=86, grad_norm=0.200156, loss=8.712271
I0406 01:06:46.452359 140322653767488 submission.py:296] 86) loss = 8.712, grad_norm = 0.200
I0406 01:06:46.897474 140264948385536 logging_writer.py:48] [87] global_step=87, grad_norm=0.185002, loss=8.703121
I0406 01:06:46.900972 140322653767488 submission.py:296] 87) loss = 8.703, grad_norm = 0.185
I0406 01:06:47.344806 140264956778240 logging_writer.py:48] [88] global_step=88, grad_norm=0.188210, loss=8.675689
I0406 01:06:47.348159 140322653767488 submission.py:296] 88) loss = 8.676, grad_norm = 0.188
I0406 01:06:47.791364 140264948385536 logging_writer.py:48] [89] global_step=89, grad_norm=0.178679, loss=8.704554
I0406 01:06:47.794992 140322653767488 submission.py:296] 89) loss = 8.705, grad_norm = 0.179
I0406 01:06:48.236598 140264956778240 logging_writer.py:48] [90] global_step=90, grad_norm=0.180409, loss=8.714974
I0406 01:06:48.239951 140322653767488 submission.py:296] 90) loss = 8.715, grad_norm = 0.180
I0406 01:06:48.684226 140264948385536 logging_writer.py:48] [91] global_step=91, grad_norm=0.172427, loss=8.691193
I0406 01:06:48.687649 140322653767488 submission.py:296] 91) loss = 8.691, grad_norm = 0.172
I0406 01:06:49.132114 140264956778240 logging_writer.py:48] [92] global_step=92, grad_norm=0.175898, loss=8.691761
I0406 01:06:49.135605 140322653767488 submission.py:296] 92) loss = 8.692, grad_norm = 0.176
I0406 01:06:49.579748 140264948385536 logging_writer.py:48] [93] global_step=93, grad_norm=0.181676, loss=8.661180
I0406 01:06:49.583353 140322653767488 submission.py:296] 93) loss = 8.661, grad_norm = 0.182
I0406 01:06:50.026975 140264956778240 logging_writer.py:48] [94] global_step=94, grad_norm=0.174904, loss=8.676803
I0406 01:06:50.030272 140322653767488 submission.py:296] 94) loss = 8.677, grad_norm = 0.175
I0406 01:06:50.473348 140264948385536 logging_writer.py:48] [95] global_step=95, grad_norm=0.177216, loss=8.706350
I0406 01:06:50.476888 140322653767488 submission.py:296] 95) loss = 8.706, grad_norm = 0.177
I0406 01:06:50.919878 140264956778240 logging_writer.py:48] [96] global_step=96, grad_norm=0.169650, loss=8.640891
I0406 01:06:50.923222 140322653767488 submission.py:296] 96) loss = 8.641, grad_norm = 0.170
I0406 01:06:51.366789 140264948385536 logging_writer.py:48] [97] global_step=97, grad_norm=0.174138, loss=8.670826
I0406 01:06:51.370239 140322653767488 submission.py:296] 97) loss = 8.671, grad_norm = 0.174
I0406 01:06:51.815225 140264956778240 logging_writer.py:48] [98] global_step=98, grad_norm=0.171133, loss=8.665342
I0406 01:06:51.818645 140322653767488 submission.py:296] 98) loss = 8.665, grad_norm = 0.171
I0406 01:06:52.265799 140264948385536 logging_writer.py:48] [99] global_step=99, grad_norm=0.168263, loss=8.674988
I0406 01:06:52.269247 140322653767488 submission.py:296] 99) loss = 8.675, grad_norm = 0.168
I0406 01:06:52.713286 140264956778240 logging_writer.py:48] [100] global_step=100, grad_norm=0.193505, loss=8.660657
I0406 01:06:52.716747 140322653767488 submission.py:296] 100) loss = 8.661, grad_norm = 0.194
I0406 01:09:48.476633 140264948385536 logging_writer.py:48] [500] global_step=500, grad_norm=0.516405, loss=6.901414
I0406 01:09:48.479983 140322653767488 submission.py:296] 500) loss = 6.901, grad_norm = 0.516
I0406 01:13:28.428799 140264956778240 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.569169, loss=5.733371
I0406 01:13:28.432818 140322653767488 submission.py:296] 1000) loss = 5.733, grad_norm = 0.569
I0406 01:17:08.322129 140264948385536 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.858232, loss=4.920192
I0406 01:17:08.325575 140322653767488 submission.py:296] 1500) loss = 4.920, grad_norm = 0.858
I0406 01:20:07.973955 140322653767488 submission_runner.py:373] Before eval at step 1909: RAM USED (GB) 20.546023424
I0406 01:20:07.974184 140322653767488 spec.py:298] Evaluating on the training split.
I0406 01:20:11.837360 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 01:22:41.227460 140322653767488 spec.py:310] Evaluating on the validation split.
I0406 01:22:44.928146 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 01:25:18.824210 140322653767488 spec.py:326] Evaluating on the test split.
I0406 01:25:22.621919 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 01:27:33.087550 140322653767488 submission_runner.py:382] Time since start: 1666.69s, 	Step: 1909, 	{'train/accuracy': 0.46076649635954237, 'train/loss': 3.492766947844872, 'train/bleu': 17.268559466024023, 'validation/accuracy': 0.4578864490210909, 'validation/loss': 3.5406167468475283, 'validation/bleu': 13.085936722948535, 'validation/num_examples': 3000, 'test/accuracy': 0.44893382139329496, 'test/loss': 3.66843188948928, 'test/bleu': 11.308096913984675, 'test/num_examples': 3003}
I0406 01:27:33.088054 140322653767488 submission_runner.py:396] After eval at step 1909: RAM USED (GB) 20.726738944
I0406 01:27:33.096674 140264956778240 logging_writer.py:48] [1909] global_step=1909, preemption_count=0, score=840.400414, test/accuracy=0.448934, test/bleu=11.308097, test/loss=3.668432, test/num_examples=3003, total_duration=1666.689421, train/accuracy=0.460766, train/bleu=17.268559, train/loss=3.492767, validation/accuracy=0.457886, validation/bleu=13.085937, validation/loss=3.540617, validation/num_examples=3000
I0406 01:27:35.295673 140322653767488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1/checkpoint_1909.
I0406 01:27:35.296331 140322653767488 submission_runner.py:416] After logging and checkpointing eval at step 1909: RAM USED (GB) 20.726063104
I0406 01:28:15.592059 140264948385536 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.647265, loss=4.428421
I0406 01:28:15.595453 140322653767488 submission.py:296] 2000) loss = 4.428, grad_norm = 0.647
I0406 01:31:55.293152 140264956778240 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.534557, loss=4.074468
I0406 01:31:55.296430 140322653767488 submission.py:296] 2500) loss = 4.074, grad_norm = 0.535
I0406 01:35:34.769903 140264948385536 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.561857, loss=3.881697
I0406 01:35:34.773404 140322653767488 submission.py:296] 3000) loss = 3.882, grad_norm = 0.562
I0406 01:39:14.299160 140264956778240 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.458643, loss=3.750686
I0406 01:39:14.302184 140322653767488 submission.py:296] 3500) loss = 3.751, grad_norm = 0.459
I0406 01:41:35.713084 140322653767488 submission_runner.py:373] Before eval at step 3823: RAM USED (GB) 20.81067008
I0406 01:41:35.713311 140322653767488 spec.py:298] Evaluating on the training split.
I0406 01:41:39.563075 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 01:44:00.695076 140322653767488 spec.py:310] Evaluating on the validation split.
I0406 01:44:04.412626 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 01:46:20.458945 140322653767488 spec.py:326] Evaluating on the test split.
I0406 01:46:24.245468 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 01:48:32.428405 140322653767488 submission_runner.py:382] Time since start: 2954.43s, 	Step: 3823, 	{'train/accuracy': 0.5509937902536993, 'train/loss': 2.6250723195941736, 'train/bleu': 24.931406415300167, 'validation/accuracy': 0.5541902766239725, 'validation/loss': 2.5849309447496003, 'validation/bleu': 21.117097562781698, 'validation/num_examples': 3000, 'test/accuracy': 0.5565975248387659, 'test/loss': 2.5970743637789786, 'test/bleu': 19.719799227633967, 'test/num_examples': 3003}
I0406 01:48:32.428822 140322653767488 submission_runner.py:396] After eval at step 3823: RAM USED (GB) 20.954759168
I0406 01:48:32.437397 140264948385536 logging_writer.py:48] [3823] global_step=3823, preemption_count=0, score=1677.314255, test/accuracy=0.556598, test/bleu=19.719799, test/loss=2.597074, test/num_examples=3003, total_duration=2954.428575, train/accuracy=0.550994, train/bleu=24.931406, train/loss=2.625072, validation/accuracy=0.554190, validation/bleu=21.117098, validation/loss=2.584931, validation/num_examples=3000
I0406 01:48:34.700098 140322653767488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1/checkpoint_3823.
I0406 01:48:34.700722 140322653767488 submission_runner.py:416] After logging and checkpointing eval at step 3823: RAM USED (GB) 20.95486976
I0406 01:49:52.746978 140264956778240 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.383709, loss=3.668103
I0406 01:49:52.750084 140322653767488 submission.py:296] 4000) loss = 3.668, grad_norm = 0.384
I0406 01:53:32.179026 140264948385536 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.393464, loss=3.562847
I0406 01:53:32.182381 140322653767488 submission.py:296] 4500) loss = 3.563, grad_norm = 0.393
I0406 01:57:11.713612 140264956778240 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.311184, loss=3.466180
I0406 01:57:11.716976 140322653767488 submission.py:296] 5000) loss = 3.466, grad_norm = 0.311
I0406 02:00:51.261971 140264948385536 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.251449, loss=3.476407
I0406 02:00:51.265000 140322653767488 submission.py:296] 5500) loss = 3.476, grad_norm = 0.251
I0406 02:02:34.958182 140322653767488 submission_runner.py:373] Before eval at step 5737: RAM USED (GB) 21.038247936
I0406 02:02:34.958385 140322653767488 spec.py:298] Evaluating on the training split.
I0406 02:02:38.796569 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 02:05:05.837656 140322653767488 spec.py:310] Evaluating on the validation split.
I0406 02:05:09.540888 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 02:07:26.617469 140322653767488 spec.py:326] Evaluating on the test split.
I0406 02:07:30.391403 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 02:09:58.331753 140322653767488 submission_runner.py:382] Time since start: 4213.67s, 	Step: 5737, 	{'train/accuracy': 0.5727771253976035, 'train/loss': 2.378219662421762, 'train/bleu': 26.946295599056715, 'validation/accuracy': 0.591090005083632, 'validation/loss': 2.248659928271193, 'validation/bleu': 23.617921126962262, 'validation/num_examples': 3000, 'test/accuracy': 0.5930858172099239, 'test/loss': 2.2390989047702052, 'test/bleu': 22.48464593183674, 'test/num_examples': 3003}
I0406 02:09:58.332186 140322653767488 submission_runner.py:396] After eval at step 5737: RAM USED (GB) 21.08424192
I0406 02:09:58.340434 140264956778240 logging_writer.py:48] [5737] global_step=5737, preemption_count=0, score=2513.907006, test/accuracy=0.593086, test/bleu=22.484646, test/loss=2.239099, test/num_examples=3003, total_duration=4213.673659, train/accuracy=0.572777, train/bleu=26.946296, train/loss=2.378220, validation/accuracy=0.591090, validation/bleu=23.617921, validation/loss=2.248660, validation/num_examples=3000
I0406 02:10:00.611062 140322653767488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1/checkpoint_5737.
I0406 02:10:00.611680 140322653767488 submission_runner.py:416] After logging and checkpointing eval at step 5737: RAM USED (GB) 21.083885568
I0406 02:11:56.416418 140264948385536 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.221006, loss=3.329558
I0406 02:11:56.419702 140322653767488 submission.py:296] 6000) loss = 3.330, grad_norm = 0.221
I0406 02:15:35.634121 140264956778240 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.218615, loss=3.301277
I0406 02:15:35.637272 140322653767488 submission.py:296] 6500) loss = 3.301, grad_norm = 0.219
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0406 02:19:15.202675 140264948385536 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.207991, loss=3.387616
I0406 02:19:15.205748 140322653767488 submission.py:296] 7000) loss = 3.388, grad_norm = 0.208
I0406 02:22:54.308454 140264956778240 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.199239, loss=3.333143
I0406 02:22:54.311935 140322653767488 submission.py:296] 7500) loss = 3.333, grad_norm = 0.199
I0406 02:24:00.975776 140322653767488 submission_runner.py:373] Before eval at step 7653: RAM USED (GB) 21.404094464
I0406 02:24:00.976008 140322653767488 spec.py:298] Evaluating on the training split.
I0406 02:24:04.820581 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 02:26:30.363651 140322653767488 spec.py:310] Evaluating on the validation split.
I0406 02:26:34.064162 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 02:28:38.806748 140322653767488 spec.py:326] Evaluating on the test split.
I0406 02:28:42.591276 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 02:30:47.499957 140322653767488 submission_runner.py:382] Time since start: 5499.69s, 	Step: 7653, 	{'train/accuracy': 0.6020893562617339, 'train/loss': 2.1515269203303347, 'train/bleu': 28.160351066808328, 'validation/accuracy': 0.610717784032436, 'validation/loss': 2.0871013688608944, 'validation/bleu': 25.113564291971688, 'validation/num_examples': 3000, 'test/accuracy': 0.6144093893440242, 'test/loss': 2.0560266689907616, 'test/bleu': 23.453365053694544, 'test/num_examples': 3003}
I0406 02:30:47.500382 140322653767488 submission_runner.py:396] After eval at step 7653: RAM USED (GB) 21.466243072
I0406 02:30:47.508160 140264948385536 logging_writer.py:48] [7653] global_step=7653, preemption_count=0, score=3350.473961, test/accuracy=0.614409, test/bleu=23.453365, test/loss=2.056027, test/num_examples=3003, total_duration=5499.691316, train/accuracy=0.602089, train/bleu=28.160351, train/loss=2.151527, validation/accuracy=0.610718, validation/bleu=25.113564, validation/loss=2.087101, validation/num_examples=3000
I0406 02:30:49.654818 140322653767488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1/checkpoint_7653.
I0406 02:30:49.655436 140322653767488 submission_runner.py:416] After logging and checkpointing eval at step 7653: RAM USED (GB) 21.466517504
I0406 02:33:21.962182 140264956778240 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.166581, loss=3.315211
I0406 02:33:21.966169 140322653767488 submission.py:296] 8000) loss = 3.315, grad_norm = 0.167
I0406 02:37:01.208052 140264948385536 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.162488, loss=3.288229
I0406 02:37:01.211303 140322653767488 submission.py:296] 8500) loss = 3.288, grad_norm = 0.162
I0406 02:40:40.619656 140264956778240 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.162218, loss=3.259354
I0406 02:40:40.623030 140322653767488 submission.py:296] 9000) loss = 3.259, grad_norm = 0.162
I0406 02:44:20.053010 140264948385536 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.138311, loss=3.141706
I0406 02:44:20.056199 140322653767488 submission.py:296] 9500) loss = 3.142, grad_norm = 0.138
I0406 02:44:49.824412 140322653767488 submission_runner.py:373] Before eval at step 9569: RAM USED (GB) 21.653598208
I0406 02:44:49.824605 140322653767488 spec.py:298] Evaluating on the training split.
I0406 02:44:53.679952 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 02:47:15.482160 140322653767488 spec.py:310] Evaluating on the validation split.
I0406 02:47:19.204567 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 02:49:30.435570 140322653767488 spec.py:326] Evaluating on the test split.
I0406 02:49:34.203882 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 02:51:43.484750 140322653767488 submission_runner.py:382] Time since start: 6748.54s, 	Step: 9569, 	{'train/accuracy': 0.6107719764917187, 'train/loss': 2.073677943366413, 'train/bleu': 29.677133910502665, 'validation/accuracy': 0.6280145317479014, 'validation/loss': 1.9432975490074518, 'validation/bleu': 25.997886671765407, 'validation/num_examples': 3000, 'test/accuracy': 0.632409505548777, 'test/loss': 1.9045991662308988, 'test/bleu': 25.11441475962948, 'test/num_examples': 3003}
I0406 02:51:43.485165 140322653767488 submission_runner.py:396] After eval at step 9569: RAM USED (GB) 21.687066624
I0406 02:51:43.493120 140264956778240 logging_writer.py:48] [9569] global_step=9569, preemption_count=0, score=4187.045918, test/accuracy=0.632410, test/bleu=25.114415, test/loss=1.904599, test/num_examples=3003, total_duration=6748.537552, train/accuracy=0.610772, train/bleu=29.677134, train/loss=2.073678, validation/accuracy=0.628015, validation/bleu=25.997887, validation/loss=1.943298, validation/num_examples=3000
I0406 02:51:45.653152 140322653767488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1/checkpoint_9569.
I0406 02:51:45.653711 140322653767488 submission_runner.py:416] After logging and checkpointing eval at step 9569: RAM USED (GB) 21.686665216
I0406 02:54:54.448026 140322653767488 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 21.68864768
I0406 02:54:54.448232 140322653767488 spec.py:298] Evaluating on the training split.
I0406 02:54:58.301630 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 02:57:22.792896 140322653767488 spec.py:310] Evaluating on the validation split.
I0406 02:57:26.511868 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 02:59:36.379946 140322653767488 spec.py:326] Evaluating on the test split.
I0406 02:59:40.159590 140322653767488 workload.py:130] Translating evaluation dataset.
I0406 03:01:48.797066 140322653767488 submission_runner.py:382] Time since start: 7353.16s, 	Step: 10000, 	{'train/accuracy': 0.6143393548091673, 'train/loss': 2.043121057707718, 'train/bleu': 29.5937323272165, 'validation/accuracy': 0.6289444644207759, 'validation/loss': 1.93488398314962, 'validation/bleu': 26.48251236197099, 'validation/num_examples': 3000, 'test/accuracy': 0.636778804253094, 'test/loss': 1.890546380221951, 'test/bleu': 25.413979336005596, 'test/num_examples': 3003}
I0406 03:01:48.797474 140322653767488 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 21.745922048
I0406 03:01:48.805268 140264948385536 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4374.895929, test/accuracy=0.636779, test/bleu=25.413979, test/loss=1.890546, test/num_examples=3003, total_duration=7353.160999, train/accuracy=0.614339, train/bleu=29.593732, train/loss=2.043121, validation/accuracy=0.628944, validation/bleu=26.482512, validation/loss=1.934884, validation/num_examples=3000
I0406 03:01:51.198798 140322653767488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1/checkpoint_10000.
I0406 03:01:51.199405 140322653767488 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.745033216
I0406 03:01:51.207071 140264956778240 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4374.895929
I0406 03:01:55.434820 140322653767488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/wmt_pytorch/trial_1/checkpoint_10000.
I0406 03:01:55.457824 140322653767488 submission_runner.py:550] Tuning trial 1/1
I0406 03:01:55.458003 140322653767488 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0406 03:01:55.458827 140322653767488 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000662925329462459, 'train/loss': 11.188470814140883, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.180824633296549, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.191381238742665, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 3.947802782058716, 'total_duration': 3.950085163116455, 'global_step': 1, 'preemption_count': 0}), (1909, {'train/accuracy': 0.46076649635954237, 'train/loss': 3.492766947844872, 'train/bleu': 17.268559466024023, 'validation/accuracy': 0.4578864490210909, 'validation/loss': 3.5406167468475283, 'validation/bleu': 13.085936722948535, 'validation/num_examples': 3000, 'test/accuracy': 0.44893382139329496, 'test/loss': 3.66843188948928, 'test/bleu': 11.308096913984675, 'test/num_examples': 3003, 'score': 840.4004139900208, 'total_duration': 1666.689421415329, 'global_step': 1909, 'preemption_count': 0}), (3823, {'train/accuracy': 0.5509937902536993, 'train/loss': 2.6250723195941736, 'train/bleu': 24.931406415300167, 'validation/accuracy': 0.5541902766239725, 'validation/loss': 2.5849309447496003, 'validation/bleu': 21.117097562781698, 'validation/num_examples': 3000, 'test/accuracy': 0.5565975248387659, 'test/loss': 2.5970743637789786, 'test/bleu': 19.719799227633967, 'test/num_examples': 3003, 'score': 1677.3142552375793, 'total_duration': 2954.42857503891, 'global_step': 3823, 'preemption_count': 0}), (5737, {'train/accuracy': 0.5727771253976035, 'train/loss': 2.378219662421762, 'train/bleu': 26.946295599056715, 'validation/accuracy': 0.591090005083632, 'validation/loss': 2.248659928271193, 'validation/bleu': 23.617921126962262, 'validation/num_examples': 3000, 'test/accuracy': 0.5930858172099239, 'test/loss': 2.2390989047702052, 'test/bleu': 22.48464593183674, 'test/num_examples': 3003, 'score': 2513.907006263733, 'total_duration': 4213.673659086227, 'global_step': 5737, 'preemption_count': 0}), (7653, {'train/accuracy': 0.6020893562617339, 'train/loss': 2.1515269203303347, 'train/bleu': 28.160351066808328, 'validation/accuracy': 0.610717784032436, 'validation/loss': 2.0871013688608944, 'validation/bleu': 25.113564291971688, 'validation/num_examples': 3000, 'test/accuracy': 0.6144093893440242, 'test/loss': 2.0560266689907616, 'test/bleu': 23.453365053694544, 'test/num_examples': 3003, 'score': 3350.473961353302, 'total_duration': 5499.69131565094, 'global_step': 7653, 'preemption_count': 0}), (9569, {'train/accuracy': 0.6107719764917187, 'train/loss': 2.073677943366413, 'train/bleu': 29.677133910502665, 'validation/accuracy': 0.6280145317479014, 'validation/loss': 1.9432975490074518, 'validation/bleu': 25.997886671765407, 'validation/num_examples': 3000, 'test/accuracy': 0.632409505548777, 'test/loss': 1.9045991662308988, 'test/bleu': 25.11441475962948, 'test/num_examples': 3003, 'score': 4187.045917510986, 'total_duration': 6748.53755235672, 'global_step': 9569, 'preemption_count': 0}), (10000, {'train/accuracy': 0.6143393548091673, 'train/loss': 2.043121057707718, 'train/bleu': 29.5937323272165, 'validation/accuracy': 0.6289444644207759, 'validation/loss': 1.93488398314962, 'validation/bleu': 26.48251236197099, 'validation/num_examples': 3000, 'test/accuracy': 0.636778804253094, 'test/loss': 1.890546380221951, 'test/bleu': 25.413979336005596, 'test/num_examples': 3003, 'score': 4374.895928859711, 'total_duration': 7353.160999298096, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0406 03:01:55.458927 140322653767488 submission_runner.py:553] Timing: 4374.895928859711
I0406 03:01:55.458983 140322653767488 submission_runner.py:554] ====================
I0406 03:01:55.459086 140322653767488 submission_runner.py:613] Final wmt score: 4374.895928859711
