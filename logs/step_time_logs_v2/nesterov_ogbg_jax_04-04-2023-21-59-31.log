I0404 21:59:44.953786 139659058521920 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov_v2/ogbg_jax.
I0404 21:59:44.997038 139659058521920 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 21:59:45.860251 139659058521920 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0404 21:59:45.860921 139659058521920 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 21:59:45.864413 139659058521920 submission_runner.py:511] Using RNG seed 1153657245
I0404 21:59:47.103525 139659058521920 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 21:59:47.103737 139659058521920 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1.
I0404 21:59:47.103921 139659058521920 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/hparams.json.
I0404 21:59:47.224931 139659058521920 submission_runner.py:230] Starting train once: RAM USED (GB) 4.21513216
I0404 21:59:47.225116 139659058521920 submission_runner.py:231] Initializing dataset.
I0404 21:59:47.451570 139659058521920 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:59:47.456538 139659058521920 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:59:47.617556 139659058521920 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:59:47.645238 139659058521920 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.287717376
I0404 21:59:47.645416 139659058521920 submission_runner.py:240] Initializing model.
I0404 21:59:54.604651 139659058521920 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.112099328
I0404 21:59:54.604853 139659058521920 submission_runner.py:252] Initializing optimizer.
I0404 21:59:54.939743 139659058521920 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.112144384
I0404 21:59:54.939913 139659058521920 submission_runner.py:261] Initializing metrics bundle.
I0404 21:59:54.939963 139659058521920 submission_runner.py:276] Initializing checkpoint and logger.
I0404 21:59:54.940973 139659058521920 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1 with prefix checkpoint_
I0404 21:59:54.941228 139659058521920 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0404 21:59:54.941327 139659058521920 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0404 21:59:55.838155 139659058521920 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/meta_data_0.json.
I0404 21:59:55.839085 139659058521920 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/flags_0.json.
I0404 21:59:55.842156 139659058521920 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.113041408
I0404 21:59:55.842355 139659058521920 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.113041408
I0404 21:59:55.842422 139659058521920 submission_runner.py:313] Starting training loop.
I0404 21:59:57.516593 139659058521920 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.265674752
I0404 22:00:14.181481 139482837800704 logging_writer.py:48] [0] global_step=0, grad_norm=3.3233747482299805, loss=0.7680476903915405
I0404 22:00:14.189844 139659058521920 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 10.804473856
I0404 22:00:14.190064 139659058521920 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 10.804473856
I0404 22:00:14.190140 139659058521920 spec.py:298] Evaluating on the training split.
I0404 22:00:14.197670 139659058521920 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 22:00:14.201288 139659058521920 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 22:00:14.256462 139659058521920 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0404 22:00:30.296272 139659058521920 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0404 22:01:42.735024 139659058521920 spec.py:310] Evaluating on the validation split.
I0404 22:01:42.738221 139659058521920 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 22:01:42.742316 139659058521920 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 22:01:42.797563 139659058521920 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 22:02:45.881144 139659058521920 spec.py:326] Evaluating on the test split.
I0404 22:02:45.883869 139659058521920 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 22:02:45.887682 139659058521920 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 22:02:45.939898 139659058521920 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 22:03:48.212059 139659058521920 submission_runner.py:382] Time since start: 18.35s, 	Step: 1, 	{'train/accuracy': 0.4714087247848511, 'train/loss': 0.7688907980918884, 'train/mean_average_precision': 0.025189926403083752, 'validation/accuracy': 0.47185733914375305, 'validation/loss': 0.7654543519020081, 'validation/mean_average_precision': 0.028851278779153285, 'validation/num_examples': 43793, 'test/accuracy': 0.4718925952911377, 'test/loss': 0.7646992206573486, 'test/mean_average_precision': 0.029731542389477398, 'test/num_examples': 43793}
I0404 22:03:48.212525 139659058521920 submission_runner.py:396] After eval at step 1: RAM USED (GB) 12.1785344
I0404 22:03:48.219260 139472997533440 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=18.273342, test/accuracy=0.471893, test/loss=0.764699, test/mean_average_precision=0.029732, test/num_examples=43793, total_duration=18.347692, train/accuracy=0.471409, train/loss=0.768891, train/mean_average_precision=0.025190, validation/accuracy=0.471857, validation/loss=0.765454, validation/mean_average_precision=0.028851, validation/num_examples=43793
I0404 22:03:48.243934 139659058521920 checkpoints.py:356] Saving checkpoint at step: 1
I0404 22:03:48.315382 139659058521920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_1
I0404 22:03:48.315607 139659058521920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_1.
I0404 22:03:48.316236 139659058521920 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 12.191305728
I0404 22:03:48.529366 139659058521920 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 12.228988928
I0404 22:03:48.540777 139659058521920 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 12.229496832
I0404 22:04:10.332011 139473005926144 logging_writer.py:48] [100] global_step=100, grad_norm=0.06509798020124435, loss=0.0886111855506897
I0404 22:04:32.443974 139474197542656 logging_writer.py:48] [200] global_step=200, grad_norm=0.015475020743906498, loss=0.05683910846710205
I0404 22:04:54.425428 139473005926144 logging_writer.py:48] [300] global_step=300, grad_norm=0.014870328828692436, loss=0.05956284701824188
I0404 22:05:16.303607 139474197542656 logging_writer.py:48] [400] global_step=400, grad_norm=0.010719376616179943, loss=0.05284376069903374
I0404 22:05:37.958567 139473005926144 logging_writer.py:48] [500] global_step=500, grad_norm=0.010113751515746117, loss=0.054057519882917404
I0404 22:05:59.950145 139474197542656 logging_writer.py:48] [600] global_step=600, grad_norm=0.012384681962430477, loss=0.051279064267873764
I0404 22:06:22.312623 139473005926144 logging_writer.py:48] [700] global_step=700, grad_norm=0.010984880849719048, loss=0.057388804852962494
I0404 22:06:44.322595 139474197542656 logging_writer.py:48] [800] global_step=800, grad_norm=0.007770664524286985, loss=0.05453940108418465
I0404 22:07:06.244032 139473005926144 logging_writer.py:48] [900] global_step=900, grad_norm=0.019800551235675812, loss=0.05368277058005333
I0404 22:07:28.062461 139474197542656 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.017311450093984604, loss=0.04981164634227753
I0404 22:07:48.395388 139659058521920 submission_runner.py:373] Before eval at step 1093: RAM USED (GB) 13.029875712
I0404 22:07:48.395567 139659058521920 spec.py:298] Evaluating on the training split.
I0404 22:09:00.518375 139659058521920 spec.py:310] Evaluating on the validation split.
I0404 22:09:03.073319 139659058521920 spec.py:326] Evaluating on the test split.
I0404 22:09:05.528584 139659058521920 submission_runner.py:382] Time since start: 472.55s, 	Step: 1093, 	{'train/accuracy': 0.9867286086082458, 'train/loss': 0.05484534800052643, 'train/mean_average_precision': 0.032540673104441885, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06503599882125854, 'validation/mean_average_precision': 0.036524141261199364, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06840448081493378, 'test/mean_average_precision': 0.03813010897216277, 'test/num_examples': 43793}
I0404 22:09:05.528998 139659058521920 submission_runner.py:396] After eval at step 1093: RAM USED (GB) 13.51479296
I0404 22:09:05.535779 139473005926144 logging_writer.py:48] [1093] global_step=1093, preemption_count=0, score=257.360853, test/accuracy=0.983142, test/loss=0.068404, test/mean_average_precision=0.038130, test/num_examples=43793, total_duration=472.552645, train/accuracy=0.986729, train/loss=0.054845, train/mean_average_precision=0.032541, validation/accuracy=0.984118, validation/loss=0.065036, validation/mean_average_precision=0.036524, validation/num_examples=43793
I0404 22:09:05.560881 139659058521920 checkpoints.py:356] Saving checkpoint at step: 1093
I0404 22:09:05.632033 139659058521920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_1093
I0404 22:09:05.632230 139659058521920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_1093.
I0404 22:09:05.632829 139659058521920 submission_runner.py:416] After logging and checkpointing eval at step 1093: RAM USED (GB) 13.517058048
I0404 22:09:07.404480 139474197542656 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.03890279680490494, loss=0.05052599683403969
I0404 22:09:29.445423 139474172364544 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.02994430437684059, loss=0.053424786776304245
I0404 22:09:51.566169 139474197542656 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.05589601397514343, loss=0.054110538214445114
I0404 22:10:13.766609 139474172364544 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.04767395183444023, loss=0.05404934287071228
I0404 22:10:35.450876 139474197542656 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.048199836164712906, loss=0.05710103362798691
I0404 22:10:56.882010 139474172364544 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.05457533523440361, loss=0.05226696655154228
I0404 22:11:18.624178 139474197542656 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.028349172323942184, loss=0.05325040593743324
I0404 22:11:40.471682 139474172364544 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.06141214817762375, loss=0.05701792612671852
I0404 22:12:02.344689 139474197542656 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.04378031939268112, loss=0.052390165627002716
I0404 22:12:24.131502 139474172364544 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.03933561593294144, loss=0.0551486499607563
I0404 22:12:46.108155 139474197542656 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.024452848359942436, loss=0.05069880187511444
I0404 22:13:05.806684 139659058521920 submission_runner.py:373] Before eval at step 2190: RAM USED (GB) 13.870272512
I0404 22:13:05.806896 139659058521920 spec.py:298] Evaluating on the training split.
I0404 22:14:19.469033 139659058521920 spec.py:310] Evaluating on the validation split.
I0404 22:14:22.012720 139659058521920 spec.py:326] Evaluating on the test split.
I0404 22:14:24.446909 139659058521920 submission_runner.py:382] Time since start: 789.96s, 	Step: 2190, 	{'train/accuracy': 0.9867268204689026, 'train/loss': 0.05134933441877365, 'train/mean_average_precision': 0.06193919000135435, 'validation/accuracy': 0.984148383140564, 'validation/loss': 0.06069037318229675, 'validation/mean_average_precision': 0.05597393477972391, 'validation/num_examples': 43793, 'test/accuracy': 0.9831804037094116, 'test/loss': 0.06390812247991562, 'test/mean_average_precision': 0.057653108600610355, 'test/num_examples': 43793}
I0404 22:14:24.447332 139659058521920 submission_runner.py:396] After eval at step 2190: RAM USED (GB) 14.18399744
I0404 22:14:24.454271 139474172364544 logging_writer.py:48] [2190] global_step=2190, preemption_count=0, score=496.544000, test/accuracy=0.983180, test/loss=0.063908, test/mean_average_precision=0.057653, test/num_examples=43793, total_duration=789.963946, train/accuracy=0.986727, train/loss=0.051349, train/mean_average_precision=0.061939, validation/accuracy=0.984148, validation/loss=0.060690, validation/mean_average_precision=0.055974, validation/num_examples=43793
I0404 22:14:24.478776 139659058521920 checkpoints.py:356] Saving checkpoint at step: 2190
I0404 22:14:24.534972 139659058521920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_2190
I0404 22:14:24.535162 139659058521920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_2190.
I0404 22:14:24.535783 139659058521920 submission_runner.py:416] After logging and checkpointing eval at step 2190: RAM USED (GB) 14.182924288
I0404 22:14:26.962333 139474197542656 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.04361080005764961, loss=0.04691436514258385
I0404 22:14:49.003386 139474147186432 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.08590067923069, loss=0.055090613663196564
I0404 22:15:11.200001 139474197542656 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.046383336186409, loss=0.04927203804254532
I0404 22:15:33.863491 139474147186432 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.03092639520764351, loss=0.0515432208776474
I0404 22:15:56.304992 139474197542656 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.05919240415096283, loss=0.05297530069947243
I0404 22:16:18.822885 139474147186432 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.028002288192510605, loss=0.04971493408083916
I0404 22:16:40.873912 139474197542656 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.05963611230254173, loss=0.053211525082588196
I0404 22:17:02.851660 139474147186432 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0939837247133255, loss=0.04950672388076782
I0404 22:17:25.051942 139474197542656 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.04858461022377014, loss=0.052520036697387695
I0404 22:17:47.501830 139474147186432 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.1473749577999115, loss=0.048658400774002075
I0404 22:18:09.930221 139474197542656 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.060482438653707504, loss=0.05159846320748329
I0404 22:18:24.689732 139659058521920 submission_runner.py:373] Before eval at step 3267: RAM USED (GB) 14.459342848
I0404 22:18:24.689912 139659058521920 spec.py:298] Evaluating on the training split.
I0404 22:19:37.342372 139659058521920 spec.py:310] Evaluating on the validation split.
I0404 22:19:39.909753 139659058521920 spec.py:326] Evaluating on the test split.
I0404 22:19:42.455535 139659058521920 submission_runner.py:382] Time since start: 1108.85s, 	Step: 3267, 	{'train/accuracy': 0.9869576096534729, 'train/loss': 0.048612337559461594, 'train/mean_average_precision': 0.09376641176566736, 'validation/accuracy': 0.9842214584350586, 'validation/loss': 0.05885200947523117, 'validation/mean_average_precision': 0.08828153175612286, 'validation/num_examples': 43793, 'test/accuracy': 0.9832250475883484, 'test/loss': 0.062140174210071564, 'test/mean_average_precision': 0.08793391525054774, 'test/num_examples': 43793}
I0404 22:19:42.456016 139659058521920 submission_runner.py:396] After eval at step 3267: RAM USED (GB) 14.64518656
I0404 22:19:42.463260 139474147186432 logging_writer.py:48] [3267] global_step=3267, preemption_count=0, score=735.682196, test/accuracy=0.983225, test/loss=0.062140, test/mean_average_precision=0.087934, test/num_examples=43793, total_duration=1108.846962, train/accuracy=0.986958, train/loss=0.048612, train/mean_average_precision=0.093766, validation/accuracy=0.984221, validation/loss=0.058852, validation/mean_average_precision=0.088282, validation/num_examples=43793
I0404 22:19:42.490122 139659058521920 checkpoints.py:356] Saving checkpoint at step: 3267
I0404 22:19:42.560102 139659058521920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_3267
I0404 22:19:42.560307 139659058521920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_3267.
I0404 22:19:42.560881 139659058521920 submission_runner.py:416] After logging and checkpointing eval at step 3267: RAM USED (GB) 14.672109568
I0404 22:19:50.024130 139474197542656 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.05247585475444794, loss=0.05256257578730583
I0404 22:20:11.975703 139474138793728 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.04983305558562279, loss=0.04885539039969444
I0404 22:20:34.416505 139474197542656 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.11416110396385193, loss=0.05170873925089836
I0404 22:20:56.799633 139474138793728 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.08426149189472198, loss=0.053448278456926346
I0404 22:21:19.250655 139474197542656 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.04720745608210564, loss=0.04872501641511917
I0404 22:21:41.549636 139474138793728 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.09395410120487213, loss=0.05142270028591156
I0404 22:22:03.887284 139474197542656 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.047869544476270676, loss=0.04444681853055954
I0404 22:22:26.071361 139474138793728 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.039276063442230225, loss=0.04962967708706856
I0404 22:22:48.125364 139474197542656 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.04033176228404045, loss=0.04681503400206566
I0404 22:23:10.229409 139474138793728 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.08057920634746552, loss=0.04807722195982933
I0404 22:23:32.355118 139474197542656 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.07072393596172333, loss=0.04475521668791771
I0404 22:23:42.585937 139659058521920 submission_runner.py:373] Before eval at step 4347: RAM USED (GB) 14.851489792
I0404 22:23:42.586143 139659058521920 spec.py:298] Evaluating on the training split.
I0404 22:24:58.220668 139659058521920 spec.py:310] Evaluating on the validation split.
I0404 22:25:00.745843 139659058521920 spec.py:326] Evaluating on the test split.
I0404 22:25:03.210983 139659058521920 submission_runner.py:382] Time since start: 1426.74s, 	Step: 4347, 	{'train/accuracy': 0.9875346422195435, 'train/loss': 0.04537472873926163, 'train/mean_average_precision': 0.12238997237809346, 'validation/accuracy': 0.9848214387893677, 'validation/loss': 0.05417918041348457, 'validation/mean_average_precision': 0.1149660057957302, 'validation/num_examples': 43793, 'test/accuracy': 0.9838227033615112, 'test/loss': 0.056952256709337234, 'test/mean_average_precision': 0.11752612562498503, 'test/num_examples': 43793}
I0404 22:25:03.211415 139659058521920 submission_runner.py:396] After eval at step 4347: RAM USED (GB) 15.264907264
I0404 22:25:03.218442 139474138793728 logging_writer.py:48] [4347] global_step=4347, preemption_count=0, score=974.713420, test/accuracy=0.983823, test/loss=0.056952, test/mean_average_precision=0.117526, test/num_examples=43793, total_duration=1426.743114, train/accuracy=0.987535, train/loss=0.045375, train/mean_average_precision=0.122390, validation/accuracy=0.984821, validation/loss=0.054179, validation/mean_average_precision=0.114966, validation/num_examples=43793
I0404 22:25:03.242288 139659058521920 checkpoints.py:356] Saving checkpoint at step: 4347
I0404 22:25:03.295843 139659058521920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_4347
I0404 22:25:03.296022 139659058521920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_4347.
I0404 22:25:03.296564 139659058521920 submission_runner.py:416] After logging and checkpointing eval at step 4347: RAM USED (GB) 15.264014336
I0404 22:25:15.170665 139474197542656 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.07481478899717331, loss=0.04762756824493408
I0404 22:25:37.337369 139474130401024 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.04164710268378258, loss=0.04456557705998421
I0404 22:25:59.245997 139474197542656 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0928533747792244, loss=0.04719962924718857
I0404 22:26:21.231272 139474130401024 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.1466541588306427, loss=0.05726192519068718
I0404 22:26:43.264451 139474197542656 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.050188254565000534, loss=0.04689578711986542
I0404 22:27:05.689767 139474130401024 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.05435943976044655, loss=0.04677228629589081
I0404 22:27:28.240564 139474197542656 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.04758232831954956, loss=0.04468892142176628
I0404 22:27:50.257021 139474130401024 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.08725512027740479, loss=0.04764369875192642
I0404 22:28:12.097349 139474197542656 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.05281847342848778, loss=0.04742004722356796
I0404 22:28:33.726246 139474130401024 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.03922593221068382, loss=0.044007621705532074
I0404 22:28:55.825635 139474197542656 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.1067507266998291, loss=0.046814341098070145
I0404 22:29:03.428683 139659058521920 submission_runner.py:373] Before eval at step 5435: RAM USED (GB) 15.436652544
I0404 22:29:03.428882 139659058521920 spec.py:298] Evaluating on the training split.
I0404 22:30:16.696573 139659058521920 spec.py:310] Evaluating on the validation split.
I0404 22:30:19.316898 139659058521920 spec.py:326] Evaluating on the test split.
I0404 22:30:21.869801 139659058521920 submission_runner.py:382] Time since start: 1747.59s, 	Step: 5435, 	{'train/accuracy': 0.9876285791397095, 'train/loss': 0.043815892189741135, 'train/mean_average_precision': 0.1509402895507643, 'validation/accuracy': 0.9848636388778687, 'validation/loss': 0.053496405482292175, 'validation/mean_average_precision': 0.13963858950883393, 'validation/num_examples': 43793, 'test/accuracy': 0.9838610291481018, 'test/loss': 0.05655556172132492, 'test/mean_average_precision': 0.14191535209661607, 'test/num_examples': 43793}
I0404 22:30:21.870231 139659058521920 submission_runner.py:396] After eval at step 5435: RAM USED (GB) 15.858663424
I0404 22:30:21.877500 139474130401024 logging_writer.py:48] [5435] global_step=5435, preemption_count=0, score=1213.815115, test/accuracy=0.983861, test/loss=0.056556, test/mean_average_precision=0.141915, test/num_examples=43793, total_duration=1747.585909, train/accuracy=0.987629, train/loss=0.043816, train/mean_average_precision=0.150940, validation/accuracy=0.984864, validation/loss=0.053496, validation/mean_average_precision=0.139639, validation/num_examples=43793
I0404 22:30:21.903152 139659058521920 checkpoints.py:356] Saving checkpoint at step: 5435
I0404 22:30:21.955689 139659058521920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_5435
I0404 22:30:21.955862 139659058521920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_5435.
I0404 22:30:21.956446 139659058521920 submission_runner.py:416] After logging and checkpointing eval at step 5435: RAM USED (GB) 15.857557504
I0404 22:30:37.213276 139474197542656 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.05774831026792526, loss=0.04505078122019768
I0404 22:30:58.831651 139474122008320 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.12978625297546387, loss=0.04974210262298584
I0404 22:31:20.454644 139474197542656 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.08685767650604248, loss=0.04872843995690346
I0404 22:31:42.201879 139474122008320 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.049194447696208954, loss=0.045037444680929184
I0404 22:32:04.085002 139474197542656 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.03499884158372879, loss=0.04439443722367287
I0404 22:32:25.693685 139659058521920 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 15.995129856
I0404 22:32:25.693867 139659058521920 spec.py:298] Evaluating on the training split.
I0404 22:33:36.948218 139659058521920 spec.py:310] Evaluating on the validation split.
I0404 22:33:39.451795 139659058521920 spec.py:326] Evaluating on the test split.
I0404 22:33:41.888872 139659058521920 submission_runner.py:382] Time since start: 1949.85s, 	Step: 6000, 	{'train/accuracy': 0.9878443479537964, 'train/loss': 0.04256270080804825, 'train/mean_average_precision': 0.16678505982184172, 'validation/accuracy': 0.9850950241088867, 'validation/loss': 0.052495747804641724, 'validation/mean_average_precision': 0.14910489276023978, 'validation/num_examples': 43793, 'test/accuracy': 0.9841259717941284, 'test/loss': 0.055705875158309937, 'test/mean_average_precision': 0.15090507544732845, 'test/num_examples': 43793}
I0404 22:33:41.889295 139659058521920 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 16.320364544
I0404 22:33:41.896628 139474122008320 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1337.043033, test/accuracy=0.984126, test/loss=0.055706, test/mean_average_precision=0.150905, test/num_examples=43793, total_duration=1949.850929, train/accuracy=0.987844, train/loss=0.042563, train/mean_average_precision=0.166785, validation/accuracy=0.985095, validation/loss=0.052496, validation/mean_average_precision=0.149105, validation/num_examples=43793
I0404 22:33:41.922115 139659058521920 checkpoints.py:356] Saving checkpoint at step: 6000
I0404 22:33:41.977477 139659058521920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_6000
I0404 22:33:41.977659 139659058521920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_6000.
I0404 22:33:41.978197 139659058521920 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 16.319291392
I0404 22:33:41.984182 139474197542656 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1337.043033
I0404 22:33:42.002880 139659058521920 checkpoints.py:356] Saving checkpoint at step: 6000
I0404 22:33:42.105364 139659058521920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_6000
I0404 22:33:42.105572 139659058521920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/ogbg_jax/trial_1/checkpoint_6000.
I0404 22:33:42.249252 139659058521920 submission_runner.py:550] Tuning trial 1/1
I0404 22:33:42.249469 139659058521920 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0404 22:33:42.252461 139659058521920 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4714087247848511, 'train/loss': 0.7688907980918884, 'train/mean_average_precision': 0.025189926403083752, 'validation/accuracy': 0.47185733914375305, 'validation/loss': 0.7654543519020081, 'validation/mean_average_precision': 0.028851278779153285, 'validation/num_examples': 43793, 'test/accuracy': 0.4718925952911377, 'test/loss': 0.7646992206573486, 'test/mean_average_precision': 0.029731542389477398, 'test/num_examples': 43793, 'score': 18.27334237098694, 'total_duration': 18.347692251205444, 'global_step': 1, 'preemption_count': 0}), (1093, {'train/accuracy': 0.9867286086082458, 'train/loss': 0.05484534800052643, 'train/mean_average_precision': 0.032540673104441885, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06503599882125854, 'validation/mean_average_precision': 0.036524141261199364, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06840448081493378, 'test/mean_average_precision': 0.03813010897216277, 'test/num_examples': 43793, 'score': 257.36085295677185, 'total_duration': 472.55264496803284, 'global_step': 1093, 'preemption_count': 0}), (2190, {'train/accuracy': 0.9867268204689026, 'train/loss': 0.05134933441877365, 'train/mean_average_precision': 0.06193919000135435, 'validation/accuracy': 0.984148383140564, 'validation/loss': 0.06069037318229675, 'validation/mean_average_precision': 0.05597393477972391, 'validation/num_examples': 43793, 'test/accuracy': 0.9831804037094116, 'test/loss': 0.06390812247991562, 'test/mean_average_precision': 0.057653108600610355, 'test/num_examples': 43793, 'score': 496.5440003871918, 'total_duration': 789.9639458656311, 'global_step': 2190, 'preemption_count': 0}), (3267, {'train/accuracy': 0.9869576096534729, 'train/loss': 0.048612337559461594, 'train/mean_average_precision': 0.09376641176566736, 'validation/accuracy': 0.9842214584350586, 'validation/loss': 0.05885200947523117, 'validation/mean_average_precision': 0.08828153175612286, 'validation/num_examples': 43793, 'test/accuracy': 0.9832250475883484, 'test/loss': 0.062140174210071564, 'test/mean_average_precision': 0.08793391525054774, 'test/num_examples': 43793, 'score': 735.6821956634521, 'total_duration': 1108.8469622135162, 'global_step': 3267, 'preemption_count': 0}), (4347, {'train/accuracy': 0.9875346422195435, 'train/loss': 0.04537472873926163, 'train/mean_average_precision': 0.12238997237809346, 'validation/accuracy': 0.9848214387893677, 'validation/loss': 0.05417918041348457, 'validation/mean_average_precision': 0.1149660057957302, 'validation/num_examples': 43793, 'test/accuracy': 0.9838227033615112, 'test/loss': 0.056952256709337234, 'test/mean_average_precision': 0.11752612562498503, 'test/num_examples': 43793, 'score': 974.7134199142456, 'total_duration': 1426.7431144714355, 'global_step': 4347, 'preemption_count': 0}), (5435, {'train/accuracy': 0.9876285791397095, 'train/loss': 0.043815892189741135, 'train/mean_average_precision': 0.1509402895507643, 'validation/accuracy': 0.9848636388778687, 'validation/loss': 0.053496405482292175, 'validation/mean_average_precision': 0.13963858950883393, 'validation/num_examples': 43793, 'test/accuracy': 0.9838610291481018, 'test/loss': 0.05655556172132492, 'test/mean_average_precision': 0.14191535209661607, 'test/num_examples': 43793, 'score': 1213.8151149749756, 'total_duration': 1747.585909128189, 'global_step': 5435, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9878443479537964, 'train/loss': 0.04256270080804825, 'train/mean_average_precision': 0.16678505982184172, 'validation/accuracy': 0.9850950241088867, 'validation/loss': 0.052495747804641724, 'validation/mean_average_precision': 0.14910489276023978, 'validation/num_examples': 43793, 'test/accuracy': 0.9841259717941284, 'test/loss': 0.055705875158309937, 'test/mean_average_precision': 0.15090507544732845, 'test/num_examples': 43793, 'score': 1337.043033361435, 'total_duration': 1949.8509285449982, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0404 22:33:42.252620 139659058521920 submission_runner.py:553] Timing: 1337.043033361435
I0404 22:33:42.252672 139659058521920 submission_runner.py:554] ====================
I0404 22:33:42.252787 139659058521920 submission_runner.py:613] Final ogbg score: 1337.043033361435
