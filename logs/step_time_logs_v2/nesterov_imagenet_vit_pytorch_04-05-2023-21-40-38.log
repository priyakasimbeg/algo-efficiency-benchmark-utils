WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 21:41:00.203933 140208488617792 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 21:41:00.203962 139973148800832 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 21:41:00.203987 140509406975808 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 21:41:00.204800 139938398992192 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 21:41:00.204831 140031866324800 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 21:41:00.204868 140574261905216 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 21:41:00.205166 140031866324800 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:00.205157 139938398992192 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:00.205200 140574261905216 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:00.205159 140716701861696 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 21:41:00.205215 140075594889024 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 21:41:00.205545 140716701861696 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:00.205637 140075594889024 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:00.214633 139973148800832 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:00.214658 140509406975808 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:00.214720 140208488617792 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:02.321063 140075594889024 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch.
W0405 21:41:02.388670 140574261905216 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:02.388666 140031866324800 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:02.389111 140716701861696 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:02.389887 139938398992192 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:02.389986 139973148800832 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:02.390238 140208488617792 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:02.390667 140509406975808 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:02.391545 140075594889024 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 21:41:02.395332 140075594889024 submission_runner.py:511] Using RNG seed 635443038
I0405 21:41:02.396635 140075594889024 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 21:41:02.396749 140075594889024 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1.
I0405 21:41:02.396967 140075594889024 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/hparams.json.
I0405 21:41:02.397887 140075594889024 submission_runner.py:230] Starting train once: RAM USED (GB) 5.730516992
I0405 21:41:02.397973 140075594889024 submission_runner.py:231] Initializing dataset.
I0405 21:41:06.613151 140075594889024 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 7.688732672
I0405 21:41:06.613312 140075594889024 submission_runner.py:240] Initializing model.
I0405 21:41:11.226397 140075594889024 submission_runner.py:251] After Initializing model: RAM USED (GB) 17.991716864
I0405 21:41:11.226578 140075594889024 submission_runner.py:252] Initializing optimizer.
I0405 21:41:11.778514 140075594889024 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 17.995968512
I0405 21:41:11.778694 140075594889024 submission_runner.py:261] Initializing metrics bundle.
I0405 21:41:11.778743 140075594889024 submission_runner.py:276] Initializing checkpoint and logger.
I0405 21:41:12.579150 140075594889024 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0405 21:41:12.580018 140075594889024 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/flags_0.json.
I0405 21:41:12.624732 140075594889024 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 18.048958464
I0405 21:41:12.625948 140075594889024 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.048958464
I0405 21:41:12.626072 140075594889024 submission_runner.py:313] Starting training loop.
I0405 21:41:15.180329 140075594889024 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 23.51128576
I0405 21:41:19.078529 140046711637760 logging_writer.py:48] [0] global_step=0, grad_norm=0.304528, loss=6.907754
I0405 21:41:19.105348 140075594889024 submission.py:139] 0) loss = 6.908, grad_norm = 0.305
I0405 21:41:19.106218 140075594889024 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.809257472
I0405 21:41:19.107373 140075594889024 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.811354624
I0405 21:41:19.107497 140075594889024 spec.py:298] Evaluating on the training split.
I0405 21:42:10.527186 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 21:42:56.871033 140075594889024 spec.py:326] Evaluating on the test split.
I0405 21:42:56.887551 140075594889024 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 21:42:56.893605 140075594889024 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0405 21:42:56.975239 140075594889024 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 21:43:10.225824 140075594889024 submission_runner.py:382] Time since start: 6.48s, 	Step: 1, 	{'train/accuracy': 0.00140625, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000}
I0405 21:43:10.228016 140075594889024 submission_runner.py:396] After eval at step 1: RAM USED (GB) 92.238311424
I0405 21:43:10.241212 140041703651072 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.479354, test/accuracy=0.001000, test/loss=6.907755, test/num_examples=10000, total_duration=6.481495, train/accuracy=0.001406, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0405 21:43:10.543632 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_1.
I0405 21:43:10.544347 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 92.240433152
I0405 21:43:10.566101 140075594889024 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 92.240560128
I0405 21:43:10.573754 140075594889024 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:43:10.573753 140208488617792 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:43:10.573763 140716701861696 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:43:10.573774 140509406975808 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:43:10.573785 139973148800832 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:43:10.573792 140574261905216 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:43:10.573798 140031866324800 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:43:10.573838 139938398992192 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:43:11.156584 140041695258368 logging_writer.py:48] [1] global_step=1, grad_norm=0.311200, loss=6.907754
I0405 21:43:11.160098 140075594889024 submission.py:139] 1) loss = 6.908, grad_norm = 0.311
I0405 21:43:11.161139 140075594889024 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 92.2740736
I0405 21:43:11.570131 140041703651072 logging_writer.py:48] [2] global_step=2, grad_norm=0.301331, loss=6.907755
I0405 21:43:11.575939 140075594889024 submission.py:139] 2) loss = 6.908, grad_norm = 0.301
I0405 21:43:11.966176 140041695258368 logging_writer.py:48] [3] global_step=3, grad_norm=0.305808, loss=6.907753
I0405 21:43:11.969607 140075594889024 submission.py:139] 3) loss = 6.908, grad_norm = 0.306
I0405 21:43:12.358551 140041703651072 logging_writer.py:48] [4] global_step=4, grad_norm=0.300485, loss=6.907754
I0405 21:43:12.362890 140075594889024 submission.py:139] 4) loss = 6.908, grad_norm = 0.300
I0405 21:43:12.755635 140041695258368 logging_writer.py:48] [5] global_step=5, grad_norm=0.307004, loss=6.907752
I0405 21:43:12.759808 140075594889024 submission.py:139] 5) loss = 6.908, grad_norm = 0.307
I0405 21:43:13.163614 140041703651072 logging_writer.py:48] [6] global_step=6, grad_norm=0.308962, loss=6.907749
I0405 21:43:13.168611 140075594889024 submission.py:139] 6) loss = 6.908, grad_norm = 0.309
I0405 21:43:13.566411 140041695258368 logging_writer.py:48] [7] global_step=7, grad_norm=0.313807, loss=6.907746
I0405 21:43:13.571800 140075594889024 submission.py:139] 7) loss = 6.908, grad_norm = 0.314
I0405 21:43:13.989771 140041703651072 logging_writer.py:48] [8] global_step=8, grad_norm=0.309917, loss=6.907743
I0405 21:43:13.993558 140075594889024 submission.py:139] 8) loss = 6.908, grad_norm = 0.310
I0405 21:43:14.393351 140041695258368 logging_writer.py:48] [9] global_step=9, grad_norm=0.314814, loss=6.907745
I0405 21:43:14.397568 140075594889024 submission.py:139] 9) loss = 6.908, grad_norm = 0.315
I0405 21:43:14.788532 140041703651072 logging_writer.py:48] [10] global_step=10, grad_norm=0.301095, loss=6.907735
I0405 21:43:14.793416 140075594889024 submission.py:139] 10) loss = 6.908, grad_norm = 0.301
I0405 21:43:15.194266 140041695258368 logging_writer.py:48] [11] global_step=11, grad_norm=0.307390, loss=6.907722
I0405 21:43:15.198378 140075594889024 submission.py:139] 11) loss = 6.908, grad_norm = 0.307
I0405 21:43:15.590918 140041703651072 logging_writer.py:48] [12] global_step=12, grad_norm=0.306951, loss=6.907721
I0405 21:43:15.595702 140075594889024 submission.py:139] 12) loss = 6.908, grad_norm = 0.307
I0405 21:43:15.987262 140041695258368 logging_writer.py:48] [13] global_step=13, grad_norm=0.308847, loss=6.907722
I0405 21:43:15.994183 140075594889024 submission.py:139] 13) loss = 6.908, grad_norm = 0.309
I0405 21:43:16.392947 140041703651072 logging_writer.py:48] [14] global_step=14, grad_norm=0.307072, loss=6.907740
I0405 21:43:16.396821 140075594889024 submission.py:139] 14) loss = 6.908, grad_norm = 0.307
I0405 21:43:16.797521 140041695258368 logging_writer.py:48] [15] global_step=15, grad_norm=0.301063, loss=6.907715
I0405 21:43:16.801459 140075594889024 submission.py:139] 15) loss = 6.908, grad_norm = 0.301
I0405 21:43:17.192947 140041703651072 logging_writer.py:48] [16] global_step=16, grad_norm=0.299787, loss=6.907700
I0405 21:43:17.197038 140075594889024 submission.py:139] 16) loss = 6.908, grad_norm = 0.300
I0405 21:43:17.597958 140041695258368 logging_writer.py:48] [17] global_step=17, grad_norm=0.297677, loss=6.907654
I0405 21:43:17.602847 140075594889024 submission.py:139] 17) loss = 6.908, grad_norm = 0.298
I0405 21:43:17.998455 140041703651072 logging_writer.py:48] [18] global_step=18, grad_norm=0.313406, loss=6.907674
I0405 21:43:18.002948 140075594889024 submission.py:139] 18) loss = 6.908, grad_norm = 0.313
I0405 21:43:18.405704 140041695258368 logging_writer.py:48] [19] global_step=19, grad_norm=0.305240, loss=6.907736
I0405 21:43:18.410131 140075594889024 submission.py:139] 19) loss = 6.908, grad_norm = 0.305
I0405 21:43:18.803275 140041703651072 logging_writer.py:48] [20] global_step=20, grad_norm=0.302892, loss=6.907587
I0405 21:43:18.806794 140075594889024 submission.py:139] 20) loss = 6.908, grad_norm = 0.303
I0405 21:43:19.197991 140041695258368 logging_writer.py:48] [21] global_step=21, grad_norm=0.300702, loss=6.907651
I0405 21:43:19.203077 140075594889024 submission.py:139] 21) loss = 6.908, grad_norm = 0.301
I0405 21:43:19.594037 140041703651072 logging_writer.py:48] [22] global_step=22, grad_norm=0.309376, loss=6.907607
I0405 21:43:19.597918 140075594889024 submission.py:139] 22) loss = 6.908, grad_norm = 0.309
I0405 21:43:19.996915 140041695258368 logging_writer.py:48] [23] global_step=23, grad_norm=0.300354, loss=6.907590
I0405 21:43:20.002075 140075594889024 submission.py:139] 23) loss = 6.908, grad_norm = 0.300
I0405 21:43:20.407055 140041703651072 logging_writer.py:48] [24] global_step=24, grad_norm=0.312931, loss=6.907564
I0405 21:43:20.411109 140075594889024 submission.py:139] 24) loss = 6.908, grad_norm = 0.313
I0405 21:43:20.808067 140041695258368 logging_writer.py:48] [25] global_step=25, grad_norm=0.307016, loss=6.907525
I0405 21:43:20.811890 140075594889024 submission.py:139] 25) loss = 6.908, grad_norm = 0.307
I0405 21:43:21.203605 140041703651072 logging_writer.py:48] [26] global_step=26, grad_norm=0.314223, loss=6.907445
I0405 21:43:21.207961 140075594889024 submission.py:139] 26) loss = 6.907, grad_norm = 0.314
I0405 21:43:21.603886 140041695258368 logging_writer.py:48] [27] global_step=27, grad_norm=0.304044, loss=6.907511
I0405 21:43:21.607495 140075594889024 submission.py:139] 27) loss = 6.908, grad_norm = 0.304
I0405 21:43:21.997339 140041703651072 logging_writer.py:48] [28] global_step=28, grad_norm=0.304600, loss=6.907494
I0405 21:43:22.001129 140075594889024 submission.py:139] 28) loss = 6.907, grad_norm = 0.305
I0405 21:43:22.395069 140041695258368 logging_writer.py:48] [29] global_step=29, grad_norm=0.303388, loss=6.907711
I0405 21:43:22.399131 140075594889024 submission.py:139] 29) loss = 6.908, grad_norm = 0.303
I0405 21:43:22.792246 140041703651072 logging_writer.py:48] [30] global_step=30, grad_norm=0.307276, loss=6.907344
I0405 21:43:22.796137 140075594889024 submission.py:139] 30) loss = 6.907, grad_norm = 0.307
I0405 21:43:23.187535 140041695258368 logging_writer.py:48] [31] global_step=31, grad_norm=0.307297, loss=6.907417
I0405 21:43:23.192186 140075594889024 submission.py:139] 31) loss = 6.907, grad_norm = 0.307
I0405 21:43:23.597884 140041703651072 logging_writer.py:48] [32] global_step=32, grad_norm=0.298687, loss=6.907610
I0405 21:43:23.602334 140075594889024 submission.py:139] 32) loss = 6.908, grad_norm = 0.299
I0405 21:43:24.005338 140041695258368 logging_writer.py:48] [33] global_step=33, grad_norm=0.309009, loss=6.907292
I0405 21:43:24.009488 140075594889024 submission.py:139] 33) loss = 6.907, grad_norm = 0.309
I0405 21:43:24.417021 140041703651072 logging_writer.py:48] [34] global_step=34, grad_norm=0.306880, loss=6.907302
I0405 21:43:24.422354 140075594889024 submission.py:139] 34) loss = 6.907, grad_norm = 0.307
I0405 21:43:24.828058 140041695258368 logging_writer.py:48] [35] global_step=35, grad_norm=0.308782, loss=6.907284
I0405 21:43:24.837743 140075594889024 submission.py:139] 35) loss = 6.907, grad_norm = 0.309
I0405 21:43:25.231289 140041703651072 logging_writer.py:48] [36] global_step=36, grad_norm=0.300864, loss=6.907306
I0405 21:43:25.234787 140075594889024 submission.py:139] 36) loss = 6.907, grad_norm = 0.301
I0405 21:43:25.628881 140041695258368 logging_writer.py:48] [37] global_step=37, grad_norm=0.302879, loss=6.907302
I0405 21:43:25.632399 140075594889024 submission.py:139] 37) loss = 6.907, grad_norm = 0.303
I0405 21:43:26.029683 140041703651072 logging_writer.py:48] [38] global_step=38, grad_norm=0.301697, loss=6.907378
I0405 21:43:26.033352 140075594889024 submission.py:139] 38) loss = 6.907, grad_norm = 0.302
I0405 21:43:26.430749 140041695258368 logging_writer.py:48] [39] global_step=39, grad_norm=0.309092, loss=6.907400
I0405 21:43:26.434681 140075594889024 submission.py:139] 39) loss = 6.907, grad_norm = 0.309
I0405 21:43:26.834732 140041703651072 logging_writer.py:48] [40] global_step=40, grad_norm=0.307849, loss=6.907055
I0405 21:43:26.838742 140075594889024 submission.py:139] 40) loss = 6.907, grad_norm = 0.308
I0405 21:43:27.241729 140041695258368 logging_writer.py:48] [41] global_step=41, grad_norm=0.305749, loss=6.907141
I0405 21:43:27.245868 140075594889024 submission.py:139] 41) loss = 6.907, grad_norm = 0.306
I0405 21:43:27.656885 140041703651072 logging_writer.py:48] [42] global_step=42, grad_norm=0.302133, loss=6.907315
I0405 21:43:27.664254 140075594889024 submission.py:139] 42) loss = 6.907, grad_norm = 0.302
I0405 21:43:28.068748 140041695258368 logging_writer.py:48] [43] global_step=43, grad_norm=0.310541, loss=6.906990
I0405 21:43:28.074491 140075594889024 submission.py:139] 43) loss = 6.907, grad_norm = 0.311
I0405 21:43:28.467494 140041703651072 logging_writer.py:48] [44] global_step=44, grad_norm=0.309681, loss=6.906977
I0405 21:43:28.471161 140075594889024 submission.py:139] 44) loss = 6.907, grad_norm = 0.310
I0405 21:43:28.862960 140041695258368 logging_writer.py:48] [45] global_step=45, grad_norm=0.296250, loss=6.906878
I0405 21:43:28.867125 140075594889024 submission.py:139] 45) loss = 6.907, grad_norm = 0.296
I0405 21:43:29.261652 140041703651072 logging_writer.py:48] [46] global_step=46, grad_norm=0.309954, loss=6.906816
I0405 21:43:29.265737 140075594889024 submission.py:139] 46) loss = 6.907, grad_norm = 0.310
I0405 21:43:29.667759 140041695258368 logging_writer.py:48] [47] global_step=47, grad_norm=0.303850, loss=6.906748
I0405 21:43:29.671813 140075594889024 submission.py:139] 47) loss = 6.907, grad_norm = 0.304
I0405 21:43:30.071238 140041703651072 logging_writer.py:48] [48] global_step=48, grad_norm=0.305802, loss=6.906699
I0405 21:43:30.075437 140075594889024 submission.py:139] 48) loss = 6.907, grad_norm = 0.306
I0405 21:43:30.478048 140041695258368 logging_writer.py:48] [49] global_step=49, grad_norm=0.308764, loss=6.907306
I0405 21:43:30.481852 140075594889024 submission.py:139] 49) loss = 6.907, grad_norm = 0.309
I0405 21:43:30.892540 140041703651072 logging_writer.py:48] [50] global_step=50, grad_norm=0.302218, loss=6.906825
I0405 21:43:30.897241 140075594889024 submission.py:139] 50) loss = 6.907, grad_norm = 0.302
I0405 21:43:31.298828 140041695258368 logging_writer.py:48] [51] global_step=51, grad_norm=0.305336, loss=6.906831
I0405 21:43:31.302673 140075594889024 submission.py:139] 51) loss = 6.907, grad_norm = 0.305
I0405 21:43:31.715036 140041703651072 logging_writer.py:48] [52] global_step=52, grad_norm=0.299257, loss=6.906611
I0405 21:43:31.719208 140075594889024 submission.py:139] 52) loss = 6.907, grad_norm = 0.299
I0405 21:43:32.127149 140041695258368 logging_writer.py:48] [53] global_step=53, grad_norm=0.297799, loss=6.906476
I0405 21:43:32.132267 140075594889024 submission.py:139] 53) loss = 6.906, grad_norm = 0.298
I0405 21:43:32.530674 140041703651072 logging_writer.py:48] [54] global_step=54, grad_norm=0.310266, loss=6.906504
I0405 21:43:32.535549 140075594889024 submission.py:139] 54) loss = 6.907, grad_norm = 0.310
I0405 21:43:32.932682 140041695258368 logging_writer.py:48] [55] global_step=55, grad_norm=0.302505, loss=6.906988
I0405 21:43:32.936509 140075594889024 submission.py:139] 55) loss = 6.907, grad_norm = 0.303
I0405 21:43:33.327692 140041703651072 logging_writer.py:48] [56] global_step=56, grad_norm=0.299087, loss=6.906600
I0405 21:43:33.332038 140075594889024 submission.py:139] 56) loss = 6.907, grad_norm = 0.299
I0405 21:43:33.732937 140041695258368 logging_writer.py:48] [57] global_step=57, grad_norm=0.308321, loss=6.907096
I0405 21:43:33.736799 140075594889024 submission.py:139] 57) loss = 6.907, grad_norm = 0.308
I0405 21:43:34.137022 140041703651072 logging_writer.py:48] [58] global_step=58, grad_norm=0.311304, loss=6.906451
I0405 21:43:34.142119 140075594889024 submission.py:139] 58) loss = 6.906, grad_norm = 0.311
I0405 21:43:34.535881 140041695258368 logging_writer.py:48] [59] global_step=59, grad_norm=0.308376, loss=6.906634
I0405 21:43:34.543720 140075594889024 submission.py:139] 59) loss = 6.907, grad_norm = 0.308
I0405 21:43:34.947052 140041703651072 logging_writer.py:48] [60] global_step=60, grad_norm=0.305077, loss=6.906211
I0405 21:43:34.950949 140075594889024 submission.py:139] 60) loss = 6.906, grad_norm = 0.305
I0405 21:43:35.356057 140041695258368 logging_writer.py:48] [61] global_step=61, grad_norm=0.302983, loss=6.905584
I0405 21:43:35.361222 140075594889024 submission.py:139] 61) loss = 6.906, grad_norm = 0.303
I0405 21:43:35.764148 140041703651072 logging_writer.py:48] [62] global_step=62, grad_norm=0.296662, loss=6.906627
I0405 21:43:35.767946 140075594889024 submission.py:139] 62) loss = 6.907, grad_norm = 0.297
I0405 21:43:36.178834 140041695258368 logging_writer.py:48] [63] global_step=63, grad_norm=0.308720, loss=6.905948
I0405 21:43:36.182629 140075594889024 submission.py:139] 63) loss = 6.906, grad_norm = 0.309
I0405 21:43:36.586886 140041703651072 logging_writer.py:48] [64] global_step=64, grad_norm=0.311474, loss=6.905806
I0405 21:43:36.592030 140075594889024 submission.py:139] 64) loss = 6.906, grad_norm = 0.311
I0405 21:43:36.991283 140041695258368 logging_writer.py:48] [65] global_step=65, grad_norm=0.299295, loss=6.906279
I0405 21:43:36.995196 140075594889024 submission.py:139] 65) loss = 6.906, grad_norm = 0.299
I0405 21:43:37.387958 140041703651072 logging_writer.py:48] [66] global_step=66, grad_norm=0.301706, loss=6.906464
I0405 21:43:37.391524 140075594889024 submission.py:139] 66) loss = 6.906, grad_norm = 0.302
I0405 21:43:37.794296 140041695258368 logging_writer.py:48] [67] global_step=67, grad_norm=0.310631, loss=6.906176
I0405 21:43:37.798152 140075594889024 submission.py:139] 67) loss = 6.906, grad_norm = 0.311
I0405 21:43:38.194606 140041703651072 logging_writer.py:48] [68] global_step=68, grad_norm=0.295871, loss=6.906391
I0405 21:43:38.198548 140075594889024 submission.py:139] 68) loss = 6.906, grad_norm = 0.296
I0405 21:43:38.601290 140041695258368 logging_writer.py:48] [69] global_step=69, grad_norm=0.298665, loss=6.905904
I0405 21:43:38.605240 140075594889024 submission.py:139] 69) loss = 6.906, grad_norm = 0.299
I0405 21:43:39.010541 140041703651072 logging_writer.py:48] [70] global_step=70, grad_norm=0.307206, loss=6.905699
I0405 21:43:39.014446 140075594889024 submission.py:139] 70) loss = 6.906, grad_norm = 0.307
I0405 21:43:39.421085 140041695258368 logging_writer.py:48] [71] global_step=71, grad_norm=0.301291, loss=6.906121
I0405 21:43:39.430351 140075594889024 submission.py:139] 71) loss = 6.906, grad_norm = 0.301
I0405 21:43:39.831462 140041703651072 logging_writer.py:48] [72] global_step=72, grad_norm=0.312643, loss=6.905253
I0405 21:43:39.836020 140075594889024 submission.py:139] 72) loss = 6.905, grad_norm = 0.313
I0405 21:43:40.240026 140041695258368 logging_writer.py:48] [73] global_step=73, grad_norm=0.310969, loss=6.905847
I0405 21:43:40.243890 140075594889024 submission.py:139] 73) loss = 6.906, grad_norm = 0.311
I0405 21:43:40.639489 140041703651072 logging_writer.py:48] [74] global_step=74, grad_norm=0.292215, loss=6.905628
I0405 21:43:40.643546 140075594889024 submission.py:139] 74) loss = 6.906, grad_norm = 0.292
I0405 21:43:41.038410 140041695258368 logging_writer.py:48] [75] global_step=75, grad_norm=0.301664, loss=6.905590
I0405 21:43:41.042277 140075594889024 submission.py:139] 75) loss = 6.906, grad_norm = 0.302
I0405 21:43:41.443412 140041703651072 logging_writer.py:48] [76] global_step=76, grad_norm=0.298947, loss=6.906265
I0405 21:43:41.447137 140075594889024 submission.py:139] 76) loss = 6.906, grad_norm = 0.299
I0405 21:43:41.848059 140041695258368 logging_writer.py:48] [77] global_step=77, grad_norm=0.305775, loss=6.905187
I0405 21:43:41.851972 140075594889024 submission.py:139] 77) loss = 6.905, grad_norm = 0.306
I0405 21:43:42.252272 140041703651072 logging_writer.py:48] [78] global_step=78, grad_norm=0.300432, loss=6.905306
I0405 21:43:42.256053 140075594889024 submission.py:139] 78) loss = 6.905, grad_norm = 0.300
I0405 21:43:42.669069 140041695258368 logging_writer.py:48] [79] global_step=79, grad_norm=0.316965, loss=6.906473
I0405 21:43:42.677289 140075594889024 submission.py:139] 79) loss = 6.906, grad_norm = 0.317
I0405 21:43:43.079684 140041703651072 logging_writer.py:48] [80] global_step=80, grad_norm=0.302160, loss=6.904864
I0405 21:43:43.084109 140075594889024 submission.py:139] 80) loss = 6.905, grad_norm = 0.302
I0405 21:43:43.485729 140041695258368 logging_writer.py:48] [81] global_step=81, grad_norm=0.301965, loss=6.906000
I0405 21:43:43.490622 140075594889024 submission.py:139] 81) loss = 6.906, grad_norm = 0.302
I0405 21:43:43.883578 140041703651072 logging_writer.py:48] [82] global_step=82, grad_norm=0.303553, loss=6.904700
I0405 21:43:43.888480 140075594889024 submission.py:139] 82) loss = 6.905, grad_norm = 0.304
I0405 21:43:44.292146 140041695258368 logging_writer.py:48] [83] global_step=83, grad_norm=0.308426, loss=6.904525
I0405 21:43:44.296201 140075594889024 submission.py:139] 83) loss = 6.905, grad_norm = 0.308
I0405 21:43:44.689168 140041703651072 logging_writer.py:48] [84] global_step=84, grad_norm=0.301578, loss=6.906243
I0405 21:43:44.693176 140075594889024 submission.py:139] 84) loss = 6.906, grad_norm = 0.302
I0405 21:43:45.089225 140041695258368 logging_writer.py:48] [85] global_step=85, grad_norm=0.302821, loss=6.905552
I0405 21:43:45.092953 140075594889024 submission.py:139] 85) loss = 6.906, grad_norm = 0.303
I0405 21:43:45.484614 140041703651072 logging_writer.py:48] [86] global_step=86, grad_norm=0.308550, loss=6.904588
I0405 21:43:45.488862 140075594889024 submission.py:139] 86) loss = 6.905, grad_norm = 0.309
I0405 21:43:45.882614 140041695258368 logging_writer.py:48] [87] global_step=87, grad_norm=0.297871, loss=6.904795
I0405 21:43:45.886535 140075594889024 submission.py:139] 87) loss = 6.905, grad_norm = 0.298
I0405 21:43:46.287828 140041703651072 logging_writer.py:48] [88] global_step=88, grad_norm=0.307604, loss=6.903543
I0405 21:43:46.291919 140075594889024 submission.py:139] 88) loss = 6.904, grad_norm = 0.308
I0405 21:43:46.697736 140041695258368 logging_writer.py:48] [89] global_step=89, grad_norm=0.308308, loss=6.903297
I0405 21:43:46.701970 140075594889024 submission.py:139] 89) loss = 6.903, grad_norm = 0.308
I0405 21:43:47.104192 140041703651072 logging_writer.py:48] [90] global_step=90, grad_norm=0.308659, loss=6.905703
I0405 21:43:47.109399 140075594889024 submission.py:139] 90) loss = 6.906, grad_norm = 0.309
I0405 21:43:47.518416 140041695258368 logging_writer.py:48] [91] global_step=91, grad_norm=0.298947, loss=6.903675
I0405 21:43:47.523077 140075594889024 submission.py:139] 91) loss = 6.904, grad_norm = 0.299
I0405 21:43:47.927503 140041703651072 logging_writer.py:48] [92] global_step=92, grad_norm=0.308335, loss=6.904025
I0405 21:43:47.932549 140075594889024 submission.py:139] 92) loss = 6.904, grad_norm = 0.308
I0405 21:43:48.335669 140041695258368 logging_writer.py:48] [93] global_step=93, grad_norm=0.303586, loss=6.905296
I0405 21:43:48.339759 140075594889024 submission.py:139] 93) loss = 6.905, grad_norm = 0.304
I0405 21:43:48.738432 140041703651072 logging_writer.py:48] [94] global_step=94, grad_norm=0.301757, loss=6.904588
I0405 21:43:48.743119 140075594889024 submission.py:139] 94) loss = 6.905, grad_norm = 0.302
I0405 21:43:49.142770 140041695258368 logging_writer.py:48] [95] global_step=95, grad_norm=0.309536, loss=6.903990
I0405 21:43:49.146416 140075594889024 submission.py:139] 95) loss = 6.904, grad_norm = 0.310
I0405 21:43:49.540507 140041703651072 logging_writer.py:48] [96] global_step=96, grad_norm=0.306134, loss=6.904157
I0405 21:43:49.544131 140075594889024 submission.py:139] 96) loss = 6.904, grad_norm = 0.306
I0405 21:43:49.936462 140041695258368 logging_writer.py:48] [97] global_step=97, grad_norm=0.310856, loss=6.902717
I0405 21:43:49.940992 140075594889024 submission.py:139] 97) loss = 6.903, grad_norm = 0.311
I0405 21:43:50.334678 140041703651072 logging_writer.py:48] [98] global_step=98, grad_norm=0.301214, loss=6.904574
I0405 21:43:50.339472 140075594889024 submission.py:139] 98) loss = 6.905, grad_norm = 0.301
I0405 21:43:50.734208 140041695258368 logging_writer.py:48] [99] global_step=99, grad_norm=0.303917, loss=6.904088
I0405 21:43:50.739766 140075594889024 submission.py:139] 99) loss = 6.904, grad_norm = 0.304
I0405 21:43:51.136441 140041703651072 logging_writer.py:48] [100] global_step=100, grad_norm=0.308834, loss=6.902553
I0405 21:43:51.140110 140075594889024 submission.py:139] 100) loss = 6.903, grad_norm = 0.309
I0405 21:46:24.928649 140041695258368 logging_writer.py:48] [500] global_step=500, grad_norm=0.797456, loss=6.760189
I0405 21:46:24.934164 140075594889024 submission.py:139] 500) loss = 6.760, grad_norm = 0.797
I0405 21:49:37.500937 140041703651072 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.759527, loss=6.479231
I0405 21:49:37.505440 140075594889024 submission.py:139] 1000) loss = 6.479, grad_norm = 0.760
I0405 21:50:10.916456 140075594889024 submission_runner.py:373] Before eval at step 1088: RAM USED (GB) 98.442084352
I0405 21:50:10.916690 140075594889024 spec.py:298] Evaluating on the training split.
I0405 21:50:55.345474 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 21:51:41.404032 140075594889024 spec.py:326] Evaluating on the test split.
I0405 21:51:42.839042 140075594889024 submission_runner.py:382] Time since start: 538.29s, 	Step: 1088, 	{'train/accuracy': 0.049921875, 'train/loss': 5.877193603515625, 'validation/accuracy': 0.04814, 'validation/loss': 5.90101875, 'validation/num_examples': 50000, 'test/accuracy': 0.0371, 'test/loss': 6.009240625, 'test/num_examples': 10000}
I0405 21:51:42.839504 140075594889024 submission_runner.py:396] After eval at step 1088: RAM USED (GB) 98.539761664
I0405 21:51:42.848402 140032425842432 logging_writer.py:48] [1088] global_step=1088, preemption_count=0, score=424.365913, test/accuracy=0.037100, test/loss=6.009241, test/num_examples=10000, total_duration=538.290334, train/accuracy=0.049922, train/loss=5.877194, validation/accuracy=0.048140, validation/loss=5.901019, validation/num_examples=50000
I0405 21:51:43.151566 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_1088.
I0405 21:51:43.152384 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 1088: RAM USED (GB) 98.53773824
I0405 21:54:24.716316 140032434235136 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.831320, loss=6.341228
I0405 21:54:24.721916 140075594889024 submission.py:139] 1500) loss = 6.341, grad_norm = 0.831
I0405 21:57:37.063149 140032425842432 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.979498, loss=6.368763
I0405 21:57:37.068366 140075594889024 submission.py:139] 2000) loss = 6.369, grad_norm = 0.979
I0405 21:58:43.317731 140075594889024 submission_runner.py:373] Before eval at step 2173: RAM USED (GB) 99.990155264
I0405 21:58:43.318014 140075594889024 spec.py:298] Evaluating on the training split.
I0405 21:59:27.395325 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 22:00:12.466759 140075594889024 spec.py:326] Evaluating on the test split.
I0405 22:00:13.883053 140075594889024 submission_runner.py:382] Time since start: 1050.69s, 	Step: 2173, 	{'train/accuracy': 0.08765625, 'train/loss': 5.354835205078125, 'validation/accuracy': 0.08188, 'validation/loss': 5.397565, 'validation/num_examples': 50000, 'test/accuracy': 0.0614, 'test/loss': 5.58733828125, 'test/num_examples': 10000}
I0405 22:00:13.883401 140075594889024 submission_runner.py:396] After eval at step 2173: RAM USED (GB) 99.897049088
I0405 22:00:13.891158 140032434235136 logging_writer.py:48] [2173] global_step=2173, preemption_count=0, score=842.042881, test/accuracy=0.061400, test/loss=5.587338, test/num_examples=10000, total_duration=1050.691445, train/accuracy=0.087656, train/loss=5.354835, validation/accuracy=0.081880, validation/loss=5.397565, validation/num_examples=50000
I0405 22:00:14.171059 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_2173.
I0405 22:00:14.171760 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 2173: RAM USED (GB) 99.89644288
I0405 22:02:22.872924 140032425842432 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.810206, loss=6.384009
I0405 22:02:22.877216 140075594889024 submission.py:139] 2500) loss = 6.384, grad_norm = 0.810
I0405 22:05:37.230266 140032434235136 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.813520, loss=6.016101
I0405 22:05:37.234963 140075594889024 submission.py:139] 3000) loss = 6.016, grad_norm = 0.814
I0405 22:07:14.181959 140075594889024 submission_runner.py:373] Before eval at step 3253: RAM USED (GB) 100.322271232
I0405 22:07:14.182177 140075594889024 spec.py:298] Evaluating on the training split.
I0405 22:07:58.146169 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 22:08:43.308824 140075594889024 spec.py:326] Evaluating on the test split.
I0405 22:08:44.721455 140075594889024 submission_runner.py:382] Time since start: 1561.56s, 	Step: 3253, 	{'train/accuracy': 0.12060546875, 'train/loss': 5.018494567871094, 'validation/accuracy': 0.11136, 'validation/loss': 5.073610625, 'validation/num_examples': 50000, 'test/accuracy': 0.0827, 'test/loss': 5.3203125, 'test/num_examples': 10000}
I0405 22:08:44.721797 140075594889024 submission_runner.py:396] After eval at step 3253: RAM USED (GB) 100.484681728
I0405 22:08:44.729667 140032425842432 logging_writer.py:48] [3253] global_step=3253, preemption_count=0, score=1259.597217, test/accuracy=0.082700, test/loss=5.320312, test/num_examples=10000, total_duration=1561.556044, train/accuracy=0.120605, train/loss=5.018495, validation/accuracy=0.111360, validation/loss=5.073611, validation/num_examples=50000
I0405 22:08:45.009225 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_3253.
I0405 22:08:45.009883 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 3253: RAM USED (GB) 100.48407552
I0405 22:10:20.107456 140032434235136 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.903588, loss=5.995402
I0405 22:10:20.111950 140075594889024 submission.py:139] 3500) loss = 5.995, grad_norm = 0.904
I0405 22:13:37.060895 140032425842432 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.746678, loss=5.928185
I0405 22:13:37.065636 140075594889024 submission.py:139] 4000) loss = 5.928, grad_norm = 0.747
I0405 22:15:45.153385 140075594889024 submission_runner.py:373] Before eval at step 4334: RAM USED (GB) 100.558036992
I0405 22:15:45.153743 140075594889024 spec.py:298] Evaluating on the training split.
I0405 22:16:30.391797 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 22:17:16.557627 140075594889024 spec.py:326] Evaluating on the test split.
I0405 22:17:17.968862 140075594889024 submission_runner.py:382] Time since start: 2072.53s, 	Step: 4334, 	{'train/accuracy': 0.1575, 'train/loss': 4.6798638916015625, 'validation/accuracy': 0.1436, 'validation/loss': 4.753645, 'validation/num_examples': 50000, 'test/accuracy': 0.1084, 'test/loss': 5.0664765625, 'test/num_examples': 10000}
I0405 22:17:17.969210 140075594889024 submission_runner.py:396] After eval at step 4334: RAM USED (GB) 100.618141696
I0405 22:17:17.977710 140032434235136 logging_writer.py:48] [4334] global_step=4334, preemption_count=0, score=1677.294174, test/accuracy=0.108400, test/loss=5.066477, test/num_examples=10000, total_duration=2072.527116, train/accuracy=0.157500, train/loss=4.679864, validation/accuracy=0.143600, validation/loss=4.753645, validation/num_examples=50000
I0405 22:17:18.259536 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_4334.
I0405 22:17:18.260239 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 4334: RAM USED (GB) 100.617019392
I0405 22:18:22.468421 140032425842432 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.739734, loss=5.849989
I0405 22:18:22.472511 140075594889024 submission.py:139] 4500) loss = 5.850, grad_norm = 0.740
I0405 22:21:36.980048 140032434235136 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.973137, loss=5.753860
I0405 22:21:36.984005 140075594889024 submission.py:139] 5000) loss = 5.754, grad_norm = 0.973
I0405 22:24:18.492810 140075594889024 submission_runner.py:373] Before eval at step 5415: RAM USED (GB) 100.710682624
I0405 22:24:18.493051 140075594889024 spec.py:298] Evaluating on the training split.
I0405 22:25:03.297790 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 22:25:49.078909 140075594889024 spec.py:326] Evaluating on the test split.
I0405 22:25:50.492063 140075594889024 submission_runner.py:382] Time since start: 2585.87s, 	Step: 5415, 	{'train/accuracy': 0.1994140625, 'train/loss': 4.316759033203125, 'validation/accuracy': 0.18184, 'validation/loss': 4.417443125, 'validation/num_examples': 50000, 'test/accuracy': 0.1424, 'test/loss': 4.760786328125, 'test/num_examples': 10000}
I0405 22:25:50.492486 140075594889024 submission_runner.py:396] After eval at step 5415: RAM USED (GB) 100.778377216
I0405 22:25:50.502947 140032425842432 logging_writer.py:48] [5415] global_step=5415, preemption_count=0, score=2095.096754, test/accuracy=0.142400, test/loss=4.760786, test/num_examples=10000, total_duration=2585.866962, train/accuracy=0.199414, train/loss=4.316759, validation/accuracy=0.181840, validation/loss=4.417443, validation/num_examples=50000
I0405 22:25:50.786068 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_5415.
I0405 22:25:50.786963 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 5415: RAM USED (GB) 100.777762816
I0405 22:26:23.924326 140032434235136 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.714144, loss=5.792541
I0405 22:26:23.928495 140075594889024 submission.py:139] 5500) loss = 5.793, grad_norm = 0.714
I0405 22:29:36.669040 140032425842432 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.643365, loss=5.650143
I0405 22:29:36.675037 140075594889024 submission.py:139] 6000) loss = 5.650, grad_norm = 0.643
I0405 22:32:50.971037 140075594889024 submission_runner.py:373] Before eval at step 6491: RAM USED (GB) 100.667629568
I0405 22:32:50.971290 140075594889024 spec.py:298] Evaluating on the training split.
I0405 22:33:35.336583 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 22:34:20.707452 140075594889024 spec.py:326] Evaluating on the test split.
I0405 22:34:22.123675 140075594889024 submission_runner.py:382] Time since start: 3098.34s, 	Step: 6491, 	{'train/accuracy': 0.2435546875, 'train/loss': 3.9768695068359374, 'validation/accuracy': 0.22194, 'validation/loss': 4.09501625, 'validation/num_examples': 50000, 'test/accuracy': 0.1749, 'test/loss': 4.486615625, 'test/num_examples': 10000}
I0405 22:34:22.124023 140075594889024 submission_runner.py:396] After eval at step 6491: RAM USED (GB) 100.580814848
I0405 22:34:22.131961 140032434235136 logging_writer.py:48] [6491] global_step=6491, preemption_count=0, score=2512.852598, test/accuracy=0.174900, test/loss=4.486616, test/num_examples=10000, total_duration=3098.344885, train/accuracy=0.243555, train/loss=3.976870, validation/accuracy=0.221940, validation/loss=4.095016, validation/num_examples=50000
I0405 22:34:22.414960 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_6491.
I0405 22:34:22.415633 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 6491: RAM USED (GB) 100.580245504
I0405 22:34:26.256103 140032425842432 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.664023, loss=5.921908
I0405 22:34:26.260520 140075594889024 submission.py:139] 6500) loss = 5.922, grad_norm = 0.664
I0405 22:37:38.583890 140032434235136 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.666466, loss=5.590228
I0405 22:37:38.588653 140075594889024 submission.py:139] 7000) loss = 5.590, grad_norm = 0.666
I0405 22:40:54.524473 140032425842432 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.709943, loss=5.497429
I0405 22:40:54.528967 140075594889024 submission.py:139] 7500) loss = 5.497, grad_norm = 0.710
I0405 22:41:22.626374 140075594889024 submission_runner.py:373] Before eval at step 7568: RAM USED (GB) 100.677300224
I0405 22:41:22.626607 140075594889024 spec.py:298] Evaluating on the training split.
I0405 22:42:08.201117 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 22:42:54.571944 140075594889024 spec.py:326] Evaluating on the test split.
I0405 22:42:55.979542 140075594889024 submission_runner.py:382] Time since start: 3610.00s, 	Step: 7568, 	{'train/accuracy': 0.280078125, 'train/loss': 3.717835693359375, 'validation/accuracy': 0.25688, 'validation/loss': 3.8475146875, 'validation/num_examples': 50000, 'test/accuracy': 0.1934, 'test/loss': 4.26933828125, 'test/num_examples': 10000}
I0405 22:42:55.979882 140075594889024 submission_runner.py:396] After eval at step 7568: RAM USED (GB) 100.71642112
I0405 22:42:55.988551 140032434235136 logging_writer.py:48] [7568] global_step=7568, preemption_count=0, score=2930.641352, test/accuracy=0.193400, test/loss=4.269338, test/num_examples=10000, total_duration=3610.000331, train/accuracy=0.280078, train/loss=3.717836, validation/accuracy=0.256880, validation/loss=3.847515, validation/num_examples=50000
I0405 22:42:56.285948 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_7568.
I0405 22:42:56.286712 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 7568: RAM USED (GB) 100.71580672
I0405 22:45:43.310031 140032425842432 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.605281, loss=5.621968
I0405 22:45:43.314341 140075594889024 submission.py:139] 8000) loss = 5.622, grad_norm = 0.605
I0405 22:48:56.097975 140032434235136 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.671140, loss=5.260507
I0405 22:48:56.103324 140075594889024 submission.py:139] 8500) loss = 5.261, grad_norm = 0.671
I0405 22:49:56.492085 140075594889024 submission_runner.py:373] Before eval at step 8655: RAM USED (GB) 100.529106944
I0405 22:49:56.492321 140075594889024 spec.py:298] Evaluating on the training split.
I0405 22:50:41.408964 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 22:51:29.540568 140075594889024 spec.py:326] Evaluating on the test split.
I0405 22:51:30.952444 140075594889024 submission_runner.py:382] Time since start: 4123.87s, 	Step: 8655, 	{'train/accuracy': 0.3173828125, 'train/loss': 3.466884765625, 'validation/accuracy': 0.29098, 'validation/loss': 3.6074553125, 'validation/num_examples': 50000, 'test/accuracy': 0.2276, 'test/loss': 4.05486171875, 'test/num_examples': 10000}
I0405 22:51:30.952884 140075594889024 submission_runner.py:396] After eval at step 8655: RAM USED (GB) 100.508811264
I0405 22:51:30.962752 140032425842432 logging_writer.py:48] [8655] global_step=8655, preemption_count=0, score=3348.396210, test/accuracy=0.227600, test/loss=4.054862, test/num_examples=10000, total_duration=4123.866236, train/accuracy=0.317383, train/loss=3.466885, validation/accuracy=0.290980, validation/loss=3.607455, validation/num_examples=50000
I0405 22:51:31.266069 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_8655.
I0405 22:51:31.266808 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 8655: RAM USED (GB) 100.5094912
I0405 22:53:47.489531 140032434235136 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.559084, loss=5.500987
I0405 22:53:47.493736 140075594889024 submission.py:139] 9000) loss = 5.501, grad_norm = 0.559
I0405 22:56:59.938351 140032425842432 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.710109, loss=5.102293
I0405 22:56:59.942778 140075594889024 submission.py:139] 9500) loss = 5.102, grad_norm = 0.710
I0405 22:58:31.376628 140075594889024 submission_runner.py:373] Before eval at step 9738: RAM USED (GB) 100.555030528
I0405 22:58:31.376929 140075594889024 spec.py:298] Evaluating on the training split.
I0405 22:59:17.341197 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 23:00:07.289272 140075594889024 spec.py:326] Evaluating on the test split.
I0405 23:00:08.703003 140075594889024 submission_runner.py:382] Time since start: 4638.75s, 	Step: 9738, 	{'train/accuracy': 0.352890625, 'train/loss': 3.244925231933594, 'validation/accuracy': 0.32488, 'validation/loss': 3.3906175, 'validation/num_examples': 50000, 'test/accuracy': 0.2569, 'test/loss': 3.8719359375, 'test/num_examples': 10000}
I0405 23:00:08.703343 140075594889024 submission_runner.py:396] After eval at step 9738: RAM USED (GB) 100.658548736
I0405 23:00:08.712126 140032434235136 logging_writer.py:48] [9738] global_step=9738, preemption_count=0, score=3766.077940, test/accuracy=0.256900, test/loss=3.871936, test/num_examples=10000, total_duration=4638.750475, train/accuracy=0.352891, train/loss=3.244925, validation/accuracy=0.324880, validation/loss=3.390617, validation/num_examples=50000
I0405 23:00:08.994213 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_9738.
I0405 23:00:08.994827 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 9738: RAM USED (GB) 100.657385472
I0405 23:01:53.195773 140032425842432 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.560029, loss=5.158124
I0405 23:01:53.199916 140075594889024 submission.py:139] 10000) loss = 5.158, grad_norm = 0.560
I0405 23:05:07.624227 140032434235136 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.620765, loss=5.289872
I0405 23:05:07.629789 140075594889024 submission.py:139] 10500) loss = 5.290, grad_norm = 0.621
I0405 23:07:09.246766 140075594889024 submission_runner.py:373] Before eval at step 10817: RAM USED (GB) 100.49024
I0405 23:07:09.247097 140075594889024 spec.py:298] Evaluating on the training split.
I0405 23:07:54.760035 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 23:08:41.948170 140075594889024 spec.py:326] Evaluating on the test split.
I0405 23:08:43.361407 140075594889024 submission_runner.py:382] Time since start: 5156.62s, 	Step: 10817, 	{'train/accuracy': 0.38611328125, 'train/loss': 3.0907135009765625, 'validation/accuracy': 0.3545, 'validation/loss': 3.25206, 'validation/num_examples': 50000, 'test/accuracy': 0.2743, 'test/loss': 3.745692578125, 'test/num_examples': 10000}
I0405 23:08:43.361732 140075594889024 submission_runner.py:396] After eval at step 10817: RAM USED (GB) 100.563918848
I0405 23:08:43.369439 140032434235136 logging_writer.py:48] [10817] global_step=10817, preemption_count=0, score=4183.919849, test/accuracy=0.274300, test/loss=3.745693, test/num_examples=10000, total_duration=5156.620736, train/accuracy=0.386113, train/loss=3.090714, validation/accuracy=0.354500, validation/loss=3.252060, validation/num_examples=50000
I0405 23:08:43.650416 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_10817.
I0405 23:08:43.651036 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 10817: RAM USED (GB) 100.563050496
I0405 23:09:54.822066 140032425842432 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.588190, loss=5.150529
I0405 23:09:54.828195 140075594889024 submission.py:139] 11000) loss = 5.151, grad_norm = 0.588
I0405 23:13:12.284476 140032434235136 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.550283, loss=5.124797
I0405 23:13:12.290666 140075594889024 submission.py:139] 11500) loss = 5.125, grad_norm = 0.550
I0405 23:15:43.871857 140075594889024 submission_runner.py:373] Before eval at step 11895: RAM USED (GB) 100.616282112
I0405 23:15:43.872179 140075594889024 spec.py:298] Evaluating on the training split.
I0405 23:16:27.706913 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 23:17:13.144750 140075594889024 spec.py:326] Evaluating on the test split.
I0405 23:17:14.559223 140075594889024 submission_runner.py:382] Time since start: 5671.25s, 	Step: 11895, 	{'train/accuracy': 0.4151171875, 'train/loss': 2.9003829956054688, 'validation/accuracy': 0.37912, 'validation/loss': 3.0825946875, 'validation/num_examples': 50000, 'test/accuracy': 0.2956, 'test/loss': 3.60544140625, 'test/num_examples': 10000}
I0405 23:17:14.559567 140075594889024 submission_runner.py:396] After eval at step 11895: RAM USED (GB) 100.617244672
I0405 23:17:14.567439 140032425842432 logging_writer.py:48] [11895] global_step=11895, preemption_count=0, score=4601.732285, test/accuracy=0.295600, test/loss=3.605441, test/num_examples=10000, total_duration=5671.245554, train/accuracy=0.415117, train/loss=2.900383, validation/accuracy=0.379120, validation/loss=3.082595, validation/num_examples=50000
I0405 23:17:14.852915 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_11895.
I0405 23:17:14.853546 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 11895: RAM USED (GB) 100.616122368
I0405 23:17:55.548096 140032434235136 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.608126, loss=5.019818
I0405 23:17:55.552433 140075594889024 submission.py:139] 12000) loss = 5.020, grad_norm = 0.608
I0405 23:21:11.474316 140032425842432 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.547714, loss=5.246005
I0405 23:21:11.480506 140075594889024 submission.py:139] 12500) loss = 5.246, grad_norm = 0.548
I0405 23:24:14.975322 140075594889024 submission_runner.py:373] Before eval at step 12972: RAM USED (GB) 100.464328704
I0405 23:24:14.975618 140075594889024 spec.py:298] Evaluating on the training split.
I0405 23:24:59.735765 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 23:25:46.260184 140075594889024 spec.py:326] Evaluating on the test split.
I0405 23:25:47.668352 140075594889024 submission_runner.py:382] Time since start: 6182.35s, 	Step: 12972, 	{'train/accuracy': 0.4346875, 'train/loss': 2.779293212890625, 'validation/accuracy': 0.4002, 'validation/loss': 2.9643015625, 'validation/num_examples': 50000, 'test/accuracy': 0.3082, 'test/loss': 3.489490625, 'test/num_examples': 10000}
I0405 23:25:47.668673 140075594889024 submission_runner.py:396] After eval at step 12972: RAM USED (GB) 100.52718592
I0405 23:25:47.676518 140032434235136 logging_writer.py:48] [12972] global_step=12972, preemption_count=0, score=5019.478593, test/accuracy=0.308200, test/loss=3.489491, test/num_examples=10000, total_duration=6182.349006, train/accuracy=0.434688, train/loss=2.779293, validation/accuracy=0.400200, validation/loss=2.964302, validation/num_examples=50000
I0405 23:25:47.955035 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_12972.
I0405 23:25:47.955684 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 12972: RAM USED (GB) 100.52605952
I0405 23:25:59.307228 140032425842432 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.571829, loss=5.178753
I0405 23:25:59.310729 140075594889024 submission.py:139] 13000) loss = 5.179, grad_norm = 0.572
I0405 23:29:11.488836 140032434235136 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.568197, loss=4.841945
I0405 23:29:11.496717 140075594889024 submission.py:139] 13500) loss = 4.842, grad_norm = 0.568
I0405 23:32:29.601015 140075594889024 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 100.596076544
I0405 23:32:29.601233 140075594889024 spec.py:298] Evaluating on the training split.
I0405 23:33:13.199243 140075594889024 spec.py:310] Evaluating on the validation split.
I0405 23:33:59.494206 140075594889024 spec.py:326] Evaluating on the test split.
I0405 23:34:00.910870 140075594889024 submission_runner.py:382] Time since start: 6676.97s, 	Step: 14000, 	{'train/accuracy': 0.46126953125, 'train/loss': 2.663082275390625, 'validation/accuracy': 0.42408, 'validation/loss': 2.8437453125, 'validation/num_examples': 50000, 'test/accuracy': 0.3325, 'test/loss': 3.39037265625, 'test/num_examples': 10000}
I0405 23:34:00.911200 140075594889024 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 100.547006464
I0405 23:34:00.919039 140032425842432 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5418.825193, test/accuracy=0.332500, test/loss=3.390373, test/num_examples=10000, total_duration=6676.974924, train/accuracy=0.461270, train/loss=2.663082, validation/accuracy=0.424080, validation/loss=2.843745, validation/num_examples=50000
I0405 23:34:01.218092 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0405 23:34:01.218790 140075594889024 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.571582464
I0405 23:34:01.226992 140032434235136 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5418.825193
I0405 23:34:02.000517 140075594889024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0405 23:34:02.339462 140075594889024 submission_runner.py:550] Tuning trial 1/1
I0405 23:34:02.339678 140075594889024 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 23:34:02.340221 140075594889024 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00140625, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.4793541431427, 'total_duration': 6.481494903564453, 'global_step': 1, 'preemption_count': 0}), (1088, {'train/accuracy': 0.049921875, 'train/loss': 5.877193603515625, 'validation/accuracy': 0.04814, 'validation/loss': 5.90101875, 'validation/num_examples': 50000, 'test/accuracy': 0.0371, 'test/loss': 6.009240625, 'test/num_examples': 10000, 'score': 424.3659131526947, 'total_duration': 538.2903337478638, 'global_step': 1088, 'preemption_count': 0}), (2173, {'train/accuracy': 0.08765625, 'train/loss': 5.354835205078125, 'validation/accuracy': 0.08188, 'validation/loss': 5.397565, 'validation/num_examples': 50000, 'test/accuracy': 0.0614, 'test/loss': 5.58733828125, 'test/num_examples': 10000, 'score': 842.0428812503815, 'total_duration': 1050.691445350647, 'global_step': 2173, 'preemption_count': 0}), (3253, {'train/accuracy': 0.12060546875, 'train/loss': 5.018494567871094, 'validation/accuracy': 0.11136, 'validation/loss': 5.073610625, 'validation/num_examples': 50000, 'test/accuracy': 0.0827, 'test/loss': 5.3203125, 'test/num_examples': 10000, 'score': 1259.5972170829773, 'total_duration': 1561.556043624878, 'global_step': 3253, 'preemption_count': 0}), (4334, {'train/accuracy': 0.1575, 'train/loss': 4.6798638916015625, 'validation/accuracy': 0.1436, 'validation/loss': 4.753645, 'validation/num_examples': 50000, 'test/accuracy': 0.1084, 'test/loss': 5.0664765625, 'test/num_examples': 10000, 'score': 1677.2941737174988, 'total_duration': 2072.5271158218384, 'global_step': 4334, 'preemption_count': 0}), (5415, {'train/accuracy': 0.1994140625, 'train/loss': 4.316759033203125, 'validation/accuracy': 0.18184, 'validation/loss': 4.417443125, 'validation/num_examples': 50000, 'test/accuracy': 0.1424, 'test/loss': 4.760786328125, 'test/num_examples': 10000, 'score': 2095.096753835678, 'total_duration': 2585.8669617176056, 'global_step': 5415, 'preemption_count': 0}), (6491, {'train/accuracy': 0.2435546875, 'train/loss': 3.9768695068359374, 'validation/accuracy': 0.22194, 'validation/loss': 4.09501625, 'validation/num_examples': 50000, 'test/accuracy': 0.1749, 'test/loss': 4.486615625, 'test/num_examples': 10000, 'score': 2512.8525977134705, 'total_duration': 3098.344884634018, 'global_step': 6491, 'preemption_count': 0}), (7568, {'train/accuracy': 0.280078125, 'train/loss': 3.717835693359375, 'validation/accuracy': 0.25688, 'validation/loss': 3.8475146875, 'validation/num_examples': 50000, 'test/accuracy': 0.1934, 'test/loss': 4.26933828125, 'test/num_examples': 10000, 'score': 2930.641351699829, 'total_duration': 3610.000330686569, 'global_step': 7568, 'preemption_count': 0}), (8655, {'train/accuracy': 0.3173828125, 'train/loss': 3.466884765625, 'validation/accuracy': 0.29098, 'validation/loss': 3.6074553125, 'validation/num_examples': 50000, 'test/accuracy': 0.2276, 'test/loss': 4.05486171875, 'test/num_examples': 10000, 'score': 3348.396210193634, 'total_duration': 4123.866235733032, 'global_step': 8655, 'preemption_count': 0}), (9738, {'train/accuracy': 0.352890625, 'train/loss': 3.244925231933594, 'validation/accuracy': 0.32488, 'validation/loss': 3.3906175, 'validation/num_examples': 50000, 'test/accuracy': 0.2569, 'test/loss': 3.8719359375, 'test/num_examples': 10000, 'score': 3766.0779399871826, 'total_duration': 4638.75047492981, 'global_step': 9738, 'preemption_count': 0}), (10817, {'train/accuracy': 0.38611328125, 'train/loss': 3.0907135009765625, 'validation/accuracy': 0.3545, 'validation/loss': 3.25206, 'validation/num_examples': 50000, 'test/accuracy': 0.2743, 'test/loss': 3.745692578125, 'test/num_examples': 10000, 'score': 4183.919848918915, 'total_duration': 5156.62073636055, 'global_step': 10817, 'preemption_count': 0}), (11895, {'train/accuracy': 0.4151171875, 'train/loss': 2.9003829956054688, 'validation/accuracy': 0.37912, 'validation/loss': 3.0825946875, 'validation/num_examples': 50000, 'test/accuracy': 0.2956, 'test/loss': 3.60544140625, 'test/num_examples': 10000, 'score': 4601.732284545898, 'total_duration': 5671.245553731918, 'global_step': 11895, 'preemption_count': 0}), (12972, {'train/accuracy': 0.4346875, 'train/loss': 2.779293212890625, 'validation/accuracy': 0.4002, 'validation/loss': 2.9643015625, 'validation/num_examples': 50000, 'test/accuracy': 0.3082, 'test/loss': 3.489490625, 'test/num_examples': 10000, 'score': 5019.478593111038, 'total_duration': 6182.349006414413, 'global_step': 12972, 'preemption_count': 0}), (14000, {'train/accuracy': 0.46126953125, 'train/loss': 2.663082275390625, 'validation/accuracy': 0.42408, 'validation/loss': 2.8437453125, 'validation/num_examples': 50000, 'test/accuracy': 0.3325, 'test/loss': 3.39037265625, 'test/num_examples': 10000, 'score': 5418.825193166733, 'total_duration': 6676.974924087524, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0405 23:34:02.340325 140075594889024 submission_runner.py:553] Timing: 5418.825193166733
I0405 23:34:02.340377 140075594889024 submission_runner.py:554] ====================
I0405 23:34:02.340503 140075594889024 submission_runner.py:613] Final imagenet_vit score: 5418.825193166733
