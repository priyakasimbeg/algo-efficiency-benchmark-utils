I0404 19:49:16.851043 139817482643264 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax.
I0404 19:49:16.904369 139817482643264 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 19:49:17.830468 139817482643264 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0404 19:49:17.831146 139817482643264 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 19:49:17.839559 139817482643264 submission_runner.py:511] Using RNG seed 676222596
I0404 19:49:20.144278 139817482643264 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 19:49:20.144498 139817482643264 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1.
I0404 19:49:20.144662 139817482643264 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/hparams.json.
I0404 19:49:20.266658 139817482643264 submission_runner.py:230] Starting train once: RAM USED (GB) 4.163207168
I0404 19:49:20.266839 139817482643264 submission_runner.py:231] Initializing dataset.
I0404 19:49:20.278379 139817482643264 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:49:20.286084 139817482643264 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:49:20.286210 139817482643264 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:49:20.527805 139817482643264 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:49:26.998185 139817482643264 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.331933696
I0404 19:49:26.998403 139817482643264 submission_runner.py:240] Initializing model.
I0404 19:49:37.716911 139817482643264 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.338210816
I0404 19:49:37.717100 139817482643264 submission_runner.py:252] Initializing optimizer.
I0404 19:49:38.351881 139817482643264 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.338460672
I0404 19:49:38.352046 139817482643264 submission_runner.py:261] Initializing metrics bundle.
I0404 19:49:38.352093 139817482643264 submission_runner.py:276] Initializing checkpoint and logger.
I0404 19:49:38.352887 139817482643264 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0404 19:49:39.158522 139817482643264 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/meta_data_0.json.
I0404 19:49:39.159501 139817482643264 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/flags_0.json.
I0404 19:49:39.163565 139817482643264 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.336572416
I0404 19:49:39.163766 139817482643264 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.336572416
I0404 19:49:39.163830 139817482643264 submission_runner.py:313] Starting training loop.
I0404 19:49:42.227779 139817482643264 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 14.327861248
I0404 19:50:29.463940 139641818683136 logging_writer.py:48] [0] global_step=0, grad_norm=0.3288801312446594, loss=6.907756805419922
I0404 19:50:29.481368 139817482643264 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 46.183092224
I0404 19:50:29.481665 139817482643264 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 46.183092224
I0404 19:50:29.481753 139817482643264 spec.py:298] Evaluating on the training split.
I0404 19:50:29.488380 139817482643264 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:29.495429 139817482643264 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:50:29.495553 139817482643264 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:50:29.560975 139817482643264 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:49.387585 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 19:50:49.395516 139817482643264 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:49.412941 139817482643264 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:50:49.413224 139817482643264 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:50:49.462379 139817482643264 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:51:07.659448 139817482643264 spec.py:326] Evaluating on the test split.
I0404 19:51:07.665949 139817482643264 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 19:51:07.671351 139817482643264 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0404 19:51:07.703500 139817482643264 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 19:51:17.663186 139817482643264 submission_runner.py:382] Time since start: 50.32s, 	Step: 1, 	{'train/accuracy': 0.0010742187732830644, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000}
I0404 19:51:17.663626 139817482643264 submission_runner.py:396] After eval at step 1: RAM USED (GB) 105.78642944
I0404 19:51:17.671942 139580909012736 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=50.235558, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=50.317852, train/accuracy=0.001074, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0404 19:51:17.870070 139817482643264 checkpoints.py:356] Saving checkpoint at step: 1
I0404 19:51:18.436725 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_1
I0404 19:51:18.437670 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_1.
I0404 19:51:18.439949 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 105.961320448
I0404 19:51:18.443860 139817482643264 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 105.961320448
I0404 19:51:36.493047 139817482643264 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 105.387245568
I0404 19:52:15.275919 139636458333952 logging_writer.py:48] [100] global_step=100, grad_norm=0.40990012884140015, loss=6.8998260498046875
I0404 19:52:54.544102 139636466726656 logging_writer.py:48] [200] global_step=200, grad_norm=0.5253755450248718, loss=6.842803955078125
I0404 19:53:33.812239 139636458333952 logging_writer.py:48] [300] global_step=300, grad_norm=0.4572606682777405, loss=6.852840900421143
I0404 19:54:14.155485 139636466726656 logging_writer.py:48] [400] global_step=400, grad_norm=0.5668123960494995, loss=6.804161548614502
I0404 19:54:54.827710 139636458333952 logging_writer.py:48] [500] global_step=500, grad_norm=0.8028825521469116, loss=6.635400295257568
I0404 19:55:35.235080 139636466726656 logging_writer.py:48] [600] global_step=600, grad_norm=0.8647240996360779, loss=6.78497314453125
I0404 19:56:15.768951 139636458333952 logging_writer.py:48] [700] global_step=700, grad_norm=0.8331140875816345, loss=6.515485763549805
I0404 19:56:55.791326 139636466726656 logging_writer.py:48] [800] global_step=800, grad_norm=1.3624552488327026, loss=6.423257350921631
I0404 19:57:36.079333 139636458333952 logging_writer.py:48] [900] global_step=900, grad_norm=2.114579677581787, loss=6.358247756958008
I0404 19:58:16.423205 139636466726656 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.0095218420028687, loss=6.319119930267334
I0404 19:58:18.548546 139817482643264 submission_runner.py:373] Before eval at step 1007: RAM USED (GB) 80.094240768
I0404 19:58:18.548816 139817482643264 spec.py:298] Evaluating on the training split.
I0404 19:58:29.673043 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 19:58:36.102685 139817482643264 spec.py:326] Evaluating on the test split.
I0404 19:58:38.034171 139817482643264 submission_runner.py:382] Time since start: 519.38s, 	Step: 1007, 	{'train/accuracy': 0.030996093526482582, 'train/loss': 5.9534831047058105, 'validation/accuracy': 0.030559999868273735, 'validation/loss': 5.978891849517822, 'validation/num_examples': 50000, 'test/accuracy': 0.025200001895427704, 'test/loss': 6.07716178894043, 'test/num_examples': 10000}
I0404 19:58:38.034796 139817482643264 submission_runner.py:396] After eval at step 1007: RAM USED (GB) 82.764435456
I0404 19:58:38.054837 139581219378944 logging_writer.py:48] [1007] global_step=1007, preemption_count=0, score=462.665373, test/accuracy=0.025200, test/loss=6.077162, test/num_examples=10000, total_duration=519.380371, train/accuracy=0.030996, train/loss=5.953483, validation/accuracy=0.030560, validation/loss=5.978892, validation/num_examples=50000
I0404 19:58:38.961949 139817482643264 checkpoints.py:356] Saving checkpoint at step: 1007
I0404 19:58:41.593450 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_1007
I0404 19:58:41.607094 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_1007.
I0404 19:58:41.609516 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 1007: RAM USED (GB) 95.551926272
I0404 19:59:18.626652 139581227771648 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.1604515314102173, loss=6.300290107727051
I0404 19:59:57.984487 139640984114944 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2868717908859253, loss=6.349646091461182
I0404 20:00:37.935478 139581227771648 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0373597145080566, loss=6.3981122970581055
I0404 20:01:18.170679 139640984114944 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.1638000011444092, loss=6.129167079925537
I0404 20:01:58.386761 139581227771648 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.544874906539917, loss=6.069727897644043
I0404 20:02:38.796787 139640984114944 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.916567862033844, loss=6.4593353271484375
I0404 20:03:19.351409 139581227771648 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.4071683883666992, loss=6.036360740661621
I0404 20:03:59.406284 139640984114944 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.2787013053894043, loss=6.076651573181152
I0404 20:04:39.949871 139581227771648 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.0753453969955444, loss=5.954891204833984
I0404 20:05:20.338175 139640984114944 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.3472093343734741, loss=5.9629693031311035
I0404 20:05:41.907507 139817482643264 submission_runner.py:373] Before eval at step 2055: RAM USED (GB) 86.508490752
I0404 20:05:41.907819 139817482643264 spec.py:298] Evaluating on the training split.
I0404 20:05:53.303116 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 20:06:00.110920 139817482643264 spec.py:326] Evaluating on the test split.
I0404 20:06:01.785038 139817482643264 submission_runner.py:382] Time since start: 962.74s, 	Step: 2055, 	{'train/accuracy': 0.07628905773162842, 'train/loss': 5.268614292144775, 'validation/accuracy': 0.07277999818325043, 'validation/loss': 5.308732986450195, 'validation/num_examples': 50000, 'test/accuracy': 0.05560000240802765, 'test/loss': 5.506089687347412, 'test/num_examples': 10000}
I0404 20:06:01.785499 139817482643264 submission_runner.py:396] After eval at step 2055: RAM USED (GB) 87.680028672
I0404 20:06:01.794856 139581227771648 logging_writer.py:48] [2055] global_step=2055, preemption_count=0, score=874.986306, test/accuracy=0.055600, test/loss=5.506090, test/num_examples=10000, total_duration=962.742816, train/accuracy=0.076289, train/loss=5.268614, validation/accuracy=0.072780, validation/loss=5.308733, validation/num_examples=50000
I0404 20:06:01.931134 139817482643264 checkpoints.py:356] Saving checkpoint at step: 2055
I0404 20:06:06.850504 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_2055
I0404 20:06:06.869808 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_2055.
I0404 20:06:06.872711 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 2055: RAM USED (GB) 103.007965184
I0404 20:06:25.041716 139640984114944 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.2014777660369873, loss=5.944116592407227
I0404 20:07:04.633442 139640950544128 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.6342555284500122, loss=5.912117004394531
I0404 20:07:44.113680 139640984114944 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.707925796508789, loss=5.8317952156066895
I0404 20:08:24.392231 139640950544128 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0433675050735474, loss=6.142086982727051
I0404 20:09:04.733618 139640984114944 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.365923523902893, loss=5.7055439949035645
I0404 20:09:44.977425 139640950544128 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.0921227931976318, loss=5.683827877044678
I0404 20:10:25.227968 139640984114944 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.713355541229248, loss=6.537418365478516
I0404 20:11:05.470963 139640950544128 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9154925346374512, loss=6.603507041931152
I0404 20:11:45.803726 139640984114944 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9535991549491882, loss=6.5007734298706055
I0404 20:12:26.204303 139640950544128 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0814721584320068, loss=5.6544599533081055
I0404 20:13:06.455435 139640984114944 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.0973042249679565, loss=5.568173408508301
I0404 20:13:07.015968 139817482643264 submission_runner.py:373] Before eval at step 3103: RAM USED (GB) 93.384040448
I0404 20:13:07.016276 139817482643264 spec.py:298] Evaluating on the training split.
I0404 20:13:18.721296 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 20:13:26.169955 139817482643264 spec.py:326] Evaluating on the test split.
I0404 20:13:27.834195 139817482643264 submission_runner.py:382] Time since start: 1407.85s, 	Step: 3103, 	{'train/accuracy': 0.13789062201976776, 'train/loss': 4.718212604522705, 'validation/accuracy': 0.1292800009250641, 'validation/loss': 4.773195743560791, 'validation/num_examples': 50000, 'test/accuracy': 0.09780000150203705, 'test/loss': 5.058539867401123, 'test/num_examples': 10000}
I0404 20:13:27.834815 139817482643264 submission_runner.py:396] After eval at step 3103: RAM USED (GB) 92.973031424
I0404 20:13:27.845074 139640950544128 logging_writer.py:48] [3103] global_step=3103, preemption_count=0, score=1287.162875, test/accuracy=0.097800, test/loss=5.058540, test/num_examples=10000, total_duration=1407.848100, train/accuracy=0.137891, train/loss=4.718213, validation/accuracy=0.129280, validation/loss=4.773196, validation/num_examples=50000
I0404 20:13:27.981565 139817482643264 checkpoints.py:356] Saving checkpoint at step: 3103
I0404 20:13:31.055263 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_3103
I0404 20:13:31.078094 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_3103.
I0404 20:13:31.081380 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 3103: RAM USED (GB) 104.84156416
I0404 20:14:09.928640 139640984114944 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.030784249305725, loss=5.566213130950928
I0404 20:14:49.431849 139640870782720 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.1454012393951416, loss=5.588786602020264
I0404 20:15:29.960840 139640984114944 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.0567227602005005, loss=5.786427974700928
I0404 20:16:10.114224 139640870782720 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.2260642051696777, loss=5.358025550842285
I0404 20:16:50.449665 139640984114944 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8909388780593872, loss=6.016897678375244
I0404 20:17:30.761897 139640870782720 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.1225340366363525, loss=5.276012420654297
I0404 20:18:11.110347 139640984114944 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8963982462882996, loss=6.529210090637207
I0404 20:18:51.416566 139640870782720 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.0528879165649414, loss=5.374457836151123
I0404 20:19:31.810454 139640984114944 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.2221962213516235, loss=5.239125728607178
I0404 20:20:12.201851 139640870782720 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.1149427890777588, loss=5.544628143310547
I0404 20:20:31.212344 139817482643264 submission_runner.py:373] Before eval at step 4149: RAM USED (GB) 99.524706304
I0404 20:20:31.212650 139817482643264 spec.py:298] Evaluating on the training split.
I0404 20:20:44.101183 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 20:20:53.151257 139817482643264 spec.py:326] Evaluating on the test split.
I0404 20:20:54.805188 139817482643264 submission_runner.py:382] Time since start: 1852.04s, 	Step: 4149, 	{'train/accuracy': 0.20373046398162842, 'train/loss': 4.156890869140625, 'validation/accuracy': 0.184579998254776, 'validation/loss': 4.2703962326049805, 'validation/num_examples': 50000, 'test/accuracy': 0.13830000162124634, 'test/loss': 4.640542030334473, 'test/num_examples': 10000}
I0404 20:20:54.805726 139817482643264 submission_runner.py:396] After eval at step 4149: RAM USED (GB) 102.263934976
I0404 20:20:54.819428 139640984114944 logging_writer.py:48] [4149] global_step=4149, preemption_count=0, score=1699.170535, test/accuracy=0.138300, test/loss=4.640542, test/num_examples=10000, total_duration=1852.044866, train/accuracy=0.203730, train/loss=4.156891, validation/accuracy=0.184580, validation/loss=4.270396, validation/num_examples=50000
I0404 20:20:55.008723 139817482643264 checkpoints.py:356] Saving checkpoint at step: 4149
I0404 20:20:56.380436 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_4149
I0404 20:20:56.399233 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_4149.
I0404 20:20:56.401959 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 4149: RAM USED (GB) 108.74079232
I0404 20:21:16.947409 139640870782720 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.1142603158950806, loss=5.148047924041748
I0404 20:21:56.554152 139640862390016 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.1225286722183228, loss=5.183976650238037
I0404 20:22:37.202403 139640870782720 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8321688175201416, loss=5.876273155212402
I0404 20:23:17.842457 139640862390016 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0919077396392822, loss=5.102453708648682
I0404 20:23:58.060543 139640870782720 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.1436508893966675, loss=5.048497676849365
I0404 20:24:38.536093 139640862390016 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.3243268728256226, loss=5.364973068237305
I0404 20:25:18.830987 139640870782720 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.1895335912704468, loss=5.6566972732543945
I0404 20:25:59.315912 139640862390016 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.042559027671814, loss=5.058187961578369
I0404 20:26:39.721771 139640870782720 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.1928651332855225, loss=5.7893218994140625
I0404 20:27:20.154387 139640862390016 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8219709396362305, loss=5.957146644592285
I0404 20:27:56.499794 139817482643264 submission_runner.py:373] Before eval at step 5192: RAM USED (GB) 104.799793152
I0404 20:27:56.500156 139817482643264 spec.py:298] Evaluating on the training split.
I0404 20:28:10.781235 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 20:28:20.598542 139817482643264 spec.py:326] Evaluating on the test split.
I0404 20:28:22.245350 139817482643264 submission_runner.py:382] Time since start: 2297.34s, 	Step: 5192, 	{'train/accuracy': 0.26029297709465027, 'train/loss': 3.7300033569335938, 'validation/accuracy': 0.24211999773979187, 'validation/loss': 3.8295702934265137, 'validation/num_examples': 50000, 'test/accuracy': 0.17920000851154327, 'test/loss': 4.276892185211182, 'test/num_examples': 10000}
I0404 20:28:22.246001 139817482643264 submission_runner.py:396] After eval at step 5192: RAM USED (GB) 109.6382464
I0404 20:28:22.258262 139640870782720 logging_writer.py:48] [5192] global_step=5192, preemption_count=0, score=2110.607340, test/accuracy=0.179200, test/loss=4.276892, test/num_examples=10000, total_duration=2297.335542, train/accuracy=0.260293, train/loss=3.730003, validation/accuracy=0.242120, validation/loss=3.829570, validation/num_examples=50000
I0404 20:28:22.403747 139817482643264 checkpoints.py:356] Saving checkpoint at step: 5192
I0404 20:28:23.727125 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_5192
I0404 20:28:23.740603 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_5192.
I0404 20:28:23.743484 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 5192: RAM USED (GB) 115.695480832
I0404 20:28:27.371315 139640862390016 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.0400776863098145, loss=5.223813056945801
I0404 20:29:06.838571 139640853997312 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9024080038070679, loss=5.569357872009277
I0404 20:29:46.919469 139640862390016 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.747669517993927, loss=6.248300552368164
I0404 20:30:27.252477 139640853997312 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.0460925102233887, loss=5.014629364013672
I0404 20:31:07.669796 139640862390016 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.078677773475647, loss=4.993558883666992
I0404 20:31:48.558373 139640853997312 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.9942835569381714, loss=5.2293925285339355
I0404 20:32:28.766012 139640862390016 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.392478346824646, loss=4.882832050323486
I0404 20:33:09.279092 139640853997312 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.121533751487732, loss=4.92303466796875
I0404 20:33:49.987196 139640862390016 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.0048857927322388, loss=4.819580554962158
I0404 20:34:30.529007 139640853997312 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.917618989944458, loss=4.796358585357666
I0404 20:35:10.966250 139640862390016 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.9831658005714417, loss=4.621378421783447
I0404 20:35:24.054789 139817482643264 submission_runner.py:373] Before eval at step 6234: RAM USED (GB) 110.9317632
I0404 20:35:24.055146 139817482643264 spec.py:298] Evaluating on the training split.
I0404 20:35:38.334113 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 20:35:48.538149 139817482643264 spec.py:326] Evaluating on the test split.
I0404 20:35:50.182279 139817482643264 submission_runner.py:382] Time since start: 2744.89s, 	Step: 6234, 	{'train/accuracy': 0.3008984327316284, 'train/loss': 3.4451608657836914, 'validation/accuracy': 0.27897998690605164, 'validation/loss': 3.573329210281372, 'validation/num_examples': 50000, 'test/accuracy': 0.21070000529289246, 'test/loss': 4.047236919403076, 'test/num_examples': 10000}
I0404 20:35:50.182916 139817482643264 submission_runner.py:396] After eval at step 6234: RAM USED (GB) 115.632943104
I0404 20:35:50.196085 139640853997312 logging_writer.py:48] [6234] global_step=6234, preemption_count=0, score=2520.704590, test/accuracy=0.210700, test/loss=4.047237, test/num_examples=10000, total_duration=2744.886052, train/accuracy=0.300898, train/loss=3.445161, validation/accuracy=0.278980, validation/loss=3.573329, validation/num_examples=50000
I0404 20:35:50.406445 139817482643264 checkpoints.py:356] Saving checkpoint at step: 6234
I0404 20:35:51.767150 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_6234
I0404 20:35:51.782706 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_6234.
I0404 20:35:51.785846 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 6234: RAM USED (GB) 122.275381248
I0404 20:36:18.110147 139640862390016 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9823119044303894, loss=4.875221252441406
I0404 20:36:57.855108 139640845604608 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.895419716835022, loss=5.410133361816406
I0404 20:37:38.132395 139640862390016 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8546932339668274, loss=4.577911376953125
I0404 20:38:18.589631 139640845604608 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.0593090057373047, loss=6.334527492523193
I0404 20:38:58.974885 139640862390016 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.030175805091858, loss=4.588233947753906
I0404 20:39:39.267272 139640845604608 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.8906219005584717, loss=5.382904529571533
I0404 20:40:19.527389 139640862390016 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.0164240598678589, loss=4.717138767242432
I0404 20:40:59.771772 139640845604608 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8013485074043274, loss=5.334423065185547
I0404 20:41:40.303047 139640862390016 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.8942047953605652, loss=4.467325210571289
I0404 20:42:20.577323 139640845604608 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.9448032975196838, loss=4.649475574493408
I0404 20:42:51.816223 139817482643264 submission_runner.py:373] Before eval at step 7279: RAM USED (GB) 116.4237824
I0404 20:42:51.816528 139817482643264 spec.py:298] Evaluating on the training split.
I0404 20:43:06.654987 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 20:43:17.231539 139817482643264 spec.py:326] Evaluating on the test split.
I0404 20:43:18.887790 139817482643264 submission_runner.py:382] Time since start: 3192.65s, 	Step: 7279, 	{'train/accuracy': 0.36183592677116394, 'train/loss': 3.027066230773926, 'validation/accuracy': 0.3379199802875519, 'validation/loss': 3.17329740524292, 'validation/num_examples': 50000, 'test/accuracy': 0.2597000002861023, 'test/loss': 3.7151219844818115, 'test/num_examples': 10000}
I0404 20:43:18.888447 139817482643264 submission_runner.py:396] After eval at step 7279: RAM USED (GB) 122.824237056
I0404 20:43:18.901462 139640862390016 logging_writer.py:48] [7279] global_step=7279, preemption_count=0, score=2931.780342, test/accuracy=0.259700, test/loss=3.715122, test/num_examples=10000, total_duration=3192.650243, train/accuracy=0.361836, train/loss=3.027066, validation/accuracy=0.337920, validation/loss=3.173297, validation/num_examples=50000
I0404 20:43:19.052572 139817482643264 checkpoints.py:356] Saving checkpoint at step: 7279
I0404 20:43:20.495605 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_7279
I0404 20:43:20.517089 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_7279.
I0404 20:43:20.520349 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 7279: RAM USED (GB) 129.360334848
I0404 20:43:29.138026 139640845604608 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7666795253753662, loss=6.196189880371094
I0404 20:44:08.733786 139640837211904 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7572501301765442, loss=5.865671157836914
I0404 20:44:49.084962 139640845604608 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8561238050460815, loss=4.481498718261719
I0404 20:45:29.637921 139640837211904 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6220520734786987, loss=6.1112165451049805
I0404 20:46:09.947741 139640845604608 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.9593654870986938, loss=4.35521936416626
I0404 20:46:50.401609 139640837211904 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8631432056427002, loss=5.367020130157471
I0404 20:47:30.978483 139640845604608 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.9174348711967468, loss=4.405706405639648
I0404 20:48:11.535159 139640837211904 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8821978569030762, loss=4.391571521759033
I0404 20:48:52.711404 139640845604608 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7919168472290039, loss=6.061093330383301
I0404 20:49:33.471264 139640837211904 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7987422347068787, loss=4.390341758728027
I0404 20:50:14.232539 139640845604608 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6275826096534729, loss=5.5864338874816895
I0404 20:50:20.792281 139817482643264 submission_runner.py:373] Before eval at step 8318: RAM USED (GB) 122.2820864
I0404 20:50:20.792569 139817482643264 spec.py:298] Evaluating on the training split.
I0404 20:50:35.538672 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 20:50:46.412811 139817482643264 spec.py:326] Evaluating on the test split.
I0404 20:50:48.065776 139817482643264 submission_runner.py:382] Time since start: 3641.63s, 	Step: 8318, 	{'train/accuracy': 0.39986327290534973, 'train/loss': 2.8454415798187256, 'validation/accuracy': 0.3688199818134308, 'validation/loss': 3.016512393951416, 'validation/num_examples': 50000, 'test/accuracy': 0.28620001673698425, 'test/loss': 3.5641932487487793, 'test/num_examples': 10000}
I0404 20:50:48.066225 139817482643264 submission_runner.py:396] After eval at step 8318: RAM USED (GB) 130.551656448
I0404 20:50:48.075940 139640837211904 logging_writer.py:48] [8318] global_step=8318, preemption_count=0, score=3342.873785, test/accuracy=0.286200, test/loss=3.564193, test/num_examples=10000, total_duration=3641.627766, train/accuracy=0.399863, train/loss=2.845442, validation/accuracy=0.368820, validation/loss=3.016512, validation/num_examples=50000
I0404 20:50:48.322467 139817482643264 checkpoints.py:356] Saving checkpoint at step: 8318
I0404 20:50:49.696628 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_8318
I0404 20:50:49.714720 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_8318.
I0404 20:50:49.731294 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 8318: RAM USED (GB) 137.264001024
I0404 20:51:22.348145 139640845604608 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7179861068725586, loss=5.8473310470581055
I0404 20:52:03.021812 139640828819200 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9177266955375671, loss=4.279541015625
I0404 20:52:43.461213 139640845604608 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8251798748970032, loss=4.271294593811035
I0404 20:53:24.088669 139640828819200 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8558778762817383, loss=4.184065341949463
I0404 20:54:05.374471 139640845604608 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7760986685752869, loss=4.649318695068359
I0404 20:54:45.841058 139640828819200 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.8353940844535828, loss=4.228597640991211
I0404 20:55:26.321342 139640845604608 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.703842043876648, loss=4.896605968475342
I0404 20:56:06.992697 139640828819200 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.8246100544929504, loss=4.263759613037109
I0404 20:56:47.641334 139640845604608 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7856287956237793, loss=4.349381446838379
I0404 20:57:28.050065 139640828819200 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.9208193421363831, loss=4.156163692474365
I0404 20:57:49.927082 139817482643264 submission_runner.py:373] Before eval at step 9356: RAM USED (GB) 128.502124544
I0404 20:57:49.927370 139817482643264 spec.py:298] Evaluating on the training split.
I0404 20:58:04.406091 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 20:58:15.425371 139817482643264 spec.py:326] Evaluating on the test split.
I0404 20:58:17.075922 139817482643264 submission_runner.py:382] Time since start: 4090.76s, 	Step: 9356, 	{'train/accuracy': 0.44115233421325684, 'train/loss': 2.582890272140503, 'validation/accuracy': 0.3951199948787689, 'validation/loss': 2.8160898685455322, 'validation/num_examples': 50000, 'test/accuracy': 0.3050000071525574, 'test/loss': 3.4034509658813477, 'test/num_examples': 10000}
I0404 20:58:17.076367 139817482643264 submission_runner.py:396] After eval at step 9356: RAM USED (GB) 135.712387072
I0404 20:58:17.086321 139640845604608 logging_writer.py:48] [9356] global_step=9356, preemption_count=0, score=3753.581874, test/accuracy=0.305000, test/loss=3.403451, test/num_examples=10000, total_duration=4090.762111, train/accuracy=0.441152, train/loss=2.582890, validation/accuracy=0.395120, validation/loss=2.816090, validation/num_examples=50000
I0404 20:58:17.268722 139817482643264 checkpoints.py:356] Saving checkpoint at step: 9356
I0404 20:58:18.721574 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_9356
I0404 20:58:18.743079 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_9356.
I0404 20:58:18.746109 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 9356: RAM USED (GB) 142.451400704
I0404 20:58:36.449803 139640828819200 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.9676066040992737, loss=4.130936145782471
I0404 20:59:16.406965 139640820426496 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8185991048812866, loss=4.228870391845703
I0404 20:59:56.823137 139640828819200 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5879212021827698, loss=5.972635746002197
I0404 21:00:37.511807 139640820426496 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7759261727333069, loss=4.100846767425537
I0404 21:01:18.050648 139640828819200 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7760346531867981, loss=4.14839506149292
I0404 21:01:58.643966 139640820426496 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8145679831504822, loss=4.578314781188965
I0404 21:02:39.198088 139640828819200 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7480108737945557, loss=4.500468730926514
I0404 21:03:19.890015 139640820426496 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.8090898394584656, loss=4.057859420776367
I0404 21:04:00.234483 139640828819200 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6303573846817017, loss=5.544422149658203
I0404 21:04:40.789496 139640820426496 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.8670940399169922, loss=4.004961967468262
I0404 21:05:18.958027 139817482643264 submission_runner.py:373] Before eval at step 10396: RAM USED (GB) 134.93669888
I0404 21:05:18.958309 139817482643264 spec.py:298] Evaluating on the training split.
I0404 21:05:33.868056 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 21:05:45.375661 139817482643264 spec.py:326] Evaluating on the test split.
I0404 21:05:47.020617 139817482643264 submission_runner.py:382] Time since start: 4539.78s, 	Step: 10396, 	{'train/accuracy': 0.46162107586860657, 'train/loss': 2.4928317070007324, 'validation/accuracy': 0.42980000376701355, 'validation/loss': 2.655757427215576, 'validation/num_examples': 50000, 'test/accuracy': 0.33240002393722534, 'test/loss': 3.2552826404571533, 'test/num_examples': 10000}
I0404 21:05:47.021180 139817482643264 submission_runner.py:396] After eval at step 10396: RAM USED (GB) 142.773764096
I0404 21:05:47.041243 139640828819200 logging_writer.py:48] [10396] global_step=10396, preemption_count=0, score=4162.821450, test/accuracy=0.332400, test/loss=3.255283, test/num_examples=10000, total_duration=4539.782830, train/accuracy=0.461621, train/loss=2.492832, validation/accuracy=0.429800, validation/loss=2.655757, validation/num_examples=50000
I0404 21:05:47.239832 139817482643264 checkpoints.py:356] Saving checkpoint at step: 10396
I0404 21:05:48.686866 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_10396
I0404 21:05:48.709696 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_10396.
I0404 21:05:48.714148 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 10396: RAM USED (GB) 149.709795328
I0404 21:05:50.751233 139640820426496 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.8425300121307373, loss=4.030137062072754
I0404 21:06:30.351882 139640812033792 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.624869704246521, loss=5.88553524017334
I0404 21:07:10.807903 139640820426496 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8474401235580444, loss=5.494072437286377
I0404 21:07:50.912775 139640812033792 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.7429482936859131, loss=4.016002655029297
I0404 21:08:31.518373 139640820426496 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7601056098937988, loss=4.111420631408691
I0404 21:09:11.790327 139640812033792 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6038005948066711, loss=5.4436845779418945
I0404 21:09:52.423307 139640820426496 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6331433653831482, loss=5.267097473144531
I0404 21:10:33.494902 139640812033792 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7376607060432434, loss=4.0991740226745605
I0404 21:11:14.043728 139640820426496 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7720361948013306, loss=3.8253488540649414
I0404 21:11:54.623657 139640812033792 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6996694803237915, loss=4.381836414337158
I0404 21:12:35.341053 139640820426496 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7935562133789062, loss=4.045984745025635
I0404 21:12:48.788880 139817482643264 submission_runner.py:373] Before eval at step 11435: RAM USED (GB) 142.201040896
I0404 21:12:48.789250 139817482643264 spec.py:298] Evaluating on the training split.
I0404 21:13:03.163291 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 21:13:14.918064 139817482643264 spec.py:326] Evaluating on the test split.
I0404 21:13:16.577639 139817482643264 submission_runner.py:382] Time since start: 4989.62s, 	Step: 11435, 	{'train/accuracy': 0.4916406273841858, 'train/loss': 2.3397717475891113, 'validation/accuracy': 0.4578399956226349, 'validation/loss': 2.504760980606079, 'validation/num_examples': 50000, 'test/accuracy': 0.35360002517700195, 'test/loss': 3.117572784423828, 'test/num_examples': 10000}
I0404 21:13:16.578106 139817482643264 submission_runner.py:396] After eval at step 11435: RAM USED (GB) 146.980708352
I0404 21:13:16.588739 139640812033792 logging_writer.py:48] [11435] global_step=11435, preemption_count=0, score=4572.374096, test/accuracy=0.353600, test/loss=3.117573, test/num_examples=10000, total_duration=4989.620086, train/accuracy=0.491641, train/loss=2.339772, validation/accuracy=0.457840, validation/loss=2.504761, validation/num_examples=50000
I0404 21:13:16.816645 139817482643264 checkpoints.py:356] Saving checkpoint at step: 11435
I0404 21:13:18.253897 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_11435
I0404 21:13:18.274392 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_11435.
I0404 21:13:18.286079 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 11435: RAM USED (GB) 153.771446272
I0404 21:13:44.315158 139640820426496 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.8147726655006409, loss=3.904451847076416
I0404 21:14:24.950509 139640602347264 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.7074317932128906, loss=3.9282355308532715
I0404 21:15:05.883664 139640820426496 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.1265219449996948, loss=3.9698541164398193
I0404 21:15:46.979664 139640602347264 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.8228188753128052, loss=4.4829559326171875
I0404 21:16:27.683375 139640820426496 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7389328479766846, loss=5.494174003601074
I0404 21:17:08.562340 139640602347264 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8108470439910889, loss=3.798889636993408
I0404 21:17:49.058050 139640820426496 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5263823866844177, loss=5.7704644203186035
I0404 21:18:29.829817 139640602347264 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8507049679756165, loss=3.7025482654571533
I0404 21:19:10.455798 139640820426496 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6730871200561523, loss=5.362027168273926
I0404 21:19:51.157463 139640602347264 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6001289486885071, loss=5.69050931930542
I0404 21:20:18.716754 139817482643264 submission_runner.py:373] Before eval at step 12469: RAM USED (GB) 148.6718976
I0404 21:20:18.717039 139817482643264 spec.py:298] Evaluating on the training split.
I0404 21:20:33.092081 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 21:20:44.510768 139817482643264 spec.py:326] Evaluating on the test split.
I0404 21:20:46.159401 139817482643264 submission_runner.py:382] Time since start: 5439.55s, 	Step: 12469, 	{'train/accuracy': 0.5178124904632568, 'train/loss': 2.219208240509033, 'validation/accuracy': 0.4760800004005432, 'validation/loss': 2.4093570709228516, 'validation/num_examples': 50000, 'test/accuracy': 0.3718000054359436, 'test/loss': 3.026749849319458, 'test/num_examples': 10000}
I0404 21:20:46.159992 139817482643264 submission_runner.py:396] After eval at step 12469: RAM USED (GB) 153.593499648
I0404 21:20:46.174946 139640820426496 logging_writer.py:48] [12469] global_step=12469, preemption_count=0, score=4981.425890, test/accuracy=0.371800, test/loss=3.026750, test/num_examples=10000, total_duration=5439.548825, train/accuracy=0.517812, train/loss=2.219208, validation/accuracy=0.476080, validation/loss=2.409357, validation/num_examples=50000
I0404 21:20:46.399631 139817482643264 checkpoints.py:356] Saving checkpoint at step: 12469
I0404 21:20:47.860669 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_12469
I0404 21:20:47.882878 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_12469.
I0404 21:20:47.886199 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 12469: RAM USED (GB) 160.568365056
I0404 21:21:00.476036 139640602347264 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8358503580093384, loss=3.8436877727508545
I0404 21:21:40.558928 139640593954560 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7366411089897156, loss=3.8463919162750244
I0404 21:22:21.342015 139640602347264 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6847658157348633, loss=4.245363235473633
I0404 21:23:02.501689 139640593954560 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7200723886489868, loss=3.8617098331451416
I0404 21:23:43.395467 139640602347264 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.8121664524078369, loss=5.233786582946777
I0404 21:24:23.980510 139640593954560 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7820935845375061, loss=3.740405559539795
I0404 21:25:05.019094 139640602347264 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6960980296134949, loss=4.2330803871154785
I0404 21:25:45.871134 139640593954560 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.788966715335846, loss=3.737914800643921
I0404 21:26:26.892873 139640602347264 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6563836932182312, loss=4.7305779457092285
I0404 21:27:07.620439 139640593954560 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.9209461808204651, loss=3.683215379714966
I0404 21:27:48.280834 139640602347264 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7921372056007385, loss=3.668689489364624
I0404 21:27:48.298266 139817482643264 submission_runner.py:373] Before eval at step 13501: RAM USED (GB) 154.121150464
I0404 21:27:48.298979 139817482643264 spec.py:298] Evaluating on the training split.
I0404 21:28:02.343718 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 21:28:14.030447 139817482643264 spec.py:326] Evaluating on the test split.
I0404 21:28:15.672641 139817482643264 submission_runner.py:382] Time since start: 5889.13s, 	Step: 13501, 	{'train/accuracy': 0.5456640720367432, 'train/loss': 2.0818076133728027, 'validation/accuracy': 0.49969998002052307, 'validation/loss': 2.3035476207733154, 'validation/num_examples': 50000, 'test/accuracy': 0.39080002903938293, 'test/loss': 2.9351282119750977, 'test/num_examples': 10000}
I0404 21:28:15.673338 139817482643264 submission_runner.py:396] After eval at step 13501: RAM USED (GB) 159.589978112
I0404 21:28:15.688188 139640593954560 logging_writer.py:48] [13501] global_step=13501, preemption_count=0, score=5389.605035, test/accuracy=0.390800, test/loss=2.935128, test/num_examples=10000, total_duration=5889.130476, train/accuracy=0.545664, train/loss=2.081808, validation/accuracy=0.499700, validation/loss=2.303548, validation/num_examples=50000
I0404 21:28:15.916450 139817482643264 checkpoints.py:356] Saving checkpoint at step: 13501
I0404 21:28:17.367076 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_13501
I0404 21:28:17.388669 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_13501.
I0404 21:28:17.391223 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 13501: RAM USED (GB) 166.556577792
I0404 21:28:56.927318 139640602347264 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.766754150390625, loss=3.729556083679199
I0404 21:29:37.537167 139640585561856 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.7035126090049744, loss=4.0650634765625
I0404 21:30:18.094005 139640602347264 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6466540098190308, loss=4.756712913513184
I0404 21:30:58.732796 139640585561856 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.8441324234008789, loss=3.698413133621216
I0404 21:31:39.037181 139817482643264 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 160.469663744
I0404 21:31:39.037439 139817482643264 spec.py:298] Evaluating on the training split.
I0404 21:31:53.280539 139817482643264 spec.py:310] Evaluating on the validation split.
I0404 21:32:05.528330 139817482643264 spec.py:326] Evaluating on the test split.
I0404 21:32:07.179343 139817482643264 submission_runner.py:382] Time since start: 6119.87s, 	Step: 14000, 	{'train/accuracy': 0.5507031083106995, 'train/loss': 2.0417442321777344, 'validation/accuracy': 0.5090000033378601, 'validation/loss': 2.231877326965332, 'validation/num_examples': 50000, 'test/accuracy': 0.3928000330924988, 'test/loss': 2.869612455368042, 'test/num_examples': 10000}
I0404 21:32:07.179818 139817482643264 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 164.21009408
I0404 21:32:07.191338 139640602347264 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5585.606358, test/accuracy=0.392800, test/loss=2.869612, test/num_examples=10000, total_duration=6119.870479, train/accuracy=0.550703, train/loss=2.041744, validation/accuracy=0.509000, validation/loss=2.231877, validation/num_examples=50000
I0404 21:32:07.379636 139817482643264 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 21:32:08.844298 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_14000
I0404 21:32:08.866501 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_14000.
I0404 21:32:08.869548 139817482643264 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 171.064311808
I0404 21:32:08.890687 139640585561856 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5585.606358
I0404 21:32:09.111594 139817482643264 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 21:32:10.644155 139817482643264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_14000
I0404 21:32:10.665157 139817482643264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_vit_jax/trial_1/checkpoint_14000.
I0404 21:32:11.439869 139817482643264 submission_runner.py:550] Tuning trial 1/1
I0404 21:32:11.440878 139817482643264 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0404 21:32:11.446185 139817482643264 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010742187732830644, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 50.23555827140808, 'total_duration': 50.31785225868225, 'global_step': 1, 'preemption_count': 0}), (1007, {'train/accuracy': 0.030996093526482582, 'train/loss': 5.9534831047058105, 'validation/accuracy': 0.030559999868273735, 'validation/loss': 5.978891849517822, 'validation/num_examples': 50000, 'test/accuracy': 0.025200001895427704, 'test/loss': 6.07716178894043, 'test/num_examples': 10000, 'score': 462.66537284851074, 'total_duration': 519.3803706169128, 'global_step': 1007, 'preemption_count': 0}), (2055, {'train/accuracy': 0.07628905773162842, 'train/loss': 5.268614292144775, 'validation/accuracy': 0.07277999818325043, 'validation/loss': 5.308732986450195, 'validation/num_examples': 50000, 'test/accuracy': 0.05560000240802765, 'test/loss': 5.506089687347412, 'test/num_examples': 10000, 'score': 874.9863064289093, 'total_duration': 962.7428159713745, 'global_step': 2055, 'preemption_count': 0}), (3103, {'train/accuracy': 0.13789062201976776, 'train/loss': 4.718212604522705, 'validation/accuracy': 0.1292800009250641, 'validation/loss': 4.773195743560791, 'validation/num_examples': 50000, 'test/accuracy': 0.09780000150203705, 'test/loss': 5.058539867401123, 'test/num_examples': 10000, 'score': 1287.162875175476, 'total_duration': 1407.8480997085571, 'global_step': 3103, 'preemption_count': 0}), (4149, {'train/accuracy': 0.20373046398162842, 'train/loss': 4.156890869140625, 'validation/accuracy': 0.184579998254776, 'validation/loss': 4.2703962326049805, 'validation/num_examples': 50000, 'test/accuracy': 0.13830000162124634, 'test/loss': 4.640542030334473, 'test/num_examples': 10000, 'score': 1699.170535326004, 'total_duration': 1852.0448660850525, 'global_step': 4149, 'preemption_count': 0}), (5192, {'train/accuracy': 0.26029297709465027, 'train/loss': 3.7300033569335938, 'validation/accuracy': 0.24211999773979187, 'validation/loss': 3.8295702934265137, 'validation/num_examples': 50000, 'test/accuracy': 0.17920000851154327, 'test/loss': 4.276892185211182, 'test/num_examples': 10000, 'score': 2110.607340335846, 'total_duration': 2297.3355417251587, 'global_step': 5192, 'preemption_count': 0}), (6234, {'train/accuracy': 0.3008984327316284, 'train/loss': 3.4451608657836914, 'validation/accuracy': 0.27897998690605164, 'validation/loss': 3.573329210281372, 'validation/num_examples': 50000, 'test/accuracy': 0.21070000529289246, 'test/loss': 4.047236919403076, 'test/num_examples': 10000, 'score': 2520.7045900821686, 'total_duration': 2744.8860518932343, 'global_step': 6234, 'preemption_count': 0}), (7279, {'train/accuracy': 0.36183592677116394, 'train/loss': 3.027066230773926, 'validation/accuracy': 0.3379199802875519, 'validation/loss': 3.17329740524292, 'validation/num_examples': 50000, 'test/accuracy': 0.2597000002861023, 'test/loss': 3.7151219844818115, 'test/num_examples': 10000, 'score': 2931.7803423404694, 'total_duration': 3192.6502425670624, 'global_step': 7279, 'preemption_count': 0}), (8318, {'train/accuracy': 0.39986327290534973, 'train/loss': 2.8454415798187256, 'validation/accuracy': 0.3688199818134308, 'validation/loss': 3.016512393951416, 'validation/num_examples': 50000, 'test/accuracy': 0.28620001673698425, 'test/loss': 3.5641932487487793, 'test/num_examples': 10000, 'score': 3342.873785495758, 'total_duration': 3641.627765893936, 'global_step': 8318, 'preemption_count': 0}), (9356, {'train/accuracy': 0.44115233421325684, 'train/loss': 2.582890272140503, 'validation/accuracy': 0.3951199948787689, 'validation/loss': 2.8160898685455322, 'validation/num_examples': 50000, 'test/accuracy': 0.3050000071525574, 'test/loss': 3.4034509658813477, 'test/num_examples': 10000, 'score': 3753.581874370575, 'total_duration': 4090.7621109485626, 'global_step': 9356, 'preemption_count': 0}), (10396, {'train/accuracy': 0.46162107586860657, 'train/loss': 2.4928317070007324, 'validation/accuracy': 0.42980000376701355, 'validation/loss': 2.655757427215576, 'validation/num_examples': 50000, 'test/accuracy': 0.33240002393722534, 'test/loss': 3.2552826404571533, 'test/num_examples': 10000, 'score': 4162.821449756622, 'total_duration': 4539.782829761505, 'global_step': 10396, 'preemption_count': 0}), (11435, {'train/accuracy': 0.4916406273841858, 'train/loss': 2.3397717475891113, 'validation/accuracy': 0.4578399956226349, 'validation/loss': 2.504760980606079, 'validation/num_examples': 50000, 'test/accuracy': 0.35360002517700195, 'test/loss': 3.117572784423828, 'test/num_examples': 10000, 'score': 4572.3740956783295, 'total_duration': 4989.620085954666, 'global_step': 11435, 'preemption_count': 0}), (12469, {'train/accuracy': 0.5178124904632568, 'train/loss': 2.219208240509033, 'validation/accuracy': 0.4760800004005432, 'validation/loss': 2.4093570709228516, 'validation/num_examples': 50000, 'test/accuracy': 0.3718000054359436, 'test/loss': 3.026749849319458, 'test/num_examples': 10000, 'score': 4981.425890207291, 'total_duration': 5439.5488250255585, 'global_step': 12469, 'preemption_count': 0}), (13501, {'train/accuracy': 0.5456640720367432, 'train/loss': 2.0818076133728027, 'validation/accuracy': 0.49969998002052307, 'validation/loss': 2.3035476207733154, 'validation/num_examples': 50000, 'test/accuracy': 0.39080002903938293, 'test/loss': 2.9351282119750977, 'test/num_examples': 10000, 'score': 5389.605034828186, 'total_duration': 5889.130475759506, 'global_step': 13501, 'preemption_count': 0}), (14000, {'train/accuracy': 0.5507031083106995, 'train/loss': 2.0417442321777344, 'validation/accuracy': 0.5090000033378601, 'validation/loss': 2.231877326965332, 'validation/num_examples': 50000, 'test/accuracy': 0.3928000330924988, 'test/loss': 2.869612455368042, 'test/num_examples': 10000, 'score': 5585.606358289719, 'total_duration': 6119.8704788684845, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0404 21:32:11.446305 139817482643264 submission_runner.py:553] Timing: 5585.606358289719
I0404 21:32:11.446358 139817482643264 submission_runner.py:554] ====================
I0404 21:32:11.446473 139817482643264 submission_runner.py:613] Final imagenet_vit score: 5585.606358289719
