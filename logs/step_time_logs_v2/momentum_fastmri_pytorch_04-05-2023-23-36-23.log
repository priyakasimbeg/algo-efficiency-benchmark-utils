WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 23:36:45.752417 139690358040384 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 23:36:45.752462 140683134461760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 23:36:45.752477 140297895130944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 23:36:45.753356 140565664229184 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 23:36:45.753884 140493094577984 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 23:36:46.723149 139650895021888 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 23:36:46.723426 140155287697216 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 23:36:46.725873 140265624962880 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 23:36:46.726305 140265624962880 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:36:46.733816 139650895021888 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:36:46.734082 140155287697216 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:36:46.734620 139690358040384 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:36:46.734641 140683134461760 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:36:46.734776 140297895130944 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:36:46.734802 140565664229184 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:36:46.734858 140493094577984 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:36:47.263891 140265624962880 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch.
W0405 23:36:47.378322 139690358040384 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:36:47.379233 140565664229184 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:36:47.379380 140265624962880 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:36:47.379899 140493094577984 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:36:47.380088 139650895021888 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:36:47.380787 140297895130944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:36:47.380914 140155287697216 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:36:47.381023 140683134461760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 23:36:47.385282 140265624962880 submission_runner.py:511] Using RNG seed 1270195194
I0405 23:36:47.386433 140265624962880 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 23:36:47.386553 140265624962880 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1.
I0405 23:36:47.386778 140265624962880 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/hparams.json.
I0405 23:36:47.387785 140265624962880 submission_runner.py:230] Starting train once: RAM USED (GB) 5.586235392
I0405 23:36:47.387884 140265624962880 submission_runner.py:231] Initializing dataset.
I0405 23:36:47.388135 140265624962880 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.586857984
I0405 23:36:47.388224 140265624962880 submission_runner.py:240] Initializing model.
I0405 23:36:51.496664 140265624962880 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.233531904
I0405 23:36:51.496843 140265624962880 submission_runner.py:252] Initializing optimizer.
I0405 23:36:52.066743 140265624962880 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.237844992
I0405 23:36:52.066922 140265624962880 submission_runner.py:261] Initializing metrics bundle.
I0405 23:36:52.066972 140265624962880 submission_runner.py:276] Initializing checkpoint and logger.
I0405 23:36:52.070216 140265624962880 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 23:36:52.070343 140265624962880 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 23:36:52.673866 140265624962880 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/meta_data_0.json.
I0405 23:36:52.674791 140265624962880 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/flags_0.json.
I0405 23:36:52.718033 140265624962880 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.290535936
I0405 23:36:52.719146 140265624962880 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.290535936
I0405 23:36:52.719272 140265624962880 submission_runner.py:313] Starting training loop.
I0405 23:37:34.637359 140265624962880 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 23.8881792
I0405 23:37:38.645694 140223451223808 logging_writer.py:48] [0] global_step=0, grad_norm=3.176845, loss=0.745954
I0405 23:37:38.651326 140265624962880 submission.py:139] 0) loss = 0.746, grad_norm = 3.177
I0405 23:37:38.651827 140265624962880 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 32.466231296
I0405 23:37:38.652971 140265624962880 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 32.466231296
I0405 23:37:38.653095 140265624962880 spec.py:298] Evaluating on the training split.
I0405 23:39:19.187685 140265624962880 spec.py:310] Evaluating on the validation split.
I0405 23:40:23.213088 140265624962880 spec.py:326] Evaluating on the test split.
I0405 23:41:26.964038 140265624962880 submission_runner.py:382] Time since start: 45.93s, 	Step: 1, 	{'train/ssim': 0.27292992387499126, 'train/loss': 0.7443235261099679, 'validation/ssim': 0.26595291369539603, 'validation/loss': 0.7523124664770329, 'validation/num_examples': 3554, 'test/ssim': 0.28794061894591244, 'test/loss': 0.7553481182150587, 'test/num_examples': 3581}
I0405 23:41:26.964516 140265624962880 submission_runner.py:396] After eval at step 1: RAM USED (GB) 68.841275392
I0405 23:41:26.973523 140199493342976 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=45.931708, test/loss=0.755348, test/num_examples=3581, test/ssim=0.287941, total_duration=45.933770, train/loss=0.744324, train/ssim=0.272930, validation/loss=0.752312, validation/num_examples=3554, validation/ssim=0.265953
I0405 23:41:27.073752 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_1.
I0405 23:41:27.074220 140265624962880 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 68.84214784
I0405 23:41:27.085756 140265624962880 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 68.863307776
I0405 23:41:27.089765 140265624962880 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:27.089903 140683134461760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:27.089926 139690358040384 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:27.089953 140565664229184 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:27.089964 140155287697216 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:27.089972 140493094577984 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:27.089990 139650895021888 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:27.090079 140297895130944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:27.149757 140199409481472 logging_writer.py:48] [1] global_step=1, grad_norm=3.484553, loss=0.730082
I0405 23:41:27.153763 140265624962880 submission.py:139] 1) loss = 0.730, grad_norm = 3.485
I0405 23:41:27.154827 140265624962880 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 68.87569408
I0405 23:41:27.224658 140199493342976 logging_writer.py:48] [2] global_step=2, grad_norm=3.127075, loss=0.742620
I0405 23:41:27.230597 140265624962880 submission.py:139] 2) loss = 0.743, grad_norm = 3.127
I0405 23:41:27.297596 140199409481472 logging_writer.py:48] [3] global_step=3, grad_norm=3.553408, loss=0.697153
I0405 23:41:27.300770 140265624962880 submission.py:139] 3) loss = 0.697, grad_norm = 3.553
I0405 23:41:27.364372 140199493342976 logging_writer.py:48] [4] global_step=4, grad_norm=3.340995, loss=0.672058
I0405 23:41:27.367987 140265624962880 submission.py:139] 4) loss = 0.672, grad_norm = 3.341
I0405 23:41:27.434566 140199409481472 logging_writer.py:48] [5] global_step=5, grad_norm=2.599784, loss=0.668962
I0405 23:41:27.440622 140265624962880 submission.py:139] 5) loss = 0.669, grad_norm = 2.600
I0405 23:41:27.507452 140199493342976 logging_writer.py:48] [6] global_step=6, grad_norm=1.516879, loss=0.672231
I0405 23:41:27.511826 140265624962880 submission.py:139] 6) loss = 0.672, grad_norm = 1.517
I0405 23:41:27.581122 140199409481472 logging_writer.py:48] [7] global_step=7, grad_norm=1.403732, loss=0.577474
I0405 23:41:27.586254 140265624962880 submission.py:139] 7) loss = 0.577, grad_norm = 1.404
I0405 23:41:27.653160 140199493342976 logging_writer.py:48] [8] global_step=8, grad_norm=1.037820, loss=0.602540
I0405 23:41:27.656523 140265624962880 submission.py:139] 8) loss = 0.603, grad_norm = 1.038
I0405 23:41:27.721227 140199409481472 logging_writer.py:48] [9] global_step=9, grad_norm=1.240427, loss=0.598216
I0405 23:41:27.728141 140265624962880 submission.py:139] 9) loss = 0.598, grad_norm = 1.240
I0405 23:41:27.798708 140199493342976 logging_writer.py:48] [10] global_step=10, grad_norm=1.411273, loss=0.593427
I0405 23:41:27.804764 140265624962880 submission.py:139] 10) loss = 0.593, grad_norm = 1.411
I0405 23:41:27.877739 140199409481472 logging_writer.py:48] [11] global_step=11, grad_norm=1.573998, loss=0.599450
I0405 23:41:27.883192 140265624962880 submission.py:139] 11) loss = 0.599, grad_norm = 1.574
I0405 23:41:27.957006 140199493342976 logging_writer.py:48] [12] global_step=12, grad_norm=1.630400, loss=0.535066
I0405 23:41:27.962274 140265624962880 submission.py:139] 12) loss = 0.535, grad_norm = 1.630
I0405 23:41:28.042233 140199409481472 logging_writer.py:48] [13] global_step=13, grad_norm=1.453570, loss=0.601963
I0405 23:41:28.048022 140265624962880 submission.py:139] 13) loss = 0.602, grad_norm = 1.454
I0405 23:41:28.163272 140199493342976 logging_writer.py:48] [14] global_step=14, grad_norm=1.494379, loss=0.511266
I0405 23:41:28.166785 140265624962880 submission.py:139] 14) loss = 0.511, grad_norm = 1.494
I0405 23:41:28.437185 140199409481472 logging_writer.py:48] [15] global_step=15, grad_norm=1.226847, loss=0.480764
I0405 23:41:28.440365 140265624962880 submission.py:139] 15) loss = 0.481, grad_norm = 1.227
I0405 23:41:28.705440 140199493342976 logging_writer.py:48] [16] global_step=16, grad_norm=1.110389, loss=0.456529
I0405 23:41:28.708616 140265624962880 submission.py:139] 16) loss = 0.457, grad_norm = 1.110
I0405 23:41:28.992877 140199409481472 logging_writer.py:48] [17] global_step=17, grad_norm=0.840373, loss=0.424268
I0405 23:41:28.998208 140265624962880 submission.py:139] 17) loss = 0.424, grad_norm = 0.840
I0405 23:41:29.228615 140199493342976 logging_writer.py:48] [18] global_step=18, grad_norm=0.735556, loss=0.434228
I0405 23:41:29.233899 140265624962880 submission.py:139] 18) loss = 0.434, grad_norm = 0.736
I0405 23:41:29.450291 140199409481472 logging_writer.py:48] [19] global_step=19, grad_norm=0.706741, loss=0.382043
I0405 23:41:29.454823 140265624962880 submission.py:139] 19) loss = 0.382, grad_norm = 0.707
I0405 23:41:29.745801 140199493342976 logging_writer.py:48] [20] global_step=20, grad_norm=0.803763, loss=0.444355
I0405 23:41:29.750698 140265624962880 submission.py:139] 20) loss = 0.444, grad_norm = 0.804
I0405 23:41:30.022733 140199409481472 logging_writer.py:48] [21] global_step=21, grad_norm=0.761701, loss=0.450147
I0405 23:41:30.028330 140265624962880 submission.py:139] 21) loss = 0.450, grad_norm = 0.762
I0405 23:41:30.303162 140199493342976 logging_writer.py:48] [22] global_step=22, grad_norm=1.031189, loss=0.389517
I0405 23:41:30.310754 140265624962880 submission.py:139] 22) loss = 0.390, grad_norm = 1.031
I0405 23:41:30.524584 140199409481472 logging_writer.py:48] [23] global_step=23, grad_norm=1.090482, loss=0.372451
I0405 23:41:30.529584 140265624962880 submission.py:139] 23) loss = 0.372, grad_norm = 1.090
I0405 23:41:30.771199 140199493342976 logging_writer.py:48] [24] global_step=24, grad_norm=0.976098, loss=0.403794
I0405 23:41:30.776828 140265624962880 submission.py:139] 24) loss = 0.404, grad_norm = 0.976
I0405 23:41:31.070981 140199409481472 logging_writer.py:48] [25] global_step=25, grad_norm=0.802594, loss=0.438808
I0405 23:41:31.076809 140265624962880 submission.py:139] 25) loss = 0.439, grad_norm = 0.803
I0405 23:41:31.258537 140199493342976 logging_writer.py:48] [26] global_step=26, grad_norm=0.630109, loss=0.369401
I0405 23:41:31.261846 140265624962880 submission.py:139] 26) loss = 0.369, grad_norm = 0.630
I0405 23:41:31.546717 140199409481472 logging_writer.py:48] [27] global_step=27, grad_norm=0.403864, loss=0.334980
I0405 23:41:31.549761 140265624962880 submission.py:139] 27) loss = 0.335, grad_norm = 0.404
I0405 23:41:31.878524 140199493342976 logging_writer.py:48] [28] global_step=28, grad_norm=0.283237, loss=0.292400
I0405 23:41:31.881712 140265624962880 submission.py:139] 28) loss = 0.292, grad_norm = 0.283
I0405 23:41:32.212137 140199409481472 logging_writer.py:48] [29] global_step=29, grad_norm=0.452788, loss=0.349965
I0405 23:41:32.217422 140265624962880 submission.py:139] 29) loss = 0.350, grad_norm = 0.453
I0405 23:41:32.418914 140199493342976 logging_writer.py:48] [30] global_step=30, grad_norm=0.548227, loss=0.464018
I0405 23:41:32.422022 140265624962880 submission.py:139] 30) loss = 0.464, grad_norm = 0.548
I0405 23:41:32.680549 140199409481472 logging_writer.py:48] [31] global_step=31, grad_norm=0.457930, loss=0.385182
I0405 23:41:32.684136 140265624962880 submission.py:139] 31) loss = 0.385, grad_norm = 0.458
I0405 23:41:32.990246 140199493342976 logging_writer.py:48] [32] global_step=32, grad_norm=0.638391, loss=0.323923
I0405 23:41:32.994067 140265624962880 submission.py:139] 32) loss = 0.324, grad_norm = 0.638
I0405 23:41:33.221057 140199409481472 logging_writer.py:48] [33] global_step=33, grad_norm=0.523314, loss=0.367282
I0405 23:41:33.225392 140265624962880 submission.py:139] 33) loss = 0.367, grad_norm = 0.523
I0405 23:41:33.462667 140199493342976 logging_writer.py:48] [34] global_step=34, grad_norm=0.393338, loss=0.387399
I0405 23:41:33.468439 140265624962880 submission.py:139] 34) loss = 0.387, grad_norm = 0.393
I0405 23:41:33.739077 140199409481472 logging_writer.py:48] [35] global_step=35, grad_norm=0.599522, loss=0.373009
I0405 23:41:33.745665 140265624962880 submission.py:139] 35) loss = 0.373, grad_norm = 0.600
I0405 23:41:34.019122 140199493342976 logging_writer.py:48] [36] global_step=36, grad_norm=0.368526, loss=0.352032
I0405 23:41:34.023308 140265624962880 submission.py:139] 36) loss = 0.352, grad_norm = 0.369
I0405 23:41:34.269122 140199409481472 logging_writer.py:48] [37] global_step=37, grad_norm=0.386639, loss=0.383903
I0405 23:41:34.275040 140265624962880 submission.py:139] 37) loss = 0.384, grad_norm = 0.387
I0405 23:41:34.525802 140199493342976 logging_writer.py:48] [38] global_step=38, grad_norm=0.428544, loss=0.350924
I0405 23:41:34.530404 140265624962880 submission.py:139] 38) loss = 0.351, grad_norm = 0.429
I0405 23:41:34.758863 140199409481472 logging_writer.py:48] [39] global_step=39, grad_norm=0.296907, loss=0.410514
I0405 23:41:34.766244 140265624962880 submission.py:139] 39) loss = 0.411, grad_norm = 0.297
I0405 23:41:35.045630 140199493342976 logging_writer.py:48] [40] global_step=40, grad_norm=0.356375, loss=0.327896
I0405 23:41:35.051634 140265624962880 submission.py:139] 40) loss = 0.328, grad_norm = 0.356
I0405 23:41:35.348659 140199409481472 logging_writer.py:48] [41] global_step=41, grad_norm=0.386828, loss=0.374233
I0405 23:41:35.355260 140265624962880 submission.py:139] 41) loss = 0.374, grad_norm = 0.387
I0405 23:41:35.590516 140199493342976 logging_writer.py:48] [42] global_step=42, grad_norm=0.400625, loss=0.353997
I0405 23:41:35.596331 140265624962880 submission.py:139] 42) loss = 0.354, grad_norm = 0.401
I0405 23:41:35.877662 140199409481472 logging_writer.py:48] [43] global_step=43, grad_norm=0.381154, loss=0.302227
I0405 23:41:35.882532 140265624962880 submission.py:139] 43) loss = 0.302, grad_norm = 0.381
I0405 23:41:36.171121 140199493342976 logging_writer.py:48] [44] global_step=44, grad_norm=0.302206, loss=0.314155
I0405 23:41:36.176773 140265624962880 submission.py:139] 44) loss = 0.314, grad_norm = 0.302
I0405 23:41:36.402813 140199409481472 logging_writer.py:48] [45] global_step=45, grad_norm=0.293559, loss=0.396363
I0405 23:41:36.406821 140265624962880 submission.py:139] 45) loss = 0.396, grad_norm = 0.294
I0405 23:41:36.661903 140199493342976 logging_writer.py:48] [46] global_step=46, grad_norm=0.177966, loss=0.332303
I0405 23:41:36.665245 140265624962880 submission.py:139] 46) loss = 0.332, grad_norm = 0.178
I0405 23:41:36.995044 140199409481472 logging_writer.py:48] [47] global_step=47, grad_norm=0.137025, loss=0.511068
I0405 23:41:36.998547 140265624962880 submission.py:139] 47) loss = 0.511, grad_norm = 0.137
I0405 23:41:37.254739 140199493342976 logging_writer.py:48] [48] global_step=48, grad_norm=0.159742, loss=0.346525
I0405 23:41:37.257917 140265624962880 submission.py:139] 48) loss = 0.347, grad_norm = 0.160
I0405 23:41:37.523835 140199409481472 logging_writer.py:48] [49] global_step=49, grad_norm=0.251754, loss=0.355319
I0405 23:41:37.526913 140265624962880 submission.py:139] 49) loss = 0.355, grad_norm = 0.252
I0405 23:41:37.767281 140199493342976 logging_writer.py:48] [50] global_step=50, grad_norm=0.220434, loss=0.499246
I0405 23:41:37.770422 140265624962880 submission.py:139] 50) loss = 0.499, grad_norm = 0.220
I0405 23:41:38.061898 140199409481472 logging_writer.py:48] [51] global_step=51, grad_norm=0.234713, loss=0.465715
I0405 23:41:38.065576 140265624962880 submission.py:139] 51) loss = 0.466, grad_norm = 0.235
I0405 23:41:38.291441 140199493342976 logging_writer.py:48] [52] global_step=52, grad_norm=0.241894, loss=0.450744
I0405 23:41:38.296037 140265624962880 submission.py:139] 52) loss = 0.451, grad_norm = 0.242
I0405 23:41:38.576881 140199409481472 logging_writer.py:48] [53] global_step=53, grad_norm=0.199355, loss=0.366581
I0405 23:41:38.581375 140265624962880 submission.py:139] 53) loss = 0.367, grad_norm = 0.199
I0405 23:41:38.799416 140199493342976 logging_writer.py:48] [54] global_step=54, grad_norm=0.169822, loss=0.375749
I0405 23:41:38.804621 140265624962880 submission.py:139] 54) loss = 0.376, grad_norm = 0.170
I0405 23:41:39.060658 140199409481472 logging_writer.py:48] [55] global_step=55, grad_norm=0.233966, loss=0.375653
I0405 23:41:39.066400 140265624962880 submission.py:139] 55) loss = 0.376, grad_norm = 0.234
I0405 23:41:39.337041 140199493342976 logging_writer.py:48] [56] global_step=56, grad_norm=0.126772, loss=0.354545
I0405 23:41:39.341979 140265624962880 submission.py:139] 56) loss = 0.355, grad_norm = 0.127
I0405 23:41:39.537164 140199409481472 logging_writer.py:48] [57] global_step=57, grad_norm=0.160347, loss=0.450246
I0405 23:41:39.541720 140265624962880 submission.py:139] 57) loss = 0.450, grad_norm = 0.160
I0405 23:41:39.893702 140199493342976 logging_writer.py:48] [58] global_step=58, grad_norm=0.167744, loss=0.327856
I0405 23:41:39.897049 140265624962880 submission.py:139] 58) loss = 0.328, grad_norm = 0.168
I0405 23:41:40.157420 140199409481472 logging_writer.py:48] [59] global_step=59, grad_norm=0.267990, loss=0.452538
I0405 23:41:40.162667 140265624962880 submission.py:139] 59) loss = 0.453, grad_norm = 0.268
I0405 23:41:40.403768 140199493342976 logging_writer.py:48] [60] global_step=60, grad_norm=0.185229, loss=0.300966
I0405 23:41:40.408877 140265624962880 submission.py:139] 60) loss = 0.301, grad_norm = 0.185
I0405 23:41:40.711619 140199409481472 logging_writer.py:48] [61] global_step=61, grad_norm=0.194439, loss=0.333327
I0405 23:41:40.717033 140265624962880 submission.py:139] 61) loss = 0.333, grad_norm = 0.194
I0405 23:41:40.959476 140199493342976 logging_writer.py:48] [62] global_step=62, grad_norm=0.199752, loss=0.374813
I0405 23:41:40.962762 140265624962880 submission.py:139] 62) loss = 0.375, grad_norm = 0.200
I0405 23:41:41.235164 140199409481472 logging_writer.py:48] [63] global_step=63, grad_norm=0.112390, loss=0.349276
I0405 23:41:41.238354 140265624962880 submission.py:139] 63) loss = 0.349, grad_norm = 0.112
I0405 23:41:41.494162 140199493342976 logging_writer.py:48] [64] global_step=64, grad_norm=0.101054, loss=0.318877
I0405 23:41:41.499786 140265624962880 submission.py:139] 64) loss = 0.319, grad_norm = 0.101
I0405 23:41:41.803603 140199409481472 logging_writer.py:48] [65] global_step=65, grad_norm=0.110710, loss=0.362523
I0405 23:41:41.806866 140265624962880 submission.py:139] 65) loss = 0.363, grad_norm = 0.111
I0405 23:41:42.033557 140199493342976 logging_writer.py:48] [66] global_step=66, grad_norm=0.231459, loss=0.262462
I0405 23:41:42.038471 140265624962880 submission.py:139] 66) loss = 0.262, grad_norm = 0.231
I0405 23:41:42.313624 140199409481472 logging_writer.py:48] [67] global_step=67, grad_norm=0.230479, loss=0.273350
I0405 23:41:42.319064 140265624962880 submission.py:139] 67) loss = 0.273, grad_norm = 0.230
I0405 23:41:42.573127 140199493342976 logging_writer.py:48] [68] global_step=68, grad_norm=0.224749, loss=0.279412
I0405 23:41:42.576977 140265624962880 submission.py:139] 68) loss = 0.279, grad_norm = 0.225
I0405 23:41:42.862362 140199409481472 logging_writer.py:48] [69] global_step=69, grad_norm=0.162338, loss=0.284914
I0405 23:41:42.866615 140265624962880 submission.py:139] 69) loss = 0.285, grad_norm = 0.162
I0405 23:41:43.084964 140199493342976 logging_writer.py:48] [70] global_step=70, grad_norm=0.268524, loss=0.341687
I0405 23:41:43.088019 140265624962880 submission.py:139] 70) loss = 0.342, grad_norm = 0.269
I0405 23:41:43.400254 140199409481472 logging_writer.py:48] [71] global_step=71, grad_norm=0.170011, loss=0.308007
I0405 23:41:43.403426 140265624962880 submission.py:139] 71) loss = 0.308, grad_norm = 0.170
I0405 23:41:43.630217 140199493342976 logging_writer.py:48] [72] global_step=72, grad_norm=0.315861, loss=0.313196
I0405 23:41:43.633631 140265624962880 submission.py:139] 72) loss = 0.313, grad_norm = 0.316
I0405 23:41:43.902776 140199409481472 logging_writer.py:48] [73] global_step=73, grad_norm=0.324583, loss=0.348857
I0405 23:41:43.907250 140265624962880 submission.py:139] 73) loss = 0.349, grad_norm = 0.325
I0405 23:41:44.192116 140199493342976 logging_writer.py:48] [74] global_step=74, grad_norm=0.241498, loss=0.301287
I0405 23:41:44.197488 140265624962880 submission.py:139] 74) loss = 0.301, grad_norm = 0.241
I0405 23:41:44.453221 140199409481472 logging_writer.py:48] [75] global_step=75, grad_norm=0.196464, loss=0.341811
I0405 23:41:44.458073 140265624962880 submission.py:139] 75) loss = 0.342, grad_norm = 0.196
I0405 23:41:44.671401 140199493342976 logging_writer.py:48] [76] global_step=76, grad_norm=0.467379, loss=0.258224
I0405 23:41:44.677085 140265624962880 submission.py:139] 76) loss = 0.258, grad_norm = 0.467
I0405 23:41:44.957367 140199409481472 logging_writer.py:48] [77] global_step=77, grad_norm=0.291919, loss=0.296515
I0405 23:41:44.962012 140265624962880 submission.py:139] 77) loss = 0.297, grad_norm = 0.292
I0405 23:41:45.247780 140199493342976 logging_writer.py:48] [78] global_step=78, grad_norm=0.262362, loss=0.375685
I0405 23:41:45.251858 140265624962880 submission.py:139] 78) loss = 0.376, grad_norm = 0.262
I0405 23:41:45.529602 140199409481472 logging_writer.py:48] [79] global_step=79, grad_norm=0.338111, loss=0.282357
I0405 23:41:45.533786 140265624962880 submission.py:139] 79) loss = 0.282, grad_norm = 0.338
I0405 23:41:45.777219 140199493342976 logging_writer.py:48] [80] global_step=80, grad_norm=0.182043, loss=0.276267
I0405 23:41:45.781548 140265624962880 submission.py:139] 80) loss = 0.276, grad_norm = 0.182
I0405 23:41:46.037223 140199409481472 logging_writer.py:48] [81] global_step=81, grad_norm=0.203539, loss=0.260865
I0405 23:41:46.040542 140265624962880 submission.py:139] 81) loss = 0.261, grad_norm = 0.204
I0405 23:41:46.306870 140199493342976 logging_writer.py:48] [82] global_step=82, grad_norm=0.270940, loss=0.314612
I0405 23:41:46.311557 140265624962880 submission.py:139] 82) loss = 0.315, grad_norm = 0.271
I0405 23:41:46.615306 140199409481472 logging_writer.py:48] [83] global_step=83, grad_norm=0.205906, loss=0.299697
I0405 23:41:46.618506 140265624962880 submission.py:139] 83) loss = 0.300, grad_norm = 0.206
I0405 23:41:46.868206 140199493342976 logging_writer.py:48] [84] global_step=84, grad_norm=0.285062, loss=0.226305
I0405 23:41:46.873454 140265624962880 submission.py:139] 84) loss = 0.226, grad_norm = 0.285
I0405 23:41:47.088336 140199409481472 logging_writer.py:48] [85] global_step=85, grad_norm=0.173681, loss=0.341763
I0405 23:41:47.092995 140265624962880 submission.py:139] 85) loss = 0.342, grad_norm = 0.174
I0405 23:41:47.350481 140199493342976 logging_writer.py:48] [86] global_step=86, grad_norm=0.196294, loss=0.346479
I0405 23:41:47.353750 140265624962880 submission.py:139] 86) loss = 0.346, grad_norm = 0.196
I0405 23:41:47.627184 140199409481472 logging_writer.py:48] [87] global_step=87, grad_norm=0.259102, loss=0.292802
I0405 23:41:47.630534 140265624962880 submission.py:139] 87) loss = 0.293, grad_norm = 0.259
I0405 23:41:47.944275 140199493342976 logging_writer.py:48] [88] global_step=88, grad_norm=0.248382, loss=0.378571
I0405 23:41:47.947545 140265624962880 submission.py:139] 88) loss = 0.379, grad_norm = 0.248
I0405 23:41:48.185239 140199409481472 logging_writer.py:48] [89] global_step=89, grad_norm=0.110531, loss=0.332109
I0405 23:41:48.189129 140265624962880 submission.py:139] 89) loss = 0.332, grad_norm = 0.111
I0405 23:41:48.442791 140199493342976 logging_writer.py:48] [90] global_step=90, grad_norm=0.143740, loss=0.329868
I0405 23:41:48.448671 140265624962880 submission.py:139] 90) loss = 0.330, grad_norm = 0.144
I0405 23:41:48.759641 140199409481472 logging_writer.py:48] [91] global_step=91, grad_norm=0.133643, loss=0.289889
I0405 23:41:48.765051 140265624962880 submission.py:139] 91) loss = 0.290, grad_norm = 0.134
I0405 23:41:48.987738 140199493342976 logging_writer.py:48] [92] global_step=92, grad_norm=0.331049, loss=0.264673
I0405 23:41:48.993346 140265624962880 submission.py:139] 92) loss = 0.265, grad_norm = 0.331
I0405 23:41:49.272341 140199409481472 logging_writer.py:48] [93] global_step=93, grad_norm=0.189395, loss=0.272753
I0405 23:41:49.278038 140265624962880 submission.py:139] 93) loss = 0.273, grad_norm = 0.189
I0405 23:41:49.464890 140199493342976 logging_writer.py:48] [94] global_step=94, grad_norm=0.173032, loss=0.306092
I0405 23:41:49.468294 140265624962880 submission.py:139] 94) loss = 0.306, grad_norm = 0.173
I0405 23:41:49.726540 140199409481472 logging_writer.py:48] [95] global_step=95, grad_norm=0.373451, loss=0.417411
I0405 23:41:49.729728 140265624962880 submission.py:139] 95) loss = 0.417, grad_norm = 0.373
I0405 23:41:50.021525 140199493342976 logging_writer.py:48] [96] global_step=96, grad_norm=0.142760, loss=0.360282
I0405 23:41:50.024669 140265624962880 submission.py:139] 96) loss = 0.360, grad_norm = 0.143
I0405 23:41:50.296066 140199409481472 logging_writer.py:48] [97] global_step=97, grad_norm=0.149967, loss=0.310356
I0405 23:41:50.300746 140265624962880 submission.py:139] 97) loss = 0.310, grad_norm = 0.150
I0405 23:41:50.570898 140199493342976 logging_writer.py:48] [98] global_step=98, grad_norm=0.118434, loss=0.302553
I0405 23:41:50.576573 140265624962880 submission.py:139] 98) loss = 0.303, grad_norm = 0.118
I0405 23:41:50.784046 140199409481472 logging_writer.py:48] [99] global_step=99, grad_norm=0.137048, loss=0.267816
I0405 23:41:50.787691 140265624962880 submission.py:139] 99) loss = 0.268, grad_norm = 0.137
I0405 23:41:51.084567 140199493342976 logging_writer.py:48] [100] global_step=100, grad_norm=0.230206, loss=0.296827
I0405 23:41:51.091445 140265624962880 submission.py:139] 100) loss = 0.297, grad_norm = 0.230
I0405 23:42:47.158377 140265624962880 submission_runner.py:373] Before eval at step 311: RAM USED (GB) 86.576730112
I0405 23:42:47.158630 140265624962880 spec.py:298] Evaluating on the training split.
I0405 23:42:49.202930 140265624962880 spec.py:310] Evaluating on the validation split.
I0405 23:42:53.883574 140265624962880 spec.py:326] Evaluating on the test split.
I0405 23:42:56.586022 140265624962880 submission_runner.py:382] Time since start: 354.42s, 	Step: 311, 	{'train/ssim': 0.7095303535461426, 'train/loss': 0.30453528676714214, 'validation/ssim': 0.6875130519748874, 'validation/loss': 0.32320334070105866, 'validation/num_examples': 3554, 'test/ssim': 0.705924333723122, 'test/loss': 0.32483305580886973, 'test/num_examples': 3581}
I0405 23:42:56.586373 140265624962880 submission_runner.py:396] After eval at step 311: RAM USED (GB) 87.7712384
I0405 23:42:56.597179 140199409481472 logging_writer.py:48] [311] global_step=311, preemption_count=0, score=121.737352, test/loss=0.324833, test/num_examples=3581, test/ssim=0.705924, total_duration=354.421854, train/loss=0.304535, train/ssim=0.709530, validation/loss=0.323203, validation/num_examples=3554, validation/ssim=0.687513
I0405 23:42:56.703399 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_311.
I0405 23:42:56.704560 140265624962880 submission_runner.py:416] After logging and checkpointing eval at step 311: RAM USED (GB) 87.784914944
I0405 23:44:01.926962 140199493342976 logging_writer.py:48] [500] global_step=500, grad_norm=0.688654, loss=0.269005
I0405 23:44:01.930792 140265624962880 submission.py:139] 500) loss = 0.269, grad_norm = 0.689
I0405 23:44:17.074620 140265624962880 submission_runner.py:373] Before eval at step 543: RAM USED (GB) 105.297956864
I0405 23:44:17.074819 140265624962880 spec.py:298] Evaluating on the training split.
I0405 23:44:19.137682 140265624962880 spec.py:310] Evaluating on the validation split.
I0405 23:44:23.703319 140265624962880 spec.py:326] Evaluating on the test split.
I0405 23:44:25.881094 140265624962880 submission_runner.py:382] Time since start: 444.34s, 	Step: 543, 	{'train/ssim': 0.7142853055681501, 'train/loss': 0.2972524506705148, 'validation/ssim': 0.6928096120788196, 'validation/loss': 0.31564799895364376, 'validation/num_examples': 3554, 'test/ssim': 0.7105289172673485, 'test/loss': 0.3173396907615715, 'test/num_examples': 3581}
I0405 23:44:25.881446 140265624962880 submission_runner.py:396] After eval at step 543: RAM USED (GB) 106.5327616
I0405 23:44:25.891053 140199409481472 logging_writer.py:48] [543] global_step=543, preemption_count=0, score=197.495383, test/loss=0.317340, test/num_examples=3581, test/ssim=0.710529, total_duration=444.338609, train/loss=0.297252, train/ssim=0.714285, validation/loss=0.315648, validation/num_examples=3554, validation/ssim=0.692810
I0405 23:44:26.010088 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_543.
I0405 23:44:26.010862 140265624962880 submission_runner.py:416] After logging and checkpointing eval at step 543: RAM USED (GB) 106.54846976
I0405 23:45:46.052706 140265624962880 submission_runner.py:373] Before eval at step 773: RAM USED (GB) 123.883466752
I0405 23:45:46.052930 140265624962880 spec.py:298] Evaluating on the training split.
I0405 23:45:48.046606 140265624962880 spec.py:310] Evaluating on the validation split.
I0405 23:45:51.244651 140265624962880 spec.py:326] Evaluating on the test split.
I0405 23:45:53.439827 140265624962880 submission_runner.py:382] Time since start: 533.32s, 	Step: 773, 	{'train/ssim': 0.7224389484950474, 'train/loss': 0.2891500677381243, 'validation/ssim': 0.7002455282560144, 'validation/loss': 0.3073345092017797, 'validation/num_examples': 3554, 'test/ssim': 0.7178131163170204, 'test/loss': 0.30923515603183466, 'test/num_examples': 3581}
I0405 23:45:53.440194 140265624962880 submission_runner.py:396] After eval at step 773: RAM USED (GB) 125.026435072
I0405 23:45:53.453491 140199493342976 logging_writer.py:48] [773] global_step=773, preemption_count=0, score=273.029560, test/loss=0.309235, test/num_examples=3581, test/ssim=0.717813, total_duration=533.315546, train/loss=0.289150, train/ssim=0.722439, validation/loss=0.307335, validation/num_examples=3554, validation/ssim=0.700246
I0405 23:45:53.562280 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_773.
I0405 23:45:53.562827 140265624962880 submission_runner.py:416] After logging and checkpointing eval at step 773: RAM USED (GB) 125.045747712
I0405 23:47:11.406860 140199409481472 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.407045, loss=0.260645
I0405 23:47:11.414873 140265624962880 submission.py:139] 1000) loss = 0.261, grad_norm = 0.407
I0405 23:47:13.671218 140265624962880 submission_runner.py:373] Before eval at step 1009: RAM USED (GB) 142.059368448
I0405 23:47:13.671415 140265624962880 spec.py:298] Evaluating on the training split.
I0405 23:47:15.706294 140265624962880 spec.py:310] Evaluating on the validation split.
I0405 23:47:20.183496 140265624962880 spec.py:326] Evaluating on the test split.
I0405 23:47:22.469222 140265624962880 submission_runner.py:382] Time since start: 620.94s, 	Step: 1009, 	{'train/ssim': 0.7265102522713798, 'train/loss': 0.2847043105534145, 'validation/ssim': 0.7048984886087859, 'validation/loss': 0.3027989822756929, 'validation/num_examples': 3554, 'test/ssim': 0.7223170029321418, 'test/loss': 0.3045793377264556, 'test/num_examples': 3581}
I0405 23:47:22.469568 140265624962880 submission_runner.py:396] After eval at step 1009: RAM USED (GB) 142.398124032
I0405 23:47:22.477861 140199493342976 logging_writer.py:48] [1009] global_step=1009, preemption_count=0, score=348.747770, test/loss=0.304579, test/num_examples=3581, test/ssim=0.722317, total_duration=620.937839, train/loss=0.284704, train/ssim=0.726510, validation/loss=0.302799, validation/num_examples=3554, validation/ssim=0.704898
I0405 23:47:22.579108 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_1009.
I0405 23:47:22.579617 140265624962880 submission_runner.py:416] After logging and checkpointing eval at step 1009: RAM USED (GB) 142.408978432
I0405 23:48:42.648194 140265624962880 submission_runner.py:373] Before eval at step 1322: RAM USED (GB) 142.81740288
I0405 23:48:42.648459 140265624962880 spec.py:298] Evaluating on the training split.
I0405 23:48:44.608551 140265624962880 spec.py:310] Evaluating on the validation split.
I0405 23:48:46.741269 140265624962880 spec.py:326] Evaluating on the test split.
I0405 23:48:48.790025 140265624962880 submission_runner.py:382] Time since start: 709.91s, 	Step: 1322, 	{'train/ssim': 0.7291274751935687, 'train/loss': 0.2816474437713623, 'validation/ssim': 0.7071884232554868, 'validation/loss': 0.30008029025393923, 'validation/num_examples': 3554, 'test/ssim': 0.7246356230146956, 'test/loss': 0.30170633912053196, 'test/num_examples': 3581}
I0405 23:48:48.790489 140265624962880 submission_runner.py:396] After eval at step 1322: RAM USED (GB) 142.824722432
I0405 23:48:48.799380 140199409481472 logging_writer.py:48] [1322] global_step=1322, preemption_count=0, score=421.513230, test/loss=0.301706, test/num_examples=3581, test/ssim=0.724636, total_duration=709.906904, train/loss=0.281647, train/ssim=0.729127, validation/loss=0.300080, validation/num_examples=3554, validation/ssim=0.707188
I0405 23:48:48.901520 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_1322.
I0405 23:48:48.902092 140265624962880 submission_runner.py:416] After logging and checkpointing eval at step 1322: RAM USED (GB) 142.824173568
I0405 23:49:33.272504 140199493342976 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.165111, loss=0.297953
I0405 23:49:33.275627 140265624962880 submission.py:139] 1500) loss = 0.298, grad_norm = 0.165
I0405 23:50:09.032804 140265624962880 submission_runner.py:373] Before eval at step 1637: RAM USED (GB) 142.900637696
I0405 23:50:09.033043 140265624962880 spec.py:298] Evaluating on the training split.
I0405 23:50:11.097293 140265624962880 spec.py:310] Evaluating on the validation split.
I0405 23:50:13.356887 140265624962880 spec.py:326] Evaluating on the test split.
I0405 23:50:15.529456 140265624962880 submission_runner.py:382] Time since start: 796.29s, 	Step: 1637, 	{'train/ssim': 0.7243145533970424, 'train/loss': 0.2886723109654018, 'validation/ssim': 0.7028903392304445, 'validation/loss': 0.3067409534701217, 'validation/num_examples': 3554, 'test/ssim': 0.7198680972493717, 'test/loss': 0.30878580365557806, 'test/num_examples': 3581}
I0405 23:50:15.529839 140265624962880 submission_runner.py:396] After eval at step 1637: RAM USED (GB) 142.907465728
I0405 23:50:15.538148 140199409481472 logging_writer.py:48] [1637] global_step=1637, preemption_count=0, score=494.302725, test/loss=0.308786, test/num_examples=3581, test/ssim=0.719868, total_duration=796.291538, train/loss=0.288672, train/ssim=0.724315, validation/loss=0.306741, validation/num_examples=3554, validation/ssim=0.702890
I0405 23:50:15.634974 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_1637.
I0405 23:50:15.635506 140265624962880 submission_runner.py:416] After logging and checkpointing eval at step 1637: RAM USED (GB) 142.906892288
I0405 23:51:35.870412 140265624962880 submission_runner.py:373] Before eval at step 1951: RAM USED (GB) 143.000178688
I0405 23:51:35.870629 140265624962880 spec.py:298] Evaluating on the training split.
I0405 23:51:37.845456 140265624962880 spec.py:310] Evaluating on the validation split.
I0405 23:51:40.045803 140265624962880 spec.py:326] Evaluating on the test split.
I0405 23:51:42.122952 140265624962880 submission_runner.py:382] Time since start: 883.13s, 	Step: 1951, 	{'train/ssim': 0.7245744296482631, 'train/loss': 0.2861356735229492, 'validation/ssim': 0.7032575805870146, 'validation/loss': 0.3048307282507386, 'validation/num_examples': 3554, 'test/ssim': 0.7199851565772479, 'test/loss': 0.30684873425937237, 'test/num_examples': 3581}
I0405 23:51:42.123324 140265624962880 submission_runner.py:396] After eval at step 1951: RAM USED (GB) 143.013052416
I0405 23:51:42.132012 140199493342976 logging_writer.py:48] [1951] global_step=1951, preemption_count=0, score=567.238107, test/loss=0.306849, test/num_examples=3581, test/ssim=0.719985, total_duration=883.127438, train/loss=0.286136, train/ssim=0.724574, validation/loss=0.304831, validation/num_examples=3554, validation/ssim=0.703258
I0405 23:51:42.228820 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_1951.
I0405 23:51:42.229333 140265624962880 submission_runner.py:416] After logging and checkpointing eval at step 1951: RAM USED (GB) 143.013511168
I0405 23:51:53.125110 140199409481472 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.133665, loss=0.264391
I0405 23:51:53.129059 140265624962880 submission.py:139] 2000) loss = 0.264, grad_norm = 0.134
I0405 23:53:02.381927 140265624962880 submission_runner.py:373] Before eval at step 2263: RAM USED (GB) 143.169183744
I0405 23:53:02.382143 140265624962880 spec.py:298] Evaluating on the training split.
I0405 23:53:04.365094 140265624962880 spec.py:310] Evaluating on the validation split.
I0405 23:53:07.207758 140265624962880 spec.py:326] Evaluating on the test split.
I0405 23:53:09.359375 140265624962880 submission_runner.py:382] Time since start: 969.64s, 	Step: 2263, 	{'train/ssim': 0.7369100025721959, 'train/loss': 0.27731776237487793, 'validation/ssim': 0.713639533580121, 'validation/loss': 0.2965162767744091, 'validation/num_examples': 3554, 'test/ssim': 0.7307679091908684, 'test/loss': 0.2983854880249058, 'test/num_examples': 3581}
I0405 23:53:09.359740 140265624962880 submission_runner.py:396] After eval at step 2263: RAM USED (GB) 143.117463552
I0405 23:53:09.367936 140199493342976 logging_writer.py:48] [2263] global_step=2263, preemption_count=0, score=640.208583, test/loss=0.298385, test/num_examples=3581, test/ssim=0.730768, total_duration=969.639287, train/loss=0.277318, train/ssim=0.736910, validation/loss=0.296516, validation/num_examples=3554, validation/ssim=0.713640
I0405 23:53:09.468245 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_2263.
I0405 23:53:09.469030 140265624962880 submission_runner.py:416] After logging and checkpointing eval at step 2263: RAM USED (GB) 143.109562368
I0405 23:54:09.437116 140199409481472 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.188749, loss=0.305663
I0405 23:54:09.440708 140265624962880 submission.py:139] 2500) loss = 0.306, grad_norm = 0.189
I0405 23:54:29.572884 140265624962880 submission_runner.py:373] Before eval at step 2577: RAM USED (GB) 143.174578176
I0405 23:54:29.573097 140265624962880 spec.py:298] Evaluating on the training split.
I0405 23:54:31.528932 140265624962880 spec.py:310] Evaluating on the validation split.
I0405 23:54:33.694238 140265624962880 spec.py:326] Evaluating on the test split.
I0405 23:54:35.849789 140265624962880 submission_runner.py:382] Time since start: 1056.83s, 	Step: 2577, 	{'train/ssim': 0.7367104802812848, 'train/loss': 0.27696551595415386, 'validation/ssim': 0.7137108385797692, 'validation/loss': 0.2967253488037247, 'validation/num_examples': 3554, 'test/ssim': 0.7306786659409034, 'test/loss': 0.29855074825249583, 'test/num_examples': 3581}
I0405 23:54:35.850181 140265624962880 submission_runner.py:396] After eval at step 2577: RAM USED (GB) 143.12327168
I0405 23:54:35.859101 140199493342976 logging_writer.py:48] [2577] global_step=2577, preemption_count=0, score=713.020641, test/loss=0.298551, test/num_examples=3581, test/ssim=0.730679, total_duration=1056.832913, train/loss=0.276966, train/ssim=0.736710, validation/loss=0.296725, validation/num_examples=3554, validation/ssim=0.713711
I0405 23:54:35.956748 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_2577.
I0405 23:54:35.957228 140265624962880 submission_runner.py:416] After logging and checkpointing eval at step 2577: RAM USED (GB) 143.124492288
I0405 23:55:09.595056 140265624962880 submission_runner.py:373] Before eval at step 2714: RAM USED (GB) 143.21078272
I0405 23:55:09.595302 140265624962880 spec.py:298] Evaluating on the training split.
I0405 23:55:11.567892 140265624962880 spec.py:310] Evaluating on the validation split.
I0405 23:55:14.010084 140265624962880 spec.py:326] Evaluating on the test split.
I0405 23:55:16.111147 140265624962880 submission_runner.py:382] Time since start: 1096.85s, 	Step: 2714, 	{'train/ssim': 0.726970740727016, 'train/loss': 0.27854299545288086, 'validation/ssim': 0.7061273663917417, 'validation/loss': 0.2970598571811691, 'validation/num_examples': 3554, 'test/ssim': 0.7234765515917342, 'test/loss': 0.29850050205293566, 'test/num_examples': 3581}
I0405 23:55:16.111527 140265624962880 submission_runner.py:396] After eval at step 2714: RAM USED (GB) 143.134191616
I0405 23:55:16.119592 140199409481472 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=743.511508, test/loss=0.298501, test/num_examples=3581, test/ssim=0.723477, total_duration=1096.852733, train/loss=0.278543, train/ssim=0.726971, validation/loss=0.297060, validation/num_examples=3554, validation/ssim=0.706127
I0405 23:55:16.216421 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_2714.
I0405 23:55:16.216912 140265624962880 submission_runner.py:416] After logging and checkpointing eval at step 2714: RAM USED (GB) 143.133650944
I0405 23:55:16.224579 140199493342976 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=743.511508
I0405 23:55:16.411541 140265624962880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/fastmri_pytorch/trial_1/checkpoint_2714.
I0405 23:55:17.669240 140265624962880 submission_runner.py:550] Tuning trial 1/1
I0405 23:55:17.669480 140265624962880 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 23:55:17.675112 140265624962880 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ssim': 0.27292992387499126, 'train/loss': 0.7443235261099679, 'validation/ssim': 0.26595291369539603, 'validation/loss': 0.7523124664770329, 'validation/num_examples': 3554, 'test/ssim': 0.28794061894591244, 'test/loss': 0.7553481182150587, 'test/num_examples': 3581, 'score': 45.931708335876465, 'total_duration': 45.933769941329956, 'global_step': 1, 'preemption_count': 0}), (311, {'train/ssim': 0.7095303535461426, 'train/loss': 0.30453528676714214, 'validation/ssim': 0.6875130519748874, 'validation/loss': 0.32320334070105866, 'validation/num_examples': 3554, 'test/ssim': 0.705924333723122, 'test/loss': 0.32483305580886973, 'test/num_examples': 3581, 'score': 121.73735165596008, 'total_duration': 354.4218535423279, 'global_step': 311, 'preemption_count': 0}), (543, {'train/ssim': 0.7142853055681501, 'train/loss': 0.2972524506705148, 'validation/ssim': 0.6928096120788196, 'validation/loss': 0.31564799895364376, 'validation/num_examples': 3554, 'test/ssim': 0.7105289172673485, 'test/loss': 0.3173396907615715, 'test/num_examples': 3581, 'score': 197.49538254737854, 'total_duration': 444.3386092185974, 'global_step': 543, 'preemption_count': 0}), (773, {'train/ssim': 0.7224389484950474, 'train/loss': 0.2891500677381243, 'validation/ssim': 0.7002455282560144, 'validation/loss': 0.3073345092017797, 'validation/num_examples': 3554, 'test/ssim': 0.7178131163170204, 'test/loss': 0.30923515603183466, 'test/num_examples': 3581, 'score': 273.0295603275299, 'total_duration': 533.315545797348, 'global_step': 773, 'preemption_count': 0}), (1009, {'train/ssim': 0.7265102522713798, 'train/loss': 0.2847043105534145, 'validation/ssim': 0.7048984886087859, 'validation/loss': 0.3027989822756929, 'validation/num_examples': 3554, 'test/ssim': 0.7223170029321418, 'test/loss': 0.3045793377264556, 'test/num_examples': 3581, 'score': 348.74777030944824, 'total_duration': 620.9378387928009, 'global_step': 1009, 'preemption_count': 0}), (1322, {'train/ssim': 0.7291274751935687, 'train/loss': 0.2816474437713623, 'validation/ssim': 0.7071884232554868, 'validation/loss': 0.30008029025393923, 'validation/num_examples': 3554, 'test/ssim': 0.7246356230146956, 'test/loss': 0.30170633912053196, 'test/num_examples': 3581, 'score': 421.51322984695435, 'total_duration': 709.906904220581, 'global_step': 1322, 'preemption_count': 0}), (1637, {'train/ssim': 0.7243145533970424, 'train/loss': 0.2886723109654018, 'validation/ssim': 0.7028903392304445, 'validation/loss': 0.3067409534701217, 'validation/num_examples': 3554, 'test/ssim': 0.7198680972493717, 'test/loss': 0.30878580365557806, 'test/num_examples': 3581, 'score': 494.30272483825684, 'total_duration': 796.2915382385254, 'global_step': 1637, 'preemption_count': 0}), (1951, {'train/ssim': 0.7245744296482631, 'train/loss': 0.2861356735229492, 'validation/ssim': 0.7032575805870146, 'validation/loss': 0.3048307282507386, 'validation/num_examples': 3554, 'test/ssim': 0.7199851565772479, 'test/loss': 0.30684873425937237, 'test/num_examples': 3581, 'score': 567.2381072044373, 'total_duration': 883.1274375915527, 'global_step': 1951, 'preemption_count': 0}), (2263, {'train/ssim': 0.7369100025721959, 'train/loss': 0.27731776237487793, 'validation/ssim': 0.713639533580121, 'validation/loss': 0.2965162767744091, 'validation/num_examples': 3554, 'test/ssim': 0.7307679091908684, 'test/loss': 0.2983854880249058, 'test/num_examples': 3581, 'score': 640.20858335495, 'total_duration': 969.6392872333527, 'global_step': 2263, 'preemption_count': 0}), (2577, {'train/ssim': 0.7367104802812848, 'train/loss': 0.27696551595415386, 'validation/ssim': 0.7137108385797692, 'validation/loss': 0.2967253488037247, 'validation/num_examples': 3554, 'test/ssim': 0.7306786659409034, 'test/loss': 0.29855074825249583, 'test/num_examples': 3581, 'score': 713.0206410884857, 'total_duration': 1056.832913160324, 'global_step': 2577, 'preemption_count': 0}), (2714, {'train/ssim': 0.726970740727016, 'train/loss': 0.27854299545288086, 'validation/ssim': 0.7061273663917417, 'validation/loss': 0.2970598571811691, 'validation/num_examples': 3554, 'test/ssim': 0.7234765515917342, 'test/loss': 0.29850050205293566, 'test/num_examples': 3581, 'score': 743.5115079879761, 'total_duration': 1096.8527331352234, 'global_step': 2714, 'preemption_count': 0})], 'global_step': 2714}
I0405 23:55:17.675257 140265624962880 submission_runner.py:553] Timing: 743.5115079879761
I0405 23:55:17.675308 140265624962880 submission_runner.py:554] ====================
I0405 23:55:17.675400 140265624962880 submission_runner.py:613] Final fastmri score: 743.5115079879761
