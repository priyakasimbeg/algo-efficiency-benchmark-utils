I0405 03:38:36.236776 140374562969408 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum_v2/librispeech_conformer_jax.
I0405 03:38:36.296751 140374562969408 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0405 03:38:37.175079 140374562969408 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0405 03:38:37.175669 140374562969408 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0405 03:38:37.180737 140374562969408 submission_runner.py:511] Using RNG seed 1240354615
I0405 03:38:39.514660 140374562969408 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 03:38:39.514858 140374562969408 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1.
I0405 03:38:39.515025 140374562969408 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/hparams.json.
I0405 03:38:39.641436 140374562969408 submission_runner.py:230] Starting train once: RAM USED (GB) 4.302659584
I0405 03:38:39.641598 140374562969408 submission_runner.py:231] Initializing dataset.
I0405 03:38:39.641778 140374562969408 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.302659584
I0405 03:38:39.641834 140374562969408 submission_runner.py:240] Initializing model.
I0405 03:38:45.247260 140374562969408 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.73224448
I0405 03:38:45.247455 140374562969408 submission_runner.py:252] Initializing optimizer.
I0405 03:38:45.959145 140374562969408 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.732461568
I0405 03:38:45.959309 140374562969408 submission_runner.py:261] Initializing metrics bundle.
I0405 03:38:45.959378 140374562969408 submission_runner.py:276] Initializing checkpoint and logger.
I0405 03:38:45.960417 140374562969408 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0405 03:38:45.960694 140374562969408 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 03:38:45.960763 140374562969408 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 03:38:46.759578 140374562969408 submission_runner.py:297] Saving meta data to /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0405 03:38:46.760425 140374562969408 submission_runner.py:300] Saving flags to /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/flags_0.json.
I0405 03:38:46.766780 140374562969408 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.729508352
I0405 03:38:46.766995 140374562969408 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.729508352
I0405 03:38:46.767063 140374562969408 submission_runner.py:313] Starting training loop.
I0405 03:38:46.965720 140374562969408 input_pipeline.py:20] Loading split = train-clean-100
I0405 03:38:46.997911 140374562969408 input_pipeline.py:20] Loading split = train-clean-360
I0405 03:38:47.325754 140374562969408 input_pipeline.py:20] Loading split = train-other-500
I0405 03:38:50.749369 140374562969408 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.896970752
2023-04-05 03:39:44.606074: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-05 03:39:44.813745: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0405 03:39:46.560471 140199963121408 logging_writer.py:48] [0] global_step=0, grad_norm=45.90095901489258, loss=32.03889083862305
I0405 03:39:46.576515 140374562969408 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.187670016
I0405 03:39:46.576766 140374562969408 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.187670016
I0405 03:39:46.576849 140374562969408 spec.py:298] Evaluating on the training split.
I0405 03:39:46.683561 140374562969408 input_pipeline.py:20] Loading split = train-clean-100
I0405 03:39:46.709237 140374562969408 input_pipeline.py:20] Loading split = train-clean-360
I0405 03:39:47.005786 140374562969408 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0405 03:40:28.478120 140374562969408 spec.py:310] Evaluating on the validation split.
I0405 03:40:28.538486 140374562969408 input_pipeline.py:20] Loading split = dev-clean
I0405 03:40:28.543585 140374562969408 input_pipeline.py:20] Loading split = dev-other
I0405 03:41:06.242529 140374562969408 spec.py:326] Evaluating on the test split.
I0405 03:41:06.303994 140374562969408 input_pipeline.py:20] Loading split = test-clean
I0405 03:41:33.586746 140374562969408 submission_runner.py:382] Time since start: 59.81s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.411068, dtype=float32), 'train/wer': 0.9518199612685588, 'validation/ctc_loss': DeviceArray(30.424536, dtype=float32), 'validation/wer': 0.9717508128394872, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.636398, dtype=float32), 'test/wer': 0.9788353340239271, 'test/num_examples': 2472}
I0405 03:41:33.587871 140374562969408 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.549758976
I0405 03:41:33.599430 140195752048384 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=59.611012, test/ctc_loss=30.636398315429688, test/num_examples=2472, test/wer=0.978835, total_duration=59.809761, train/ctc_loss=31.411067962646484, train/wer=0.951820, validation/ctc_loss=30.424535751342773, validation/num_examples=5348, validation/wer=0.971751
I0405 03:41:33.827095 140374562969408 checkpoints.py:356] Saving checkpoint at step: 1
I0405 03:41:34.771674 140374562969408 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_1
I0405 03:41:34.792895 140374562969408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_1.
I0405 03:41:34.809578 140374562969408 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.560195584
I0405 03:41:34.873365 140374562969408 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.559187968
I0405 03:41:48.905433 140374562969408 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.88077312
I0405 03:43:03.826483 140200768517888 logging_writer.py:48] [100] global_step=100, grad_norm=11.157035827636719, loss=6.134118556976318
I0405 03:44:19.767753 140200776910592 logging_writer.py:48] [200] global_step=200, grad_norm=27.864870071411133, loss=37.48086929321289
I0405 03:45:34.035052 140200768517888 logging_writer.py:48] [300] global_step=300, grad_norm=0.0, loss=1757.1435546875
I0405 03:46:47.641293 140200776910592 logging_writer.py:48] [400] global_step=400, grad_norm=0.0, loss=1818.685791015625
I0405 03:48:01.558564 140200768517888 logging_writer.py:48] [500] global_step=500, grad_norm=0.0, loss=1767.08935546875
I0405 03:49:15.288062 140200776910592 logging_writer.py:48] [600] global_step=600, grad_norm=0.0, loss=1862.3470458984375
I0405 03:50:39.580317 140200768517888 logging_writer.py:48] [700] global_step=700, grad_norm=0.0, loss=1792.7047119140625
I0405 03:52:06.040983 140200776910592 logging_writer.py:48] [800] global_step=800, grad_norm=0.0, loss=1807.2589111328125
I0405 03:53:33.680238 140200768517888 logging_writer.py:48] [900] global_step=900, grad_norm=0.0, loss=1807.769287109375
I0405 03:54:58.613839 140200776910592 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0, loss=1796.2265625
I0405 03:56:18.254020 140201574946560 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0, loss=1792.8302001953125
I0405 03:57:32.011773 140201566553856 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0, loss=1877.9212646484375
I0405 03:58:45.652246 140201574946560 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0, loss=1840.257568359375
I0405 04:00:01.020908 140201566553856 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0, loss=1752.4522705078125
I0405 04:01:26.784779 140201574946560 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1864.7889404296875
I0405 04:02:56.086439 140201566553856 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0, loss=1860.31689453125
I0405 04:04:20.829346 140201574946560 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0, loss=1766.4796142578125
I0405 04:05:49.845463 140201566553856 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0, loss=1845.43115234375
I0405 04:07:15.838380 140201574946560 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0, loss=1817.13671875
I0405 04:08:44.561215 140201566553856 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1827.7755126953125
I0405 04:10:07.545681 140202230306560 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0, loss=1811.22265625
I0405 04:11:21.047726 140202221913856 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1859.506103515625
I0405 04:12:34.610812 140202230306560 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0, loss=1839.5963134765625
I0405 04:13:48.457286 140202221913856 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1879.2998046875
I0405 04:15:04.559756 140202230306560 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1829.081298828125
I0405 04:16:31.652336 140202221913856 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1782.3450927734375
I0405 04:17:56.599238 140202230306560 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1790.9488525390625
I0405 04:19:23.108879 140202221913856 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1795.974609375
I0405 04:20:48.952936 140202230306560 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1818.0400390625
I0405 04:21:35.471234 140374562969408 submission_runner.py:373] Before eval at step 2957: RAM USED (GB) 22.464200704
I0405 04:21:35.471453 140374562969408 spec.py:298] Evaluating on the training split.
I0405 04:22:01.945083 140374562969408 spec.py:310] Evaluating on the validation split.
I0405 04:22:39.254751 140374562969408 spec.py:326] Evaluating on the test split.
I0405 04:22:58.914478 140374562969408 submission_runner.py:382] Time since start: 2568.70s, 	Step: 2957, 	{'train/ctc_loss': DeviceArray(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 04:22:58.915917 140374562969408 submission_runner.py:396] After eval at step 2957: RAM USED (GB) 20.557524992
I0405 04:22:58.935955 140202230306560 logging_writer.py:48] [2957] global_step=2957, preemption_count=0, score=2454.453073, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=2568.702260, train/ctc_loss=1767.6815185546875, train/wer=0.944636, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0405 04:22:59.150291 140374562969408 checkpoints.py:356] Saving checkpoint at step: 2957
I0405 04:23:00.117850 140374562969408 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_2957
I0405 04:23:00.139804 140374562969408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_2957.
I0405 04:23:00.150760 140374562969408 submission_runner.py:416] After logging and checkpointing eval at step 2957: RAM USED (GB) 20.52591616
I0405 04:23:32.495520 140202221913856 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1858.156494140625
I0405 04:24:49.969411 140201574946560 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1838.6715087890625
I0405 04:26:03.616895 140201566553856 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1840.257568359375
I0405 04:27:17.331389 140201574946560 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1843.0396728515625
I0405 04:28:31.172622 140201566553856 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1824.6488037109375
I0405 04:29:46.834230 140201574946560 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1877.783447265625
I0405 04:31:09.382012 140201566553856 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1810.582275390625
I0405 04:32:31.552932 140201574946560 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1882.8936767578125
I0405 04:33:54.760623 140201566553856 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1821.2735595703125
I0405 04:35:19.260035 140201574946560 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1782.3450927734375
I0405 04:36:44.704602 140201566553856 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1819.3321533203125
I0405 04:38:07.625337 140201574946560 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1825.5596923828125
I0405 04:39:26.273742 140200591906560 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1815.46142578125
I0405 04:40:40.185634 140200583513856 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1879.437744140625
I0405 04:41:53.926121 140200591906560 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1867.6458740234375
I0405 04:43:12.663578 140200583513856 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1841.5814208984375
I0405 04:44:33.943578 140200591906560 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1873.9346923828125
I0405 04:45:54.993657 140200583513856 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1860.85791015625
I0405 04:47:18.514426 140200591906560 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1817.65283203125
I0405 04:48:42.215262 140200583513856 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1732.8790283203125
I0405 04:50:05.727186 140200591906560 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1847.695556640625
I0405 04:51:29.168687 140200583513856 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1757.1435546875
I0405 04:52:51.848380 140200591906560 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1834.718017578125
I0405 04:54:05.345588 140200583513856 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1776.7783203125
I0405 04:55:19.287181 140200591906560 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1863.024658203125
I0405 04:56:33.216140 140200583513856 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1861.399169921875
I0405 04:57:50.923272 140200591906560 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1869.009521484375
I0405 04:59:17.315599 140200583513856 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1802.9312744140625
I0405 05:00:43.398688 140200591906560 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1800.648681640625
I0405 05:02:12.076837 140200583513856 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1764.4097900390625
I0405 05:03:01.348058 140374562969408 submission_runner.py:373] Before eval at step 5956: RAM USED (GB) 21.641732096
I0405 05:03:01.348320 140374562969408 spec.py:298] Evaluating on the training split.
I0405 05:03:28.198971 140374562969408 spec.py:310] Evaluating on the validation split.
I0405 05:04:04.508635 140374562969408 spec.py:326] Evaluating on the test split.
I0405 05:04:24.679895 140374562969408 submission_runner.py:382] Time since start: 5054.58s, 	Step: 5956, 	{'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 05:04:24.681230 140374562969408 submission_runner.py:396] After eval at step 5956: RAM USED (GB) 20.436803584
I0405 05:04:24.700854 140200300066560 logging_writer.py:48] [5956] global_step=5956, preemption_count=0, score=4849.685253, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=5054.578206, train/ctc_loss=1761.5706787109375, train/wer=0.942722, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0405 05:04:24.896669 140374562969408 checkpoints.py:356] Saving checkpoint at step: 5956
I0405 05:04:25.873819 140374562969408 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_5956
I0405 05:04:25.895163 140374562969408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_5956.
I0405 05:04:25.905662 140374562969408 submission_runner.py:416] After logging and checkpointing eval at step 5956: RAM USED (GB) 20.458565632
I0405 05:04:58.884748 140200291673856 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1852.7772216796875
I0405 05:06:12.354751 140197316630272 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1806.111328125
I0405 05:07:30.651727 140200300066560 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1838.4073486328125
I0405 05:08:44.238228 140200291673856 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1840.9193115234375
I0405 05:09:57.910349 140200300066560 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1857.3477783203125
I0405 05:11:14.410902 140200291673856 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1822.9595947265625
I0405 05:12:35.908895 140200300066560 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1846.7625732421875
I0405 05:13:59.909763 140200291673856 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1847.4288330078125
I0405 05:15:23.396776 140200300066560 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1806.3662109375
I0405 05:16:50.614162 140200291673856 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1793.5838623046875
I0405 05:18:16.780780 140200300066560 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1808.1524658203125
I0405 05:19:39.798835 140200291673856 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1828.5587158203125
I0405 05:21:04.470725 140200300066560 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1752.9322509765625
I0405 05:22:22.501129 140200300066560 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1838.0113525390625
I0405 05:23:36.213148 140200291673856 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1823.34912109375
I0405 05:24:50.128480 140200300066560 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1844.5003662109375
I0405 05:26:03.964960 140200291673856 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1837.219970703125
I0405 05:27:18.959815 140200300066560 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1878.058837890625
I0405 05:28:43.000673 140200291673856 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1816.4920654296875
I0405 05:30:06.630803 140200300066560 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1789.0712890625
I0405 05:31:29.886163 140200291673856 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1828.036376953125
I0405 05:32:53.938599 140200300066560 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1824.5186767578125
I0405 05:34:15.311752 140200291673856 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1807.8970947265625
I0405 05:35:37.159841 140200300066560 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1856.6741943359375
I0405 05:36:50.696862 140200291673856 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1843.836181640625
I0405 05:38:04.510221 140200300066560 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1804.3292236328125
I0405 05:39:18.344421 140200291673856 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1849.1636962890625
I0405 05:40:35.631767 140200300066560 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1836.29736328125
I0405 05:41:59.750952 140200291673856 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1875.03271484375
I0405 05:43:24.071521 140200300066560 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1867.6458740234375
I0405 05:44:26.453527 140374562969408 submission_runner.py:373] Before eval at step 8975: RAM USED (GB) 22.22768128
I0405 05:44:26.453754 140374562969408 spec.py:298] Evaluating on the training split.
I0405 05:44:53.497884 140374562969408 spec.py:310] Evaluating on the validation split.
I0405 05:45:28.647033 140374562969408 spec.py:326] Evaluating on the test split.
I0405 05:45:48.190582 140374562969408 submission_runner.py:382] Time since start: 7539.68s, 	Step: 8975, 	{'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 05:45:48.192245 140374562969408 submission_runner.py:396] After eval at step 8975: RAM USED (GB) 21.282639872
I0405 05:45:48.214943 140200991266560 logging_writer.py:48] [8975] global_step=8975, preemption_count=0, score=7244.240147, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=7539.684682, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0405 05:45:48.464463 140374562969408 checkpoints.py:356] Saving checkpoint at step: 8975
I0405 05:45:49.567048 140374562969408 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_8975
I0405 05:45:49.588584 140374562969408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_8975.
I0405 05:45:49.604654 140374562969408 submission_runner.py:416] After logging and checkpointing eval at step 8975: RAM USED (GB) 21.294751744
I0405 05:46:08.697960 140200982873856 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1829.3426513671875
I0405 05:47:22.493855 140200591906560 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1872.7010498046875
I0405 05:48:36.084085 140200982873856 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1852.7772216796875
I0405 05:49:54.690423 140200663586560 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1857.3477783203125
I0405 05:51:08.325988 140200655193856 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1813.275390625
I0405 05:52:22.014359 140200663586560 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1841.97900390625
I0405 05:53:35.578495 140200655193856 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1799.0035400390625
I0405 05:54:49.568613 140200663586560 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1804.2020263671875
I0405 05:56:11.507958 140200655193856 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1883.170654296875
I0405 05:57:36.589459 140200663586560 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1822.4405517578125
I0405 05:58:59.524638 140374562969408 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 22.847991808
I0405 05:58:59.524848 140374562969408 spec.py:298] Evaluating on the training split.
I0405 05:59:26.056456 140374562969408 spec.py:310] Evaluating on the validation split.
I0405 06:00:01.608953 140374562969408 spec.py:326] Evaluating on the test split.
I0405 06:00:20.844833 140374562969408 submission_runner.py:382] Time since start: 8412.76s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 06:00:20.846101 140374562969408 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 21.729509376
I0405 06:00:20.863054 140201144866560 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=8032.159474, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=8412.756447, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0405 06:00:21.048549 140374562969408 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 06:00:21.983353 140374562969408 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_10000
I0405 06:00:22.004982 140374562969408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0405 06:00:22.016171 140374562969408 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.742665728
I0405 06:00:22.023411 140201136473856 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=8032.159474
I0405 06:00:22.179259 140374562969408 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 06:00:23.489147 140374562969408 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_10000
I0405 06:00:23.510719 140374562969408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0405 06:00:25.001403 140374562969408 submission_runner.py:550] Tuning trial 1/1
I0405 06:00:25.001636 140374562969408 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 06:00:25.005672 140374562969408 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.411068, dtype=float32), 'train/wer': 0.9518199612685588, 'validation/ctc_loss': DeviceArray(30.424536, dtype=float32), 'validation/wer': 0.9717508128394872, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.636398, dtype=float32), 'test/wer': 0.9788353340239271, 'test/num_examples': 2472, 'score': 59.61101150512695, 'total_duration': 59.80976057052612, 'global_step': 1, 'preemption_count': 0}), (2957, {'train/ctc_loss': DeviceArray(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2454.4530730247498, 'total_duration': 2568.702260017395, 'global_step': 2957, 'preemption_count': 0}), (5956, {'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4849.685252666473, 'total_duration': 5054.578206062317, 'global_step': 5956, 'preemption_count': 0}), (8975, {'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7244.2401468753815, 'total_duration': 7539.684681653976, 'global_step': 8975, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8032.159473657608, 'total_duration': 8412.756446838379, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0405 06:00:25.005829 140374562969408 submission_runner.py:553] Timing: 8032.159473657608
I0405 06:00:25.005880 140374562969408 submission_runner.py:554] ====================
I0405 06:00:25.006225 140374562969408 submission_runner.py:613] Final librispeech_conformer score: 8032.159473657608
