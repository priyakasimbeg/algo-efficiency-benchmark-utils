WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 23:46:35.600077 140307734935360 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 23:46:35.600108 139678405625664 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 23:46:35.600133 140406412138304 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 23:46:35.601121 139812139226944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 23:46:35.601174 139701627266880 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 23:46:35.601202 140337230477120 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 23:46:35.601440 139812139226944 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:46:35.601261 140543227336512 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 23:46:35.601523 139701627266880 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:46:35.601552 140337230477120 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:46:35.601455 140415612692288 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 23:46:35.601643 140543227336512 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:46:35.601855 140415612692288 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:46:35.610728 140307734935360 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:46:35.610757 139678405625664 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:46:35.610785 140406412138304 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:46:36.149197 139812139226944 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch.
W0405 23:46:36.186817 139701627266880 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:46:36.186814 140543227336512 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:46:36.186861 140406412138304 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:46:36.187911 139812139226944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:46:36.188234 140337230477120 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:46:36.188882 140415612692288 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:46:36.189120 140307734935360 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:46:36.189707 139678405625664 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 23:46:36.192527 139812139226944 submission_runner.py:511] Using RNG seed 3622829885
I0405 23:46:36.193505 139812139226944 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 23:46:36.193613 139812139226944 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1.
I0405 23:46:36.193786 139812139226944 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/hparams.json.
I0405 23:46:36.194638 139812139226944 submission_runner.py:230] Starting train once: RAM USED (GB) 5.582680064
I0405 23:46:36.194729 139812139226944 submission_runner.py:231] Initializing dataset.
I0405 23:46:36.194894 139812139226944 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.582680064
I0405 23:46:36.194952 139812139226944 submission_runner.py:240] Initializing model.
I0405 23:46:40.270681 139812139226944 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.221231616
I0405 23:46:40.270949 139812139226944 submission_runner.py:252] Initializing optimizer.
I0405 23:46:40.271790 139812139226944 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.221231616
I0405 23:46:40.271917 139812139226944 submission_runner.py:261] Initializing metrics bundle.
I0405 23:46:40.271976 139812139226944 submission_runner.py:276] Initializing checkpoint and logger.
I0405 23:46:40.275714 139812139226944 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 23:46:40.275811 139812139226944 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 23:46:40.876064 139812139226944 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/meta_data_0.json.
I0405 23:46:40.876871 139812139226944 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/flags_0.json.
I0405 23:46:40.916877 139812139226944 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.274418176
I0405 23:46:40.917972 139812139226944 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.274418176
I0405 23:46:40.918106 139812139226944 submission_runner.py:313] Starting training loop.
I0405 23:47:25.994826 139812139226944 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 24.0374784
I0405 23:47:30.111553 139770265065216 logging_writer.py:48] [0] global_step=0, grad_norm=3.555728, loss=0.981741
I0405 23:47:30.118869 139812139226944 submission.py:119] 0) loss = 0.982, grad_norm = 3.556
I0405 23:47:30.119762 139812139226944 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 32.883703808
I0405 23:47:30.120707 139812139226944 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 32.884736
I0405 23:47:30.120874 139812139226944 spec.py:298] Evaluating on the training split.
I0405 23:49:09.004641 139812139226944 spec.py:310] Evaluating on the validation split.
I0405 23:50:12.070850 139812139226944 spec.py:326] Evaluating on the test split.
I0405 23:51:14.175408 139812139226944 submission_runner.py:382] Time since start: 49.20s, 	Step: 1, 	{'train/ssim': 0.2054400954927717, 'train/loss': 1.0082744189671107, 'validation/ssim': 0.1999599476107467, 'validation/loss': 1.0065798440137872, 'validation/num_examples': 3554, 'test/ssim': 0.22305075425204202, 'test/loss': 1.0047498680099833, 'test/num_examples': 3581}
I0405 23:51:14.175802 139812139226944 submission_runner.py:396] After eval at step 1: RAM USED (GB) 68.413886464
I0405 23:51:14.183928 139752279873280 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=49.200939, test/loss=1.004750, test/num_examples=3581, test/ssim=0.223051, total_duration=49.202911, train/loss=1.008274, train/ssim=0.205440, validation/loss=1.006580, validation/num_examples=3554, validation/ssim=0.199960
I0405 23:51:14.329388 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_1.
I0405 23:51:14.329897 139812139226944 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 68.415680512
I0405 23:51:14.338703 139812139226944 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 68.427382784
I0405 23:51:14.342100 139701627266880 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:51:14.342105 140337230477120 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:51:14.342107 140406412138304 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:51:14.342130 140543227336512 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:51:14.342139 140307734935360 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:51:14.342169 139678405625664 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:51:14.342230 140415612692288 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:51:14.342950 139812139226944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:51:14.402778 139752196011776 logging_writer.py:48] [1] global_step=1, grad_norm=3.954084, loss=0.994822
I0405 23:51:14.408304 139812139226944 submission.py:119] 1) loss = 0.995, grad_norm = 3.954
I0405 23:51:14.409276 139812139226944 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 68.445446144
I0405 23:51:14.479156 139752279873280 logging_writer.py:48] [2] global_step=2, grad_norm=3.524115, loss=0.986500
I0405 23:51:14.484827 139812139226944 submission.py:119] 2) loss = 0.987, grad_norm = 3.524
I0405 23:51:14.551569 139752196011776 logging_writer.py:48] [3] global_step=3, grad_norm=3.578910, loss=1.004319
I0405 23:51:14.554952 139812139226944 submission.py:119] 3) loss = 1.004, grad_norm = 3.579
I0405 23:51:14.618455 139752279873280 logging_writer.py:48] [4] global_step=4, grad_norm=3.644137, loss=0.976575
I0405 23:51:14.622157 139812139226944 submission.py:119] 4) loss = 0.977, grad_norm = 3.644
I0405 23:51:14.685568 139752196011776 logging_writer.py:48] [5] global_step=5, grad_norm=3.433054, loss=0.986194
I0405 23:51:14.688747 139812139226944 submission.py:119] 5) loss = 0.986, grad_norm = 3.433
I0405 23:51:14.758004 139752279873280 logging_writer.py:48] [6] global_step=6, grad_norm=3.378486, loss=0.998940
I0405 23:51:14.763867 139812139226944 submission.py:119] 6) loss = 0.999, grad_norm = 3.378
I0405 23:51:14.834478 139752196011776 logging_writer.py:48] [7] global_step=7, grad_norm=3.145739, loss=0.970888
I0405 23:51:14.840290 139812139226944 submission.py:119] 7) loss = 0.971, grad_norm = 3.146
I0405 23:51:14.910062 139752279873280 logging_writer.py:48] [8] global_step=8, grad_norm=3.529286, loss=0.998476
I0405 23:51:14.913782 139812139226944 submission.py:119] 8) loss = 0.998, grad_norm = 3.529
I0405 23:51:14.981027 139752196011776 logging_writer.py:48] [9] global_step=9, grad_norm=3.343696, loss=0.994904
I0405 23:51:14.985951 139812139226944 submission.py:119] 9) loss = 0.995, grad_norm = 3.344
I0405 23:51:15.062525 139752279873280 logging_writer.py:48] [10] global_step=10, grad_norm=3.379054, loss=0.915921
I0405 23:51:15.068389 139812139226944 submission.py:119] 10) loss = 0.916, grad_norm = 3.379
I0405 23:51:15.140420 139752196011776 logging_writer.py:48] [11] global_step=11, grad_norm=3.252149, loss=0.958362
I0405 23:51:15.146550 139812139226944 submission.py:119] 11) loss = 0.958, grad_norm = 3.252
I0405 23:51:15.211927 139752279873280 logging_writer.py:48] [12] global_step=12, grad_norm=3.249414, loss=0.943623
I0405 23:51:15.215229 139812139226944 submission.py:119] 12) loss = 0.944, grad_norm = 3.249
I0405 23:51:15.279338 139752196011776 logging_writer.py:48] [13] global_step=13, grad_norm=3.241658, loss=0.990690
I0405 23:51:15.284870 139812139226944 submission.py:119] 13) loss = 0.991, grad_norm = 3.242
I0405 23:51:15.503241 139752279873280 logging_writer.py:48] [14] global_step=14, grad_norm=3.256872, loss=0.885369
I0405 23:51:15.506820 139812139226944 submission.py:119] 14) loss = 0.885, grad_norm = 3.257
I0405 23:51:15.778972 139752196011776 logging_writer.py:48] [15] global_step=15, grad_norm=3.104133, loss=0.898784
I0405 23:51:15.783822 139812139226944 submission.py:119] 15) loss = 0.899, grad_norm = 3.104
I0405 23:51:16.023546 139752279873280 logging_writer.py:48] [16] global_step=16, grad_norm=3.223417, loss=0.887972
I0405 23:51:16.027908 139812139226944 submission.py:119] 16) loss = 0.888, grad_norm = 3.223
I0405 23:51:16.376889 139752196011776 logging_writer.py:48] [17] global_step=17, grad_norm=3.096025, loss=0.878030
I0405 23:51:16.381362 139812139226944 submission.py:119] 17) loss = 0.878, grad_norm = 3.096
I0405 23:51:16.663544 139752279873280 logging_writer.py:48] [18] global_step=18, grad_norm=3.160987, loss=0.872102
I0405 23:51:16.667559 139812139226944 submission.py:119] 18) loss = 0.872, grad_norm = 3.161
I0405 23:51:16.922834 139752196011776 logging_writer.py:48] [19] global_step=19, grad_norm=2.852081, loss=0.821112
I0405 23:51:16.927562 139812139226944 submission.py:119] 19) loss = 0.821, grad_norm = 2.852
I0405 23:51:17.169256 139752279873280 logging_writer.py:48] [20] global_step=20, grad_norm=2.829361, loss=0.805792
I0405 23:51:17.175102 139812139226944 submission.py:119] 20) loss = 0.806, grad_norm = 2.829
I0405 23:51:17.437046 139752196011776 logging_writer.py:48] [21] global_step=21, grad_norm=2.864253, loss=0.810143
I0405 23:51:17.441919 139812139226944 submission.py:119] 21) loss = 0.810, grad_norm = 2.864
I0405 23:51:17.699031 139752279873280 logging_writer.py:48] [22] global_step=22, grad_norm=2.866708, loss=0.777705
I0405 23:51:17.704469 139812139226944 submission.py:119] 22) loss = 0.778, grad_norm = 2.867
I0405 23:51:17.987349 139752196011776 logging_writer.py:48] [23] global_step=23, grad_norm=2.695270, loss=0.751593
I0405 23:51:17.992157 139812139226944 submission.py:119] 23) loss = 0.752, grad_norm = 2.695
I0405 23:51:18.281668 139752279873280 logging_writer.py:48] [24] global_step=24, grad_norm=2.154151, loss=0.812052
I0405 23:51:18.287243 139812139226944 submission.py:119] 24) loss = 0.812, grad_norm = 2.154
I0405 23:51:18.598700 139752196011776 logging_writer.py:48] [25] global_step=25, grad_norm=2.299900, loss=0.796919
I0405 23:51:18.604362 139812139226944 submission.py:119] 25) loss = 0.797, grad_norm = 2.300
I0405 23:51:18.833764 139752279873280 logging_writer.py:48] [26] global_step=26, grad_norm=2.307695, loss=0.733710
I0405 23:51:18.837261 139812139226944 submission.py:119] 26) loss = 0.734, grad_norm = 2.308
I0405 23:51:19.088788 139752196011776 logging_writer.py:48] [27] global_step=27, grad_norm=2.101698, loss=0.727572
I0405 23:51:19.092356 139812139226944 submission.py:119] 27) loss = 0.728, grad_norm = 2.102
I0405 23:51:19.344832 139752279873280 logging_writer.py:48] [28] global_step=28, grad_norm=2.374058, loss=0.712823
I0405 23:51:19.348126 139812139226944 submission.py:119] 28) loss = 0.713, grad_norm = 2.374
I0405 23:51:19.687624 139752196011776 logging_writer.py:48] [29] global_step=29, grad_norm=2.132299, loss=0.713310
I0405 23:51:19.693343 139812139226944 submission.py:119] 29) loss = 0.713, grad_norm = 2.132
I0405 23:51:19.951641 139752279873280 logging_writer.py:48] [30] global_step=30, grad_norm=1.789753, loss=0.683264
I0405 23:51:19.954782 139812139226944 submission.py:119] 30) loss = 0.683, grad_norm = 1.790
I0405 23:51:20.169924 139752196011776 logging_writer.py:48] [31] global_step=31, grad_norm=1.638136, loss=0.670733
I0405 23:51:20.175370 139812139226944 submission.py:119] 31) loss = 0.671, grad_norm = 1.638
I0405 23:51:20.435921 139752279873280 logging_writer.py:48] [32] global_step=32, grad_norm=1.692831, loss=0.703532
I0405 23:51:20.439123 139812139226944 submission.py:119] 32) loss = 0.704, grad_norm = 1.693
I0405 23:51:20.709911 139752196011776 logging_writer.py:48] [33] global_step=33, grad_norm=1.623768, loss=0.675354
I0405 23:51:20.716857 139812139226944 submission.py:119] 33) loss = 0.675, grad_norm = 1.624
I0405 23:51:20.963198 139752279873280 logging_writer.py:48] [34] global_step=34, grad_norm=1.462147, loss=0.650949
I0405 23:51:20.971466 139812139226944 submission.py:119] 34) loss = 0.651, grad_norm = 1.462
I0405 23:51:21.251745 139752196011776 logging_writer.py:48] [35] global_step=35, grad_norm=1.316003, loss=0.721845
I0405 23:51:21.256412 139812139226944 submission.py:119] 35) loss = 0.722, grad_norm = 1.316
I0405 23:51:21.495341 139752279873280 logging_writer.py:48] [36] global_step=36, grad_norm=1.425945, loss=0.636623
I0405 23:51:21.501339 139812139226944 submission.py:119] 36) loss = 0.637, grad_norm = 1.426
I0405 23:51:21.779294 139752196011776 logging_writer.py:48] [37] global_step=37, grad_norm=1.473346, loss=0.572230
I0405 23:51:21.784130 139812139226944 submission.py:119] 37) loss = 0.572, grad_norm = 1.473
I0405 23:51:22.006536 139752279873280 logging_writer.py:48] [38] global_step=38, grad_norm=1.240904, loss=0.605396
I0405 23:51:22.012607 139812139226944 submission.py:119] 38) loss = 0.605, grad_norm = 1.241
I0405 23:51:22.303735 139752196011776 logging_writer.py:48] [39] global_step=39, grad_norm=1.309919, loss=0.572825
I0405 23:51:22.309452 139812139226944 submission.py:119] 39) loss = 0.573, grad_norm = 1.310
I0405 23:51:22.515764 139752279873280 logging_writer.py:48] [40] global_step=40, grad_norm=1.259744, loss=0.591001
I0405 23:51:22.522877 139812139226944 submission.py:119] 40) loss = 0.591, grad_norm = 1.260
I0405 23:51:22.822516 139752196011776 logging_writer.py:48] [41] global_step=41, grad_norm=1.195286, loss=0.558309
I0405 23:51:22.830573 139812139226944 submission.py:119] 41) loss = 0.558, grad_norm = 1.195
I0405 23:51:23.099199 139752279873280 logging_writer.py:48] [42] global_step=42, grad_norm=1.031343, loss=0.601425
I0405 23:51:23.105080 139812139226944 submission.py:119] 42) loss = 0.601, grad_norm = 1.031
I0405 23:51:23.332505 139752196011776 logging_writer.py:48] [43] global_step=43, grad_norm=1.114081, loss=0.552760
I0405 23:51:23.335680 139812139226944 submission.py:119] 43) loss = 0.553, grad_norm = 1.114
I0405 23:51:23.630436 139752279873280 logging_writer.py:48] [44] global_step=44, grad_norm=1.095036, loss=0.609983
I0405 23:51:23.633747 139812139226944 submission.py:119] 44) loss = 0.610, grad_norm = 1.095
I0405 23:51:23.924060 139752196011776 logging_writer.py:48] [45] global_step=45, grad_norm=1.113133, loss=0.516499
I0405 23:51:23.927179 139812139226944 submission.py:119] 45) loss = 0.516, grad_norm = 1.113
I0405 23:51:24.210963 139752279873280 logging_writer.py:48] [46] global_step=46, grad_norm=1.060166, loss=0.569608
I0405 23:51:24.214886 139812139226944 submission.py:119] 46) loss = 0.570, grad_norm = 1.060
I0405 23:51:24.478743 139752196011776 logging_writer.py:48] [47] global_step=47, grad_norm=1.076842, loss=0.514649
I0405 23:51:24.482114 139812139226944 submission.py:119] 47) loss = 0.515, grad_norm = 1.077
I0405 23:51:24.729423 139752279873280 logging_writer.py:48] [48] global_step=48, grad_norm=1.020840, loss=0.554474
I0405 23:51:24.732692 139812139226944 submission.py:119] 48) loss = 0.554, grad_norm = 1.021
I0405 23:51:24.997565 139752196011776 logging_writer.py:48] [49] global_step=49, grad_norm=0.898262, loss=0.617956
I0405 23:51:25.000876 139812139226944 submission.py:119] 49) loss = 0.618, grad_norm = 0.898
I0405 23:51:25.257850 139752279873280 logging_writer.py:48] [50] global_step=50, grad_norm=0.801965, loss=0.602366
I0405 23:51:25.263212 139812139226944 submission.py:119] 50) loss = 0.602, grad_norm = 0.802
I0405 23:51:25.571984 139752196011776 logging_writer.py:48] [51] global_step=51, grad_norm=0.950927, loss=0.533277
I0405 23:51:25.575269 139812139226944 submission.py:119] 51) loss = 0.533, grad_norm = 0.951
I0405 23:51:25.796994 139752279873280 logging_writer.py:48] [52] global_step=52, grad_norm=0.908691, loss=0.538245
I0405 23:51:25.801942 139812139226944 submission.py:119] 52) loss = 0.538, grad_norm = 0.909
I0405 23:51:26.047103 139752196011776 logging_writer.py:48] [53] global_step=53, grad_norm=0.888293, loss=0.589919
I0405 23:51:26.052843 139812139226944 submission.py:119] 53) loss = 0.590, grad_norm = 0.888
I0405 23:51:26.314210 139752279873280 logging_writer.py:48] [54] global_step=54, grad_norm=1.054018, loss=0.518738
I0405 23:51:26.319327 139812139226944 submission.py:119] 54) loss = 0.519, grad_norm = 1.054
I0405 23:51:26.585328 139752196011776 logging_writer.py:48] [55] global_step=55, grad_norm=0.979036, loss=0.505755
I0405 23:51:26.591326 139812139226944 submission.py:119] 55) loss = 0.506, grad_norm = 0.979
I0405 23:51:26.855449 139752279873280 logging_writer.py:48] [56] global_step=56, grad_norm=0.994243, loss=0.500393
I0405 23:51:26.861156 139812139226944 submission.py:119] 56) loss = 0.500, grad_norm = 0.994
I0405 23:51:27.102988 139752196011776 logging_writer.py:48] [57] global_step=57, grad_norm=1.017520, loss=0.474514
I0405 23:51:27.106551 139812139226944 submission.py:119] 57) loss = 0.475, grad_norm = 1.018
I0405 23:51:27.383477 139752279873280 logging_writer.py:48] [58] global_step=58, grad_norm=0.901623, loss=0.595364
I0405 23:51:27.388267 139812139226944 submission.py:119] 58) loss = 0.595, grad_norm = 0.902
I0405 23:51:27.677018 139752196011776 logging_writer.py:48] [59] global_step=59, grad_norm=0.764838, loss=0.544205
I0405 23:51:27.683173 139812139226944 submission.py:119] 59) loss = 0.544, grad_norm = 0.765
I0405 23:51:27.893911 139752279873280 logging_writer.py:48] [60] global_step=60, grad_norm=0.961190, loss=0.448924
I0405 23:51:27.898499 139812139226944 submission.py:119] 60) loss = 0.449, grad_norm = 0.961
I0405 23:51:28.181056 139752196011776 logging_writer.py:48] [61] global_step=61, grad_norm=0.775346, loss=0.448084
I0405 23:51:28.186439 139812139226944 submission.py:119] 61) loss = 0.448, grad_norm = 0.775
I0405 23:51:28.481655 139752279873280 logging_writer.py:48] [62] global_step=62, grad_norm=0.846614, loss=0.419635
I0405 23:51:28.487253 139812139226944 submission.py:119] 62) loss = 0.420, grad_norm = 0.847
I0405 23:51:28.739359 139752196011776 logging_writer.py:48] [63] global_step=63, grad_norm=0.855240, loss=0.547502
I0405 23:51:28.743340 139812139226944 submission.py:119] 63) loss = 0.548, grad_norm = 0.855
I0405 23:51:29.003349 139752279873280 logging_writer.py:48] [64] global_step=64, grad_norm=0.906741, loss=0.464971
I0405 23:51:29.007145 139812139226944 submission.py:119] 64) loss = 0.465, grad_norm = 0.907
I0405 23:51:29.210408 139752196011776 logging_writer.py:48] [65] global_step=65, grad_norm=0.719711, loss=0.503779
I0405 23:51:29.215726 139812139226944 submission.py:119] 65) loss = 0.504, grad_norm = 0.720
I0405 23:51:29.523807 139752279873280 logging_writer.py:48] [66] global_step=66, grad_norm=0.991605, loss=0.390021
I0405 23:51:29.528213 139812139226944 submission.py:119] 66) loss = 0.390, grad_norm = 0.992
I0405 23:51:29.802893 139752196011776 logging_writer.py:48] [67] global_step=67, grad_norm=0.850367, loss=0.470192
I0405 23:51:29.806169 139812139226944 submission.py:119] 67) loss = 0.470, grad_norm = 0.850
I0405 23:51:30.047896 139752279873280 logging_writer.py:48] [68] global_step=68, grad_norm=0.723960, loss=0.452245
I0405 23:51:30.052518 139812139226944 submission.py:119] 68) loss = 0.452, grad_norm = 0.724
I0405 23:51:30.298921 139752196011776 logging_writer.py:48] [69] global_step=69, grad_norm=0.765128, loss=0.462425
I0405 23:51:30.307064 139812139226944 submission.py:119] 69) loss = 0.462, grad_norm = 0.765
I0405 23:51:30.543253 139752279873280 logging_writer.py:48] [70] global_step=70, grad_norm=0.815057, loss=0.425072
I0405 23:51:30.546656 139812139226944 submission.py:119] 70) loss = 0.425, grad_norm = 0.815
I0405 23:51:30.804275 139752196011776 logging_writer.py:48] [71] global_step=71, grad_norm=0.754869, loss=0.423561
I0405 23:51:30.807709 139812139226944 submission.py:119] 71) loss = 0.424, grad_norm = 0.755
I0405 23:51:31.028689 139752279873280 logging_writer.py:48] [72] global_step=72, grad_norm=0.702398, loss=0.434101
I0405 23:51:31.032317 139812139226944 submission.py:119] 72) loss = 0.434, grad_norm = 0.702
I0405 23:51:31.357613 139752196011776 logging_writer.py:48] [73] global_step=73, grad_norm=0.733386, loss=0.384993
I0405 23:51:31.362653 139812139226944 submission.py:119] 73) loss = 0.385, grad_norm = 0.733
I0405 23:51:31.608406 139752279873280 logging_writer.py:48] [74] global_step=74, grad_norm=0.890354, loss=0.395493
I0405 23:51:31.613226 139812139226944 submission.py:119] 74) loss = 0.395, grad_norm = 0.890
I0405 23:51:31.854613 139752196011776 logging_writer.py:48] [75] global_step=75, grad_norm=0.794307, loss=0.390510
I0405 23:51:31.859503 139812139226944 submission.py:119] 75) loss = 0.391, grad_norm = 0.794
I0405 23:51:32.150544 139752279873280 logging_writer.py:48] [76] global_step=76, grad_norm=0.778097, loss=0.430639
I0405 23:51:32.156044 139812139226944 submission.py:119] 76) loss = 0.431, grad_norm = 0.778
I0405 23:51:32.388820 139752196011776 logging_writer.py:48] [77] global_step=77, grad_norm=0.728197, loss=0.419920
I0405 23:51:32.394804 139812139226944 submission.py:119] 77) loss = 0.420, grad_norm = 0.728
I0405 23:51:32.619014 139752279873280 logging_writer.py:48] [78] global_step=78, grad_norm=0.635737, loss=0.403097
I0405 23:51:32.623774 139812139226944 submission.py:119] 78) loss = 0.403, grad_norm = 0.636
I0405 23:51:32.929695 139752196011776 logging_writer.py:48] [79] global_step=79, grad_norm=0.869873, loss=0.354150
I0405 23:51:32.932773 139812139226944 submission.py:119] 79) loss = 0.354, grad_norm = 0.870
I0405 23:51:33.267646 139752279873280 logging_writer.py:48] [80] global_step=80, grad_norm=0.650554, loss=0.407370
I0405 23:51:33.272573 139812139226944 submission.py:119] 80) loss = 0.407, grad_norm = 0.651
I0405 23:51:33.541038 139752196011776 logging_writer.py:48] [81] global_step=81, grad_norm=0.762853, loss=0.316238
I0405 23:51:33.544332 139812139226944 submission.py:119] 81) loss = 0.316, grad_norm = 0.763
I0405 23:51:33.824635 139752279873280 logging_writer.py:48] [82] global_step=82, grad_norm=0.649013, loss=0.367068
I0405 23:51:33.827733 139812139226944 submission.py:119] 82) loss = 0.367, grad_norm = 0.649
I0405 23:51:34.080400 139752196011776 logging_writer.py:48] [83] global_step=83, grad_norm=0.628793, loss=0.349981
I0405 23:51:34.084091 139812139226944 submission.py:119] 83) loss = 0.350, grad_norm = 0.629
I0405 23:51:34.351285 139752279873280 logging_writer.py:48] [84] global_step=84, grad_norm=0.691737, loss=0.319989
I0405 23:51:34.357361 139812139226944 submission.py:119] 84) loss = 0.320, grad_norm = 0.692
I0405 23:51:34.583507 139752196011776 logging_writer.py:48] [85] global_step=85, grad_norm=0.567685, loss=0.416569
I0405 23:51:34.588354 139812139226944 submission.py:119] 85) loss = 0.417, grad_norm = 0.568
I0405 23:51:34.902286 139752279873280 logging_writer.py:48] [86] global_step=86, grad_norm=0.627087, loss=0.336155
I0405 23:51:34.907650 139812139226944 submission.py:119] 86) loss = 0.336, grad_norm = 0.627
I0405 23:51:35.115154 139752196011776 logging_writer.py:48] [87] global_step=87, grad_norm=0.555258, loss=0.440021
I0405 23:51:35.118390 139812139226944 submission.py:119] 87) loss = 0.440, grad_norm = 0.555
I0405 23:51:35.396174 139752279873280 logging_writer.py:48] [88] global_step=88, grad_norm=0.448366, loss=0.359514
I0405 23:51:35.399946 139812139226944 submission.py:119] 88) loss = 0.360, grad_norm = 0.448
I0405 23:51:35.682587 139752196011776 logging_writer.py:48] [89] global_step=89, grad_norm=0.619317, loss=0.321396
I0405 23:51:35.686203 139812139226944 submission.py:119] 89) loss = 0.321, grad_norm = 0.619
I0405 23:51:35.952989 139752279873280 logging_writer.py:48] [90] global_step=90, grad_norm=0.567851, loss=0.324031
I0405 23:51:35.956370 139812139226944 submission.py:119] 90) loss = 0.324, grad_norm = 0.568
I0405 23:51:36.194784 139752196011776 logging_writer.py:48] [91] global_step=91, grad_norm=0.518164, loss=0.378729
I0405 23:51:36.199213 139812139226944 submission.py:119] 91) loss = 0.379, grad_norm = 0.518
I0405 23:51:36.473382 139752279873280 logging_writer.py:48] [92] global_step=92, grad_norm=0.595653, loss=0.316911
I0405 23:51:36.477978 139812139226944 submission.py:119] 92) loss = 0.317, grad_norm = 0.596
I0405 23:51:36.792895 139752196011776 logging_writer.py:48] [93] global_step=93, grad_norm=0.533951, loss=0.340876
I0405 23:51:36.798631 139812139226944 submission.py:119] 93) loss = 0.341, grad_norm = 0.534
I0405 23:51:37.062294 139752279873280 logging_writer.py:48] [94] global_step=94, grad_norm=0.458574, loss=0.389668
I0405 23:51:37.066135 139812139226944 submission.py:119] 94) loss = 0.390, grad_norm = 0.459
I0405 23:51:37.347182 139752196011776 logging_writer.py:48] [95] global_step=95, grad_norm=0.470377, loss=0.305956
I0405 23:51:37.350688 139812139226944 submission.py:119] 95) loss = 0.306, grad_norm = 0.470
I0405 23:51:37.594043 139752279873280 logging_writer.py:48] [96] global_step=96, grad_norm=0.538412, loss=0.280824
I0405 23:51:37.597431 139812139226944 submission.py:119] 96) loss = 0.281, grad_norm = 0.538
I0405 23:51:37.889565 139752196011776 logging_writer.py:48] [97] global_step=97, grad_norm=0.575873, loss=0.293374
I0405 23:51:37.895091 139812139226944 submission.py:119] 97) loss = 0.293, grad_norm = 0.576
I0405 23:51:38.151576 139752279873280 logging_writer.py:48] [98] global_step=98, grad_norm=0.501719, loss=0.326465
I0405 23:51:38.154742 139812139226944 submission.py:119] 98) loss = 0.326, grad_norm = 0.502
I0405 23:51:38.339729 139752196011776 logging_writer.py:48] [99] global_step=99, grad_norm=0.358367, loss=0.381886
I0405 23:51:38.345050 139812139226944 submission.py:119] 99) loss = 0.382, grad_norm = 0.358
I0405 23:51:38.607844 139752279873280 logging_writer.py:48] [100] global_step=100, grad_norm=0.534041, loss=0.276854
I0405 23:51:38.612794 139812139226944 submission.py:119] 100) loss = 0.277, grad_norm = 0.534
I0405 23:52:34.552860 139812139226944 submission_runner.py:373] Before eval at step 310: RAM USED (GB) 86.017810432
I0405 23:52:34.553144 139812139226944 spec.py:298] Evaluating on the training split.
I0405 23:52:36.465870 139812139226944 spec.py:310] Evaluating on the validation split.
I0405 23:52:40.971305 139812139226944 spec.py:326] Evaluating on the test split.
I0405 23:52:43.460199 139812139226944 submission_runner.py:382] Time since start: 353.62s, 	Step: 310, 	{'train/ssim': 0.7054905891418457, 'train/loss': 0.2995874881744385, 'validation/ssim': 0.6828125549556837, 'validation/loss': 0.32252302368370145, 'validation/num_examples': 3554, 'test/ssim': 0.7005799652244484, 'test/loss': 0.3245725868734292, 'test/num_examples': 3581}
I0405 23:52:43.460569 139812139226944 submission_runner.py:396] After eval at step 310: RAM USED (GB) 87.021420544
I0405 23:52:43.469835 139752196011776 logging_writer.py:48] [310] global_step=310, preemption_count=0, score=125.725512, test/loss=0.324573, test/num_examples=3581, test/ssim=0.700580, total_duration=353.618182, train/loss=0.299587, train/ssim=0.705491, validation/loss=0.322523, validation/num_examples=3554, validation/ssim=0.682813
I0405 23:52:43.622646 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_310.
I0405 23:52:43.623313 139812139226944 submission_runner.py:416] After logging and checkpointing eval at step 310: RAM USED (GB) 87.03479808
I0405 23:53:51.786111 139752279873280 logging_writer.py:48] [500] global_step=500, grad_norm=0.411905, loss=0.238342
I0405 23:53:51.791598 139812139226944 submission.py:119] 500) loss = 0.238, grad_norm = 0.412
I0405 23:54:03.720222 139812139226944 submission_runner.py:373] Before eval at step 532: RAM USED (GB) 104.575684608
I0405 23:54:03.720405 139812139226944 spec.py:298] Evaluating on the training split.
I0405 23:54:05.665464 139812139226944 spec.py:310] Evaluating on the validation split.
I0405 23:54:08.184834 139812139226944 spec.py:326] Evaluating on the test split.
I0405 23:54:10.269604 139812139226944 submission_runner.py:382] Time since start: 442.79s, 	Step: 532, 	{'train/ssim': 0.7180110386439732, 'train/loss': 0.28861587388174875, 'validation/ssim': 0.694719047310249, 'validation/loss': 0.3113878686976822, 'validation/num_examples': 3554, 'test/ssim': 0.7124120248359397, 'test/loss': 0.3133251080463732, 'test/num_examples': 3581}
I0405 23:54:10.270049 139812139226944 submission_runner.py:396] After eval at step 532: RAM USED (GB) 105.723953152
I0405 23:54:10.282392 139752196011776 logging_writer.py:48] [532] global_step=532, preemption_count=0, score=201.506368, test/loss=0.313325, test/num_examples=3581, test/ssim=0.712412, total_duration=442.785773, train/loss=0.288616, train/ssim=0.718011, validation/loss=0.311388, validation/num_examples=3554, validation/ssim=0.694719
I0405 23:54:10.439652 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_532.
I0405 23:54:10.441830 139812139226944 submission_runner.py:416] After logging and checkpointing eval at step 532: RAM USED (GB) 105.743253504
I0405 23:55:30.630820 139812139226944 submission_runner.py:373] Before eval at step 764: RAM USED (GB) 123.355136
I0405 23:55:30.631095 139812139226944 spec.py:298] Evaluating on the training split.
I0405 23:55:32.656949 139812139226944 spec.py:310] Evaluating on the validation split.
I0405 23:55:35.330607 139812139226944 spec.py:326] Evaluating on the test split.
I0405 23:55:37.509359 139812139226944 submission_runner.py:382] Time since start: 529.70s, 	Step: 764, 	{'train/ssim': 0.7231929642813546, 'train/loss': 0.2837950161525181, 'validation/ssim': 0.7004588250035172, 'validation/loss': 0.30599256009953574, 'validation/num_examples': 3554, 'test/ssim': 0.7179680818687169, 'test/loss': 0.3077219068521363, 'test/num_examples': 3581}
I0405 23:55:37.509823 139812139226944 submission_runner.py:396] After eval at step 764: RAM USED (GB) 124.501676032
I0405 23:55:37.522859 139752279873280 logging_writer.py:48] [764] global_step=764, preemption_count=0, score=277.551164, test/loss=0.307722, test/num_examples=3581, test/ssim=0.717968, total_duration=529.696411, train/loss=0.283795, train/ssim=0.723193, validation/loss=0.305993, validation/num_examples=3554, validation/ssim=0.700459
I0405 23:55:37.686644 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_764.
I0405 23:55:37.688405 139812139226944 submission_runner.py:416] After logging and checkpointing eval at step 764: RAM USED (GB) 124.520738816
I0405 23:56:57.960634 139752196011776 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.431122, loss=0.315394
I0405 23:56:57.966435 139812139226944 submission.py:119] 1000) loss = 0.315, grad_norm = 0.431
I0405 23:56:57.968102 139812139226944 submission_runner.py:373] Before eval at step 1001: RAM USED (GB) 142.147944448
I0405 23:56:57.968229 139812139226944 spec.py:298] Evaluating on the training split.
I0405 23:56:59.969903 139812139226944 spec.py:310] Evaluating on the validation split.
I0405 23:57:02.915405 139812139226944 spec.py:326] Evaluating on the test split.
I0405 23:57:05.837743 139812139226944 submission_runner.py:382] Time since start: 617.05s, 	Step: 1001, 	{'train/ssim': 0.7112624304635184, 'train/loss': 0.29811876160757883, 'validation/ssim': 0.6894678256453995, 'validation/loss': 0.3208989114927722, 'validation/num_examples': 3554, 'test/ssim': 0.7065634217484641, 'test/loss': 0.32307266623106323, 'test/num_examples': 3581}
I0405 23:57:05.838122 139812139226944 submission_runner.py:396] After eval at step 1001: RAM USED (GB) 142.618365952
I0405 23:57:05.847481 139752279873280 logging_writer.py:48] [1001] global_step=1001, preemption_count=0, score=353.450806, test/loss=0.323073, test/num_examples=3581, test/ssim=0.706563, total_duration=617.050261, train/loss=0.298119, train/ssim=0.711262, validation/loss=0.320899, validation/num_examples=3554, validation/ssim=0.689468
I0405 23:57:05.995065 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_1001.
I0405 23:57:05.995680 139812139226944 submission_runner.py:416] After logging and checkpointing eval at step 1001: RAM USED (GB) 142.628212736
I0405 23:58:26.208887 139812139226944 submission_runner.py:373] Before eval at step 1315: RAM USED (GB) 143.096885248
I0405 23:58:26.209138 139812139226944 spec.py:298] Evaluating on the training split.
I0405 23:58:28.092510 139812139226944 spec.py:310] Evaluating on the validation split.
I0405 23:58:30.313224 139812139226944 spec.py:326] Evaluating on the test split.
I0405 23:58:32.310969 139812139226944 submission_runner.py:382] Time since start: 705.27s, 	Step: 1315, 	{'train/ssim': 0.7257983343941825, 'train/loss': 0.28201673712049213, 'validation/ssim': 0.7037377558736635, 'validation/loss': 0.3036632977916608, 'validation/num_examples': 3554, 'test/ssim': 0.7208987238419785, 'test/loss': 0.3056432345298974, 'test/num_examples': 3581}
I0405 23:58:32.311324 139812139226944 submission_runner.py:396] After eval at step 1315: RAM USED (GB) 143.09963776
I0405 23:58:32.320207 139752196011776 logging_writer.py:48] [1315] global_step=1315, preemption_count=0, score=426.522135, test/loss=0.305643, test/num_examples=3581, test/ssim=0.720899, total_duration=705.270698, train/loss=0.282017, train/ssim=0.725798, validation/loss=0.303663, validation/num_examples=3554, validation/ssim=0.703738
I0405 23:58:32.457972 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_1315.
I0405 23:58:32.458531 139812139226944 submission_runner.py:416] After logging and checkpointing eval at step 1315: RAM USED (GB) 143.098556416
I0405 23:59:18.730428 139752279873280 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.081850, loss=0.278269
I0405 23:59:18.734280 139812139226944 submission.py:119] 1500) loss = 0.278, grad_norm = 0.082
I0405 23:59:52.677830 139812139226944 submission_runner.py:373] Before eval at step 1628: RAM USED (GB) 143.135977472
I0405 23:59:52.678058 139812139226944 spec.py:298] Evaluating on the training split.
I0405 23:59:54.539358 139812139226944 spec.py:310] Evaluating on the validation split.
I0405 23:59:56.529230 139812139226944 spec.py:326] Evaluating on the test split.
I0405 23:59:58.511523 139812139226944 submission_runner.py:382] Time since start: 791.74s, 	Step: 1628, 	{'train/ssim': 0.7313988549368722, 'train/loss': 0.27751406601497103, 'validation/ssim': 0.7074818179120357, 'validation/loss': 0.30043520092897086, 'validation/num_examples': 3554, 'test/ssim': 0.7249315779024714, 'test/loss': 0.3022029379232407, 'test/num_examples': 3581}
I0405 23:59:58.511898 139812139226944 submission_runner.py:396] After eval at step 1628: RAM USED (GB) 143.142711296
I0405 23:59:58.522081 139752196011776 logging_writer.py:48] [1628] global_step=1628, preemption_count=0, score=499.692955, test/loss=0.302203, test/num_examples=3581, test/ssim=0.724932, total_duration=791.738087, train/loss=0.277514, train/ssim=0.731399, validation/loss=0.300435, validation/num_examples=3554, validation/ssim=0.707482
I0405 23:59:58.663069 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_1628.
I0405 23:59:58.663688 139812139226944 submission_runner.py:416] After logging and checkpointing eval at step 1628: RAM USED (GB) 143.141609472
I0406 00:01:18.766551 139812139226944 submission_runner.py:373] Before eval at step 1941: RAM USED (GB) 143.1768064
I0406 00:01:18.766817 139812139226944 spec.py:298] Evaluating on the training split.
I0406 00:01:20.643643 139812139226944 spec.py:310] Evaluating on the validation split.
I0406 00:01:23.065490 139812139226944 spec.py:326] Evaluating on the test split.
I0406 00:01:25.110850 139812139226944 submission_runner.py:382] Time since start: 877.83s, 	Step: 1941, 	{'train/ssim': 0.7314398629324776, 'train/loss': 0.27694220202309744, 'validation/ssim': 0.7090881725476575, 'validation/loss': 0.29871182503429233, 'validation/num_examples': 3554, 'test/ssim': 0.7263667648046984, 'test/loss': 0.3003778486936261, 'test/num_examples': 3581}
I0406 00:01:25.111231 139812139226944 submission_runner.py:396] After eval at step 1941: RAM USED (GB) 143.181156352
I0406 00:01:25.119590 139752279873280 logging_writer.py:48] [1941] global_step=1941, preemption_count=0, score=572.707148, test/loss=0.300378, test/num_examples=3581, test/ssim=0.726367, total_duration=877.827023, train/loss=0.276942, train/ssim=0.731440, validation/loss=0.298712, validation/num_examples=3554, validation/ssim=0.709088
I0406 00:01:25.262935 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_1941.
I0406 00:01:25.263484 139812139226944 submission_runner.py:416] After logging and checkpointing eval at step 1941: RAM USED (GB) 143.181307904
I0406 00:01:38.766395 139752196011776 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.118643, loss=0.295472
I0406 00:01:38.769797 139812139226944 submission.py:119] 2000) loss = 0.295, grad_norm = 0.119
I0406 00:02:45.550186 139812139226944 submission_runner.py:373] Before eval at step 2252: RAM USED (GB) 143.390224384
I0406 00:02:45.550470 139812139226944 spec.py:298] Evaluating on the training split.
I0406 00:02:47.437734 139812139226944 spec.py:310] Evaluating on the validation split.
I0406 00:02:49.572628 139812139226944 spec.py:326] Evaluating on the test split.
I0406 00:02:52.058012 139812139226944 submission_runner.py:382] Time since start: 964.61s, 	Step: 2252, 	{'train/ssim': 0.7358926364353725, 'train/loss': 0.2734157698495047, 'validation/ssim': 0.7121530510296497, 'validation/loss': 0.2962998200750914, 'validation/num_examples': 3554, 'test/ssim': 0.7290926722633343, 'test/loss': 0.29829123379031697, 'test/num_examples': 3581}
I0406 00:02:52.058371 139812139226944 submission_runner.py:396] After eval at step 2252: RAM USED (GB) 143.322832896
I0406 00:02:52.066624 139752279873280 logging_writer.py:48] [2252] global_step=2252, preemption_count=0, score=645.830577, test/loss=0.298291, test/num_examples=3581, test/ssim=0.729093, total_duration=964.612573, train/loss=0.273416, train/ssim=0.735893, validation/loss=0.296300, validation/num_examples=3554, validation/ssim=0.712153
I0406 00:02:52.208285 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_2252.
I0406 00:02:52.208856 139812139226944 submission_runner.py:416] After logging and checkpointing eval at step 2252: RAM USED (GB) 143.322763264
I0406 00:03:55.263311 139752196011776 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.165976, loss=0.251604
I0406 00:03:55.267077 139812139226944 submission.py:119] 2500) loss = 0.252, grad_norm = 0.166
I0406 00:04:12.292686 139812139226944 submission_runner.py:373] Before eval at step 2566: RAM USED (GB) 143.387009024
I0406 00:04:12.292932 139812139226944 spec.py:298] Evaluating on the training split.
I0406 00:04:14.176796 139812139226944 spec.py:310] Evaluating on the validation split.
I0406 00:04:16.213953 139812139226944 spec.py:326] Evaluating on the test split.
I0406 00:04:18.203067 139812139226944 submission_runner.py:382] Time since start: 1051.36s, 	Step: 2566, 	{'train/ssim': 0.7383089746747699, 'train/loss': 0.2727013485772269, 'validation/ssim': 0.7145919842738815, 'validation/loss': 0.2956328641583427, 'validation/num_examples': 3554, 'test/ssim': 0.7316612279871195, 'test/loss': 0.29738254115688706, 'test/num_examples': 3581}
I0406 00:04:18.203424 139812139226944 submission_runner.py:396] After eval at step 2566: RAM USED (GB) 143.38463744
I0406 00:04:18.211731 139752279873280 logging_writer.py:48] [2566] global_step=2566, preemption_count=0, score=718.715614, test/loss=0.297383, test/num_examples=3581, test/ssim=0.731661, total_duration=1051.355617, train/loss=0.272701, train/ssim=0.738309, validation/loss=0.295633, validation/num_examples=3554, validation/ssim=0.714592
I0406 00:04:18.352381 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_2566.
I0406 00:04:18.352992 139812139226944 submission_runner.py:416] After logging and checkpointing eval at step 2566: RAM USED (GB) 143.383523328
I0406 00:04:54.965132 139812139226944 submission_runner.py:373] Before eval at step 2714: RAM USED (GB) 143.387987968
I0406 00:04:54.965336 139812139226944 spec.py:298] Evaluating on the training split.
I0406 00:04:56.839641 139812139226944 spec.py:310] Evaluating on the validation split.
I0406 00:04:58.811049 139812139226944 spec.py:326] Evaluating on the test split.
I0406 00:05:00.790913 139812139226944 submission_runner.py:382] Time since start: 1094.02s, 	Step: 2714, 	{'train/ssim': 0.7385573387145996, 'train/loss': 0.27016115188598633, 'validation/ssim': 0.7150321106060074, 'validation/loss': 0.2929801876516777, 'validation/num_examples': 3554, 'test/ssim': 0.7322203447884669, 'test/loss': 0.29459408161128176, 'test/num_examples': 3581}
I0406 00:05:00.791278 139812139226944 submission_runner.py:396] After eval at step 2714: RAM USED (GB) 143.384887296
I0406 00:05:00.799644 139752196011776 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=751.972593, test/loss=0.294594, test/num_examples=3581, test/ssim=0.732220, total_duration=1094.023002, train/loss=0.270161, train/ssim=0.738557, validation/loss=0.292980, validation/num_examples=3554, validation/ssim=0.715032
I0406 00:05:00.940975 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_2714.
I0406 00:05:00.941572 139812139226944 submission_runner.py:416] After logging and checkpointing eval at step 2714: RAM USED (GB) 143.383773184
I0406 00:05:00.949715 139752279873280 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=751.972593
I0406 00:05:01.202813 139812139226944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/fastmri_pytorch/trial_1/checkpoint_2714.
I0406 00:05:02.589082 139812139226944 submission_runner.py:550] Tuning trial 1/1
I0406 00:05:02.589302 139812139226944 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0406 00:05:02.598603 139812139226944 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ssim': 0.2054400954927717, 'train/loss': 1.0082744189671107, 'validation/ssim': 0.1999599476107467, 'validation/loss': 1.0065798440137872, 'validation/num_examples': 3554, 'test/ssim': 0.22305075425204202, 'test/loss': 1.0047498680099833, 'test/num_examples': 3581, 'score': 49.20093870162964, 'total_duration': 49.202911376953125, 'global_step': 1, 'preemption_count': 0}), (310, {'train/ssim': 0.7054905891418457, 'train/loss': 0.2995874881744385, 'validation/ssim': 0.6828125549556837, 'validation/loss': 0.32252302368370145, 'validation/num_examples': 3554, 'test/ssim': 0.7005799652244484, 'test/loss': 0.3245725868734292, 'test/num_examples': 3581, 'score': 125.72551226615906, 'total_duration': 353.6181824207306, 'global_step': 310, 'preemption_count': 0}), (532, {'train/ssim': 0.7180110386439732, 'train/loss': 0.28861587388174875, 'validation/ssim': 0.694719047310249, 'validation/loss': 0.3113878686976822, 'validation/num_examples': 3554, 'test/ssim': 0.7124120248359397, 'test/loss': 0.3133251080463732, 'test/num_examples': 3581, 'score': 201.50636839866638, 'total_duration': 442.78577280044556, 'global_step': 532, 'preemption_count': 0}), (764, {'train/ssim': 0.7231929642813546, 'train/loss': 0.2837950161525181, 'validation/ssim': 0.7004588250035172, 'validation/loss': 0.30599256009953574, 'validation/num_examples': 3554, 'test/ssim': 0.7179680818687169, 'test/loss': 0.3077219068521363, 'test/num_examples': 3581, 'score': 277.55116415023804, 'total_duration': 529.6964108943939, 'global_step': 764, 'preemption_count': 0}), (1001, {'train/ssim': 0.7112624304635184, 'train/loss': 0.29811876160757883, 'validation/ssim': 0.6894678256453995, 'validation/loss': 0.3208989114927722, 'validation/num_examples': 3554, 'test/ssim': 0.7065634217484641, 'test/loss': 0.32307266623106323, 'test/num_examples': 3581, 'score': 353.4508059024811, 'total_duration': 617.0502610206604, 'global_step': 1001, 'preemption_count': 0}), (1315, {'train/ssim': 0.7257983343941825, 'train/loss': 0.28201673712049213, 'validation/ssim': 0.7037377558736635, 'validation/loss': 0.3036632977916608, 'validation/num_examples': 3554, 'test/ssim': 0.7208987238419785, 'test/loss': 0.3056432345298974, 'test/num_examples': 3581, 'score': 426.52213501930237, 'total_duration': 705.2706983089447, 'global_step': 1315, 'preemption_count': 0}), (1628, {'train/ssim': 0.7313988549368722, 'train/loss': 0.27751406601497103, 'validation/ssim': 0.7074818179120357, 'validation/loss': 0.30043520092897086, 'validation/num_examples': 3554, 'test/ssim': 0.7249315779024714, 'test/loss': 0.3022029379232407, 'test/num_examples': 3581, 'score': 499.69295501708984, 'total_duration': 791.7380874156952, 'global_step': 1628, 'preemption_count': 0}), (1941, {'train/ssim': 0.7314398629324776, 'train/loss': 0.27694220202309744, 'validation/ssim': 0.7090881725476575, 'validation/loss': 0.29871182503429233, 'validation/num_examples': 3554, 'test/ssim': 0.7263667648046984, 'test/loss': 0.3003778486936261, 'test/num_examples': 3581, 'score': 572.7071475982666, 'total_duration': 877.8270225524902, 'global_step': 1941, 'preemption_count': 0}), (2252, {'train/ssim': 0.7358926364353725, 'train/loss': 0.2734157698495047, 'validation/ssim': 0.7121530510296497, 'validation/loss': 0.2962998200750914, 'validation/num_examples': 3554, 'test/ssim': 0.7290926722633343, 'test/loss': 0.29829123379031697, 'test/num_examples': 3581, 'score': 645.8305766582489, 'total_duration': 964.6125729084015, 'global_step': 2252, 'preemption_count': 0}), (2566, {'train/ssim': 0.7383089746747699, 'train/loss': 0.2727013485772269, 'validation/ssim': 0.7145919842738815, 'validation/loss': 0.2956328641583427, 'validation/num_examples': 3554, 'test/ssim': 0.7316612279871195, 'test/loss': 0.29738254115688706, 'test/num_examples': 3581, 'score': 718.7156138420105, 'total_duration': 1051.3556170463562, 'global_step': 2566, 'preemption_count': 0}), (2714, {'train/ssim': 0.7385573387145996, 'train/loss': 0.27016115188598633, 'validation/ssim': 0.7150321106060074, 'validation/loss': 0.2929801876516777, 'validation/num_examples': 3554, 'test/ssim': 0.7322203447884669, 'test/loss': 0.29459408161128176, 'test/num_examples': 3581, 'score': 751.9725925922394, 'total_duration': 1094.0230021476746, 'global_step': 2714, 'preemption_count': 0})], 'global_step': 2714}
I0406 00:05:02.598793 139812139226944 submission_runner.py:553] Timing: 751.9725925922394
I0406 00:05:02.598846 139812139226944 submission_runner.py:554] ====================
I0406 00:05:02.598945 139812139226944 submission_runner.py:613] Final fastmri score: 751.9725925922394
