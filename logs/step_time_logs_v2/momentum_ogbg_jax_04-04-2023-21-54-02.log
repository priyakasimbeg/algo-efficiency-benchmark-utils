I0404 21:54:16.444431 140225109563200 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum_v2/ogbg_jax.
I0404 21:54:16.489571 140225109563200 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 21:54:17.319795 140225109563200 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0404 21:54:17.320405 140225109563200 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 21:54:17.323744 140225109563200 submission_runner.py:511] Using RNG seed 1512902461
I0404 21:54:18.559331 140225109563200 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 21:54:18.559535 140225109563200 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1.
I0404 21:54:18.559705 140225109563200 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/hparams.json.
I0404 21:54:18.683204 140225109563200 submission_runner.py:230] Starting train once: RAM USED (GB) 4.247994368
I0404 21:54:18.683357 140225109563200 submission_runner.py:231] Initializing dataset.
I0404 21:54:18.916011 140225109563200 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:54:18.921527 140225109563200 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:54:19.077907 140225109563200 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:54:19.105093 140225109563200 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.315377664
I0404 21:54:19.105247 140225109563200 submission_runner.py:240] Initializing model.
I0404 21:54:25.956594 140225109563200 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.124428288
I0404 21:54:25.956799 140225109563200 submission_runner.py:252] Initializing optimizer.
I0404 21:54:26.287253 140225109563200 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.12394496
I0404 21:54:26.287425 140225109563200 submission_runner.py:261] Initializing metrics bundle.
I0404 21:54:26.287472 140225109563200 submission_runner.py:276] Initializing checkpoint and logger.
I0404 21:54:26.288199 140225109563200 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1 with prefix checkpoint_
I0404 21:54:26.288586 140225109563200 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0404 21:54:26.288712 140225109563200 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0404 21:54:27.184998 140225109563200 submission_runner.py:297] Saving meta data to /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/meta_data_0.json.
I0404 21:54:27.186089 140225109563200 submission_runner.py:300] Saving flags to /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/flags_0.json.
I0404 21:54:27.188965 140225109563200 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.119750656
I0404 21:54:27.189153 140225109563200 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.119750656
I0404 21:54:27.189225 140225109563200 submission_runner.py:313] Starting training loop.
I0404 21:54:28.719243 140225109563200 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.271740928
I0404 21:54:44.992597 140048909461248 logging_writer.py:48] [0] global_step=0, grad_norm=3.0788276195526123, loss=0.6643843054771423
I0404 21:54:45.000071 140225109563200 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 10.93019648
I0404 21:54:45.000316 140225109563200 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 10.93019648
I0404 21:54:45.000391 140225109563200 spec.py:298] Evaluating on the training split.
I0404 21:54:45.008546 140225109563200 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:54:45.012733 140225109563200 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:54:45.067698 140225109563200 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0404 21:55:00.326233 140225109563200 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0404 21:56:12.092904 140225109563200 spec.py:310] Evaluating on the validation split.
I0404 21:56:12.095746 140225109563200 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:56:12.099085 140225109563200 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:56:12.150922 140225109563200 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:57:14.635150 140225109563200 spec.py:326] Evaluating on the test split.
I0404 21:57:14.637795 140225109563200 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:57:14.641267 140225109563200 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:57:14.692964 140225109563200 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:58:17.968572 140225109563200 submission_runner.py:382] Time since start: 17.81s, 	Step: 1, 	{'train/accuracy': 0.5932507514953613, 'train/loss': 0.6651369333267212, 'train/mean_average_precision': 0.02078458895719195, 'validation/accuracy': 0.5885758399963379, 'validation/loss': 0.6738957762718201, 'validation/mean_average_precision': 0.024708111914358996, 'validation/num_examples': 43793, 'test/accuracy': 0.5855077505111694, 'test/loss': 0.6777306199073792, 'test/mean_average_precision': 0.02613490034100992, 'test/num_examples': 43793}
I0404 21:58:17.969134 140225109563200 submission_runner.py:396] After eval at step 1: RAM USED (GB) 12.32470016
I0404 21:58:17.977560 140039060784896 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=17.738333, test/accuracy=0.585508, test/loss=0.677731, test/mean_average_precision=0.026135, test/num_examples=43793, total_duration=17.811137, train/accuracy=0.593251, train/loss=0.665137, train/mean_average_precision=0.020785, validation/accuracy=0.588576, validation/loss=0.673896, validation/mean_average_precision=0.024708, validation/num_examples=43793
I0404 21:58:18.001744 140225109563200 checkpoints.py:356] Saving checkpoint at step: 1
I0404 21:58:18.077304 140225109563200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_1
I0404 21:58:18.077584 140225109563200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_1.
I0404 21:58:18.078210 140225109563200 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 12.32627712
I0404 21:58:18.301514 140225109563200 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 12.36574208
I0404 21:58:18.315118 140225109563200 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 12.366258176
I0404 21:58:40.764421 140039069177600 logging_writer.py:48] [100] global_step=100, grad_norm=0.06508440524339676, loss=0.08566757291555405
I0404 21:59:03.412870 140040193701632 logging_writer.py:48] [200] global_step=200, grad_norm=0.017629999667406082, loss=0.06282205879688263
I0404 21:59:25.965024 140039069177600 logging_writer.py:48] [300] global_step=300, grad_norm=0.01020028069615364, loss=0.05013059452176094
I0404 21:59:48.430974 140040193701632 logging_writer.py:48] [400] global_step=400, grad_norm=0.011327105574309826, loss=0.054774004966020584
I0404 22:00:11.104664 140039069177600 logging_writer.py:48] [500] global_step=500, grad_norm=0.011536193080246449, loss=0.054492827504873276
I0404 22:00:33.601217 140040193701632 logging_writer.py:48] [600] global_step=600, grad_norm=0.013017482124269009, loss=0.05198008194565773
I0404 22:00:56.391047 140039069177600 logging_writer.py:48] [700] global_step=700, grad_norm=0.011741681024432182, loss=0.05170571431517601
I0404 22:01:19.118544 140040193701632 logging_writer.py:48] [800] global_step=800, grad_norm=0.024274442344903946, loss=0.05245205760002136
I0404 22:01:41.686434 140039069177600 logging_writer.py:48] [900] global_step=900, grad_norm=0.027492312714457512, loss=0.055886708199977875
I0404 22:02:04.434988 140040193701632 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.03568189591169357, loss=0.050177156925201416
I0404 22:02:18.170079 140225109563200 submission_runner.py:373] Before eval at step 1061: RAM USED (GB) 13.125660672
I0404 22:02:18.170298 140225109563200 spec.py:298] Evaluating on the training split.
I0404 22:03:29.529855 140225109563200 spec.py:310] Evaluating on the validation split.
I0404 22:03:32.183748 140225109563200 spec.py:326] Evaluating on the test split.
I0404 22:03:34.847814 140225109563200 submission_runner.py:382] Time since start: 470.98s, 	Step: 1061, 	{'train/accuracy': 0.9866985082626343, 'train/loss': 0.05496447905898094, 'train/mean_average_precision': 0.034420632379848715, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06497004628181458, 'validation/mean_average_precision': 0.03765970569096363, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0683794766664505, 'test/mean_average_precision': 0.03942628861632006, 'test/num_examples': 43793}
I0404 22:03:34.848322 140225109563200 submission_runner.py:396] After eval at step 1061: RAM USED (GB) 13.59714304
I0404 22:03:34.856084 140039069177600 logging_writer.py:48] [1061] global_step=1061, preemption_count=0, score=256.794696, test/accuracy=0.983142, test/loss=0.068379, test/mean_average_precision=0.039426, test/num_examples=43793, total_duration=470.980550, train/accuracy=0.986699, train/loss=0.054964, train/mean_average_precision=0.034421, validation/accuracy=0.984118, validation/loss=0.064970, validation/mean_average_precision=0.037660, validation/num_examples=43793
I0404 22:03:34.880714 140225109563200 checkpoints.py:356] Saving checkpoint at step: 1061
I0404 22:03:34.946264 140225109563200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_1061
I0404 22:03:34.946478 140225109563200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_1061.
I0404 22:03:34.947034 140225109563200 submission_runner.py:416] After logging and checkpointing eval at step 1061: RAM USED (GB) 13.596069888
I0404 22:03:44.004252 140040193701632 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.054418180137872696, loss=0.057764772325754166
I0404 22:04:06.348999 140040168523520 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.023065686225891113, loss=0.05539299547672272
I0404 22:04:28.572016 140040193701632 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.01915210671722889, loss=0.055951040238142014
I0404 22:04:51.501670 140040168523520 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.02672276459634304, loss=0.05813803896307945
I0404 22:05:14.489244 140040193701632 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.04506528750061989, loss=0.06181987375020981
I0404 22:05:37.186186 140040168523520 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.06446778774261475, loss=0.051774702966213226
I0404 22:06:00.040401 140040193701632 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0896545946598053, loss=0.05451244115829468
I0404 22:06:23.284989 140040168523520 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.04255695641040802, loss=0.057151708751916885
I0404 22:06:46.306829 140040193701632 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.025202633813023567, loss=0.05129311606287956
I0404 22:07:09.323795 140040168523520 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.04495235159993172, loss=0.05554148554801941
I0404 22:07:31.961189 140040193701632 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.019508754834532738, loss=0.05535900965332985
I0404 22:07:35.120309 140225109563200 submission_runner.py:373] Before eval at step 2115: RAM USED (GB) 14.097952768
I0404 22:07:35.120487 140225109563200 spec.py:298] Evaluating on the training split.
I0404 22:08:46.846266 140225109563200 spec.py:310] Evaluating on the validation split.
I0404 22:08:49.407973 140225109563200 spec.py:326] Evaluating on the test split.
I0404 22:08:51.905341 140225109563200 submission_runner.py:382] Time since start: 787.93s, 	Step: 2115, 	{'train/accuracy': 0.9866834878921509, 'train/loss': 0.05206090956926346, 'train/mean_average_precision': 0.0530944775394645, 'validation/accuracy': 0.9841203689575195, 'validation/loss': 0.06155455484986305, 'validation/mean_average_precision': 0.05142840547192814, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06478904187679291, 'test/mean_average_precision': 0.05392913776924145, 'test/num_examples': 43793}
I0404 22:08:51.905746 140225109563200 submission_runner.py:396] After eval at step 2115: RAM USED (GB) 14.40817152
I0404 22:08:51.913183 140040168523520 logging_writer.py:48] [2115] global_step=2115, preemption_count=0, score=496.003849, test/accuracy=0.983142, test/loss=0.064789, test/mean_average_precision=0.053929, test/num_examples=43793, total_duration=787.930762, train/accuracy=0.986683, train/loss=0.052061, train/mean_average_precision=0.053094, validation/accuracy=0.984120, validation/loss=0.061555, validation/mean_average_precision=0.051428, validation/num_examples=43793
I0404 22:08:51.937108 140225109563200 checkpoints.py:356] Saving checkpoint at step: 2115
I0404 22:08:51.997614 140225109563200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_2115
I0404 22:08:51.997807 140225109563200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_2115.
I0404 22:08:51.998350 140225109563200 submission_runner.py:416] After logging and checkpointing eval at step 2115: RAM USED (GB) 14.40708608
I0404 22:09:11.287904 140040193701632 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.03299081325531006, loss=0.04914350435137749
I0404 22:09:33.473673 140040143345408 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.07285794615745544, loss=0.0536288246512413
I0404 22:09:56.075888 140040193701632 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.058932676911354065, loss=0.05293281376361847
I0404 22:10:18.517369 140040143345408 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.06377187371253967, loss=0.04732998088002205
I0404 22:10:40.975505 140040193701632 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.06839589029550552, loss=0.05166672542691231
I0404 22:11:03.180823 140040143345408 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.04675806686282158, loss=0.05005854740738869
I0404 22:11:25.968161 140040193701632 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0602787546813488, loss=0.04956542328000069
I0404 22:11:48.599824 140040143345408 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.04868684709072113, loss=0.05323394760489464
I0404 22:12:11.209261 140040193701632 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.08120109140872955, loss=0.0524187795817852
I0404 22:12:33.416085 140040143345408 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.060882434248924255, loss=0.04860809072852135
I0404 22:12:52.106326 140225109563200 submission_runner.py:373] Before eval at step 3184: RAM USED (GB) 14.779498496
I0404 22:12:52.106514 140225109563200 spec.py:298] Evaluating on the training split.
I0404 22:14:04.811470 140225109563200 spec.py:310] Evaluating on the validation split.
I0404 22:14:07.391634 140225109563200 spec.py:326] Evaluating on the test split.
I0404 22:14:09.870834 140225109563200 submission_runner.py:382] Time since start: 1104.92s, 	Step: 3184, 	{'train/accuracy': 0.9870291352272034, 'train/loss': 0.04938273876905441, 'train/mean_average_precision': 0.08117056701442071, 'validation/accuracy': 0.9842222332954407, 'validation/loss': 0.060186173766851425, 'validation/mean_average_precision': 0.07797129308011154, 'validation/num_examples': 43793, 'test/accuracy': 0.9832591414451599, 'test/loss': 0.0636085718870163, 'test/mean_average_precision': 0.08051214322354991, 'test/num_examples': 43793}
I0404 22:14:09.871253 140225109563200 submission_runner.py:396] After eval at step 3184: RAM USED (GB) 15.088979968
I0404 22:14:09.878690 140040193701632 logging_writer.py:48] [3184] global_step=3184, preemption_count=0, score=735.176036, test/accuracy=0.983259, test/loss=0.063609, test/mean_average_precision=0.080512, test/num_examples=43793, total_duration=1104.916854, train/accuracy=0.987029, train/loss=0.049383, train/mean_average_precision=0.081171, validation/accuracy=0.984222, validation/loss=0.060186, validation/mean_average_precision=0.077971, validation/num_examples=43793
I0404 22:14:09.908664 140225109563200 checkpoints.py:356] Saving checkpoint at step: 3184
I0404 22:14:09.969743 140225109563200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_3184
I0404 22:14:09.969946 140225109563200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_3184.
I0404 22:14:09.970881 140225109563200 submission_runner.py:416] After logging and checkpointing eval at step 3184: RAM USED (GB) 15.0933504
I0404 22:14:13.866470 140040143345408 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.034796569496393204, loss=0.0548618882894516
I0404 22:14:36.534126 140040134952704 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.04359360784292221, loss=0.05284295976161957
I0404 22:14:59.083758 140040143345408 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.03249978646636009, loss=0.04978381097316742
I0404 22:15:21.742346 140040134952704 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.1342993825674057, loss=0.04879314824938774
I0404 22:15:44.241563 140040143345408 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.05346551164984703, loss=0.04866676405072212
I0404 22:16:06.937343 140040134952704 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.044873952865600586, loss=0.04989931732416153
I0404 22:16:29.461609 140040143345408 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.04996455833315849, loss=0.04388321936130524
I0404 22:16:51.782932 140040134952704 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.06008550897240639, loss=0.04771055281162262
I0404 22:17:14.201164 140040143345408 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.06742733716964722, loss=0.04863040894269943
I0404 22:17:36.561920 140040134952704 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.04188183322548866, loss=0.052314065396785736
I0404 22:17:58.970245 140040143345408 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.07324039191007614, loss=0.04709768667817116
I0404 22:18:09.983016 140225109563200 submission_runner.py:373] Before eval at step 4250: RAM USED (GB) 15.298506752
I0404 22:18:09.983203 140225109563200 spec.py:298] Evaluating on the training split.
I0404 22:19:22.223146 140225109563200 spec.py:310] Evaluating on the validation split.
I0404 22:19:24.821764 140225109563200 spec.py:326] Evaluating on the test split.
I0404 22:19:27.320055 140225109563200 submission_runner.py:382] Time since start: 1422.79s, 	Step: 4250, 	{'train/accuracy': 0.9871639609336853, 'train/loss': 0.04656725376844406, 'train/mean_average_precision': 0.10931297426911277, 'validation/accuracy': 0.9844974875450134, 'validation/loss': 0.055331286042928696, 'validation/mean_average_precision': 0.1077324527457038, 'validation/num_examples': 43793, 'test/accuracy': 0.9835240840911865, 'test/loss': 0.05820418521761894, 'test/mean_average_precision': 0.10967194251898456, 'test/num_examples': 43793}
I0404 22:19:27.320494 140225109563200 submission_runner.py:396] After eval at step 4250: RAM USED (GB) 15.668731904
I0404 22:19:27.327673 140040134952704 logging_writer.py:48] [4250] global_step=4250, preemption_count=0, score=974.218412, test/accuracy=0.983524, test/loss=0.058204, test/mean_average_precision=0.109672, test/num_examples=43793, total_duration=1422.793499, train/accuracy=0.987164, train/loss=0.046567, train/mean_average_precision=0.109313, validation/accuracy=0.984497, validation/loss=0.055331, validation/mean_average_precision=0.107732, validation/num_examples=43793
I0404 22:19:27.351680 140225109563200 checkpoints.py:356] Saving checkpoint at step: 4250
I0404 22:19:27.410190 140225109563200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_4250
I0404 22:19:27.410382 140225109563200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_4250.
I0404 22:19:27.410958 140225109563200 submission_runner.py:416] After logging and checkpointing eval at step 4250: RAM USED (GB) 15.6679168
I0404 22:19:38.721383 140040143345408 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.05507386103272438, loss=0.054133955389261246
I0404 22:20:01.094152 140040126560000 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.047825559973716736, loss=0.048481713980436325
I0404 22:20:23.541881 140040143345408 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.055856626480817795, loss=0.04702242836356163
I0404 22:20:45.831472 140040126560000 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.03753448277711868, loss=0.04553068056702614
I0404 22:21:08.273550 140040143345408 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.037866365164518356, loss=0.0486789345741272
I0404 22:21:30.592181 140040126560000 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.06200746074318886, loss=0.050535719841718674
I0404 22:21:52.764768 140040143345408 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.059952154755592346, loss=0.04651707038283348
I0404 22:22:14.853164 140040126560000 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.05208168178796768, loss=0.03995848447084427
I0404 22:22:36.860238 140040143345408 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.06349597871303558, loss=0.05688612535595894
I0404 22:22:58.855488 140040126560000 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.04789333790540695, loss=0.04924185574054718
I0404 22:23:21.205379 140040143345408 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.039596058428287506, loss=0.04642023146152496
I0404 22:23:27.493515 140225109563200 submission_runner.py:373] Before eval at step 5329: RAM USED (GB) 15.902838784
I0404 22:23:27.493698 140225109563200 spec.py:298] Evaluating on the training split.
I0404 22:24:38.984698 140225109563200 spec.py:310] Evaluating on the validation split.
I0404 22:24:41.531952 140225109563200 spec.py:326] Evaluating on the test split.
I0404 22:24:44.002693 140225109563200 submission_runner.py:382] Time since start: 1740.30s, 	Step: 5329, 	{'train/accuracy': 0.9872983694076538, 'train/loss': 0.0449337512254715, 'train/mean_average_precision': 0.13852604201471172, 'validation/accuracy': 0.9846314787864685, 'validation/loss': 0.05432455241680145, 'validation/mean_average_precision': 0.1298903110134809, 'validation/num_examples': 43793, 'test/accuracy': 0.9836058020591736, 'test/loss': 0.05735371261835098, 'test/mean_average_precision': 0.1302033938134869, 'test/num_examples': 43793}
I0404 22:24:44.003166 140225109563200 submission_runner.py:396] After eval at step 5329: RAM USED (GB) 16.3263488
I0404 22:24:44.010979 140040126560000 logging_writer.py:48] [5329] global_step=5329, preemption_count=0, score=1213.349897, test/accuracy=0.983606, test/loss=0.057354, test/mean_average_precision=0.130203, test/num_examples=43793, total_duration=1740.303938, train/accuracy=0.987298, train/loss=0.044934, train/mean_average_precision=0.138526, validation/accuracy=0.984631, validation/loss=0.054325, validation/mean_average_precision=0.129890, validation/num_examples=43793
I0404 22:24:44.035071 140225109563200 checkpoints.py:356] Saving checkpoint at step: 5329
I0404 22:24:44.090797 140225109563200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_5329
I0404 22:24:44.090972 140225109563200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_5329.
I0404 22:24:44.091557 140225109563200 submission_runner.py:416] After logging and checkpointing eval at step 5329: RAM USED (GB) 16.325267456
I0404 22:25:00.302182 140040143345408 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.04889272153377533, loss=0.04755550995469093
I0404 22:25:23.054544 140040118167296 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.029925739392638206, loss=0.04117150604724884
I0404 22:25:45.350788 140040143345408 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.06699835509061813, loss=0.04454896226525307
I0404 22:26:07.935724 140040118167296 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.08084584027528763, loss=0.05025683343410492
I0404 22:26:30.255735 140040143345408 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.02717733010649681, loss=0.04766872897744179
I0404 22:26:52.454177 140040118167296 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.057591523975133896, loss=0.04625639319419861
I0404 22:27:14.438728 140225109563200 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 16.374177792
I0404 22:27:14.438918 140225109563200 spec.py:298] Evaluating on the training split.
I0404 22:28:27.151973 140225109563200 spec.py:310] Evaluating on the validation split.
I0404 22:28:29.691227 140225109563200 spec.py:326] Evaluating on the test split.
I0404 22:28:32.183981 140225109563200 submission_runner.py:382] Time since start: 1967.25s, 	Step: 6000, 	{'train/accuracy': 0.9873517751693726, 'train/loss': 0.044712334871292114, 'train/mean_average_precision': 0.13754088800782077, 'validation/accuracy': 0.9847784042358398, 'validation/loss': 0.05376804247498512, 'validation/mean_average_precision': 0.1271603885178295, 'validation/num_examples': 43793, 'test/accuracy': 0.9838239550590515, 'test/loss': 0.05668700486421585, 'test/mean_average_precision': 0.13387098323065155, 'test/num_examples': 43793}
I0404 22:28:32.184444 140225109563200 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 16.708059136
I0404 22:28:32.191973 140040143345408 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1363.108472, test/accuracy=0.983824, test/loss=0.056687, test/mean_average_precision=0.133871, test/num_examples=43793, total_duration=1967.249220, train/accuracy=0.987352, train/loss=0.044712, train/mean_average_precision=0.137541, validation/accuracy=0.984778, validation/loss=0.053768, validation/mean_average_precision=0.127160, validation/num_examples=43793
I0404 22:28:32.215427 140225109563200 checkpoints.py:356] Saving checkpoint at step: 6000
I0404 22:28:32.270555 140225109563200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_6000
I0404 22:28:32.270713 140225109563200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_6000.
I0404 22:28:32.271423 140225109563200 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 16.707239936
I0404 22:28:32.277336 140040118167296 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1363.108472
I0404 22:28:32.296006 140225109563200 checkpoints.py:356] Saving checkpoint at step: 6000
I0404 22:28:32.384178 140225109563200 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_6000
I0404 22:28:32.384378 140225109563200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/ogbg_jax/trial_1/checkpoint_6000.
I0404 22:28:32.531428 140225109563200 submission_runner.py:550] Tuning trial 1/1
I0404 22:28:32.531640 140225109563200 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0404 22:28:32.534473 140225109563200 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5932507514953613, 'train/loss': 0.6651369333267212, 'train/mean_average_precision': 0.02078458895719195, 'validation/accuracy': 0.5885758399963379, 'validation/loss': 0.6738957762718201, 'validation/mean_average_precision': 0.024708111914358996, 'validation/num_examples': 43793, 'test/accuracy': 0.5855077505111694, 'test/loss': 0.6777306199073792, 'test/mean_average_precision': 0.02613490034100992, 'test/num_examples': 43793, 'score': 17.738333225250244, 'total_duration': 17.811136960983276, 'global_step': 1, 'preemption_count': 0}), (1061, {'train/accuracy': 0.9866985082626343, 'train/loss': 0.05496447905898094, 'train/mean_average_precision': 0.034420632379848715, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06497004628181458, 'validation/mean_average_precision': 0.03765970569096363, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0683794766664505, 'test/mean_average_precision': 0.03942628861632006, 'test/num_examples': 43793, 'score': 256.7946960926056, 'total_duration': 470.9805498123169, 'global_step': 1061, 'preemption_count': 0}), (2115, {'train/accuracy': 0.9866834878921509, 'train/loss': 0.05206090956926346, 'train/mean_average_precision': 0.0530944775394645, 'validation/accuracy': 0.9841203689575195, 'validation/loss': 0.06155455484986305, 'validation/mean_average_precision': 0.05142840547192814, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06478904187679291, 'test/mean_average_precision': 0.05392913776924145, 'test/num_examples': 43793, 'score': 496.003849029541, 'total_duration': 787.9307615756989, 'global_step': 2115, 'preemption_count': 0}), (3184, {'train/accuracy': 0.9870291352272034, 'train/loss': 0.04938273876905441, 'train/mean_average_precision': 0.08117056701442071, 'validation/accuracy': 0.9842222332954407, 'validation/loss': 0.060186173766851425, 'validation/mean_average_precision': 0.07797129308011154, 'validation/num_examples': 43793, 'test/accuracy': 0.9832591414451599, 'test/loss': 0.0636085718870163, 'test/mean_average_precision': 0.08051214322354991, 'test/num_examples': 43793, 'score': 735.1760356426239, 'total_duration': 1104.9168541431427, 'global_step': 3184, 'preemption_count': 0}), (4250, {'train/accuracy': 0.9871639609336853, 'train/loss': 0.04656725376844406, 'train/mean_average_precision': 0.10931297426911277, 'validation/accuracy': 0.9844974875450134, 'validation/loss': 0.055331286042928696, 'validation/mean_average_precision': 0.1077324527457038, 'validation/num_examples': 43793, 'test/accuracy': 0.9835240840911865, 'test/loss': 0.05820418521761894, 'test/mean_average_precision': 0.10967194251898456, 'test/num_examples': 43793, 'score': 974.2184119224548, 'total_duration': 1422.793499469757, 'global_step': 4250, 'preemption_count': 0}), (5329, {'train/accuracy': 0.9872983694076538, 'train/loss': 0.0449337512254715, 'train/mean_average_precision': 0.13852604201471172, 'validation/accuracy': 0.9846314787864685, 'validation/loss': 0.05432455241680145, 'validation/mean_average_precision': 0.1298903110134809, 'validation/num_examples': 43793, 'test/accuracy': 0.9836058020591736, 'test/loss': 0.05735371261835098, 'test/mean_average_precision': 0.1302033938134869, 'test/num_examples': 43793, 'score': 1213.3498969078064, 'total_duration': 1740.3039376735687, 'global_step': 5329, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9873517751693726, 'train/loss': 0.044712334871292114, 'train/mean_average_precision': 0.13754088800782077, 'validation/accuracy': 0.9847784042358398, 'validation/loss': 0.05376804247498512, 'validation/mean_average_precision': 0.1271603885178295, 'validation/num_examples': 43793, 'test/accuracy': 0.9838239550590515, 'test/loss': 0.05668700486421585, 'test/mean_average_precision': 0.13387098323065155, 'test/num_examples': 43793, 'score': 1363.1084716320038, 'total_duration': 1967.2492201328278, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0404 22:28:32.534629 140225109563200 submission_runner.py:553] Timing: 1363.1084716320038
I0404 22:28:32.534674 140225109563200 submission_runner.py:554] ====================
I0404 22:28:32.534797 140225109563200 submission_runner.py:613] Final ogbg score: 1363.1084716320038
