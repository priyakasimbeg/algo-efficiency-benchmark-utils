I0404 19:48:50.364946 140169517000512 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw_v2/imagenet_vit_jax.
I0404 19:48:50.416782 140169517000512 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 19:48:51.288342 140169517000512 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0404 19:48:51.289147 140169517000512 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 19:48:51.293876 140169517000512 submission_runner.py:511] Using RNG seed 852745037
I0404 19:48:53.725781 140169517000512 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 19:48:53.726035 140169517000512 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1.
I0404 19:48:53.726229 140169517000512 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/hparams.json.
I0404 19:48:53.861474 140169517000512 submission_runner.py:230] Starting train once: RAM USED (GB) 4.207017984
I0404 19:48:53.861685 140169517000512 submission_runner.py:231] Initializing dataset.
I0404 19:48:53.873352 140169517000512 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:48:53.880640 140169517000512 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:48:53.880769 140169517000512 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:48:54.135365 140169517000512 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:49:00.702469 140169517000512 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.377792512
I0404 19:49:00.702694 140169517000512 submission_runner.py:240] Initializing model.
I0404 19:49:11.550866 140169517000512 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.422608896
I0404 19:49:11.551103 140169517000512 submission_runner.py:252] Initializing optimizer.
I0404 19:49:12.181904 140169517000512 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.423112704
I0404 19:49:12.182076 140169517000512 submission_runner.py:261] Initializing metrics bundle.
I0404 19:49:12.182124 140169517000512 submission_runner.py:276] Initializing checkpoint and logger.
I0404 19:49:12.183049 140169517000512 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0404 19:49:12.990689 140169517000512 submission_runner.py:297] Saving meta data to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/meta_data_0.json.
I0404 19:49:12.991662 140169517000512 submission_runner.py:300] Saving flags to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/flags_0.json.
I0404 19:49:12.995840 140169517000512 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.42088448
I0404 19:49:12.996068 140169517000512 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.42088448
I0404 19:49:12.996128 140169517000512 submission_runner.py:313] Starting training loop.
I0404 19:49:16.196130 140169517000512 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 14.49445376
I0404 19:50:01.874135 139993745975040 logging_writer.py:48] [0] global_step=0, grad_norm=0.3362998962402344, loss=6.907756805419922
I0404 19:50:01.887603 140169517000512 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 46.357078016
I0404 19:50:01.887880 140169517000512 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 46.357078016
I0404 19:50:01.887969 140169517000512 spec.py:298] Evaluating on the training split.
I0404 19:50:01.893782 140169517000512 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:01.899548 140169517000512 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:50:01.899647 140169517000512 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:50:01.958848 140169517000512 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:21.087657 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 19:50:21.095889 140169517000512 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:21.108353 140169517000512 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:50:21.108571 140169517000512 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:50:21.159452 140169517000512 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:39.371209 140169517000512 spec.py:326] Evaluating on the test split.
I0404 19:50:39.377595 140169517000512 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 19:50:39.382336 140169517000512 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0404 19:50:39.413553 140169517000512 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 19:50:50.021339 140169517000512 submission_runner.py:382] Time since start: 48.89s, 	Step: 1, 	{'train/accuracy': 0.0009960937313735485, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000}
I0404 19:50:50.021945 140169517000512 submission_runner.py:396] After eval at step 1: RAM USED (GB) 105.787621376
I0404 19:50:50.029998 139928583251712 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=48.812074, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=48.891744, train/accuracy=0.000996, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0404 19:50:50.266437 140169517000512 checkpoints.py:356] Saving checkpoint at step: 1
I0404 19:50:50.882066 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_1
I0404 19:50:50.882997 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_1.
I0404 19:50:50.885204 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 105.986383872
I0404 19:50:50.889320 140169517000512 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 105.976553472
I0404 19:51:07.841209 140169517000512 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 105.408331776
I0404 19:51:46.744672 139989258024704 logging_writer.py:48] [100] global_step=100, grad_norm=0.45960646867752075, loss=6.897284984588623
I0404 19:52:26.038038 139989266417408 logging_writer.py:48] [200] global_step=200, grad_norm=0.5174973607063293, loss=6.841923713684082
I0404 19:53:05.434028 139989258024704 logging_writer.py:48] [300] global_step=300, grad_norm=0.44437217712402344, loss=6.828097820281982
I0404 19:53:44.836009 139989266417408 logging_writer.py:48] [400] global_step=400, grad_norm=0.7055392861366272, loss=6.718433380126953
I0404 19:54:24.832870 139989258024704 logging_writer.py:48] [500] global_step=500, grad_norm=0.5448024272918701, loss=6.766307353973389
I0404 19:55:04.979958 139989266417408 logging_writer.py:48] [600] global_step=600, grad_norm=0.7349758744239807, loss=6.777517795562744
I0404 19:55:44.767878 139989258024704 logging_writer.py:48] [700] global_step=700, grad_norm=0.9335293769836426, loss=6.541695594787598
I0404 19:56:24.667039 139989266417408 logging_writer.py:48] [800] global_step=800, grad_norm=0.9618918299674988, loss=6.486264228820801
I0404 19:57:04.484143 139989258024704 logging_writer.py:48] [900] global_step=900, grad_norm=0.781801700592041, loss=6.399263381958008
I0404 19:57:44.369446 139989266417408 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9032207131385803, loss=6.763283729553223
I0404 19:57:50.962746 140169517000512 submission_runner.py:373] Before eval at step 1018: RAM USED (GB) 80.306757632
I0404 19:57:50.963129 140169517000512 spec.py:298] Evaluating on the training split.
I0404 19:58:02.087905 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 19:58:08.587724 140169517000512 spec.py:326] Evaluating on the test split.
I0404 19:58:10.633600 140169517000512 submission_runner.py:382] Time since start: 517.96s, 	Step: 1018, 	{'train/accuracy': 0.02529296837747097, 'train/loss': 6.127302169799805, 'validation/accuracy': 0.024159999564290047, 'validation/loss': 6.146903991699219, 'validation/num_examples': 50000, 'test/accuracy': 0.018800001591444016, 'test/loss': 6.229177951812744, 'test/num_examples': 10000}
I0404 19:58:10.634303 140169517000512 submission_runner.py:396] After eval at step 1018: RAM USED (GB) 85.7755648
I0404 19:58:10.651485 139928759432960 logging_writer.py:48] [1018] global_step=1018, preemption_count=0, score=460.947579, test/accuracy=0.018800, test/loss=6.229178, test/num_examples=10000, total_duration=517.962486, train/accuracy=0.025293, train/loss=6.127302, validation/accuracy=0.024160, validation/loss=6.146904, validation/num_examples=50000
I0404 19:58:12.024252 140169517000512 checkpoints.py:356] Saving checkpoint at step: 1018
I0404 19:58:13.462191 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_1018
I0404 19:58:13.479355 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_1018.
I0404 19:58:13.482208 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 1018: RAM USED (GB) 97.420009472
I0404 19:58:46.286254 139928843294464 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8141575455665588, loss=6.278897285461426
I0404 19:59:25.774483 139993443968768 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.0635251998901367, loss=6.384062767028809
I0404 20:00:05.247749 139928843294464 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8558697700500488, loss=6.270646095275879
I0404 20:00:44.716168 139993443968768 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8860195279121399, loss=6.255728721618652
I0404 20:01:24.695204 139928843294464 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.936578631401062, loss=6.362344741821289
I0404 20:02:04.702127 139993443968768 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.9371303915977478, loss=6.1034626960754395
I0404 20:02:45.024836 139928843294464 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.4026297330856323, loss=6.192600727081299
I0404 20:03:24.950530 139993443968768 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8496462106704712, loss=6.129720687866211
I0404 20:04:05.107986 139928843294464 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7209323644638062, loss=6.101551055908203
I0404 20:04:45.051032 139993443968768 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9643505215644836, loss=6.332205772399902
I0404 20:05:13.653727 140169517000512 submission_runner.py:373] Before eval at step 2073: RAM USED (GB) 87.338528768
I0404 20:05:13.654060 140169517000512 spec.py:298] Evaluating on the training split.
I0404 20:05:25.039789 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 20:05:31.976142 140169517000512 spec.py:326] Evaluating on the test split.
I0404 20:05:33.630151 140169517000512 submission_runner.py:382] Time since start: 960.66s, 	Step: 2073, 	{'train/accuracy': 0.06554687023162842, 'train/loss': 5.470003128051758, 'validation/accuracy': 0.06227999925613403, 'validation/loss': 5.505962371826172, 'validation/num_examples': 50000, 'test/accuracy': 0.04990000277757645, 'test/loss': 5.686747074127197, 'test/num_examples': 10000}
I0404 20:05:33.630665 140169517000512 submission_runner.py:396] After eval at step 2073: RAM USED (GB) 90.597113856
I0404 20:05:33.642966 139928843294464 logging_writer.py:48] [2073] global_step=2073, preemption_count=0, score=873.152259, test/accuracy=0.049900, test/loss=5.686747, test/num_examples=10000, total_duration=960.655947, train/accuracy=0.065547, train/loss=5.470003, validation/accuracy=0.062280, validation/loss=5.505962, validation/num_examples=50000
I0404 20:05:33.781636 140169517000512 checkpoints.py:356] Saving checkpoint at step: 2073
I0404 20:05:36.876920 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_2073
I0404 20:05:36.894427 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_2073.
I0404 20:05:36.897307 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 2073: RAM USED (GB) 102.613659648
I0404 20:05:47.995948 139993443968768 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.0466755628585815, loss=6.509983062744141
I0404 20:06:27.706446 139993410397952 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0690819025039673, loss=6.018894195556641
I0404 20:07:07.273250 139993443968768 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7562307715415955, loss=6.02631139755249
I0404 20:07:46.798000 139993410397952 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8810192346572876, loss=6.25351095199585
I0404 20:08:26.610011 139993443968768 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8707091808319092, loss=6.034429550170898
I0404 20:09:06.526465 139993410397952 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.2712475061416626, loss=5.894146919250488
I0404 20:09:46.439687 139993443968768 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9101957678794861, loss=5.81914758682251
I0404 20:10:26.181908 139993410397952 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8759042024612427, loss=5.772284030914307
I0404 20:11:06.358436 139993443968768 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9212896823883057, loss=5.86574649810791
I0404 20:11:46.250521 139993410397952 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8480110168457031, loss=6.380246162414551
I0404 20:12:26.373808 139993443968768 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6519768238067627, loss=6.588080406188965
I0404 20:12:37.168285 140169517000512 submission_runner.py:373] Before eval at step 3129: RAM USED (GB) 92.588290048
I0404 20:12:37.168608 140169517000512 spec.py:298] Evaluating on the training split.
I0404 20:12:48.766225 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 20:12:56.155647 140169517000512 spec.py:326] Evaluating on the test split.
I0404 20:12:57.823076 140169517000512 submission_runner.py:382] Time since start: 1404.17s, 	Step: 3129, 	{'train/accuracy': 0.11753906309604645, 'train/loss': 4.8886613845825195, 'validation/accuracy': 0.11285999417304993, 'validation/loss': 4.940011978149414, 'validation/num_examples': 50000, 'test/accuracy': 0.08190000057220459, 'test/loss': 5.198060035705566, 'test/num_examples': 10000}
I0404 20:12:57.823582 140169517000512 submission_runner.py:396] After eval at step 3129: RAM USED (GB) 94.585925632
I0404 20:12:57.834570 139993410397952 logging_writer.py:48] [3129] global_step=3129, preemption_count=0, score=1285.116518, test/accuracy=0.081900, test/loss=5.198060, test/num_examples=10000, total_duration=1404.167656, train/accuracy=0.117539, train/loss=4.888661, validation/accuracy=0.112860, validation/loss=4.940012, validation/num_examples=50000
I0404 20:12:57.972295 140169517000512 checkpoints.py:356] Saving checkpoint at step: 3129
I0404 20:13:00.477088 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_3129
I0404 20:13:00.497646 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_3129.
I0404 20:13:00.500876 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 3129: RAM USED (GB) 104.903094272
I0404 20:13:28.998826 139993443968768 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.1569061279296875, loss=5.883986949920654
I0404 20:14:08.571689 139992613517056 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7678262591362, loss=6.590860843658447
I0404 20:14:48.318784 139993443968768 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7660487294197083, loss=5.66216516494751
I0404 20:15:27.978900 139992613517056 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.899781346321106, loss=5.6367645263671875
I0404 20:16:07.829224 139993443968768 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9527141451835632, loss=5.657544136047363
I0404 20:16:47.713032 139992613517056 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7985010743141174, loss=6.001447677612305
I0404 20:17:27.819417 139993443968768 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8295371532440186, loss=5.655291557312012
I0404 20:18:08.023540 139992613517056 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8016009330749512, loss=5.486352920532227
I0404 20:18:48.178302 139993443968768 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8015139698982239, loss=5.681188106536865
I0404 20:19:28.140400 139992613517056 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9131167531013489, loss=5.496644973754883
I0404 20:20:00.768742 140169517000512 submission_runner.py:373] Before eval at step 4183: RAM USED (GB) 98.777255936
I0404 20:20:00.769074 140169517000512 spec.py:298] Evaluating on the training split.
I0404 20:20:12.513321 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 20:20:21.879127 140169517000512 spec.py:326] Evaluating on the test split.
I0404 20:20:23.531809 140169517000512 submission_runner.py:382] Time since start: 1847.77s, 	Step: 4183, 	{'train/accuracy': 0.1609765589237213, 'train/loss': 4.5238165855407715, 'validation/accuracy': 0.1485999971628189, 'validation/loss': 4.616313457489014, 'validation/num_examples': 50000, 'test/accuracy': 0.11390000581741333, 'test/loss': 4.929190635681152, 'test/num_examples': 10000}
I0404 20:20:23.532229 140169517000512 submission_runner.py:396] After eval at step 4183: RAM USED (GB) 105.37957376
I0404 20:20:23.539909 139993443968768 logging_writer.py:48] [4183] global_step=4183, preemption_count=0, score=1697.392712, test/accuracy=0.113900, test/loss=4.929191, test/num_examples=10000, total_duration=1847.769081, train/accuracy=0.160977, train/loss=4.523817, validation/accuracy=0.148600, validation/loss=4.616313, validation/num_examples=50000
I0404 20:20:24.876171 140169517000512 checkpoints.py:356] Saving checkpoint at step: 4183
I0404 20:20:26.069426 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_4183
I0404 20:20:26.085127 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_4183.
I0404 20:20:26.088318 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 4183: RAM USED (GB) 111.285116928
I0404 20:20:33.245098 139992613517056 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7356986999511719, loss=6.125113010406494
I0404 20:21:12.823101 139992605124352 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9669184684753418, loss=5.394406795501709
I0404 20:21:52.403401 139992613517056 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6407763957977295, loss=6.419095039367676
I0404 20:22:31.965991 139992605124352 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8780680894851685, loss=5.337513446807861
I0404 20:23:11.837942 139992613517056 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8447330594062805, loss=5.642066478729248
I0404 20:23:52.046131 139992605124352 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7564191818237305, loss=5.40591287612915
I0404 20:24:32.023866 139992613517056 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7781465649604797, loss=5.317116737365723
I0404 20:25:12.109285 139992605124352 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.0090023279190063, loss=5.349399566650391
I0404 20:25:52.083717 139992613517056 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.691733717918396, loss=5.99185848236084
I0404 20:26:31.940058 139992605124352 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.9341772794723511, loss=5.313498497009277
I0404 20:27:11.732270 139992613517056 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.1807557344436646, loss=5.216746807098389
I0404 20:27:26.247116 140169517000512 submission_runner.py:373] Before eval at step 5238: RAM USED (GB) 104.901443584
I0404 20:27:26.247428 140169517000512 spec.py:298] Evaluating on the training split.
I0404 20:27:40.264182 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 20:27:48.787381 140169517000512 spec.py:326] Evaluating on the test split.
I0404 20:27:50.444574 140169517000512 submission_runner.py:382] Time since start: 2293.25s, 	Step: 5238, 	{'train/accuracy': 0.2009570300579071, 'train/loss': 4.178868770599365, 'validation/accuracy': 0.186039999127388, 'validation/loss': 4.271564960479736, 'validation/num_examples': 50000, 'test/accuracy': 0.1421000063419342, 'test/loss': 4.6551666259765625, 'test/num_examples': 10000}
I0404 20:27:50.445189 140169517000512 submission_runner.py:396] After eval at step 5238: RAM USED (GB) 108.850143232
I0404 20:27:50.465078 139992605124352 logging_writer.py:48] [5238] global_step=5238, preemption_count=0, score=2109.260979, test/accuracy=0.142100, test/loss=4.655167, test/num_examples=10000, total_duration=2293.247310, train/accuracy=0.200957, train/loss=4.178869, validation/accuracy=0.186040, validation/loss=4.271565, validation/num_examples=50000
I0404 20:27:50.607261 140169517000512 checkpoints.py:356] Saving checkpoint at step: 5238
I0404 20:27:51.874165 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_5238
I0404 20:27:51.893215 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_5238.
I0404 20:27:51.895796 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 5238: RAM USED (GB) 114.769031168
I0404 20:28:16.885672 139992613517056 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6900959610939026, loss=6.4103217124938965
I0404 20:28:56.658209 139992596731648 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8686392307281494, loss=5.173582077026367
I0404 20:29:36.589158 139992613517056 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7472635507583618, loss=5.200114727020264
I0404 20:30:16.686530 139992596731648 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9517940282821655, loss=5.131760597229004
I0404 20:30:56.940900 139992613517056 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.639384925365448, loss=5.79580020904541
I0404 20:31:37.128192 139992596731648 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7679966688156128, loss=4.945066928863525
I0404 20:32:17.737406 139992613517056 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6920269131660461, loss=5.790054798126221
I0404 20:32:58.083223 139992596731648 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7082774639129639, loss=5.348531246185303
I0404 20:33:37.977433 139992613517056 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8241749405860901, loss=5.107457160949707
I0404 20:34:17.882216 139992596731648 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6136009693145752, loss=6.22465181350708
I0404 20:34:52.030045 140169517000512 submission_runner.py:373] Before eval at step 6287: RAM USED (GB) 110.652796928
I0404 20:34:52.030315 140169517000512 spec.py:298] Evaluating on the training split.
I0404 20:35:06.158779 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 20:35:15.388280 140169517000512 spec.py:326] Evaluating on the test split.
I0404 20:35:17.048352 140169517000512 submission_runner.py:382] Time since start: 2739.03s, 	Step: 6287, 	{'train/accuracy': 0.24074217677116394, 'train/loss': 3.9561846256256104, 'validation/accuracy': 0.22360000014305115, 'validation/loss': 4.064438819885254, 'validation/num_examples': 50000, 'test/accuracy': 0.170400008559227, 'test/loss': 4.474737644195557, 'test/num_examples': 10000}
I0404 20:35:17.048811 140169517000512 submission_runner.py:396] After eval at step 6287: RAM USED (GB) 116.642013184
I0404 20:35:17.061273 139992613517056 logging_writer.py:48] [6287] global_step=6287, preemption_count=0, score=2518.895379, test/accuracy=0.170400, test/loss=4.474738, test/num_examples=10000, total_duration=2739.029403, train/accuracy=0.240742, train/loss=3.956185, validation/accuracy=0.223600, validation/loss=4.064439, validation/num_examples=50000
I0404 20:35:17.202011 140169517000512 checkpoints.py:356] Saving checkpoint at step: 6287
I0404 20:35:18.492368 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_6287
I0404 20:35:18.513318 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_6287.
I0404 20:35:18.516475 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 6287: RAM USED (GB) 122.483621888
I0404 20:35:24.191761 139992596731648 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8808923363685608, loss=5.059006690979004
I0404 20:36:03.605533 139992043091712 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.8176490068435669, loss=5.562876224517822
I0404 20:36:43.318060 139992596731648 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6012861132621765, loss=5.394271373748779
I0404 20:37:23.414704 139992043091712 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7474826574325562, loss=5.807046413421631
I0404 20:38:03.720113 139992596731648 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8978073596954346, loss=4.952600955963135
I0404 20:38:43.973317 139992043091712 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9096980094909668, loss=4.932154655456543
I0404 20:39:24.120223 139992596731648 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5374952554702759, loss=5.899400234222412
I0404 20:40:04.458821 139992043091712 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7333237528800964, loss=4.899361610412598
I0404 20:40:44.522856 139992596731648 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5381922721862793, loss=6.004534721374512
I0404 20:41:24.660464 139992043091712 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5388721823692322, loss=5.954161643981934
I0404 20:42:04.664190 139992596731648 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.8764333724975586, loss=4.7633442878723145
I0404 20:42:18.747797 140169517000512 submission_runner.py:373] Before eval at step 7337: RAM USED (GB) 118.318444544
I0404 20:42:18.748092 140169517000512 spec.py:298] Evaluating on the training split.
I0404 20:42:32.836987 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 20:42:42.045210 140169517000512 spec.py:326] Evaluating on the test split.
I0404 20:42:43.698339 140169517000512 submission_runner.py:382] Time since start: 3185.75s, 	Step: 7337, 	{'train/accuracy': 0.28605467081069946, 'train/loss': 3.5760562419891357, 'validation/accuracy': 0.26767998933792114, 'validation/loss': 3.687535285949707, 'validation/num_examples': 50000, 'test/accuracy': 0.2046000063419342, 'test/loss': 4.1556396484375, 'test/num_examples': 10000}
I0404 20:42:43.698776 140169517000512 submission_runner.py:396] After eval at step 7337: RAM USED (GB) 121.532669952
I0404 20:42:43.710133 139992043091712 logging_writer.py:48] [7337] global_step=7337, preemption_count=0, score=2929.379344, test/accuracy=0.204600, test/loss=4.155640, test/num_examples=10000, total_duration=3185.747931, train/accuracy=0.286055, train/loss=3.576056, validation/accuracy=0.267680, validation/loss=3.687535, validation/num_examples=50000
I0404 20:42:44.468423 140169517000512 checkpoints.py:356] Saving checkpoint at step: 7337
I0404 20:42:45.533656 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_7337
I0404 20:42:45.552653 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_7337.
I0404 20:42:45.556118 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 7337: RAM USED (GB) 128.929243136
I0404 20:43:10.859936 139992596731648 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7182482481002808, loss=4.861881256103516
I0404 20:43:50.519039 139992034699008 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5156917572021484, loss=5.585573196411133
I0404 20:44:30.382756 139992596731648 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.484883189201355, loss=6.087649345397949
I0404 20:45:10.156654 139992034699008 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6798004508018494, loss=4.732110500335693
I0404 20:45:50.185611 139992596731648 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7694190740585327, loss=4.721823692321777
I0404 20:46:30.497098 139992034699008 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6988179087638855, loss=4.728114604949951
I0404 20:47:10.762461 139992596731648 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5119408369064331, loss=5.337197303771973
I0404 20:47:50.710818 139992034699008 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6316589117050171, loss=4.936753273010254
I0404 20:48:30.739698 139992596731648 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6988946199417114, loss=4.708853721618652
I0404 20:49:11.007690 139992034699008 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6696970462799072, loss=4.595693588256836
I0404 20:49:45.750635 140169517000512 submission_runner.py:373] Before eval at step 8388: RAM USED (GB) 124.223315968
I0404 20:49:45.750977 140169517000512 spec.py:298] Evaluating on the training split.
I0404 20:49:59.712167 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 20:50:09.319854 140169517000512 spec.py:326] Evaluating on the test split.
I0404 20:50:10.964687 140169517000512 submission_runner.py:382] Time since start: 3632.75s, 	Step: 8388, 	{'train/accuracy': 0.3361913859844208, 'train/loss': 3.2604820728302, 'validation/accuracy': 0.3120400011539459, 'validation/loss': 3.3918142318725586, 'validation/num_examples': 50000, 'test/accuracy': 0.24730001389980316, 'test/loss': 3.8822743892669678, 'test/num_examples': 10000}
I0404 20:50:10.965222 140169517000512 submission_runner.py:396] After eval at step 8388: RAM USED (GB) 128.968351744
I0404 20:50:10.982947 139992596731648 logging_writer.py:48] [8388] global_step=8388, preemption_count=0, score=3339.744169, test/accuracy=0.247300, test/loss=3.882274, test/num_examples=10000, total_duration=3632.750265, train/accuracy=0.336191, train/loss=3.260482, validation/accuracy=0.312040, validation/loss=3.391814, validation/num_examples=50000
I0404 20:50:11.128659 140169517000512 checkpoints.py:356] Saving checkpoint at step: 8388
I0404 20:50:12.370166 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_8388
I0404 20:50:12.393115 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_8388.
I0404 20:50:12.396283 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 8388: RAM USED (GB) 134.974889984
I0404 20:50:17.569265 139992034699008 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5844873189926147, loss=5.560586452484131
I0404 20:50:57.146222 139992026306304 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7140176892280579, loss=4.535115718841553
I0404 20:51:37.706019 139992034699008 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.4977017343044281, loss=6.29011344909668
I0404 20:52:17.913225 139992026306304 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8341007828712463, loss=4.557034492492676
I0404 20:52:58.097206 139992034699008 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6487125158309937, loss=4.664886474609375
I0404 20:53:38.222084 139992026306304 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6470304131507874, loss=4.6893696784973145
I0404 20:54:18.185647 139992034699008 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6485385298728943, loss=4.590664863586426
I0404 20:54:58.084014 139992026306304 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5796194076538086, loss=4.592731475830078
I0404 20:55:38.172113 139992034699008 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7489580512046814, loss=4.5496721267700195
I0404 20:56:18.205663 139992026306304 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6923782229423523, loss=4.388403415679932
I0404 20:56:58.488273 139992034699008 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.48129361867904663, loss=5.971713542938232
I0404 20:57:12.663499 140169517000512 submission_runner.py:373] Before eval at step 9437: RAM USED (GB) 129.96855808
I0404 20:57:12.663820 140169517000512 spec.py:298] Evaluating on the training split.
I0404 20:57:26.293638 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 20:57:37.172538 140169517000512 spec.py:326] Evaluating on the test split.
I0404 20:57:38.829112 140169517000512 submission_runner.py:382] Time since start: 4079.66s, 	Step: 9437, 	{'train/accuracy': 0.3753906190395355, 'train/loss': 3.025383234024048, 'validation/accuracy': 0.3392999768257141, 'validation/loss': 3.207136392593384, 'validation/num_examples': 50000, 'test/accuracy': 0.26510000228881836, 'test/loss': 3.724943161010742, 'test/num_examples': 10000}
I0404 20:57:38.829598 140169517000512 submission_runner.py:396] After eval at step 9437: RAM USED (GB) 137.074614272
I0404 20:57:38.837693 139992026306304 logging_writer.py:48] [9437] global_step=9437, preemption_count=0, score=3749.904514, test/accuracy=0.265100, test/loss=3.724943, test/num_examples=10000, total_duration=4079.662554, train/accuracy=0.375391, train/loss=3.025383, validation/accuracy=0.339300, validation/loss=3.207136, validation/num_examples=50000
I0404 20:57:38.940881 140169517000512 checkpoints.py:356] Saving checkpoint at step: 9437
I0404 20:57:39.961681 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_9437
I0404 20:57:39.980869 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_9437.
I0404 20:57:39.984540 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 9437: RAM USED (GB) 141.880324096
I0404 20:58:05.247019 139992034699008 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5615129470825195, loss=5.164722442626953
I0404 20:58:44.902284 139992017913600 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7958576083183289, loss=4.341256141662598
I0404 20:59:25.013292 139992034699008 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.4679223895072937, loss=5.47841739654541
I0404 21:00:05.283599 139992017913600 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5739918351173401, loss=4.452904224395752
I0404 21:00:45.652460 139992034699008 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5027716755867004, loss=5.746260166168213
I0404 21:01:26.126435 139992017913600 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6145257353782654, loss=4.9360032081604
I0404 21:02:06.890284 139992034699008 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.615824818611145, loss=5.748868465423584
I0404 21:02:47.235877 139992017913600 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7260746359825134, loss=4.399984359741211
I0404 21:03:27.677272 139992034699008 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6824885606765747, loss=4.372748374938965
I0404 21:04:08.148432 139992017913600 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5811030268669128, loss=4.319571495056152
I0404 21:04:40.347749 140169517000512 submission_runner.py:373] Before eval at step 10481: RAM USED (GB) 136.64227328
I0404 21:04:40.348076 140169517000512 spec.py:298] Evaluating on the training split.
I0404 21:04:54.006150 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 21:05:05.252789 140169517000512 spec.py:326] Evaluating on the test split.
I0404 21:05:06.907742 140169517000512 submission_runner.py:382] Time since start: 4527.35s, 	Step: 10481, 	{'train/accuracy': 0.40085935592651367, 'train/loss': 2.927534818649292, 'validation/accuracy': 0.3681199848651886, 'validation/loss': 3.0791518688201904, 'validation/num_examples': 50000, 'test/accuracy': 0.2833999991416931, 'test/loss': 3.630736827850342, 'test/num_examples': 10000}
I0404 21:05:06.908277 140169517000512 submission_runner.py:396] After eval at step 10481: RAM USED (GB) 142.694109184
I0404 21:05:06.928822 139992034699008 logging_writer.py:48] [10481] global_step=10481, preemption_count=0, score=4158.475700, test/accuracy=0.283400, test/loss=3.630737, test/num_examples=10000, total_duration=4527.350088, train/accuracy=0.400859, train/loss=2.927535, validation/accuracy=0.368120, validation/loss=3.079152, validation/num_examples=50000
I0404 21:05:07.088983 140169517000512 checkpoints.py:356] Saving checkpoint at step: 10481
I0404 21:05:08.349984 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_10481
I0404 21:05:08.371350 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_10481.
I0404 21:05:08.374541 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 10481: RAM USED (GB) 148.753620992
I0404 21:05:16.225049 139992017913600 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5152124762535095, loss=5.648192405700684
I0404 21:05:55.753811 139992009520896 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5535328984260559, loss=4.699792861938477
I0404 21:06:35.686708 139992017913600 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6384683847427368, loss=4.177653789520264
I0404 21:07:15.777235 139992009520896 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6372522711753845, loss=5.457663059234619
I0404 21:07:55.782462 139992017913600 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5279529094696045, loss=5.616836071014404
I0404 21:08:36.262536 139992009520896 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6539735794067383, loss=4.5353193283081055
I0404 21:09:16.446402 139992017913600 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6306806802749634, loss=4.21731424331665
I0404 21:09:56.535600 139992009520896 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6408212780952454, loss=4.135303020477295
I0404 21:10:36.915127 139992017913600 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6469079256057739, loss=4.386319637298584
I0404 21:11:17.374393 139992009520896 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6497722268104553, loss=4.4310760498046875
I0404 21:11:57.530083 139992017913600 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6391497850418091, loss=4.103021144866943
I0404 21:12:08.676356 140169517000512 submission_runner.py:373] Before eval at step 11529: RAM USED (GB) 142.28660224
I0404 21:12:08.676615 140169517000512 spec.py:298] Evaluating on the training split.
I0404 21:12:23.156100 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 21:12:33.821154 140169517000512 spec.py:326] Evaluating on the test split.
I0404 21:12:35.474636 140169517000512 submission_runner.py:382] Time since start: 4975.68s, 	Step: 11529, 	{'train/accuracy': 0.4470312297344208, 'train/loss': 2.577753782272339, 'validation/accuracy': 0.4108999967575073, 'validation/loss': 2.7618184089660645, 'validation/num_examples': 50000, 'test/accuracy': 0.31780001521110535, 'test/loss': 3.356403350830078, 'test/num_examples': 10000}
I0404 21:12:35.475170 140169517000512 submission_runner.py:396] After eval at step 11529: RAM USED (GB) 147.824214016
I0404 21:12:35.486787 139992009520896 logging_writer.py:48] [11529] global_step=11529, preemption_count=0, score=4568.004769, test/accuracy=0.317800, test/loss=3.356403, test/num_examples=10000, total_duration=4975.676434, train/accuracy=0.447031, train/loss=2.577754, validation/accuracy=0.410900, validation/loss=2.761818, validation/num_examples=50000
I0404 21:12:35.619441 140169517000512 checkpoints.py:356] Saving checkpoint at step: 11529
I0404 21:12:36.853547 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_11529
I0404 21:12:36.875978 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_11529.
I0404 21:12:36.879106 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 11529: RAM USED (GB) 153.719758848
I0404 21:13:05.384083 139992017913600 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6768554449081421, loss=4.185398578643799
I0404 21:13:45.385610 139991036458752 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6124900579452515, loss=4.795541286468506
I0404 21:14:25.766638 139992017913600 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6165596842765808, loss=4.099038124084473
I0404 21:15:06.031715 139991036458752 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5661414861679077, loss=5.357793807983398
I0404 21:15:47.000036 139992017913600 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8074097037315369, loss=4.114831924438477
I0404 21:16:27.715342 139991036458752 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6375102400779724, loss=4.091011047363281
I0404 21:17:07.769646 139992017913600 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6068957448005676, loss=4.093758583068848
I0404 21:17:48.432543 139991036458752 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6594635248184204, loss=4.096318244934082
I0404 21:18:28.577397 139992017913600 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.667414128780365, loss=4.709822654724121
I0404 21:19:08.924133 139991036458752 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6498764157295227, loss=4.642820835113525
I0404 21:19:37.267134 140169517000512 submission_runner.py:373] Before eval at step 12572: RAM USED (GB) 147.70434048
I0404 21:19:37.267455 140169517000512 spec.py:298] Evaluating on the training split.
I0404 21:19:52.338441 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 21:20:02.768392 140169517000512 spec.py:326] Evaluating on the test split.
I0404 21:20:04.419651 140169517000512 submission_runner.py:382] Time since start: 5424.27s, 	Step: 12572, 	{'train/accuracy': 0.46248045563697815, 'train/loss': 2.532460927963257, 'validation/accuracy': 0.4269599914550781, 'validation/loss': 2.701698064804077, 'validation/num_examples': 50000, 'test/accuracy': 0.32820001244544983, 'test/loss': 3.300185203552246, 'test/num_examples': 10000}
I0404 21:20:04.420367 140169517000512 submission_runner.py:396] After eval at step 12572: RAM USED (GB) 154.34164224
I0404 21:20:04.434936 139992017913600 logging_writer.py:48] [12572] global_step=12572, preemption_count=0, score=4976.581693, test/accuracy=0.328200, test/loss=3.300185, test/num_examples=10000, total_duration=5424.266800, train/accuracy=0.462480, train/loss=2.532461, validation/accuracy=0.426960, validation/loss=2.701698, validation/num_examples=50000
I0404 21:20:04.582024 140169517000512 checkpoints.py:356] Saving checkpoint at step: 12572
I0404 21:20:05.862431 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_12572
I0404 21:20:05.885149 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_12572.
I0404 21:20:05.888347 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 12572: RAM USED (GB) 160.493563904
I0404 21:20:17.394830 139991036458752 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5580072402954102, loss=5.169729232788086
I0404 21:20:57.111804 139989627172608 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.597737729549408, loss=4.0338850021362305
I0404 21:21:37.298581 139991036458752 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7337851524353027, loss=3.970822811126709
I0404 21:22:17.591645 139989627172608 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5761162638664246, loss=3.94974684715271
I0404 21:22:58.136467 139991036458752 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5203831195831299, loss=5.5756120681762695
I0404 21:23:38.389697 139989627172608 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6375809907913208, loss=4.192590713500977
I0404 21:24:18.605703 139991036458752 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.633527934551239, loss=3.9043827056884766
I0404 21:24:58.767719 139989627172608 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6396784782409668, loss=3.8900938034057617
I0404 21:25:39.047619 139991036458752 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.646371603012085, loss=5.61054801940918
I0404 21:26:19.170496 139989627172608 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6245359778404236, loss=4.060532569885254
I0404 21:26:59.401384 139991036458752 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.549579918384552, loss=4.865228176116943
I0404 21:27:05.988300 140169517000512 submission_runner.py:373] Before eval at step 13618: RAM USED (GB) 154.160836608
I0404 21:27:05.988628 140169517000512 spec.py:298] Evaluating on the training split.
I0404 21:27:20.444401 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 21:27:31.516747 140169517000512 spec.py:326] Evaluating on the test split.
I0404 21:27:33.176860 140169517000512 submission_runner.py:382] Time since start: 5872.99s, 	Step: 13618, 	{'train/accuracy': 0.4950195252895355, 'train/loss': 2.3433725833892822, 'validation/accuracy': 0.4491199851036072, 'validation/loss': 2.567450523376465, 'validation/num_examples': 50000, 'test/accuracy': 0.3522000312805176, 'test/loss': 3.180906057357788, 'test/num_examples': 10000}
I0404 21:27:33.177436 140169517000512 submission_runner.py:396] After eval at step 13618: RAM USED (GB) 159.51081472
I0404 21:27:33.191935 139989627172608 logging_writer.py:48] [13618] global_step=13618, preemption_count=0, score=5385.045358, test/accuracy=0.352200, test/loss=3.180906, test/num_examples=10000, total_duration=5872.985769, train/accuracy=0.495020, train/loss=2.343373, validation/accuracy=0.449120, validation/loss=2.567451, validation/num_examples=50000
I0404 21:27:33.338421 140169517000512 checkpoints.py:356] Saving checkpoint at step: 13618
I0404 21:27:34.603756 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_13618
I0404 21:27:34.627473 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_13618.
I0404 21:27:34.630591 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 13618: RAM USED (GB) 165.612855296
I0404 21:28:07.421845 139991036458752 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5732361674308777, loss=4.901396751403809
I0404 21:28:47.775593 139989618779904 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6637121438980103, loss=3.807018756866455
I0404 21:29:27.937888 139991036458752 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7525690793991089, loss=3.894641637802124
I0404 21:30:07.548325 140169517000512 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 160.27529216
I0404 21:30:07.548684 140169517000512 spec.py:298] Evaluating on the training split.
I0404 21:30:21.834765 140169517000512 spec.py:310] Evaluating on the validation split.
I0404 21:30:32.885061 140169517000512 spec.py:326] Evaluating on the test split.
I0404 21:30:34.542696 140169517000512 submission_runner.py:382] Time since start: 6054.55s, 	Step: 14000, 	{'train/accuracy': 0.49503904581069946, 'train/loss': 2.282902240753174, 'validation/accuracy': 0.45809999108314514, 'validation/loss': 2.4766836166381836, 'validation/num_examples': 50000, 'test/accuracy': 0.35670000314712524, 'test/loss': 3.0800671577453613, 'test/num_examples': 10000}
I0404 21:30:34.543251 140169517000512 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 165.608984576
I0404 21:30:34.557768 139989618779904 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5533.014826, test/accuracy=0.356700, test/loss=3.080067, test/num_examples=10000, total_duration=6054.550215, train/accuracy=0.495039, train/loss=2.282902, validation/accuracy=0.458100, validation/loss=2.476684, validation/num_examples=50000
I0404 21:30:34.703636 140169517000512 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 21:30:35.873404 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_14000
I0404 21:30:35.894469 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_14000.
I0404 21:30:35.897211 140169517000512 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 171.337740288
I0404 21:30:35.906342 139991036458752 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5533.014826
I0404 21:30:36.024893 140169517000512 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 21:30:37.416825 140169517000512 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_14000
I0404 21:30:37.435534 140169517000512 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_vit_jax/trial_1/checkpoint_14000.
I0404 21:30:38.262522 140169517000512 submission_runner.py:550] Tuning trial 1/1
I0404 21:30:38.262722 140169517000512 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0404 21:30:38.266915 140169517000512 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009960937313735485, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 48.812073707580566, 'total_duration': 48.89174437522888, 'global_step': 1, 'preemption_count': 0}), (1018, {'train/accuracy': 0.02529296837747097, 'train/loss': 6.127302169799805, 'validation/accuracy': 0.024159999564290047, 'validation/loss': 6.146903991699219, 'validation/num_examples': 50000, 'test/accuracy': 0.018800001591444016, 'test/loss': 6.229177951812744, 'test/num_examples': 10000, 'score': 460.94757890701294, 'total_duration': 517.9624862670898, 'global_step': 1018, 'preemption_count': 0}), (2073, {'train/accuracy': 0.06554687023162842, 'train/loss': 5.470003128051758, 'validation/accuracy': 0.06227999925613403, 'validation/loss': 5.505962371826172, 'validation/num_examples': 50000, 'test/accuracy': 0.04990000277757645, 'test/loss': 5.686747074127197, 'test/num_examples': 10000, 'score': 873.152259349823, 'total_duration': 960.6559467315674, 'global_step': 2073, 'preemption_count': 0}), (3129, {'train/accuracy': 0.11753906309604645, 'train/loss': 4.8886613845825195, 'validation/accuracy': 0.11285999417304993, 'validation/loss': 4.940011978149414, 'validation/num_examples': 50000, 'test/accuracy': 0.08190000057220459, 'test/loss': 5.198060035705566, 'test/num_examples': 10000, 'score': 1285.1165175437927, 'total_duration': 1404.1676564216614, 'global_step': 3129, 'preemption_count': 0}), (4183, {'train/accuracy': 0.1609765589237213, 'train/loss': 4.5238165855407715, 'validation/accuracy': 0.1485999971628189, 'validation/loss': 4.616313457489014, 'validation/num_examples': 50000, 'test/accuracy': 0.11390000581741333, 'test/loss': 4.929190635681152, 'test/num_examples': 10000, 'score': 1697.3927116394043, 'total_duration': 1847.7690811157227, 'global_step': 4183, 'preemption_count': 0}), (5238, {'train/accuracy': 0.2009570300579071, 'train/loss': 4.178868770599365, 'validation/accuracy': 0.186039999127388, 'validation/loss': 4.271564960479736, 'validation/num_examples': 50000, 'test/accuracy': 0.1421000063419342, 'test/loss': 4.6551666259765625, 'test/num_examples': 10000, 'score': 2109.2609786987305, 'total_duration': 2293.247309923172, 'global_step': 5238, 'preemption_count': 0}), (6287, {'train/accuracy': 0.24074217677116394, 'train/loss': 3.9561846256256104, 'validation/accuracy': 0.22360000014305115, 'validation/loss': 4.064438819885254, 'validation/num_examples': 50000, 'test/accuracy': 0.170400008559227, 'test/loss': 4.474737644195557, 'test/num_examples': 10000, 'score': 2518.8953790664673, 'total_duration': 2739.0294029712677, 'global_step': 6287, 'preemption_count': 0}), (7337, {'train/accuracy': 0.28605467081069946, 'train/loss': 3.5760562419891357, 'validation/accuracy': 0.26767998933792114, 'validation/loss': 3.687535285949707, 'validation/num_examples': 50000, 'test/accuracy': 0.2046000063419342, 'test/loss': 4.1556396484375, 'test/num_examples': 10000, 'score': 2929.3793444633484, 'total_duration': 3185.7479310035706, 'global_step': 7337, 'preemption_count': 0}), (8388, {'train/accuracy': 0.3361913859844208, 'train/loss': 3.2604820728302, 'validation/accuracy': 0.3120400011539459, 'validation/loss': 3.3918142318725586, 'validation/num_examples': 50000, 'test/accuracy': 0.24730001389980316, 'test/loss': 3.8822743892669678, 'test/num_examples': 10000, 'score': 3339.7441685199738, 'total_duration': 3632.7502648830414, 'global_step': 8388, 'preemption_count': 0}), (9437, {'train/accuracy': 0.3753906190395355, 'train/loss': 3.025383234024048, 'validation/accuracy': 0.3392999768257141, 'validation/loss': 3.207136392593384, 'validation/num_examples': 50000, 'test/accuracy': 0.26510000228881836, 'test/loss': 3.724943161010742, 'test/num_examples': 10000, 'score': 3749.9045140743256, 'total_duration': 4079.6625542640686, 'global_step': 9437, 'preemption_count': 0}), (10481, {'train/accuracy': 0.40085935592651367, 'train/loss': 2.927534818649292, 'validation/accuracy': 0.3681199848651886, 'validation/loss': 3.0791518688201904, 'validation/num_examples': 50000, 'test/accuracy': 0.2833999991416931, 'test/loss': 3.630736827850342, 'test/num_examples': 10000, 'score': 4158.475699901581, 'total_duration': 4527.350087881088, 'global_step': 10481, 'preemption_count': 0}), (11529, {'train/accuracy': 0.4470312297344208, 'train/loss': 2.577753782272339, 'validation/accuracy': 0.4108999967575073, 'validation/loss': 2.7618184089660645, 'validation/num_examples': 50000, 'test/accuracy': 0.31780001521110535, 'test/loss': 3.356403350830078, 'test/num_examples': 10000, 'score': 4568.004768610001, 'total_duration': 4975.676433801651, 'global_step': 11529, 'preemption_count': 0}), (12572, {'train/accuracy': 0.46248045563697815, 'train/loss': 2.532460927963257, 'validation/accuracy': 0.4269599914550781, 'validation/loss': 2.701698064804077, 'validation/num_examples': 50000, 'test/accuracy': 0.32820001244544983, 'test/loss': 3.300185203552246, 'test/num_examples': 10000, 'score': 4976.581693410873, 'total_duration': 5424.266800403595, 'global_step': 12572, 'preemption_count': 0}), (13618, {'train/accuracy': 0.4950195252895355, 'train/loss': 2.3433725833892822, 'validation/accuracy': 0.4491199851036072, 'validation/loss': 2.567450523376465, 'validation/num_examples': 50000, 'test/accuracy': 0.3522000312805176, 'test/loss': 3.180906057357788, 'test/num_examples': 10000, 'score': 5385.045357704163, 'total_duration': 5872.985769033432, 'global_step': 13618, 'preemption_count': 0}), (14000, {'train/accuracy': 0.49503904581069946, 'train/loss': 2.282902240753174, 'validation/accuracy': 0.45809999108314514, 'validation/loss': 2.4766836166381836, 'validation/num_examples': 50000, 'test/accuracy': 0.35670000314712524, 'test/loss': 3.0800671577453613, 'test/num_examples': 10000, 'score': 5533.014825582504, 'total_duration': 6054.550214767456, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0404 21:30:38.267046 140169517000512 submission_runner.py:553] Timing: 5533.014825582504
I0404 21:30:38.267090 140169517000512 submission_runner.py:554] ====================
I0404 21:30:38.267187 140169517000512 submission_runner.py:613] Final imagenet_vit score: 5533.014825582504
