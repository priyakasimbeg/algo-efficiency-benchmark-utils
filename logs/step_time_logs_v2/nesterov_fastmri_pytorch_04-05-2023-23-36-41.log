WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 23:37:03.907906 140554164913984 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 23:37:03.907944 140103426230080 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 23:37:03.907975 140187073738560 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 23:37:03.908723 139795510474560 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 23:37:04.895389 140044525696832 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 23:37:04.895665 140606207977280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 23:37:04.895794 140636194756416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 23:37:04.905944 140044525696832 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:37:04.905973 140570875250496 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 23:37:04.906166 140606207977280 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:37:04.906312 140570875250496 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:37:04.906389 140636194756416 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:37:04.915241 140554164913984 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:37:04.915301 140187073738560 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:37:04.915328 139795510474560 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:37:04.915323 140103426230080 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:37:05.446369 140570875250496 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch.
W0405 23:37:05.565661 140554164913984 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:37:05.565675 140103426230080 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:37:05.565716 140187073738560 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:37:05.566695 140570875250496 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:37:05.566996 140606207977280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:37:05.567265 139795510474560 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:37:05.567525 140636194756416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:37:05.570071 140044525696832 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 23:37:05.572829 140570875250496 submission_runner.py:511] Using RNG seed 62984215
I0405 23:37:05.573779 140570875250496 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 23:37:05.573884 140570875250496 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1.
I0405 23:37:05.574085 140570875250496 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/hparams.json.
I0405 23:37:05.574961 140570875250496 submission_runner.py:230] Starting train once: RAM USED (GB) 5.586931712
I0405 23:37:05.575052 140570875250496 submission_runner.py:231] Initializing dataset.
I0405 23:37:05.575214 140570875250496 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.586931712
I0405 23:37:05.575279 140570875250496 submission_runner.py:240] Initializing model.
I0405 23:37:09.880489 140570875250496 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.211507712
I0405 23:37:09.880671 140570875250496 submission_runner.py:252] Initializing optimizer.
I0405 23:37:10.448008 140570875250496 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.216050176
I0405 23:37:10.448207 140570875250496 submission_runner.py:261] Initializing metrics bundle.
I0405 23:37:10.448264 140570875250496 submission_runner.py:276] Initializing checkpoint and logger.
I0405 23:37:10.451654 140570875250496 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 23:37:10.451822 140570875250496 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 23:37:11.037959 140570875250496 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/meta_data_0.json.
I0405 23:37:11.038954 140570875250496 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/flags_0.json.
I0405 23:37:11.081203 140570875250496 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.26800384
I0405 23:37:11.082264 140570875250496 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.26800384
I0405 23:37:11.082383 140570875250496 submission_runner.py:313] Starting training loop.
I0405 23:37:52.921476 140570875250496 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 23.980208128
I0405 23:37:56.884975 140528729446144 logging_writer.py:48] [0] global_step=0, grad_norm=2.116896, loss=0.584466
I0405 23:37:56.893065 140570875250496 submission.py:139] 0) loss = 0.584, grad_norm = 2.117
I0405 23:37:56.893821 140570875250496 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 32.405843968
I0405 23:37:56.894564 140570875250496 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 32.406360064
I0405 23:37:56.894711 140570875250496 spec.py:298] Evaluating on the training split.
I0405 23:39:36.781796 140570875250496 spec.py:310] Evaluating on the validation split.
I0405 23:40:39.876199 140570875250496 spec.py:326] Evaluating on the test split.
I0405 23:41:42.250171 140570875250496 submission_runner.py:382] Time since start: 45.81s, 	Step: 1, 	{'train/ssim': 0.3042278289794922, 'train/loss': 0.6253479548863002, 'validation/ssim': 0.2952190135125035, 'validation/loss': 0.6419411886254924, 'validation/num_examples': 3554, 'test/ssim': 0.3184590492791818, 'test/loss': 0.6413412640062134, 'test/num_examples': 3581}
I0405 23:41:42.250632 140570875250496 submission_runner.py:396] After eval at step 1: RAM USED (GB) 68.711686144
I0405 23:41:42.260018 140505266505472 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=45.810710, test/loss=0.641341, test/num_examples=3581, test/ssim=0.318459, total_duration=45.812635, train/loss=0.625348, train/ssim=0.304228, validation/loss=0.641941, validation/num_examples=3554, validation/ssim=0.295219
I0405 23:41:42.358699 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_1.
I0405 23:41:42.359149 140570875250496 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 68.712177664
I0405 23:41:42.368202 140570875250496 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 68.724273152
I0405 23:41:42.371660 140606207977280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:42.371666 140187073738560 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:42.371666 140554164913984 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:42.371676 140044525696832 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:42.371715 140636194756416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:42.371726 139795510474560 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:42.371716 140103426230080 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:42.372225 140570875250496 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:41:42.431391 140505258112768 logging_writer.py:48] [1] global_step=1, grad_norm=1.902872, loss=0.618006
I0405 23:41:42.436591 140570875250496 submission.py:139] 1) loss = 0.618, grad_norm = 1.903
I0405 23:41:42.437858 140570875250496 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 68.73962496
I0405 23:41:42.508234 140505266505472 logging_writer.py:48] [2] global_step=2, grad_norm=1.935557, loss=0.557964
I0405 23:41:42.513518 140570875250496 submission.py:139] 2) loss = 0.558, grad_norm = 1.936
I0405 23:41:42.581750 140505258112768 logging_writer.py:48] [3] global_step=3, grad_norm=1.948259, loss=0.635675
I0405 23:41:42.584936 140570875250496 submission.py:139] 3) loss = 0.636, grad_norm = 1.948
I0405 23:41:42.647782 140505266505472 logging_writer.py:48] [4] global_step=4, grad_norm=1.734146, loss=0.625327
I0405 23:41:42.650994 140570875250496 submission.py:139] 4) loss = 0.625, grad_norm = 1.734
I0405 23:41:42.719929 140505258112768 logging_writer.py:48] [5] global_step=5, grad_norm=1.394483, loss=0.608307
I0405 23:41:42.726845 140570875250496 submission.py:139] 5) loss = 0.608, grad_norm = 1.394
I0405 23:41:42.795202 140505266505472 logging_writer.py:48] [6] global_step=6, grad_norm=1.062048, loss=0.610550
I0405 23:41:42.800135 140570875250496 submission.py:139] 6) loss = 0.611, grad_norm = 1.062
I0405 23:41:42.870885 140505258112768 logging_writer.py:48] [7] global_step=7, grad_norm=0.969796, loss=0.553833
I0405 23:41:42.876487 140570875250496 submission.py:139] 7) loss = 0.554, grad_norm = 0.970
I0405 23:41:42.941492 140505266505472 logging_writer.py:48] [8] global_step=8, grad_norm=0.946881, loss=0.589991
I0405 23:41:42.944836 140570875250496 submission.py:139] 8) loss = 0.590, grad_norm = 0.947
I0405 23:41:43.008903 140505258112768 logging_writer.py:48] [9] global_step=9, grad_norm=1.011708, loss=0.605883
I0405 23:41:43.014043 140570875250496 submission.py:139] 9) loss = 0.606, grad_norm = 1.012
I0405 23:41:43.081506 140505266505472 logging_writer.py:48] [10] global_step=10, grad_norm=1.110508, loss=0.494804
I0405 23:41:43.087056 140570875250496 submission.py:139] 10) loss = 0.495, grad_norm = 1.111
I0405 23:41:43.155623 140505258112768 logging_writer.py:48] [11] global_step=11, grad_norm=1.219137, loss=0.569899
I0405 23:41:43.160131 140570875250496 submission.py:139] 11) loss = 0.570, grad_norm = 1.219
I0405 23:41:43.224370 140505266505472 logging_writer.py:48] [12] global_step=12, grad_norm=1.117341, loss=0.517464
I0405 23:41:43.227730 140570875250496 submission.py:139] 12) loss = 0.517, grad_norm = 1.117
I0405 23:41:43.292060 140505258112768 logging_writer.py:48] [13] global_step=13, grad_norm=1.070490, loss=0.539513
I0405 23:41:43.297655 140570875250496 submission.py:139] 13) loss = 0.540, grad_norm = 1.070
I0405 23:41:43.509211 140505266505472 logging_writer.py:48] [14] global_step=14, grad_norm=0.853885, loss=0.471067
I0405 23:41:43.512664 140570875250496 submission.py:139] 14) loss = 0.471, grad_norm = 0.854
I0405 23:41:43.755535 140505258112768 logging_writer.py:48] [15] global_step=15, grad_norm=0.614057, loss=0.398018
I0405 23:41:43.763450 140570875250496 submission.py:139] 15) loss = 0.398, grad_norm = 0.614
I0405 23:41:43.980552 140505266505472 logging_writer.py:48] [16] global_step=16, grad_norm=0.538340, loss=0.441336
I0405 23:41:43.986381 140570875250496 submission.py:139] 16) loss = 0.441, grad_norm = 0.538
I0405 23:41:44.288670 140505258112768 logging_writer.py:48] [17] global_step=17, grad_norm=0.629196, loss=0.422432
I0405 23:41:44.294354 140570875250496 submission.py:139] 17) loss = 0.422, grad_norm = 0.629
I0405 23:41:44.586659 140505266505472 logging_writer.py:48] [18] global_step=18, grad_norm=0.742526, loss=0.417412
I0405 23:41:44.590012 140570875250496 submission.py:139] 18) loss = 0.417, grad_norm = 0.743
I0405 23:41:44.928339 140505258112768 logging_writer.py:48] [19] global_step=19, grad_norm=0.680774, loss=0.503357
I0405 23:41:44.933131 140570875250496 submission.py:139] 19) loss = 0.503, grad_norm = 0.681
I0405 23:41:45.120974 140505266505472 logging_writer.py:48] [20] global_step=20, grad_norm=0.767748, loss=0.424749
I0405 23:41:45.127757 140570875250496 submission.py:139] 20) loss = 0.425, grad_norm = 0.768
I0405 23:41:45.388478 140505258112768 logging_writer.py:48] [21] global_step=21, grad_norm=0.367817, loss=0.505409
I0405 23:41:45.394346 140570875250496 submission.py:139] 21) loss = 0.505, grad_norm = 0.368
I0405 23:41:45.687558 140505266505472 logging_writer.py:48] [22] global_step=22, grad_norm=0.540872, loss=0.410425
I0405 23:41:45.692745 140570875250496 submission.py:139] 22) loss = 0.410, grad_norm = 0.541
I0405 23:41:45.939970 140505258112768 logging_writer.py:48] [23] global_step=23, grad_norm=0.663994, loss=0.351592
I0405 23:41:45.945375 140570875250496 submission.py:139] 23) loss = 0.352, grad_norm = 0.664
I0405 23:41:46.234493 140505266505472 logging_writer.py:48] [24] global_step=24, grad_norm=0.419190, loss=0.411712
I0405 23:41:46.239934 140570875250496 submission.py:139] 24) loss = 0.412, grad_norm = 0.419
I0405 23:41:46.448956 140505258112768 logging_writer.py:48] [25] global_step=25, grad_norm=0.548993, loss=0.369670
I0405 23:41:46.454789 140570875250496 submission.py:139] 25) loss = 0.370, grad_norm = 0.549
I0405 23:41:46.699447 140505266505472 logging_writer.py:48] [26] global_step=26, grad_norm=0.421348, loss=0.409282
I0405 23:41:46.702674 140570875250496 submission.py:139] 26) loss = 0.409, grad_norm = 0.421
I0405 23:41:47.024243 140505258112768 logging_writer.py:48] [27] global_step=27, grad_norm=0.462354, loss=0.338647
I0405 23:41:47.030596 140570875250496 submission.py:139] 27) loss = 0.339, grad_norm = 0.462
I0405 23:41:47.237721 140505266505472 logging_writer.py:48] [28] global_step=28, grad_norm=0.357570, loss=0.353256
I0405 23:41:47.240938 140570875250496 submission.py:139] 28) loss = 0.353, grad_norm = 0.358
I0405 23:41:47.480506 140505258112768 logging_writer.py:48] [29] global_step=29, grad_norm=0.285802, loss=0.408479
I0405 23:41:47.484013 140570875250496 submission.py:139] 29) loss = 0.408, grad_norm = 0.286
I0405 23:41:47.774934 140505266505472 logging_writer.py:48] [30] global_step=30, grad_norm=0.298029, loss=0.364549
I0405 23:41:47.778061 140570875250496 submission.py:139] 30) loss = 0.365, grad_norm = 0.298
I0405 23:41:48.049355 140505258112768 logging_writer.py:48] [31] global_step=31, grad_norm=0.180988, loss=0.347432
I0405 23:41:48.052624 140570875250496 submission.py:139] 31) loss = 0.347, grad_norm = 0.181
I0405 23:41:48.328424 140505266505472 logging_writer.py:48] [32] global_step=32, grad_norm=0.250307, loss=0.428390
I0405 23:41:48.332145 140570875250496 submission.py:139] 32) loss = 0.428, grad_norm = 0.250
I0405 23:41:48.592526 140505258112768 logging_writer.py:48] [33] global_step=33, grad_norm=0.286066, loss=0.347328
I0405 23:41:48.597339 140570875250496 submission.py:139] 33) loss = 0.347, grad_norm = 0.286
I0405 23:41:48.886871 140505266505472 logging_writer.py:48] [34] global_step=34, grad_norm=0.251329, loss=0.312132
I0405 23:41:48.890349 140570875250496 submission.py:139] 34) loss = 0.312, grad_norm = 0.251
I0405 23:41:49.165051 140505258112768 logging_writer.py:48] [35] global_step=35, grad_norm=0.224317, loss=0.378013
I0405 23:41:49.171030 140570875250496 submission.py:139] 35) loss = 0.378, grad_norm = 0.224
I0405 23:41:49.398160 140505266505472 logging_writer.py:48] [36] global_step=36, grad_norm=0.294529, loss=0.312495
I0405 23:41:49.403259 140570875250496 submission.py:139] 36) loss = 0.312, grad_norm = 0.295
I0405 23:41:49.617814 140505258112768 logging_writer.py:48] [37] global_step=37, grad_norm=0.183398, loss=0.331774
I0405 23:41:49.622957 140570875250496 submission.py:139] 37) loss = 0.332, grad_norm = 0.183
I0405 23:41:49.887105 140505266505472 logging_writer.py:48] [38] global_step=38, grad_norm=0.274630, loss=0.281184
I0405 23:41:49.892276 140570875250496 submission.py:139] 38) loss = 0.281, grad_norm = 0.275
I0405 23:41:50.178255 140505258112768 logging_writer.py:48] [39] global_step=39, grad_norm=0.223801, loss=0.303335
I0405 23:41:50.184521 140570875250496 submission.py:139] 39) loss = 0.303, grad_norm = 0.224
I0405 23:41:50.463188 140505266505472 logging_writer.py:48] [40] global_step=40, grad_norm=0.194610, loss=0.346226
I0405 23:41:50.467784 140570875250496 submission.py:139] 40) loss = 0.346, grad_norm = 0.195
I0405 23:41:50.726150 140505258112768 logging_writer.py:48] [41] global_step=41, grad_norm=0.130107, loss=0.287874
I0405 23:41:50.733123 140570875250496 submission.py:139] 41) loss = 0.288, grad_norm = 0.130
I0405 23:41:51.000612 140505266505472 logging_writer.py:48] [42] global_step=42, grad_norm=0.265936, loss=0.326276
I0405 23:41:51.005523 140570875250496 submission.py:139] 42) loss = 0.326, grad_norm = 0.266
I0405 23:41:51.232908 140505258112768 logging_writer.py:48] [43] global_step=43, grad_norm=0.211525, loss=0.271626
I0405 23:41:51.239691 140570875250496 submission.py:139] 43) loss = 0.272, grad_norm = 0.212
I0405 23:41:51.481607 140505266505472 logging_writer.py:48] [44] global_step=44, grad_norm=0.261111, loss=0.301880
I0405 23:41:51.485519 140570875250496 submission.py:139] 44) loss = 0.302, grad_norm = 0.261
I0405 23:41:51.760001 140505258112768 logging_writer.py:48] [45] global_step=45, grad_norm=0.235280, loss=0.367537
I0405 23:41:51.763318 140570875250496 submission.py:139] 45) loss = 0.368, grad_norm = 0.235
I0405 23:41:52.033723 140505266505472 logging_writer.py:48] [46] global_step=46, grad_norm=0.258994, loss=0.405171
I0405 23:41:52.037209 140570875250496 submission.py:139] 46) loss = 0.405, grad_norm = 0.259
I0405 23:41:52.269884 140505258112768 logging_writer.py:48] [47] global_step=47, grad_norm=0.179140, loss=0.483499
I0405 23:41:52.273074 140570875250496 submission.py:139] 47) loss = 0.483, grad_norm = 0.179
I0405 23:41:52.552578 140505266505472 logging_writer.py:48] [48] global_step=48, grad_norm=0.176330, loss=0.379188
I0405 23:41:52.556142 140570875250496 submission.py:139] 48) loss = 0.379, grad_norm = 0.176
I0405 23:41:52.796836 140505258112768 logging_writer.py:48] [49] global_step=49, grad_norm=0.230800, loss=0.506608
I0405 23:41:52.804756 140570875250496 submission.py:139] 49) loss = 0.507, grad_norm = 0.231
I0405 23:41:53.092328 140505266505472 logging_writer.py:48] [50] global_step=50, grad_norm=0.108628, loss=0.407431
I0405 23:41:53.095854 140570875250496 submission.py:139] 50) loss = 0.407, grad_norm = 0.109
I0405 23:41:53.308598 140505258112768 logging_writer.py:48] [51] global_step=51, grad_norm=0.205940, loss=0.441158
I0405 23:41:53.314064 140570875250496 submission.py:139] 51) loss = 0.441, grad_norm = 0.206
I0405 23:41:53.581994 140505266505472 logging_writer.py:48] [52] global_step=52, grad_norm=0.270187, loss=0.390273
I0405 23:41:53.587251 140570875250496 submission.py:139] 52) loss = 0.390, grad_norm = 0.270
I0405 23:41:53.845506 140505258112768 logging_writer.py:48] [53] global_step=53, grad_norm=0.270630, loss=0.299814
I0405 23:41:53.851937 140570875250496 submission.py:139] 53) loss = 0.300, grad_norm = 0.271
I0405 23:41:54.134734 140505266505472 logging_writer.py:48] [54] global_step=54, grad_norm=0.137325, loss=0.396967
I0405 23:41:54.140121 140570875250496 submission.py:139] 54) loss = 0.397, grad_norm = 0.137
I0405 23:41:54.387239 140505258112768 logging_writer.py:48] [55] global_step=55, grad_norm=0.111201, loss=0.367726
I0405 23:41:54.392391 140570875250496 submission.py:139] 55) loss = 0.368, grad_norm = 0.111
I0405 23:41:54.687011 140505266505472 logging_writer.py:48] [56] global_step=56, grad_norm=0.130650, loss=0.397506
I0405 23:41:54.692092 140570875250496 submission.py:139] 56) loss = 0.398, grad_norm = 0.131
I0405 23:41:54.941655 140505258112768 logging_writer.py:48] [57] global_step=57, grad_norm=0.147947, loss=0.335450
I0405 23:41:54.947398 140570875250496 submission.py:139] 57) loss = 0.335, grad_norm = 0.148
I0405 23:41:55.206574 140505266505472 logging_writer.py:48] [58] global_step=58, grad_norm=0.167477, loss=0.338005
I0405 23:41:55.211548 140570875250496 submission.py:139] 58) loss = 0.338, grad_norm = 0.167
I0405 23:41:55.512435 140505258112768 logging_writer.py:48] [59] global_step=59, grad_norm=0.097381, loss=0.349664
I0405 23:41:55.517805 140570875250496 submission.py:139] 59) loss = 0.350, grad_norm = 0.097
I0405 23:41:55.811907 140505266505472 logging_writer.py:48] [60] global_step=60, grad_norm=0.154050, loss=0.362144
I0405 23:41:55.816058 140570875250496 submission.py:139] 60) loss = 0.362, grad_norm = 0.154
I0405 23:41:56.053881 140505258112768 logging_writer.py:48] [61] global_step=61, grad_norm=0.100521, loss=0.319429
I0405 23:41:56.058912 140570875250496 submission.py:139] 61) loss = 0.319, grad_norm = 0.101
I0405 23:41:56.352515 140505266505472 logging_writer.py:48] [62] global_step=62, grad_norm=0.094224, loss=0.283828
I0405 23:41:56.357991 140570875250496 submission.py:139] 62) loss = 0.284, grad_norm = 0.094
I0405 23:41:56.623631 140505258112768 logging_writer.py:48] [63] global_step=63, grad_norm=0.138927, loss=0.252537
I0405 23:41:56.626968 140570875250496 submission.py:139] 63) loss = 0.253, grad_norm = 0.139
I0405 23:41:56.862126 140505266505472 logging_writer.py:48] [64] global_step=64, grad_norm=0.140819, loss=0.299068
I0405 23:41:56.868029 140570875250496 submission.py:139] 64) loss = 0.299, grad_norm = 0.141
I0405 23:41:57.176574 140505258112768 logging_writer.py:48] [65] global_step=65, grad_norm=0.103416, loss=0.362231
I0405 23:41:57.182106 140570875250496 submission.py:139] 65) loss = 0.362, grad_norm = 0.103
I0405 23:41:57.487592 140505266505472 logging_writer.py:48] [66] global_step=66, grad_norm=0.263287, loss=0.369089
I0405 23:41:57.490699 140570875250496 submission.py:139] 66) loss = 0.369, grad_norm = 0.263
I0405 23:41:57.757064 140505258112768 logging_writer.py:48] [67] global_step=67, grad_norm=0.088368, loss=0.350048
I0405 23:41:57.761446 140570875250496 submission.py:139] 67) loss = 0.350, grad_norm = 0.088
I0405 23:41:58.063971 140505266505472 logging_writer.py:48] [68] global_step=68, grad_norm=0.216632, loss=0.280074
I0405 23:41:58.069688 140570875250496 submission.py:139] 68) loss = 0.280, grad_norm = 0.217
I0405 23:41:58.272270 140505258112768 logging_writer.py:48] [69] global_step=69, grad_norm=0.068097, loss=0.381191
I0405 23:41:58.275688 140570875250496 submission.py:139] 69) loss = 0.381, grad_norm = 0.068
I0405 23:41:58.586646 140505266505472 logging_writer.py:48] [70] global_step=70, grad_norm=0.136596, loss=0.251035
I0405 23:41:58.590207 140570875250496 submission.py:139] 70) loss = 0.251, grad_norm = 0.137
I0405 23:41:58.826677 140505258112768 logging_writer.py:48] [71] global_step=71, grad_norm=0.139137, loss=0.363785
I0405 23:41:58.830165 140570875250496 submission.py:139] 71) loss = 0.364, grad_norm = 0.139
I0405 23:41:59.051424 140505266505472 logging_writer.py:48] [72] global_step=72, grad_norm=0.080800, loss=0.250717
I0405 23:41:59.054758 140570875250496 submission.py:139] 72) loss = 0.251, grad_norm = 0.081
I0405 23:41:59.296020 140505258112768 logging_writer.py:48] [73] global_step=73, grad_norm=0.099909, loss=0.340737
I0405 23:41:59.301903 140570875250496 submission.py:139] 73) loss = 0.341, grad_norm = 0.100
I0405 23:41:59.537316 140505266505472 logging_writer.py:48] [74] global_step=74, grad_norm=0.098320, loss=0.336911
I0405 23:41:59.543730 140570875250496 submission.py:139] 74) loss = 0.337, grad_norm = 0.098
I0405 23:41:59.827872 140505258112768 logging_writer.py:48] [75] global_step=75, grad_norm=0.092686, loss=0.253598
I0405 23:41:59.833604 140570875250496 submission.py:139] 75) loss = 0.254, grad_norm = 0.093
I0405 23:42:00.122359 140505266505472 logging_writer.py:48] [76] global_step=76, grad_norm=0.121511, loss=0.298755
I0405 23:42:00.128575 140570875250496 submission.py:139] 76) loss = 0.299, grad_norm = 0.122
I0405 23:42:00.319256 140505258112768 logging_writer.py:48] [77] global_step=77, grad_norm=0.116375, loss=0.392507
I0405 23:42:00.324774 140570875250496 submission.py:139] 77) loss = 0.393, grad_norm = 0.116
I0405 23:42:00.604718 140505266505472 logging_writer.py:48] [78] global_step=78, grad_norm=0.192255, loss=0.254523
I0405 23:42:00.609894 140570875250496 submission.py:139] 78) loss = 0.255, grad_norm = 0.192
I0405 23:42:00.873826 140505258112768 logging_writer.py:48] [79] global_step=79, grad_norm=0.111791, loss=0.215492
I0405 23:42:00.877047 140570875250496 submission.py:139] 79) loss = 0.215, grad_norm = 0.112
I0405 23:42:01.181015 140505266505472 logging_writer.py:48] [80] global_step=80, grad_norm=0.153708, loss=0.207563
I0405 23:42:01.185333 140570875250496 submission.py:139] 80) loss = 0.208, grad_norm = 0.154
I0405 23:42:01.468831 140505258112768 logging_writer.py:48] [81] global_step=81, grad_norm=0.257826, loss=0.286780
I0405 23:42:01.472124 140570875250496 submission.py:139] 81) loss = 0.287, grad_norm = 0.258
I0405 23:42:01.769489 140505266505472 logging_writer.py:48] [82] global_step=82, grad_norm=0.119469, loss=0.267871
I0405 23:42:01.773062 140570875250496 submission.py:139] 82) loss = 0.268, grad_norm = 0.119
I0405 23:42:02.023244 140505258112768 logging_writer.py:48] [83] global_step=83, grad_norm=0.151395, loss=0.296672
I0405 23:42:02.026727 140570875250496 submission.py:139] 83) loss = 0.297, grad_norm = 0.151
I0405 23:42:02.288582 140505266505472 logging_writer.py:48] [84] global_step=84, grad_norm=0.145747, loss=0.379579
I0405 23:42:02.294018 140570875250496 submission.py:139] 84) loss = 0.380, grad_norm = 0.146
I0405 23:42:02.546579 140505258112768 logging_writer.py:48] [85] global_step=85, grad_norm=0.137793, loss=0.358518
I0405 23:42:02.551871 140570875250496 submission.py:139] 85) loss = 0.359, grad_norm = 0.138
I0405 23:42:02.833304 140505266505472 logging_writer.py:48] [86] global_step=86, grad_norm=0.084332, loss=0.285543
I0405 23:42:02.838229 140570875250496 submission.py:139] 86) loss = 0.286, grad_norm = 0.084
I0405 23:42:03.063422 140505258112768 logging_writer.py:48] [87] global_step=87, grad_norm=0.127558, loss=0.284919
I0405 23:42:03.066691 140570875250496 submission.py:139] 87) loss = 0.285, grad_norm = 0.128
I0405 23:42:03.350576 140505266505472 logging_writer.py:48] [88] global_step=88, grad_norm=0.124731, loss=0.291813
I0405 23:42:03.353699 140570875250496 submission.py:139] 88) loss = 0.292, grad_norm = 0.125
I0405 23:42:03.640884 140505258112768 logging_writer.py:48] [89] global_step=89, grad_norm=0.130904, loss=0.389589
I0405 23:42:03.644287 140570875250496 submission.py:139] 89) loss = 0.390, grad_norm = 0.131
I0405 23:42:03.890356 140505266505472 logging_writer.py:48] [90] global_step=90, grad_norm=0.118199, loss=0.266080
I0405 23:42:03.894185 140570875250496 submission.py:139] 90) loss = 0.266, grad_norm = 0.118
I0405 23:42:04.185179 140505258112768 logging_writer.py:48] [91] global_step=91, grad_norm=0.065564, loss=0.291285
I0405 23:42:04.191147 140570875250496 submission.py:139] 91) loss = 0.291, grad_norm = 0.066
I0405 23:42:04.410596 140505266505472 logging_writer.py:48] [92] global_step=92, grad_norm=0.084343, loss=0.294934
I0405 23:42:04.416676 140570875250496 submission.py:139] 92) loss = 0.295, grad_norm = 0.084
I0405 23:42:04.750754 140505258112768 logging_writer.py:48] [93] global_step=93, grad_norm=0.094263, loss=0.324187
I0405 23:42:04.757148 140570875250496 submission.py:139] 93) loss = 0.324, grad_norm = 0.094
I0405 23:42:04.975862 140505266505472 logging_writer.py:48] [94] global_step=94, grad_norm=0.242564, loss=0.314984
I0405 23:42:04.979259 140570875250496 submission.py:139] 94) loss = 0.315, grad_norm = 0.243
I0405 23:42:05.261325 140505258112768 logging_writer.py:48] [95] global_step=95, grad_norm=0.158956, loss=0.288424
I0405 23:42:05.265016 140570875250496 submission.py:139] 95) loss = 0.288, grad_norm = 0.159
I0405 23:42:05.568740 140505266505472 logging_writer.py:48] [96] global_step=96, grad_norm=0.077220, loss=0.266356
I0405 23:42:05.571903 140570875250496 submission.py:139] 96) loss = 0.266, grad_norm = 0.077
I0405 23:42:05.810020 140505258112768 logging_writer.py:48] [97] global_step=97, grad_norm=0.098581, loss=0.238942
I0405 23:42:05.814819 140570875250496 submission.py:139] 97) loss = 0.239, grad_norm = 0.099
I0405 23:42:06.102066 140505266505472 logging_writer.py:48] [98] global_step=98, grad_norm=0.244050, loss=0.344506
I0405 23:42:06.106278 140570875250496 submission.py:139] 98) loss = 0.345, grad_norm = 0.244
I0405 23:42:06.323858 140505258112768 logging_writer.py:48] [99] global_step=99, grad_norm=0.177794, loss=0.360663
I0405 23:42:06.331419 140570875250496 submission.py:139] 99) loss = 0.361, grad_norm = 0.178
I0405 23:42:06.611604 140505266505472 logging_writer.py:48] [100] global_step=100, grad_norm=0.096667, loss=0.374627
I0405 23:42:06.617262 140570875250496 submission.py:139] 100) loss = 0.375, grad_norm = 0.097
I0405 23:43:02.416148 140570875250496 submission_runner.py:373] Before eval at step 311: RAM USED (GB) 86.635159552
I0405 23:43:02.416428 140570875250496 spec.py:298] Evaluating on the training split.
I0405 23:43:04.529082 140570875250496 spec.py:310] Evaluating on the validation split.
I0405 23:43:09.702483 140570875250496 spec.py:326] Evaluating on the test split.
I0405 23:43:12.621696 140570875250496 submission_runner.py:382] Time since start: 351.32s, 	Step: 311, 	{'train/ssim': 0.7010790961129325, 'train/loss': 0.31345326559884207, 'validation/ssim': 0.679321082978686, 'validation/loss': 0.3322411807118739, 'validation/num_examples': 3554, 'test/ssim': 0.6974096822858489, 'test/loss': 0.3343082482306793, 'test/num_examples': 3581}
I0405 23:43:12.622094 140570875250496 submission_runner.py:396] After eval at step 311: RAM USED (GB) 87.831769088
I0405 23:43:12.636053 140505258112768 logging_writer.py:48] [311] global_step=311, preemption_count=0, score=121.928250, test/loss=0.334308, test/num_examples=3581, test/ssim=0.697410, total_duration=351.317476, train/loss=0.313453, train/ssim=0.701079, validation/loss=0.332241, validation/num_examples=3554, validation/ssim=0.679321
I0405 23:43:12.740444 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_311.
I0405 23:43:12.741239 140570875250496 submission_runner.py:416] After logging and checkpointing eval at step 311: RAM USED (GB) 87.84623616
I0405 23:44:18.027795 140505266505472 logging_writer.py:48] [500] global_step=500, grad_norm=0.173668, loss=0.250246
I0405 23:44:18.031649 140570875250496 submission.py:139] 500) loss = 0.250, grad_norm = 0.174
I0405 23:44:32.845381 140570875250496 submission_runner.py:373] Before eval at step 539: RAM USED (GB) 105.153929216
I0405 23:44:32.845574 140570875250496 spec.py:298] Evaluating on the training split.
I0405 23:44:34.811609 140570875250496 spec.py:310] Evaluating on the validation split.
I0405 23:44:39.701116 140570875250496 spec.py:326] Evaluating on the test split.
I0405 23:44:42.897208 140570875250496 submission_runner.py:382] Time since start: 441.74s, 	Step: 539, 	{'train/ssim': 0.7066529818943569, 'train/loss': 0.29866320746285574, 'validation/ssim': 0.6862345766873593, 'validation/loss': 0.31748911740072805, 'validation/num_examples': 3554, 'test/ssim': 0.703690798114179, 'test/loss': 0.31943622536913574, 'test/num_examples': 3581}
I0405 23:44:42.897585 140570875250496 submission_runner.py:396] After eval at step 539: RAM USED (GB) 106.394185728
I0405 23:44:42.906664 140505258112768 logging_writer.py:48] [539] global_step=539, preemption_count=0, score=197.547584, test/loss=0.319436, test/num_examples=3581, test/ssim=0.703691, total_duration=441.741409, train/loss=0.298663, train/ssim=0.706653, validation/loss=0.317489, validation/num_examples=3554, validation/ssim=0.686235
I0405 23:44:43.024632 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_539.
I0405 23:44:43.025280 140570875250496 submission_runner.py:416] After logging and checkpointing eval at step 539: RAM USED (GB) 106.415804416
I0405 23:46:03.114669 140570875250496 submission_runner.py:373] Before eval at step 766: RAM USED (GB) 123.67939584
I0405 23:46:03.114889 140570875250496 spec.py:298] Evaluating on the training split.
I0405 23:46:05.115237 140570875250496 spec.py:310] Evaluating on the validation split.
I0405 23:46:09.542018 140570875250496 spec.py:326] Evaluating on the test split.
I0405 23:46:11.824602 140570875250496 submission_runner.py:382] Time since start: 532.01s, 	Step: 766, 	{'train/ssim': 0.7015461240495954, 'train/loss': 0.3105768476213728, 'validation/ssim': 0.6814586529922271, 'validation/loss': 0.32932004569015544, 'validation/num_examples': 3554, 'test/ssim': 0.6986110232389695, 'test/loss': 0.3311835413903239, 'test/num_examples': 3581}
I0405 23:46:11.825005 140570875250496 submission_runner.py:396] After eval at step 766: RAM USED (GB) 124.836671488
I0405 23:46:11.834700 140505266505472 logging_writer.py:48] [766] global_step=766, preemption_count=0, score=273.459214, test/loss=0.331184, test/num_examples=3581, test/ssim=0.698611, total_duration=532.011103, train/loss=0.310577, train/ssim=0.701546, validation/loss=0.329320, validation/num_examples=3554, validation/ssim=0.681459
I0405 23:46:11.942197 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_766.
I0405 23:46:11.942816 140570875250496 submission_runner.py:416] After logging and checkpointing eval at step 766: RAM USED (GB) 124.85926912
I0405 23:47:32.160325 140570875250496 submission_runner.py:373] Before eval at step 999: RAM USED (GB) 142.04596224
I0405 23:47:32.160682 140570875250496 spec.py:298] Evaluating on the training split.
I0405 23:47:34.159753 140570875250496 spec.py:310] Evaluating on the validation split.
I0405 23:47:36.584799 140570875250496 spec.py:326] Evaluating on the test split.
I0405 23:47:38.576542 140570875250496 submission_runner.py:382] Time since start: 621.06s, 	Step: 999, 	{'train/ssim': 0.7068470546177456, 'train/loss': 0.29835687364850727, 'validation/ssim': 0.6856827529280388, 'validation/loss': 0.31790317413038127, 'validation/num_examples': 3554, 'test/ssim': 0.7040128646633272, 'test/loss': 0.3191086705987329, 'test/num_examples': 3581}
I0405 23:47:38.576892 140570875250496 submission_runner.py:396] After eval at step 999: RAM USED (GB) 142.677188608
I0405 23:47:38.585498 140505258112768 logging_writer.py:48] [999] global_step=999, preemption_count=0, score=349.116612, test/loss=0.319109, test/num_examples=3581, test/ssim=0.704013, total_duration=621.058864, train/loss=0.298357, train/ssim=0.706847, validation/loss=0.317903, validation/num_examples=3554, validation/ssim=0.685683
I0405 23:47:38.685197 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_999.
I0405 23:47:38.685732 140570875250496 submission_runner.py:416] After logging and checkpointing eval at step 999: RAM USED (GB) 142.686420992
I0405 23:47:38.808165 140505266505472 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.269890, loss=0.367046
I0405 23:47:38.811730 140570875250496 submission.py:139] 1000) loss = 0.367, grad_norm = 0.270
I0405 23:48:58.952203 140570875250496 submission_runner.py:373] Before eval at step 1315: RAM USED (GB) 143.152443392
I0405 23:48:58.952431 140570875250496 spec.py:298] Evaluating on the training split.
I0405 23:49:00.850813 140570875250496 spec.py:310] Evaluating on the validation split.
I0405 23:49:02.904552 140570875250496 spec.py:326] Evaluating on the test split.
I0405 23:49:04.891943 140570875250496 submission_runner.py:382] Time since start: 707.85s, 	Step: 1315, 	{'train/ssim': 0.699239867074149, 'train/loss': 0.32309818267822266, 'validation/ssim': 0.6814520583101786, 'validation/loss': 0.34075683868528417, 'validation/num_examples': 3554, 'test/ssim': 0.6974532471725775, 'test/loss': 0.34381933566479334, 'test/num_examples': 3581}
I0405 23:49:04.892303 140570875250496 submission_runner.py:396] After eval at step 1315: RAM USED (GB) 143.161229312
I0405 23:49:04.901290 140505258112768 logging_writer.py:48] [1315] global_step=1315, preemption_count=0, score=422.018943, test/loss=0.343819, test/num_examples=3581, test/ssim=0.697453, total_duration=707.846684, train/loss=0.323098, train/ssim=0.699240, validation/loss=0.340757, validation/num_examples=3554, validation/ssim=0.681452
I0405 23:49:05.003191 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_1315.
I0405 23:49:05.003720 140570875250496 submission_runner.py:416] After logging and checkpointing eval at step 1315: RAM USED (GB) 143.1605248
I0405 23:49:50.940749 140505266505472 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.507066, loss=0.360285
I0405 23:49:50.944118 140570875250496 submission.py:139] 1500) loss = 0.360, grad_norm = 0.507
I0405 23:50:25.244350 140570875250496 submission_runner.py:373] Before eval at step 1632: RAM USED (GB) 143.184531456
I0405 23:50:25.244543 140570875250496 spec.py:298] Evaluating on the training split.
I0405 23:50:27.160228 140570875250496 spec.py:310] Evaluating on the validation split.
I0405 23:50:29.200314 140570875250496 spec.py:326] Evaluating on the test split.
I0405 23:50:31.169268 140570875250496 submission_runner.py:382] Time since start: 794.14s, 	Step: 1632, 	{'train/ssim': 0.6636321885245187, 'train/loss': 0.3157750197819301, 'validation/ssim': 0.6548479431186691, 'validation/loss': 0.33334973989474886, 'validation/num_examples': 3554, 'test/ssim': 0.6698269049104649, 'test/loss': 0.3342441621688251, 'test/num_examples': 3581}
I0405 23:50:31.169613 140570875250496 submission_runner.py:396] After eval at step 1632: RAM USED (GB) 143.176822784
I0405 23:50:31.177637 140505258112768 logging_writer.py:48] [1632] global_step=1632, preemption_count=0, score=494.884974, test/loss=0.334244, test/num_examples=3581, test/ssim=0.669827, total_duration=794.139878, train/loss=0.315775, train/ssim=0.663632, validation/loss=0.333350, validation/num_examples=3554, validation/ssim=0.654848
I0405 23:50:31.274014 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_1632.
I0405 23:50:31.274521 140570875250496 submission_runner.py:416] After logging and checkpointing eval at step 1632: RAM USED (GB) 143.176634368
I0405 23:51:51.528490 140570875250496 submission_runner.py:373] Before eval at step 1948: RAM USED (GB) 143.189245952
I0405 23:51:51.528693 140570875250496 spec.py:298] Evaluating on the training split.
I0405 23:51:53.442674 140570875250496 spec.py:310] Evaluating on the validation split.
I0405 23:51:55.477369 140570875250496 spec.py:326] Evaluating on the test split.
I0405 23:51:57.453840 140570875250496 submission_runner.py:382] Time since start: 880.42s, 	Step: 1948, 	{'train/ssim': 0.664881706237793, 'train/loss': 0.4176586014883859, 'validation/ssim': 0.6517524268429938, 'validation/loss': 0.42638535035347497, 'validation/num_examples': 3554, 'test/ssim': 0.6676676818626082, 'test/loss': 0.4295366250479964, 'test/num_examples': 3581}
I0405 23:51:57.454217 140570875250496 submission_runner.py:396] After eval at step 1948: RAM USED (GB) 143.242985472
I0405 23:51:57.462510 140505266505472 logging_writer.py:48] [1948] global_step=1948, preemption_count=0, score=567.739505, test/loss=0.429537, test/num_examples=3581, test/ssim=0.667668, total_duration=880.423242, train/loss=0.417659, train/ssim=0.664882, validation/loss=0.426385, validation/num_examples=3554, validation/ssim=0.651752
I0405 23:51:57.559270 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_1948.
I0405 23:51:57.559784 140570875250496 submission_runner.py:416] After logging and checkpointing eval at step 1948: RAM USED (GB) 143.242276864
I0405 23:52:09.108986 140505258112768 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.586904, loss=0.299172
I0405 23:52:09.112750 140570875250496 submission.py:139] 2000) loss = 0.299, grad_norm = 0.587
I0405 23:53:17.827883 140570875250496 submission_runner.py:373] Before eval at step 2262: RAM USED (GB) 143.439286272
I0405 23:53:17.828108 140570875250496 spec.py:298] Evaluating on the training split.
I0405 23:53:19.830384 140570875250496 spec.py:310] Evaluating on the validation split.
I0405 23:53:22.122613 140570875250496 spec.py:326] Evaluating on the test split.
I0405 23:53:24.271221 140570875250496 submission_runner.py:382] Time since start: 966.72s, 	Step: 2262, 	{'train/ssim': 0.6940243584769112, 'train/loss': 0.3519287109375, 'validation/ssim': 0.678933576713738, 'validation/loss': 0.3641465615877357, 'validation/num_examples': 3554, 'test/ssim': 0.6941037277907708, 'test/loss': 0.367510657375733, 'test/num_examples': 3581}
I0405 23:53:24.271728 140570875250496 submission_runner.py:396] After eval at step 2262: RAM USED (GB) 143.438270464
I0405 23:53:24.282224 140505266505472 logging_writer.py:48] [2262] global_step=2262, preemption_count=0, score=640.740288, test/loss=0.367511, test/num_examples=3581, test/ssim=0.694104, total_duration=966.722920, train/loss=0.351929, train/ssim=0.694024, validation/loss=0.364147, validation/num_examples=3554, validation/ssim=0.678934
I0405 23:53:24.389356 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_2262.
I0405 23:53:24.389970 140570875250496 submission_runner.py:416] After logging and checkpointing eval at step 2262: RAM USED (GB) 143.437565952
I0405 23:54:24.185086 140505258112768 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.611164, loss=0.328815
I0405 23:54:24.188766 140570875250496 submission.py:139] 2500) loss = 0.329, grad_norm = 0.611
I0405 23:54:44.643614 140570875250496 submission_runner.py:373] Before eval at step 2579: RAM USED (GB) 143.445594112
I0405 23:54:44.643857 140570875250496 spec.py:298] Evaluating on the training split.
I0405 23:54:46.631566 140570875250496 spec.py:310] Evaluating on the validation split.
I0405 23:54:48.802149 140570875250496 spec.py:326] Evaluating on the test split.
I0405 23:54:50.921269 140570875250496 submission_runner.py:382] Time since start: 1053.54s, 	Step: 2579, 	{'train/ssim': 0.7213269642421177, 'train/loss': 0.290250335420881, 'validation/ssim': 0.700643750879291, 'validation/loss': 0.3088932584763647, 'validation/num_examples': 3554, 'test/ssim': 0.7180856866098855, 'test/loss': 0.3101789937342921, 'test/num_examples': 3581}
I0405 23:54:50.921747 140570875250496 submission_runner.py:396] After eval at step 2579: RAM USED (GB) 143.447904256
I0405 23:54:50.930652 140505266505472 logging_writer.py:48] [2579] global_step=2579, preemption_count=0, score=713.701432, test/loss=0.310179, test/num_examples=3581, test/ssim=0.718086, total_duration=1053.542121, train/loss=0.290250, train/ssim=0.721327, validation/loss=0.308893, validation/num_examples=3554, validation/ssim=0.700644
I0405 23:54:51.033141 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_2579.
I0405 23:54:51.033750 140570875250496 submission_runner.py:416] After logging and checkpointing eval at step 2579: RAM USED (GB) 143.447195648
I0405 23:55:23.902833 140570875250496 submission_runner.py:373] Before eval at step 2714: RAM USED (GB) 143.472775168
I0405 23:55:23.903069 140570875250496 spec.py:298] Evaluating on the training split.
I0405 23:55:25.927701 140570875250496 spec.py:310] Evaluating on the validation split.
I0405 23:55:28.057417 140570875250496 spec.py:326] Evaluating on the test split.
I0405 23:55:30.099915 140570875250496 submission_runner.py:382] Time since start: 1092.80s, 	Step: 2714, 	{'train/ssim': 0.7247837611607143, 'train/loss': 0.28512913840157644, 'validation/ssim': 0.7039656845719612, 'validation/loss': 0.3033465813168261, 'validation/num_examples': 3554, 'test/ssim': 0.7205972466446174, 'test/loss': 0.30540662742599833, 'test/num_examples': 3581}
I0405 23:55:30.100284 140570875250496 submission_runner.py:396] After eval at step 2714: RAM USED (GB) 143.39493888
I0405 23:55:30.109363 140505258112768 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=743.466431, test/loss=0.305407, test/num_examples=3581, test/ssim=0.720597, total_duration=1092.797035, train/loss=0.285129, train/ssim=0.724784, validation/loss=0.303347, validation/num_examples=3554, validation/ssim=0.703966
I0405 23:55:30.206368 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_2714.
I0405 23:55:30.206865 140570875250496 submission_runner.py:416] After logging and checkpointing eval at step 2714: RAM USED (GB) 143.395008512
I0405 23:55:30.214809 140505266505472 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=743.466431
I0405 23:55:30.391896 140570875250496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_2714.
I0405 23:55:31.835790 140570875250496 submission_runner.py:550] Tuning trial 1/1
I0405 23:55:31.836012 140570875250496 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 23:55:31.846088 140570875250496 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ssim': 0.3042278289794922, 'train/loss': 0.6253479548863002, 'validation/ssim': 0.2952190135125035, 'validation/loss': 0.6419411886254924, 'validation/num_examples': 3554, 'test/ssim': 0.3184590492791818, 'test/loss': 0.6413412640062134, 'test/num_examples': 3581, 'score': 45.810709953308105, 'total_duration': 45.81263542175293, 'global_step': 1, 'preemption_count': 0}), (311, {'train/ssim': 0.7010790961129325, 'train/loss': 0.31345326559884207, 'validation/ssim': 0.679321082978686, 'validation/loss': 0.3322411807118739, 'validation/num_examples': 3554, 'test/ssim': 0.6974096822858489, 'test/loss': 0.3343082482306793, 'test/num_examples': 3581, 'score': 121.92825031280518, 'total_duration': 351.31747603416443, 'global_step': 311, 'preemption_count': 0}), (539, {'train/ssim': 0.7066529818943569, 'train/loss': 0.29866320746285574, 'validation/ssim': 0.6862345766873593, 'validation/loss': 0.31748911740072805, 'validation/num_examples': 3554, 'test/ssim': 0.703690798114179, 'test/loss': 0.31943622536913574, 'test/num_examples': 3581, 'score': 197.54758429527283, 'total_duration': 441.7414085865021, 'global_step': 539, 'preemption_count': 0}), (766, {'train/ssim': 0.7015461240495954, 'train/loss': 0.3105768476213728, 'validation/ssim': 0.6814586529922271, 'validation/loss': 0.32932004569015544, 'validation/num_examples': 3554, 'test/ssim': 0.6986110232389695, 'test/loss': 0.3311835413903239, 'test/num_examples': 3581, 'score': 273.45921421051025, 'total_duration': 532.0111033916473, 'global_step': 766, 'preemption_count': 0}), (999, {'train/ssim': 0.7068470546177456, 'train/loss': 0.29835687364850727, 'validation/ssim': 0.6856827529280388, 'validation/loss': 0.31790317413038127, 'validation/num_examples': 3554, 'test/ssim': 0.7040128646633272, 'test/loss': 0.3191086705987329, 'test/num_examples': 3581, 'score': 349.1166124343872, 'total_duration': 621.0588643550873, 'global_step': 999, 'preemption_count': 0}), (1315, {'train/ssim': 0.699239867074149, 'train/loss': 0.32309818267822266, 'validation/ssim': 0.6814520583101786, 'validation/loss': 0.34075683868528417, 'validation/num_examples': 3554, 'test/ssim': 0.6974532471725775, 'test/loss': 0.34381933566479334, 'test/num_examples': 3581, 'score': 422.0189425945282, 'total_duration': 707.8466835021973, 'global_step': 1315, 'preemption_count': 0}), (1632, {'train/ssim': 0.6636321885245187, 'train/loss': 0.3157750197819301, 'validation/ssim': 0.6548479431186691, 'validation/loss': 0.33334973989474886, 'validation/num_examples': 3554, 'test/ssim': 0.6698269049104649, 'test/loss': 0.3342441621688251, 'test/num_examples': 3581, 'score': 494.884973526001, 'total_duration': 794.1398782730103, 'global_step': 1632, 'preemption_count': 0}), (1948, {'train/ssim': 0.664881706237793, 'train/loss': 0.4176586014883859, 'validation/ssim': 0.6517524268429938, 'validation/loss': 0.42638535035347497, 'validation/num_examples': 3554, 'test/ssim': 0.6676676818626082, 'test/loss': 0.4295366250479964, 'test/num_examples': 3581, 'score': 567.7395045757294, 'total_duration': 880.423241853714, 'global_step': 1948, 'preemption_count': 0}), (2262, {'train/ssim': 0.6940243584769112, 'train/loss': 0.3519287109375, 'validation/ssim': 0.678933576713738, 'validation/loss': 0.3641465615877357, 'validation/num_examples': 3554, 'test/ssim': 0.6941037277907708, 'test/loss': 0.367510657375733, 'test/num_examples': 3581, 'score': 640.7402880191803, 'total_duration': 966.7229204177856, 'global_step': 2262, 'preemption_count': 0}), (2579, {'train/ssim': 0.7213269642421177, 'train/loss': 0.290250335420881, 'validation/ssim': 0.700643750879291, 'validation/loss': 0.3088932584763647, 'validation/num_examples': 3554, 'test/ssim': 0.7180856866098855, 'test/loss': 0.3101789937342921, 'test/num_examples': 3581, 'score': 713.7014315128326, 'total_duration': 1053.5421206951141, 'global_step': 2579, 'preemption_count': 0}), (2714, {'train/ssim': 0.7247837611607143, 'train/loss': 0.28512913840157644, 'validation/ssim': 0.7039656845719612, 'validation/loss': 0.3033465813168261, 'validation/num_examples': 3554, 'test/ssim': 0.7205972466446174, 'test/loss': 0.30540662742599833, 'test/num_examples': 3581, 'score': 743.4664309024811, 'total_duration': 1092.797034740448, 'global_step': 2714, 'preemption_count': 0})], 'global_step': 2714}
I0405 23:55:31.846295 140570875250496 submission_runner.py:553] Timing: 743.4664309024811
I0405 23:55:31.846350 140570875250496 submission_runner.py:554] ====================
I0405 23:55:31.846447 140570875250496 submission_runner.py:613] Final fastmri score: 743.4664309024811
