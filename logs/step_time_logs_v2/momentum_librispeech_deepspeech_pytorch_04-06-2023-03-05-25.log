WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 03:05:40.500570 140695790892864 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 03:05:40.500608 140170394228544 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 03:05:40.501442 139763326580544 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 03:05:41.463057 140069201405760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 03:05:41.463352 140050168239936 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 03:05:41.464693 140624742090560 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 03:05:41.472385 140138047801152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 03:05:41.474880 139917146335040 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 03:05:41.475187 139917146335040 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:05:41.475435 140624742090560 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:05:41.483013 140138047801152 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:05:41.483821 140069201405760 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:05:41.484040 140695790892864 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:05:41.484215 139763326580544 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:05:41.484240 140050168239936 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:05:41.484298 140170394228544 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:05:41.853641 139917146335040 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_momentum/librispeech_deepspeech_pytorch.
W0406 03:05:41.903624 139763326580544 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:05:41.903974 140695790892864 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:05:41.905130 140069201405760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:05:41.905501 140050168239936 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:05:41.905914 140170394228544 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:05:41.907673 140138047801152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:05:41.929544 139917146335040 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:05:41.931201 140624742090560 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 03:05:41.933276 139917146335040 submission_runner.py:511] Using RNG seed 850581702
I0406 03:05:41.934243 139917146335040 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 03:05:41.934362 139917146335040 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_momentum/librispeech_deepspeech_pytorch/trial_1.
I0406 03:05:41.934556 139917146335040 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_momentum/librispeech_deepspeech_pytorch/trial_1/hparams.json.
I0406 03:05:41.935407 139917146335040 submission_runner.py:230] Starting train once: RAM USED (GB) 5.687877632
I0406 03:05:41.935493 139917146335040 submission_runner.py:231] Initializing dataset.
I0406 03:05:41.935570 139917146335040 input_pipeline.py:20] Loading split = train-clean-100
I0406 03:05:41.963088 139917146335040 input_pipeline.py:20] Loading split = train-clean-360
I0406 03:05:42.281209 139917146335040 input_pipeline.py:20] Loading split = train-other-500
I0406 03:05:42.708576 139917146335040 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.965791232
I0406 03:05:42.708758 139917146335040 submission_runner.py:240] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0406 03:05:49.838670 139917146335040 submission_runner.py:251] After Initializing model: RAM USED (GB) 21.784444928
I0406 03:05:49.838861 139917146335040 submission_runner.py:252] Initializing optimizer.
I0406 03:05:50.019708 139917146335040 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 21.258416128
I0406 03:05:50.019884 139917146335040 submission_runner.py:261] Initializing metrics bundle.
I0406 03:05:50.019933 139917146335040 submission_runner.py:276] Initializing checkpoint and logger.
I0406 03:05:50.021926 139917146335040 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 03:05:50.022064 139917146335040 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 03:05:50.846143 139917146335040 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_momentum/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I0406 03:05:50.847020 139917146335040 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_momentum/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I0406 03:05:50.849879 139917146335040 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 21.263310848
I0406 03:05:50.850981 139917146335040 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 21.263310848
I0406 03:05:50.851084 139917146335040 submission_runner.py:313] Starting training loop.
I0406 03:05:53.210609 139917146335040 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 25.805336576
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0406 03:05:59.683058 139888846427904 logging_writer.py:48] [0] global_step=0, grad_norm=23.164816, loss=33.668446
I0406 03:05:59.692873 139917146335040 submission.py:139] 0) loss = 33.668, grad_norm = 23.165
I0406 03:05:59.693620 139917146335040 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.616221184
I0406 03:05:59.694169 139917146335040 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.616221184
I0406 03:05:59.694293 139917146335040 spec.py:298] Evaluating on the training split.
I0406 03:05:59.695005 139917146335040 input_pipeline.py:20] Loading split = train-clean-100
I0406 03:05:59.723385 139917146335040 input_pipeline.py:20] Loading split = train-clean-360
I0406 03:06:00.133872 139917146335040 input_pipeline.py:20] Loading split = train-other-500
I0406 03:06:16.359410 139917146335040 spec.py:310] Evaluating on the validation split.
I0406 03:06:16.360565 139917146335040 input_pipeline.py:20] Loading split = dev-clean
I0406 03:06:16.364787 139917146335040 input_pipeline.py:20] Loading split = dev-other
I0406 03:06:27.722777 139917146335040 spec.py:326] Evaluating on the test split.
I0406 03:06:27.724019 139917146335040 input_pipeline.py:20] Loading split = test-clean
I0406 03:06:34.518845 139917146335040 submission_runner.py:382] Time since start: 8.84s, 	Step: 1, 	{'train/ctc_loss': 32.1424357638654, 'train/wer': 3.790084273922745, 'validation/ctc_loss': 31.023049778983324, 'validation/wer': 3.509032974460484, 'validation/num_examples': 5348, 'test/ctc_loss': 31.03515065488096, 'test/wer': 3.7368634858732963, 'test/num_examples': 2472}
I0406 03:06:34.519616 139917146335040 submission_runner.py:396] After eval at step 1: RAM USED (GB) 45.89615104
I0406 03:06:34.532232 139886498739968 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=8.841523, test/ctc_loss=31.035151, test/num_examples=2472, test/wer=3.736863, total_duration=8.843745, train/ctc_loss=32.142436, train/wer=3.790084, validation/ctc_loss=31.023050, validation/num_examples=5348, validation/wer=3.509033
I0406 03:06:34.719439 139917146335040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/librispeech_deepspeech_pytorch/trial_1/checkpoint_1.
I0406 03:06:34.719908 139917146335040 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 45.92361472
I0406 03:06:34.722947 139917146335040 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 45.928050688
I0406 03:06:34.762136 139917146335040 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:06:34.762191 140170394228544 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:06:34.762179 140138047801152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:06:34.763194 140050168239936 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:06:34.763996 140695790892864 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:06:34.764060 140624742090560 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:06:34.764371 139763326580544 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:06:34.764786 140069201405760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:06:35.852601 139886490347264 logging_writer.py:48] [1] global_step=1, grad_norm=22.094660, loss=33.055561
I0406 03:06:35.855763 139917146335040 submission.py:139] 1) loss = 33.056, grad_norm = 22.095
I0406 03:06:35.856516 139917146335040 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 46.124548096
I0406 03:06:36.817282 139886498739968 logging_writer.py:48] [2] global_step=2, grad_norm=28.699476, loss=33.516468
I0406 03:06:36.820518 139917146335040 submission.py:139] 2) loss = 33.516, grad_norm = 28.699
I0406 03:06:37.646855 139886490347264 logging_writer.py:48] [3] global_step=3, grad_norm=46.930637, loss=33.300545
I0406 03:06:37.650032 139917146335040 submission.py:139] 3) loss = 33.301, grad_norm = 46.931
I0406 03:06:38.467100 139886498739968 logging_writer.py:48] [4] global_step=4, grad_norm=58.500591, loss=32.063461
I0406 03:06:38.470311 139917146335040 submission.py:139] 4) loss = 32.063, grad_norm = 58.501
I0406 03:06:39.282343 139886490347264 logging_writer.py:48] [5] global_step=5, grad_norm=54.910633, loss=30.795380
I0406 03:06:39.285300 139917146335040 submission.py:139] 5) loss = 30.795, grad_norm = 54.911
I0406 03:06:40.100807 139886498739968 logging_writer.py:48] [6] global_step=6, grad_norm=45.508442, loss=29.478952
I0406 03:06:40.103685 139917146335040 submission.py:139] 6) loss = 29.479, grad_norm = 45.508
I0406 03:06:40.915363 139886490347264 logging_writer.py:48] [7] global_step=7, grad_norm=31.464632, loss=27.233627
I0406 03:06:40.918552 139917146335040 submission.py:139] 7) loss = 27.234, grad_norm = 31.465
I0406 03:06:41.729423 139886498739968 logging_writer.py:48] [8] global_step=8, grad_norm=26.569313, loss=26.242121
I0406 03:06:41.732411 139917146335040 submission.py:139] 8) loss = 26.242, grad_norm = 26.569
I0406 03:06:42.538222 139886490347264 logging_writer.py:48] [9] global_step=9, grad_norm=23.870008, loss=24.904110
I0406 03:06:42.541950 139917146335040 submission.py:139] 9) loss = 24.904, grad_norm = 23.870
I0406 03:06:43.346411 139886498739968 logging_writer.py:48] [10] global_step=10, grad_norm=24.087151, loss=23.771090
I0406 03:06:43.349817 139917146335040 submission.py:139] 10) loss = 23.771, grad_norm = 24.087
I0406 03:06:44.157994 139886490347264 logging_writer.py:48] [11] global_step=11, grad_norm=26.063744, loss=22.527880
I0406 03:06:44.161282 139917146335040 submission.py:139] 11) loss = 22.528, grad_norm = 26.064
I0406 03:06:44.970907 139886498739968 logging_writer.py:48] [12] global_step=12, grad_norm=27.671453, loss=21.319872
I0406 03:06:44.973968 139917146335040 submission.py:139] 12) loss = 21.320, grad_norm = 27.671
I0406 03:06:45.779301 139886490347264 logging_writer.py:48] [13] global_step=13, grad_norm=27.747913, loss=19.311811
I0406 03:06:45.782469 139917146335040 submission.py:139] 13) loss = 19.312, grad_norm = 27.748
I0406 03:06:46.594473 139886498739968 logging_writer.py:48] [14] global_step=14, grad_norm=26.412046, loss=17.209587
I0406 03:06:46.597739 139917146335040 submission.py:139] 14) loss = 17.210, grad_norm = 26.412
I0406 03:06:47.407850 139886490347264 logging_writer.py:48] [15] global_step=15, grad_norm=20.116442, loss=14.756395
I0406 03:06:47.410972 139917146335040 submission.py:139] 15) loss = 14.756, grad_norm = 20.116
I0406 03:06:48.214391 139886498739968 logging_writer.py:48] [16] global_step=16, grad_norm=13.203077, loss=13.915194
I0406 03:06:48.217700 139917146335040 submission.py:139] 16) loss = 13.915, grad_norm = 13.203
I0406 03:06:49.030613 139886490347264 logging_writer.py:48] [17] global_step=17, grad_norm=12.488457, loss=13.074231
I0406 03:06:49.033816 139917146335040 submission.py:139] 17) loss = 13.074, grad_norm = 12.488
I0406 03:06:49.841161 139886498739968 logging_writer.py:48] [18] global_step=18, grad_norm=13.246453, loss=12.499186
I0406 03:06:49.844206 139917146335040 submission.py:139] 18) loss = 12.499, grad_norm = 13.246
I0406 03:06:50.671484 139886490347264 logging_writer.py:48] [19] global_step=19, grad_norm=10.700697, loss=12.123064
I0406 03:06:50.674564 139917146335040 submission.py:139] 19) loss = 12.123, grad_norm = 10.701
I0406 03:06:51.505798 139886498739968 logging_writer.py:48] [20] global_step=20, grad_norm=9.630790, loss=11.328579
I0406 03:06:51.508860 139917146335040 submission.py:139] 20) loss = 11.329, grad_norm = 9.631
I0406 03:06:52.343798 139886490347264 logging_writer.py:48] [21] global_step=21, grad_norm=10.110627, loss=10.862648
I0406 03:06:52.346900 139917146335040 submission.py:139] 21) loss = 10.863, grad_norm = 10.111
I0406 03:06:53.161092 139886498739968 logging_writer.py:48] [22] global_step=22, grad_norm=9.587848, loss=10.301113
I0406 03:06:53.163983 139917146335040 submission.py:139] 22) loss = 10.301, grad_norm = 9.588
I0406 03:06:53.988633 139886490347264 logging_writer.py:48] [23] global_step=23, grad_norm=7.974976, loss=9.622596
I0406 03:06:53.991792 139917146335040 submission.py:139] 23) loss = 9.623, grad_norm = 7.975
I0406 03:06:54.803330 139886498739968 logging_writer.py:48] [24] global_step=24, grad_norm=7.228997, loss=9.432034
I0406 03:06:54.806652 139917146335040 submission.py:139] 24) loss = 9.432, grad_norm = 7.229
I0406 03:06:55.624525 139886490347264 logging_writer.py:48] [25] global_step=25, grad_norm=11.377875, loss=8.920906
I0406 03:06:55.627618 139917146335040 submission.py:139] 25) loss = 8.921, grad_norm = 11.378
I0406 03:06:56.438952 139886498739968 logging_writer.py:48] [26] global_step=26, grad_norm=7.482377, loss=8.491325
I0406 03:06:56.442073 139917146335040 submission.py:139] 26) loss = 8.491, grad_norm = 7.482
I0406 03:06:57.263831 139886490347264 logging_writer.py:48] [27] global_step=27, grad_norm=13.716630, loss=8.628677
I0406 03:06:57.266782 139917146335040 submission.py:139] 27) loss = 8.629, grad_norm = 13.717
I0406 03:06:58.094040 139886498739968 logging_writer.py:48] [28] global_step=28, grad_norm=13.522591, loss=8.105482
I0406 03:06:58.097168 139917146335040 submission.py:139] 28) loss = 8.105, grad_norm = 13.523
I0406 03:06:58.906904 139886490347264 logging_writer.py:48] [29] global_step=29, grad_norm=5.037446, loss=8.067242
I0406 03:06:58.910202 139917146335040 submission.py:139] 29) loss = 8.067, grad_norm = 5.037
I0406 03:06:59.726697 139886498739968 logging_writer.py:48] [30] global_step=30, grad_norm=6.339623, loss=7.841114
I0406 03:06:59.729931 139917146335040 submission.py:139] 30) loss = 7.841, grad_norm = 6.340
I0406 03:07:00.545104 139886490347264 logging_writer.py:48] [31] global_step=31, grad_norm=5.749253, loss=7.827271
I0406 03:07:00.548187 139917146335040 submission.py:139] 31) loss = 7.827, grad_norm = 5.749
I0406 03:07:01.358136 139886498739968 logging_writer.py:48] [32] global_step=32, grad_norm=5.342321, loss=7.713004
I0406 03:07:01.361338 139917146335040 submission.py:139] 32) loss = 7.713, grad_norm = 5.342
I0406 03:07:02.186911 139886490347264 logging_writer.py:48] [33] global_step=33, grad_norm=10.640661, loss=7.801693
I0406 03:07:02.190104 139917146335040 submission.py:139] 33) loss = 7.802, grad_norm = 10.641
I0406 03:07:03.012238 139886498739968 logging_writer.py:48] [34] global_step=34, grad_norm=7.178616, loss=7.639046
I0406 03:07:03.015602 139917146335040 submission.py:139] 34) loss = 7.639, grad_norm = 7.179
I0406 03:07:03.823142 139886490347264 logging_writer.py:48] [35] global_step=35, grad_norm=19.815153, loss=7.648252
I0406 03:07:03.826709 139917146335040 submission.py:139] 35) loss = 7.648, grad_norm = 19.815
I0406 03:07:04.635135 139886498739968 logging_writer.py:48] [36] global_step=36, grad_norm=35.335636, loss=8.145183
I0406 03:07:04.638421 139917146335040 submission.py:139] 36) loss = 8.145, grad_norm = 35.336
I0406 03:07:05.454916 139886490347264 logging_writer.py:48] [37] global_step=37, grad_norm=15.648076, loss=7.496615
I0406 03:07:05.457932 139917146335040 submission.py:139] 37) loss = 7.497, grad_norm = 15.648
I0406 03:07:06.284440 139886498739968 logging_writer.py:48] [38] global_step=38, grad_norm=19.115030, loss=7.635995
I0406 03:07:06.287407 139917146335040 submission.py:139] 38) loss = 7.636, grad_norm = 19.115
I0406 03:07:07.113236 139886490347264 logging_writer.py:48] [39] global_step=39, grad_norm=12.168456, loss=7.611151
I0406 03:07:07.116311 139917146335040 submission.py:139] 39) loss = 7.611, grad_norm = 12.168
I0406 03:07:07.939133 139886498739968 logging_writer.py:48] [40] global_step=40, grad_norm=9.424173, loss=7.568660
I0406 03:07:07.942507 139917146335040 submission.py:139] 40) loss = 7.569, grad_norm = 9.424
I0406 03:07:08.763286 139886490347264 logging_writer.py:48] [41] global_step=41, grad_norm=7.038643, loss=7.402865
I0406 03:07:08.766452 139917146335040 submission.py:139] 41) loss = 7.403, grad_norm = 7.039
I0406 03:07:09.595407 139886498739968 logging_writer.py:48] [42] global_step=42, grad_norm=12.235914, loss=7.187863
I0406 03:07:09.598481 139917146335040 submission.py:139] 42) loss = 7.188, grad_norm = 12.236
I0406 03:07:10.412495 139886490347264 logging_writer.py:48] [43] global_step=43, grad_norm=10.849444, loss=7.157052
I0406 03:07:10.415488 139917146335040 submission.py:139] 43) loss = 7.157, grad_norm = 10.849
I0406 03:07:11.229639 139886498739968 logging_writer.py:48] [44] global_step=44, grad_norm=13.113268, loss=7.119396
I0406 03:07:11.232833 139917146335040 submission.py:139] 44) loss = 7.119, grad_norm = 13.113
I0406 03:07:12.048992 139886490347264 logging_writer.py:48] [45] global_step=45, grad_norm=14.441799, loss=7.102950
I0406 03:07:12.052247 139917146335040 submission.py:139] 45) loss = 7.103, grad_norm = 14.442
I0406 03:07:12.873852 139886498739968 logging_writer.py:48] [46] global_step=46, grad_norm=20.542419, loss=7.164174
I0406 03:07:12.877136 139917146335040 submission.py:139] 46) loss = 7.164, grad_norm = 20.542
I0406 03:07:13.696000 139886490347264 logging_writer.py:48] [47] global_step=47, grad_norm=5.799298, loss=6.814891
I0406 03:07:13.699171 139917146335040 submission.py:139] 47) loss = 6.815, grad_norm = 5.799
I0406 03:07:14.513737 139886498739968 logging_writer.py:48] [48] global_step=48, grad_norm=13.853667, loss=7.111428
I0406 03:07:14.516880 139917146335040 submission.py:139] 48) loss = 7.111, grad_norm = 13.854
I0406 03:07:15.330193 139886490347264 logging_writer.py:48] [49] global_step=49, grad_norm=8.003818, loss=6.834332
I0406 03:07:15.333328 139917146335040 submission.py:139] 49) loss = 6.834, grad_norm = 8.004
I0406 03:07:16.170474 139886498739968 logging_writer.py:48] [50] global_step=50, grad_norm=8.224997, loss=6.807325
I0406 03:07:16.173404 139917146335040 submission.py:139] 50) loss = 6.807, grad_norm = 8.225
I0406 03:07:16.984540 139886490347264 logging_writer.py:48] [51] global_step=51, grad_norm=7.993670, loss=6.867471
I0406 03:07:16.987787 139917146335040 submission.py:139] 51) loss = 6.867, grad_norm = 7.994
I0406 03:07:17.796748 139886498739968 logging_writer.py:48] [52] global_step=52, grad_norm=4.573378, loss=6.682860
I0406 03:07:17.799925 139917146335040 submission.py:139] 52) loss = 6.683, grad_norm = 4.573
I0406 03:07:18.611122 139886490347264 logging_writer.py:48] [53] global_step=53, grad_norm=3.971131, loss=6.609099
I0406 03:07:18.614277 139917146335040 submission.py:139] 53) loss = 6.609, grad_norm = 3.971
I0406 03:07:19.433866 139886498739968 logging_writer.py:48] [54] global_step=54, grad_norm=7.636901, loss=6.614831
I0406 03:07:19.437036 139917146335040 submission.py:139] 54) loss = 6.615, grad_norm = 7.637
I0406 03:07:20.252642 139886490347264 logging_writer.py:48] [55] global_step=55, grad_norm=4.618213, loss=6.493681
I0406 03:07:20.255695 139917146335040 submission.py:139] 55) loss = 6.494, grad_norm = 4.618
I0406 03:07:21.065008 139886498739968 logging_writer.py:48] [56] global_step=56, grad_norm=6.956146, loss=6.440507
I0406 03:07:21.068214 139917146335040 submission.py:139] 56) loss = 6.441, grad_norm = 6.956
I0406 03:07:21.888330 139886490347264 logging_writer.py:48] [57] global_step=57, grad_norm=11.530293, loss=6.518918
I0406 03:07:21.891591 139917146335040 submission.py:139] 57) loss = 6.519, grad_norm = 11.530
I0406 03:07:22.697490 139886498739968 logging_writer.py:48] [58] global_step=58, grad_norm=14.710933, loss=6.651419
I0406 03:07:22.700587 139917146335040 submission.py:139] 58) loss = 6.651, grad_norm = 14.711
I0406 03:07:23.521037 139886490347264 logging_writer.py:48] [59] global_step=59, grad_norm=9.557812, loss=6.500371
I0406 03:07:23.523987 139917146335040 submission.py:139] 59) loss = 6.500, grad_norm = 9.558
I0406 03:07:24.328084 139886498739968 logging_writer.py:48] [60] global_step=60, grad_norm=19.685596, loss=6.720845
I0406 03:07:24.331521 139917146335040 submission.py:139] 60) loss = 6.721, grad_norm = 19.686
I0406 03:07:25.185206 139886490347264 logging_writer.py:48] [61] global_step=61, grad_norm=1.423766, loss=6.321780
I0406 03:07:25.188277 139917146335040 submission.py:139] 61) loss = 6.322, grad_norm = 1.424
I0406 03:07:26.008304 139886498739968 logging_writer.py:48] [62] global_step=62, grad_norm=10.904986, loss=6.486836
I0406 03:07:26.011845 139917146335040 submission.py:139] 62) loss = 6.487, grad_norm = 10.905
I0406 03:07:26.831218 139886490347264 logging_writer.py:48] [63] global_step=63, grad_norm=6.394435, loss=6.371137
I0406 03:07:26.834257 139917146335040 submission.py:139] 63) loss = 6.371, grad_norm = 6.394
I0406 03:07:27.650286 139886498739968 logging_writer.py:48] [64] global_step=64, grad_norm=9.908154, loss=6.374714
I0406 03:07:27.654109 139917146335040 submission.py:139] 64) loss = 6.375, grad_norm = 9.908
I0406 03:07:28.476042 139886490347264 logging_writer.py:48] [65] global_step=65, grad_norm=5.003171, loss=6.330397
I0406 03:07:28.479017 139917146335040 submission.py:139] 65) loss = 6.330, grad_norm = 5.003
I0406 03:07:29.286631 139886498739968 logging_writer.py:48] [66] global_step=66, grad_norm=4.306796, loss=6.309457
I0406 03:07:29.290031 139917146335040 submission.py:139] 66) loss = 6.309, grad_norm = 4.307
I0406 03:07:30.099874 139886490347264 logging_writer.py:48] [67] global_step=67, grad_norm=6.793391, loss=6.331347
I0406 03:07:30.102832 139917146335040 submission.py:139] 67) loss = 6.331, grad_norm = 6.793
I0406 03:07:30.911954 139886498739968 logging_writer.py:48] [68] global_step=68, grad_norm=1.727838, loss=6.191668
I0406 03:07:30.915095 139917146335040 submission.py:139] 68) loss = 6.192, grad_norm = 1.728
I0406 03:07:31.738108 139886490347264 logging_writer.py:48] [69] global_step=69, grad_norm=7.994310, loss=6.254065
I0406 03:07:31.741194 139917146335040 submission.py:139] 69) loss = 6.254, grad_norm = 7.994
I0406 03:07:32.555281 139886498739968 logging_writer.py:48] [70] global_step=70, grad_norm=1.056901, loss=6.146944
I0406 03:07:32.558327 139917146335040 submission.py:139] 70) loss = 6.147, grad_norm = 1.057
I0406 03:07:33.383791 139886490347264 logging_writer.py:48] [71] global_step=71, grad_norm=8.865441, loss=6.261774
I0406 03:07:33.386758 139917146335040 submission.py:139] 71) loss = 6.262, grad_norm = 8.865
I0406 03:07:34.193634 139886498739968 logging_writer.py:48] [72] global_step=72, grad_norm=2.633253, loss=6.137504
I0406 03:07:34.196599 139917146335040 submission.py:139] 72) loss = 6.138, grad_norm = 2.633
I0406 03:07:35.015635 139886490347264 logging_writer.py:48] [73] global_step=73, grad_norm=6.404767, loss=6.188012
I0406 03:07:35.018802 139917146335040 submission.py:139] 73) loss = 6.188, grad_norm = 6.405
I0406 03:07:35.841349 139886498739968 logging_writer.py:48] [74] global_step=74, grad_norm=5.921789, loss=6.190738
I0406 03:07:35.844491 139917146335040 submission.py:139] 74) loss = 6.191, grad_norm = 5.922
I0406 03:07:36.655813 139886490347264 logging_writer.py:48] [75] global_step=75, grad_norm=1.733836, loss=6.136611
I0406 03:07:36.658897 139917146335040 submission.py:139] 75) loss = 6.137, grad_norm = 1.734
I0406 03:07:37.483772 139886498739968 logging_writer.py:48] [76] global_step=76, grad_norm=5.633717, loss=6.152997
I0406 03:07:37.486866 139917146335040 submission.py:139] 76) loss = 6.153, grad_norm = 5.634
I0406 03:07:38.314161 139886490347264 logging_writer.py:48] [77] global_step=77, grad_norm=3.277446, loss=6.131315
I0406 03:07:38.317191 139917146335040 submission.py:139] 77) loss = 6.131, grad_norm = 3.277
I0406 03:07:39.136874 139886498739968 logging_writer.py:48] [78] global_step=78, grad_norm=3.328161, loss=6.112308
I0406 03:07:39.139806 139917146335040 submission.py:139] 78) loss = 6.112, grad_norm = 3.328
I0406 03:07:39.955993 139886490347264 logging_writer.py:48] [79] global_step=79, grad_norm=5.851489, loss=6.122876
I0406 03:07:39.959049 139917146335040 submission.py:139] 79) loss = 6.123, grad_norm = 5.851
I0406 03:07:40.765308 139886498739968 logging_writer.py:48] [80] global_step=80, grad_norm=3.475382, loss=6.100236
I0406 03:07:40.768439 139917146335040 submission.py:139] 80) loss = 6.100, grad_norm = 3.475
I0406 03:07:41.579930 139886490347264 logging_writer.py:48] [81] global_step=81, grad_norm=1.282583, loss=6.083347
I0406 03:07:41.583052 139917146335040 submission.py:139] 81) loss = 6.083, grad_norm = 1.283
I0406 03:07:42.396239 139886498739968 logging_writer.py:48] [82] global_step=82, grad_norm=3.503860, loss=6.091765
I0406 03:07:42.399220 139917146335040 submission.py:139] 82) loss = 6.092, grad_norm = 3.504
I0406 03:07:43.216064 139886490347264 logging_writer.py:48] [83] global_step=83, grad_norm=2.345559, loss=6.075814
I0406 03:07:43.219014 139917146335040 submission.py:139] 83) loss = 6.076, grad_norm = 2.346
I0406 03:07:44.036725 139886498739968 logging_writer.py:48] [84] global_step=84, grad_norm=3.455220, loss=6.087306
I0406 03:07:44.039912 139917146335040 submission.py:139] 84) loss = 6.087, grad_norm = 3.455
I0406 03:07:44.850301 139886490347264 logging_writer.py:48] [85] global_step=85, grad_norm=7.082092, loss=6.078837
I0406 03:07:44.853768 139917146335040 submission.py:139] 85) loss = 6.079, grad_norm = 7.082
I0406 03:07:45.677185 139886498739968 logging_writer.py:48] [86] global_step=86, grad_norm=3.915904, loss=6.043657
I0406 03:07:45.680149 139917146335040 submission.py:139] 86) loss = 6.044, grad_norm = 3.916
I0406 03:07:46.504704 139886490347264 logging_writer.py:48] [87] global_step=87, grad_norm=3.068449, loss=6.025035
I0406 03:07:46.507700 139917146335040 submission.py:139] 87) loss = 6.025, grad_norm = 3.068
I0406 03:07:47.325056 139886498739968 logging_writer.py:48] [88] global_step=88, grad_norm=5.788813, loss=6.067216
I0406 03:07:47.328229 139917146335040 submission.py:139] 88) loss = 6.067, grad_norm = 5.789
I0406 03:07:48.145074 139886490347264 logging_writer.py:48] [89] global_step=89, grad_norm=2.088855, loss=6.042132
I0406 03:07:48.148409 139917146335040 submission.py:139] 89) loss = 6.042, grad_norm = 2.089
I0406 03:07:48.972908 139886498739968 logging_writer.py:48] [90] global_step=90, grad_norm=5.387451, loss=6.039416
I0406 03:07:48.976335 139917146335040 submission.py:139] 90) loss = 6.039, grad_norm = 5.387
I0406 03:07:49.790021 139886490347264 logging_writer.py:48] [91] global_step=91, grad_norm=9.088064, loss=6.098097
I0406 03:07:49.793221 139917146335040 submission.py:139] 91) loss = 6.098, grad_norm = 9.088
I0406 03:07:50.618348 139886498739968 logging_writer.py:48] [92] global_step=92, grad_norm=6.337605, loss=6.037066
I0406 03:07:50.621608 139917146335040 submission.py:139] 92) loss = 6.037, grad_norm = 6.338
I0406 03:07:51.444846 139886490347264 logging_writer.py:48] [93] global_step=93, grad_norm=0.737043, loss=5.999052
I0406 03:07:51.447766 139917146335040 submission.py:139] 93) loss = 5.999, grad_norm = 0.737
I0406 03:07:52.258663 139886498739968 logging_writer.py:48] [94] global_step=94, grad_norm=4.021191, loss=6.013052
I0406 03:07:52.261957 139917146335040 submission.py:139] 94) loss = 6.013, grad_norm = 4.021
I0406 03:07:53.090665 139886490347264 logging_writer.py:48] [95] global_step=95, grad_norm=7.222182, loss=6.038068
I0406 03:07:53.094084 139917146335040 submission.py:139] 95) loss = 6.038, grad_norm = 7.222
I0406 03:07:53.910996 139886498739968 logging_writer.py:48] [96] global_step=96, grad_norm=7.516057, loss=6.064083
I0406 03:07:53.914129 139917146335040 submission.py:139] 96) loss = 6.064, grad_norm = 7.516
I0406 03:07:54.732710 139886490347264 logging_writer.py:48] [97] global_step=97, grad_norm=3.590977, loss=5.988672
I0406 03:07:54.735702 139917146335040 submission.py:139] 97) loss = 5.989, grad_norm = 3.591
I0406 03:07:55.565396 139886498739968 logging_writer.py:48] [98] global_step=98, grad_norm=2.497092, loss=5.991345
I0406 03:07:55.568742 139917146335040 submission.py:139] 98) loss = 5.991, grad_norm = 2.497
I0406 03:07:56.390847 139886490347264 logging_writer.py:48] [99] global_step=99, grad_norm=6.540637, loss=6.031313
I0406 03:07:56.394034 139917146335040 submission.py:139] 99) loss = 6.031, grad_norm = 6.541
I0406 03:07:57.217050 139886498739968 logging_writer.py:48] [100] global_step=100, grad_norm=7.155294, loss=6.023034
I0406 03:07:57.220011 139917146335040 submission.py:139] 100) loss = 6.023, grad_norm = 7.155
I0406 03:13:19.273000 139886490347264 logging_writer.py:48] [500] global_step=500, grad_norm=3.274377, loss=5.978545
I0406 03:13:19.277772 139917146335040 submission.py:139] 500) loss = 5.979, grad_norm = 3.274
I0406 03:20:01.639669 139886498739968 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.983680, loss=4.771228
I0406 03:20:01.644176 139917146335040 submission.py:139] 1000) loss = 4.771, grad_norm = 0.984
I0406 03:26:45.687214 139886498739968 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.968607, loss=3.545536
I0406 03:26:45.696726 139917146335040 submission.py:139] 1500) loss = 3.546, grad_norm = 0.969
I0406 03:33:28.135744 139886490347264 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.043052, loss=3.091521
I0406 03:33:28.140225 139917146335040 submission.py:139] 2000) loss = 3.092, grad_norm = 1.043
I0406 03:40:12.351072 139886498739968 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.094471, loss=2.954552
I0406 03:40:12.357506 139917146335040 submission.py:139] 2500) loss = 2.955, grad_norm = 1.094
I0406 03:46:35.206083 139917146335040 submission_runner.py:373] Before eval at step 2978: RAM USED (GB) 43.96505088
I0406 03:46:35.206308 139917146335040 spec.py:298] Evaluating on the training split.
I0406 03:46:45.678941 139917146335040 spec.py:310] Evaluating on the validation split.
I0406 03:46:54.864561 139917146335040 spec.py:326] Evaluating on the test split.
I0406 03:46:59.941833 139917146335040 submission_runner.py:382] Time since start: 2444.03s, 	Step: 2978, 	{'train/ctc_loss': 2.6592696310904245, 'train/wer': 0.6065772655386815, 'validation/ctc_loss': 2.855153432351316, 'validation/wer': 0.6185680490513205, 'validation/num_examples': 5348, 'test/ctc_loss': 2.3876145235176462, 'test/wer': 0.5492251132370565, 'test/num_examples': 2472}
I0406 03:46:59.942570 139917146335040 submission_runner.py:396] After eval at step 2978: RAM USED (GB) 42.088931328
I0406 03:46:59.958569 139886498739968 logging_writer.py:48] [2978] global_step=2978, preemption_count=0, score=1469.131759, test/ctc_loss=2.387615, test/num_examples=2472, test/wer=0.549225, total_duration=2444.034610, train/ctc_loss=2.659270, train/wer=0.606577, validation/ctc_loss=2.855153, validation/num_examples=5348, validation/wer=0.618568
I0406 03:47:00.152597 139917146335040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/librispeech_deepspeech_pytorch/trial_1/checkpoint_2978.
I0406 03:47:00.153090 139917146335040 submission_runner.py:416] After logging and checkpointing eval at step 2978: RAM USED (GB) 42.094485504
I0406 03:47:18.814645 139886490347264 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.942771, loss=2.836107
I0406 03:47:18.818027 139917146335040 submission.py:139] 3000) loss = 2.836, grad_norm = 0.943
I0406 03:54:01.179366 139886498739968 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.901873, loss=2.822524
I0406 03:54:01.193438 139917146335040 submission.py:139] 3500) loss = 2.823, grad_norm = 0.902
I0406 04:00:41.342287 139886490347264 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.918503, loss=2.879250
I0406 04:00:41.346898 139917146335040 submission.py:139] 4000) loss = 2.879, grad_norm = 0.919
I0406 04:07:23.977908 139886498739968 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.978231, loss=2.799603
I0406 04:07:23.983922 139917146335040 submission.py:139] 4500) loss = 2.800, grad_norm = 0.978
I0406 04:14:03.882190 139886490347264 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.796113, loss=2.733694
I0406 04:14:03.886674 139917146335040 submission.py:139] 5000) loss = 2.734, grad_norm = 0.796
I0406 04:20:40.767163 139886498739968 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0406 04:20:40.773737 139917146335040 submission.py:139] 5500) loss = nan, grad_norm = nan
I0406 04:27:00.547264 139917146335040 submission_runner.py:373] Before eval at step 5991: RAM USED (GB) 42.129821696
I0406 04:27:00.547490 139917146335040 spec.py:298] Evaluating on the training split.
I0406 04:27:10.204210 139917146335040 spec.py:310] Evaluating on the validation split.
I0406 04:27:18.985787 139917146335040 spec.py:326] Evaluating on the test split.
I0406 04:27:24.496703 139917146335040 submission_runner.py:382] Time since start: 4869.40s, 	Step: 5991, 	{'train/ctc_loss': nan, 'train/wer': 0.9417665027405273, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0406 04:27:24.497539 139917146335040 submission_runner.py:396] After eval at step 5991: RAM USED (GB) 41.96743168
I0406 04:27:24.515017 139886498739968 logging_writer.py:48] [5991] global_step=5991, preemption_count=0, score=2903.946143, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=4869.402531, train/ctc_loss=nan, train/wer=0.941767, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0406 04:27:24.719215 139917146335040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/librispeech_deepspeech_pytorch/trial_1/checkpoint_5991.
I0406 04:27:24.719711 139917146335040 submission_runner.py:416] After logging and checkpointing eval at step 5991: RAM USED (GB) 41.973231616
I0406 04:27:32.514569 139886490347264 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0406 04:27:32.518115 139917146335040 submission.py:139] 6000) loss = nan, grad_norm = nan
I0406 04:34:03.239178 139886498739968 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0406 04:34:03.245999 139917146335040 submission.py:139] 6500) loss = nan, grad_norm = nan
I0406 04:40:31.438970 139886490347264 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0406 04:40:31.443239 139917146335040 submission.py:139] 7000) loss = nan, grad_norm = nan
I0406 04:47:01.275613 139886498739968 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0406 04:47:01.283176 139917146335040 submission.py:139] 7500) loss = nan, grad_norm = nan
I0406 04:53:28.731737 139917146335040 submission_runner.py:373] Before eval at step 8000: RAM USED (GB) 42.0748288
I0406 04:53:28.731946 139917146335040 spec.py:298] Evaluating on the training split.
I0406 04:53:38.317231 139917146335040 spec.py:310] Evaluating on the validation split.
I0406 04:53:46.978622 139917146335040 spec.py:326] Evaluating on the test split.
I0406 04:53:52.072416 139917146335040 submission_runner.py:382] Time since start: 6457.57s, 	Step: 8000, 	{'train/ctc_loss': nan, 'train/wer': 0.9417665027405273, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0406 04:53:52.073181 139917146335040 submission_runner.py:396] After eval at step 8000: RAM USED (GB) 41.954279424
I0406 04:53:52.089185 139886498739968 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=3853.812407, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=6457.569993, train/ctc_loss=nan, train/wer=0.941767, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0406 04:53:52.289535 139917146335040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/librispeech_deepspeech_pytorch/trial_1/checkpoint_8000.
I0406 04:53:52.289996 139917146335040 submission_runner.py:416] After logging and checkpointing eval at step 8000: RAM USED (GB) 41.95960832
I0406 04:53:52.299532 139886490347264 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=3853.812407
I0406 04:53:52.615975 139917146335040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/librispeech_deepspeech_pytorch/trial_1/checkpoint_8000.
I0406 04:53:52.773725 139917146335040 submission_runner.py:550] Tuning trial 1/1
I0406 04:53:52.774090 139917146335040 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0406 04:53:52.774628 139917146335040 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': 32.1424357638654, 'train/wer': 3.790084273922745, 'validation/ctc_loss': 31.023049778983324, 'validation/wer': 3.509032974460484, 'validation/num_examples': 5348, 'test/ctc_loss': 31.03515065488096, 'test/wer': 3.7368634858732963, 'test/num_examples': 2472, 'score': 8.841522932052612, 'total_duration': 8.84374451637268, 'global_step': 1, 'preemption_count': 0}), (2978, {'train/ctc_loss': 2.6592696310904245, 'train/wer': 0.6065772655386815, 'validation/ctc_loss': 2.855153432351316, 'validation/wer': 0.6185680490513205, 'validation/num_examples': 5348, 'test/ctc_loss': 2.3876145235176462, 'test/wer': 0.5492251132370565, 'test/num_examples': 2472, 'score': 1469.131758928299, 'total_duration': 2444.034610271454, 'global_step': 2978, 'preemption_count': 0}), (5991, {'train/ctc_loss': nan, 'train/wer': 0.9417665027405273, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2903.946143388748, 'total_duration': 4869.402531385422, 'global_step': 5991, 'preemption_count': 0}), (8000, {'train/ctc_loss': nan, 'train/wer': 0.9417665027405273, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 3853.8124074935913, 'total_duration': 6457.569993019104, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0406 04:53:52.774778 139917146335040 submission_runner.py:553] Timing: 3853.8124074935913
I0406 04:53:52.774870 139917146335040 submission_runner.py:554] ====================
I0406 04:53:52.775091 139917146335040 submission_runner.py:613] Final librispeech_deepspeech score: 3853.8124074935913
