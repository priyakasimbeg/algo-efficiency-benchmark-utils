I0404 22:29:29.475333 139650457450304 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum_v2/wmt_jax.
I0404 22:29:29.518843 139650457450304 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 22:29:30.424855 139650457450304 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0404 22:29:30.425655 139650457450304 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 22:29:30.429626 139650457450304 submission_runner.py:511] Using RNG seed 3040261938
I0404 22:29:31.814096 139650457450304 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 22:29:31.814356 139650457450304 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_momentum_v2/wmt_jax/trial_1.
I0404 22:29:31.814587 139650457450304 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/hparams.json.
I0404 22:29:31.943006 139650457450304 submission_runner.py:230] Starting train once: RAM USED (GB) 4.281524224
I0404 22:29:31.943239 139650457450304 submission_runner.py:231] Initializing dataset.
I0404 22:29:31.952614 139650457450304 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:29:31.956547 139650457450304 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:29:31.956684 139650457450304 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:29:32.038867 139650457450304 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:29:33.889567 139650457450304 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.381106176
I0404 22:29:33.889758 139650457450304 submission_runner.py:240] Initializing model.
I0404 22:29:46.074158 139650457450304 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.465977344
I0404 22:29:46.074356 139650457450304 submission_runner.py:252] Initializing optimizer.
I0404 22:29:46.680298 139650457450304 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.465969152
I0404 22:29:46.680472 139650457450304 submission_runner.py:261] Initializing metrics bundle.
I0404 22:29:46.680527 139650457450304 submission_runner.py:276] Initializing checkpoint and logger.
I0404 22:29:46.681515 139650457450304 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum_v2/wmt_jax/trial_1 with prefix checkpoint_
I0404 22:29:46.681775 139650457450304 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0404 22:29:46.681837 139650457450304 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0404 22:29:47.668785 139650457450304 submission_runner.py:297] Saving meta data to /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/meta_data_0.json.
I0404 22:29:47.669765 139650457450304 submission_runner.py:300] Saving flags to /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/flags_0.json.
I0404 22:29:47.672758 139650457450304 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.46002176
I0404 22:29:47.672943 139650457450304 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.46002176
I0404 22:29:47.673001 139650457450304 submission_runner.py:313] Starting training loop.
I0404 22:29:48.416971 139650457450304 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.59297792
I0404 22:30:16.863224 139474449192704 logging_writer.py:48] [0] global_step=0, grad_norm=5.494716167449951, loss=11.066926956176758
I0404 22:30:16.874773 139650457450304 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 11.17151232
I0404 22:30:16.875016 139650457450304 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 11.17151232
I0404 22:30:16.875088 139650457450304 spec.py:298] Evaluating on the training split.
I0404 22:30:16.877457 139650457450304 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:30:16.879940 139650457450304 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:30:16.880040 139650457450304 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:30:16.910634 139650457450304 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:30:25.065216 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 22:35:31.546973 139650457450304 spec.py:310] Evaluating on the validation split.
I0404 22:35:31.550434 139650457450304 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:35:31.553728 139650457450304 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:35:31.553855 139650457450304 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:35:31.582340 139650457450304 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:35:39.077278 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 22:40:37.681488 139650457450304 spec.py:326] Evaluating on the test split.
I0404 22:40:37.683730 139650457450304 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:40:37.686118 139650457450304 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:40:37.686221 139650457450304 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:40:37.715591 139650457450304 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:40:44.613640 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 22:45:38.450633 139650457450304 submission_runner.py:382] Time since start: 29.20s, 	Step: 1, 	{'train/accuracy': 0.0006525621865876019, 'train/loss': 11.036073684692383, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.021673202514648, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.035765647888184, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0404 22:45:38.451164 139650457450304 submission_runner.py:396] After eval at step 1: RAM USED (GB) 11.61279488
I0404 22:45:38.458375 139463157610240 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=28.997737, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.035766, test/num_examples=3003, total_duration=29.202037, train/accuracy=0.000653, train/bleu=0.000000, train/loss=11.036074, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.021673, validation/num_examples=3000
I0404 22:45:39.161136 139650457450304 checkpoints.py:356] Saving checkpoint at step: 1
I0404 22:45:41.687293 139650457450304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_1
I0404 22:45:41.690499 139650457450304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_1.
I0404 22:45:41.695879 139650457450304 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 12.320137216
I0404 22:45:41.698149 139650457450304 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 12.320137216
I0404 22:45:41.765273 139650457450304 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 12.319735808
I0404 22:46:17.911203 139463166002944 logging_writer.py:48] [100] global_step=100, grad_norm=0.16262196004390717, loss=9.059389114379883
I0404 22:46:54.156174 139463266715392 logging_writer.py:48] [200] global_step=200, grad_norm=0.13324499130249023, loss=8.917145729064941
I0404 22:47:30.468917 139463166002944 logging_writer.py:48] [300] global_step=300, grad_norm=0.14414958655834198, loss=8.813738822937012
I0404 22:48:06.761773 139463266715392 logging_writer.py:48] [400] global_step=400, grad_norm=0.2522840201854706, loss=8.640533447265625
I0404 22:48:43.053927 139463166002944 logging_writer.py:48] [500] global_step=500, grad_norm=0.45477038621902466, loss=8.446401596069336
I0404 22:49:19.385535 139463266715392 logging_writer.py:48] [600] global_step=600, grad_norm=0.6178975105285645, loss=8.267616271972656
I0404 22:49:55.717236 139463166002944 logging_writer.py:48] [700] global_step=700, grad_norm=0.5606810450553894, loss=8.1384916305542
I0404 22:50:32.041423 139463266715392 logging_writer.py:48] [800] global_step=800, grad_norm=0.5965147018432617, loss=7.996344089508057
I0404 22:51:08.397338 139463166002944 logging_writer.py:48] [900] global_step=900, grad_norm=0.6107342839241028, loss=7.915640830993652
I0404 22:51:44.779849 139463266715392 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7115595936775208, loss=7.811742305755615
I0404 22:52:21.121423 139463166002944 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8004469871520996, loss=7.755585670471191
I0404 22:52:57.454823 139463266715392 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5243878960609436, loss=7.633831977844238
I0404 22:53:33.814527 139463166002944 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.684262216091156, loss=7.582794666290283
I0404 22:54:10.183230 139463266715392 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6271941661834717, loss=7.500566005706787
I0404 22:54:46.488192 139463166002944 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7310376167297363, loss=7.403207778930664
I0404 22:55:22.808960 139463266715392 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8383029103279114, loss=7.39316463470459
I0404 22:55:59.175233 139463166002944 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.652201771736145, loss=7.285606861114502
I0404 22:56:35.543135 139463266715392 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.592767059803009, loss=7.1881208419799805
I0404 22:57:11.881638 139463166002944 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7185672521591187, loss=7.070544719696045
I0404 22:57:48.231518 139463266715392 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6651297807693481, loss=7.141368389129639
I0404 22:58:24.617449 139463166002944 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7241443991661072, loss=6.881500244140625
I0404 22:59:00.961788 139463266715392 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6452762484550476, loss=6.898483753204346
I0404 22:59:37.307286 139463166002944 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.6971669793128967, loss=6.906179904937744
I0404 22:59:41.735411 139650457450304 submission_runner.py:373] Before eval at step 2314: RAM USED (GB) 12.020334592
I0404 22:59:41.735593 139650457450304 spec.py:298] Evaluating on the training split.
I0404 22:59:44.805104 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 23:04:17.951985 139650457450304 spec.py:310] Evaluating on the validation split.
I0404 23:04:20.661326 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 23:08:22.313673 139650457450304 spec.py:326] Evaluating on the test split.
I0404 23:08:25.094517 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 23:13:09.985965 139650457450304 submission_runner.py:382] Time since start: 1794.06s, 	Step: 2314, 	{'train/accuracy': 0.3068881034851074, 'train/loss': 5.556504249572754, 'train/bleu': 6.788609908567206, 'validation/accuracy': 0.2845593988895416, 'validation/loss': 5.833271503448486, 'validation/bleu': 3.6738598825363415, 'validation/num_examples': 3000, 'test/accuracy': 0.26205334067344666, 'test/loss': 6.128880023956299, 'test/bleu': 2.7703302990458947, 'test/num_examples': 3003}
I0404 23:13:09.986449 139650457450304 submission_runner.py:396] After eval at step 2314: RAM USED (GB) 12.289875968
I0404 23:13:09.993672 139463266715392 logging_writer.py:48] [2314] global_step=2314, preemption_count=0, score=865.462611, test/accuracy=0.262053, test/bleu=2.770330, test/loss=6.128880, test/num_examples=3003, total_duration=1794.061671, train/accuracy=0.306888, train/bleu=6.788610, train/loss=5.556504, validation/accuracy=0.284559, validation/bleu=3.673860, validation/loss=5.833272, validation/num_examples=3000
I0404 23:13:10.712793 139650457450304 checkpoints.py:356] Saving checkpoint at step: 2314
I0404 23:13:13.215114 139650457450304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_2314
I0404 23:13:13.218353 139650457450304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_2314.
I0404 23:13:13.222967 139650457450304 submission_runner.py:416] After logging and checkpointing eval at step 2314: RAM USED (GB) 13.026496512
I0404 23:13:44.748284 139463166002944 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.6272372603416443, loss=6.73218297958374
I0404 23:14:20.953483 139463241537280 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6829312443733215, loss=6.750791072845459
I0404 23:14:57.266690 139463166002944 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7782904505729675, loss=6.6528143882751465
I0404 23:15:33.579488 139463241537280 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6438302993774414, loss=6.571782112121582
I0404 23:16:09.921046 139463166002944 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6660619378089905, loss=6.5971269607543945
I0404 23:16:46.257250 139463241537280 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6021985411643982, loss=6.6095380783081055
I0404 23:17:22.622550 139463166002944 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6532776355743408, loss=6.450531005859375
I0404 23:17:58.972280 139463241537280 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.5850865244865417, loss=6.369100570678711
I0404 23:18:35.319018 139463166002944 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8513192534446716, loss=6.301889419555664
I0404 23:19:11.624719 139463241537280 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.6084035634994507, loss=6.271796226501465
I0404 23:19:47.968891 139463166002944 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.66590416431427, loss=6.222230911254883
I0404 23:20:24.235706 139463241537280 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7930570840835571, loss=6.141774654388428
I0404 23:21:00.581091 139463166002944 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5538738965988159, loss=6.157446384429932
I0404 23:21:36.900607 139463241537280 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6855068802833557, loss=6.022010803222656
I0404 23:22:13.235111 139463166002944 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7847695350646973, loss=5.993070602416992
I0404 23:22:49.576079 139463241537280 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7777935862541199, loss=5.913699626922607
I0404 23:23:25.932681 139463166002944 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6743268966674805, loss=5.961116790771484
I0404 23:24:02.262207 139463241537280 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8030263185501099, loss=5.8905110359191895
I0404 23:24:38.610551 139463166002944 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5810943245887756, loss=5.794224739074707
I0404 23:25:14.962172 139463241537280 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6185124516487122, loss=5.742082118988037
I0404 23:25:51.277058 139463166002944 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6175823211669922, loss=5.746510028839111
I0404 23:26:27.642275 139463241537280 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6646889448165894, loss=5.63330602645874
I0404 23:27:03.967391 139463166002944 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6360276341438293, loss=5.66834020614624
I0404 23:27:13.483174 139650457450304 submission_runner.py:373] Before eval at step 4628: RAM USED (GB) 12.445642752
I0404 23:27:13.483370 139650457450304 spec.py:298] Evaluating on the training split.
I0404 23:27:16.548928 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 23:30:15.979706 139650457450304 spec.py:310] Evaluating on the validation split.
I0404 23:30:18.679222 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 23:33:17.659673 139650457450304 spec.py:326] Evaluating on the test split.
I0404 23:33:20.423164 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 23:36:02.253535 139650457450304 submission_runner.py:382] Time since start: 3445.81s, 	Step: 4628, 	{'train/accuracy': 0.45774126052856445, 'train/loss': 3.8848347663879395, 'train/bleu': 16.890854943940315, 'validation/accuracy': 0.4482027590274811, 'validation/loss': 3.9847161769866943, 'validation/bleu': 12.715853348708489, 'validation/num_examples': 3000, 'test/accuracy': 0.434838205575943, 'test/loss': 4.146467208862305, 'test/bleu': 10.890841195152493, 'test/num_examples': 3003}
I0404 23:36:02.253978 139650457450304 submission_runner.py:396] After eval at step 4628: RAM USED (GB) 12.60888064
I0404 23:36:02.261220 139463241537280 logging_writer.py:48] [4628] global_step=4628, preemption_count=0, score=1702.714684, test/accuracy=0.434838, test/bleu=10.890841, test/loss=4.146467, test/num_examples=3003, total_duration=3445.809722, train/accuracy=0.457741, train/bleu=16.890855, train/loss=3.884835, validation/accuracy=0.448203, validation/bleu=12.715853, validation/loss=3.984716, validation/num_examples=3000
I0404 23:36:02.968979 139650457450304 checkpoints.py:356] Saving checkpoint at step: 4628
I0404 23:36:05.500611 139650457450304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_4628
I0404 23:36:05.503937 139650457450304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_4628.
I0404 23:36:05.508628 139650457450304 submission_runner.py:416] After logging and checkpointing eval at step 4628: RAM USED (GB) 13.344342016
I0404 23:36:31.985174 139463166002944 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5648435950279236, loss=5.492884159088135
I0404 23:37:08.241040 139463224751872 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5675680637359619, loss=5.554479122161865
I0404 23:37:44.545637 139463166002944 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.569721519947052, loss=5.4515862464904785
I0404 23:38:20.844303 139463224751872 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5628578066825867, loss=5.469261169433594
I0404 23:38:57.216475 139463166002944 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5885421633720398, loss=5.4966254234313965
I0404 23:39:33.501042 139463224751872 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.576005220413208, loss=5.428407669067383
I0404 23:40:09.835801 139463166002944 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.5462900400161743, loss=5.413616180419922
I0404 23:40:46.121870 139463224751872 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5181108713150024, loss=5.390992641448975
I0404 23:41:22.448865 139463166002944 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.49390482902526855, loss=5.410722732543945
I0404 23:41:58.800135 139463224751872 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.49845176935195923, loss=5.293798923492432
I0404 23:42:35.138402 139463166002944 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.4851873815059662, loss=5.341813087463379
I0404 23:43:11.458451 139463224751872 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5121232867240906, loss=5.150228500366211
I0404 23:43:47.796855 139463166002944 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4960707128047943, loss=5.260339260101318
I0404 23:44:24.122566 139463224751872 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.4678865671157837, loss=5.175640106201172
I0404 23:45:00.431894 139463166002944 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.48474636673927307, loss=5.169768810272217
I0404 23:45:36.763903 139463224751872 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.473016619682312, loss=5.283095836639404
I0404 23:46:13.084151 139463166002944 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.45573124289512634, loss=5.192038536071777
I0404 23:46:49.430951 139463224751872 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.44786134362220764, loss=5.122969150543213
I0404 23:47:25.737781 139463166002944 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.4567583203315735, loss=5.105118751525879
I0404 23:48:02.102790 139463224751872 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.4459369480609894, loss=5.122357368469238
I0404 23:48:38.442918 139463166002944 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4336397349834442, loss=5.104339599609375
I0404 23:49:14.745325 139463224751872 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4481504559516907, loss=5.093392848968506
I0404 23:49:51.035903 139463166002944 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.4497736394405365, loss=4.998227119445801
I0404 23:50:05.641868 139650457450304 submission_runner.py:373] Before eval at step 6942: RAM USED (GB) 12.85318656
I0404 23:50:05.642056 139650457450304 spec.py:298] Evaluating on the training split.
I0404 23:50:08.705916 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 23:52:59.380014 139650457450304 spec.py:310] Evaluating on the validation split.
I0404 23:53:02.092330 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 23:55:36.612965 139650457450304 spec.py:326] Evaluating on the test split.
I0404 23:55:39.380582 139650457450304 workload.py:179] Translating evaluation dataset.
I0404 23:58:07.566925 139650457450304 submission_runner.py:382] Time since start: 4817.97s, 	Step: 6942, 	{'train/accuracy': 0.5375906229019165, 'train/loss': 3.1585991382598877, 'train/bleu': 24.13019964287036, 'validation/accuracy': 0.529875636100769, 'validation/loss': 3.1868083477020264, 'validation/bleu': 19.128923902690822, 'validation/num_examples': 3000, 'test/accuracy': 0.5251989960670471, 'test/loss': 3.2550535202026367, 'test/bleu': 17.497667780124665, 'test/num_examples': 3003}
I0404 23:58:07.567406 139650457450304 submission_runner.py:396] After eval at step 6942: RAM USED (GB) 12.945846272
I0404 23:58:07.574775 139463224751872 logging_writer.py:48] [6942] global_step=6942, preemption_count=0, score=2539.878554, test/accuracy=0.525199, test/bleu=17.497668, test/loss=3.255054, test/num_examples=3003, total_duration=4817.968405, train/accuracy=0.537591, train/bleu=24.130200, train/loss=3.158599, validation/accuracy=0.529876, validation/bleu=19.128924, validation/loss=3.186808, validation/num_examples=3000
I0404 23:58:08.292296 139650457450304 checkpoints.py:356] Saving checkpoint at step: 6942
I0404 23:58:10.810204 139650457450304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_6942
I0404 23:58:10.813482 139650457450304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_6942.
I0404 23:58:10.818605 139650457450304 submission_runner.py:416] After logging and checkpointing eval at step 6942: RAM USED (GB) 13.680734208
I0404 23:58:32.201139 139463166002944 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.41718629002571106, loss=5.105630397796631
I0404 23:59:08.455291 139463216359168 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4305703341960907, loss=4.994777202606201
I0404 23:59:44.742753 139463166002944 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.436297208070755, loss=4.992976188659668
I0405 00:00:21.033887 139463216359168 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.47396865487098694, loss=4.942549705505371
I0405 00:00:57.313515 139463166002944 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.396279901266098, loss=4.977530002593994
I0405 00:01:33.644157 139463216359168 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.42594367265701294, loss=4.850234031677246
I0405 00:02:09.960537 139463166002944 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.40895381569862366, loss=5.025515556335449
I0405 00:02:46.301686 139463216359168 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4069972038269043, loss=4.9489359855651855
I0405 00:03:22.618788 139463166002944 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.41839805245399475, loss=4.913063049316406
I0405 00:03:58.899959 139463216359168 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.3876039683818817, loss=4.942255973815918
I0405 00:04:35.219753 139463166002944 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.3748709559440613, loss=4.919973373413086
I0405 00:05:11.524815 139463216359168 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.3960452079772949, loss=4.936033248901367
I0405 00:05:47.877500 139463166002944 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.3928621709346771, loss=4.906195163726807
I0405 00:06:24.158309 139463216359168 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.38699057698249817, loss=4.936722278594971
I0405 00:07:00.493198 139463166002944 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.3723515272140503, loss=4.755860328674316
I0405 00:07:36.795723 139463216359168 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.3846805989742279, loss=4.974254608154297
I0405 00:08:13.123904 139463166002944 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3762558102607727, loss=4.801150321960449
I0405 00:08:49.415832 139463216359168 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3773559629917145, loss=4.795700550079346
I0405 00:09:25.760231 139463166002944 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.37106266617774963, loss=4.790530681610107
I0405 00:10:02.084741 139463216359168 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.37254106998443604, loss=4.747521877288818
I0405 00:10:38.431823 139463166002944 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.37502321600914, loss=4.866842746734619
I0405 00:11:14.736540 139463216359168 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.36540716886520386, loss=4.767075061798096
I0405 00:11:51.033535 139463166002944 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.37711140513420105, loss=4.866701126098633
I0405 00:12:11.045866 139650457450304 submission_runner.py:373] Before eval at step 9257: RAM USED (GB) 13.210464256
I0405 00:12:11.046054 139650457450304 spec.py:298] Evaluating on the training split.
I0405 00:12:14.103996 139650457450304 workload.py:179] Translating evaluation dataset.
I0405 00:14:52.976076 139650457450304 spec.py:310] Evaluating on the validation split.
I0405 00:14:55.678576 139650457450304 workload.py:179] Translating evaluation dataset.
I0405 00:17:29.639920 139650457450304 spec.py:326] Evaluating on the test split.
I0405 00:17:32.394556 139650457450304 workload.py:179] Translating evaluation dataset.
I0405 00:19:59.179774 139650457450304 submission_runner.py:382] Time since start: 6143.37s, 	Step: 9257, 	{'train/accuracy': 0.5624949932098389, 'train/loss': 2.893315553665161, 'train/bleu': 26.554739826725374, 'validation/accuracy': 0.5673457384109497, 'validation/loss': 2.815385103225708, 'validation/bleu': 22.165819233656062, 'validation/num_examples': 3000, 'test/accuracy': 0.5680437088012695, 'test/loss': 2.8435025215148926, 'test/bleu': 20.527377392165974, 'test/num_examples': 3003}
I0405 00:19:59.180220 139650457450304 submission_runner.py:396] After eval at step 9257: RAM USED (GB) 13.303635968
I0405 00:19:59.187993 139463216359168 logging_writer.py:48] [9257] global_step=9257, preemption_count=0, score=3376.957473, test/accuracy=0.568044, test/bleu=20.527377, test/loss=2.843503, test/num_examples=3003, total_duration=6143.372359, train/accuracy=0.562495, train/bleu=26.554740, train/loss=2.893316, validation/accuracy=0.567346, validation/bleu=22.165819, validation/loss=2.815385, validation/num_examples=3000
I0405 00:19:59.904336 139650457450304 checkpoints.py:356] Saving checkpoint at step: 9257
I0405 00:20:02.451820 139650457450304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_9257
I0405 00:20:02.454870 139650457450304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_9257.
I0405 00:20:02.459628 139650457450304 submission_runner.py:416] After logging and checkpointing eval at step 9257: RAM USED (GB) 14.043762688
I0405 00:20:18.391638 139463166002944 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3979370892047882, loss=4.8594489097595215
I0405 00:20:54.598640 139463207966464 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3579646646976471, loss=4.755890369415283
I0405 00:21:30.905807 139463166002944 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.3787154257297516, loss=4.73468017578125
I0405 00:22:07.179450 139463207966464 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.38960373401641846, loss=4.660977840423584
I0405 00:22:43.464481 139463166002944 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3709433376789093, loss=4.77908992767334
I0405 00:23:19.766009 139463207966464 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.3526105582714081, loss=4.782421588897705
I0405 00:23:56.094305 139463166002944 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.37328267097473145, loss=4.719404220581055
I0405 00:24:31.709696 139650457450304 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 13.340966912
I0405 00:24:31.709900 139650457450304 spec.py:298] Evaluating on the training split.
I0405 00:24:34.782112 139650457450304 workload.py:179] Translating evaluation dataset.
I0405 00:27:13.158240 139650457450304 spec.py:310] Evaluating on the validation split.
I0405 00:27:15.867519 139650457450304 workload.py:179] Translating evaluation dataset.
I0405 00:29:44.041312 139650457450304 spec.py:326] Evaluating on the test split.
I0405 00:29:46.798308 139650457450304 workload.py:179] Translating evaluation dataset.
I0405 00:32:12.155188 139650457450304 submission_runner.py:382] Time since start: 6884.04s, 	Step: 10000, 	{'train/accuracy': 0.5711329579353333, 'train/loss': 2.8108577728271484, 'train/bleu': 26.630754310055348, 'validation/accuracy': 0.5748843550682068, 'validation/loss': 2.7277088165283203, 'validation/bleu': 22.51752652552649, 'validation/num_examples': 3000, 'test/accuracy': 0.5767009854316711, 'test/loss': 2.746305227279663, 'test/bleu': 21.40079186515472, 'test/num_examples': 3003}
I0405 00:32:12.155648 139650457450304 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 13.379796992
I0405 00:32:12.163497 139463207966464 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3645.261713, test/accuracy=0.576701, test/bleu=21.400792, test/loss=2.746305, test/num_examples=3003, total_duration=6884.036177, train/accuracy=0.571133, train/bleu=26.630754, train/loss=2.810858, validation/accuracy=0.574884, validation/bleu=22.517527, validation/loss=2.727709, validation/num_examples=3000
I0405 00:32:12.883195 139650457450304 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 00:32:15.415390 139650457450304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_10000
I0405 00:32:15.418705 139650457450304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_10000.
I0405 00:32:15.423586 139650457450304 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 14.11602432
I0405 00:32:15.430224 139463166002944 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3645.261713
I0405 00:32:15.796632 139650457450304 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 00:32:19.588438 139650457450304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_10000
I0405 00:32:19.591506 139650457450304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/wmt_jax/trial_1/checkpoint_10000.
I0405 00:32:19.657947 139650457450304 submission_runner.py:550] Tuning trial 1/1
I0405 00:32:19.658126 139650457450304 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 00:32:19.659321 139650457450304 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006525621865876019, 'train/loss': 11.036073684692383, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.021673202514648, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.035765647888184, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 28.997737169265747, 'total_duration': 29.20203709602356, 'global_step': 1, 'preemption_count': 0}), (2314, {'train/accuracy': 0.3068881034851074, 'train/loss': 5.556504249572754, 'train/bleu': 6.788609908567206, 'validation/accuracy': 0.2845593988895416, 'validation/loss': 5.833271503448486, 'validation/bleu': 3.6738598825363415, 'validation/num_examples': 3000, 'test/accuracy': 0.26205334067344666, 'test/loss': 6.128880023956299, 'test/bleu': 2.7703302990458947, 'test/num_examples': 3003, 'score': 865.4626114368439, 'total_duration': 1794.0616705417633, 'global_step': 2314, 'preemption_count': 0}), (4628, {'train/accuracy': 0.45774126052856445, 'train/loss': 3.8848347663879395, 'train/bleu': 16.890854943940315, 'validation/accuracy': 0.4482027590274811, 'validation/loss': 3.9847161769866943, 'validation/bleu': 12.715853348708489, 'validation/num_examples': 3000, 'test/accuracy': 0.434838205575943, 'test/loss': 4.146467208862305, 'test/bleu': 10.890841195152493, 'test/num_examples': 3003, 'score': 1702.7146842479706, 'total_duration': 3445.8097217082977, 'global_step': 4628, 'preemption_count': 0}), (6942, {'train/accuracy': 0.5375906229019165, 'train/loss': 3.1585991382598877, 'train/bleu': 24.13019964287036, 'validation/accuracy': 0.529875636100769, 'validation/loss': 3.1868083477020264, 'validation/bleu': 19.128923902690822, 'validation/num_examples': 3000, 'test/accuracy': 0.5251989960670471, 'test/loss': 3.2550535202026367, 'test/bleu': 17.497667780124665, 'test/num_examples': 3003, 'score': 2539.87855386734, 'total_duration': 4817.968404531479, 'global_step': 6942, 'preemption_count': 0}), (9257, {'train/accuracy': 0.5624949932098389, 'train/loss': 2.893315553665161, 'train/bleu': 26.554739826725374, 'validation/accuracy': 0.5673457384109497, 'validation/loss': 2.815385103225708, 'validation/bleu': 22.165819233656062, 'validation/num_examples': 3000, 'test/accuracy': 0.5680437088012695, 'test/loss': 2.8435025215148926, 'test/bleu': 20.527377392165974, 'test/num_examples': 3003, 'score': 3376.957473039627, 'total_duration': 6143.372359275818, 'global_step': 9257, 'preemption_count': 0}), (10000, {'train/accuracy': 0.5711329579353333, 'train/loss': 2.8108577728271484, 'train/bleu': 26.630754310055348, 'validation/accuracy': 0.5748843550682068, 'validation/loss': 2.7277088165283203, 'validation/bleu': 22.51752652552649, 'validation/num_examples': 3000, 'test/accuracy': 0.5767009854316711, 'test/loss': 2.746305227279663, 'test/bleu': 21.40079186515472, 'test/num_examples': 3003, 'score': 3645.261712551117, 'total_duration': 6884.036177158356, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0405 00:32:19.659417 139650457450304 submission_runner.py:553] Timing: 3645.261712551117
I0405 00:32:19.659458 139650457450304 submission_runner.py:554] ====================
I0405 00:32:19.659590 139650457450304 submission_runner.py:613] Final wmt score: 3645.261712551117
