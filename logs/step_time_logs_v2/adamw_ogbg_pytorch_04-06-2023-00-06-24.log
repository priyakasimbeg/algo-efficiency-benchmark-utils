WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 00:06:44.436364 139779768338240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 00:06:44.436405 139627604227904 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 00:06:44.437269 140462249899840 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 00:06:44.437693 140427817883456 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 00:06:44.437983 140097108432704 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 00:06:45.417579 140639976159040 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 00:06:45.417584 140460969416512 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 00:06:45.420094 140358684223296 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 00:06:45.420425 140358684223296 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:06:45.428169 140639976159040 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:06:45.428211 140460969416512 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:06:45.429604 139779768338240 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:06:45.429639 139627604227904 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:06:45.429787 140427817883456 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:06:45.429834 140462249899840 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:06:45.429902 140097108432704 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:06:46.679492 140358684223296 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch.
W0406 00:06:46.718074 140358684223296 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:06:46.719048 140639976159040 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:06:46.719904 140460969416512 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:06:46.720453 140462249899840 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:06:46.721372 139627604227904 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:06:46.721796 140427817883456 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 00:06:46.722626 140358684223296 submission_runner.py:511] Using RNG seed 2401707604
W0406 00:06:46.722921 139779768338240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 00:06:46.723641 140358684223296 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 00:06:46.723768 140358684223296 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1.
I0406 00:06:46.723991 140358684223296 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/hparams.json.
I0406 00:06:46.724927 140358684223296 submission_runner.py:230] Starting train once: RAM USED (GB) 5.61862656
I0406 00:06:46.725025 140358684223296 submission_runner.py:231] Initializing dataset.
I0406 00:06:46.725197 140358684223296 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.61862656
I0406 00:06:46.725263 140358684223296 submission_runner.py:240] Initializing model.
W0406 00:06:46.725765 140097108432704 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 00:06:50.550358 140358684223296 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.26042624
I0406 00:06:50.550565 140358684223296 submission_runner.py:252] Initializing optimizer.
I0406 00:06:50.551370 140358684223296 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.26042624
I0406 00:06:50.551470 140358684223296 submission_runner.py:261] Initializing metrics bundle.
I0406 00:06:50.551524 140358684223296 submission_runner.py:276] Initializing checkpoint and logger.
I0406 00:06:50.552733 140358684223296 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 00:06:50.552845 140358684223296 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 00:06:51.224502 140358684223296 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0406 00:06:51.226491 140358684223296 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/flags_0.json.
I0406 00:06:51.259753 140358684223296 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.31006976
I0406 00:06:51.260745 140358684223296 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.31006976
I0406 00:06:51.260839 140358684223296 submission_runner.py:313] Starting training loop.
I0406 00:06:51.521595 140358684223296 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0406 00:06:51.526495 140358684223296 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0406 00:06:51.622410 140358684223296 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0406 00:06:53.199635 140358684223296 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 15.54868224
I0406 00:06:56.810329 140319693727488 logging_writer.py:48] [0] global_step=0, grad_norm=2.554082, loss=0.774536
I0406 00:06:56.816368 140358684223296 submission.py:119] 0) loss = 0.775, grad_norm = 2.554
I0406 00:06:56.816978 140358684223296 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 21.924425728
I0406 00:06:56.826963 140358684223296 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 21.924425728
I0406 00:06:56.827081 140358684223296 spec.py:298] Evaluating on the training split.
I0406 00:06:56.832423 140358684223296 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0406 00:06:56.836422 140358684223296 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0406 00:06:56.889304 140358684223296 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0406 00:07:11.775789 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:11.986447 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:11.986512 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:11.986969 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:11.999140 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:11.999266 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:11.999625 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:12.000184 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:25.477057 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:25.641323 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:25.641926 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:25.647709 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:25.647781 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:25.648280 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:25.648971 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:07:25.649158 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:07:50.404368 140358684223296 spec.py:310] Evaluating on the validation split.
I0406 00:07:50.406957 140358684223296 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0406 00:07:50.411220 140358684223296 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0406 00:07:50.464073 140358684223296 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
W0406 00:08:03.432480 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:03.595465 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:03.595703 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:03.601069 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:03.601942 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:03.602067 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:03.603034 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:03.603402 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:08.871553 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:09.022846 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:09.024092 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:09.028603 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:09.029123 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:09.029692 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:09.030222 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:09.030233 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:08:33.135765 140358684223296 spec.py:326] Evaluating on the test split.
I0406 00:08:33.138789 140358684223296 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0406 00:08:33.142659 140358684223296 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0406 00:08:33.194398 140358684223296 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
W0406 00:08:46.117821 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:46.271161 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:46.271368 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:46.276240 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:46.276299 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:46.277299 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:46.277777 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:46.277834 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:51.506179 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:51.660703 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:51.661187 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:51.666834 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:51.667059 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:51.667221 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:51.667817 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:51.667907 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:09:16.808675 140358684223296 submission_runner.py:382] Time since start: 5.57s, 	Step: 1, 	{'train/accuracy': 0.4724499803400492, 'train/loss': 0.7747153043746948, 'train/mean_average_precision': 0.02096404391248724, 'validation/accuracy': 0.4766811425119773, 'validation/loss': 0.7748718857765198, 'validation/mean_average_precision': 0.02467949256184609, 'validation/num_examples': 43793, 'test/accuracy': 0.476161433814099, 'test/loss': 0.7767800688743591, 'test/mean_average_precision': 0.02657538298230653, 'test/num_examples': 43793}
I0406 00:09:16.809078 140358684223296 submission_runner.py:396] After eval at step 1: RAM USED (GB) 25.810640896
I0406 00:09:16.815994 140306732881664 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=5.564860, test/accuracy=0.476161, test/loss=0.776780, test/mean_average_precision=0.026575, test/num_examples=43793, total_duration=5.566703, train/accuracy=0.472450, train/loss=0.774715, train/mean_average_precision=0.020964, validation/accuracy=0.476681, validation/loss=0.774872, validation/mean_average_precision=0.024679, validation/num_examples=43793
I0406 00:09:16.905236 140358684223296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/checkpoint_1.
I0406 00:09:16.905678 140358684223296 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 25.810223104
I0406 00:09:17.136154 140358684223296 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 25.842356224
I0406 00:09:17.138792 140358684223296 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:09:17.145336 140462249899840 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:09:17.145763 139627604227904 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:09:17.145765 139779768338240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:09:17.145871 140097108432704 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:09:17.145870 140427817883456 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:09:17.145882 140460969416512 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:09:17.145880 140639976159040 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:09:17.178785 140306741274368 logging_writer.py:48] [1] global_step=1, grad_norm=2.533335, loss=0.774337
I0406 00:09:17.182663 140358684223296 submission.py:119] 1) loss = 0.774, grad_norm = 2.533
I0406 00:09:17.183222 140358684223296 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 25.841811456
I0406 00:09:17.445698 140306732881664 logging_writer.py:48] [2] global_step=2, grad_norm=2.546301, loss=0.775250
I0406 00:09:17.449679 140358684223296 submission.py:119] 2) loss = 0.775, grad_norm = 2.546
I0406 00:09:17.713684 140306741274368 logging_writer.py:48] [3] global_step=3, grad_norm=2.530372, loss=0.772283
I0406 00:09:17.717583 140358684223296 submission.py:119] 3) loss = 0.772, grad_norm = 2.530
I0406 00:09:17.987456 140306732881664 logging_writer.py:48] [4] global_step=4, grad_norm=2.520579, loss=0.769274
I0406 00:09:17.991645 140358684223296 submission.py:119] 4) loss = 0.769, grad_norm = 2.521
I0406 00:09:18.251586 140306741274368 logging_writer.py:48] [5] global_step=5, grad_norm=2.511060, loss=0.767398
I0406 00:09:18.255440 140358684223296 submission.py:119] 5) loss = 0.767, grad_norm = 2.511
I0406 00:09:18.518357 140306732881664 logging_writer.py:48] [6] global_step=6, grad_norm=2.456684, loss=0.761101
I0406 00:09:18.522619 140358684223296 submission.py:119] 6) loss = 0.761, grad_norm = 2.457
I0406 00:09:18.788473 140306741274368 logging_writer.py:48] [7] global_step=7, grad_norm=2.422115, loss=0.758410
I0406 00:09:18.792438 140358684223296 submission.py:119] 7) loss = 0.758, grad_norm = 2.422
I0406 00:09:19.058576 140306732881664 logging_writer.py:48] [8] global_step=8, grad_norm=2.376844, loss=0.753609
I0406 00:09:19.062488 140358684223296 submission.py:119] 8) loss = 0.754, grad_norm = 2.377
I0406 00:09:19.324723 140306741274368 logging_writer.py:48] [9] global_step=9, grad_norm=2.351704, loss=0.748121
I0406 00:09:19.328616 140358684223296 submission.py:119] 9) loss = 0.748, grad_norm = 2.352
I0406 00:09:19.593896 140306732881664 logging_writer.py:48] [10] global_step=10, grad_norm=2.314777, loss=0.743430
I0406 00:09:19.597954 140358684223296 submission.py:119] 10) loss = 0.743, grad_norm = 2.315
I0406 00:09:19.864032 140306741274368 logging_writer.py:48] [11] global_step=11, grad_norm=2.304345, loss=0.735222
I0406 00:09:19.867825 140358684223296 submission.py:119] 11) loss = 0.735, grad_norm = 2.304
I0406 00:09:20.132198 140306732881664 logging_writer.py:48] [12] global_step=12, grad_norm=2.300731, loss=0.729920
I0406 00:09:20.135955 140358684223296 submission.py:119] 12) loss = 0.730, grad_norm = 2.301
I0406 00:09:20.398717 140306741274368 logging_writer.py:48] [13] global_step=13, grad_norm=2.275544, loss=0.721617
I0406 00:09:20.402630 140358684223296 submission.py:119] 13) loss = 0.722, grad_norm = 2.276
I0406 00:09:20.669489 140306732881664 logging_writer.py:48] [14] global_step=14, grad_norm=2.280256, loss=0.713184
I0406 00:09:20.673499 140358684223296 submission.py:119] 14) loss = 0.713, grad_norm = 2.280
I0406 00:09:20.938818 140306741274368 logging_writer.py:48] [15] global_step=15, grad_norm=2.221237, loss=0.704807
I0406 00:09:20.942837 140358684223296 submission.py:119] 15) loss = 0.705, grad_norm = 2.221
I0406 00:09:21.204110 140306732881664 logging_writer.py:48] [16] global_step=16, grad_norm=2.118612, loss=0.697086
I0406 00:09:21.207933 140358684223296 submission.py:119] 16) loss = 0.697, grad_norm = 2.119
I0406 00:09:21.470185 140306741274368 logging_writer.py:48] [17] global_step=17, grad_norm=2.013228, loss=0.685260
I0406 00:09:21.474121 140358684223296 submission.py:119] 17) loss = 0.685, grad_norm = 2.013
I0406 00:09:21.739485 140306732881664 logging_writer.py:48] [18] global_step=18, grad_norm=2.018347, loss=0.676471
I0406 00:09:21.743432 140358684223296 submission.py:119] 18) loss = 0.676, grad_norm = 2.018
I0406 00:09:22.008230 140306741274368 logging_writer.py:48] [19] global_step=19, grad_norm=1.975010, loss=0.668041
I0406 00:09:22.012206 140358684223296 submission.py:119] 19) loss = 0.668, grad_norm = 1.975
I0406 00:09:22.275211 140306732881664 logging_writer.py:48] [20] global_step=20, grad_norm=1.889927, loss=0.657470
I0406 00:09:22.279169 140358684223296 submission.py:119] 20) loss = 0.657, grad_norm = 1.890
I0406 00:09:22.542515 140306741274368 logging_writer.py:48] [21] global_step=21, grad_norm=1.835592, loss=0.648722
I0406 00:09:22.546302 140358684223296 submission.py:119] 21) loss = 0.649, grad_norm = 1.836
I0406 00:09:22.815400 140306732881664 logging_writer.py:48] [22] global_step=22, grad_norm=1.742435, loss=0.639235
I0406 00:09:22.819453 140358684223296 submission.py:119] 22) loss = 0.639, grad_norm = 1.742
I0406 00:09:23.081569 140306741274368 logging_writer.py:48] [23] global_step=23, grad_norm=1.664679, loss=0.630162
I0406 00:09:23.085627 140358684223296 submission.py:119] 23) loss = 0.630, grad_norm = 1.665
I0406 00:09:23.347620 140306732881664 logging_writer.py:48] [24] global_step=24, grad_norm=1.617155, loss=0.620780
I0406 00:09:23.351627 140358684223296 submission.py:119] 24) loss = 0.621, grad_norm = 1.617
I0406 00:09:23.617678 140306741274368 logging_writer.py:48] [25] global_step=25, grad_norm=1.614273, loss=0.612318
I0406 00:09:23.621745 140358684223296 submission.py:119] 25) loss = 0.612, grad_norm = 1.614
I0406 00:09:23.880219 140306732881664 logging_writer.py:48] [26] global_step=26, grad_norm=1.558801, loss=0.601652
I0406 00:09:23.883969 140358684223296 submission.py:119] 26) loss = 0.602, grad_norm = 1.559
I0406 00:09:24.148632 140306741274368 logging_writer.py:48] [27] global_step=27, grad_norm=1.442593, loss=0.594543
I0406 00:09:24.152423 140358684223296 submission.py:119] 27) loss = 0.595, grad_norm = 1.443
I0406 00:09:24.415451 140306732881664 logging_writer.py:48] [28] global_step=28, grad_norm=1.372502, loss=0.583389
I0406 00:09:24.419444 140358684223296 submission.py:119] 28) loss = 0.583, grad_norm = 1.373
I0406 00:09:24.683948 140306741274368 logging_writer.py:48] [29] global_step=29, grad_norm=1.295591, loss=0.576606
I0406 00:09:24.687780 140358684223296 submission.py:119] 29) loss = 0.577, grad_norm = 1.296
I0406 00:09:24.951533 140306732881664 logging_writer.py:48] [30] global_step=30, grad_norm=1.241305, loss=0.568225
I0406 00:09:24.955372 140358684223296 submission.py:119] 30) loss = 0.568, grad_norm = 1.241
I0406 00:09:25.216218 140306741274368 logging_writer.py:48] [31] global_step=31, grad_norm=1.174706, loss=0.562513
I0406 00:09:25.220205 140358684223296 submission.py:119] 31) loss = 0.563, grad_norm = 1.175
I0406 00:09:25.478697 140306732881664 logging_writer.py:48] [32] global_step=32, grad_norm=1.138867, loss=0.553746
I0406 00:09:25.482554 140358684223296 submission.py:119] 32) loss = 0.554, grad_norm = 1.139
I0406 00:09:25.749861 140306741274368 logging_writer.py:48] [33] global_step=33, grad_norm=1.197439, loss=0.545919
I0406 00:09:25.753838 140358684223296 submission.py:119] 33) loss = 0.546, grad_norm = 1.197
I0406 00:09:26.019819 140306732881664 logging_writer.py:48] [34] global_step=34, grad_norm=1.183760, loss=0.540700
I0406 00:09:26.023647 140358684223296 submission.py:119] 34) loss = 0.541, grad_norm = 1.184
I0406 00:09:26.291597 140306741274368 logging_writer.py:48] [35] global_step=35, grad_norm=1.122524, loss=0.530751
I0406 00:09:26.295414 140358684223296 submission.py:119] 35) loss = 0.531, grad_norm = 1.123
I0406 00:09:26.562880 140306732881664 logging_writer.py:48] [36] global_step=36, grad_norm=1.087274, loss=0.523832
I0406 00:09:26.566701 140358684223296 submission.py:119] 36) loss = 0.524, grad_norm = 1.087
I0406 00:09:26.832225 140306741274368 logging_writer.py:48] [37] global_step=37, grad_norm=1.032132, loss=0.516496
I0406 00:09:26.836173 140358684223296 submission.py:119] 37) loss = 0.516, grad_norm = 1.032
I0406 00:09:27.098855 140306732881664 logging_writer.py:48] [38] global_step=38, grad_norm=0.987778, loss=0.509677
I0406 00:09:27.102756 140358684223296 submission.py:119] 38) loss = 0.510, grad_norm = 0.988
I0406 00:09:27.366565 140306741274368 logging_writer.py:48] [39] global_step=39, grad_norm=0.950661, loss=0.503734
I0406 00:09:27.370760 140358684223296 submission.py:119] 39) loss = 0.504, grad_norm = 0.951
I0406 00:09:27.637237 140306732881664 logging_writer.py:48] [40] global_step=40, grad_norm=0.877547, loss=0.499577
I0406 00:09:27.641150 140358684223296 submission.py:119] 40) loss = 0.500, grad_norm = 0.878
I0406 00:09:27.906664 140306741274368 logging_writer.py:48] [41] global_step=41, grad_norm=0.873614, loss=0.489739
I0406 00:09:27.910708 140358684223296 submission.py:119] 41) loss = 0.490, grad_norm = 0.874
I0406 00:09:28.174047 140306732881664 logging_writer.py:48] [42] global_step=42, grad_norm=0.838559, loss=0.484428
I0406 00:09:28.178032 140358684223296 submission.py:119] 42) loss = 0.484, grad_norm = 0.839
I0406 00:09:28.445559 140306741274368 logging_writer.py:48] [43] global_step=43, grad_norm=0.779334, loss=0.482628
I0406 00:09:28.449967 140358684223296 submission.py:119] 43) loss = 0.483, grad_norm = 0.779
I0406 00:09:28.717666 140306732881664 logging_writer.py:48] [44] global_step=44, grad_norm=0.765564, loss=0.478487
I0406 00:09:28.721793 140358684223296 submission.py:119] 44) loss = 0.478, grad_norm = 0.766
I0406 00:09:28.985710 140306741274368 logging_writer.py:48] [45] global_step=45, grad_norm=0.770793, loss=0.471610
I0406 00:09:28.989793 140358684223296 submission.py:119] 45) loss = 0.472, grad_norm = 0.771
I0406 00:09:29.251614 140306732881664 logging_writer.py:48] [46] global_step=46, grad_norm=0.767196, loss=0.468367
I0406 00:09:29.256036 140358684223296 submission.py:119] 46) loss = 0.468, grad_norm = 0.767
I0406 00:09:29.515205 140306741274368 logging_writer.py:48] [47] global_step=47, grad_norm=0.763660, loss=0.459031
I0406 00:09:29.519168 140358684223296 submission.py:119] 47) loss = 0.459, grad_norm = 0.764
I0406 00:09:29.782635 140306732881664 logging_writer.py:48] [48] global_step=48, grad_norm=0.742522, loss=0.452551
I0406 00:09:29.786382 140358684223296 submission.py:119] 48) loss = 0.453, grad_norm = 0.743
I0406 00:09:30.042027 140306741274368 logging_writer.py:48] [49] global_step=49, grad_norm=0.730599, loss=0.447415
I0406 00:09:30.046129 140358684223296 submission.py:119] 49) loss = 0.447, grad_norm = 0.731
I0406 00:09:30.305566 140306732881664 logging_writer.py:48] [50] global_step=50, grad_norm=0.702303, loss=0.440609
I0406 00:09:30.309684 140358684223296 submission.py:119] 50) loss = 0.441, grad_norm = 0.702
I0406 00:09:30.575691 140306741274368 logging_writer.py:48] [51] global_step=51, grad_norm=0.691950, loss=0.437862
I0406 00:09:30.580055 140358684223296 submission.py:119] 51) loss = 0.438, grad_norm = 0.692
I0406 00:09:30.850025 140306732881664 logging_writer.py:48] [52] global_step=52, grad_norm=0.703473, loss=0.431652
I0406 00:09:30.854132 140358684223296 submission.py:119] 52) loss = 0.432, grad_norm = 0.703
I0406 00:09:31.124017 140306741274368 logging_writer.py:48] [53] global_step=53, grad_norm=0.672939, loss=0.429425
I0406 00:09:31.127822 140358684223296 submission.py:119] 53) loss = 0.429, grad_norm = 0.673
I0406 00:09:31.399603 140306732881664 logging_writer.py:48] [54] global_step=54, grad_norm=0.645682, loss=0.420292
I0406 00:09:31.403877 140358684223296 submission.py:119] 54) loss = 0.420, grad_norm = 0.646
I0406 00:09:31.672870 140306741274368 logging_writer.py:48] [55] global_step=55, grad_norm=0.622373, loss=0.413868
I0406 00:09:31.676724 140358684223296 submission.py:119] 55) loss = 0.414, grad_norm = 0.622
I0406 00:09:31.954751 140306732881664 logging_writer.py:48] [56] global_step=56, grad_norm=0.600016, loss=0.412923
I0406 00:09:31.958338 140358684223296 submission.py:119] 56) loss = 0.413, grad_norm = 0.600
I0406 00:09:32.220720 140306741274368 logging_writer.py:48] [57] global_step=57, grad_norm=0.615804, loss=0.406904
I0406 00:09:32.224515 140358684223296 submission.py:119] 57) loss = 0.407, grad_norm = 0.616
I0406 00:09:32.492532 140306732881664 logging_writer.py:48] [58] global_step=58, grad_norm=0.560738, loss=0.402301
I0406 00:09:32.496379 140358684223296 submission.py:119] 58) loss = 0.402, grad_norm = 0.561
I0406 00:09:32.762077 140306741274368 logging_writer.py:48] [59] global_step=59, grad_norm=0.535342, loss=0.398777
I0406 00:09:32.766084 140358684223296 submission.py:119] 59) loss = 0.399, grad_norm = 0.535
I0406 00:09:33.039150 140306732881664 logging_writer.py:48] [60] global_step=60, grad_norm=0.526975, loss=0.393457
I0406 00:09:33.043046 140358684223296 submission.py:119] 60) loss = 0.393, grad_norm = 0.527
I0406 00:09:33.304004 140306741274368 logging_writer.py:48] [61] global_step=61, grad_norm=0.502089, loss=0.389645
I0406 00:09:33.307824 140358684223296 submission.py:119] 61) loss = 0.390, grad_norm = 0.502
I0406 00:09:33.569851 140306732881664 logging_writer.py:48] [62] global_step=62, grad_norm=0.493019, loss=0.390348
I0406 00:09:33.573759 140358684223296 submission.py:119] 62) loss = 0.390, grad_norm = 0.493
I0406 00:09:33.845965 140306741274368 logging_writer.py:48] [63] global_step=63, grad_norm=0.479732, loss=0.386647
I0406 00:09:33.850193 140358684223296 submission.py:119] 63) loss = 0.387, grad_norm = 0.480
I0406 00:09:34.111791 140306732881664 logging_writer.py:48] [64] global_step=64, grad_norm=0.482687, loss=0.383932
I0406 00:09:34.115667 140358684223296 submission.py:119] 64) loss = 0.384, grad_norm = 0.483
I0406 00:09:34.382787 140306741274368 logging_writer.py:48] [65] global_step=65, grad_norm=0.469021, loss=0.378328
I0406 00:09:34.386945 140358684223296 submission.py:119] 65) loss = 0.378, grad_norm = 0.469
I0406 00:09:34.653968 140306732881664 logging_writer.py:48] [66] global_step=66, grad_norm=0.452287, loss=0.376334
I0406 00:09:34.657906 140358684223296 submission.py:119] 66) loss = 0.376, grad_norm = 0.452
I0406 00:09:34.922912 140306741274368 logging_writer.py:48] [67] global_step=67, grad_norm=0.440686, loss=0.376130
I0406 00:09:34.926732 140358684223296 submission.py:119] 67) loss = 0.376, grad_norm = 0.441
I0406 00:09:35.196281 140306732881664 logging_writer.py:48] [68] global_step=68, grad_norm=0.450906, loss=0.371969
I0406 00:09:35.200242 140358684223296 submission.py:119] 68) loss = 0.372, grad_norm = 0.451
I0406 00:09:35.467817 140306741274368 logging_writer.py:48] [69] global_step=69, grad_norm=0.431734, loss=0.370247
I0406 00:09:35.471494 140358684223296 submission.py:119] 69) loss = 0.370, grad_norm = 0.432
I0406 00:09:35.740736 140306732881664 logging_writer.py:48] [70] global_step=70, grad_norm=0.431734, loss=0.364765
I0406 00:09:35.744707 140358684223296 submission.py:119] 70) loss = 0.365, grad_norm = 0.432
I0406 00:09:36.007445 140306741274368 logging_writer.py:48] [71] global_step=71, grad_norm=0.421045, loss=0.366049
I0406 00:09:36.011387 140358684223296 submission.py:119] 71) loss = 0.366, grad_norm = 0.421
I0406 00:09:36.279357 140306732881664 logging_writer.py:48] [72] global_step=72, grad_norm=0.423742, loss=0.364637
I0406 00:09:36.283409 140358684223296 submission.py:119] 72) loss = 0.365, grad_norm = 0.424
I0406 00:09:36.547604 140306741274368 logging_writer.py:48] [73] global_step=73, grad_norm=0.415785, loss=0.362848
I0406 00:09:36.551704 140358684223296 submission.py:119] 73) loss = 0.363, grad_norm = 0.416
I0406 00:09:36.816793 140306732881664 logging_writer.py:48] [74] global_step=74, grad_norm=0.415187, loss=0.359182
I0406 00:09:36.820871 140358684223296 submission.py:119] 74) loss = 0.359, grad_norm = 0.415
I0406 00:09:37.083711 140306741274368 logging_writer.py:48] [75] global_step=75, grad_norm=0.411203, loss=0.355775
I0406 00:09:37.087581 140358684223296 submission.py:119] 75) loss = 0.356, grad_norm = 0.411
I0406 00:09:37.352653 140306732881664 logging_writer.py:48] [76] global_step=76, grad_norm=0.399610, loss=0.355338
I0406 00:09:37.356432 140358684223296 submission.py:119] 76) loss = 0.355, grad_norm = 0.400
I0406 00:09:37.622779 140306741274368 logging_writer.py:48] [77] global_step=77, grad_norm=0.404446, loss=0.354413
I0406 00:09:37.626666 140358684223296 submission.py:119] 77) loss = 0.354, grad_norm = 0.404
I0406 00:09:37.888783 140306732881664 logging_writer.py:48] [78] global_step=78, grad_norm=0.398026, loss=0.352076
I0406 00:09:37.892709 140358684223296 submission.py:119] 78) loss = 0.352, grad_norm = 0.398
I0406 00:09:38.147070 140306741274368 logging_writer.py:48] [79] global_step=79, grad_norm=0.398568, loss=0.348265
I0406 00:09:38.151546 140358684223296 submission.py:119] 79) loss = 0.348, grad_norm = 0.399
I0406 00:09:38.414108 140306732881664 logging_writer.py:48] [80] global_step=80, grad_norm=0.394144, loss=0.347291
I0406 00:09:38.418266 140358684223296 submission.py:119] 80) loss = 0.347, grad_norm = 0.394
I0406 00:09:38.686603 140306741274368 logging_writer.py:48] [81] global_step=81, grad_norm=0.387814, loss=0.345710
I0406 00:09:38.690793 140358684223296 submission.py:119] 81) loss = 0.346, grad_norm = 0.388
I0406 00:09:38.955376 140306732881664 logging_writer.py:48] [82] global_step=82, grad_norm=0.390581, loss=0.343342
I0406 00:09:38.959199 140358684223296 submission.py:119] 82) loss = 0.343, grad_norm = 0.391
I0406 00:09:39.222104 140306741274368 logging_writer.py:48] [83] global_step=83, grad_norm=0.384950, loss=0.342871
I0406 00:09:39.226124 140358684223296 submission.py:119] 83) loss = 0.343, grad_norm = 0.385
I0406 00:09:39.488681 140306732881664 logging_writer.py:48] [84] global_step=84, grad_norm=0.385130, loss=0.340474
I0406 00:09:39.492768 140358684223296 submission.py:119] 84) loss = 0.340, grad_norm = 0.385
I0406 00:09:39.758031 140306741274368 logging_writer.py:48] [85] global_step=85, grad_norm=0.381526, loss=0.338253
I0406 00:09:39.761985 140358684223296 submission.py:119] 85) loss = 0.338, grad_norm = 0.382
I0406 00:09:40.023317 140306732881664 logging_writer.py:48] [86] global_step=86, grad_norm=0.384959, loss=0.337009
I0406 00:09:40.027066 140358684223296 submission.py:119] 86) loss = 0.337, grad_norm = 0.385
I0406 00:09:40.283934 140306741274368 logging_writer.py:48] [87] global_step=87, grad_norm=0.377687, loss=0.336518
I0406 00:09:40.287879 140358684223296 submission.py:119] 87) loss = 0.337, grad_norm = 0.378
I0406 00:09:40.549716 140306732881664 logging_writer.py:48] [88] global_step=88, grad_norm=0.377511, loss=0.334558
I0406 00:09:40.553542 140358684223296 submission.py:119] 88) loss = 0.335, grad_norm = 0.378
I0406 00:09:40.815262 140306741274368 logging_writer.py:48] [89] global_step=89, grad_norm=0.380130, loss=0.331708
I0406 00:09:40.818929 140358684223296 submission.py:119] 89) loss = 0.332, grad_norm = 0.380
I0406 00:09:41.082969 140306732881664 logging_writer.py:48] [90] global_step=90, grad_norm=0.378356, loss=0.331495
I0406 00:09:41.086777 140358684223296 submission.py:119] 90) loss = 0.331, grad_norm = 0.378
I0406 00:09:41.345767 140306741274368 logging_writer.py:48] [91] global_step=91, grad_norm=0.373620, loss=0.329367
I0406 00:09:41.349511 140358684223296 submission.py:119] 91) loss = 0.329, grad_norm = 0.374
I0406 00:09:41.614235 140306732881664 logging_writer.py:48] [92] global_step=92, grad_norm=0.372606, loss=0.328509
I0406 00:09:41.618055 140358684223296 submission.py:119] 92) loss = 0.329, grad_norm = 0.373
I0406 00:09:41.879894 140306741274368 logging_writer.py:48] [93] global_step=93, grad_norm=0.370753, loss=0.328995
I0406 00:09:41.883712 140358684223296 submission.py:119] 93) loss = 0.329, grad_norm = 0.371
I0406 00:09:42.144454 140306732881664 logging_writer.py:48] [94] global_step=94, grad_norm=0.369448, loss=0.324742
I0406 00:09:42.148327 140358684223296 submission.py:119] 94) loss = 0.325, grad_norm = 0.369
I0406 00:09:42.408632 140306741274368 logging_writer.py:48] [95] global_step=95, grad_norm=0.366091, loss=0.324711
I0406 00:09:42.412826 140358684223296 submission.py:119] 95) loss = 0.325, grad_norm = 0.366
I0406 00:09:42.676141 140306732881664 logging_writer.py:48] [96] global_step=96, grad_norm=0.368207, loss=0.321382
I0406 00:09:42.679897 140358684223296 submission.py:119] 96) loss = 0.321, grad_norm = 0.368
I0406 00:09:42.940750 140306741274368 logging_writer.py:48] [97] global_step=97, grad_norm=0.366647, loss=0.321756
I0406 00:09:42.944945 140358684223296 submission.py:119] 97) loss = 0.322, grad_norm = 0.367
I0406 00:09:43.205075 140306732881664 logging_writer.py:48] [98] global_step=98, grad_norm=0.365605, loss=0.319664
I0406 00:09:43.208685 140358684223296 submission.py:119] 98) loss = 0.320, grad_norm = 0.366
I0406 00:09:43.468790 140306741274368 logging_writer.py:48] [99] global_step=99, grad_norm=0.362907, loss=0.317311
I0406 00:09:43.472695 140358684223296 submission.py:119] 99) loss = 0.317, grad_norm = 0.363
I0406 00:09:43.733338 140306732881664 logging_writer.py:48] [100] global_step=100, grad_norm=0.361356, loss=0.316772
I0406 00:09:43.737089 140358684223296 submission.py:119] 100) loss = 0.317, grad_norm = 0.361
I0406 00:11:26.503937 140306741274368 logging_writer.py:48] [500] global_step=500, grad_norm=0.085130, loss=0.058903
I0406 00:11:26.508260 140358684223296 submission.py:119] 500) loss = 0.059, grad_norm = 0.085
I0406 00:13:17.049764 140358684223296 submission_runner.py:373] Before eval at step 933: RAM USED (GB) 26.596102144
I0406 00:13:17.049979 140358684223296 spec.py:298] Evaluating on the training split.
W0406 00:13:30.561196 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:30.720046 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:30.720517 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:30.725762 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:30.726315 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:30.726564 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:30.726647 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:30.726779 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:44.433646 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:44.582954 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:44.585289 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:44.589036 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:44.589248 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:44.589469 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:44.589843 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:44.589890 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:14:09.260030 140358684223296 spec.py:310] Evaluating on the validation split.
W0406 00:14:09.615274 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:09.813889 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:09.814589 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:09.821363 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:09.821394 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:09.821404 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:09.821451 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:09.821744 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:09.963704 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:10.115527 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:10.116232 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:10.121766 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:10.122554 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:10.122561 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:10.122730 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:10.122979 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:14:12.552270 140358684223296 spec.py:326] Evaluating on the test split.
W0406 00:14:12.881793 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.036735 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.037429 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.043815 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.044160 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.044505 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.044941 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.045181 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.218421 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.370463 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.370876 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.376533 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.377488 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.377596 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.377656 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:13.377697 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:14:15.661200 140358684223296 submission_runner.py:382] Time since start: 385.79s, 	Step: 933, 	{'train/accuracy': 0.9866426927706996, 'train/loss': 0.05243820324540138, 'train/mean_average_precision': 0.049219148988067736, 'validation/accuracy': 0.9841650652306109, 'validation/loss': 0.061618585139513016, 'validation/mean_average_precision': 0.05068418577214737, 'validation/num_examples': 43793, 'test/accuracy': 0.9831905413151972, 'test/loss': 0.06497323513031006, 'test/mean_average_precision': 0.05203467172474047, 'test/num_examples': 43793}
I0406 00:14:15.661618 140358684223296 submission_runner.py:396] After eval at step 933: RAM USED (GB) 28.028198912
I0406 00:14:15.670507 140306732881664 logging_writer.py:48] [933] global_step=933, preemption_count=0, score=244.699194, test/accuracy=0.983191, test/loss=0.064973, test/mean_average_precision=0.052035, test/num_examples=43793, total_duration=385.789137, train/accuracy=0.986643, train/loss=0.052438, train/mean_average_precision=0.049219, validation/accuracy=0.984165, validation/loss=0.061619, validation/mean_average_precision=0.050684, validation/num_examples=43793
I0406 00:14:15.765578 140358684223296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/checkpoint_933.
I0406 00:14:15.766133 140358684223296 submission_runner.py:416] After logging and checkpointing eval at step 933: RAM USED (GB) 27.927093248
I0406 00:14:33.366773 140306741274368 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.025865, loss=0.051510
I0406 00:14:33.372321 140358684223296 submission.py:119] 1000) loss = 0.052, grad_norm = 0.026
I0406 00:16:41.369507 140306732881664 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.109744, loss=0.055090
I0406 00:16:41.374842 140358684223296 submission.py:119] 1500) loss = 0.055, grad_norm = 0.110
I0406 00:18:15.981266 140358684223296 submission_runner.py:373] Before eval at step 1873: RAM USED (GB) 28.38482944
I0406 00:18:15.981502 140358684223296 spec.py:298] Evaluating on the training split.
W0406 00:18:29.939601 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:30.139225 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:30.139824 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:30.145124 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:30.145904 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:30.146605 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:30.146658 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:30.147725 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:44.117954 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:44.314550 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:44.315653 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:44.321674 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:44.322027 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:44.322893 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:44.322934 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:44.323022 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:07.559930 140358684223296 spec.py:310] Evaluating on the validation split.
W0406 00:19:08.347466 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.571490 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.573010 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.573462 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.578782 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.579061 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.579096 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.583641 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.756965 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.966373 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.967609 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.972478 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.972968 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.973223 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.973287 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:08.973390 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:11.176331 140358684223296 spec.py:326] Evaluating on the test split.
W0406 00:19:11.557015 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:11.781297 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:11.782686 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:11.786403 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:11.787425 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:11.787485 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:11.788233 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:11.788580 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:11.968801 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:12.186221 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:12.186748 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:12.191994 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:12.192556 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:12.192915 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:12.193353 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:12.193846 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:14.402548 140358684223296 submission_runner.py:382] Time since start: 684.72s, 	Step: 1873, 	{'train/accuracy': 0.9873643147063328, 'train/loss': 0.047327518463134766, 'train/mean_average_precision': 0.09636129108719713, 'validation/accuracy': 0.984486164323171, 'validation/loss': 0.056699566543102264, 'validation/mean_average_precision': 0.09598750336735065, 'validation/num_examples': 43793, 'test/accuracy': 0.9834470488783796, 'test/loss': 0.05987212806940079, 'test/mean_average_precision': 0.09279715113643165, 'test/num_examples': 43793}
I0406 00:19:14.402993 140358684223296 submission_runner.py:396] After eval at step 1873: RAM USED (GB) 28.962635776
I0406 00:19:14.411815 140306741274368 logging_writer.py:48] [1873] global_step=1873, preemption_count=0, score=483.891702, test/accuracy=0.983447, test/loss=0.059872, test/mean_average_precision=0.092797, test/num_examples=43793, total_duration=684.720700, train/accuracy=0.987364, train/loss=0.047328, train/mean_average_precision=0.096361, validation/accuracy=0.984486, validation/loss=0.056700, validation/mean_average_precision=0.095988, validation/num_examples=43793
I0406 00:19:14.505048 140358684223296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/checkpoint_1873.
I0406 00:19:14.505647 140358684223296 submission_runner.py:416] After logging and checkpointing eval at step 1873: RAM USED (GB) 28.961959936
I0406 00:19:47.896776 140306732881664 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.043724, loss=0.047688
I0406 00:19:47.901931 140358684223296 submission.py:119] 2000) loss = 0.048, grad_norm = 0.044
I0406 00:21:57.878873 140306741274368 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.045189, loss=0.050069
I0406 00:21:57.884006 140358684223296 submission.py:119] 2500) loss = 0.050, grad_norm = 0.045
I0406 00:23:14.796170 140358684223296 submission_runner.py:373] Before eval at step 2801: RAM USED (GB) 29.149937664
I0406 00:23:14.796378 140358684223296 spec.py:298] Evaluating on the training split.
W0406 00:23:28.488778 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:28.726402 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:28.726743 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:28.732677 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:28.732938 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:28.733378 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:28.734113 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:28.734153 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:42.619408 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:42.858181 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:42.858737 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:42.864897 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:42.865263 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:42.865499 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:42.865661 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:42.865913 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:09.362768 140358684223296 spec.py:310] Evaluating on the validation split.
W0406 00:24:09.833858 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.091441 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.092511 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.098110 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.098316 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.098879 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.099103 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.099426 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.315945 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.569835 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.570605 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.575556 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.576164 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.576433 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.576704 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:10.577096 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:13.136871 140358684223296 spec.py:326] Evaluating on the test split.
W0406 00:24:13.597639 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:13.848522 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:13.848574 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:13.854321 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:13.854789 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:13.855238 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:13.855710 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:13.856161 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:14.077268 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:14.329708 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:14.330785 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:14.336260 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:14.336965 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:14.337175 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:14.337268 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:14.337624 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:16.840650 140358684223296 submission_runner.py:382] Time since start: 983.54s, 	Step: 2801, 	{'train/accuracy': 0.9877699330519366, 'train/loss': 0.04451543837785721, 'train/mean_average_precision': 0.1390937383151805, 'validation/accuracy': 0.9849111842345623, 'validation/loss': 0.05380653589963913, 'validation/mean_average_precision': 0.12944963932790865, 'validation/num_examples': 43793, 'test/accuracy': 0.9839263684299421, 'test/loss': 0.05675715208053589, 'test/mean_average_precision': 0.13312952175834564, 'test/num_examples': 43793}
I0406 00:24:16.841026 140358684223296 submission_runner.py:396] After eval at step 2801: RAM USED (GB) 29.538476032
I0406 00:24:16.849418 140306732881664 logging_writer.py:48] [2801] global_step=2801, preemption_count=0, score=723.089801, test/accuracy=0.983926, test/loss=0.056757, test/mean_average_precision=0.133130, test/num_examples=43793, total_duration=983.535672, train/accuracy=0.987770, train/loss=0.044515, train/mean_average_precision=0.139094, validation/accuracy=0.984911, validation/loss=0.053807, validation/mean_average_precision=0.129450, validation/num_examples=43793
I0406 00:24:16.938766 140358684223296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/checkpoint_2801.
I0406 00:24:16.939257 140358684223296 submission_runner.py:416] After logging and checkpointing eval at step 2801: RAM USED (GB) 29.53805824
I0406 00:25:08.439699 140306741274368 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.028354, loss=0.042718
I0406 00:25:08.444596 140358684223296 submission.py:119] 3000) loss = 0.043, grad_norm = 0.028
I0406 00:27:18.139617 140306732881664 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.011877, loss=0.036972
I0406 00:27:18.144978 140358684223296 submission.py:119] 3500) loss = 0.037, grad_norm = 0.012
I0406 00:28:16.978024 140358684223296 submission_runner.py:373] Before eval at step 3726: RAM USED (GB) 29.808590848
I0406 00:28:16.978250 140358684223296 spec.py:298] Evaluating on the training split.
W0406 00:28:30.931629 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:31.173630 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:31.173663 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:31.180012 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:31.180593 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:31.180647 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:31.181423 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:31.181479 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:45.148346 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:45.385474 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:45.385553 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:45.391687 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:45.392171 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:45.392278 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:45.392823 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:45.393446 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:11.992557 140358684223296 spec.py:310] Evaluating on the validation split.
W0406 00:29:12.440025 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:12.694954 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:12.695502 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:12.700905 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:12.701007 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:12.701775 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:12.702303 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:12.702332 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:12.921309 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:13.174081 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:13.174721 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:13.179301 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:13.179418 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:13.180447 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:13.180559 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:13.188189 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:15.754570 140358684223296 spec.py:326] Evaluating on the test split.
W0406 00:29:16.190109 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.474148 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.474196 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.480298 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.480343 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.480922 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.481442 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.482084 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.667299 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.916003 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.916270 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.922206 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.922279 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.923166 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.930327 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:16.931838 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:19.444310 140358684223296 submission_runner.py:382] Time since start: 1285.72s, 	Step: 3726, 	{'train/accuracy': 0.9877439628076394, 'train/loss': 0.04278272017836571, 'train/mean_average_precision': 0.1790518632109357, 'validation/accuracy': 0.9850979169558994, 'validation/loss': 0.05199084430932999, 'validation/mean_average_precision': 0.15437229179407091, 'validation/num_examples': 43793, 'test/accuracy': 0.9841373869671414, 'test/loss': 0.05495284125208855, 'test/mean_average_precision': 0.15674559237726474, 'test/num_examples': 43793}
I0406 00:29:19.444679 140358684223296 submission_runner.py:396] After eval at step 3726: RAM USED (GB) 30.13234688
I0406 00:29:19.452789 140306741274368 logging_writer.py:48] [3726] global_step=3726, preemption_count=0, score=962.180358, test/accuracy=0.984137, test/loss=0.054953, test/mean_average_precision=0.156746, test/num_examples=43793, total_duration=1285.717462, train/accuracy=0.987744, train/loss=0.042783, train/mean_average_precision=0.179052, validation/accuracy=0.985098, validation/loss=0.051991, validation/mean_average_precision=0.154372, validation/num_examples=43793
I0406 00:29:19.545042 140358684223296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/checkpoint_3726.
I0406 00:29:19.545520 140358684223296 submission_runner.py:416] After logging and checkpointing eval at step 3726: RAM USED (GB) 30.131929088
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0406 00:30:31.468122 140306732881664 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.015151, loss=0.044606
I0406 00:30:31.472978 140358684223296 submission.py:119] 4000) loss = 0.045, grad_norm = 0.015
I0406 00:32:41.180525 140306741274368 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.025511, loss=0.047474
I0406 00:32:41.185450 140358684223296 submission.py:119] 4500) loss = 0.047, grad_norm = 0.026
I0406 00:33:19.670081 140358684223296 submission_runner.py:373] Before eval at step 4650: RAM USED (GB) 30.285983744
I0406 00:33:19.670297 140358684223296 spec.py:298] Evaluating on the training split.
W0406 00:33:33.821966 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:34.085784 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:34.086479 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:34.092485 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:34.092529 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:34.092622 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:34.092662 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:34.092875 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:48.175843 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:48.410908 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:48.411438 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:48.417976 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:48.418067 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:48.418162 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:48.418208 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:48.418856 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:34:15.824176 140358684223296 spec.py:310] Evaluating on the validation split.
W0406 00:34:16.282078 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.539022 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.539058 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.544790 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.545051 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.545224 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.546174 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.546521 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.735720 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.988551 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.988607 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.994938 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.994962 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.995317 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.995737 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:16.996269 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:34:19.533168 140358684223296 spec.py:326] Evaluating on the test split.
W0406 00:34:19.988780 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.239621 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.239717 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.245959 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.246176 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.246399 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.246666 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.246702 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.462377 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.714205 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.714635 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.720361 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.720729 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.721137 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.721546 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:20.722110 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:34:23.293235 140358684223296 submission_runner.py:382] Time since start: 1588.41s, 	Step: 4650, 	{'train/accuracy': 0.9882745535670353, 'train/loss': 0.040261946618556976, 'train/mean_average_precision': 0.2063263512882126, 'validation/accuracy': 0.985369897223934, 'validation/loss': 0.050096701830625534, 'validation/mean_average_precision': 0.17488532728330075, 'validation/num_examples': 43793, 'test/accuracy': 0.9845080382899665, 'test/loss': 0.052789684385061264, 'test/mean_average_precision': 0.1775171838866071, 'test/num_examples': 43793}
I0406 00:34:23.293597 140358684223296 submission_runner.py:396] After eval at step 4650: RAM USED (GB) 30.910083072
I0406 00:34:23.301740 140306732881664 logging_writer.py:48] [4650] global_step=4650, preemption_count=0, score=1201.347726, test/accuracy=0.984508, test/loss=0.052790, test/mean_average_precision=0.177517, test/num_examples=43793, total_duration=1588.409425, train/accuracy=0.988275, train/loss=0.040262, train/mean_average_precision=0.206326, validation/accuracy=0.985370, validation/loss=0.050097, validation/mean_average_precision=0.174885, validation/num_examples=43793
I0406 00:34:23.389916 140358684223296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/checkpoint_4650.
I0406 00:34:23.390425 140358684223296 submission_runner.py:416] After logging and checkpointing eval at step 4650: RAM USED (GB) 30.909632512
I0406 00:35:54.584109 140306741274368 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.015988, loss=0.045690
I0406 00:35:54.589074 140358684223296 submission.py:119] 5000) loss = 0.046, grad_norm = 0.016
I0406 00:38:04.757584 140306732881664 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.010128, loss=0.041912
I0406 00:38:04.762375 140358684223296 submission.py:119] 5500) loss = 0.042, grad_norm = 0.010
I0406 00:38:23.419365 140358684223296 submission_runner.py:373] Before eval at step 5574: RAM USED (GB) 31.060570112
I0406 00:38:23.419567 140358684223296 spec.py:298] Evaluating on the training split.
W0406 00:38:37.400318 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:37.649070 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:37.649601 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:37.654064 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:37.654558 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:37.655565 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:37.655656 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:37.656155 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:51.555778 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:51.793422 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:51.794254 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:51.799329 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:51.799870 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:51.800396 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:51.800613 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:51.800954 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:39:18.986118 140358684223296 spec.py:310] Evaluating on the validation split.
W0406 00:39:19.443588 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:19.702717 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:19.703039 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:19.708629 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:19.708798 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:19.709300 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:19.709742 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:19.710015 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:19.922297 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:20.170367 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:20.171850 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:20.176151 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:20.177031 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:20.177108 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:20.177158 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:20.177377 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:39:22.722865 140358684223296 spec.py:326] Evaluating on the test split.
W0406 00:39:23.180846 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.429419 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.430031 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.436474 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.436662 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.437208 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.437486 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.437982 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.665726 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.919039 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.919324 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.924211 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.925430 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.925842 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.926048 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:23.926231 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:39:26.436014 140358684223296 submission_runner.py:382] Time since start: 1892.16s, 	Step: 5574, 	{'train/accuracy': 0.9886140747139375, 'train/loss': 0.03887329623103142, 'train/mean_average_precision': 0.22097166149817377, 'validation/accuracy': 0.9856743527478532, 'validation/loss': 0.04840197414159775, 'validation/mean_average_precision': 0.18587410503625704, 'validation/num_examples': 43793, 'test/accuracy': 0.9849296541696799, 'test/loss': 0.050805650651454926, 'test/mean_average_precision': 0.19398854986462333, 'test/num_examples': 43793}
I0406 00:39:26.436408 140358684223296 submission_runner.py:396] After eval at step 5574: RAM USED (GB) 31.392894976
I0406 00:39:26.444849 140306741274368 logging_writer.py:48] [5574] global_step=5574, preemption_count=0, score=1440.374100, test/accuracy=0.984930, test/loss=0.050806, test/mean_average_precision=0.193989, test/num_examples=43793, total_duration=1892.158779, train/accuracy=0.988614, train/loss=0.038873, train/mean_average_precision=0.220972, validation/accuracy=0.985674, validation/loss=0.048402, validation/mean_average_precision=0.185874, validation/num_examples=43793
I0406 00:39:26.536437 140358684223296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/checkpoint_5574.
I0406 00:39:26.536956 140358684223296 submission_runner.py:416] After logging and checkpointing eval at step 5574: RAM USED (GB) 31.392219136
I0406 00:41:12.931129 140358684223296 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 31.45568256
I0406 00:41:12.931326 140358684223296 spec.py:298] Evaluating on the training split.
W0406 00:41:26.543934 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:26.786040 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:26.788234 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:26.793030 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:26.793114 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:26.793675 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:26.794132 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:26.794285 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:40.410828 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:40.646301 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:40.646676 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:40.652181 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:40.652362 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:40.652624 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:40.653329 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:41:40.653744 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:42:09.463597 140358684223296 spec.py:310] Evaluating on the validation split.
W0406 00:42:09.914177 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.169710 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.169724 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.174589 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.174629 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.176145 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.176685 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.176874 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.377178 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.627815 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.628812 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.633181 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.633966 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.634852 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.634911 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:10.635311 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:42:13.180219 140358684223296 spec.py:326] Evaluating on the test split.
W0406 00:42:13.628486 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:13.879808 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:13.879936 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:13.885501 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:13.885657 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:13.886529 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:13.886549 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:13.886986 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:14.087534 140358684223296 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:14.339148 140427817883456 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:14.339190 140462249899840 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:14.344844 139627604227904 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:14.344996 140639976159040 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:14.345173 139779768338240 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:14.345597 140460969416512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:42:14.346095 140097108432704 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:42:16.849441 140358684223296 submission_runner.py:382] Time since start: 2061.67s, 	Step: 6000, 	{'train/accuracy': 0.9886852858298626, 'train/loss': 0.0386279821395874, 'train/mean_average_precision': 0.2549988049495583, 'validation/accuracy': 0.9857405210817183, 'validation/loss': 0.048952315002679825, 'validation/mean_average_precision': 0.20127237794639588, 'validation/num_examples': 43793, 'test/accuracy': 0.9849334449218452, 'test/loss': 0.05165569856762886, 'test/mean_average_precision': 0.20648626261707362, 'test/num_examples': 43793}
I0406 00:42:16.849842 140358684223296 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 31.799185408
I0406 00:42:16.857922 140306732881664 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1546.319081, test/accuracy=0.984933, test/loss=0.051656, test/mean_average_precision=0.206486, test/num_examples=43793, total_duration=2061.670558, train/accuracy=0.988685, train/loss=0.038628, train/mean_average_precision=0.254999, validation/accuracy=0.985741, validation/loss=0.048952, validation/mean_average_precision=0.201272, validation/num_examples=43793
I0406 00:42:16.953838 140358684223296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0406 00:42:16.954315 140358684223296 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 31.798730752
I0406 00:42:16.961775 140306741274368 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1546.319081
I0406 00:42:17.119293 140358684223296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0406 00:42:17.295278 140358684223296 submission_runner.py:550] Tuning trial 1/1
I0406 00:42:17.295557 140358684223296 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0406 00:42:17.296902 140358684223296 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4724499803400492, 'train/loss': 0.7747153043746948, 'train/mean_average_precision': 0.02096404391248724, 'validation/accuracy': 0.4766811425119773, 'validation/loss': 0.7748718857765198, 'validation/mean_average_precision': 0.02467949256184609, 'validation/num_examples': 43793, 'test/accuracy': 0.476161433814099, 'test/loss': 0.7767800688743591, 'test/mean_average_precision': 0.02657538298230653, 'test/num_examples': 43793, 'score': 5.564859628677368, 'total_duration': 5.566702842712402, 'global_step': 1, 'preemption_count': 0}), (933, {'train/accuracy': 0.9866426927706996, 'train/loss': 0.05243820324540138, 'train/mean_average_precision': 0.049219148988067736, 'validation/accuracy': 0.9841650652306109, 'validation/loss': 0.061618585139513016, 'validation/mean_average_precision': 0.05068418577214737, 'validation/num_examples': 43793, 'test/accuracy': 0.9831905413151972, 'test/loss': 0.06497323513031006, 'test/mean_average_precision': 0.05203467172474047, 'test/num_examples': 43793, 'score': 244.69919419288635, 'total_duration': 385.7891368865967, 'global_step': 933, 'preemption_count': 0}), (1873, {'train/accuracy': 0.9873643147063328, 'train/loss': 0.047327518463134766, 'train/mean_average_precision': 0.09636129108719713, 'validation/accuracy': 0.984486164323171, 'validation/loss': 0.056699566543102264, 'validation/mean_average_precision': 0.09598750336735065, 'validation/num_examples': 43793, 'test/accuracy': 0.9834470488783796, 'test/loss': 0.05987212806940079, 'test/mean_average_precision': 0.09279715113643165, 'test/num_examples': 43793, 'score': 483.8917019367218, 'total_duration': 684.7206995487213, 'global_step': 1873, 'preemption_count': 0}), (2801, {'train/accuracy': 0.9877699330519366, 'train/loss': 0.04451543837785721, 'train/mean_average_precision': 0.1390937383151805, 'validation/accuracy': 0.9849111842345623, 'validation/loss': 0.05380653589963913, 'validation/mean_average_precision': 0.12944963932790865, 'validation/num_examples': 43793, 'test/accuracy': 0.9839263684299421, 'test/loss': 0.05675715208053589, 'test/mean_average_precision': 0.13312952175834564, 'test/num_examples': 43793, 'score': 723.0898008346558, 'total_duration': 983.5356721878052, 'global_step': 2801, 'preemption_count': 0}), (3726, {'train/accuracy': 0.9877439628076394, 'train/loss': 0.04278272017836571, 'train/mean_average_precision': 0.1790518632109357, 'validation/accuracy': 0.9850979169558994, 'validation/loss': 0.05199084430932999, 'validation/mean_average_precision': 0.15437229179407091, 'validation/num_examples': 43793, 'test/accuracy': 0.9841373869671414, 'test/loss': 0.05495284125208855, 'test/mean_average_precision': 0.15674559237726474, 'test/num_examples': 43793, 'score': 962.180358171463, 'total_duration': 1285.7174615859985, 'global_step': 3726, 'preemption_count': 0}), (4650, {'train/accuracy': 0.9882745535670353, 'train/loss': 0.040261946618556976, 'train/mean_average_precision': 0.2063263512882126, 'validation/accuracy': 0.985369897223934, 'validation/loss': 0.050096701830625534, 'validation/mean_average_precision': 0.17488532728330075, 'validation/num_examples': 43793, 'test/accuracy': 0.9845080382899665, 'test/loss': 0.052789684385061264, 'test/mean_average_precision': 0.1775171838866071, 'test/num_examples': 43793, 'score': 1201.3477256298065, 'total_duration': 1588.409425497055, 'global_step': 4650, 'preemption_count': 0}), (5574, {'train/accuracy': 0.9886140747139375, 'train/loss': 0.03887329623103142, 'train/mean_average_precision': 0.22097166149817377, 'validation/accuracy': 0.9856743527478532, 'validation/loss': 0.04840197414159775, 'validation/mean_average_precision': 0.18587410503625704, 'validation/num_examples': 43793, 'test/accuracy': 0.9849296541696799, 'test/loss': 0.050805650651454926, 'test/mean_average_precision': 0.19398854986462333, 'test/num_examples': 43793, 'score': 1440.374100446701, 'total_duration': 1892.1587793827057, 'global_step': 5574, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9886852858298626, 'train/loss': 0.0386279821395874, 'train/mean_average_precision': 0.2549988049495583, 'validation/accuracy': 0.9857405210817183, 'validation/loss': 0.048952315002679825, 'validation/mean_average_precision': 0.20127237794639588, 'validation/num_examples': 43793, 'test/accuracy': 0.9849334449218452, 'test/loss': 0.05165569856762886, 'test/mean_average_precision': 0.20648626261707362, 'test/num_examples': 43793, 'score': 1546.319081068039, 'total_duration': 2061.6705577373505, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0406 00:42:17.297040 140358684223296 submission_runner.py:553] Timing: 1546.319081068039
I0406 00:42:17.297086 140358684223296 submission_runner.py:554] ====================
I0406 00:42:17.297196 140358684223296 submission_runner.py:613] Final ogbg score: 1546.319081068039
