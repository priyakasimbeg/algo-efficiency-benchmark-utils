I0405 03:44:06.141923 140128593286976 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax.
I0405 03:44:06.198177 140128593286976 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0405 03:44:07.096514 140128593286976 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0405 03:44:07.097053 140128593286976 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0405 03:44:07.102252 140128593286976 submission_runner.py:511] Using RNG seed 3815836675
I0405 03:44:09.401868 140128593286976 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 03:44:09.402099 140128593286976 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1.
I0405 03:44:09.402313 140128593286976 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/hparams.json.
I0405 03:44:09.529512 140128593286976 submission_runner.py:230] Starting train once: RAM USED (GB) 4.353536
I0405 03:44:09.529671 140128593286976 submission_runner.py:231] Initializing dataset.
I0405 03:44:09.529830 140128593286976 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.353536
I0405 03:44:09.529884 140128593286976 submission_runner.py:240] Initializing model.
I0405 03:44:15.142130 140128593286976 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.781371904
I0405 03:44:15.142325 140128593286976 submission_runner.py:252] Initializing optimizer.
I0405 03:44:15.852779 140128593286976 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.782404096
I0405 03:44:15.852938 140128593286976 submission_runner.py:261] Initializing metrics bundle.
I0405 03:44:15.852985 140128593286976 submission_runner.py:276] Initializing checkpoint and logger.
I0405 03:44:15.853963 140128593286976 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0405 03:44:15.854266 140128593286976 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 03:44:15.854349 140128593286976 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 03:44:16.659297 140128593286976 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0405 03:44:16.660234 140128593286976 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/flags_0.json.
I0405 03:44:16.666840 140128593286976 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.779487744
I0405 03:44:16.667034 140128593286976 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.779487744
I0405 03:44:16.667096 140128593286976 submission_runner.py:313] Starting training loop.
I0405 03:44:16.866587 140128593286976 input_pipeline.py:20] Loading split = train-clean-100
I0405 03:44:16.898061 140128593286976 input_pipeline.py:20] Loading split = train-clean-360
I0405 03:44:17.227431 140128593286976 input_pipeline.py:20] Loading split = train-other-500
I0405 03:44:20.683815 140128593286976 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.883716096
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0405 03:45:16.597869 139952952178432 logging_writer.py:48] [0] global_step=0, grad_norm=107.77953338623047, loss=30.89560890197754
I0405 03:45:16.613867 140128593286976 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.207396352
I0405 03:45:16.614118 140128593286976 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.207396352
I0405 03:45:16.614199 140128593286976 spec.py:298] Evaluating on the training split.
I0405 03:45:16.716273 140128593286976 input_pipeline.py:20] Loading split = train-clean-100
I0405 03:45:16.743553 140128593286976 input_pipeline.py:20] Loading split = train-clean-360
I0405 03:45:17.042189 140128593286976 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0405 03:46:10.396717 140128593286976 spec.py:310] Evaluating on the validation split.
I0405 03:46:10.457063 140128593286976 input_pipeline.py:20] Loading split = dev-clean
I0405 03:46:10.461330 140128593286976 input_pipeline.py:20] Loading split = dev-other
I0405 03:46:50.198706 140128593286976 spec.py:326] Evaluating on the test split.
I0405 03:46:50.261844 140128593286976 input_pipeline.py:20] Loading split = test-clean
I0405 03:47:18.469058 140128593286976 submission_runner.py:382] Time since start: 59.95s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(30.186682, dtype=float32), 'train/wer': 1.3814318139224955, 'validation/ctc_loss': DeviceArray(28.317572, dtype=float32), 'validation/wer': 1.4861793167324335, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(28.475523, dtype=float32), 'test/wer': 1.5244652976661994, 'test/num_examples': 2472}
I0405 03:47:18.470334 140128593286976 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.334022656
I0405 03:47:18.484306 139949730952960 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=59.747523, test/ctc_loss=28.475522994995117, test/num_examples=2472, test/wer=1.524465, total_duration=59.947057, train/ctc_loss=30.186681747436523, train/wer=1.381432, validation/ctc_loss=28.31757164001465, validation/num_examples=5348, validation/wer=1.486179
I0405 03:47:18.716044 140128593286976 checkpoints.py:356] Saving checkpoint at step: 1
I0405 03:47:19.662010 140128593286976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_1
I0405 03:47:19.684313 140128593286976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_1.
I0405 03:47:19.701881 140128593286976 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.350091264
I0405 03:47:19.760676 140128593286976 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.350926848
I0405 03:47:34.191567 140128593286976 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.75305984
I0405 03:48:48.751032 139954462201600 logging_writer.py:48] [100] global_step=100, grad_norm=19.753490447998047, loss=15.413338661193848
I0405 03:50:04.315988 139954470594304 logging_writer.py:48] [200] global_step=200, grad_norm=6.954950332641602, loss=6.231975555419922
I0405 03:51:19.857412 139954462201600 logging_writer.py:48] [300] global_step=300, grad_norm=4.663226127624512, loss=6.031809329986572
I0405 03:52:35.383236 139954470594304 logging_writer.py:48] [400] global_step=400, grad_norm=3.6198346614837646, loss=5.981804370880127
I0405 03:53:50.918876 139954462201600 logging_writer.py:48] [500] global_step=500, grad_norm=3.329193115234375, loss=5.983444690704346
I0405 03:55:06.484333 139954470594304 logging_writer.py:48] [600] global_step=600, grad_norm=1.3760921955108643, loss=5.8611650466918945
I0405 03:56:22.070483 139954462201600 logging_writer.py:48] [700] global_step=700, grad_norm=2.000798225402832, loss=5.893649101257324
I0405 03:57:37.671433 139954470594304 logging_writer.py:48] [800] global_step=800, grad_norm=1.7496564388275146, loss=5.8972625732421875
I0405 03:58:58.062654 139954462201600 logging_writer.py:48] [900] global_step=900, grad_norm=1.158777117729187, loss=5.822293758392334
I0405 04:00:22.545338 139954470594304 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.6704505681991577, loss=5.877744674682617
I0405 04:01:44.770609 139955940775680 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.1598379909992218, loss=5.793135643005371
I0405 04:03:00.466220 139955932382976 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1763023138046265, loss=5.846560001373291
I0405 04:04:16.188158 139955940775680 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.063180685043335, loss=5.824339866638184
I0405 04:05:31.834176 139955932382976 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.327887773513794, loss=5.849888324737549
I0405 04:06:47.497560 139955940775680 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.4478170871734619, loss=5.806638240814209
I0405 04:08:05.812968 139955932382976 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5745248198509216, loss=5.817883014678955
I0405 04:09:32.326721 139955940775680 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8264502882957458, loss=5.801919460296631
I0405 04:10:56.004011 139955932382976 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5803016424179077, loss=5.782445430755615
I0405 04:12:21.731697 139955940775680 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.4362313449382782, loss=5.794243812561035
I0405 04:13:44.377415 139955932382976 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7144855260848999, loss=5.808578014373779
I0405 04:15:07.293374 139955285415680 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.2483261078596115, loss=5.7758049964904785
I0405 04:16:22.852596 139955277022976 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.4131586253643036, loss=5.790988445281982
I0405 04:17:38.514838 139955285415680 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.4171753525733948, loss=5.786974906921387
I0405 04:18:54.218353 139955277022976 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.31621330976486206, loss=5.776793479919434
I0405 04:20:09.803338 139955285415680 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5431392192840576, loss=5.792791843414307
I0405 04:21:25.466456 139955277022976 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.12108893692493439, loss=5.794005393981934
I0405 04:22:46.877972 139955285415680 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.11977120488882065, loss=5.775121688842773
I0405 04:24:10.072295 139955277022976 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.12025711685419083, loss=5.773818016052246
I0405 04:25:33.332416 139955285415680 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.29377833008766174, loss=5.7897186279296875
I0405 04:26:57.165371 139955277022976 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.40893757343292236, loss=5.789229869842529
I0405 04:27:19.925974 140128593286976 submission_runner.py:373] Before eval at step 3030: RAM USED (GB) 23.475429376
I0405 04:27:19.926229 140128593286976 spec.py:298] Evaluating on the training split.
I0405 04:27:46.826549 140128593286976 spec.py:310] Evaluating on the validation split.
I0405 04:28:22.349869 140128593286976 spec.py:326] Evaluating on the test split.
I0405 04:28:40.701816 140128593286976 submission_runner.py:382] Time since start: 2583.26s, 	Step: 3030, 	{'train/ctc_loss': DeviceArray(5.9557576, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(5.9979086, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.983325, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 04:28:40.703022 140128593286976 submission_runner.py:396] After eval at step 3030: RAM USED (GB) 20.666720256
I0405 04:28:40.721071 139955285415680 logging_writer.py:48] [3030] global_step=3030, preemption_count=0, score=2454.049626, test/ctc_loss=5.983325004577637, test/num_examples=2472, test/wer=0.899580, total_duration=2583.256927, train/ctc_loss=5.9557576179504395, train/wer=0.944636, validation/ctc_loss=5.997908592224121, validation/num_examples=5348, validation/wer=0.895995
I0405 04:28:40.945674 140128593286976 checkpoints.py:356] Saving checkpoint at step: 3030
I0405 04:28:41.906365 140128593286976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_3030
I0405 04:28:41.927819 140128593286976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_3030.
I0405 04:28:41.942344 140128593286976 submission_runner.py:416] After logging and checkpointing eval at step 3030: RAM USED (GB) 20.682682368
I0405 04:29:39.209340 139955285415680 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.06740561872720718, loss=5.762140274047852
I0405 04:30:54.668130 139955277022976 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.08602035045623779, loss=5.787695407867432
I0405 04:32:10.161387 139955285415680 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.44711148738861084, loss=5.811673641204834
I0405 04:33:25.772032 139955277022976 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.1646382212638855, loss=5.788620948791504
I0405 04:34:41.511822 139955285415680 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.33412227034568787, loss=5.789917469024658
I0405 04:35:57.188444 139955277022976 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.16304999589920044, loss=5.807723522186279
I0405 04:37:15.165570 139955285415680 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0784611776471138, loss=5.7981462478637695
I0405 04:38:36.562294 139955277022976 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.2624356150627136, loss=5.813571453094482
I0405 04:39:56.228786 139955285415680 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.08198032528162003, loss=5.779496192932129
I0405 04:41:15.345473 139955277022976 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.13722403347492218, loss=5.783595561981201
I0405 04:42:38.119565 139955285415680 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.30018800497055054, loss=5.792450904846191
I0405 04:43:58.299196 139954630055680 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.3152141571044922, loss=5.789631366729736
I0405 04:45:11.997560 139954621662976 logging_writer.py:48] [4300] global_step=4300, grad_norm=nan, loss=nan
I0405 04:46:24.808949 139954630055680 logging_writer.py:48] [4400] global_step=4400, grad_norm=nan, loss=nan
I0405 04:47:37.553082 139954621662976 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0405 04:48:50.314804 139954630055680 logging_writer.py:48] [4600] global_step=4600, grad_norm=nan, loss=nan
I0405 04:50:09.962173 139954621662976 logging_writer.py:48] [4700] global_step=4700, grad_norm=nan, loss=nan
I0405 04:51:28.571493 139954630055680 logging_writer.py:48] [4800] global_step=4800, grad_norm=nan, loss=nan
I0405 04:52:48.535910 139954621662976 logging_writer.py:48] [4900] global_step=4900, grad_norm=nan, loss=nan
I0405 04:54:09.276011 139954630055680 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0405 04:55:28.988700 139954621662976 logging_writer.py:48] [5100] global_step=5100, grad_norm=nan, loss=nan
I0405 04:56:49.930765 139955285415680 logging_writer.py:48] [5200] global_step=5200, grad_norm=nan, loss=nan
I0405 04:58:02.559604 139955277022976 logging_writer.py:48] [5300] global_step=5300, grad_norm=nan, loss=nan
I0405 04:59:15.105765 139955285415680 logging_writer.py:48] [5400] global_step=5400, grad_norm=nan, loss=nan
I0405 05:00:27.678011 139955277022976 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0405 05:01:44.081950 139955285415680 logging_writer.py:48] [5600] global_step=5600, grad_norm=nan, loss=nan
I0405 05:03:08.505693 139955277022976 logging_writer.py:48] [5700] global_step=5700, grad_norm=nan, loss=nan
I0405 05:04:30.067606 139955285415680 logging_writer.py:48] [5800] global_step=5800, grad_norm=nan, loss=nan
I0405 05:05:55.686408 139955277022976 logging_writer.py:48] [5900] global_step=5900, grad_norm=nan, loss=nan
I0405 05:07:22.339869 139955285415680 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0405 05:08:42.799233 140128593286976 submission_runner.py:373] Before eval at step 6096: RAM USED (GB) 22.147444736
I0405 05:08:42.799455 140128593286976 spec.py:298] Evaluating on the training split.
I0405 05:09:09.730352 140128593286976 spec.py:310] Evaluating on the validation split.
I0405 05:09:45.455713 140128593286976 spec.py:326] Evaluating on the test split.
I0405 05:10:04.582641 140128593286976 submission_runner.py:382] Time since start: 5066.13s, 	Step: 6096, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 05:10:04.584156 140128593286976 submission_runner.py:396] After eval at step 6096: RAM USED (GB) 21.035270144
I0405 05:10:04.604037 139955357095680 logging_writer.py:48] [6096] global_step=6096, preemption_count=0, score=4848.925942, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=5066.127388, train/ctc_loss=nan, train/wer=0.942722, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0405 05:10:04.862044 140128593286976 checkpoints.py:356] Saving checkpoint at step: 6096
I0405 05:10:05.962300 140128593286976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_6096
I0405 05:10:05.984709 140128593286976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_6096.
I0405 05:10:06.001290 140128593286976 submission_runner.py:416] After logging and checkpointing eval at step 6096: RAM USED (GB) 21.044228096
I0405 05:10:09.698566 139955348702976 logging_writer.py:48] [6100] global_step=6100, grad_norm=nan, loss=nan
I0405 05:11:26.453354 139955357095680 logging_writer.py:48] [6200] global_step=6200, grad_norm=nan, loss=nan
I0405 05:12:39.019257 139955348702976 logging_writer.py:48] [6300] global_step=6300, grad_norm=nan, loss=nan
I0405 05:13:52.044077 139955357095680 logging_writer.py:48] [6400] global_step=6400, grad_norm=nan, loss=nan
I0405 05:15:04.677884 139955348702976 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0405 05:16:24.067699 139955357095680 logging_writer.py:48] [6600] global_step=6600, grad_norm=nan, loss=nan
I0405 05:17:43.418337 139955348702976 logging_writer.py:48] [6700] global_step=6700, grad_norm=nan, loss=nan
I0405 05:19:04.811168 139955357095680 logging_writer.py:48] [6800] global_step=6800, grad_norm=nan, loss=nan
I0405 05:20:28.664601 139955348702976 logging_writer.py:48] [6900] global_step=6900, grad_norm=nan, loss=nan
I0405 05:21:49.053114 139955357095680 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0405 05:23:09.259156 139955348702976 logging_writer.py:48] [7100] global_step=7100, grad_norm=nan, loss=nan
I0405 05:24:28.123288 139955357095680 logging_writer.py:48] [7200] global_step=7200, grad_norm=nan, loss=nan
I0405 05:25:45.816082 139955357095680 logging_writer.py:48] [7300] global_step=7300, grad_norm=nan, loss=nan
I0405 05:26:58.411972 139955348702976 logging_writer.py:48] [7400] global_step=7400, grad_norm=nan, loss=nan
I0405 05:28:10.963953 139955357095680 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0405 05:29:25.073539 139955348702976 logging_writer.py:48] [7600] global_step=7600, grad_norm=nan, loss=nan
I0405 05:30:45.405524 139955357095680 logging_writer.py:48] [7700] global_step=7700, grad_norm=nan, loss=nan
I0405 05:32:07.729553 139955348702976 logging_writer.py:48] [7800] global_step=7800, grad_norm=nan, loss=nan
I0405 05:33:29.181230 139955357095680 logging_writer.py:48] [7900] global_step=7900, grad_norm=nan, loss=nan
I0405 05:34:51.459035 139955348702976 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0405 05:36:12.381141 139955357095680 logging_writer.py:48] [8100] global_step=8100, grad_norm=nan, loss=nan
I0405 05:37:30.642140 139955348702976 logging_writer.py:48] [8200] global_step=8200, grad_norm=nan, loss=nan
I0405 05:38:49.118496 139955357095680 logging_writer.py:48] [8300] global_step=8300, grad_norm=nan, loss=nan
I0405 05:40:01.701442 139955348702976 logging_writer.py:48] [8400] global_step=8400, grad_norm=nan, loss=nan
I0405 05:41:14.443751 139955357095680 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0405 05:42:27.234993 139955348702976 logging_writer.py:48] [8600] global_step=8600, grad_norm=nan, loss=nan
I0405 05:43:41.022501 139955357095680 logging_writer.py:48] [8700] global_step=8700, grad_norm=nan, loss=nan
I0405 05:45:05.996213 139955348702976 logging_writer.py:48] [8800] global_step=8800, grad_norm=nan, loss=nan
I0405 05:46:31.052896 139955357095680 logging_writer.py:48] [8900] global_step=8900, grad_norm=nan, loss=nan
I0405 05:47:54.594107 139955348702976 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0405 05:49:17.215997 139955357095680 logging_writer.py:48] [9100] global_step=9100, grad_norm=nan, loss=nan
I0405 05:50:06.423545 140128593286976 submission_runner.py:373] Before eval at step 9162: RAM USED (GB) 22.397923328
I0405 05:50:06.423796 140128593286976 spec.py:298] Evaluating on the training split.
I0405 05:50:33.536906 140128593286976 spec.py:310] Evaluating on the validation split.
I0405 05:51:08.694346 140128593286976 spec.py:326] Evaluating on the test split.
I0405 05:51:27.727891 140128593286976 submission_runner.py:382] Time since start: 7549.75s, 	Step: 9162, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 05:51:27.729206 140128593286976 submission_runner.py:396] After eval at step 9162: RAM USED (GB) 21.528449024
I0405 05:51:27.748730 139955940775680 logging_writer.py:48] [9162] global_step=9162, preemption_count=0, score=7243.360863, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7549.753175, train/ctc_loss=nan, train/wer=0.943324, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0405 05:51:27.964881 140128593286976 checkpoints.py:356] Saving checkpoint at step: 9162
I0405 05:51:28.932178 140128593286976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_9162
I0405 05:51:28.953795 140128593286976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_9162.
I0405 05:51:28.967651 140128593286976 submission_runner.py:416] After logging and checkpointing eval at step 9162: RAM USED (GB) 21.537890304
I0405 05:51:57.325325 139955932382976 logging_writer.py:48] [9200] global_step=9200, grad_norm=nan, loss=nan
I0405 05:53:13.609580 139955070375680 logging_writer.py:48] [9300] global_step=9300, grad_norm=nan, loss=nan
I0405 05:54:26.407378 139955061982976 logging_writer.py:48] [9400] global_step=9400, grad_norm=nan, loss=nan
I0405 05:55:39.119908 139955070375680 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0405 05:56:51.823440 139955061982976 logging_writer.py:48] [9600] global_step=9600, grad_norm=nan, loss=nan
I0405 05:58:12.003137 139955070375680 logging_writer.py:48] [9700] global_step=9700, grad_norm=nan, loss=nan
I0405 05:59:33.850946 139955061982976 logging_writer.py:48] [9800] global_step=9800, grad_norm=nan, loss=nan
I0405 06:00:52.951243 139955070375680 logging_writer.py:48] [9900] global_step=9900, grad_norm=nan, loss=nan
I0405 06:02:16.392742 140128593286976 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 23.21276928
I0405 06:02:16.392971 140128593286976 spec.py:298] Evaluating on the training split.
I0405 06:02:42.907745 140128593286976 spec.py:310] Evaluating on the validation split.
I0405 06:03:19.907231 140128593286976 spec.py:326] Evaluating on the test split.
I0405 06:03:39.229890 140128593286976 submission_runner.py:382] Time since start: 8279.72s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 06:03:39.231355 140128593286976 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 21.44714752
I0405 06:03:39.249622 139955070375680 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7889.169927, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=8279.723221, train/ctc_loss=nan, train/wer=0.943700, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0405 06:03:39.479162 140128593286976 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 06:03:40.557947 140128593286976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_10000
I0405 06:03:40.579138 140128593286976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0405 06:03:40.591538 140128593286976 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.457186816
I0405 06:03:40.599182 139955061982976 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7889.169927
I0405 06:03:40.795859 140128593286976 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 06:03:42.120584 140128593286976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_10000
I0405 06:03:42.141917 140128593286976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0405 06:03:43.365962 140128593286976 submission_runner.py:550] Tuning trial 1/1
I0405 06:03:43.366268 140128593286976 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 06:03:43.371748 140128593286976 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(30.186682, dtype=float32), 'train/wer': 1.3814318139224955, 'validation/ctc_loss': DeviceArray(28.317572, dtype=float32), 'validation/wer': 1.4861793167324335, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(28.475523, dtype=float32), 'test/wer': 1.5244652976661994, 'test/num_examples': 2472, 'score': 59.74752330780029, 'total_duration': 59.94705677032471, 'global_step': 1, 'preemption_count': 0}), (3030, {'train/ctc_loss': DeviceArray(5.9557576, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(5.9979086, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.983325, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2454.0496261119843, 'total_duration': 2583.2569274902344, 'global_step': 3030, 'preemption_count': 0}), (6096, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4848.925942182541, 'total_duration': 5066.127388477325, 'global_step': 6096, 'preemption_count': 0}), (9162, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7243.360862970352, 'total_duration': 7549.7531752586365, 'global_step': 9162, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7889.169927358627, 'total_duration': 8279.723220825195, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0405 06:03:43.371944 140128593286976 submission_runner.py:553] Timing: 7889.169927358627
I0405 06:03:43.372034 140128593286976 submission_runner.py:554] ====================
I0405 06:03:43.372491 140128593286976 submission_runner.py:613] Final librispeech_conformer score: 7889.169927358627
