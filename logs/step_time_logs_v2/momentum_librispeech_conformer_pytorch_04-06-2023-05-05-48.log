WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 05:06:10.568412 140074170386240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 05:06:10.568479 140133330257728 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 05:06:10.568557 140324046198592 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 05:06:10.569680 140498667276096 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 05:06:10.569703 140484948686656 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 05:06:10.569742 140400993146688 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 05:06:10.570300 140611868370752 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 05:06:10.579598 140017080612672 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 05:06:10.579878 140017080612672 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:10.580411 140484948686656 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:10.580393 140498667276096 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:10.580440 140400993146688 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:10.580905 140611868370752 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:10.589460 140074170386240 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:10.589493 140133330257728 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:10.589475 140324046198592 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:11.102516 140017080612672 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_momentum/librispeech_conformer_pytorch.
W0406 05:06:11.203854 140324046198592 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:11.203860 140498667276096 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:11.203883 140484948686656 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:11.204728 140133330257728 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:11.205348 140017080612672 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:11.206164 140400993146688 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:11.206877 140074170386240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:11.206988 140611868370752 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 05:06:11.209602 140017080612672 submission_runner.py:511] Using RNG seed 1915663619
I0406 05:06:11.210670 140017080612672 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 05:06:11.210793 140017080612672 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_momentum/librispeech_conformer_pytorch/trial_1.
I0406 05:06:11.210971 140017080612672 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_momentum/librispeech_conformer_pytorch/trial_1/hparams.json.
I0406 05:06:11.211894 140017080612672 submission_runner.py:230] Starting train once: RAM USED (GB) 5.74949376
I0406 05:06:11.211990 140017080612672 submission_runner.py:231] Initializing dataset.
I0406 05:06:11.212084 140017080612672 input_pipeline.py:20] Loading split = train-clean-100
I0406 05:06:11.239454 140017080612672 input_pipeline.py:20] Loading split = train-clean-360
I0406 05:06:11.566469 140017080612672 input_pipeline.py:20] Loading split = train-other-500
I0406 05:06:11.990181 140017080612672 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 6.11446784
I0406 05:06:11.990359 140017080612672 submission_runner.py:240] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0406 05:06:18.921678 140017080612672 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.393929216
I0406 05:06:18.921865 140017080612672 submission_runner.py:252] Initializing optimizer.
I0406 05:06:19.501880 140017080612672 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.204513792
I0406 05:06:19.502057 140017080612672 submission_runner.py:261] Initializing metrics bundle.
I0406 05:06:19.502113 140017080612672 submission_runner.py:276] Initializing checkpoint and logger.
I0406 05:06:19.503806 140017080612672 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 05:06:19.503934 140017080612672 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 05:06:20.253869 140017080612672 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_momentum/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0406 05:06:20.254888 140017080612672 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_momentum/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0406 05:06:20.259917 140017080612672 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 19.209445376
I0406 05:06:20.260969 140017080612672 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.209445376
I0406 05:06:20.261092 140017080612672 submission_runner.py:313] Starting training loop.
I0406 05:06:22.049191 140017080612672 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 24.9695232
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0406 05:06:26.848756 139990734468864 logging_writer.py:48] [0] global_step=0, grad_norm=43.336250, loss=32.271145
I0406 05:06:26.863518 140017080612672 submission.py:139] 0) loss = 32.271, grad_norm = 43.336
I0406 05:06:26.864393 140017080612672 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 32.69279744
I0406 05:06:26.865119 140017080612672 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 32.693313536
I0406 05:06:26.865263 140017080612672 spec.py:298] Evaluating on the training split.
I0406 05:06:26.866105 140017080612672 input_pipeline.py:20] Loading split = train-clean-100
I0406 05:06:26.893661 140017080612672 input_pipeline.py:20] Loading split = train-clean-360
I0406 05:06:27.323889 140017080612672 input_pipeline.py:20] Loading split = train-other-500
I0406 05:06:39.417648 140017080612672 spec.py:310] Evaluating on the validation split.
I0406 05:06:39.418908 140017080612672 input_pipeline.py:20] Loading split = dev-clean
I0406 05:06:39.422875 140017080612672 input_pipeline.py:20] Loading split = dev-other
I0406 05:06:49.166107 140017080612672 spec.py:326] Evaluating on the test split.
I0406 05:06:49.174567 140017080612672 input_pipeline.py:20] Loading split = test-clean
I0406 05:06:54.340883 140017080612672 submission_runner.py:382] Time since start: 6.60s, 	Step: 1, 	{'train/ctc_loss': 31.49538697494817, 'train/wer': 1.0354120642020073, 'validation/ctc_loss': 30.06022704440426, 'validation/wer': 1.1719692946458746, 'validation/num_examples': 5348, 'test/ctc_loss': 30.196728821214272, 'test/wer': 1.176081083825889, 'test/num_examples': 2472}
I0406 05:06:54.341626 140017080612672 submission_runner.py:396] After eval at step 1: RAM USED (GB) 45.593186304
I0406 05:06:54.356232 139988444374784 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.602487, test/ctc_loss=30.196729, test/num_examples=2472, test/wer=1.176081, total_duration=6.604502, train/ctc_loss=31.495387, train/wer=1.035412, validation/ctc_loss=30.060227, validation/num_examples=5348, validation/wer=1.171969
I0406 05:06:54.744259 140017080612672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_1.
I0406 05:06:54.744857 140017080612672 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 45.622448128
I0406 05:06:54.749277 140017080612672 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 45.62712576
I0406 05:06:54.791343 140017080612672 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:06:54.791395 140498667276096 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:06:54.791395 140484948686656 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:06:54.791495 140074170386240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:06:54.792130 140133330257728 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:06:54.792135 140400993146688 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:06:54.792170 140324046198592 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:06:54.793020 140611868370752 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:06:55.870041 139988435982080 logging_writer.py:48] [1] global_step=1, grad_norm=41.788849, loss=31.484390
I0406 05:06:55.873166 140017080612672 submission.py:139] 1) loss = 31.484, grad_norm = 41.789
I0406 05:06:55.873856 140017080612672 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 45.8131456
I0406 05:06:56.757602 139988444374784 logging_writer.py:48] [2] global_step=2, grad_norm=49.648579, loss=31.964439
I0406 05:06:56.760681 140017080612672 submission.py:139] 2) loss = 31.964, grad_norm = 49.649
I0406 05:06:57.768748 139988435982080 logging_writer.py:48] [3] global_step=3, grad_norm=66.170410, loss=31.017586
I0406 05:06:57.772301 140017080612672 submission.py:139] 3) loss = 31.018, grad_norm = 66.170
I0406 05:06:58.572349 139988444374784 logging_writer.py:48] [4] global_step=4, grad_norm=118.443413, loss=27.608620
I0406 05:06:58.575582 140017080612672 submission.py:139] 4) loss = 27.609, grad_norm = 118.443
I0406 05:06:59.379926 139988435982080 logging_writer.py:48] [5] global_step=5, grad_norm=153.757156, loss=18.064016
I0406 05:06:59.382927 140017080612672 submission.py:139] 5) loss = 18.064, grad_norm = 153.757
I0406 05:07:00.185301 139988444374784 logging_writer.py:48] [6] global_step=6, grad_norm=17.595482, loss=7.373512
I0406 05:07:00.188421 140017080612672 submission.py:139] 6) loss = 7.374, grad_norm = 17.595
I0406 05:07:00.986693 139988435982080 logging_writer.py:48] [7] global_step=7, grad_norm=25.724400, loss=8.865044
I0406 05:07:00.989865 140017080612672 submission.py:139] 7) loss = 8.865, grad_norm = 25.724
I0406 05:07:01.791067 139988444374784 logging_writer.py:48] [8] global_step=8, grad_norm=27.725496, loss=10.713855
I0406 05:07:01.794458 140017080612672 submission.py:139] 8) loss = 10.714, grad_norm = 27.725
I0406 05:07:02.591558 139988435982080 logging_writer.py:48] [9] global_step=9, grad_norm=29.207218, loss=11.532922
I0406 05:07:02.594596 140017080612672 submission.py:139] 9) loss = 11.533, grad_norm = 29.207
I0406 05:07:03.393812 139988444374784 logging_writer.py:48] [10] global_step=10, grad_norm=31.131678, loss=11.381676
I0406 05:07:03.396852 140017080612672 submission.py:139] 10) loss = 11.382, grad_norm = 31.132
I0406 05:07:04.195498 139988435982080 logging_writer.py:48] [11] global_step=11, grad_norm=33.161095, loss=10.302157
I0406 05:07:04.198678 140017080612672 submission.py:139] 11) loss = 10.302, grad_norm = 33.161
I0406 05:07:05.000686 139988444374784 logging_writer.py:48] [12] global_step=12, grad_norm=30.950808, loss=8.248370
I0406 05:07:05.003753 140017080612672 submission.py:139] 12) loss = 8.248, grad_norm = 30.951
I0406 05:07:05.803914 139988435982080 logging_writer.py:48] [13] global_step=13, grad_norm=56.428066, loss=7.961381
I0406 05:07:05.807125 140017080612672 submission.py:139] 13) loss = 7.961, grad_norm = 56.428
I0406 05:07:06.609255 139988444374784 logging_writer.py:48] [14] global_step=14, grad_norm=135.580429, loss=11.670925
I0406 05:07:06.612402 140017080612672 submission.py:139] 14) loss = 11.671, grad_norm = 135.580
I0406 05:07:07.409653 139988435982080 logging_writer.py:48] [15] global_step=15, grad_norm=17.066912, loss=7.241915
I0406 05:07:07.412920 140017080612672 submission.py:139] 15) loss = 7.242, grad_norm = 17.067
I0406 05:07:08.209236 139988444374784 logging_writer.py:48] [16] global_step=16, grad_norm=33.037407, loss=9.201722
I0406 05:07:08.212456 140017080612672 submission.py:139] 16) loss = 9.202, grad_norm = 33.037
I0406 05:07:09.005846 139988435982080 logging_writer.py:48] [17] global_step=17, grad_norm=32.570110, loss=10.232876
I0406 05:07:09.009044 140017080612672 submission.py:139] 17) loss = 10.233, grad_norm = 32.570
I0406 05:07:09.810030 139988444374784 logging_writer.py:48] [18] global_step=18, grad_norm=31.831472, loss=10.007571
I0406 05:07:09.813210 140017080612672 submission.py:139] 18) loss = 10.008, grad_norm = 31.831
I0406 05:07:10.615303 139988435982080 logging_writer.py:48] [19] global_step=19, grad_norm=29.882933, loss=8.617332
I0406 05:07:10.618482 140017080612672 submission.py:139] 19) loss = 8.617, grad_norm = 29.883
I0406 05:07:11.415871 139988444374784 logging_writer.py:48] [20] global_step=20, grad_norm=3.610703, loss=6.873813
I0406 05:07:11.418993 140017080612672 submission.py:139] 20) loss = 6.874, grad_norm = 3.611
I0406 05:07:12.223609 139988435982080 logging_writer.py:48] [21] global_step=21, grad_norm=101.403755, loss=10.652495
I0406 05:07:12.226703 140017080612672 submission.py:139] 21) loss = 10.652, grad_norm = 101.404
I0406 05:07:13.026581 139988444374784 logging_writer.py:48] [22] global_step=22, grad_norm=21.161673, loss=7.006314
I0406 05:07:13.030174 140017080612672 submission.py:139] 22) loss = 7.006, grad_norm = 21.162
I0406 05:07:13.831577 139988435982080 logging_writer.py:48] [23] global_step=23, grad_norm=26.071009, loss=7.871341
I0406 05:07:13.834739 140017080612672 submission.py:139] 23) loss = 7.871, grad_norm = 26.071
I0406 05:07:14.632585 139988444374784 logging_writer.py:48] [24] global_step=24, grad_norm=28.650293, loss=8.958067
I0406 05:07:14.635818 140017080612672 submission.py:139] 24) loss = 8.958, grad_norm = 28.650
I0406 05:07:15.431944 139988435982080 logging_writer.py:48] [25] global_step=25, grad_norm=28.049202, loss=8.762564
I0406 05:07:15.435082 140017080612672 submission.py:139] 25) loss = 8.763, grad_norm = 28.049
I0406 05:07:16.232004 139988444374784 logging_writer.py:48] [26] global_step=26, grad_norm=22.341614, loss=7.351794
I0406 05:07:16.235203 140017080612672 submission.py:139] 26) loss = 7.352, grad_norm = 22.342
I0406 05:07:17.035471 139988435982080 logging_writer.py:48] [27] global_step=27, grad_norm=35.235817, loss=7.211945
I0406 05:07:17.038688 140017080612672 submission.py:139] 27) loss = 7.212, grad_norm = 35.236
I0406 05:07:17.838548 139988444374784 logging_writer.py:48] [28] global_step=28, grad_norm=62.682316, loss=8.335814
I0406 05:07:17.841767 140017080612672 submission.py:139] 28) loss = 8.336, grad_norm = 62.682
I0406 05:07:18.639205 139988435982080 logging_writer.py:48] [29] global_step=29, grad_norm=15.601748, loss=6.806870
I0406 05:07:18.642452 140017080612672 submission.py:139] 29) loss = 6.807, grad_norm = 15.602
I0406 05:07:19.443151 139988444374784 logging_writer.py:48] [30] global_step=30, grad_norm=25.780561, loss=8.029463
I0406 05:07:19.446262 140017080612672 submission.py:139] 30) loss = 8.029, grad_norm = 25.781
I0406 05:07:20.248248 139988435982080 logging_writer.py:48] [31] global_step=31, grad_norm=25.814043, loss=8.056550
I0406 05:07:20.251523 140017080612672 submission.py:139] 31) loss = 8.057, grad_norm = 25.814
I0406 05:07:21.052458 139988444374784 logging_writer.py:48] [32] global_step=32, grad_norm=18.387829, loss=6.902411
I0406 05:07:21.055891 140017080612672 submission.py:139] 32) loss = 6.902, grad_norm = 18.388
I0406 05:07:21.854125 139988435982080 logging_writer.py:48] [33] global_step=33, grad_norm=41.609325, loss=7.317129
I0406 05:07:21.857370 140017080612672 submission.py:139] 33) loss = 7.317, grad_norm = 41.609
I0406 05:07:22.655555 139988444374784 logging_writer.py:48] [34] global_step=34, grad_norm=33.773144, loss=6.983745
I0406 05:07:22.658602 140017080612672 submission.py:139] 34) loss = 6.984, grad_norm = 33.773
I0406 05:07:23.455819 139988435982080 logging_writer.py:48] [35] global_step=35, grad_norm=18.206738, loss=6.793135
I0406 05:07:23.459051 140017080612672 submission.py:139] 35) loss = 6.793, grad_norm = 18.207
I0406 05:07:24.255843 139988444374784 logging_writer.py:48] [36] global_step=36, grad_norm=23.996675, loss=7.555990
I0406 05:07:24.258871 140017080612672 submission.py:139] 36) loss = 7.556, grad_norm = 23.997
I0406 05:07:25.061518 139988435982080 logging_writer.py:48] [37] global_step=37, grad_norm=20.783588, loss=7.013273
I0406 05:07:25.065315 140017080612672 submission.py:139] 37) loss = 7.013, grad_norm = 20.784
I0406 05:07:25.864150 139988444374784 logging_writer.py:48] [38] global_step=38, grad_norm=13.052375, loss=6.436419
I0406 05:07:25.867548 140017080612672 submission.py:139] 38) loss = 6.436, grad_norm = 13.052
I0406 05:07:26.671572 139988435982080 logging_writer.py:48] [39] global_step=39, grad_norm=49.872093, loss=7.586030
I0406 05:07:26.674933 140017080612672 submission.py:139] 39) loss = 7.586, grad_norm = 49.872
I0406 05:07:27.473328 139988444374784 logging_writer.py:48] [40] global_step=40, grad_norm=15.992025, loss=6.594631
I0406 05:07:27.476308 140017080612672 submission.py:139] 40) loss = 6.595, grad_norm = 15.992
I0406 05:07:28.275461 139988435982080 logging_writer.py:48] [41] global_step=41, grad_norm=23.787075, loss=7.517969
I0406 05:07:28.278549 140017080612672 submission.py:139] 41) loss = 7.518, grad_norm = 23.787
I0406 05:07:29.076719 139988444374784 logging_writer.py:48] [42] global_step=42, grad_norm=21.081896, loss=7.016737
I0406 05:07:29.080044 140017080612672 submission.py:139] 42) loss = 7.017, grad_norm = 21.082
I0406 05:07:29.873717 139988435982080 logging_writer.py:48] [43] global_step=43, grad_norm=12.221616, loss=6.336791
I0406 05:07:29.877186 140017080612672 submission.py:139] 43) loss = 6.337, grad_norm = 12.222
I0406 05:07:30.673399 139988444374784 logging_writer.py:48] [44] global_step=44, grad_norm=50.147217, loss=7.540057
I0406 05:07:30.676588 140017080612672 submission.py:139] 44) loss = 7.540, grad_norm = 50.147
I0406 05:07:31.477299 139988435982080 logging_writer.py:48] [45] global_step=45, grad_norm=18.574242, loss=6.717640
I0406 05:07:31.480641 140017080612672 submission.py:139] 45) loss = 6.718, grad_norm = 18.574
I0406 05:07:32.282776 139988444374784 logging_writer.py:48] [46] global_step=46, grad_norm=24.284458, loss=7.701718
I0406 05:07:32.285924 140017080612672 submission.py:139] 46) loss = 7.702, grad_norm = 24.284
I0406 05:07:33.086523 139988435982080 logging_writer.py:48] [47] global_step=47, grad_norm=21.163345, loss=6.989994
I0406 05:07:33.089534 140017080612672 submission.py:139] 47) loss = 6.990, grad_norm = 21.163
I0406 05:07:33.891488 139988444374784 logging_writer.py:48] [48] global_step=48, grad_norm=22.465302, loss=6.474988
I0406 05:07:33.894751 140017080612672 submission.py:139] 48) loss = 6.475, grad_norm = 22.465
I0406 05:07:34.695245 139988435982080 logging_writer.py:48] [49] global_step=49, grad_norm=39.470154, loss=7.004912
I0406 05:07:34.698414 140017080612672 submission.py:139] 49) loss = 7.005, grad_norm = 39.470
I0406 05:07:35.500527 139988444374784 logging_writer.py:48] [50] global_step=50, grad_norm=20.371664, loss=6.886652
I0406 05:07:35.503793 140017080612672 submission.py:139] 50) loss = 6.887, grad_norm = 20.372
I0406 05:07:36.302649 139988435982080 logging_writer.py:48] [51] global_step=51, grad_norm=23.974352, loss=7.681071
I0406 05:07:36.305834 140017080612672 submission.py:139] 51) loss = 7.681, grad_norm = 23.974
I0406 05:07:37.102986 139988444374784 logging_writer.py:48] [52] global_step=52, grad_norm=18.682875, loss=6.686707
I0406 05:07:37.106138 140017080612672 submission.py:139] 52) loss = 6.687, grad_norm = 18.683
I0406 05:07:37.903773 139988435982080 logging_writer.py:48] [53] global_step=53, grad_norm=43.623226, loss=7.207766
I0406 05:07:37.906692 140017080612672 submission.py:139] 53) loss = 7.208, grad_norm = 43.623
I0406 05:07:38.705069 139988444374784 logging_writer.py:48] [54] global_step=54, grad_norm=3.764946, loss=6.108893
I0406 05:07:38.708370 140017080612672 submission.py:139] 54) loss = 6.109, grad_norm = 3.765
I0406 05:07:39.509254 139988435982080 logging_writer.py:48] [55] global_step=55, grad_norm=18.627583, loss=6.684865
I0406 05:07:39.512434 140017080612672 submission.py:139] 55) loss = 6.685, grad_norm = 18.628
I0406 05:07:40.316169 139988444374784 logging_writer.py:48] [56] global_step=56, grad_norm=16.536404, loss=6.471537
I0406 05:07:40.319268 140017080612672 submission.py:139] 56) loss = 6.472, grad_norm = 16.536
I0406 05:07:41.122906 139988435982080 logging_writer.py:48] [57] global_step=57, grad_norm=18.352510, loss=6.296844
I0406 05:07:41.126451 140017080612672 submission.py:139] 57) loss = 6.297, grad_norm = 18.353
I0406 05:07:41.929715 139988444374784 logging_writer.py:48] [58] global_step=58, grad_norm=16.125172, loss=6.248122
I0406 05:07:41.933233 140017080612672 submission.py:139] 58) loss = 6.248, grad_norm = 16.125
I0406 05:07:42.735513 139988435982080 logging_writer.py:48] [59] global_step=59, grad_norm=14.750924, loss=6.367543
I0406 05:07:42.738640 140017080612672 submission.py:139] 59) loss = 6.368, grad_norm = 14.751
I0406 05:07:43.538407 139988444374784 logging_writer.py:48] [60] global_step=60, grad_norm=15.294093, loss=6.395757
I0406 05:07:43.541649 140017080612672 submission.py:139] 60) loss = 6.396, grad_norm = 15.294
I0406 05:07:44.340833 139988435982080 logging_writer.py:48] [61] global_step=61, grad_norm=10.791958, loss=6.117512
I0406 05:07:44.344025 140017080612672 submission.py:139] 61) loss = 6.118, grad_norm = 10.792
I0406 05:07:45.144433 139988444374784 logging_writer.py:48] [62] global_step=62, grad_norm=20.009264, loss=6.318004
I0406 05:07:45.147436 140017080612672 submission.py:139] 62) loss = 6.318, grad_norm = 20.009
I0406 05:07:45.944624 139988435982080 logging_writer.py:48] [63] global_step=63, grad_norm=15.016257, loss=6.358053
I0406 05:07:45.947856 140017080612672 submission.py:139] 63) loss = 6.358, grad_norm = 15.016
I0406 05:07:46.749231 139988444374784 logging_writer.py:48] [64] global_step=64, grad_norm=16.004581, loss=6.415652
I0406 05:07:46.752611 140017080612672 submission.py:139] 64) loss = 6.416, grad_norm = 16.005
I0406 05:07:47.554398 139988435982080 logging_writer.py:48] [65] global_step=65, grad_norm=11.818596, loss=6.152564
I0406 05:07:47.557360 140017080612672 submission.py:139] 65) loss = 6.153, grad_norm = 11.819
I0406 05:07:48.360194 139988444374784 logging_writer.py:48] [66] global_step=66, grad_norm=18.345289, loss=6.236009
I0406 05:07:48.363540 140017080612672 submission.py:139] 66) loss = 6.236, grad_norm = 18.345
I0406 05:07:49.165351 139988435982080 logging_writer.py:48] [67] global_step=67, grad_norm=15.368919, loss=6.356639
I0406 05:07:49.168625 140017080612672 submission.py:139] 67) loss = 6.357, grad_norm = 15.369
I0406 05:07:49.966774 139988444374784 logging_writer.py:48] [68] global_step=68, grad_norm=15.047701, loss=6.348928
I0406 05:07:49.969880 140017080612672 submission.py:139] 68) loss = 6.349, grad_norm = 15.048
I0406 05:07:50.767648 139988435982080 logging_writer.py:48] [69] global_step=69, grad_norm=18.622480, loss=6.268969
I0406 05:07:50.771080 140017080612672 submission.py:139] 69) loss = 6.269, grad_norm = 18.622
I0406 05:07:51.570947 139988444374784 logging_writer.py:48] [70] global_step=70, grad_norm=4.532562, loss=6.006464
I0406 05:07:51.574218 140017080612672 submission.py:139] 70) loss = 6.006, grad_norm = 4.533
I0406 05:07:52.373363 139988435982080 logging_writer.py:48] [71] global_step=71, grad_norm=12.140258, loss=6.196215
I0406 05:07:52.376755 140017080612672 submission.py:139] 71) loss = 6.196, grad_norm = 12.140
I0406 05:07:53.178317 139988444374784 logging_writer.py:48] [72] global_step=72, grad_norm=5.562067, loss=5.999746
I0406 05:07:53.182101 140017080612672 submission.py:139] 72) loss = 6.000, grad_norm = 5.562
I0406 05:07:53.981902 139988435982080 logging_writer.py:48] [73] global_step=73, grad_norm=21.397577, loss=6.306262
I0406 05:07:53.985226 140017080612672 submission.py:139] 73) loss = 6.306, grad_norm = 21.398
I0406 05:07:54.786359 139988444374784 logging_writer.py:48] [74] global_step=74, grad_norm=11.020896, loss=6.153496
I0406 05:07:54.789712 140017080612672 submission.py:139] 74) loss = 6.153, grad_norm = 11.021
I0406 05:07:55.595791 139988435982080 logging_writer.py:48] [75] global_step=75, grad_norm=11.816033, loss=6.173989
I0406 05:07:55.598861 140017080612672 submission.py:139] 75) loss = 6.174, grad_norm = 11.816
I0406 05:07:56.402347 139988444374784 logging_writer.py:48] [76] global_step=76, grad_norm=14.763511, loss=6.141407
I0406 05:07:56.405666 140017080612672 submission.py:139] 76) loss = 6.141, grad_norm = 14.764
I0406 05:07:57.204544 139988435982080 logging_writer.py:48] [77] global_step=77, grad_norm=1.450150, loss=5.979824
I0406 05:07:57.207683 140017080612672 submission.py:139] 77) loss = 5.980, grad_norm = 1.450
I0406 05:07:58.009013 139988444374784 logging_writer.py:48] [78] global_step=78, grad_norm=10.053228, loss=6.125535
I0406 05:07:58.012147 140017080612672 submission.py:139] 78) loss = 6.126, grad_norm = 10.053
I0406 05:07:58.814500 139988435982080 logging_writer.py:48] [79] global_step=79, grad_norm=0.717496, loss=5.974706
I0406 05:07:58.817738 140017080612672 submission.py:139] 79) loss = 5.975, grad_norm = 0.717
I0406 05:07:59.616310 139988444374784 logging_writer.py:48] [80] global_step=80, grad_norm=13.670909, loss=6.113795
I0406 05:07:59.619705 140017080612672 submission.py:139] 80) loss = 6.114, grad_norm = 13.671
I0406 05:08:00.419202 139988435982080 logging_writer.py:48] [81] global_step=81, grad_norm=11.338436, loss=6.134836
I0406 05:08:00.422220 140017080612672 submission.py:139] 81) loss = 6.135, grad_norm = 11.338
I0406 05:08:01.220001 139988444374784 logging_writer.py:48] [82] global_step=82, grad_norm=7.446435, loss=6.023552
I0406 05:08:01.223077 140017080612672 submission.py:139] 82) loss = 6.024, grad_norm = 7.446
I0406 05:08:02.026074 139988435982080 logging_writer.py:48] [83] global_step=83, grad_norm=21.677341, loss=6.316213
I0406 05:08:02.029406 140017080612672 submission.py:139] 83) loss = 6.316, grad_norm = 21.677
I0406 05:08:02.834861 139988444374784 logging_writer.py:48] [84] global_step=84, grad_norm=13.340026, loss=6.236980
I0406 05:08:02.837948 140017080612672 submission.py:139] 84) loss = 6.237, grad_norm = 13.340
I0406 05:08:03.639057 139988435982080 logging_writer.py:48] [85] global_step=85, grad_norm=10.991764, loss=6.112299
I0406 05:08:03.642236 140017080612672 submission.py:139] 85) loss = 6.112, grad_norm = 10.992
I0406 05:08:04.440847 139988444374784 logging_writer.py:48] [86] global_step=86, grad_norm=25.347960, loss=6.411233
I0406 05:08:04.444082 140017080612672 submission.py:139] 86) loss = 6.411, grad_norm = 25.348
I0406 05:08:05.241502 139988435982080 logging_writer.py:48] [87] global_step=87, grad_norm=13.960830, loss=6.248705
I0406 05:08:05.244692 140017080612672 submission.py:139] 87) loss = 6.249, grad_norm = 13.961
I0406 05:08:06.046156 139988444374784 logging_writer.py:48] [88] global_step=88, grad_norm=12.167030, loss=6.177892
I0406 05:08:06.049527 140017080612672 submission.py:139] 88) loss = 6.178, grad_norm = 12.167
I0406 05:08:06.848074 139988435982080 logging_writer.py:48] [89] global_step=89, grad_norm=28.208509, loss=6.535816
I0406 05:08:06.851927 140017080612672 submission.py:139] 89) loss = 6.536, grad_norm = 28.209
I0406 05:08:07.654532 139988444374784 logging_writer.py:48] [90] global_step=90, grad_norm=16.298790, loss=6.430835
I0406 05:08:07.657785 140017080612672 submission.py:139] 90) loss = 6.431, grad_norm = 16.299
I0406 05:08:08.459187 139988435982080 logging_writer.py:48] [91] global_step=91, grad_norm=14.378638, loss=6.291810
I0406 05:08:08.462418 140017080612672 submission.py:139] 91) loss = 6.292, grad_norm = 14.379
I0406 05:08:09.265225 139988444374784 logging_writer.py:48] [92] global_step=92, grad_norm=34.483486, loss=6.757178
I0406 05:08:09.268352 140017080612672 submission.py:139] 92) loss = 6.757, grad_norm = 34.483
I0406 05:08:10.071742 139988435982080 logging_writer.py:48] [93] global_step=93, grad_norm=18.852272, loss=6.727173
I0406 05:08:10.075464 140017080612672 submission.py:139] 93) loss = 6.727, grad_norm = 18.852
I0406 05:08:10.878153 139988444374784 logging_writer.py:48] [94] global_step=94, grad_norm=18.602900, loss=6.702631
I0406 05:08:10.881234 140017080612672 submission.py:139] 94) loss = 6.703, grad_norm = 18.603
I0406 05:08:11.679824 139988435982080 logging_writer.py:48] [95] global_step=95, grad_norm=32.702961, loss=6.699335
I0406 05:08:11.682899 140017080612672 submission.py:139] 95) loss = 6.699, grad_norm = 32.703
I0406 05:08:12.483697 139988444374784 logging_writer.py:48] [96] global_step=96, grad_norm=14.526249, loss=6.307260
I0406 05:08:12.486720 140017080612672 submission.py:139] 96) loss = 6.307, grad_norm = 14.526
I0406 05:08:13.293416 139988435982080 logging_writer.py:48] [97] global_step=97, grad_norm=13.940234, loss=6.249949
I0406 05:08:13.296485 140017080612672 submission.py:139] 97) loss = 6.250, grad_norm = 13.940
I0406 05:08:14.093369 139988444374784 logging_writer.py:48] [98] global_step=98, grad_norm=33.460964, loss=6.732110
I0406 05:08:14.096455 140017080612672 submission.py:139] 98) loss = 6.732, grad_norm = 33.461
I0406 05:08:14.896816 139988435982080 logging_writer.py:48] [99] global_step=99, grad_norm=19.577597, loss=6.889414
I0406 05:08:14.900282 140017080612672 submission.py:139] 99) loss = 6.889, grad_norm = 19.578
I0406 05:08:15.702597 139988444374784 logging_writer.py:48] [100] global_step=100, grad_norm=18.921009, loss=6.774018
I0406 05:08:15.705836 140017080612672 submission.py:139] 100) loss = 6.774, grad_norm = 18.921
I0406 05:13:33.452594 139988435982080 logging_writer.py:48] [500] global_step=500, grad_norm=0.678508, loss=8.299078
I0406 05:13:33.457095 140017080612672 submission.py:139] 500) loss = 8.299, grad_norm = 0.679
I0406 05:20:03.455861 139988444374784 logging_writer.py:48] [1000] global_step=1000, grad_norm=nan, loss=nan
I0406 05:20:03.460527 140017080612672 submission.py:139] 1000) loss = nan, grad_norm = nan
I0406 05:26:22.981270 139988444374784 logging_writer.py:48] [1500] global_step=1500, grad_norm=nan, loss=nan
I0406 05:26:22.988494 140017080612672 submission.py:139] 1500) loss = nan, grad_norm = nan
I0406 05:32:40.572310 139988435982080 logging_writer.py:48] [2000] global_step=2000, grad_norm=nan, loss=nan
I0406 05:32:40.576597 140017080612672 submission.py:139] 2000) loss = nan, grad_norm = nan
I0406 05:39:00.040796 139988435982080 logging_writer.py:48] [2500] global_step=2500, grad_norm=nan, loss=nan
I0406 05:39:00.048453 140017080612672 submission.py:139] 2500) loss = nan, grad_norm = nan
I0406 05:45:17.776300 139988184332032 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0406 05:45:17.781111 140017080612672 submission.py:139] 3000) loss = nan, grad_norm = nan
I0406 05:46:55.454721 140017080612672 submission_runner.py:373] Before eval at step 3128: RAM USED (GB) 39.88553728
I0406 05:46:55.455140 140017080612672 spec.py:298] Evaluating on the training split.
I0406 05:47:05.263109 140017080612672 spec.py:310] Evaluating on the validation split.
I0406 05:47:14.701999 140017080612672 spec.py:326] Evaluating on the test split.
I0406 05:47:19.996550 140017080612672 submission_runner.py:382] Time since start: 2434.84s, 	Step: 3128, 	{'train/ctc_loss': nan, 'train/wer': 0.9416384864653614, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0406 05:47:19.997295 140017080612672 submission_runner.py:396] After eval at step 3128: RAM USED (GB) 39.017611264
I0406 05:47:20.012230 139988435982080 logging_writer.py:48] [3128] global_step=3128, preemption_count=0, score=1303.529591, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2434.835732, train/ctc_loss=nan, train/wer=0.941638, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0406 05:47:20.404678 140017080612672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_3128.
I0406 05:47:20.405269 140017080612672 submission_runner.py:416] After logging and checkpointing eval at step 3128: RAM USED (GB) 39.023104
I0406 05:52:02.013512 139988184332032 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0406 05:52:02.018402 140017080612672 submission.py:139] 3500) loss = nan, grad_norm = nan
I0406 05:58:19.623205 139988435982080 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0406 05:58:19.631148 140017080612672 submission.py:139] 4000) loss = nan, grad_norm = nan
I0406 06:04:39.030281 139988184332032 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0406 06:04:39.037027 140017080612672 submission.py:139] 4500) loss = nan, grad_norm = nan
I0406 06:10:56.612591 139988175939328 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0406 06:10:56.620779 140017080612672 submission.py:139] 5000) loss = nan, grad_norm = nan
I0406 06:17:15.972900 139988184332032 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0406 06:17:15.984754 140017080612672 submission.py:139] 5500) loss = nan, grad_norm = nan
I0406 06:23:33.583419 139988175939328 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0406 06:23:33.592125 140017080612672 submission.py:139] 6000) loss = nan, grad_norm = nan
I0406 06:27:21.116692 140017080612672 submission_runner.py:373] Before eval at step 6300: RAM USED (GB) 39.261769728
I0406 06:27:21.117148 140017080612672 spec.py:298] Evaluating on the training split.
I0406 06:27:30.961118 140017080612672 spec.py:310] Evaluating on the validation split.
I0406 06:27:40.631027 140017080612672 spec.py:326] Evaluating on the test split.
I0406 06:27:45.884318 140017080612672 submission_runner.py:382] Time since start: 4860.50s, 	Step: 6300, 	{'train/ctc_loss': nan, 'train/wer': 0.9416384864653614, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0406 06:27:45.885047 140017080612672 submission_runner.py:396] After eval at step 6300: RAM USED (GB) 39.043465216
I0406 06:27:45.901120 139988175939328 logging_writer.py:48] [6300] global_step=6300, preemption_count=0, score=2566.854639, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=4860.495608, train/ctc_loss=nan, train/wer=0.941638, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0406 06:27:46.290085 140017080612672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_6300.
I0406 06:27:46.290676 140017080612672 submission_runner.py:416] After logging and checkpointing eval at step 6300: RAM USED (GB) 39.049723904
I0406 06:30:18.092793 139988167546624 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0406 06:30:18.096662 140017080612672 submission.py:139] 6500) loss = nan, grad_norm = nan
I0406 06:36:35.826623 139988175939328 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0406 06:36:35.830990 140017080612672 submission.py:139] 7000) loss = nan, grad_norm = nan
I0406 06:42:55.367868 139988175939328 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0406 06:42:55.374314 140017080612672 submission.py:139] 7500) loss = nan, grad_norm = nan
I0406 06:49:13.094065 139988167546624 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0406 06:49:13.098304 140017080612672 submission.py:139] 8000) loss = nan, grad_norm = nan
I0406 06:55:32.477183 139988175939328 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0406 06:55:32.484010 140017080612672 submission.py:139] 8500) loss = nan, grad_norm = nan
I0406 07:01:50.035056 139988167546624 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0406 07:01:50.040151 140017080612672 submission.py:139] 9000) loss = nan, grad_norm = nan
I0406 07:07:46.913597 140017080612672 submission_runner.py:373] Before eval at step 9471: RAM USED (GB) 39.242047488
I0406 07:07:46.914005 140017080612672 spec.py:298] Evaluating on the training split.
I0406 07:07:56.721307 140017080612672 spec.py:310] Evaluating on the validation split.
I0406 07:08:06.005926 140017080612672 spec.py:326] Evaluating on the test split.
I0406 07:08:11.327616 140017080612672 submission_runner.py:382] Time since start: 7286.29s, 	Step: 9471, 	{'train/ctc_loss': nan, 'train/wer': 0.9416384864653614, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0406 07:08:11.328392 140017080612672 submission_runner.py:396] After eval at step 9471: RAM USED (GB) 39.063425024
I0406 07:08:11.345256 139988175939328 logging_writer.py:48] [9471] global_step=9471, preemption_count=0, score=3829.637220, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7286.293571, train/ctc_loss=nan, train/wer=0.941638, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0406 07:08:11.738352 140017080612672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_9471.
I0406 07:08:11.738948 140017080612672 submission_runner.py:416] After logging and checkpointing eval at step 9471: RAM USED (GB) 39.068286976
I0406 07:08:34.393356 139988167546624 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0406 07:08:34.396732 140017080612672 submission.py:139] 9500) loss = nan, grad_norm = nan
I0406 07:14:51.529665 140017080612672 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 39.449296896
I0406 07:14:51.529881 140017080612672 spec.py:298] Evaluating on the training split.
I0406 07:15:00.676991 140017080612672 spec.py:310] Evaluating on the validation split.
I0406 07:15:09.988745 140017080612672 spec.py:326] Evaluating on the test split.
I0406 07:15:15.253008 140017080612672 submission_runner.py:382] Time since start: 7710.91s, 	Step: 10000, 	{'train/ctc_loss': nan, 'train/wer': 0.9416384864653614, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0406 07:15:15.253908 140017080612672 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 39.603535872
I0406 07:15:15.269955 139988175939328 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4039.708004, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7710.911436, train/ctc_loss=nan, train/wer=0.941638, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0406 07:15:15.679774 140017080612672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0406 07:15:15.680511 140017080612672 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 39.618609152
I0406 07:15:15.692261 139988167546624 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4039.708004
I0406 07:15:16.420274 140017080612672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0406 07:15:16.577633 140017080612672 submission_runner.py:550] Tuning trial 1/1
I0406 07:15:16.577883 140017080612672 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0406 07:15:16.578241 140017080612672 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.49538697494817, 'train/wer': 1.0354120642020073, 'validation/ctc_loss': 30.06022704440426, 'validation/wer': 1.1719692946458746, 'validation/num_examples': 5348, 'test/ctc_loss': 30.196728821214272, 'test/wer': 1.176081083825889, 'test/num_examples': 2472, 'score': 6.602487325668335, 'total_duration': 6.604502439498901, 'global_step': 1, 'preemption_count': 0}), (3128, {'train/ctc_loss': nan, 'train/wer': 0.9416384864653614, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1303.5295913219452, 'total_duration': 2434.8357315063477, 'global_step': 3128, 'preemption_count': 0}), (6300, {'train/ctc_loss': nan, 'train/wer': 0.9416384864653614, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2566.8546385765076, 'total_duration': 4860.495607614517, 'global_step': 6300, 'preemption_count': 0}), (9471, {'train/ctc_loss': nan, 'train/wer': 0.9416384864653614, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 3829.6372203826904, 'total_duration': 7286.293570995331, 'global_step': 9471, 'preemption_count': 0}), (10000, {'train/ctc_loss': nan, 'train/wer': 0.9416384864653614, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4039.70800447464, 'total_duration': 7710.911436080933, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0406 07:15:16.578319 140017080612672 submission_runner.py:553] Timing: 4039.70800447464
I0406 07:15:16.578393 140017080612672 submission_runner.py:554] ====================
I0406 07:15:16.578545 140017080612672 submission_runner.py:613] Final librispeech_conformer score: 4039.70800447464
