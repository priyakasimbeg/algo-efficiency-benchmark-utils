I0404 19:48:57.634957 140115245111104 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum_v2/imagenet_vit_jax.
I0404 19:48:57.687059 140115245111104 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 19:48:58.564858 140115245111104 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0404 19:48:58.565411 140115245111104 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 19:48:58.570934 140115245111104 submission_runner.py:511] Using RNG seed 1592082170
I0404 19:49:00.872238 140115245111104 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 19:49:00.872430 140115245111104 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1.
I0404 19:49:00.872593 140115245111104 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/hparams.json.
I0404 19:49:00.996261 140115245111104 submission_runner.py:230] Starting train once: RAM USED (GB) 4.201852928
I0404 19:49:00.996427 140115245111104 submission_runner.py:231] Initializing dataset.
I0404 19:49:01.007581 140115245111104 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:49:01.015097 140115245111104 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:49:01.015203 140115245111104 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:49:01.261421 140115245111104 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:49:07.754249 140115245111104 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.353196032
I0404 19:49:07.754472 140115245111104 submission_runner.py:240] Initializing model.
I0404 19:49:19.052051 140115245111104 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.3509248
I0404 19:49:19.052257 140115245111104 submission_runner.py:252] Initializing optimizer.
I0404 19:49:19.534020 140115245111104 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.351174656
I0404 19:49:19.534184 140115245111104 submission_runner.py:261] Initializing metrics bundle.
I0404 19:49:19.534232 140115245111104 submission_runner.py:276] Initializing checkpoint and logger.
I0404 19:49:19.535130 140115245111104 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0404 19:49:20.353569 140115245111104 submission_runner.py:297] Saving meta data to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/meta_data_0.json.
I0404 19:49:20.354543 140115245111104 submission_runner.py:300] Saving flags to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/flags_0.json.
I0404 19:49:20.358553 140115245111104 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.348471296
I0404 19:49:20.358763 140115245111104 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.348471296
I0404 19:49:20.358832 140115245111104 submission_runner.py:313] Starting training loop.
I0404 19:49:23.575142 140115245111104 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 14.703607808
I0404 19:50:05.668391 139939714946816 logging_writer.py:48] [0] global_step=0, grad_norm=0.2974497377872467, loss=6.9077534675598145
I0404 19:50:05.679885 140115245111104 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 46.521352192
I0404 19:50:05.680144 140115245111104 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 46.521352192
I0404 19:50:05.680222 140115245111104 spec.py:298] Evaluating on the training split.
I0404 19:50:05.685862 140115245111104 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:05.691760 140115245111104 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:50:05.691860 140115245111104 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:50:05.750842 140115245111104 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:23.640370 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 19:50:23.649111 140115245111104 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:23.660179 140115245111104 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:50:23.660447 140115245111104 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:50:23.711624 140115245111104 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:42.391036 140115245111104 spec.py:326] Evaluating on the test split.
I0404 19:50:42.398849 140115245111104 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 19:50:42.403269 140115245111104 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0404 19:50:42.434077 140115245111104 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 19:50:52.322202 140115245111104 submission_runner.py:382] Time since start: 45.32s, 	Step: 1, 	{'train/accuracy': 0.0009960937313735485, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000}
I0404 19:50:52.322710 140115245111104 submission_runner.py:396] After eval at step 1: RAM USED (GB) 105.857654784
I0404 19:50:52.331816 139876448057088 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=45.241452, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=45.321327, train/accuracy=0.000996, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0404 19:50:52.435725 140115245111104 checkpoints.py:356] Saving checkpoint at step: 1
I0404 19:50:52.901501 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_1
I0404 19:50:52.902375 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_1.
I0404 19:50:52.904199 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 105.94177024
I0404 19:50:52.908692 140115245111104 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 105.931886592
I0404 19:51:07.392682 140115245111104 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 105.341288448
I0404 19:51:46.068079 139934845306624 logging_writer.py:48] [100] global_step=100, grad_norm=0.30415773391723633, loss=6.906103134155273
I0404 19:52:25.256383 139934853699328 logging_writer.py:48] [200] global_step=200, grad_norm=0.29829099774360657, loss=6.898880481719971
I0404 19:53:04.565012 139934845306624 logging_writer.py:48] [300] global_step=300, grad_norm=0.3019431531429291, loss=6.895552635192871
I0404 19:53:44.334505 139934853699328 logging_writer.py:48] [400] global_step=400, grad_norm=0.4327959716320038, loss=6.842968940734863
I0404 19:54:24.692961 139934845306624 logging_writer.py:48] [500] global_step=500, grad_norm=0.7513790130615234, loss=6.775857925415039
I0404 19:55:04.920644 139934853699328 logging_writer.py:48] [600] global_step=600, grad_norm=0.9493817687034607, loss=6.8097662925720215
I0404 19:55:44.910661 139934845306624 logging_writer.py:48] [700] global_step=700, grad_norm=0.7896558046340942, loss=6.807604789733887
I0404 19:56:24.937627 139934853699328 logging_writer.py:48] [800] global_step=800, grad_norm=0.8425645232200623, loss=6.614436149597168
I0404 19:57:04.956290 139934845306624 logging_writer.py:48] [900] global_step=900, grad_norm=1.1505074501037598, loss=6.628464221954346
I0404 19:57:45.252594 139934853699328 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8770270347595215, loss=6.597624778747559
I0404 19:57:52.939929 140115245111104 submission_runner.py:373] Before eval at step 1021: RAM USED (GB) 79.400701952
I0404 19:57:52.940242 140115245111104 spec.py:298] Evaluating on the training split.
I0404 19:58:04.073405 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 19:58:10.566859 140115245111104 spec.py:326] Evaluating on the test split.
I0404 19:58:12.579501 140115245111104 submission_runner.py:382] Time since start: 512.58s, 	Step: 1021, 	{'train/accuracy': 0.03251953050494194, 'train/loss': 6.124095439910889, 'validation/accuracy': 0.0321200005710125, 'validation/loss': 6.145279884338379, 'validation/num_examples': 50000, 'test/accuracy': 0.026500001549720764, 'test/loss': 6.233181953430176, 'test/num_examples': 10000}
I0404 19:58:12.580048 140115245111104 submission_runner.py:396] After eval at step 1021: RAM USED (GB) 84.164636672
I0404 19:58:12.591277 139876615845632 logging_writer.py:48] [1021] global_step=1021, preemption_count=0, score=458.287196, test/accuracy=0.026500, test/loss=6.233182, test/num_examples=10000, total_duration=512.578850, train/accuracy=0.032520, train/loss=6.124095, validation/accuracy=0.032120, validation/loss=6.145280, validation/num_examples=50000
I0404 19:58:13.636878 140115245111104 checkpoints.py:356] Saving checkpoint at step: 1021
I0404 19:58:15.285434 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_1021
I0404 19:58:15.297412 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_1021.
I0404 19:58:15.299134 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 1021: RAM USED (GB) 95.535443968
I0404 19:58:46.707255 139876699707136 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6773398518562317, loss=6.778433799743652
I0404 19:59:26.003874 139938154657536 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.928367018699646, loss=6.453888893127441
I0404 20:00:05.539708 139876699707136 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6336302161216736, loss=6.542882442474365
I0404 20:00:45.633509 139938154657536 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7598116993904114, loss=6.502150535583496
I0404 20:01:25.856513 139876699707136 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9211670756340027, loss=6.419526100158691
I0404 20:02:06.183008 139938154657536 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6553473472595215, loss=6.382273197174072
I0404 20:02:46.227005 139876699707136 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7833204865455627, loss=6.3822784423828125
I0404 20:03:26.568028 139938154657536 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7306739091873169, loss=6.323366165161133
I0404 20:04:06.824810 139876699707136 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.0350341796875, loss=6.552682876586914
I0404 20:04:47.177248 139938154657536 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7274839282035828, loss=6.317760944366455
I0404 20:05:15.447708 140115245111104 submission_runner.py:373] Before eval at step 2072: RAM USED (GB) 86.526386176
I0404 20:05:15.448010 140115245111104 spec.py:298] Evaluating on the training split.
I0404 20:05:26.567073 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 20:05:33.370488 140115245111104 spec.py:326] Evaluating on the test split.
I0404 20:05:35.095553 140115245111104 submission_runner.py:382] Time since start: 955.09s, 	Step: 2072, 	{'train/accuracy': 0.0597851537168026, 'train/loss': 5.681266784667969, 'validation/accuracy': 0.05857999995350838, 'validation/loss': 5.708341121673584, 'validation/num_examples': 50000, 'test/accuracy': 0.04020000249147415, 'test/loss': 5.863155841827393, 'test/num_examples': 10000}
I0404 20:05:35.096342 140115245111104 submission_runner.py:396] After eval at step 2072: RAM USED (GB) 89.578201088
I0404 20:05:35.108224 139876699707136 logging_writer.py:48] [2072] global_step=2072, preemption_count=0, score=871.397144, test/accuracy=0.040200, test/loss=5.863156, test/num_examples=10000, total_duration=955.086543, train/accuracy=0.059785, train/loss=5.681267, validation/accuracy=0.058580, validation/loss=5.708341, validation/num_examples=50000
I0404 20:05:35.217360 140115245111104 checkpoints.py:356] Saving checkpoint at step: 2072
I0404 20:05:37.708340 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_2072
I0404 20:05:37.721269 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_2072.
I0404 20:05:37.723231 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 2072: RAM USED (GB) 99.860729856
I0404 20:05:49.159518 139938154657536 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.576027512550354, loss=6.74628210067749
I0404 20:06:28.337468 139938121086720 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7591955661773682, loss=6.586599826812744
I0404 20:07:07.625712 139938154657536 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7295506596565247, loss=6.335130214691162
I0404 20:07:47.908020 139938121086720 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9608739614486694, loss=6.292517185211182
I0404 20:08:28.034839 139938154657536 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7088813781738281, loss=6.226983547210693
I0404 20:09:08.260597 139938121086720 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7584174871444702, loss=6.4453020095825195
I0404 20:09:48.157311 139938154657536 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6971657872200012, loss=6.207512855529785
I0404 20:10:28.375850 139938121086720 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6678759455680847, loss=6.529155731201172
I0404 20:11:08.457788 139938154657536 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6831322312355042, loss=6.166141986846924
I0404 20:11:48.929615 139938121086720 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7036336064338684, loss=6.157426834106445
I0404 20:12:29.109729 139938154657536 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6344822645187378, loss=6.522073745727539
I0404 20:12:37.953794 140115245111104 submission_runner.py:373] Before eval at step 3123: RAM USED (GB) 92.876578816
I0404 20:12:37.954035 140115245111104 spec.py:298] Evaluating on the training split.
I0404 20:12:49.028260 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 20:12:56.107733 140115245111104 spec.py:326] Evaluating on the test split.
I0404 20:12:57.781299 140115245111104 submission_runner.py:382] Time since start: 1397.59s, 	Step: 3123, 	{'train/accuracy': 0.08498046547174454, 'train/loss': 5.371925354003906, 'validation/accuracy': 0.07961999624967575, 'validation/loss': 5.421719074249268, 'validation/num_examples': 50000, 'test/accuracy': 0.059700001031160355, 'test/loss': 5.625901699066162, 'test/num_examples': 10000}
I0404 20:12:57.781799 140115245111104 submission_runner.py:396] After eval at step 3123: RAM USED (GB) 93.990703104
I0404 20:12:57.793497 139938121086720 logging_writer.py:48] [3123] global_step=3123, preemption_count=0, score=1284.655174, test/accuracy=0.059700, test/loss=5.625902, test/num_examples=10000, total_duration=1397.593132, train/accuracy=0.084980, train/loss=5.371925, validation/accuracy=0.079620, validation/loss=5.421719, validation/num_examples=50000
I0404 20:12:57.891006 140115245111104 checkpoints.py:356] Saving checkpoint at step: 3123
I0404 20:13:00.093136 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_3123
I0404 20:13:00.106469 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_3123.
I0404 20:13:00.108492 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 3123: RAM USED (GB) 102.76753408
I0404 20:13:30.783688 139938154657536 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7184372544288635, loss=6.089306354522705
I0404 20:14:10.031698 139938104301312 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.679491400718689, loss=6.118918418884277
I0404 20:14:50.118328 139938154657536 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7287331223487854, loss=6.116420269012451
I0404 20:15:30.545498 139938104301312 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.736483633518219, loss=6.070354461669922
I0404 20:16:10.924993 139938154657536 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8304612040519714, loss=6.113049030303955
I0404 20:16:51.164607 139938104301312 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7364974617958069, loss=6.036165237426758
I0404 20:17:31.428405 139938154657536 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.6216246485710144, loss=6.15878438949585
I0404 20:18:11.749576 139938104301312 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6971631646156311, loss=6.036759376525879
I0404 20:18:51.707930 139938154657536 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.46127817034721375, loss=6.585784435272217
I0404 20:19:31.880444 139938104301312 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6792429089546204, loss=6.051759243011475
I0404 20:20:00.352766 140115245111104 submission_runner.py:373] Before eval at step 4173: RAM USED (GB) 98.221727744
I0404 20:20:00.353016 140115245111104 spec.py:298] Evaluating on the training split.
I0404 20:20:12.065198 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 20:20:20.549000 140115245111104 spec.py:326] Evaluating on the test split.
I0404 20:20:22.218559 140115245111104 submission_runner.py:382] Time since start: 1839.99s, 	Step: 4173, 	{'train/accuracy': 0.10900390148162842, 'train/loss': 5.115922451019287, 'validation/accuracy': 0.09737999737262726, 'validation/loss': 5.200913429260254, 'validation/num_examples': 50000, 'test/accuracy': 0.07110000401735306, 'test/loss': 5.451022148132324, 'test/num_examples': 10000}
I0404 20:20:22.219043 140115245111104 submission_runner.py:396] After eval at step 4173: RAM USED (GB) 102.804959232
I0404 20:20:22.229353 139938154657536 logging_writer.py:48] [4173] global_step=4173, preemption_count=0, score=1697.931385, test/accuracy=0.071100, test/loss=5.451022, test/num_examples=10000, total_duration=1839.991816, train/accuracy=0.109004, train/loss=5.115922, validation/accuracy=0.097380, validation/loss=5.200913, validation/num_examples=50000
I0404 20:20:22.317097 140115245111104 checkpoints.py:356] Saving checkpoint at step: 4173
I0404 20:20:23.166399 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_4173
I0404 20:20:23.180050 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_4173.
I0404 20:20:23.182581 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 4173: RAM USED (GB) 106.704150528
I0404 20:20:34.387347 139938104301312 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6168584227561951, loss=6.025974273681641
I0404 20:21:13.640317 139937961723648 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6757615208625793, loss=6.103514671325684
I0404 20:21:53.619158 139938104301312 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.48409411311149597, loss=6.571160793304443
I0404 20:22:34.380923 139937961723648 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6540182828903198, loss=6.052406311035156
I0404 20:23:14.914560 139938104301312 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.4419679045677185, loss=6.588199615478516
I0404 20:23:55.688506 139937961723648 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5707612037658691, loss=5.956234931945801
I0404 20:24:36.160955 139938104301312 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6296122670173645, loss=5.943235397338867
I0404 20:25:16.561259 139937961723648 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.45964327454566956, loss=6.395105361938477
I0404 20:25:57.199237 139938104301312 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5300862789154053, loss=5.922120094299316
I0404 20:26:37.341705 139937961723648 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6687015891075134, loss=5.911816120147705
I0404 20:27:17.557231 139938104301312 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.637213945388794, loss=5.953895568847656
I0404 20:27:23.575050 140115245111104 submission_runner.py:373] Before eval at step 5217: RAM USED (GB) 104.165220352
I0404 20:27:23.575334 140115245111104 spec.py:298] Evaluating on the training split.
I0404 20:27:36.854548 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 20:27:45.578763 140115245111104 spec.py:326] Evaluating on the test split.
I0404 20:27:47.253957 140115245111104 submission_runner.py:382] Time since start: 2283.21s, 	Step: 5217, 	{'train/accuracy': 0.13236327469348907, 'train/loss': 4.834392070770264, 'validation/accuracy': 0.1211399957537651, 'validation/loss': 4.906247138977051, 'validation/num_examples': 50000, 'test/accuracy': 0.09740000218153, 'test/loss': 5.192223072052002, 'test/num_examples': 10000}
I0404 20:27:47.254575 140115245111104 submission_runner.py:396] After eval at step 5217: RAM USED (GB) 109.491802112
I0404 20:27:47.266881 139937961723648 logging_writer.py:48] [5217] global_step=5217, preemption_count=0, score=2109.177908, test/accuracy=0.097400, test/loss=5.192223, test/num_examples=10000, total_duration=2283.214778, train/accuracy=0.132363, train/loss=4.834392, validation/accuracy=0.121140, validation/loss=4.906247, validation/num_examples=50000
I0404 20:27:47.367173 140115245111104 checkpoints.py:356] Saving checkpoint at step: 5217
I0404 20:27:48.171202 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_5217
I0404 20:27:48.184261 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_5217.
I0404 20:27:48.186510 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 5217: RAM USED (GB) 113.36650752
I0404 20:28:21.371229 139938104301312 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6040768623352051, loss=5.816947937011719
I0404 20:29:01.040283 139937953330944 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5383443832397461, loss=6.508172512054443
I0404 20:29:41.434631 139938104301312 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5801472067832947, loss=6.674749851226807
I0404 20:30:21.948668 139937953330944 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6027426719665527, loss=5.835030555725098
I0404 20:31:02.362771 139938104301312 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6677466630935669, loss=5.986687660217285
I0404 20:31:42.620841 139937953330944 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6015351414680481, loss=5.885709285736084
I0404 20:32:22.825427 139938104301312 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4621228873729706, loss=6.453125953674316
I0404 20:33:03.225918 139937953330944 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.39269018173217773, loss=6.534297943115234
I0404 20:33:43.682936 139938104301312 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.47198784351348877, loss=6.312041282653809
I0404 20:34:24.122194 139937953330944 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5967361927032471, loss=5.748049259185791
I0404 20:34:48.535905 140115245111104 submission_runner.py:373] Before eval at step 6261: RAM USED (GB) 110.334050304
I0404 20:34:48.536199 140115245111104 spec.py:298] Evaluating on the training split.
I0404 20:35:02.397581 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 20:35:11.210366 140115245111104 spec.py:326] Evaluating on the test split.
I0404 20:35:12.872728 140115245111104 submission_runner.py:382] Time since start: 2728.17s, 	Step: 6261, 	{'train/accuracy': 0.16523437201976776, 'train/loss': 4.550891876220703, 'validation/accuracy': 0.1513800024986267, 'validation/loss': 4.641767501831055, 'validation/num_examples': 50000, 'test/accuracy': 0.1153000071644783, 'test/loss': 4.969973564147949, 'test/num_examples': 10000}
I0404 20:35:12.873252 140115245111104 submission_runner.py:396] After eval at step 6261: RAM USED (GB) 114.090016768
I0404 20:35:12.888314 139938104301312 logging_writer.py:48] [6261] global_step=6261, preemption_count=0, score=2520.829734, test/accuracy=0.115300, test/loss=4.969974, test/num_examples=10000, total_duration=2728.174921, train/accuracy=0.165234, train/loss=4.550892, validation/accuracy=0.151380, validation/loss=4.641768, validation/num_examples=50000
I0404 20:35:12.985389 140115245111104 checkpoints.py:356] Saving checkpoint at step: 6261
I0404 20:35:13.902703 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_6261
I0404 20:35:13.915565 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_6261.
I0404 20:35:13.917469 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 6261: RAM USED (GB) 118.176841728
I0404 20:35:29.749475 139937953330944 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6592600345611572, loss=5.759603500366211
I0404 20:36:09.335473 139937944938240 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.39653316140174866, loss=6.367849349975586
I0404 20:36:49.487346 139937953330944 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5865869522094727, loss=5.656787395477295
I0404 20:37:29.948563 139937944938240 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5107449889183044, loss=6.567313194274902
I0404 20:38:10.326573 139937953330944 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5864458084106445, loss=5.786194801330566
I0404 20:38:50.833632 139937944938240 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6850957274436951, loss=5.658925533294678
I0404 20:39:31.354450 139937953330944 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6022130846977234, loss=5.599705696105957
I0404 20:40:12.057124 139937944938240 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6025399565696716, loss=5.665274143218994
I0404 20:40:52.532444 139937953330944 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6355265974998474, loss=5.620984077453613
I0404 20:41:33.062078 139937944938240 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6375460624694824, loss=5.594088554382324
I0404 20:42:13.668349 139937953330944 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.42875179648399353, loss=6.483611106872559
I0404 20:42:14.184899 140115245111104 submission_runner.py:373] Before eval at step 7303: RAM USED (GB) 116.220166144
I0404 20:42:14.185271 140115245111104 spec.py:298] Evaluating on the training split.
I0404 20:42:28.511038 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 20:42:37.412963 140115245111104 spec.py:326] Evaluating on the test split.
I0404 20:42:39.080032 140115245111104 submission_runner.py:382] Time since start: 3173.82s, 	Step: 7303, 	{'train/accuracy': 0.19496093690395355, 'train/loss': 4.4034247398376465, 'validation/accuracy': 0.17945998907089233, 'validation/loss': 4.50022029876709, 'validation/num_examples': 50000, 'test/accuracy': 0.13370001316070557, 'test/loss': 4.853652000427246, 'test/num_examples': 10000}
I0404 20:42:39.080598 140115245111104 submission_runner.py:396] After eval at step 7303: RAM USED (GB) 120.88064
I0404 20:42:39.093337 139937944938240 logging_writer.py:48] [7303] global_step=7303, preemption_count=0, score=2931.846829, test/accuracy=0.133700, test/loss=4.853652, test/num_examples=10000, total_duration=3173.823882, train/accuracy=0.194961, train/loss=4.403425, validation/accuracy=0.179460, validation/loss=4.500220, validation/num_examples=50000
I0404 20:42:39.193853 140115245111104 checkpoints.py:356] Saving checkpoint at step: 7303
I0404 20:42:39.927565 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_7303
I0404 20:42:39.942144 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_7303.
I0404 20:42:39.943965 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 7303: RAM USED (GB) 124.416868352
I0404 20:43:18.698098 139937953330944 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5418313145637512, loss=5.633288383483887
I0404 20:43:59.510622 139937936545536 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6236264109611511, loss=5.560966491699219
I0404 20:44:39.937076 139937953330944 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6165680289268494, loss=5.649528503417969
I0404 20:45:20.422720 139937936545536 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.561439573764801, loss=5.568094730377197
I0404 20:46:01.042053 139937953330944 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5330586433410645, loss=5.518406867980957
I0404 20:46:41.480077 139937936545536 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.3910321593284607, loss=6.444070816040039
I0404 20:47:21.809386 139937953330944 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5134782791137695, loss=6.450468063354492
I0404 20:48:02.272305 139937936545536 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7073050737380981, loss=5.521891117095947
I0404 20:48:42.711537 139937953330944 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.9575060606002808, loss=5.513677597045898
I0404 20:49:22.973914 139937936545536 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6790776252746582, loss=5.484289169311523
I0404 20:49:39.988423 140115245111104 submission_runner.py:373] Before eval at step 8344: RAM USED (GB) 122.860916736
I0404 20:49:39.988713 140115245111104 spec.py:298] Evaluating on the training split.
I0404 20:49:54.323240 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 20:50:03.285314 140115245111104 spec.py:326] Evaluating on the test split.
I0404 20:50:04.957756 140115245111104 submission_runner.py:382] Time since start: 3619.63s, 	Step: 8344, 	{'train/accuracy': 0.21455077826976776, 'train/loss': 4.184172630310059, 'validation/accuracy': 0.19679999351501465, 'validation/loss': 4.278113842010498, 'validation/num_examples': 50000, 'test/accuracy': 0.1542000025510788, 'test/loss': 4.65147590637207, 'test/num_examples': 10000}
I0404 20:50:04.958319 140115245111104 submission_runner.py:396] After eval at step 8344: RAM USED (GB) 126.802763776
I0404 20:50:04.974291 139937953330944 logging_writer.py:48] [8344] global_step=8344, preemption_count=0, score=3342.382114, test/accuracy=0.154200, test/loss=4.651476, test/num_examples=10000, total_duration=3619.626693, train/accuracy=0.214551, train/loss=4.184173, validation/accuracy=0.196800, validation/loss=4.278114, validation/num_examples=50000
I0404 20:50:05.078793 140115245111104 checkpoints.py:356] Saving checkpoint at step: 8344
I0404 20:50:05.780311 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_8344
I0404 20:50:05.794341 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_8344.
I0404 20:50:05.797212 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 8344: RAM USED (GB) 130.264223744
I0404 20:50:28.533086 139937936545536 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5470308065414429, loss=5.4931206703186035
I0404 20:51:08.554603 139937928152832 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5671775937080383, loss=5.3918070793151855
I0404 20:51:49.011729 139937936545536 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5384684801101685, loss=5.495049476623535
I0404 20:52:29.490410 139937928152832 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5451945662498474, loss=5.5132975578308105
I0404 20:53:10.340245 139937936545536 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5590111017227173, loss=5.912076473236084
I0404 20:53:50.910888 139937928152832 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5175470113754272, loss=5.491806507110596
I0404 20:54:31.366378 139937936545536 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6230601668357849, loss=5.3364996910095215
I0404 20:55:12.136221 139937928152832 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5923195481300354, loss=5.421717643737793
I0404 20:55:52.725297 139937936545536 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5607613325119019, loss=5.4182586669921875
I0404 20:56:33.585455 139937928152832 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5666405558586121, loss=5.8780107498168945
I0404 20:57:06.167759 140115245111104 submission_runner.py:373] Before eval at step 9382: RAM USED (GB) 128.307871744
I0404 20:57:06.168049 140115245111104 spec.py:298] Evaluating on the training split.
I0404 20:57:20.260644 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 20:57:30.108737 140115245111104 spec.py:326] Evaluating on the test split.
I0404 20:57:31.770265 140115245111104 submission_runner.py:382] Time since start: 4065.81s, 	Step: 9382, 	{'train/accuracy': 0.26363280415534973, 'train/loss': 3.823559522628784, 'validation/accuracy': 0.2380799949169159, 'validation/loss': 3.9775593280792236, 'validation/num_examples': 50000, 'test/accuracy': 0.18330000340938568, 'test/loss': 4.415868282318115, 'test/num_examples': 10000}
I0404 20:57:31.770798 140115245111104 submission_runner.py:396] After eval at step 9382: RAM USED (GB) 135.93008128
I0404 20:57:31.784148 139937936545536 logging_writer.py:48] [9382] global_step=9382, preemption_count=0, score=3752.654942, test/accuracy=0.183300, test/loss=4.415868, test/num_examples=10000, total_duration=4065.808554, train/accuracy=0.263633, train/loss=3.823560, validation/accuracy=0.238080, validation/loss=3.977559, validation/num_examples=50000
I0404 20:57:31.931283 140115245111104 checkpoints.py:356] Saving checkpoint at step: 9382
I0404 20:57:32.862847 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_9382
I0404 20:57:32.878719 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_9382.
I0404 20:57:32.891557 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 9382: RAM USED (GB) 140.50766848
I0404 20:57:40.417931 139937928152832 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5734760761260986, loss=5.391310214996338
I0404 20:58:19.894291 139937919760128 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6242665648460388, loss=5.298686504364014
I0404 20:59:00.304716 139937928152832 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6223540306091309, loss=5.415482521057129
I0404 20:59:41.028560 139937919760128 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5502159595489502, loss=5.350121974945068
I0404 21:00:21.689837 139937928152832 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5432077646255493, loss=5.2319440841674805
I0404 21:01:02.280879 139937919760128 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5691400766372681, loss=5.33955192565918
I0404 21:01:43.209855 139937928152832 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5692998170852661, loss=5.165112018585205
I0404 21:02:23.714321 139937919760128 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6704512238502502, loss=5.261648654937744
I0404 21:03:04.425833 139937928152832 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5459581613540649, loss=5.407625675201416
I0404 21:03:45.533773 139937919760128 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5298977494239807, loss=5.28817081451416
I0404 21:04:26.477934 139937928152832 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6579686999320984, loss=5.257905960083008
I0404 21:04:32.987239 140115245111104 submission_runner.py:373] Before eval at step 10418: RAM USED (GB) 134.780141568
I0404 21:04:32.987527 140115245111104 spec.py:298] Evaluating on the training split.
I0404 21:04:47.299319 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 21:04:57.666037 140115245111104 spec.py:326] Evaluating on the test split.
I0404 21:04:59.327131 140115245111104 submission_runner.py:382] Time since start: 4512.63s, 	Step: 10418, 	{'train/accuracy': 0.2916210889816284, 'train/loss': 3.628342866897583, 'validation/accuracy': 0.2687999904155731, 'validation/loss': 3.7524280548095703, 'validation/num_examples': 50000, 'test/accuracy': 0.20170001685619354, 'test/loss': 4.2375078201293945, 'test/num_examples': 10000}
I0404 21:04:59.327623 140115245111104 submission_runner.py:396] After eval at step 10418: RAM USED (GB) 141.568086016
I0404 21:04:59.341814 139937919760128 logging_writer.py:48] [10418] global_step=10418, preemption_count=0, score=4162.046027, test/accuracy=0.201700, test/loss=4.237508, test/num_examples=10000, total_duration=4512.627081, train/accuracy=0.291621, train/loss=3.628343, validation/accuracy=0.268800, validation/loss=3.752428, validation/num_examples=50000
I0404 21:04:59.469463 140115245111104 checkpoints.py:356] Saving checkpoint at step: 10418
I0404 21:05:00.398989 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_10418
I0404 21:05:00.413900 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_10418.
I0404 21:05:00.426563 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 10418: RAM USED (GB) 146.047807488
I0404 21:05:33.061027 139937928152832 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.4898011386394501, loss=6.475099563598633
I0404 21:06:13.074604 139937911367424 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5471928715705872, loss=5.176642894744873
I0404 21:06:53.201155 139937928152832 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.524219274520874, loss=5.173360824584961
I0404 21:07:33.895943 139937911367424 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7383488416671753, loss=5.241291046142578
I0404 21:08:14.629579 139937928152832 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5950747132301331, loss=5.24022102355957
I0404 21:08:55.492519 139937911367424 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.42637044191360474, loss=6.285817623138428
I0404 21:09:35.991304 139937928152832 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5400492548942566, loss=5.151221752166748
I0404 21:10:16.556196 139937911367424 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5181110501289368, loss=5.5339508056640625
I0404 21:10:57.138348 139937928152832 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.48086416721343994, loss=5.9539031982421875
I0404 21:11:37.933386 139937911367424 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.569787323474884, loss=5.0710248947143555
I0404 21:12:00.605476 140115245111104 submission_runner.py:373] Before eval at step 11458: RAM USED (GB) 140.443107328
I0404 21:12:00.605733 140115245111104 spec.py:298] Evaluating on the training split.
I0404 21:12:15.310709 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 21:12:25.690766 140115245111104 spec.py:326] Evaluating on the test split.
I0404 21:12:27.343309 140115245111104 submission_runner.py:382] Time since start: 4960.25s, 	Step: 11458, 	{'train/accuracy': 0.31591796875, 'train/loss': 3.480926990509033, 'validation/accuracy': 0.29308000206947327, 'validation/loss': 3.6053459644317627, 'validation/num_examples': 50000, 'test/accuracy': 0.21770000457763672, 'test/loss': 4.102904319763184, 'test/num_examples': 10000}
I0404 21:12:27.343755 140115245111104 submission_runner.py:396] After eval at step 11458: RAM USED (GB) 147.174047744
I0404 21:12:27.353672 139937928152832 logging_writer.py:48] [11458] global_step=11458, preemption_count=0, score=4572.020691, test/accuracy=0.217700, test/loss=4.102904, test/num_examples=10000, total_duration=4960.245259, train/accuracy=0.315918, train/loss=3.480927, validation/accuracy=0.293080, validation/loss=3.605346, validation/num_examples=50000
I0404 21:12:27.453129 140115245111104 checkpoints.py:356] Saving checkpoint at step: 11458
I0404 21:12:28.224978 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_11458
I0404 21:12:28.239155 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_11458.
I0404 21:12:28.248035 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 11458: RAM USED (GB) 150.81826304
I0404 21:12:45.256882 139937911367424 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.47550737857818604, loss=5.423181533813477
I0404 21:13:25.266270 139937902974720 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6119597554206848, loss=5.246445655822754
I0404 21:14:06.131202 139937911367424 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5719960331916809, loss=5.397399425506592
I0404 21:14:46.933920 139937902974720 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4903937578201294, loss=6.301939964294434
I0404 21:15:27.521712 139937911367424 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5499882102012634, loss=5.32313871383667
I0404 21:16:08.271349 139937902974720 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5464813709259033, loss=5.039330005645752
I0404 21:16:48.961363 139937911367424 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5989195108413696, loss=4.9984564781188965
I0404 21:17:29.631030 139937902974720 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.4006443917751312, loss=6.430870532989502
I0404 21:18:10.149346 139937911367424 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5707573294639587, loss=5.034184455871582
I0404 21:18:50.926818 139937902974720 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5884870886802673, loss=5.082671165466309
I0404 21:19:28.474564 140115245111104 submission_runner.py:373] Before eval at step 12494: RAM USED (GB) 145.939693568
I0404 21:19:28.474821 140115245111104 spec.py:298] Evaluating on the training split.
I0404 21:19:43.439400 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 21:19:53.689509 140115245111104 spec.py:326] Evaluating on the test split.
I0404 21:19:55.350234 140115245111104 submission_runner.py:382] Time since start: 5408.11s, 	Step: 12494, 	{'train/accuracy': 0.34333983063697815, 'train/loss': 3.374774694442749, 'validation/accuracy': 0.31467998027801514, 'validation/loss': 3.5278663635253906, 'validation/num_examples': 50000, 'test/accuracy': 0.23840001225471497, 'test/loss': 4.049924850463867, 'test/num_examples': 10000}
I0404 21:19:55.350742 140115245111104 submission_runner.py:396] After eval at step 12494: RAM USED (GB) 152.63506432
I0404 21:19:55.366365 139937911367424 logging_writer.py:48] [12494] global_step=12494, preemption_count=0, score=4980.548687, test/accuracy=0.238400, test/loss=4.049925, test/num_examples=10000, total_duration=5408.113673, train/accuracy=0.343340, train/loss=3.374775, validation/accuracy=0.314680, validation/loss=3.527866, validation/num_examples=50000
I0404 21:19:55.470378 140115245111104 checkpoints.py:356] Saving checkpoint at step: 12494
I0404 21:19:56.423494 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_12494
I0404 21:19:56.440197 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_12494.
I0404 21:19:56.451486 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 12494: RAM USED (GB) 157.11092736
I0404 21:19:59.248186 139937902974720 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5345828533172607, loss=5.022611618041992
I0404 21:20:38.839677 139937693288192 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5574601888656616, loss=5.2571024894714355
I0404 21:21:19.302114 139937902974720 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.555192232131958, loss=5.392611026763916
I0404 21:21:59.968573 139937693288192 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5255725979804993, loss=5.364066123962402
I0404 21:22:40.735278 139937902974720 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5925731658935547, loss=4.933723449707031
I0404 21:23:21.578731 139937693288192 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6174477934837341, loss=5.018516540527344
I0404 21:24:02.044031 139937902974720 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.40930333733558655, loss=6.287103176116943
I0404 21:24:42.587476 139937693288192 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5611283779144287, loss=5.079723358154297
I0404 21:25:23.235575 139937902974720 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.4097896218299866, loss=6.315757751464844
I0404 21:26:03.846553 139937693288192 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5607348084449768, loss=4.937729835510254
I0404 21:26:44.812231 139937902974720 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6236648559570312, loss=5.433247089385986
I0404 21:26:56.777136 140115245111104 submission_runner.py:373] Before eval at step 13531: RAM USED (GB) 152.22730752
I0404 21:26:56.777406 140115245111104 spec.py:298] Evaluating on the training split.
I0404 21:27:11.480010 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 21:27:22.099423 140115245111104 spec.py:326] Evaluating on the test split.
I0404 21:27:23.754483 140115245111104 submission_runner.py:382] Time since start: 5856.42s, 	Step: 13531, 	{'train/accuracy': 0.36716794967651367, 'train/loss': 3.194481134414673, 'validation/accuracy': 0.33235999941825867, 'validation/loss': 3.366713762283325, 'validation/num_examples': 50000, 'test/accuracy': 0.24750001728534698, 'test/loss': 3.8934645652770996, 'test/num_examples': 10000}
I0404 21:27:23.755021 140115245111104 submission_runner.py:396] After eval at step 13531: RAM USED (GB) 159.679500288
I0404 21:27:23.768591 139937693288192 logging_writer.py:48] [13531] global_step=13531, preemption_count=0, score=5389.940274, test/accuracy=0.247500, test/loss=3.893465, test/num_examples=10000, total_duration=5856.417231, train/accuracy=0.367168, train/loss=3.194481, validation/accuracy=0.332360, validation/loss=3.366714, validation/num_examples=50000
I0404 21:27:23.868556 140115245111104 checkpoints.py:356] Saving checkpoint at step: 13531
I0404 21:27:24.670509 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_13531
I0404 21:27:24.685073 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_13531.
I0404 21:27:24.687074 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 13531: RAM USED (GB) 163.513970688
I0404 21:27:52.183735 139937902974720 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.568328320980072, loss=4.79029655456543
I0404 21:28:32.495118 139937684895488 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5131087303161621, loss=5.168626308441162
I0404 21:29:13.282425 139937902974720 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.4872327446937561, loss=5.280778884887695
I0404 21:29:53.926442 139937684895488 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.546620786190033, loss=4.875122547149658
I0404 21:30:33.940299 140115245111104 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 158.33452544
I0404 21:30:33.940614 140115245111104 spec.py:298] Evaluating on the training split.
I0404 21:30:48.950307 140115245111104 spec.py:310] Evaluating on the validation split.
I0404 21:30:59.355191 140115245111104 spec.py:326] Evaluating on the test split.
I0404 21:31:01.006063 140115245111104 submission_runner.py:382] Time since start: 6073.58s, 	Step: 14000, 	{'train/accuracy': 0.3751562535762787, 'train/loss': 3.1339433193206787, 'validation/accuracy': 0.3431999981403351, 'validation/loss': 3.2974131107330322, 'validation/num_examples': 50000, 'test/accuracy': 0.2669000029563904, 'test/loss': 3.821077585220337, 'test/num_examples': 10000}
I0404 21:31:01.006586 140115245111104 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 164.952408064
I0404 21:31:01.016258 139937902974720 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5573.778972, test/accuracy=0.266900, test/loss=3.821078, test/num_examples=10000, total_duration=6073.578899, train/accuracy=0.375156, train/loss=3.133943, validation/accuracy=0.343200, validation/loss=3.297413, validation/num_examples=50000
I0404 21:31:01.091322 140115245111104 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 21:31:01.842237 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_14000
I0404 21:31:01.855451 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_14000.
I0404 21:31:01.857321 140115245111104 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 168.459698176
I0404 21:31:01.865728 139937684895488 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5573.778972
I0404 21:31:01.947910 140115245111104 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 21:31:02.962894 140115245111104 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_14000
I0404 21:31:02.978202 140115245111104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_vit_jax/trial_1/checkpoint_14000.
I0404 21:31:03.797648 140115245111104 submission_runner.py:550] Tuning trial 1/1
I0404 21:31:03.799525 140115245111104 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0404 21:31:03.805858 140115245111104 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009960937313735485, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 45.24145150184631, 'total_duration': 45.32132697105408, 'global_step': 1, 'preemption_count': 0}), (1021, {'train/accuracy': 0.03251953050494194, 'train/loss': 6.124095439910889, 'validation/accuracy': 0.0321200005710125, 'validation/loss': 6.145279884338379, 'validation/num_examples': 50000, 'test/accuracy': 0.026500001549720764, 'test/loss': 6.233181953430176, 'test/num_examples': 10000, 'score': 458.2871959209442, 'total_duration': 512.5788495540619, 'global_step': 1021, 'preemption_count': 0}), (2072, {'train/accuracy': 0.0597851537168026, 'train/loss': 5.681266784667969, 'validation/accuracy': 0.05857999995350838, 'validation/loss': 5.708341121673584, 'validation/num_examples': 50000, 'test/accuracy': 0.04020000249147415, 'test/loss': 5.863155841827393, 'test/num_examples': 10000, 'score': 871.3971436023712, 'total_duration': 955.0865433216095, 'global_step': 2072, 'preemption_count': 0}), (3123, {'train/accuracy': 0.08498046547174454, 'train/loss': 5.371925354003906, 'validation/accuracy': 0.07961999624967575, 'validation/loss': 5.421719074249268, 'validation/num_examples': 50000, 'test/accuracy': 0.059700001031160355, 'test/loss': 5.625901699066162, 'test/num_examples': 10000, 'score': 1284.6551740169525, 'total_duration': 1397.5931324958801, 'global_step': 3123, 'preemption_count': 0}), (4173, {'train/accuracy': 0.10900390148162842, 'train/loss': 5.115922451019287, 'validation/accuracy': 0.09737999737262726, 'validation/loss': 5.200913429260254, 'validation/num_examples': 50000, 'test/accuracy': 0.07110000401735306, 'test/loss': 5.451022148132324, 'test/num_examples': 10000, 'score': 1697.931384563446, 'total_duration': 1839.9918162822723, 'global_step': 4173, 'preemption_count': 0}), (5217, {'train/accuracy': 0.13236327469348907, 'train/loss': 4.834392070770264, 'validation/accuracy': 0.1211399957537651, 'validation/loss': 4.906247138977051, 'validation/num_examples': 50000, 'test/accuracy': 0.09740000218153, 'test/loss': 5.192223072052002, 'test/num_examples': 10000, 'score': 2109.1779079437256, 'total_duration': 2283.2147784233093, 'global_step': 5217, 'preemption_count': 0}), (6261, {'train/accuracy': 0.16523437201976776, 'train/loss': 4.550891876220703, 'validation/accuracy': 0.1513800024986267, 'validation/loss': 4.641767501831055, 'validation/num_examples': 50000, 'test/accuracy': 0.1153000071644783, 'test/loss': 4.969973564147949, 'test/num_examples': 10000, 'score': 2520.8297340869904, 'total_duration': 2728.1749205589294, 'global_step': 6261, 'preemption_count': 0}), (7303, {'train/accuracy': 0.19496093690395355, 'train/loss': 4.4034247398376465, 'validation/accuracy': 0.17945998907089233, 'validation/loss': 4.50022029876709, 'validation/num_examples': 50000, 'test/accuracy': 0.13370001316070557, 'test/loss': 4.853652000427246, 'test/num_examples': 10000, 'score': 2931.8468289375305, 'total_duration': 3173.823881626129, 'global_step': 7303, 'preemption_count': 0}), (8344, {'train/accuracy': 0.21455077826976776, 'train/loss': 4.184172630310059, 'validation/accuracy': 0.19679999351501465, 'validation/loss': 4.278113842010498, 'validation/num_examples': 50000, 'test/accuracy': 0.1542000025510788, 'test/loss': 4.65147590637207, 'test/num_examples': 10000, 'score': 3342.3821136951447, 'total_duration': 3619.626693248749, 'global_step': 8344, 'preemption_count': 0}), (9382, {'train/accuracy': 0.26363280415534973, 'train/loss': 3.823559522628784, 'validation/accuracy': 0.2380799949169159, 'validation/loss': 3.9775593280792236, 'validation/num_examples': 50000, 'test/accuracy': 0.18330000340938568, 'test/loss': 4.415868282318115, 'test/num_examples': 10000, 'score': 3752.6549417972565, 'total_duration': 4065.8085536956787, 'global_step': 9382, 'preemption_count': 0}), (10418, {'train/accuracy': 0.2916210889816284, 'train/loss': 3.628342866897583, 'validation/accuracy': 0.2687999904155731, 'validation/loss': 3.7524280548095703, 'validation/num_examples': 50000, 'test/accuracy': 0.20170001685619354, 'test/loss': 4.2375078201293945, 'test/num_examples': 10000, 'score': 4162.046027421951, 'total_duration': 4512.62708067894, 'global_step': 10418, 'preemption_count': 0}), (11458, {'train/accuracy': 0.31591796875, 'train/loss': 3.480926990509033, 'validation/accuracy': 0.29308000206947327, 'validation/loss': 3.6053459644317627, 'validation/num_examples': 50000, 'test/accuracy': 0.21770000457763672, 'test/loss': 4.102904319763184, 'test/num_examples': 10000, 'score': 4572.020691156387, 'total_duration': 4960.245259284973, 'global_step': 11458, 'preemption_count': 0}), (12494, {'train/accuracy': 0.34333983063697815, 'train/loss': 3.374774694442749, 'validation/accuracy': 0.31467998027801514, 'validation/loss': 3.5278663635253906, 'validation/num_examples': 50000, 'test/accuracy': 0.23840001225471497, 'test/loss': 4.049924850463867, 'test/num_examples': 10000, 'score': 4980.548687458038, 'total_duration': 5408.113673448563, 'global_step': 12494, 'preemption_count': 0}), (13531, {'train/accuracy': 0.36716794967651367, 'train/loss': 3.194481134414673, 'validation/accuracy': 0.33235999941825867, 'validation/loss': 3.366713762283325, 'validation/num_examples': 50000, 'test/accuracy': 0.24750001728534698, 'test/loss': 3.8934645652770996, 'test/num_examples': 10000, 'score': 5389.940274477005, 'total_duration': 5856.417230844498, 'global_step': 13531, 'preemption_count': 0}), (14000, {'train/accuracy': 0.3751562535762787, 'train/loss': 3.1339433193206787, 'validation/accuracy': 0.3431999981403351, 'validation/loss': 3.2974131107330322, 'validation/num_examples': 50000, 'test/accuracy': 0.2669000029563904, 'test/loss': 3.821077585220337, 'test/num_examples': 10000, 'score': 5573.778972387314, 'total_duration': 6073.578898668289, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0404 21:31:03.805967 140115245111104 submission_runner.py:553] Timing: 5573.778972387314
I0404 21:31:03.806014 140115245111104 submission_runner.py:554] ====================
I0404 21:31:03.806150 140115245111104 submission_runner.py:613] Final imagenet_vit score: 5573.778972387314
