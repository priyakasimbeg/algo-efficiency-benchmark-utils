I0404 22:29:21.800136 140401714796352 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw_v2/wmt_jax.
I0404 22:29:21.843987 140401714796352 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 22:29:22.725376 140401714796352 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0404 22:29:22.725986 140401714796352 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 22:29:22.729367 140401714796352 submission_runner.py:511] Using RNG seed 4026417341
I0404 22:29:23.962299 140401714796352 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 22:29:23.962489 140401714796352 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_adamw_v2/wmt_jax/trial_1.
I0404 22:29:23.962672 140401714796352 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/hparams.json.
I0404 22:29:24.085051 140401714796352 submission_runner.py:230] Starting train once: RAM USED (GB) 4.289380352
I0404 22:29:24.085217 140401714796352 submission_runner.py:231] Initializing dataset.
I0404 22:29:24.093479 140401714796352 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:29:24.096829 140401714796352 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:29:24.096955 140401714796352 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:29:24.170738 140401714796352 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:29:25.999482 140401714796352 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.390772736
I0404 22:29:25.999678 140401714796352 submission_runner.py:240] Initializing model.
I0404 22:29:37.914426 140401714796352 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.504721408
I0404 22:29:37.914624 140401714796352 submission_runner.py:252] Initializing optimizer.
I0404 22:29:38.953629 140401714796352 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.505081856
I0404 22:29:38.953814 140401714796352 submission_runner.py:261] Initializing metrics bundle.
I0404 22:29:38.953863 140401714796352 submission_runner.py:276] Initializing checkpoint and logger.
I0404 22:29:38.954811 140401714796352 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw_v2/wmt_jax/trial_1 with prefix checkpoint_
I0404 22:29:38.955181 140401714796352 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0404 22:29:38.955286 140401714796352 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0404 22:29:39.942998 140401714796352 submission_runner.py:297] Saving meta data to /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/meta_data_0.json.
I0404 22:29:39.944095 140401714796352 submission_runner.py:300] Saving flags to /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/flags_0.json.
I0404 22:29:39.948007 140401714796352 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.503148544
I0404 22:29:39.948292 140401714796352 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.503148544
I0404 22:29:39.948394 140401714796352 submission_runner.py:313] Starting training loop.
I0404 22:29:40.704314 140401714796352 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.638754816
I0404 22:30:12.855028 140225732925184 logging_writer.py:48] [0] global_step=0, grad_norm=5.728671550750732, loss=11.115378379821777
I0404 22:30:12.867931 140401714796352 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 11.290783744
I0404 22:30:12.868164 140401714796352 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 11.290783744
I0404 22:30:12.868237 140401714796352 spec.py:298] Evaluating on the training split.
I0404 22:30:12.870773 140401714796352 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:30:12.873192 140401714796352 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:30:12.873297 140401714796352 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:30:12.903234 140401714796352 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:30:21.007429 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 22:35:27.779275 140401714796352 spec.py:310] Evaluating on the validation split.
I0404 22:35:27.782728 140401714796352 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:35:27.786063 140401714796352 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:35:27.786171 140401714796352 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:35:27.814813 140401714796352 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:35:35.152960 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 22:40:34.338015 140401714796352 spec.py:326] Evaluating on the test split.
I0404 22:40:34.340272 140401714796352 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:40:34.342682 140401714796352 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:40:34.342809 140401714796352 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:40:34.378684 140401714796352 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:40:41.076926 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 22:45:34.585876 140401714796352 submission_runner.py:382] Time since start: 32.92s, 	Step: 1, 	{'train/accuracy': 0.0005655586719512939, 'train/loss': 11.104360580444336, 'train/bleu': 9.507455693595377e-11, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.116703033447266, 'validation/bleu': 9.40460522687322e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.122773170471191, 'test/bleu': 4.812487968740233e-10, 'test/num_examples': 3003}
I0404 22:45:34.586308 140401714796352 submission_runner.py:396] After eval at step 1: RAM USED (GB) 11.729195008
I0404 22:45:34.593584 140214424033024 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=32.707350, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.122773, test/num_examples=3003, total_duration=32.919915, train/accuracy=0.000566, train/bleu=0.000000, train/loss=11.104361, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.116703, validation/num_examples=3000
I0404 22:45:35.656882 140401714796352 checkpoints.py:356] Saving checkpoint at step: 1
I0404 22:45:39.475789 140401714796352 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_1
I0404 22:45:39.480114 140401714796352 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_1.
I0404 22:45:39.485075 140401714796352 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 12.859805696
I0404 22:45:39.487297 140401714796352 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 12.859805696
I0404 22:45:39.563467 140401714796352 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 12.859805696
I0404 22:46:15.597618 140214432425728 logging_writer.py:48] [100] global_step=100, grad_norm=0.2465497851371765, loss=8.575089454650879
I0404 22:46:51.762241 140214541530880 logging_writer.py:48] [200] global_step=200, grad_norm=0.5811465382575989, loss=7.953495025634766
I0404 22:47:27.958945 140214432425728 logging_writer.py:48] [300] global_step=300, grad_norm=0.6327559351921082, loss=7.547781944274902
I0404 22:48:04.179119 140214541530880 logging_writer.py:48] [400] global_step=400, grad_norm=0.4677281379699707, loss=7.183966636657715
I0404 22:48:40.364008 140214432425728 logging_writer.py:48] [500] global_step=500, grad_norm=0.5757113099098206, loss=6.8867645263671875
I0404 22:49:16.509205 140214541530880 logging_writer.py:48] [600] global_step=600, grad_norm=0.6732968091964722, loss=6.6240739822387695
I0404 22:49:52.719991 140214432425728 logging_writer.py:48] [700] global_step=700, grad_norm=0.5864087343215942, loss=6.378825664520264
I0404 22:50:28.899180 140214541530880 logging_writer.py:48] [800] global_step=800, grad_norm=0.5425356030464172, loss=6.1568603515625
I0404 22:51:05.124412 140214432425728 logging_writer.py:48] [900] global_step=900, grad_norm=0.5227152109146118, loss=6.00752592086792
I0404 22:51:41.322616 140214541530880 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.38629716634750366, loss=5.921675682067871
I0404 22:52:17.550417 140214432425728 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5121791362762451, loss=5.664595603942871
I0404 22:52:53.689916 140214541530880 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6386410593986511, loss=5.491568088531494
I0404 22:53:29.905981 140214432425728 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8377968072891235, loss=5.49737024307251
I0404 22:54:06.106820 140214541530880 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.569983184337616, loss=5.280455589294434
I0404 22:54:42.303189 140214432425728 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5761600136756897, loss=5.111480236053467
I0404 22:55:18.536135 140214541530880 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.591925323009491, loss=4.918702602386475
I0404 22:55:54.706752 140214432425728 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6440789103507996, loss=4.7783589363098145
I0404 22:56:30.772784 140214541530880 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.036895107477903366, loss=7.543669700622559
I0404 22:57:06.789967 140214432425728 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.12251817435026169, loss=7.32667350769043
I0404 22:57:42.891618 140214541530880 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.25697216391563416, loss=7.023595809936523
I0404 22:58:18.995655 140214432425728 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.16053548455238342, loss=6.742800235748291
I0404 22:58:55.075865 140214541530880 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.23707215487957, loss=6.593177795410156
I0404 22:59:31.157582 140214432425728 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.26093214750289917, loss=6.440305233001709
I0404 22:59:39.561159 140401714796352 submission_runner.py:373] Before eval at step 2325: RAM USED (GB) 12.044824576
I0404 22:59:39.561354 140401714796352 spec.py:298] Evaluating on the training split.
I0404 22:59:42.543489 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 23:02:03.424223 140401714796352 spec.py:310] Evaluating on the validation split.
I0404 23:02:06.067705 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 23:04:23.641133 140401714796352 spec.py:326] Evaluating on the test split.
I0404 23:04:26.329621 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 23:06:43.784050 140401714796352 submission_runner.py:382] Time since start: 1799.61s, 	Step: 2325, 	{'train/accuracy': 0.184852734208107, 'train/loss': 5.7469072341918945, 'train/bleu': 0.0790542756797296, 'validation/accuracy': 0.1674002856016159, 'validation/loss': 5.932977199554443, 'validation/bleu': 0.02960233661475591, 'validation/num_examples': 3000, 'test/accuracy': 0.1592353731393814, 'test/loss': 6.178826332092285, 'test/bleu': 0.037630649951686156, 'test/num_examples': 3003}
I0404 23:06:43.784523 140401714796352 submission_runner.py:396] After eval at step 2325: RAM USED (GB) 12.331814912
I0404 23:06:43.792112 140214541530880 logging_writer.py:48] [2325] global_step=2325, preemption_count=0, score=868.841897, test/accuracy=0.159235, test/bleu=0.037631, test/loss=6.178826, test/num_examples=3003, total_duration=1799.612119, train/accuracy=0.184853, train/bleu=0.079054, train/loss=5.746907, validation/accuracy=0.167400, validation/bleu=0.029602, validation/loss=5.932977, validation/num_examples=3000
I0404 23:06:44.855367 140401714796352 checkpoints.py:356] Saving checkpoint at step: 2325
I0404 23:06:48.674469 140401714796352 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_2325
I0404 23:06:48.678753 140401714796352 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_2325.
I0404 23:06:48.683478 140401714796352 submission_runner.py:416] After logging and checkpointing eval at step 2325: RAM USED (GB) 13.49650432
I0404 23:07:16.066314 140214432425728 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.2694074213504791, loss=6.291687488555908
I0404 23:07:52.163924 140214516352768 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.28679168224334717, loss=6.18524169921875
I0404 23:08:28.273501 140214432425728 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.2205527126789093, loss=6.030285358428955
I0404 23:09:04.346160 140214516352768 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.2308908849954605, loss=6.218339443206787
I0404 23:09:40.426903 140214432425728 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.2536984384059906, loss=5.964775562286377
I0404 23:10:16.529835 140214516352768 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.30768507719039917, loss=5.912650108337402
I0404 23:10:52.637565 140214432425728 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.441530227661133, loss=6.512454986572266
I0404 23:11:28.769056 140214516352768 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.1753913015127182, loss=5.842605113983154
I0404 23:12:04.832300 140214432425728 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.19692672789096832, loss=5.723252773284912
I0404 23:12:40.909067 140214516352768 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.33041176199913025, loss=5.756609916687012
I0404 23:13:17.005515 140214432425728 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.22628255188465118, loss=5.746016502380371
I0404 23:13:53.121222 140214516352768 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.18547460436820984, loss=5.639188289642334
I0404 23:14:29.176651 140214432425728 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.19088517129421234, loss=5.58024787902832
I0404 23:15:05.260413 140214516352768 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.19893860816955566, loss=5.546370983123779
I0404 23:15:41.401666 140214432425728 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.25810638070106506, loss=5.533876419067383
I0404 23:16:17.464833 140214516352768 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.25691869854927063, loss=5.495918273925781
I0404 23:16:53.579328 140214432425728 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.21864768862724304, loss=5.441582202911377
I0404 23:17:29.692830 140214516352768 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.22570262849330902, loss=5.396412372589111
I0404 23:18:05.753980 140214432425728 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.19980302453041077, loss=5.352582931518555
I0404 23:18:41.846144 140214516352768 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.21627095341682434, loss=5.294612407684326
I0404 23:19:18.015890 140214432425728 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.23794904351234436, loss=5.278302192687988
I0404 23:19:54.177433 140214516352768 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.2009279578924179, loss=5.193122863769531
I0404 23:20:30.384918 140214432425728 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.22092430293560028, loss=5.18267822265625
I0404 23:20:48.873800 140401714796352 submission_runner.py:373] Before eval at step 4653: RAM USED (GB) 12.501757952
I0404 23:20:48.874039 140401714796352 spec.py:298] Evaluating on the training split.
I0404 23:20:51.880789 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 23:25:08.008258 140401714796352 spec.py:310] Evaluating on the validation split.
I0404 23:25:10.675131 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 23:29:18.484448 140401714796352 spec.py:326] Evaluating on the test split.
I0404 23:29:21.201044 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 23:33:39.462574 140401714796352 submission_runner.py:382] Time since start: 3068.92s, 	Step: 4653, 	{'train/accuracy': 0.321170449256897, 'train/loss': 4.341742038726807, 'train/bleu': 3.5664878651699015, 'validation/accuracy': 0.2866796553134918, 'validation/loss': 4.681982040405273, 'validation/bleu': 1.0695757339997194, 'validation/num_examples': 3000, 'test/accuracy': 0.2659578323364258, 'test/loss': 4.943343639373779, 'test/bleu': 0.798329278674083, 'test/num_examples': 3003}
I0404 23:33:39.463033 140401714796352 submission_runner.py:396] After eval at step 4653: RAM USED (GB) 12.647522304
I0404 23:33:39.470456 140214516352768 logging_writer.py:48] [4653] global_step=4653, preemption_count=0, score=1705.145164, test/accuracy=0.265958, test/bleu=0.798329, test/loss=4.943344, test/num_examples=3003, total_duration=3068.924473, train/accuracy=0.321170, train/bleu=3.566488, train/loss=4.341742, validation/accuracy=0.286680, validation/bleu=1.069576, validation/loss=4.681982, validation/num_examples=3000
I0404 23:33:40.531337 140401714796352 checkpoints.py:356] Saving checkpoint at step: 4653
I0404 23:33:44.282501 140401714796352 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_4653
I0404 23:33:44.286886 140401714796352 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_4653.
I0404 23:33:44.291592 140401714796352 submission_runner.py:416] After logging and checkpointing eval at step 4653: RAM USED (GB) 13.81142528
I0404 23:34:01.621202 140214432425728 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.2481268048286438, loss=5.101943492889404
I0404 23:34:37.750258 140214499567360 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.19384102523326874, loss=5.0841898918151855
I0404 23:35:13.946333 140214432425728 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.23295600712299347, loss=4.949542045593262
I0404 23:35:50.082866 140214499567360 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.2627479135990143, loss=4.82829475402832
I0404 23:36:26.212064 140214432425728 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.31561651825904846, loss=4.829617977142334
I0404 23:37:02.387433 140214499567360 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.3387906551361084, loss=4.7636260986328125
I0404 23:37:38.584187 140214432425728 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.3847786486148834, loss=4.801228046417236
I0404 23:38:14.726948 140214499567360 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5021029710769653, loss=4.833890438079834
I0404 23:38:50.950963 140214432425728 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6330826282501221, loss=4.852077484130859
I0404 23:39:27.109560 140214499567360 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.5462985038757324, loss=4.848690509796143
I0404 23:40:03.270767 140214432425728 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.9361037611961365, loss=4.868155002593994
I0404 23:40:39.409738 140214499567360 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9281690716743469, loss=4.784499168395996
I0404 23:41:15.572291 140214432425728 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.6088000535964966, loss=4.876323699951172
I0404 23:41:51.672030 140214499567360 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.4845526218414307, loss=4.885757923126221
I0404 23:42:27.859884 140214432425728 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.826383113861084, loss=4.980282306671143
I0404 23:43:04.000325 140214499567360 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.868109941482544, loss=4.897537708282471
I0404 23:43:40.133823 140214432425728 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.3876185417175293, loss=4.8790364265441895
I0404 23:44:16.260681 140214499567360 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.786447525024414, loss=4.878023147583008
I0404 23:44:52.421324 140214432425728 logging_writer.py:48] [6500] global_step=6500, grad_norm=6.156280517578125, loss=4.846961498260498
I0404 23:45:28.506493 140214499567360 logging_writer.py:48] [6600] global_step=6600, grad_norm=4.018862247467041, loss=4.941187858581543
I0404 23:46:04.674384 140214432425728 logging_writer.py:48] [6700] global_step=6700, grad_norm=5.173819541931152, loss=4.942653179168701
I0404 23:46:40.814883 140214499567360 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.113408088684082, loss=4.924015998840332
I0404 23:47:16.954239 140214432425728 logging_writer.py:48] [6900] global_step=6900, grad_norm=6.879134178161621, loss=4.964710712432861
I0404 23:47:44.474898 140401714796352 submission_runner.py:373] Before eval at step 6978: RAM USED (GB) 12.8759808
I0404 23:47:44.475172 140401714796352 spec.py:298] Evaluating on the training split.
I0404 23:47:47.467655 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 23:51:18.834838 140401714796352 spec.py:310] Evaluating on the validation split.
I0404 23:51:21.485249 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 23:55:12.348388 140401714796352 spec.py:326] Evaluating on the test split.
I0404 23:55:15.051315 140401714796352 workload.py:179] Translating evaluation dataset.
I0404 23:58:55.082135 140401714796352 submission_runner.py:382] Time since start: 4684.53s, 	Step: 6978, 	{'train/accuracy': 0.34740129113197327, 'train/loss': 4.0070695877075195, 'train/bleu': 6.061875817650749, 'validation/accuracy': 0.31504878401756287, 'validation/loss': 4.329648494720459, 'validation/bleu': 2.9817893789734, 'validation/num_examples': 3000, 'test/accuracy': 0.2927546501159668, 'test/loss': 4.593966007232666, 'test/bleu': 1.9938406650362699, 'test/num_examples': 3003}
I0404 23:58:55.082594 140401714796352 submission_runner.py:396] After eval at step 6978: RAM USED (GB) 12.993466368
I0404 23:58:55.090142 140214499567360 logging_writer.py:48] [6978] global_step=6978, preemption_count=0, score=2541.636001, test/accuracy=0.292755, test/bleu=1.993841, test/loss=4.593966, test/num_examples=3003, total_duration=4684.525838, train/accuracy=0.347401, train/bleu=6.061876, train/loss=4.007070, validation/accuracy=0.315049, validation/bleu=2.981789, validation/loss=4.329648, validation/num_examples=3000
I0404 23:58:56.158301 140401714796352 checkpoints.py:356] Saving checkpoint at step: 6978
I0404 23:58:59.882284 140401714796352 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_6978
I0404 23:58:59.886676 140401714796352 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_6978.
I0404 23:58:59.891348 140401714796352 submission_runner.py:416] After logging and checkpointing eval at step 6978: RAM USED (GB) 14.157991936
I0404 23:59:08.192487 140214432425728 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.7985358238220215, loss=4.919771194458008
I0404 23:59:44.309716 140214491174656 logging_writer.py:48] [7100] global_step=7100, grad_norm=5.02923059463501, loss=4.965001106262207
I0405 00:00:20.384969 140214432425728 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.9444177150726318, loss=4.897961616516113
I0405 00:00:56.497531 140214491174656 logging_writer.py:48] [7300] global_step=7300, grad_norm=5.443783760070801, loss=4.8232102394104
I0405 00:01:32.623691 140214432425728 logging_writer.py:48] [7400] global_step=7400, grad_norm=11.566375732421875, loss=4.944334983825684
I0405 00:02:08.698127 140214491174656 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.30397367477417, loss=4.926435947418213
I0405 00:02:44.754655 140214432425728 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.7477855682373047, loss=4.9135918617248535
I0405 00:03:20.881562 140214491174656 logging_writer.py:48] [7700] global_step=7700, grad_norm=4.214468955993652, loss=4.95851993560791
I0405 00:03:56.999390 140214432425728 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.6633124351501465, loss=4.917464733123779
I0405 00:04:33.059487 140214491174656 logging_writer.py:48] [7900] global_step=7900, grad_norm=10.255268096923828, loss=4.949105739593506
I0405 00:05:09.181963 140214432425728 logging_writer.py:48] [8000] global_step=8000, grad_norm=8.97593879699707, loss=5.0040130615234375
I0405 00:05:45.295218 140214491174656 logging_writer.py:48] [8100] global_step=8100, grad_norm=23.200820922851562, loss=5.014435291290283
I0405 00:06:21.352598 140214432425728 logging_writer.py:48] [8200] global_step=8200, grad_norm=5.478100299835205, loss=5.037618637084961
I0405 00:06:57.437803 140214491174656 logging_writer.py:48] [8300] global_step=8300, grad_norm=10.30909252166748, loss=4.936572551727295
I0405 00:07:33.530986 140214432425728 logging_writer.py:48] [8400] global_step=8400, grad_norm=7.445274353027344, loss=4.94317626953125
I0405 00:08:09.566325 140214491174656 logging_writer.py:48] [8500] global_step=8500, grad_norm=6.790441036224365, loss=5.008295059204102
I0405 00:08:45.661216 140214432425728 logging_writer.py:48] [8600] global_step=8600, grad_norm=5.001980781555176, loss=5.015923500061035
I0405 00:09:21.746256 140214491174656 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.431749105453491, loss=4.974932670593262
I0405 00:09:57.801461 140214432425728 logging_writer.py:48] [8800] global_step=8800, grad_norm=4.359619617462158, loss=4.987481117248535
I0405 00:10:33.860649 140214491174656 logging_writer.py:48] [8900] global_step=8900, grad_norm=5.089563846588135, loss=4.977639675140381
I0405 00:11:09.926626 140214432425728 logging_writer.py:48] [9000] global_step=9000, grad_norm=8.17574691772461, loss=4.944586277008057
I0405 00:11:46.007621 140214491174656 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.6936607360839844, loss=5.049348831176758
I0405 00:12:22.120242 140214432425728 logging_writer.py:48] [9200] global_step=9200, grad_norm=23.537120819091797, loss=5.0559983253479
I0405 00:12:58.176684 140214491174656 logging_writer.py:48] [9300] global_step=9300, grad_norm=10.450961112976074, loss=5.027817726135254
I0405 00:13:00.066739 140401714796352 submission_runner.py:373] Before eval at step 9307: RAM USED (GB) 13.25789184
I0405 00:13:00.066958 140401714796352 spec.py:298] Evaluating on the training split.
I0405 00:13:03.079113 140401714796352 workload.py:179] Translating evaluation dataset.
I0405 00:16:57.664541 140401714796352 spec.py:310] Evaluating on the validation split.
I0405 00:17:00.320668 140401714796352 workload.py:179] Translating evaluation dataset.
I0405 00:20:38.511742 140401714796352 spec.py:326] Evaluating on the test split.
I0405 00:20:41.209928 140401714796352 workload.py:179] Translating evaluation dataset.
I0405 00:24:22.278953 140401714796352 submission_runner.py:382] Time since start: 6200.12s, 	Step: 9307, 	{'train/accuracy': 0.3317156732082367, 'train/loss': 4.085575580596924, 'train/bleu': 4.325835321939934, 'validation/accuracy': 0.2969709038734436, 'validation/loss': 4.455580711364746, 'validation/bleu': 1.9713941849690173, 'validation/num_examples': 3000, 'test/accuracy': 0.279832661151886, 'test/loss': 4.702524662017822, 'test/bleu': 1.441659402291616, 'test/num_examples': 3003}
I0405 00:24:22.279462 140401714796352 submission_runner.py:396] After eval at step 9307: RAM USED (GB) 13.301317632
I0405 00:24:22.287736 140214432425728 logging_writer.py:48] [9307] global_step=9307, preemption_count=0, score=3378.055519, test/accuracy=0.279833, test/bleu=1.441659, test/loss=4.702525, test/num_examples=3003, total_duration=6200.117527, train/accuracy=0.331716, train/bleu=4.325835, train/loss=4.085576, validation/accuracy=0.296971, validation/bleu=1.971394, validation/loss=4.455581, validation/num_examples=3000
I0405 00:24:23.352535 140401714796352 checkpoints.py:356] Saving checkpoint at step: 9307
I0405 00:24:27.070882 140401714796352 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_9307
I0405 00:24:27.075504 140401714796352 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_9307.
I0405 00:24:27.080305 140401714796352 submission_runner.py:416] After logging and checkpointing eval at step 9307: RAM USED (GB) 14.465232896
I0405 00:25:00.862687 140214491174656 logging_writer.py:48] [9400] global_step=9400, grad_norm=6.338865756988525, loss=4.919051170349121
I0405 00:25:36.972705 140214482781952 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.7897567749023438, loss=4.889074325561523
I0405 00:26:13.056686 140214491174656 logging_writer.py:48] [9600] global_step=9600, grad_norm=11.176986694335938, loss=4.910864353179932
I0405 00:26:49.079570 140214482781952 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.502671957015991, loss=4.942236423492432
I0405 00:27:25.179110 140214491174656 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.3716883659362793, loss=4.96035623550415
I0405 00:28:01.225667 140214482781952 logging_writer.py:48] [9900] global_step=9900, grad_norm=11.653450012207031, loss=4.907111644744873
I0405 00:28:36.645056 140401714796352 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 13.329444864
I0405 00:28:36.645270 140401714796352 spec.py:298] Evaluating on the training split.
I0405 00:28:39.641146 140401714796352 workload.py:179] Translating evaluation dataset.
I0405 00:33:26.842640 140401714796352 spec.py:310] Evaluating on the validation split.
I0405 00:33:29.489438 140401714796352 workload.py:179] Translating evaluation dataset.
I0405 00:38:07.759066 140401714796352 spec.py:326] Evaluating on the test split.
I0405 00:38:10.470107 140401714796352 workload.py:179] Translating evaluation dataset.
I0405 00:42:51.117102 140401714796352 submission_runner.py:382] Time since start: 7136.70s, 	Step: 10000, 	{'train/accuracy': 0.3258500397205353, 'train/loss': 4.158332347869873, 'train/bleu': 4.760041689135564, 'validation/accuracy': 0.2918500602245331, 'validation/loss': 4.536773204803467, 'validation/bleu': 1.9967251576700007, 'validation/num_examples': 3000, 'test/accuracy': 0.27329033613204956, 'test/loss': 4.773197650909424, 'test/bleu': 1.3701429805572418, 'test/num_examples': 3003}
I0405 00:42:51.117660 140401714796352 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 13.369225216
I0405 00:42:51.126765 140214491174656 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3626.531049, test/accuracy=0.273290, test/bleu=1.370143, test/loss=4.773198, test/num_examples=3003, total_duration=7136.696003, train/accuracy=0.325850, train/bleu=4.760042, train/loss=4.158332, validation/accuracy=0.291850, validation/bleu=1.996725, validation/loss=4.536773, validation/num_examples=3000
I0405 00:42:52.451732 140401714796352 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 00:42:56.953106 140401714796352 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_10000
I0405 00:42:56.957323 140401714796352 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_10000.
I0405 00:42:56.962687 140401714796352 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 14.538371072
I0405 00:42:56.970113 140214482781952 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3626.531049
I0405 00:42:57.561725 140401714796352 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 00:43:03.808999 140401714796352 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_10000
I0405 00:43:03.813116 140401714796352 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/wmt_jax/trial_1/checkpoint_10000.
I0405 00:43:03.907746 140401714796352 submission_runner.py:550] Tuning trial 1/1
I0405 00:43:03.908030 140401714796352 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0405 00:43:03.909556 140401714796352 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005655586719512939, 'train/loss': 11.104360580444336, 'train/bleu': 9.507455693595377e-11, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.116703033447266, 'validation/bleu': 9.40460522687322e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.122773170471191, 'test/bleu': 4.812487968740233e-10, 'test/num_examples': 3003, 'score': 32.7073495388031, 'total_duration': 32.919915199279785, 'global_step': 1, 'preemption_count': 0}), (2325, {'train/accuracy': 0.184852734208107, 'train/loss': 5.7469072341918945, 'train/bleu': 0.0790542756797296, 'validation/accuracy': 0.1674002856016159, 'validation/loss': 5.932977199554443, 'validation/bleu': 0.02960233661475591, 'validation/num_examples': 3000, 'test/accuracy': 0.1592353731393814, 'test/loss': 6.178826332092285, 'test/bleu': 0.037630649951686156, 'test/num_examples': 3003, 'score': 868.8418967723846, 'total_duration': 1799.6121189594269, 'global_step': 2325, 'preemption_count': 0}), (4653, {'train/accuracy': 0.321170449256897, 'train/loss': 4.341742038726807, 'train/bleu': 3.5664878651699015, 'validation/accuracy': 0.2866796553134918, 'validation/loss': 4.681982040405273, 'validation/bleu': 1.0695757339997194, 'validation/num_examples': 3000, 'test/accuracy': 0.2659578323364258, 'test/loss': 4.943343639373779, 'test/bleu': 0.798329278674083, 'test/num_examples': 3003, 'score': 1705.145164489746, 'total_duration': 3068.924473285675, 'global_step': 4653, 'preemption_count': 0}), (6978, {'train/accuracy': 0.34740129113197327, 'train/loss': 4.0070695877075195, 'train/bleu': 6.061875817650749, 'validation/accuracy': 0.31504878401756287, 'validation/loss': 4.329648494720459, 'validation/bleu': 2.9817893789734, 'validation/num_examples': 3000, 'test/accuracy': 0.2927546501159668, 'test/loss': 4.593966007232666, 'test/bleu': 1.9938406650362699, 'test/num_examples': 3003, 'score': 2541.6360008716583, 'total_duration': 4684.525837898254, 'global_step': 6978, 'preemption_count': 0}), (9307, {'train/accuracy': 0.3317156732082367, 'train/loss': 4.085575580596924, 'train/bleu': 4.325835321939934, 'validation/accuracy': 0.2969709038734436, 'validation/loss': 4.455580711364746, 'validation/bleu': 1.9713941849690173, 'validation/num_examples': 3000, 'test/accuracy': 0.279832661151886, 'test/loss': 4.702524662017822, 'test/bleu': 1.441659402291616, 'test/num_examples': 3003, 'score': 3378.055519104004, 'total_duration': 6200.117527484894, 'global_step': 9307, 'preemption_count': 0}), (10000, {'train/accuracy': 0.3258500397205353, 'train/loss': 4.158332347869873, 'train/bleu': 4.760041689135564, 'validation/accuracy': 0.2918500602245331, 'validation/loss': 4.536773204803467, 'validation/bleu': 1.9967251576700007, 'validation/num_examples': 3000, 'test/accuracy': 0.27329033613204956, 'test/loss': 4.773197650909424, 'test/bleu': 1.3701429805572418, 'test/num_examples': 3003, 'score': 3626.5310492515564, 'total_duration': 7136.696002960205, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0405 00:43:03.909715 140401714796352 submission_runner.py:553] Timing: 3626.5310492515564
I0405 00:43:03.909760 140401714796352 submission_runner.py:554] ====================
I0405 00:43:03.909853 140401714796352 submission_runner.py:613] Final wmt score: 3626.5310492515564
