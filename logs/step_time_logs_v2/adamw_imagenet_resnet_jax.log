I0404 18:23:28.263481 139700680906560 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax.
I0404 18:23:28.309264 139700680906560 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 18:23:29.265836 139700680906560 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0404 18:23:29.266570 139700680906560 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 18:23:29.270031 139700680906560 submission_runner.py:511] Using RNG seed 3202022430
I0404 18:23:30.633169 139700680906560 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 18:23:30.633355 139700680906560 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1.
I0404 18:23:30.633541 139700680906560 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/hparams.json.
I0404 18:23:30.753643 139700680906560 submission_runner.py:230] Starting train once: RAM USED (GB) 4.148662272
I0404 18:23:30.753793 139700680906560 submission_runner.py:231] Initializing dataset.
I0404 18:23:30.764419 139700680906560 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:23:30.771147 139700680906560 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 18:23:30.771251 139700680906560 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 18:23:30.990362 139700680906560 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:23:31.909684 139700680906560 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.218957824
I0404 18:23:31.909850 139700680906560 submission_runner.py:240] Initializing model.
I0404 18:23:43.034899 139700680906560 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.32473088
I0404 18:23:43.035076 139700680906560 submission_runner.py:252] Initializing optimizer.
I0404 18:23:44.150079 139700680906560 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.325541888
I0404 18:23:44.150246 139700680906560 submission_runner.py:261] Initializing metrics bundle.
I0404 18:23:44.150297 139700680906560 submission_runner.py:276] Initializing checkpoint and logger.
I0404 18:23:44.151238 139700680906560 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0404 18:23:44.895912 139700680906560 submission_runner.py:297] Saving meta data to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0404 18:23:44.896774 139700680906560 submission_runner.py:300] Saving flags to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/flags_0.json.
I0404 18:23:44.899635 139700680906560 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.321716224
I0404 18:23:44.899821 139700680906560 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.321716224
I0404 18:23:44.899906 139700680906560 submission_runner.py:313] Starting training loop.
I0404 18:23:48.233669 139700680906560 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 13.044441088
I0404 18:24:28.898079 139519483426560 logging_writer.py:48] [0] global_step=0, grad_norm=0.6066890358924866, loss=6.925991058349609
I0404 18:24:28.910877 139700680906560 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 24.356376576
I0404 18:24:28.911123 139700680906560 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 24.356376576
I0404 18:24:28.911210 139700680906560 spec.py:298] Evaluating on the training split.
I0404 18:24:29.386326 139700680906560 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:24:29.392401 139700680906560 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 18:24:29.392528 139700680906560 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 18:24:29.451986 139700680906560 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:24:40.889773 139700680906560 spec.py:310] Evaluating on the validation split.
I0404 18:24:41.530154 139700680906560 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:24:41.549923 139700680906560 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 18:24:41.550250 139700680906560 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 18:24:41.609411 139700680906560 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:24:58.500018 139700680906560 spec.py:326] Evaluating on the test split.
I0404 18:24:58.905160 139700680906560 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 18:24:58.909941 139700680906560 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0404 18:24:58.939608 139700680906560 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 18:25:07.936199 139700680906560 submission_runner.py:382] Time since start: 44.01s, 	Step: 1, 	{'train/accuracy': 0.0011758609907701612, 'train/loss': 6.91163444519043, 'validation/accuracy': 0.0007800000021234155, 'validation/loss': 6.911706924438477, 'validation/num_examples': 50000, 'test/accuracy': 0.0007000000332482159, 'test/loss': 6.912187099456787, 'test/num_examples': 10000}
I0404 18:25:07.936880 139700680906560 submission_runner.py:396] After eval at step 1: RAM USED (GB) 65.250070528
I0404 18:25:07.943457 139495034844928 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=43.931886, test/accuracy=0.000700, test/loss=6.912187, test/num_examples=10000, total_duration=44.011272, train/accuracy=0.001176, train/loss=6.911634, validation/accuracy=0.000780, validation/loss=6.911707, validation/num_examples=50000
I0404 18:25:08.110553 139700680906560 checkpoints.py:356] Saving checkpoint at step: 1
I0404 18:25:08.736561 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_1
I0404 18:25:08.737816 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_1.
I0404 18:25:08.743717 139700680906560 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 65.333215232
I0404 18:25:08.748923 139700680906560 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 65.331687424
I0404 18:25:08.860234 139700680906560 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 65.827438592
I0404 18:25:42.715400 139495185815296 logging_writer.py:48] [100] global_step=100, grad_norm=0.5931012034416199, loss=6.877882480621338
I0404 18:26:16.670162 139495403927296 logging_writer.py:48] [200] global_step=200, grad_norm=0.6787827610969543, loss=6.72794246673584
I0404 18:26:50.900947 139495185815296 logging_writer.py:48] [300] global_step=300, grad_norm=0.732586145401001, loss=6.557866096496582
I0404 18:27:24.940612 139495403927296 logging_writer.py:48] [400] global_step=400, grad_norm=0.8277578353881836, loss=6.404168128967285
I0404 18:27:58.799536 139495185815296 logging_writer.py:48] [500] global_step=500, grad_norm=1.0111792087554932, loss=6.253931999206543
I0404 18:28:32.905304 139495403927296 logging_writer.py:48] [600] global_step=600, grad_norm=2.746617555618286, loss=6.156796932220459
I0404 18:29:06.947589 139495185815296 logging_writer.py:48] [700] global_step=700, grad_norm=2.3309147357940674, loss=5.964174270629883
I0404 18:29:40.999785 139495403927296 logging_writer.py:48] [800] global_step=800, grad_norm=2.61775541305542, loss=5.820456504821777
I0404 18:30:14.881300 139495185815296 logging_writer.py:48] [900] global_step=900, grad_norm=2.738769054412842, loss=5.788573741912842
I0404 18:30:49.003106 139495403927296 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.168511152267456, loss=5.621226787567139
I0404 18:31:22.914431 139495185815296 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.3437607288360596, loss=5.6943488121032715
I0404 18:31:56.966828 139495403927296 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.14800763130188, loss=5.525230884552002
I0404 18:32:31.203417 139495185815296 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.705935001373291, loss=5.474294662475586
I0404 18:33:04.990282 139495403927296 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.278330087661743, loss=5.3679728507995605
I0404 18:33:39.080061 139495185815296 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.532979965209961, loss=5.306153774261475
I0404 18:33:39.088182 139700680906560 submission_runner.py:373] Before eval at step 1501: RAM USED (GB) 66.506059776
I0404 18:33:39.088282 139700680906560 spec.py:298] Evaluating on the training split.
I0404 18:33:46.026260 139700680906560 spec.py:310] Evaluating on the validation split.
I0404 18:33:53.703763 139700680906560 spec.py:326] Evaluating on the test split.
I0404 18:33:55.691625 139700680906560 submission_runner.py:382] Time since start: 594.19s, 	Step: 1501, 	{'train/accuracy': 0.11475605517625809, 'train/loss': 4.803328514099121, 'validation/accuracy': 0.10047999769449234, 'validation/loss': 4.912456512451172, 'validation/num_examples': 50000, 'test/accuracy': 0.07370000332593918, 'test/loss': 5.250838756561279, 'test/num_examples': 10000}
I0404 18:33:55.692341 139700680906560 submission_runner.py:396] After eval at step 1501: RAM USED (GB) 73.303617536
I0404 18:33:55.699177 139495420712704 logging_writer.py:48] [1501] global_step=1501, preemption_count=0, score=548.589165, test/accuracy=0.073700, test/loss=5.250839, test/num_examples=10000, total_duration=594.187073, train/accuracy=0.114756, train/loss=4.803329, validation/accuracy=0.100480, validation/loss=4.912457, validation/num_examples=50000
I0404 18:33:55.873439 139700680906560 checkpoints.py:356] Saving checkpoint at step: 1501
I0404 18:33:56.629920 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_1501
I0404 18:33:56.630906 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_1501.
I0404 18:33:56.638156 139700680906560 submission_runner.py:416] After logging and checkpointing eval at step 1501: RAM USED (GB) 73.288175616
I0404 18:34:30.562799 139495429105408 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.117844343185425, loss=5.234507083892822
I0404 18:35:04.378370 139524545959680 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.647615432739258, loss=5.1920623779296875
I0404 18:35:38.147742 139495429105408 logging_writer.py:48] [1800] global_step=1800, grad_norm=5.1113972663879395, loss=5.088479995727539
I0404 18:36:12.151992 139524545959680 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.754069805145264, loss=4.978452682495117
I0404 18:36:46.130875 139495429105408 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.3321549892425537, loss=4.898488998413086
I0404 18:37:19.813597 139524545959680 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.207586765289307, loss=4.9174485206604
I0404 18:37:53.665218 139495429105408 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.792633056640625, loss=4.8882060050964355
I0404 18:38:27.458246 139524545959680 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.387648582458496, loss=4.845065116882324
I0404 18:39:01.477699 139495429105408 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.7841498851776123, loss=4.6662678718566895
I0404 18:39:35.388998 139524545959680 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.378547191619873, loss=4.627560615539551
I0404 18:40:09.283070 139495429105408 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.0467758178710938, loss=4.691457271575928
I0404 18:40:43.203298 139524545959680 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.09567403793335, loss=4.496289253234863
I0404 18:41:17.159984 139495429105408 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.94066321849823, loss=4.5073113441467285
I0404 18:41:51.061513 139524545959680 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.4260926246643066, loss=4.419719696044922
I0404 18:42:24.871899 139495429105408 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.671114444732666, loss=4.470195293426514
I0404 18:42:26.682207 139700680906560 submission_runner.py:373] Before eval at step 3007: RAM USED (GB) 74.40269312
I0404 18:42:26.682385 139700680906560 spec.py:298] Evaluating on the training split.
I0404 18:42:33.437140 139700680906560 spec.py:310] Evaluating on the validation split.
I0404 18:42:41.153637 139700680906560 spec.py:326] Evaluating on the test split.
I0404 18:42:43.336075 139700680906560 submission_runner.py:382] Time since start: 1121.78s, 	Step: 3007, 	{'train/accuracy': 0.27270010113716125, 'train/loss': 3.529395818710327, 'validation/accuracy': 0.24723999202251434, 'validation/loss': 3.6926686763763428, 'validation/num_examples': 50000, 'test/accuracy': 0.18050001561641693, 'test/loss': 4.236555576324463, 'test/num_examples': 10000}
I0404 18:42:43.336726 139700680906560 submission_runner.py:396] After eval at step 3007: RAM USED (GB) 79.677825024
I0404 18:42:43.343429 139524545959680 logging_writer.py:48] [3007] global_step=3007, preemption_count=0, score=1052.964352, test/accuracy=0.180500, test/loss=4.236556, test/num_examples=10000, total_duration=1121.780918, train/accuracy=0.272700, train/loss=3.529396, validation/accuracy=0.247240, validation/loss=3.692669, validation/num_examples=50000
I0404 18:42:43.527083 139700680906560 checkpoints.py:356] Saving checkpoint at step: 3007
I0404 18:42:44.136939 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_3007
I0404 18:42:44.137942 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_3007.
I0404 18:42:44.145117 139700680906560 submission_runner.py:416] After logging and checkpointing eval at step 3007: RAM USED (GB) 79.641788416
I0404 18:43:16.210600 139495429105408 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.6724534034729004, loss=4.352806091308594
I0404 18:43:50.265002 139524512388864 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.8300206661224365, loss=4.280643463134766
I0404 18:44:24.267747 139495429105408 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.2554705142974854, loss=4.20670223236084
I0404 18:44:58.435178 139524512388864 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.886625051498413, loss=4.157801151275635
I0404 18:45:32.599934 139495429105408 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.97395920753479, loss=4.085480213165283
I0404 18:46:06.731927 139524512388864 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.696882963180542, loss=4.140449523925781
I0404 18:46:40.811489 139495429105408 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.2529728412628174, loss=4.1081318855285645
I0404 18:47:14.789111 139524512388864 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.5426547527313232, loss=4.003669261932373
I0404 18:47:48.833580 139495429105408 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.089021921157837, loss=3.9281206130981445
I0404 18:48:22.875087 139524512388864 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.8231961727142334, loss=3.9528229236602783
I0404 18:48:56.897332 139495429105408 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.7263989448547363, loss=4.004743576049805
I0404 18:49:30.889884 139524512388864 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.8928534984588623, loss=3.862790584564209
I0404 18:50:04.986380 139495429105408 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.8482660055160522, loss=3.8399453163146973
I0404 18:50:38.917944 139524512388864 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.4717042446136475, loss=3.799950122833252
I0404 18:51:12.966784 139495429105408 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.3959996700286865, loss=3.8774077892303467
I0404 18:51:14.414244 139700680906560 submission_runner.py:373] Before eval at step 4506: RAM USED (GB) 80.440147968
I0404 18:51:14.414458 139700680906560 spec.py:298] Evaluating on the training split.
I0404 18:51:21.059581 139700680906560 spec.py:310] Evaluating on the validation split.
I0404 18:51:29.223906 139700680906560 spec.py:326] Evaluating on the test split.
I0404 18:51:31.112252 139700680906560 submission_runner.py:382] Time since start: 1649.51s, 	Step: 4506, 	{'train/accuracy': 0.3784677982330322, 'train/loss': 2.909409999847412, 'validation/accuracy': 0.3495599925518036, 'validation/loss': 3.0838727951049805, 'validation/num_examples': 50000, 'test/accuracy': 0.26570001244544983, 'test/loss': 3.716608762741089, 'test/num_examples': 10000}
I0404 18:51:31.112994 139700680906560 submission_runner.py:396] After eval at step 4506: RAM USED (GB) 85.697400832
I0404 18:51:31.120077 139524512388864 logging_writer.py:48] [4506] global_step=4506, preemption_count=0, score=1557.606317, test/accuracy=0.265700, test/loss=3.716609, test/num_examples=10000, total_duration=1649.513051, train/accuracy=0.378468, train/loss=2.909410, validation/accuracy=0.349560, validation/loss=3.083873, validation/num_examples=50000
I0404 18:51:31.354316 139700680906560 checkpoints.py:356] Saving checkpoint at step: 4506
I0404 18:51:32.353163 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_4506
I0404 18:51:32.367283 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_4506.
I0404 18:51:32.375950 139700680906560 submission_runner.py:416] After logging and checkpointing eval at step 4506: RAM USED (GB) 85.788274688
I0404 18:52:04.452233 139495429105408 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.2572131156921387, loss=3.8568711280822754
I0404 18:52:38.344642 139523958765312 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.932454228401184, loss=3.6997578144073486
I0404 18:53:12.397053 139495429105408 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.8538501262664795, loss=3.7913782596588135
I0404 18:53:46.206590 139523958765312 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.435983419418335, loss=3.722783327102661
I0404 18:54:20.112306 139495429105408 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.406616449356079, loss=3.7742552757263184
I0404 18:54:53.939203 139523958765312 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.004732370376587, loss=3.4733002185821533
I0404 18:55:28.073287 139495429105408 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.5804842710494995, loss=3.6654739379882812
I0404 18:56:02.011260 139523958765312 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.6381890773773193, loss=3.654330253601074
I0404 18:56:35.871626 139495429105408 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.7438770532608032, loss=3.7361814975738525
I0404 18:57:09.544961 139523958765312 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.518772840499878, loss=3.5667333602905273
I0404 18:57:43.369373 139495429105408 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.871486783027649, loss=3.5871920585632324
I0404 18:58:17.467163 139523958765312 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.249859094619751, loss=3.531531810760498
I0404 18:58:51.398221 139495429105408 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.6739693880081177, loss=3.4094252586364746
I0404 18:59:25.155344 139523958765312 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.453622579574585, loss=3.4769339561462402
I0404 18:59:58.884292 139495429105408 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.905207633972168, loss=3.4519412517547607
I0404 19:00:02.699530 139700680906560 submission_runner.py:373] Before eval at step 6013: RAM USED (GB) 86.452092928
I0404 19:00:02.699714 139700680906560 spec.py:298] Evaluating on the training split.
I0404 19:00:09.739509 139700680906560 spec.py:310] Evaluating on the validation split.
I0404 19:00:18.467428 139700680906560 spec.py:326] Evaluating on the test split.
I0404 19:00:20.374207 139700680906560 submission_runner.py:382] Time since start: 2177.80s, 	Step: 6013, 	{'train/accuracy': 0.4518893361091614, 'train/loss': 2.5563063621520996, 'validation/accuracy': 0.4223800003528595, 'validation/loss': 2.7196712493896484, 'validation/num_examples': 50000, 'test/accuracy': 0.32360002398490906, 'test/loss': 3.3443400859832764, 'test/num_examples': 10000}
I0404 19:00:20.374852 139700680906560 submission_runner.py:396] After eval at step 6013: RAM USED (GB) 91.673317376
I0404 19:00:20.381923 139523958765312 logging_writer.py:48] [6013] global_step=6013, preemption_count=0, score=2058.547843, test/accuracy=0.323600, test/loss=3.344340, test/num_examples=10000, total_duration=2177.798191, train/accuracy=0.451889, train/loss=2.556306, validation/accuracy=0.422380, validation/loss=2.719671, validation/num_examples=50000
I0404 19:00:20.791180 139700680906560 checkpoints.py:356] Saving checkpoint at step: 6013
I0404 19:00:21.593181 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_6013
I0404 19:00:21.606533 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_6013.
I0404 19:00:21.615465 139700680906560 submission_runner.py:416] After logging and checkpointing eval at step 6013: RAM USED (GB) 91.766407168
I0404 19:00:51.566214 139495429105408 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.6446441411972046, loss=3.5341994762420654
I0404 19:01:25.578026 139522515924736 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.388572096824646, loss=3.4646925926208496
I0404 19:01:59.665383 139495429105408 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.9032353162765503, loss=3.3706979751586914
I0404 19:02:33.496889 139522515924736 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.8785187005996704, loss=3.387610912322998
I0404 19:03:07.402545 139495429105408 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.9823325872421265, loss=3.4621598720550537
I0404 19:03:41.281971 139522515924736 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.2993532419204712, loss=3.3316760063171387
I0404 19:04:15.118605 139495429105408 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.68287992477417, loss=3.3237528800964355
I0404 19:04:48.956446 139522515924736 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.2768206596374512, loss=3.2392499446868896
I0404 19:05:23.149561 139495429105408 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.9992185831069946, loss=3.3870296478271484
I0404 19:05:57.117061 139522515924736 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.256174087524414, loss=3.1918721199035645
I0404 19:06:31.031456 139495429105408 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.62244713306427, loss=3.3530890941619873
I0404 19:07:04.841536 139522515924736 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.0881588459014893, loss=3.264073610305786
I0404 19:07:38.764012 139495429105408 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.2402026653289795, loss=3.3297879695892334
I0404 19:08:12.675745 139522515924736 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.0523418188095093, loss=3.4098517894744873
I0404 19:08:46.732361 139495429105408 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.9387223720550537, loss=3.2554941177368164
I0404 19:08:51.911327 139700680906560 submission_runner.py:373] Before eval at step 7517: RAM USED (GB) 92.411871232
I0404 19:08:51.911501 139700680906560 spec.py:298] Evaluating on the training split.
I0404 19:08:59.013424 139700680906560 spec.py:310] Evaluating on the validation split.
I0404 19:09:08.294851 139700680906560 spec.py:326] Evaluating on the test split.
I0404 19:09:10.412554 139700680906560 submission_runner.py:382] Time since start: 2707.01s, 	Step: 7517, 	{'train/accuracy': 0.5073939561843872, 'train/loss': 2.262202739715576, 'validation/accuracy': 0.47189998626708984, 'validation/loss': 2.4403023719787598, 'validation/num_examples': 50000, 'test/accuracy': 0.35930001735687256, 'test/loss': 3.1378402709960938, 'test/num_examples': 10000}
I0404 19:09:10.413193 139700680906560 submission_runner.py:396] After eval at step 7517: RAM USED (GB) 97.534328832
I0404 19:09:10.420163 139522515924736 logging_writer.py:48] [7517] global_step=7517, preemption_count=0, score=2560.888284, test/accuracy=0.359300, test/loss=3.137840, test/num_examples=10000, total_duration=2707.010149, train/accuracy=0.507394, train/loss=2.262203, validation/accuracy=0.471900, validation/loss=2.440302, validation/num_examples=50000
I0404 19:09:10.601208 139700680906560 checkpoints.py:356] Saving checkpoint at step: 7517
I0404 19:09:11.284289 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_7517
I0404 19:09:11.285251 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_7517.
I0404 19:09:11.292172 139700680906560 submission_runner.py:416] After logging and checkpointing eval at step 7517: RAM USED (GB) 97.612541952
I0404 19:09:39.612627 139495429105408 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.51569664478302, loss=3.2156224250793457
I0404 19:10:13.406981 139522507532032 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.0146912336349487, loss=3.2803843021392822
I0404 19:10:47.260412 139495429105408 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.1338177919387817, loss=3.189392328262329
I0404 19:11:20.979616 139522507532032 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.1668457984924316, loss=3.1889214515686035
I0404 19:11:54.646084 139495429105408 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9453870058059692, loss=3.1394946575164795
I0404 19:12:28.384580 139522507532032 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0699714422225952, loss=3.144991159439087
I0404 19:13:02.291083 139495429105408 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.2744942903518677, loss=3.2179908752441406
I0404 19:13:36.023696 139522507532032 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.8629768490791321, loss=3.1712474822998047
I0404 19:14:09.851215 139495429105408 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.1441822052001953, loss=3.0587215423583984
I0404 19:14:43.650719 139522507532032 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9405912160873413, loss=3.125900983810425
I0404 19:15:17.525115 139495429105408 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.0013206005096436, loss=3.0767624378204346
I0404 19:15:51.244852 139522507532032 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.9758332371711731, loss=3.033515691757202
I0404 19:16:24.907570 139495429105408 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.8003271818161011, loss=3.0713448524475098
I0404 19:16:58.828512 139522507532032 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.9096721410751343, loss=3.192772626876831
I0404 19:17:32.504227 139495429105408 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.9763079881668091, loss=3.126938819885254
I0404 19:17:41.410570 139700680906560 submission_runner.py:373] Before eval at step 9028: RAM USED (GB) 98.266394624
I0404 19:17:41.410756 139700680906560 spec.py:298] Evaluating on the training split.
I0404 19:17:48.530508 139700680906560 spec.py:310] Evaluating on the validation split.
I0404 19:17:57.699215 139700680906560 spec.py:326] Evaluating on the test split.
I0404 19:17:59.820524 139700680906560 submission_runner.py:382] Time since start: 3236.51s, 	Step: 9028, 	{'train/accuracy': 0.5632573366165161, 'train/loss': 1.902837872505188, 'validation/accuracy': 0.5180000066757202, 'validation/loss': 2.129683017730713, 'validation/num_examples': 50000, 'test/accuracy': 0.3992000222206116, 'test/loss': 2.8358540534973145, 'test/num_examples': 10000}
I0404 19:17:59.821240 139700680906560 submission_runner.py:396] After eval at step 9028: RAM USED (GB) 103.480098816
I0404 19:17:59.829545 139522507532032 logging_writer.py:48] [9028] global_step=9028, preemption_count=0, score=3064.553940, test/accuracy=0.399200, test/loss=2.835854, test/num_examples=10000, total_duration=3236.509354, train/accuracy=0.563257, train/loss=1.902838, validation/accuracy=0.518000, validation/loss=2.129683, validation/num_examples=50000
I0404 19:18:00.085120 139700680906560 checkpoints.py:356] Saving checkpoint at step: 9028
I0404 19:18:01.079953 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_9028
I0404 19:18:01.095239 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_9028.
I0404 19:18:01.104079 139700680906560 submission_runner.py:416] After logging and checkpointing eval at step 9028: RAM USED (GB) 103.606960128
I0404 19:18:25.851764 139495429105408 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.9991724491119385, loss=3.1154985427856445
I0404 19:18:59.866531 139522499139328 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.3236433267593384, loss=3.0550074577331543
I0404 19:19:34.091243 139495429105408 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.9194881319999695, loss=3.058650493621826
I0404 19:20:08.069653 139522499139328 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.054774522781372, loss=3.0767922401428223
I0404 19:20:41.962657 139495429105408 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.752791702747345, loss=2.9890785217285156
I0404 19:21:15.882168 139522499139328 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.9522658586502075, loss=3.0601694583892822
I0404 19:21:49.854366 139495429105408 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.1072003841400146, loss=3.083465099334717
I0404 19:22:23.857061 139522499139328 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.8398891687393188, loss=2.8605620861053467
I0404 19:22:57.784786 139495429105408 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7405030727386475, loss=3.0999979972839355
I0404 19:23:31.882715 139522499139328 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.826723039150238, loss=2.9465489387512207
I0404 19:24:05.707596 139495429105408 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.8158297538757324, loss=3.022796630859375
I0404 19:24:39.749851 139522499139328 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6946600079536438, loss=2.9282279014587402
I0404 19:25:13.668824 139495429105408 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.8217958211898804, loss=2.9968504905700684
I0404 19:25:47.737957 139522499139328 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7186146974563599, loss=2.88751482963562
I0404 19:26:21.593513 139495429105408 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6604067087173462, loss=2.943481922149658
I0404 19:26:31.200958 139700680906560 submission_runner.py:373] Before eval at step 10530: RAM USED (GB) 104.236773376
I0404 19:26:31.201131 139700680906560 spec.py:298] Evaluating on the training split.
I0404 19:26:38.187925 139700680906560 spec.py:310] Evaluating on the validation split.
I0404 19:26:47.804108 139700680906560 spec.py:326] Evaluating on the test split.
I0404 19:26:49.904828 139700680906560 submission_runner.py:382] Time since start: 3766.30s, 	Step: 10530, 	{'train/accuracy': 0.6196986436843872, 'train/loss': 1.6795371770858765, 'validation/accuracy': 0.549019992351532, 'validation/loss': 2.037830352783203, 'validation/num_examples': 50000, 'test/accuracy': 0.4350000321865082, 'test/loss': 2.6935064792633057, 'test/num_examples': 10000}
I0404 19:26:49.905551 139700680906560 submission_runner.py:396] After eval at step 10530: RAM USED (GB) 109.3986304
I0404 19:26:49.913787 139522499139328 logging_writer.py:48] [10530] global_step=10530, preemption_count=0, score=3566.150243, test/accuracy=0.435000, test/loss=2.693506, test/num_examples=10000, total_duration=3766.299851, train/accuracy=0.619699, train/loss=1.679537, validation/accuracy=0.549020, validation/loss=2.037830, validation/num_examples=50000
I0404 19:26:50.166725 139700680906560 checkpoints.py:356] Saving checkpoint at step: 10530
I0404 19:26:51.213053 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_10530
I0404 19:26:51.228159 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_10530.
I0404 19:26:51.240014 139700680906560 submission_runner.py:416] After logging and checkpointing eval at step 10530: RAM USED (GB) 109.394202624
I0404 19:27:15.047435 139495429105408 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6865601539611816, loss=2.9284939765930176
I0404 19:27:48.830709 139521945499392 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.8782246112823486, loss=2.981247663497925
I0404 19:28:22.572455 139495429105408 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6230364441871643, loss=2.8622546195983887
I0404 19:28:56.356944 139521945499392 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7318902611732483, loss=2.830747604370117
I0404 19:29:30.090837 139495429105408 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7836905717849731, loss=2.98454213142395
I0404 19:30:03.866212 139521945499392 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5939991474151611, loss=2.9245572090148926
I0404 19:30:37.609651 139495429105408 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.8330367803573608, loss=2.9554603099823
I0404 19:31:11.445154 139521945499392 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.8315079808235168, loss=2.8775041103363037
I0404 19:31:45.134042 139495429105408 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7694564461708069, loss=2.750343084335327
I0404 19:32:18.995493 139521945499392 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5521575212478638, loss=2.846649408340454
I0404 19:32:52.861914 139495429105408 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6870070099830627, loss=2.882814645767212
I0404 19:33:26.649857 139521945499392 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6606425642967224, loss=2.8309364318847656
I0404 19:34:00.329860 139495429105408 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6220772862434387, loss=2.8133468627929688
I0404 19:34:34.015938 139521945499392 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.6258214116096497, loss=2.8199045658111572
I0404 19:35:07.719900 139495429105408 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6216614246368408, loss=2.766402244567871
I0404 19:35:21.523312 139700680906560 submission_runner.py:373] Before eval at step 12043: RAM USED (GB) 110.046257152
I0404 19:35:21.523488 139700680906560 spec.py:298] Evaluating on the training split.
I0404 19:35:28.417886 139700680906560 spec.py:310] Evaluating on the validation split.
I0404 19:35:38.061425 139700680906560 spec.py:326] Evaluating on the test split.
I0404 19:35:40.010261 139700680906560 submission_runner.py:382] Time since start: 4296.62s, 	Step: 12043, 	{'train/accuracy': 0.6431162357330322, 'train/loss': 1.5697346925735474, 'validation/accuracy': 0.5818799734115601, 'validation/loss': 1.8793631792068481, 'validation/num_examples': 50000, 'test/accuracy': 0.4524000287055969, 'test/loss': 2.603156089782715, 'test/num_examples': 10000}
I0404 19:35:40.010934 139700680906560 submission_runner.py:396] After eval at step 12043: RAM USED (GB) 115.297271808
I0404 19:35:40.019261 139521945499392 logging_writer.py:48] [12043] global_step=12043, preemption_count=0, score=4070.684143, test/accuracy=0.452400, test/loss=2.603156, test/num_examples=10000, total_duration=4296.620708, train/accuracy=0.643116, train/loss=1.569735, validation/accuracy=0.581880, validation/loss=1.879363, validation/num_examples=50000
I0404 19:35:40.246648 139700680906560 checkpoints.py:356] Saving checkpoint at step: 12043
I0404 19:35:41.177030 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_12043
I0404 19:35:41.189539 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_12043.
I0404 19:35:41.197920 139700680906560 submission_runner.py:416] After logging and checkpointing eval at step 12043: RAM USED (GB) 115.421462528
I0404 19:36:00.975627 139495429105408 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.8338028192520142, loss=2.9221339225769043
I0404 19:36:34.762635 139520095794944 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6200783848762512, loss=2.8381636142730713
I0404 19:37:08.653513 139495429105408 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6339039206504822, loss=2.7913432121276855
I0404 19:37:42.495579 139520095794944 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6468871235847473, loss=2.9126105308532715
I0404 19:38:16.309884 139495429105408 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5200083255767822, loss=2.801185131072998
I0404 19:38:50.255096 139520095794944 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5188871026039124, loss=2.720304250717163
I0404 19:39:24.305186 139495429105408 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.4927850365638733, loss=2.8300421237945557
I0404 19:39:58.243053 139520095794944 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6427682042121887, loss=2.729567527770996
I0404 19:40:32.036706 139495429105408 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5129957795143127, loss=2.8109500408172607
I0404 19:41:06.045231 139520095794944 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6138620376586914, loss=2.791898727416992
I0404 19:41:40.058225 139495429105408 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.7307410836219788, loss=2.7709057331085205
I0404 19:42:14.083109 139520095794944 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5896161794662476, loss=2.7166855335235596
I0404 19:42:48.035867 139495429105408 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6182622313499451, loss=2.727205991744995
I0404 19:43:21.812115 139520095794944 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5495359301567078, loss=2.734748363494873
I0404 19:43:55.783985 139495429105408 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5142683386802673, loss=2.726534605026245
I0404 19:44:11.495603 139700680906560 submission_runner.py:373] Before eval at step 13548: RAM USED (GB) 116.028817408
I0404 19:44:11.495782 139700680906560 spec.py:298] Evaluating on the training split.
I0404 19:44:18.564405 139700680906560 spec.py:310] Evaluating on the validation split.
I0404 19:44:28.151907 139700680906560 spec.py:326] Evaluating on the test split.
I0404 19:44:30.255675 139700680906560 submission_runner.py:382] Time since start: 4826.59s, 	Step: 13548, 	{'train/accuracy': 0.6570272445678711, 'train/loss': 1.5081181526184082, 'validation/accuracy': 0.5936200022697449, 'validation/loss': 1.7985562086105347, 'validation/num_examples': 50000, 'test/accuracy': 0.4645000100135803, 'test/loss': 2.527843952178955, 'test/num_examples': 10000}
I0404 19:44:30.256317 139700680906560 submission_runner.py:396] After eval at step 13548: RAM USED (GB) 121.237536768
I0404 19:44:30.263850 139520095794944 logging_writer.py:48] [13548] global_step=13548, preemption_count=0, score=4575.360831, test/accuracy=0.464500, test/loss=2.527844, test/num_examples=10000, total_duration=4826.594536, train/accuracy=0.657027, train/loss=1.508118, validation/accuracy=0.593620, validation/loss=1.798556, validation/num_examples=50000
I0404 19:44:30.534930 139700680906560 checkpoints.py:356] Saving checkpoint at step: 13548
I0404 19:44:31.506681 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_13548
I0404 19:44:31.521322 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_13548.
I0404 19:44:31.530140 139700680906560 submission_runner.py:416] After logging and checkpointing eval at step 13548: RAM USED (GB) 121.38514432
I0404 19:44:49.423717 139495429105408 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.7106496095657349, loss=2.674231767654419
I0404 19:45:23.197667 139520087402240 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.55869060754776, loss=2.7059452533721924
I0404 19:45:56.913594 139495429105408 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6116880774497986, loss=2.7365434169769287
I0404 19:46:30.642698 139520087402240 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.8253059983253479, loss=2.807925224304199
I0404 19:47:03.724514 139700680906560 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 121.8458624
I0404 19:47:03.724702 139700680906560 spec.py:298] Evaluating on the training split.
I0404 19:47:10.584491 139700680906560 spec.py:310] Evaluating on the validation split.
I0404 19:47:20.333763 139700680906560 spec.py:326] Evaluating on the test split.
I0404 19:47:22.453790 139700680906560 submission_runner.py:382] Time since start: 4998.82s, 	Step: 14000, 	{'train/accuracy': 0.6557915806770325, 'train/loss': 1.4958105087280273, 'validation/accuracy': 0.5992000102996826, 'validation/loss': 1.7710636854171753, 'validation/num_examples': 50000, 'test/accuracy': 0.4765000343322754, 'test/loss': 2.458461046218872, 'test/num_examples': 10000}
I0404 19:47:22.454553 139700680906560 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 127.020302336
I0404 19:47:22.461802 139495429105408 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4725.812516, test/accuracy=0.476500, test/loss=2.458461, test/num_examples=10000, total_duration=4998.823414, train/accuracy=0.655792, train/loss=1.495811, validation/accuracy=0.599200, validation/loss=1.771064, validation/num_examples=50000
I0404 19:47:22.703984 139700680906560 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 19:47:23.651994 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_14000
I0404 19:47:23.665768 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0404 19:47:23.674455 139700680906560 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 127.169716224
I0404 19:47:23.680949 139520087402240 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4725.812516
I0404 19:47:23.840625 139700680906560 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 19:47:25.165192 139700680906560 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_14000
I0404 19:47:25.177757 139700680906560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0404 19:47:25.390723 139700680906560 submission_runner.py:550] Tuning trial 1/1
I0404 19:47:25.391652 139700680906560 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0404 19:47:25.394925 139700680906560 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011758609907701612, 'train/loss': 6.91163444519043, 'validation/accuracy': 0.0007800000021234155, 'validation/loss': 6.911706924438477, 'validation/num_examples': 50000, 'test/accuracy': 0.0007000000332482159, 'test/loss': 6.912187099456787, 'test/num_examples': 10000, 'score': 43.931886434555054, 'total_duration': 44.01127219200134, 'global_step': 1, 'preemption_count': 0}), (1501, {'train/accuracy': 0.11475605517625809, 'train/loss': 4.803328514099121, 'validation/accuracy': 0.10047999769449234, 'validation/loss': 4.912456512451172, 'validation/num_examples': 50000, 'test/accuracy': 0.07370000332593918, 'test/loss': 5.250838756561279, 'test/num_examples': 10000, 'score': 548.5891652107239, 'total_duration': 594.1870725154877, 'global_step': 1501, 'preemption_count': 0}), (3007, {'train/accuracy': 0.27270010113716125, 'train/loss': 3.529395818710327, 'validation/accuracy': 0.24723999202251434, 'validation/loss': 3.6926686763763428, 'validation/num_examples': 50000, 'test/accuracy': 0.18050001561641693, 'test/loss': 4.236555576324463, 'test/num_examples': 10000, 'score': 1052.9643523693085, 'total_duration': 1121.7809178829193, 'global_step': 3007, 'preemption_count': 0}), (4506, {'train/accuracy': 0.3784677982330322, 'train/loss': 2.909409999847412, 'validation/accuracy': 0.3495599925518036, 'validation/loss': 3.0838727951049805, 'validation/num_examples': 50000, 'test/accuracy': 0.26570001244544983, 'test/loss': 3.716608762741089, 'test/num_examples': 10000, 'score': 1557.6063170433044, 'total_duration': 1649.5130505561829, 'global_step': 4506, 'preemption_count': 0}), (6013, {'train/accuracy': 0.4518893361091614, 'train/loss': 2.5563063621520996, 'validation/accuracy': 0.4223800003528595, 'validation/loss': 2.7196712493896484, 'validation/num_examples': 50000, 'test/accuracy': 0.32360002398490906, 'test/loss': 3.3443400859832764, 'test/num_examples': 10000, 'score': 2058.547842979431, 'total_duration': 2177.7981905937195, 'global_step': 6013, 'preemption_count': 0}), (7517, {'train/accuracy': 0.5073939561843872, 'train/loss': 2.262202739715576, 'validation/accuracy': 0.47189998626708984, 'validation/loss': 2.4403023719787598, 'validation/num_examples': 50000, 'test/accuracy': 0.35930001735687256, 'test/loss': 3.1378402709960938, 'test/num_examples': 10000, 'score': 2560.888283729553, 'total_duration': 2707.010149002075, 'global_step': 7517, 'preemption_count': 0}), (9028, {'train/accuracy': 0.5632573366165161, 'train/loss': 1.902837872505188, 'validation/accuracy': 0.5180000066757202, 'validation/loss': 2.129683017730713, 'validation/num_examples': 50000, 'test/accuracy': 0.3992000222206116, 'test/loss': 2.8358540534973145, 'test/num_examples': 10000, 'score': 3064.5539400577545, 'total_duration': 3236.5093536376953, 'global_step': 9028, 'preemption_count': 0}), (10530, {'train/accuracy': 0.6196986436843872, 'train/loss': 1.6795371770858765, 'validation/accuracy': 0.549019992351532, 'validation/loss': 2.037830352783203, 'validation/num_examples': 50000, 'test/accuracy': 0.4350000321865082, 'test/loss': 2.6935064792633057, 'test/num_examples': 10000, 'score': 3566.150242805481, 'total_duration': 3766.2998509407043, 'global_step': 10530, 'preemption_count': 0}), (12043, {'train/accuracy': 0.6431162357330322, 'train/loss': 1.5697346925735474, 'validation/accuracy': 0.5818799734115601, 'validation/loss': 1.8793631792068481, 'validation/num_examples': 50000, 'test/accuracy': 0.4524000287055969, 'test/loss': 2.603156089782715, 'test/num_examples': 10000, 'score': 4070.6841428279877, 'total_duration': 4296.620707511902, 'global_step': 12043, 'preemption_count': 0}), (13548, {'train/accuracy': 0.6570272445678711, 'train/loss': 1.5081181526184082, 'validation/accuracy': 0.5936200022697449, 'validation/loss': 1.7985562086105347, 'validation/num_examples': 50000, 'test/accuracy': 0.4645000100135803, 'test/loss': 2.527843952178955, 'test/num_examples': 10000, 'score': 4575.360831022263, 'total_duration': 4826.594536304474, 'global_step': 13548, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6557915806770325, 'train/loss': 1.4958105087280273, 'validation/accuracy': 0.5992000102996826, 'validation/loss': 1.7710636854171753, 'validation/num_examples': 50000, 'test/accuracy': 0.4765000343322754, 'test/loss': 2.458461046218872, 'test/num_examples': 10000, 'score': 4725.812515735626, 'total_duration': 4998.823413610458, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0404 19:47:25.395020 139700680906560 submission_runner.py:553] Timing: 4725.812515735626
I0404 19:47:25.395077 139700680906560 submission_runner.py:554] ====================
I0404 19:47:25.395164 139700680906560 submission_runner.py:613] Final imagenet_resnet score: 4725.812515735626
