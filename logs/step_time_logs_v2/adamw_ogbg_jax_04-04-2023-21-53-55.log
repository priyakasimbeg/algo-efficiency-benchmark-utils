I0404 21:54:09.080859 139639505643328 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw_v2/ogbg_jax.
I0404 21:54:09.122690 139639505643328 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 21:54:09.994977 139639505643328 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0404 21:54:09.995696 139639505643328 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 21:54:09.998817 139639505643328 submission_runner.py:511] Using RNG seed 3860925265
I0404 21:54:11.247941 139639505643328 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 21:54:11.248128 139639505643328 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1.
I0404 21:54:11.248309 139639505643328 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/hparams.json.
I0404 21:54:11.369322 139639505643328 submission_runner.py:230] Starting train once: RAM USED (GB) 4.214300672
I0404 21:54:11.369486 139639505643328 submission_runner.py:231] Initializing dataset.
I0404 21:54:11.597880 139639505643328 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:54:11.602485 139639505643328 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:54:11.762248 139639505643328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:54:11.789380 139639505643328 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.28607488
I0404 21:54:11.789534 139639505643328 submission_runner.py:240] Initializing model.
I0404 21:54:18.887186 139639505643328 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.13860864
I0404 21:54:18.887382 139639505643328 submission_runner.py:252] Initializing optimizer.
I0404 21:54:19.277378 139639505643328 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.139419648
I0404 21:54:19.277543 139639505643328 submission_runner.py:261] Initializing metrics bundle.
I0404 21:54:19.277590 139639505643328 submission_runner.py:276] Initializing checkpoint and logger.
I0404 21:54:19.278466 139639505643328 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1 with prefix checkpoint_
I0404 21:54:19.278687 139639505643328 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0404 21:54:19.278748 139639505643328 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0404 21:54:20.205648 139639505643328 submission_runner.py:297] Saving meta data to /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/meta_data_0.json.
I0404 21:54:20.206572 139639505643328 submission_runner.py:300] Saving flags to /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/flags_0.json.
I0404 21:54:20.209442 139639505643328 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.1362944
I0404 21:54:20.209625 139639505643328 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.1362944
I0404 21:54:20.209762 139639505643328 submission_runner.py:313] Starting training loop.
I0404 21:54:21.744626 139639505643328 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.28868608
I0404 21:54:39.555864 139463258797824 logging_writer.py:48] [0] global_step=0, grad_norm=2.296187162399292, loss=0.6897733807563782
I0404 21:54:39.563235 139639505643328 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 10.91352576
I0404 21:54:39.563484 139639505643328 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 10.91352576
I0404 21:54:39.563564 139639505643328 spec.py:298] Evaluating on the training split.
I0404 21:54:39.571062 139639505643328 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:54:39.574773 139639505643328 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:54:39.630221 139639505643328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0404 21:54:54.866646 139639505643328 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0404 21:56:10.063694 139639505643328 spec.py:310] Evaluating on the validation split.
I0404 21:56:10.066545 139639505643328 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:56:10.070059 139639505643328 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:56:10.123470 139639505643328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:57:13.695799 139639505643328 spec.py:326] Evaluating on the test split.
I0404 21:57:13.698410 139639505643328 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:57:13.702183 139639505643328 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:57:13.754591 139639505643328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:58:17.748383 139639505643328 submission_runner.py:382] Time since start: 19.35s, 	Step: 1, 	{'train/accuracy': 0.5317080616950989, 'train/loss': 0.6891660690307617, 'train/mean_average_precision': 0.021722251148711496, 'validation/accuracy': 0.5377244353294373, 'validation/loss': 0.690660297870636, 'validation/mean_average_precision': 0.024333663800619888, 'validation/num_examples': 43793, 'test/accuracy': 0.5383701920509338, 'test/loss': 0.6907147765159607, 'test/mean_average_precision': 0.027019049759419576, 'test/num_examples': 43793}
I0404 21:58:17.748822 139639505643328 submission_runner.py:396] After eval at step 1: RAM USED (GB) 12.33010688
I0404 21:58:17.755971 139453342996224 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=19.277758, test/accuracy=0.538370, test/loss=0.690715, test/mean_average_precision=0.027019, test/num_examples=43793, total_duration=19.353801, train/accuracy=0.531708, train/loss=0.689166, train/mean_average_precision=0.021722, validation/accuracy=0.537724, validation/loss=0.690660, validation/mean_average_precision=0.024334, validation/num_examples=43793
I0404 21:58:17.789447 139639505643328 checkpoints.py:356] Saving checkpoint at step: 1
I0404 21:58:17.897528 139639505643328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_1
I0404 21:58:17.897956 139639505643328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_1.
I0404 21:58:17.898854 139639505643328 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 12.333817856
I0404 21:58:18.157785 139639505643328 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 12.376571904
I0404 21:58:18.170194 139639505643328 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 12.376547328
I0404 21:58:41.832852 139453351388928 logging_writer.py:48] [100] global_step=100, grad_norm=0.39446261525154114, loss=0.34677788615226746
I0404 21:59:04.675974 139454551185152 logging_writer.py:48] [200] global_step=200, grad_norm=0.26034867763519287, loss=0.20272569358348846
I0404 21:59:26.955082 139453351388928 logging_writer.py:48] [300] global_step=300, grad_norm=0.1204499751329422, loss=0.11292747408151627
I0404 21:59:49.027399 139454551185152 logging_writer.py:48] [400] global_step=400, grad_norm=0.05979495123028755, loss=0.07983379065990448
I0404 22:00:11.024603 139453351388928 logging_writer.py:48] [500] global_step=500, grad_norm=0.04926493763923645, loss=0.060533519834280014
I0404 22:00:32.976243 139454551185152 logging_writer.py:48] [600] global_step=600, grad_norm=0.13257305324077606, loss=0.0583505816757679
I0404 22:00:55.297956 139453351388928 logging_writer.py:48] [700] global_step=700, grad_norm=0.022470716387033463, loss=0.050302524119615555
I0404 22:01:17.594277 139454551185152 logging_writer.py:48] [800] global_step=800, grad_norm=0.032370224595069885, loss=0.0541079081594944
I0404 22:01:39.654839 139453351388928 logging_writer.py:48] [900] global_step=900, grad_norm=0.03090832196176052, loss=0.05331212654709816
I0404 22:02:01.834231 139454551185152 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.014940558932721615, loss=0.055570684373378754
I0404 22:02:18.059501 139639505643328 submission_runner.py:373] Before eval at step 1075: RAM USED (GB) 13.098893312
I0404 22:02:18.059675 139639505643328 spec.py:298] Evaluating on the training split.
I0404 22:03:29.844813 139639505643328 spec.py:310] Evaluating on the validation split.
I0404 22:03:32.390001 139639505643328 spec.py:326] Evaluating on the test split.
I0404 22:03:34.964462 139639505643328 submission_runner.py:382] Time since start: 477.85s, 	Step: 1075, 	{'train/accuracy': 0.9867675304412842, 'train/loss': 0.051333051174879074, 'train/mean_average_precision': 0.0636281747812992, 'validation/accuracy': 0.9842023849487305, 'validation/loss': 0.061630092561244965, 'validation/mean_average_precision': 0.06005331154434566, 'validation/num_examples': 43793, 'test/accuracy': 0.9832128286361694, 'test/loss': 0.06496874988079071, 'test/mean_average_precision': 0.061361950990213465, 'test/num_examples': 43793}
I0404 22:03:34.964907 139639505643328 submission_runner.py:396] After eval at step 1075: RAM USED (GB) 13.54973184
I0404 22:03:34.972387 139453351388928 logging_writer.py:48] [1075] global_step=1075, preemption_count=0, score=258.337710, test/accuracy=0.983213, test/loss=0.064969, test/mean_average_precision=0.061362, test/num_examples=43793, total_duration=477.849384, train/accuracy=0.986768, train/loss=0.051333, train/mean_average_precision=0.063628, validation/accuracy=0.984202, validation/loss=0.061630, validation/mean_average_precision=0.060053, validation/num_examples=43793
I0404 22:03:35.004209 139639505643328 checkpoints.py:356] Saving checkpoint at step: 1075
I0404 22:03:35.115333 139639505643328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_1075
I0404 22:03:35.115958 139639505643328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_1075.
I0404 22:03:35.116850 139639505643328 submission_runner.py:416] After logging and checkpointing eval at step 1075: RAM USED (GB) 13.552410624
I0404 22:03:40.795878 139454551185152 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.036124709993600845, loss=0.05489623546600342
I0404 22:04:02.778865 139454450751232 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.018338186666369438, loss=0.053439490497112274
I0404 22:04:24.505859 139454551185152 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.019313842058181763, loss=0.05502723902463913
I0404 22:04:46.995591 139454450751232 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.02233116514980793, loss=0.04675665125250816
I0404 22:05:09.475399 139454551185152 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.016615020111203194, loss=0.04970291256904602
I0404 22:05:32.125474 139454450751232 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0215068981051445, loss=0.05165955424308777
I0404 22:05:54.663605 139454551185152 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.03723795339465141, loss=0.04969377443194389
I0404 22:06:17.223223 139454450751232 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.07179603725671768, loss=0.04861195757985115
I0404 22:06:39.690943 139454551185152 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.017621435225009918, loss=0.04920637980103493
I0404 22:07:02.222440 139454450751232 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.018222277984023094, loss=0.05214545875787735
I0404 22:07:24.325402 139454551185152 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.03590995818376541, loss=0.04874676093459129
I0404 22:07:35.310935 139639505643328 submission_runner.py:373] Before eval at step 2151: RAM USED (GB) 14.016700416
I0404 22:07:35.311111 139639505643328 spec.py:298] Evaluating on the training split.
I0404 22:08:47.311208 139639505643328 spec.py:310] Evaluating on the validation split.
I0404 22:08:49.866608 139639505643328 spec.py:326] Evaluating on the test split.
I0404 22:08:52.341519 139639505643328 submission_runner.py:382] Time since start: 795.10s, 	Step: 2151, 	{'train/accuracy': 0.9873174428939819, 'train/loss': 0.046632155776023865, 'train/mean_average_precision': 0.11358443251983602, 'validation/accuracy': 0.9844881296157837, 'validation/loss': 0.05601973459124565, 'validation/mean_average_precision': 0.11215186903218972, 'validation/num_examples': 43793, 'test/accuracy': 0.9834731221199036, 'test/loss': 0.05924900621175766, 'test/mean_average_precision': 0.11043365280928909, 'test/num_examples': 43793}
I0404 22:08:52.341980 139639505643328 submission_runner.py:396] After eval at step 2151: RAM USED (GB) 14.365196288
I0404 22:08:52.348926 139454450751232 logging_writer.py:48] [2151] global_step=2151, preemption_count=0, score=497.415941, test/accuracy=0.983473, test/loss=0.059249, test/mean_average_precision=0.110434, test/num_examples=43793, total_duration=795.100805, train/accuracy=0.987317, train/loss=0.046632, train/mean_average_precision=0.113584, validation/accuracy=0.984488, validation/loss=0.056020, validation/mean_average_precision=0.112152, validation/num_examples=43793
I0404 22:08:52.381469 139639505643328 checkpoints.py:356] Saving checkpoint at step: 2151
I0404 22:08:52.482988 139639505643328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_2151
I0404 22:08:52.483194 139639505643328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_2151.
I0404 22:08:52.484286 139639505643328 submission_runner.py:416] After logging and checkpointing eval at step 2151: RAM USED (GB) 14.36213248
I0404 22:09:03.511659 139454551185152 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.05331205576658249, loss=0.05140628293156624
I0404 22:09:25.732357 139454425573120 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0167658943682909, loss=0.04946153983473778
I0404 22:09:47.733723 139454551185152 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.01786431111395359, loss=0.047008268535137177
I0404 22:10:09.605993 139454425573120 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.016170745715498924, loss=0.04739242419600487
I0404 22:10:31.486729 139454551185152 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.01226215809583664, loss=0.04795074835419655
I0404 22:10:53.358981 139454425573120 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.013471804559230804, loss=0.04727262631058693
I0404 22:11:15.841182 139454551185152 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.018371138721704483, loss=0.047649458050727844
I0404 22:11:38.135400 139454425573120 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.02065853774547577, loss=0.049957577139139175
I0404 22:12:00.347393 139454551185152 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.017118453979492188, loss=0.04580554738640785
I0404 22:12:22.593961 139454425573120 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.016121599823236465, loss=0.0455198772251606
I0404 22:12:44.684468 139454551185152 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.027095852419734, loss=0.044847819954156876
I0404 22:12:52.545293 139639505643328 submission_runner.py:373] Before eval at step 3236: RAM USED (GB) 14.610690048
I0404 22:12:52.545466 139639505643328 spec.py:298] Evaluating on the training split.
I0404 22:14:04.217496 139639505643328 spec.py:310] Evaluating on the validation split.
I0404 22:14:06.790569 139639505643328 spec.py:326] Evaluating on the test split.
I0404 22:14:09.274358 139639505643328 submission_runner.py:382] Time since start: 1112.34s, 	Step: 3236, 	{'train/accuracy': 0.9876401424407959, 'train/loss': 0.04446329176425934, 'train/mean_average_precision': 0.13919662843542774, 'validation/accuracy': 0.9849342703819275, 'validation/loss': 0.05345775559544563, 'validation/mean_average_precision': 0.12717235974346333, 'validation/num_examples': 43793, 'test/accuracy': 0.9839621186256409, 'test/loss': 0.05631861463189125, 'test/mean_average_precision': 0.12386465085801988, 'test/num_examples': 43793}
I0404 22:14:09.274760 139639505643328 submission_runner.py:396] After eval at step 3236: RAM USED (GB) 14.93075968
I0404 22:14:09.281854 139454425573120 logging_writer.py:48] [3236] global_step=3236, preemption_count=0, score=736.378540, test/accuracy=0.983962, test/loss=0.056319, test/mean_average_precision=0.123865, test/num_examples=43793, total_duration=1112.335221, train/accuracy=0.987640, train/loss=0.044463, train/mean_average_precision=0.139197, validation/accuracy=0.984934, validation/loss=0.053458, validation/mean_average_precision=0.127172, validation/num_examples=43793
I0404 22:14:09.315042 139639505643328 checkpoints.py:356] Saving checkpoint at step: 3236
I0404 22:14:09.416110 139639505643328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_3236
I0404 22:14:09.416311 139639505643328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_3236.
I0404 22:14:09.417147 139639505643328 submission_runner.py:416] After logging and checkpointing eval at step 3236: RAM USED (GB) 14.930051072
I0404 22:14:23.979321 139454551185152 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.01743122562766075, loss=0.045029446482658386
I0404 22:14:46.337174 139454417180416 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.03134669363498688, loss=0.04613363370299339
I0404 22:15:08.998054 139454551185152 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.017675071954727173, loss=0.04961757734417915
I0404 22:15:31.078176 139454417180416 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.014977223239839077, loss=0.044479694217443466
I0404 22:15:53.318266 139454551185152 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.012331732548773289, loss=0.04980374872684479
I0404 22:16:15.656255 139454417180416 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.015328388661146164, loss=0.04349255934357643
I0404 22:16:37.694561 139454551185152 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.010228032246232033, loss=0.0434974730014801
I0404 22:17:00.058257 139454417180416 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.009217258542776108, loss=0.04308104142546654
I0404 22:17:22.649044 139454551185152 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.021482188254594803, loss=0.04363108053803444
I0404 22:17:44.808376 139454417180416 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.013883518986403942, loss=0.048923689872026443
I0404 22:18:06.385042 139454551185152 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.01255867537111044, loss=0.047781966626644135
I0404 22:18:09.613545 139639505643328 submission_runner.py:373] Before eval at step 4316: RAM USED (GB) 15.176691712
I0404 22:18:09.613711 139639505643328 spec.py:298] Evaluating on the training split.
I0404 22:19:22.069030 139639505643328 spec.py:310] Evaluating on the validation split.
I0404 22:19:24.607115 139639505643328 spec.py:326] Evaluating on the test split.
I0404 22:19:27.069247 139639505643328 submission_runner.py:382] Time since start: 1429.40s, 	Step: 4316, 	{'train/accuracy': 0.9878405928611755, 'train/loss': 0.04312082752585411, 'train/mean_average_precision': 0.1670217004771286, 'validation/accuracy': 0.9850853085517883, 'validation/loss': 0.05189371109008789, 'validation/mean_average_precision': 0.14812075210561806, 'validation/num_examples': 43793, 'test/accuracy': 0.984192967414856, 'test/loss': 0.054418958723545074, 'test/mean_average_precision': 0.14964143261691898, 'test/num_examples': 43793}
I0404 22:19:27.069657 139639505643328 submission_runner.py:396] After eval at step 4316: RAM USED (GB) 15.55165184
I0404 22:19:27.076959 139454417180416 logging_writer.py:48] [4316] global_step=4316, preemption_count=0, score=975.427370, test/accuracy=0.984193, test/loss=0.054419, test/mean_average_precision=0.149641, test/num_examples=43793, total_duration=1429.403452, train/accuracy=0.987841, train/loss=0.043121, train/mean_average_precision=0.167022, validation/accuracy=0.985085, validation/loss=0.051894, validation/mean_average_precision=0.148121, validation/num_examples=43793
I0404 22:19:27.109526 139639505643328 checkpoints.py:356] Saving checkpoint at step: 4316
I0404 22:19:27.198413 139639505643328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_4316
I0404 22:19:27.198814 139639505643328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_4316.
I0404 22:19:27.199641 139639505643328 submission_runner.py:416] After logging and checkpointing eval at step 4316: RAM USED (GB) 15.552233472
I0404 22:19:45.439740 139454551185152 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.012181895785033703, loss=0.04503989219665527
I0404 22:20:06.995908 139454408787712 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.012319055385887623, loss=0.04208015650510788
I0404 22:20:28.672002 139454551185152 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.012358101084828377, loss=0.04515007883310318
I0404 22:20:50.518612 139454408787712 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.011689323000609875, loss=0.04277906194329262
I0404 22:21:12.770302 139454551185152 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.009119728580117226, loss=0.040117062628269196
I0404 22:21:34.607552 139454408787712 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.009203532710671425, loss=0.043190956115722656
I0404 22:21:56.298969 139454551185152 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.010059666819870472, loss=0.042986515909433365
I0404 22:22:17.919461 139454408787712 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.012741240672767162, loss=0.04274911433458328
I0404 22:22:39.713341 139454551185152 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.008836857043206692, loss=0.04476089030504227
I0404 22:23:01.633324 139454408787712 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.013356759212911129, loss=0.039660241454839706
I0404 22:23:23.544473 139454551185152 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.009335877373814583, loss=0.04238642379641533
I0404 22:23:27.291823 139639505643328 submission_runner.py:373] Before eval at step 5418: RAM USED (GB) 15.736639488
I0404 22:23:27.292013 139639505643328 spec.py:298] Evaluating on the training split.
I0404 22:24:37.921230 139639505643328 spec.py:310] Evaluating on the validation split.
I0404 22:24:40.446837 139639505643328 spec.py:326] Evaluating on the test split.
I0404 22:24:42.918119 139639505643328 submission_runner.py:382] Time since start: 1747.08s, 	Step: 5418, 	{'train/accuracy': 0.9885454773902893, 'train/loss': 0.039863795042037964, 'train/mean_average_precision': 0.20968926598693127, 'validation/accuracy': 0.985526978969574, 'validation/loss': 0.05006464570760727, 'validation/mean_average_precision': 0.17755372384452478, 'validation/num_examples': 43793, 'test/accuracy': 0.9846112132072449, 'test/loss': 0.05293874815106392, 'test/mean_average_precision': 0.17357629549502412, 'test/num_examples': 43793}
I0404 22:24:42.918547 139639505643328 submission_runner.py:396] After eval at step 5418: RAM USED (GB) 16.127291392
I0404 22:24:42.925770 139454408787712 logging_writer.py:48] [5418] global_step=5418, preemption_count=0, score=1214.435319, test/accuracy=0.984611, test/loss=0.052939, test/mean_average_precision=0.173576, test/num_examples=43793, total_duration=1747.081790, train/accuracy=0.988545, train/loss=0.039864, train/mean_average_precision=0.209689, validation/accuracy=0.985527, validation/loss=0.050065, validation/mean_average_precision=0.177554, validation/num_examples=43793
I0404 22:24:42.958713 139639505643328 checkpoints.py:356] Saving checkpoint at step: 5418
I0404 22:24:43.046242 139639505643328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_5418
I0404 22:24:43.046639 139639505643328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_5418.
I0404 22:24:43.047497 139639505643328 submission_runner.py:416] After logging and checkpointing eval at step 5418: RAM USED (GB) 16.127500288
I0404 22:25:01.167050 139454551185152 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.011599979363381863, loss=0.038748063147068024
I0404 22:25:23.071521 139454400395008 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.011221387423574924, loss=0.04321487620472908
I0404 22:25:44.916173 139454551185152 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.011248316615819931, loss=0.04077155515551567
I0404 22:26:06.601597 139454400395008 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.010682569816708565, loss=0.04507366567850113
I0404 22:26:28.120378 139454551185152 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.010723129846155643, loss=0.042202964425086975
I0404 22:26:49.673768 139639505643328 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 16.212873216
I0404 22:26:49.673958 139639505643328 spec.py:298] Evaluating on the training split.
I0404 22:28:01.639748 139639505643328 spec.py:310] Evaluating on the validation split.
I0404 22:28:04.168818 139639505643328 spec.py:326] Evaluating on the test split.
I0404 22:28:06.658401 139639505643328 submission_runner.py:382] Time since start: 1949.46s, 	Step: 6000, 	{'train/accuracy': 0.9887275099754333, 'train/loss': 0.03867368400096893, 'train/mean_average_precision': 0.233064981314629, 'validation/accuracy': 0.9857250452041626, 'validation/loss': 0.04863763600587845, 'validation/mean_average_precision': 0.1894393003786495, 'validation/num_examples': 43793, 'test/accuracy': 0.9848592877388, 'test/loss': 0.051391374319791794, 'test/mean_average_precision': 0.19487706105222913, 'test/num_examples': 43793}
I0404 22:28:06.658838 139639505643328 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 16.540868608
I0404 22:28:06.666239 139454400395008 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1340.474849, test/accuracy=0.984859, test/loss=0.051391, test/mean_average_precision=0.194877, test/num_examples=43793, total_duration=1949.463659, train/accuracy=0.988728, train/loss=0.038674, train/mean_average_precision=0.233065, validation/accuracy=0.985725, validation/loss=0.048638, validation/mean_average_precision=0.189439, validation/num_examples=43793
I0404 22:28:06.698110 139639505643328 checkpoints.py:356] Saving checkpoint at step: 6000
I0404 22:28:06.786932 139639505643328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_6000
I0404 22:28:06.787336 139639505643328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_6000.
I0404 22:28:06.788408 139639505643328 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 16.54116352
I0404 22:28:06.795262 139454551185152 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1340.474849
I0404 22:28:06.824244 139639505643328 checkpoints.py:356] Saving checkpoint at step: 6000
I0404 22:28:06.977017 139639505643328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_6000
I0404 22:28:06.977482 139639505643328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/ogbg_jax/trial_1/checkpoint_6000.
I0404 22:28:07.133352 139639505643328 submission_runner.py:550] Tuning trial 1/1
I0404 22:28:07.133555 139639505643328 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0404 22:28:07.134719 139639505643328 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5317080616950989, 'train/loss': 0.6891660690307617, 'train/mean_average_precision': 0.021722251148711496, 'validation/accuracy': 0.5377244353294373, 'validation/loss': 0.690660297870636, 'validation/mean_average_precision': 0.024333663800619888, 'validation/num_examples': 43793, 'test/accuracy': 0.5383701920509338, 'test/loss': 0.6907147765159607, 'test/mean_average_precision': 0.027019049759419576, 'test/num_examples': 43793, 'score': 19.27775812149048, 'total_duration': 19.353800535202026, 'global_step': 1, 'preemption_count': 0}), (1075, {'train/accuracy': 0.9867675304412842, 'train/loss': 0.051333051174879074, 'train/mean_average_precision': 0.0636281747812992, 'validation/accuracy': 0.9842023849487305, 'validation/loss': 0.061630092561244965, 'validation/mean_average_precision': 0.06005331154434566, 'validation/num_examples': 43793, 'test/accuracy': 0.9832128286361694, 'test/loss': 0.06496874988079071, 'test/mean_average_precision': 0.061361950990213465, 'test/num_examples': 43793, 'score': 258.3377101421356, 'total_duration': 477.8493835926056, 'global_step': 1075, 'preemption_count': 0}), (2151, {'train/accuracy': 0.9873174428939819, 'train/loss': 0.046632155776023865, 'train/mean_average_precision': 0.11358443251983602, 'validation/accuracy': 0.9844881296157837, 'validation/loss': 0.05601973459124565, 'validation/mean_average_precision': 0.11215186903218972, 'validation/num_examples': 43793, 'test/accuracy': 0.9834731221199036, 'test/loss': 0.05924900621175766, 'test/mean_average_precision': 0.11043365280928909, 'test/num_examples': 43793, 'score': 497.4159414768219, 'total_duration': 795.1008050441742, 'global_step': 2151, 'preemption_count': 0}), (3236, {'train/accuracy': 0.9876401424407959, 'train/loss': 0.04446329176425934, 'train/mean_average_precision': 0.13919662843542774, 'validation/accuracy': 0.9849342703819275, 'validation/loss': 0.05345775559544563, 'validation/mean_average_precision': 0.12717235974346333, 'validation/num_examples': 43793, 'test/accuracy': 0.9839621186256409, 'test/loss': 0.05631861463189125, 'test/mean_average_precision': 0.12386465085801988, 'test/num_examples': 43793, 'score': 736.3785395622253, 'total_duration': 1112.3352212905884, 'global_step': 3236, 'preemption_count': 0}), (4316, {'train/accuracy': 0.9878405928611755, 'train/loss': 0.04312082752585411, 'train/mean_average_precision': 0.1670217004771286, 'validation/accuracy': 0.9850853085517883, 'validation/loss': 0.05189371109008789, 'validation/mean_average_precision': 0.14812075210561806, 'validation/num_examples': 43793, 'test/accuracy': 0.984192967414856, 'test/loss': 0.054418958723545074, 'test/mean_average_precision': 0.14964143261691898, 'test/num_examples': 43793, 'score': 975.4273698329926, 'total_duration': 1429.4034521579742, 'global_step': 4316, 'preemption_count': 0}), (5418, {'train/accuracy': 0.9885454773902893, 'train/loss': 0.039863795042037964, 'train/mean_average_precision': 0.20968926598693127, 'validation/accuracy': 0.985526978969574, 'validation/loss': 0.05006464570760727, 'validation/mean_average_precision': 0.17755372384452478, 'validation/num_examples': 43793, 'test/accuracy': 0.9846112132072449, 'test/loss': 0.05293874815106392, 'test/mean_average_precision': 0.17357629549502412, 'test/num_examples': 43793, 'score': 1214.4353189468384, 'total_duration': 1747.0817897319794, 'global_step': 5418, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9887275099754333, 'train/loss': 0.03867368400096893, 'train/mean_average_precision': 0.233064981314629, 'validation/accuracy': 0.9857250452041626, 'validation/loss': 0.04863763600587845, 'validation/mean_average_precision': 0.1894393003786495, 'validation/num_examples': 43793, 'test/accuracy': 0.9848592877388, 'test/loss': 0.051391374319791794, 'test/mean_average_precision': 0.19487706105222913, 'test/num_examples': 43793, 'score': 1340.4748487472534, 'total_duration': 1949.4636585712433, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0404 22:28:07.134824 139639505643328 submission_runner.py:553] Timing: 1340.4748487472534
I0404 22:28:07.134865 139639505643328 submission_runner.py:554] ====================
I0404 22:28:07.134956 139639505643328 submission_runner.py:613] Final ogbg score: 1340.4748487472534
