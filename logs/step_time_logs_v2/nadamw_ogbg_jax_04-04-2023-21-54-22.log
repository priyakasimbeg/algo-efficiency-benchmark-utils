I0404 21:54:36.096970 140380240287552 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw_v2/ogbg_jax.
I0404 21:54:36.149201 140380240287552 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 21:54:36.981657 140380240287552 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0404 21:54:36.982252 140380240287552 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 21:54:36.985455 140380240287552 submission_runner.py:511] Using RNG seed 2933426324
I0404 21:54:38.215878 140380240287552 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 21:54:38.216073 140380240287552 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1.
I0404 21:54:38.216271 140380240287552 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/hparams.json.
I0404 21:54:38.336563 140380240287552 submission_runner.py:230] Starting train once: RAM USED (GB) 4.207947776
I0404 21:54:38.336737 140380240287552 submission_runner.py:231] Initializing dataset.
I0404 21:54:38.563755 140380240287552 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:54:38.569926 140380240287552 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:54:38.726097 140380240287552 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:54:38.755407 140380240287552 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.280520704
I0404 21:54:38.755553 140380240287552 submission_runner.py:240] Initializing model.
I0404 21:54:45.846130 140380240287552 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.102797312
I0404 21:54:45.846325 140380240287552 submission_runner.py:252] Initializing optimizer.
I0404 21:54:46.234349 140380240287552 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.104890368
I0404 21:54:46.234525 140380240287552 submission_runner.py:261] Initializing metrics bundle.
I0404 21:54:46.234581 140380240287552 submission_runner.py:276] Initializing checkpoint and logger.
I0404 21:54:46.235408 140380240287552 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1 with prefix checkpoint_
I0404 21:54:46.235617 140380240287552 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0404 21:54:46.235675 140380240287552 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0404 21:54:47.142932 140380240287552 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/meta_data_0.json.
I0404 21:54:47.143863 140380240287552 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/flags_0.json.
I0404 21:54:47.146694 140380240287552 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.104583168
I0404 21:54:47.146887 140380240287552 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.104583168
I0404 21:54:47.146962 140380240287552 submission_runner.py:313] Starting training loop.
I0404 21:54:48.776593 140380240287552 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.256856064
I0404 21:55:06.644340 140204258088704 logging_writer.py:48] [0] global_step=0, grad_norm=3.0385234355926514, loss=0.768011748790741
I0404 21:55:06.652455 140380240287552 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 10.847387648
I0404 21:55:06.652748 140380240287552 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 10.847387648
I0404 21:55:06.652857 140380240287552 spec.py:298] Evaluating on the training split.
I0404 21:55:06.660021 140380240287552 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:55:06.663730 140380240287552 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:55:06.722521 140380240287552 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0404 21:55:22.118889 140380240287552 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0404 21:56:35.840651 140380240287552 spec.py:310] Evaluating on the validation split.
I0404 21:56:35.843829 140380240287552 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:56:35.847970 140380240287552 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:56:35.902330 140380240287552 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:57:39.258874 140380240287552 spec.py:326] Evaluating on the test split.
I0404 21:57:39.261653 140380240287552 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:57:39.265368 140380240287552 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0404 21:57:39.317990 140380240287552 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0404 21:58:42.784933 140380240287552 submission_runner.py:382] Time since start: 19.51s, 	Step: 1, 	{'train/accuracy': 0.5222074389457703, 'train/loss': 0.7682572603225708, 'train/mean_average_precision': 0.024919972309053066, 'validation/accuracy': 0.5291063189506531, 'validation/loss': 0.7628283500671387, 'validation/mean_average_precision': 0.028528493773170974, 'validation/num_examples': 43793, 'test/accuracy': 0.5308337211608887, 'test/loss': 0.7617497444152832, 'test/mean_average_precision': 0.02984625100891499, 'test/num_examples': 43793}
I0404 21:58:42.785364 140380240287552 submission_runner.py:396] After eval at step 1: RAM USED (GB) 12.197249024
I0404 21:58:42.792313 140194300356352 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=19.430513, test/accuracy=0.530834, test/loss=0.761750, test/mean_average_precision=0.029846, test/num_examples=43793, total_duration=19.505807, train/accuracy=0.522207, train/loss=0.768257, train/mean_average_precision=0.024920, validation/accuracy=0.529106, validation/loss=0.762828, validation/mean_average_precision=0.028528, validation/num_examples=43793
I0404 21:58:42.824743 140380240287552 checkpoints.py:356] Saving checkpoint at step: 1
I0404 21:58:42.932931 140380240287552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_1
I0404 21:58:42.933392 140380240287552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_1.
I0404 21:58:42.934202 140380240287552 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 12.2001408
I0404 21:58:43.145756 140380240287552 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 12.253978624
I0404 21:58:43.160417 140380240287552 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 12.254236672
I0404 21:59:05.506432 140194308749056 logging_writer.py:48] [100] global_step=100, grad_norm=0.4097111225128174, loss=0.3630216121673584
I0404 21:59:27.801221 140195617830656 logging_writer.py:48] [200] global_step=200, grad_norm=0.22607344388961792, loss=0.20381256937980652
I0404 21:59:50.110831 140194308749056 logging_writer.py:48] [300] global_step=300, grad_norm=0.09880270063877106, loss=0.10861153900623322
I0404 22:00:12.556020 140195617830656 logging_writer.py:48] [400] global_step=400, grad_norm=0.05764488875865936, loss=0.07199661433696747
I0404 22:00:34.844430 140194308749056 logging_writer.py:48] [500] global_step=500, grad_norm=0.11944200098514557, loss=0.06057132035493851
I0404 22:00:57.215840 140195617830656 logging_writer.py:48] [600] global_step=600, grad_norm=0.053599052131175995, loss=0.05420512706041336
I0404 22:01:19.615806 140194308749056 logging_writer.py:48] [700] global_step=700, grad_norm=0.09919176250696182, loss=0.05326342582702637
I0404 22:01:42.106983 140195617830656 logging_writer.py:48] [800] global_step=800, grad_norm=0.08234751969575882, loss=0.05646350234746933
I0404 22:02:04.921506 140194308749056 logging_writer.py:48] [900] global_step=900, grad_norm=0.06657400727272034, loss=0.05711657926440239
I0404 22:02:27.352182 140195617830656 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.07029298692941666, loss=0.047659099102020264
I0404 22:02:43.139301 140380240287552 submission_runner.py:373] Before eval at step 1072: RAM USED (GB) 12.957216768
I0404 22:02:43.139500 140380240287552 spec.py:298] Evaluating on the training split.
I0404 22:03:55.230735 140380240287552 spec.py:310] Evaluating on the validation split.
I0404 22:03:57.802044 140380240287552 spec.py:326] Evaluating on the test split.
I0404 22:04:00.269453 140380240287552 submission_runner.py:382] Time since start: 475.99s, 	Step: 1072, 	{'train/accuracy': 0.9869810342788696, 'train/loss': 0.048510149121284485, 'train/mean_average_precision': 0.08545791075558358, 'validation/accuracy': 0.9843822121620178, 'validation/loss': 0.057287104427814484, 'validation/mean_average_precision': 0.09047279213003705, 'validation/num_examples': 43793, 'test/accuracy': 0.9833863377571106, 'test/loss': 0.06066584959626198, 'test/mean_average_precision': 0.08602141984339969, 'test/num_examples': 43793}
I0404 22:04:00.269867 140380240287552 submission_runner.py:396] After eval at step 1072: RAM USED (GB) 13.428084736
I0404 22:04:00.276735 140194308749056 logging_writer.py:48] [1072] global_step=1072, preemption_count=0, score=258.558123, test/accuracy=0.983386, test/loss=0.060666, test/mean_average_precision=0.086021, test/num_examples=43793, total_duration=475.991925, train/accuracy=0.986981, train/loss=0.048510, train/mean_average_precision=0.085458, validation/accuracy=0.984382, validation/loss=0.057287, validation/mean_average_precision=0.090473, validation/num_examples=43793
I0404 22:04:00.309031 140380240287552 checkpoints.py:356] Saving checkpoint at step: 1072
I0404 22:04:00.411313 140380240287552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_1072
I0404 22:04:00.411512 140380240287552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_1072.
I0404 22:04:00.412274 140380240287552 submission_runner.py:416] After logging and checkpointing eval at step 1072: RAM USED (GB) 13.429436416
I0404 22:04:06.905083 140195617830656 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.04231229051947594, loss=0.05037853121757507
I0404 22:04:29.217935 140195483612928 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.04868311434984207, loss=0.04406098648905754
I0404 22:04:51.573136 140195617830656 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.03464216738939285, loss=0.05039380490779877
I0404 22:05:13.892813 140195483612928 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.03434305265545845, loss=0.04139269143342972
I0404 22:05:36.124604 140195617830656 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.06858690083026886, loss=0.04885615035891533
I0404 22:05:58.403055 140195483612928 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.06334146112203598, loss=0.0468636080622673
I0404 22:06:20.938659 140195617830656 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.04724711924791336, loss=0.051255904138088226
I0404 22:06:43.348605 140195483612928 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.10690196603536606, loss=0.046810124069452286
I0404 22:07:05.922693 140195617830656 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0863298550248146, loss=0.04322453960776329
I0404 22:07:28.260880 140195483612928 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.044458918273448944, loss=0.042353708297014236
I0404 22:07:50.613168 140195617830656 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.03808264806866646, loss=0.048473164439201355
I0404 22:08:00.457358 140380240287552 submission_runner.py:373] Before eval at step 2145: RAM USED (GB) 13.731237888
I0404 22:08:00.457548 140380240287552 spec.py:298] Evaluating on the training split.
I0404 22:09:12.724872 140380240287552 spec.py:310] Evaluating on the validation split.
I0404 22:09:15.519125 140380240287552 spec.py:326] Evaluating on the test split.
I0404 22:09:18.005565 140380240287552 submission_runner.py:382] Time since start: 793.31s, 	Step: 2145, 	{'train/accuracy': 0.9879341721534729, 'train/loss': 0.04328532516956329, 'train/mean_average_precision': 0.15910628527515358, 'validation/accuracy': 0.9850232005119324, 'validation/loss': 0.05299021303653717, 'validation/mean_average_precision': 0.149408827804613, 'validation/num_examples': 43793, 'test/accuracy': 0.9840796589851379, 'test/loss': 0.055817436426877975, 'test/mean_average_precision': 0.15292988073160385, 'test/num_examples': 43793}
I0404 22:09:18.005998 140380240287552 submission_runner.py:396] After eval at step 2145: RAM USED (GB) 14.105915392
I0404 22:09:18.012843 140195483612928 logging_writer.py:48] [2145] global_step=2145, preemption_count=0, score=497.423852, test/accuracy=0.984080, test/loss=0.055817, test/mean_average_precision=0.152930, test/num_examples=43793, total_duration=793.309870, train/accuracy=0.987934, train/loss=0.043285, train/mean_average_precision=0.159106, validation/accuracy=0.985023, validation/loss=0.052990, validation/mean_average_precision=0.149409, validation/num_examples=43793
I0404 22:09:18.046082 140380240287552 checkpoints.py:356] Saving checkpoint at step: 2145
I0404 22:09:18.154524 140380240287552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_2145
I0404 22:09:18.154767 140380240287552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_2145.
I0404 22:09:18.155538 140380240287552 submission_runner.py:416] After logging and checkpointing eval at step 2145: RAM USED (GB) 14.104707072
I0404 22:09:30.653101 140195617830656 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.033572934567928314, loss=0.04567362368106842
I0404 22:09:53.136291 140195248731904 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.034150607883930206, loss=0.044488970190286636
I0404 22:10:15.668636 140195617830656 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.04388176649808884, loss=0.043262235820293427
I0404 22:10:38.114954 140195248731904 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.05599179118871689, loss=0.044212281703948975
I0404 22:11:00.561638 140195617830656 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.020482027903199196, loss=0.04305645450949669
I0404 22:11:22.982753 140195248731904 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.07685073465108871, loss=0.04317595064640045
I0404 22:11:45.305970 140195617830656 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.03814602270722389, loss=0.03938501328229904
I0404 22:12:07.943124 140195248731904 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.02415015734732151, loss=0.0474301241338253
I0404 22:12:30.590714 140195617830656 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.03700685128569603, loss=0.04998808354139328
I0404 22:12:53.064917 140195248731904 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.025252440944314003, loss=0.043818820267915726
I0404 22:13:15.402279 140195617830656 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.027435854077339172, loss=0.04699823260307312
I0404 22:13:18.319246 140380240287552 submission_runner.py:373] Before eval at step 3214: RAM USED (GB) 14.474059776
I0404 22:13:18.319417 140380240287552 spec.py:298] Evaluating on the training split.
I0404 22:14:31.337072 140380240287552 spec.py:310] Evaluating on the validation split.
I0404 22:14:33.952075 140380240287552 spec.py:326] Evaluating on the test split.
I0404 22:14:36.415059 140380240287552 submission_runner.py:382] Time since start: 1111.17s, 	Step: 3214, 	{'train/accuracy': 0.9884986877441406, 'train/loss': 0.03989969193935394, 'train/mean_average_precision': 0.2032254092631307, 'validation/accuracy': 0.9855090975761414, 'validation/loss': 0.04939572885632515, 'validation/mean_average_precision': 0.1775411058690885, 'validation/num_examples': 43793, 'test/accuracy': 0.9846318364143372, 'test/loss': 0.05214964970946312, 'test/mean_average_precision': 0.1724099888458276, 'test/num_examples': 43793}
I0404 22:14:36.415475 140380240287552 submission_runner.py:396] After eval at step 3214: RAM USED (GB) 14.775865344
I0404 22:14:36.424010 140195248731904 logging_writer.py:48] [3214] global_step=3214, preemption_count=0, score=736.470229, test/accuracy=0.984632, test/loss=0.052150, test/mean_average_precision=0.172410, test/num_examples=43793, total_duration=1111.171888, train/accuracy=0.988499, train/loss=0.039900, train/mean_average_precision=0.203225, validation/accuracy=0.985509, validation/loss=0.049396, validation/mean_average_precision=0.177541, validation/num_examples=43793
I0404 22:14:36.457102 140380240287552 checkpoints.py:356] Saving checkpoint at step: 3214
I0404 22:14:36.553040 140380240287552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_3214
I0404 22:14:36.553437 140380240287552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_3214.
I0404 22:14:36.554257 140380240287552 submission_runner.py:416] After logging and checkpointing eval at step 3214: RAM USED (GB) 14.776180736
I0404 22:14:56.229687 140195617830656 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.03207355737686157, loss=0.04054531082510948
I0404 22:15:18.908926 140195240339200 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.04823864996433258, loss=0.04120635241270065
I0404 22:15:41.394373 140195617830656 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.02885677106678486, loss=0.04036075994372368
I0404 22:16:03.924416 140195240339200 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.03344891220331192, loss=0.042588770389556885
I0404 22:16:26.579556 140195617830656 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.025066131725907326, loss=0.04051978513598442
I0404 22:16:49.223416 140195240339200 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.09138239920139313, loss=0.04181256890296936
I0404 22:17:11.669557 140195617830656 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.03000257909297943, loss=0.041752029210329056
I0404 22:17:34.013991 140195240339200 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.030718091875314713, loss=0.04462374746799469
I0404 22:17:56.572361 140195617830656 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.029744382947683334, loss=0.039123617112636566
I0404 22:18:19.414137 140195240339200 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.029104674234986305, loss=0.03809423744678497
I0404 22:18:36.769299 140380240287552 submission_runner.py:373] Before eval at step 4278: RAM USED (GB) 15.128604672
I0404 22:18:36.769518 140380240287552 spec.py:298] Evaluating on the training split.
I0404 22:19:48.986662 140380240287552 spec.py:310] Evaluating on the validation split.
I0404 22:19:51.569527 140380240287552 spec.py:326] Evaluating on the test split.
I0404 22:19:54.055788 140380240287552 submission_runner.py:382] Time since start: 1429.62s, 	Step: 4278, 	{'train/accuracy': 0.9888212084770203, 'train/loss': 0.03876031935214996, 'train/mean_average_precision': 0.22428875843932342, 'validation/accuracy': 0.9857092499732971, 'validation/loss': 0.04849950224161148, 'validation/mean_average_precision': 0.18606680173788592, 'validation/num_examples': 43793, 'test/accuracy': 0.9847859740257263, 'test/loss': 0.05117609724402428, 'test/mean_average_precision': 0.18562720963438703, 'test/num_examples': 43793}
I0404 22:19:54.056236 140380240287552 submission_runner.py:396] After eval at step 4278: RAM USED (GB) 15.46264576
I0404 22:19:54.063791 140195617830656 logging_writer.py:48] [4278] global_step=4278, preemption_count=0, score=975.543824, test/accuracy=0.984786, test/loss=0.051176, test/mean_average_precision=0.185627, test/num_examples=43793, total_duration=1429.621788, train/accuracy=0.988821, train/loss=0.038760, train/mean_average_precision=0.224289, validation/accuracy=0.985709, validation/loss=0.048500, validation/mean_average_precision=0.186067, validation/num_examples=43793
I0404 22:19:54.096660 140380240287552 checkpoints.py:356] Saving checkpoint at step: 4278
I0404 22:19:54.188747 140380240287552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_4278
I0404 22:19:54.188960 140380240287552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_4278.
I0404 22:19:54.189767 140380240287552 submission_runner.py:416] After logging and checkpointing eval at step 4278: RAM USED (GB) 15.46008576
I0404 22:19:59.435699 140195240339200 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.014897610992193222, loss=0.03712037205696106
I0404 22:20:22.001464 140195231946496 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.020103834569454193, loss=0.03920878469944
I0404 22:20:44.818631 140195240339200 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.03644914925098419, loss=0.042231447994709015
I0404 22:21:07.002753 140195231946496 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.021549038589000702, loss=0.041587717831134796
I0404 22:21:29.225187 140195240339200 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.02154139056801796, loss=0.04647441953420639
I0404 22:21:51.521649 140195231946496 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.014887078665196896, loss=0.04312416538596153
I0404 22:22:13.876436 140195240339200 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.015696831047534943, loss=0.03433496877551079
I0404 22:22:36.068922 140195231946496 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.022359833121299744, loss=0.037355996668338776
I0404 22:22:59.162669 140195240339200 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.014364802278578281, loss=0.03915337845683098
I0404 22:23:21.684174 140195231946496 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.01358120795339346, loss=0.04031169414520264
I0404 22:23:44.262311 140195240339200 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.013324631378054619, loss=0.04115147143602371
I0404 22:23:54.256942 140380240287552 submission_runner.py:373] Before eval at step 5345: RAM USED (GB) 15.583735808
I0404 22:23:54.257120 140380240287552 spec.py:298] Evaluating on the training split.
I0404 22:25:07.609502 140380240287552 spec.py:310] Evaluating on the validation split.
I0404 22:25:10.136674 140380240287552 spec.py:326] Evaluating on the test split.
I0404 22:25:12.606497 140380240287552 submission_runner.py:382] Time since start: 1747.11s, 	Step: 5345, 	{'train/accuracy': 0.9889538884162903, 'train/loss': 0.0374986007809639, 'train/mean_average_precision': 0.2620843687596368, 'validation/accuracy': 0.9859194755554199, 'validation/loss': 0.04722851142287254, 'validation/mean_average_precision': 0.217743872754471, 'validation/num_examples': 43793, 'test/accuracy': 0.9850500822067261, 'test/loss': 0.04990021139383316, 'test/mean_average_precision': 0.21893054308157292, 'test/num_examples': 43793}
I0404 22:25:12.606925 140380240287552 submission_runner.py:396] After eval at step 5345: RAM USED (GB) 15.98732288
I0404 22:25:12.613877 140195231946496 logging_writer.py:48] [5345] global_step=5345, preemption_count=0, score=1214.487633, test/accuracy=0.985050, test/loss=0.049900, test/mean_average_precision=0.218931, test/num_examples=43793, total_duration=1747.109530, train/accuracy=0.988954, train/loss=0.037499, train/mean_average_precision=0.262084, validation/accuracy=0.985919, validation/loss=0.047229, validation/mean_average_precision=0.217744, validation/num_examples=43793
I0404 22:25:12.645441 140380240287552 checkpoints.py:356] Saving checkpoint at step: 5345
I0404 22:25:12.731713 140380240287552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_5345
I0404 22:25:12.732298 140380240287552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_5345.
I0404 22:25:12.733086 140380240287552 submission_runner.py:416] After logging and checkpointing eval at step 5345: RAM USED (GB) 15.989997568
I0404 22:25:25.402582 140195240339200 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.02137797884643078, loss=0.04152249917387962
I0404 22:25:47.898696 140195080959744 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.02207275666296482, loss=0.039981599897146225
I0404 22:26:10.529543 140195240339200 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.033327411860227585, loss=0.04173208773136139
I0404 22:26:32.992934 140195080959744 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.01933450624346733, loss=0.03862253949046135
I0404 22:26:55.481740 140195240339200 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.01077173836529255, loss=0.040462374687194824
I0404 22:27:17.941600 140195080959744 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.015486610122025013, loss=0.04230446368455887
I0404 22:27:40.379522 140380240287552 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 16.054489088
I0404 22:27:40.379692 140380240287552 spec.py:298] Evaluating on the training split.
I0404 22:28:53.561288 140380240287552 spec.py:310] Evaluating on the validation split.
I0404 22:28:56.105114 140380240287552 spec.py:326] Evaluating on the test split.
I0404 22:28:58.588763 140380240287552 submission_runner.py:382] Time since start: 1973.23s, 	Step: 6000, 	{'train/accuracy': 0.9891172647476196, 'train/loss': 0.036659400910139084, 'train/mean_average_precision': 0.2834327730246339, 'validation/accuracy': 0.986320972442627, 'validation/loss': 0.046395380049943924, 'validation/mean_average_precision': 0.2200300329230741, 'validation/num_examples': 43793, 'test/accuracy': 0.9854232668876648, 'test/loss': 0.049058523029088974, 'test/mean_average_precision': 0.22188537708893483, 'test/num_examples': 43793}
I0404 22:28:58.589183 140380240287552 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 16.386998272
I0404 22:28:58.596642 140195240339200 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1361.466839, test/accuracy=0.985423, test/loss=0.049059, test/mean_average_precision=0.221885, test/num_examples=43793, total_duration=1973.232181, train/accuracy=0.989117, train/loss=0.036659, train/mean_average_precision=0.283433, validation/accuracy=0.986321, validation/loss=0.046395, validation/mean_average_precision=0.220030, validation/num_examples=43793
I0404 22:28:58.629244 140380240287552 checkpoints.py:356] Saving checkpoint at step: 6000
I0404 22:28:58.721059 140380240287552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_6000
I0404 22:28:58.721683 140380240287552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_6000.
I0404 22:28:58.722568 140380240287552 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 16.38731776
I0404 22:28:58.728761 140195080959744 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1361.466839
I0404 22:28:58.755996 140380240287552 checkpoints.py:356] Saving checkpoint at step: 6000
I0404 22:28:58.915608 140380240287552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_6000
I0404 22:28:58.916258 140380240287552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/ogbg_jax/trial_1/checkpoint_6000.
I0404 22:28:59.062206 140380240287552 submission_runner.py:550] Tuning trial 1/1
I0404 22:28:59.062411 140380240287552 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0404 22:28:59.063689 140380240287552 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5222074389457703, 'train/loss': 0.7682572603225708, 'train/mean_average_precision': 0.024919972309053066, 'validation/accuracy': 0.5291063189506531, 'validation/loss': 0.7628283500671387, 'validation/mean_average_precision': 0.028528493773170974, 'validation/num_examples': 43793, 'test/accuracy': 0.5308337211608887, 'test/loss': 0.7617497444152832, 'test/mean_average_precision': 0.02984625100891499, 'test/num_examples': 43793, 'score': 19.430513381958008, 'total_duration': 19.505806922912598, 'global_step': 1, 'preemption_count': 0}), (1072, {'train/accuracy': 0.9869810342788696, 'train/loss': 0.048510149121284485, 'train/mean_average_precision': 0.08545791075558358, 'validation/accuracy': 0.9843822121620178, 'validation/loss': 0.057287104427814484, 'validation/mean_average_precision': 0.09047279213003705, 'validation/num_examples': 43793, 'test/accuracy': 0.9833863377571106, 'test/loss': 0.06066584959626198, 'test/mean_average_precision': 0.08602141984339969, 'test/num_examples': 43793, 'score': 258.55812311172485, 'total_duration': 475.99192452430725, 'global_step': 1072, 'preemption_count': 0}), (2145, {'train/accuracy': 0.9879341721534729, 'train/loss': 0.04328532516956329, 'train/mean_average_precision': 0.15910628527515358, 'validation/accuracy': 0.9850232005119324, 'validation/loss': 0.05299021303653717, 'validation/mean_average_precision': 0.149408827804613, 'validation/num_examples': 43793, 'test/accuracy': 0.9840796589851379, 'test/loss': 0.055817436426877975, 'test/mean_average_precision': 0.15292988073160385, 'test/num_examples': 43793, 'score': 497.4238519668579, 'total_duration': 793.3098704814911, 'global_step': 2145, 'preemption_count': 0}), (3214, {'train/accuracy': 0.9884986877441406, 'train/loss': 0.03989969193935394, 'train/mean_average_precision': 0.2032254092631307, 'validation/accuracy': 0.9855090975761414, 'validation/loss': 0.04939572885632515, 'validation/mean_average_precision': 0.1775411058690885, 'validation/num_examples': 43793, 'test/accuracy': 0.9846318364143372, 'test/loss': 0.05214964970946312, 'test/mean_average_precision': 0.1724099888458276, 'test/num_examples': 43793, 'score': 736.4702293872833, 'total_duration': 1111.1718878746033, 'global_step': 3214, 'preemption_count': 0}), (4278, {'train/accuracy': 0.9888212084770203, 'train/loss': 0.03876031935214996, 'train/mean_average_precision': 0.22428875843932342, 'validation/accuracy': 0.9857092499732971, 'validation/loss': 0.04849950224161148, 'validation/mean_average_precision': 0.18606680173788592, 'validation/num_examples': 43793, 'test/accuracy': 0.9847859740257263, 'test/loss': 0.05117609724402428, 'test/mean_average_precision': 0.18562720963438703, 'test/num_examples': 43793, 'score': 975.5438244342804, 'total_duration': 1429.6217875480652, 'global_step': 4278, 'preemption_count': 0}), (5345, {'train/accuracy': 0.9889538884162903, 'train/loss': 0.0374986007809639, 'train/mean_average_precision': 0.2620843687596368, 'validation/accuracy': 0.9859194755554199, 'validation/loss': 0.04722851142287254, 'validation/mean_average_precision': 0.217743872754471, 'validation/num_examples': 43793, 'test/accuracy': 0.9850500822067261, 'test/loss': 0.04990021139383316, 'test/mean_average_precision': 0.21893054308157292, 'test/num_examples': 43793, 'score': 1214.4876327514648, 'total_duration': 1747.1095297336578, 'global_step': 5345, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9891172647476196, 'train/loss': 0.036659400910139084, 'train/mean_average_precision': 0.2834327730246339, 'validation/accuracy': 0.986320972442627, 'validation/loss': 0.046395380049943924, 'validation/mean_average_precision': 0.2200300329230741, 'validation/num_examples': 43793, 'test/accuracy': 0.9854232668876648, 'test/loss': 0.049058523029088974, 'test/mean_average_precision': 0.22188537708893483, 'test/num_examples': 43793, 'score': 1361.466839313507, 'total_duration': 1973.2321808338165, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0404 22:28:59.063817 140380240287552 submission_runner.py:553] Timing: 1361.466839313507
I0404 22:28:59.063879 140380240287552 submission_runner.py:554] ====================
I0404 22:28:59.064002 140380240287552 submission_runner.py:613] Final ogbg score: 1361.466839313507
