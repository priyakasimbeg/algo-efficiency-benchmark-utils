WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 00:12:04.043109 139921904072512 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 00:12:04.043118 140491634988864 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 00:12:04.043147 140037191497536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 00:12:04.043164 140218964391744 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 00:12:04.044326 139715832743744 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 00:12:05.033432 140602268116800 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 00:12:05.034714 139689671128896 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 00:12:05.037894 140617241605952 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 00:12:05.038222 140617241605952 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:12:05.044104 140602268116800 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:12:05.045343 139689671128896 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:12:05.047300 139921904072512 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:12:05.047324 140491634988864 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:12:05.047348 140037191497536 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:12:05.047369 140218964391744 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:12:05.047399 139715832743744 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:12:06.207349 140617241605952 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch.
W0406 00:12:06.246609 140491634988864 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:12:06.248125 139689671128896 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:12:06.248378 140218964391744 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:12:06.248518 139921904072512 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:12:06.248815 139715832743744 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:12:06.249798 140602268116800 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:12:06.250424 140037191497536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:12:06.250542 140617241605952 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 00:12:06.254143 140617241605952 submission_runner.py:511] Using RNG seed 3358329930
I0406 00:12:06.255233 140617241605952 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 00:12:06.255361 140617241605952 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1.
I0406 00:12:06.255628 140617241605952 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/hparams.json.
I0406 00:12:06.256677 140617241605952 submission_runner.py:230] Starting train once: RAM USED (GB) 5.631102976
I0406 00:12:06.256788 140617241605952 submission_runner.py:231] Initializing dataset.
I0406 00:12:06.256966 140617241605952 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.631684608
I0406 00:12:06.257030 140617241605952 submission_runner.py:240] Initializing model.
I0406 00:12:10.087274 140617241605952 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.29544704
I0406 00:12:10.087497 140617241605952 submission_runner.py:252] Initializing optimizer.
I0406 00:12:10.088358 140617241605952 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.295442944
I0406 00:12:10.088482 140617241605952 submission_runner.py:261] Initializing metrics bundle.
I0406 00:12:10.088544 140617241605952 submission_runner.py:276] Initializing checkpoint and logger.
I0406 00:12:10.090263 140617241605952 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 00:12:10.090406 140617241605952 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 00:12:10.704760 140617241605952 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0406 00:12:10.706899 140617241605952 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/flags_0.json.
I0406 00:12:10.740040 140617241605952 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.34889984
I0406 00:12:10.741218 140617241605952 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.34889984
I0406 00:12:10.741327 140617241605952 submission_runner.py:313] Starting training loop.
I0406 00:12:10.992920 140617241605952 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0406 00:12:10.998297 140617241605952 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0406 00:12:11.094629 140617241605952 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0406 00:12:12.508879 140617241605952 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 15.579660288
I0406 00:12:16.174248 140577937028864 logging_writer.py:48] [0] global_step=0, grad_norm=2.701962, loss=0.771482
I0406 00:12:16.179363 140617241605952 submission.py:296] 0) loss = 0.771, grad_norm = 2.702
I0406 00:12:16.179815 140617241605952 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 21.929091072
I0406 00:12:16.185149 140617241605952 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 21.929091072
I0406 00:12:16.185250 140617241605952 spec.py:298] Evaluating on the training split.
I0406 00:12:16.190168 140617241605952 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0406 00:12:16.193804 140617241605952 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0406 00:12:16.245989 140617241605952 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0406 00:12:30.929613 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:31.161754 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:31.161837 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:31.161849 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:31.176536 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:31.176624 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:31.176725 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:31.176962 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:44.959430 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:45.121894 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:45.122134 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:45.127031 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:45.127384 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:45.128027 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:45.128028 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:12:45.128448 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:13:09.883727 140617241605952 spec.py:310] Evaluating on the validation split.
I0406 00:13:09.886407 140617241605952 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0406 00:13:09.890335 140617241605952 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0406 00:13:09.944036 140617241605952 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
W0406 00:13:22.982179 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:23.127658 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:23.128066 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:23.133317 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:23.133908 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:23.134283 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:23.134422 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:23.134706 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:28.354018 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:28.497944 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:28.498164 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:28.504227 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:28.504730 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:28.505463 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:28.505626 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:28.506137 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:13:52.815570 140617241605952 spec.py:326] Evaluating on the test split.
I0406 00:13:52.818211 140617241605952 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0406 00:13:52.822120 140617241605952 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0406 00:13:52.873837 140617241605952 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
W0406 00:14:05.969377 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:06.109058 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:06.110431 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:06.116633 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:06.116688 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:06.116727 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:06.117701 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:06.118869 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:11.458927 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:11.600432 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:11.601019 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:11.607028 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:11.607643 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:11.607704 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:11.607724 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:11.608375 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:14:34.306830 140617241605952 submission_runner.py:382] Time since start: 5.44s, 	Step: 1, 	{'train/accuracy': 0.4264367895287525, 'train/loss': 0.7727322578430176, 'train/mean_average_precision': 0.02069788431423971, 'validation/accuracy': 0.4293835709304242, 'validation/loss': 0.7737170457839966, 'validation/mean_average_precision': 0.026079616779733864, 'validation/num_examples': 43793, 'test/accuracy': 0.4310954978921312, 'test/loss': 0.7734853625297546, 'test/mean_average_precision': 0.02720952804873771, 'test/num_examples': 43793}
I0406 00:14:34.307302 140617241605952 submission_runner.py:396] After eval at step 1: RAM USED (GB) 25.887645696
I0406 00:14:34.316091 140565286549248 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=5.442570, test/accuracy=0.431095, test/loss=0.773485, test/mean_average_precision=0.027210, test/num_examples=43793, total_duration=5.444583, train/accuracy=0.426437, train/loss=0.772732, train/mean_average_precision=0.020698, validation/accuracy=0.429384, validation/loss=0.773717, validation/mean_average_precision=0.026080, validation/num_examples=43793
I0406 00:14:34.703787 140617241605952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_1.
I0406 00:14:34.704459 140617241605952 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 25.88725248
I0406 00:14:34.935077 140617241605952 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 25.932484608
I0406 00:14:34.937768 140617241605952 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:14:34.943876 139689671128896 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:14:34.943880 140491634988864 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:14:34.945122 140602268116800 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:14:34.945130 139715832743744 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:14:34.945130 140218964391744 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:14:34.945142 139921904072512 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:14:34.945135 140037191497536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:14:34.977862 140565294941952 logging_writer.py:48] [1] global_step=1, grad_norm=2.732383, loss=0.773909
I0406 00:14:34.981977 140617241605952 submission.py:296] 1) loss = 0.774, grad_norm = 2.732
I0406 00:14:34.982683 140617241605952 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 25.932193792
I0406 00:14:35.244419 140565286549248 logging_writer.py:48] [2] global_step=2, grad_norm=2.734930, loss=0.770796
I0406 00:14:35.248647 140617241605952 submission.py:296] 2) loss = 0.771, grad_norm = 2.735
I0406 00:14:35.519295 140565294941952 logging_writer.py:48] [3] global_step=3, grad_norm=2.717707, loss=0.767474
I0406 00:14:35.523358 140617241605952 submission.py:296] 3) loss = 0.767, grad_norm = 2.718
I0406 00:14:35.783414 140565286549248 logging_writer.py:48] [4] global_step=4, grad_norm=2.682834, loss=0.767467
I0406 00:14:35.787422 140617241605952 submission.py:296] 4) loss = 0.767, grad_norm = 2.683
I0406 00:14:36.053504 140565294941952 logging_writer.py:48] [5] global_step=5, grad_norm=2.663322, loss=0.762663
I0406 00:14:36.057812 140617241605952 submission.py:296] 5) loss = 0.763, grad_norm = 2.663
I0406 00:14:36.329751 140565286549248 logging_writer.py:48] [6] global_step=6, grad_norm=2.591035, loss=0.758640
I0406 00:14:36.333931 140617241605952 submission.py:296] 6) loss = 0.759, grad_norm = 2.591
I0406 00:14:36.595041 140565294941952 logging_writer.py:48] [7] global_step=7, grad_norm=2.569834, loss=0.754262
I0406 00:14:36.599241 140617241605952 submission.py:296] 7) loss = 0.754, grad_norm = 2.570
I0406 00:14:36.863846 140565286549248 logging_writer.py:48] [8] global_step=8, grad_norm=2.536719, loss=0.747652
I0406 00:14:36.867916 140617241605952 submission.py:296] 8) loss = 0.748, grad_norm = 2.537
I0406 00:14:37.130156 140565294941952 logging_writer.py:48] [9] global_step=9, grad_norm=2.390947, loss=0.742041
I0406 00:14:37.134473 140617241605952 submission.py:296] 9) loss = 0.742, grad_norm = 2.391
I0406 00:14:37.400185 140565286549248 logging_writer.py:48] [10] global_step=10, grad_norm=2.289468, loss=0.733920
I0406 00:14:37.404601 140617241605952 submission.py:296] 10) loss = 0.734, grad_norm = 2.289
I0406 00:14:37.669838 140565294941952 logging_writer.py:48] [11] global_step=11, grad_norm=2.238405, loss=0.726784
I0406 00:14:37.674392 140617241605952 submission.py:296] 11) loss = 0.727, grad_norm = 2.238
I0406 00:14:37.947917 140565286549248 logging_writer.py:48] [12] global_step=12, grad_norm=2.183966, loss=0.720448
I0406 00:14:37.952136 140617241605952 submission.py:296] 12) loss = 0.720, grad_norm = 2.184
I0406 00:14:38.212122 140565294941952 logging_writer.py:48] [13] global_step=13, grad_norm=2.160623, loss=0.712680
I0406 00:14:38.216642 140617241605952 submission.py:296] 13) loss = 0.713, grad_norm = 2.161
I0406 00:14:38.480556 140565286549248 logging_writer.py:48] [14] global_step=14, grad_norm=2.145807, loss=0.704419
I0406 00:14:38.485004 140617241605952 submission.py:296] 14) loss = 0.704, grad_norm = 2.146
I0406 00:14:38.756366 140565294941952 logging_writer.py:48] [15] global_step=15, grad_norm=2.086318, loss=0.697291
I0406 00:14:38.760591 140617241605952 submission.py:296] 15) loss = 0.697, grad_norm = 2.086
I0406 00:14:39.019228 140565286549248 logging_writer.py:48] [16] global_step=16, grad_norm=2.012134, loss=0.688414
I0406 00:14:39.023624 140617241605952 submission.py:296] 16) loss = 0.688, grad_norm = 2.012
I0406 00:14:39.284407 140565294941952 logging_writer.py:48] [17] global_step=17, grad_norm=1.896864, loss=0.680666
I0406 00:14:39.288510 140617241605952 submission.py:296] 17) loss = 0.681, grad_norm = 1.897
I0406 00:14:39.559765 140565286549248 logging_writer.py:48] [18] global_step=18, grad_norm=1.734846, loss=0.672952
I0406 00:14:39.564153 140617241605952 submission.py:296] 18) loss = 0.673, grad_norm = 1.735
I0406 00:14:39.836104 140565294941952 logging_writer.py:48] [19] global_step=19, grad_norm=1.682542, loss=0.667294
I0406 00:14:39.840096 140617241605952 submission.py:296] 19) loss = 0.667, grad_norm = 1.683
I0406 00:14:40.100469 140565286549248 logging_writer.py:48] [20] global_step=20, grad_norm=1.565342, loss=0.660077
I0406 00:14:40.104587 140617241605952 submission.py:296] 20) loss = 0.660, grad_norm = 1.565
I0406 00:14:40.384124 140565294941952 logging_writer.py:48] [21] global_step=21, grad_norm=1.522854, loss=0.653441
I0406 00:14:40.388404 140617241605952 submission.py:296] 21) loss = 0.653, grad_norm = 1.523
I0406 00:14:40.669943 140565286549248 logging_writer.py:48] [22] global_step=22, grad_norm=1.507150, loss=0.646280
I0406 00:14:40.674113 140617241605952 submission.py:296] 22) loss = 0.646, grad_norm = 1.507
I0406 00:14:40.937967 140565294941952 logging_writer.py:48] [23] global_step=23, grad_norm=1.473923, loss=0.640989
I0406 00:14:40.942348 140617241605952 submission.py:296] 23) loss = 0.641, grad_norm = 1.474
I0406 00:14:41.212697 140565286549248 logging_writer.py:48] [24] global_step=24, grad_norm=1.449135, loss=0.633379
I0406 00:14:41.216779 140617241605952 submission.py:296] 24) loss = 0.633, grad_norm = 1.449
I0406 00:14:41.483801 140565294941952 logging_writer.py:48] [25] global_step=25, grad_norm=1.374520, loss=0.629336
I0406 00:14:41.488109 140617241605952 submission.py:296] 25) loss = 0.629, grad_norm = 1.375
I0406 00:14:41.754728 140565286549248 logging_writer.py:48] [26] global_step=26, grad_norm=1.340031, loss=0.621588
I0406 00:14:41.758875 140617241605952 submission.py:296] 26) loss = 0.622, grad_norm = 1.340
I0406 00:14:42.024083 140565294941952 logging_writer.py:48] [27] global_step=27, grad_norm=1.276743, loss=0.617968
I0406 00:14:42.028661 140617241605952 submission.py:296] 27) loss = 0.618, grad_norm = 1.277
I0406 00:14:42.298724 140565286549248 logging_writer.py:48] [28] global_step=28, grad_norm=1.235045, loss=0.609773
I0406 00:14:42.303179 140617241605952 submission.py:296] 28) loss = 0.610, grad_norm = 1.235
I0406 00:14:42.564382 140565294941952 logging_writer.py:48] [29] global_step=29, grad_norm=1.216939, loss=0.604716
I0406 00:14:42.568636 140617241605952 submission.py:296] 29) loss = 0.605, grad_norm = 1.217
I0406 00:14:42.829502 140565286549248 logging_writer.py:48] [30] global_step=30, grad_norm=1.171351, loss=0.600141
I0406 00:14:42.833476 140617241605952 submission.py:296] 30) loss = 0.600, grad_norm = 1.171
I0406 00:14:43.097399 140565294941952 logging_writer.py:48] [31] global_step=31, grad_norm=1.194810, loss=0.594996
I0406 00:14:43.101934 140617241605952 submission.py:296] 31) loss = 0.595, grad_norm = 1.195
I0406 00:14:43.365646 140565286549248 logging_writer.py:48] [32] global_step=32, grad_norm=1.173719, loss=0.588512
I0406 00:14:43.369780 140617241605952 submission.py:296] 32) loss = 0.589, grad_norm = 1.174
I0406 00:14:43.639919 140565294941952 logging_writer.py:48] [33] global_step=33, grad_norm=1.137830, loss=0.583346
I0406 00:14:43.644139 140617241605952 submission.py:296] 33) loss = 0.583, grad_norm = 1.138
I0406 00:14:43.921151 140565286549248 logging_writer.py:48] [34] global_step=34, grad_norm=1.097607, loss=0.575492
I0406 00:14:43.925311 140617241605952 submission.py:296] 34) loss = 0.575, grad_norm = 1.098
I0406 00:14:44.209381 140565294941952 logging_writer.py:48] [35] global_step=35, grad_norm=1.043551, loss=0.571314
I0406 00:14:44.213390 140617241605952 submission.py:296] 35) loss = 0.571, grad_norm = 1.044
I0406 00:14:44.475518 140565286549248 logging_writer.py:48] [36] global_step=36, grad_norm=1.000245, loss=0.567246
I0406 00:14:44.479646 140617241605952 submission.py:296] 36) loss = 0.567, grad_norm = 1.000
I0406 00:14:44.751439 140565294941952 logging_writer.py:48] [37] global_step=37, grad_norm=0.962384, loss=0.562566
I0406 00:14:44.755782 140617241605952 submission.py:296] 37) loss = 0.563, grad_norm = 0.962
I0406 00:14:45.014550 140565286549248 logging_writer.py:48] [38] global_step=38, grad_norm=0.938950, loss=0.556318
I0406 00:14:45.018637 140617241605952 submission.py:296] 38) loss = 0.556, grad_norm = 0.939
I0406 00:14:45.287290 140565294941952 logging_writer.py:48] [39] global_step=39, grad_norm=0.917417, loss=0.553179
I0406 00:14:45.291099 140617241605952 submission.py:296] 39) loss = 0.553, grad_norm = 0.917
I0406 00:14:45.564637 140565286549248 logging_writer.py:48] [40] global_step=40, grad_norm=0.893403, loss=0.548370
I0406 00:14:45.569291 140617241605952 submission.py:296] 40) loss = 0.548, grad_norm = 0.893
I0406 00:14:45.839345 140565294941952 logging_writer.py:48] [41] global_step=41, grad_norm=0.874489, loss=0.542805
I0406 00:14:45.843440 140617241605952 submission.py:296] 41) loss = 0.543, grad_norm = 0.874
I0406 00:14:46.108720 140565286549248 logging_writer.py:48] [42] global_step=42, grad_norm=0.856581, loss=0.536715
I0406 00:14:46.112932 140617241605952 submission.py:296] 42) loss = 0.537, grad_norm = 0.857
I0406 00:14:46.376639 140565294941952 logging_writer.py:48] [43] global_step=43, grad_norm=0.856902, loss=0.533022
I0406 00:14:46.380566 140617241605952 submission.py:296] 43) loss = 0.533, grad_norm = 0.857
I0406 00:14:46.645466 140565286549248 logging_writer.py:48] [44] global_step=44, grad_norm=0.843120, loss=0.528753
I0406 00:14:46.649961 140617241605952 submission.py:296] 44) loss = 0.529, grad_norm = 0.843
I0406 00:14:46.917760 140565294941952 logging_writer.py:48] [45] global_step=45, grad_norm=0.817600, loss=0.527392
I0406 00:14:46.921879 140617241605952 submission.py:296] 45) loss = 0.527, grad_norm = 0.818
I0406 00:14:47.182286 140565286549248 logging_writer.py:48] [46] global_step=46, grad_norm=0.812020, loss=0.520596
I0406 00:14:47.186723 140617241605952 submission.py:296] 46) loss = 0.521, grad_norm = 0.812
I0406 00:14:47.449460 140565294941952 logging_writer.py:48] [47] global_step=47, grad_norm=0.789907, loss=0.516669
I0406 00:14:47.454128 140617241605952 submission.py:296] 47) loss = 0.517, grad_norm = 0.790
I0406 00:14:47.724326 140565286549248 logging_writer.py:48] [48] global_step=48, grad_norm=0.767908, loss=0.511948
I0406 00:14:47.728989 140617241605952 submission.py:296] 48) loss = 0.512, grad_norm = 0.768
I0406 00:14:48.011245 140565294941952 logging_writer.py:48] [49] global_step=49, grad_norm=0.747815, loss=0.508707
I0406 00:14:48.015637 140617241605952 submission.py:296] 49) loss = 0.509, grad_norm = 0.748
I0406 00:14:48.275465 140565286549248 logging_writer.py:48] [50] global_step=50, grad_norm=0.727339, loss=0.505196
I0406 00:14:48.279498 140617241605952 submission.py:296] 50) loss = 0.505, grad_norm = 0.727
I0406 00:14:48.541714 140565294941952 logging_writer.py:48] [51] global_step=51, grad_norm=0.716000, loss=0.500058
I0406 00:14:48.545851 140617241605952 submission.py:296] 51) loss = 0.500, grad_norm = 0.716
I0406 00:14:48.827334 140565286549248 logging_writer.py:48] [52] global_step=52, grad_norm=0.707578, loss=0.494577
I0406 00:14:48.831856 140617241605952 submission.py:296] 52) loss = 0.495, grad_norm = 0.708
I0406 00:14:49.093562 140565294941952 logging_writer.py:48] [53] global_step=53, grad_norm=0.687372, loss=0.495450
I0406 00:14:49.097581 140617241605952 submission.py:296] 53) loss = 0.495, grad_norm = 0.687
I0406 00:14:49.355280 140565286549248 logging_writer.py:48] [54] global_step=54, grad_norm=0.688293, loss=0.489669
I0406 00:14:49.359250 140617241605952 submission.py:296] 54) loss = 0.490, grad_norm = 0.688
I0406 00:14:49.625958 140565294941952 logging_writer.py:48] [55] global_step=55, grad_norm=0.673716, loss=0.483008
I0406 00:14:49.629996 140617241605952 submission.py:296] 55) loss = 0.483, grad_norm = 0.674
I0406 00:14:49.893254 140565286549248 logging_writer.py:48] [56] global_step=56, grad_norm=0.658738, loss=0.482524
I0406 00:14:49.897670 140617241605952 submission.py:296] 56) loss = 0.483, grad_norm = 0.659
I0406 00:14:50.167623 140565294941952 logging_writer.py:48] [57] global_step=57, grad_norm=0.650036, loss=0.477593
I0406 00:14:50.171801 140617241605952 submission.py:296] 57) loss = 0.478, grad_norm = 0.650
I0406 00:14:50.439468 140565286549248 logging_writer.py:48] [58] global_step=58, grad_norm=0.646191, loss=0.473309
I0406 00:14:50.443551 140617241605952 submission.py:296] 58) loss = 0.473, grad_norm = 0.646
I0406 00:14:50.714747 140565294941952 logging_writer.py:48] [59] global_step=59, grad_norm=0.634947, loss=0.471528
I0406 00:14:50.719237 140617241605952 submission.py:296] 59) loss = 0.472, grad_norm = 0.635
I0406 00:14:50.999096 140565286549248 logging_writer.py:48] [60] global_step=60, grad_norm=0.624236, loss=0.465847
I0406 00:14:51.003748 140617241605952 submission.py:296] 60) loss = 0.466, grad_norm = 0.624
I0406 00:14:51.287703 140565294941952 logging_writer.py:48] [61] global_step=61, grad_norm=0.615534, loss=0.464003
I0406 00:14:51.292454 140617241605952 submission.py:296] 61) loss = 0.464, grad_norm = 0.616
I0406 00:14:51.568951 140565286549248 logging_writer.py:48] [62] global_step=62, grad_norm=0.609202, loss=0.464039
I0406 00:14:51.573757 140617241605952 submission.py:296] 62) loss = 0.464, grad_norm = 0.609
I0406 00:14:51.849927 140565294941952 logging_writer.py:48] [63] global_step=63, grad_norm=0.593043, loss=0.457016
I0406 00:14:51.854080 140617241605952 submission.py:296] 63) loss = 0.457, grad_norm = 0.593
I0406 00:14:52.126147 140565286549248 logging_writer.py:48] [64] global_step=64, grad_norm=0.582747, loss=0.454009
I0406 00:14:52.130096 140617241605952 submission.py:296] 64) loss = 0.454, grad_norm = 0.583
I0406 00:14:52.400474 140565294941952 logging_writer.py:48] [65] global_step=65, grad_norm=0.563675, loss=0.451624
I0406 00:14:52.404721 140617241605952 submission.py:296] 65) loss = 0.452, grad_norm = 0.564
I0406 00:14:52.691983 140565286549248 logging_writer.py:48] [66] global_step=66, grad_norm=0.555646, loss=0.449642
I0406 00:14:52.696329 140617241605952 submission.py:296] 66) loss = 0.450, grad_norm = 0.556
I0406 00:14:52.965678 140565294941952 logging_writer.py:48] [67] global_step=67, grad_norm=0.545057, loss=0.447537
I0406 00:14:52.970132 140617241605952 submission.py:296] 67) loss = 0.448, grad_norm = 0.545
I0406 00:14:53.233295 140565286549248 logging_writer.py:48] [68] global_step=68, grad_norm=0.537636, loss=0.443532
I0406 00:14:53.237030 140617241605952 submission.py:296] 68) loss = 0.444, grad_norm = 0.538
I0406 00:14:53.506193 140565294941952 logging_writer.py:48] [69] global_step=69, grad_norm=0.534154, loss=0.438006
I0406 00:14:53.510278 140617241605952 submission.py:296] 69) loss = 0.438, grad_norm = 0.534
I0406 00:14:53.781905 140565286549248 logging_writer.py:48] [70] global_step=70, grad_norm=0.522476, loss=0.438736
I0406 00:14:53.786224 140617241605952 submission.py:296] 70) loss = 0.439, grad_norm = 0.522
I0406 00:14:54.045338 140565294941952 logging_writer.py:48] [71] global_step=71, grad_norm=0.518668, loss=0.435126
I0406 00:14:54.049494 140617241605952 submission.py:296] 71) loss = 0.435, grad_norm = 0.519
I0406 00:14:54.307667 140565286549248 logging_writer.py:48] [72] global_step=72, grad_norm=0.510859, loss=0.430127
I0406 00:14:54.311675 140617241605952 submission.py:296] 72) loss = 0.430, grad_norm = 0.511
I0406 00:14:54.569735 140565294941952 logging_writer.py:48] [73] global_step=73, grad_norm=0.503258, loss=0.430045
I0406 00:14:54.574059 140617241605952 submission.py:296] 73) loss = 0.430, grad_norm = 0.503
I0406 00:14:54.844225 140565286549248 logging_writer.py:48] [74] global_step=74, grad_norm=0.500603, loss=0.426542
I0406 00:14:54.848482 140617241605952 submission.py:296] 74) loss = 0.427, grad_norm = 0.501
I0406 00:14:55.116407 140565294941952 logging_writer.py:48] [75] global_step=75, grad_norm=0.498763, loss=0.419633
I0406 00:14:55.120339 140617241605952 submission.py:296] 75) loss = 0.420, grad_norm = 0.499
I0406 00:14:55.384711 140565286549248 logging_writer.py:48] [76] global_step=76, grad_norm=0.489679, loss=0.421122
I0406 00:14:55.388929 140617241605952 submission.py:296] 76) loss = 0.421, grad_norm = 0.490
I0406 00:14:55.662659 140565294941952 logging_writer.py:48] [77] global_step=77, grad_norm=0.490816, loss=0.415343
I0406 00:14:55.666860 140617241605952 submission.py:296] 77) loss = 0.415, grad_norm = 0.491
I0406 00:14:55.932099 140565286549248 logging_writer.py:48] [78] global_step=78, grad_norm=0.486519, loss=0.411826
I0406 00:14:55.936273 140617241605952 submission.py:296] 78) loss = 0.412, grad_norm = 0.487
I0406 00:14:56.203670 140565294941952 logging_writer.py:48] [79] global_step=79, grad_norm=0.478479, loss=0.413665
I0406 00:14:56.207874 140617241605952 submission.py:296] 79) loss = 0.414, grad_norm = 0.478
I0406 00:14:56.469237 140565286549248 logging_writer.py:48] [80] global_step=80, grad_norm=0.469824, loss=0.409008
I0406 00:14:56.473491 140617241605952 submission.py:296] 80) loss = 0.409, grad_norm = 0.470
I0406 00:14:56.733887 140565294941952 logging_writer.py:48] [81] global_step=81, grad_norm=0.465641, loss=0.406885
I0406 00:14:56.738114 140617241605952 submission.py:296] 81) loss = 0.407, grad_norm = 0.466
I0406 00:14:57.002897 140565286549248 logging_writer.py:48] [82] global_step=82, grad_norm=0.461857, loss=0.406388
I0406 00:14:57.006984 140617241605952 submission.py:296] 82) loss = 0.406, grad_norm = 0.462
I0406 00:14:57.274652 140565294941952 logging_writer.py:48] [83] global_step=83, grad_norm=0.456619, loss=0.399938
I0406 00:14:57.279051 140617241605952 submission.py:296] 83) loss = 0.400, grad_norm = 0.457
I0406 00:14:57.547324 140565286549248 logging_writer.py:48] [84] global_step=84, grad_norm=0.451331, loss=0.398819
I0406 00:14:57.551195 140617241605952 submission.py:296] 84) loss = 0.399, grad_norm = 0.451
I0406 00:14:57.821696 140565294941952 logging_writer.py:48] [85] global_step=85, grad_norm=0.448271, loss=0.395933
I0406 00:14:57.825779 140617241605952 submission.py:296] 85) loss = 0.396, grad_norm = 0.448
I0406 00:14:58.094392 140565286549248 logging_writer.py:48] [86] global_step=86, grad_norm=0.440432, loss=0.394642
I0406 00:14:58.098555 140617241605952 submission.py:296] 86) loss = 0.395, grad_norm = 0.440
I0406 00:14:58.364846 140565294941952 logging_writer.py:48] [87] global_step=87, grad_norm=0.441127, loss=0.390588
I0406 00:14:58.369068 140617241605952 submission.py:296] 87) loss = 0.391, grad_norm = 0.441
I0406 00:14:58.626549 140565286549248 logging_writer.py:48] [88] global_step=88, grad_norm=0.443451, loss=0.390856
I0406 00:14:58.630889 140617241605952 submission.py:296] 88) loss = 0.391, grad_norm = 0.443
I0406 00:14:58.901242 140565294941952 logging_writer.py:48] [89] global_step=89, grad_norm=0.438529, loss=0.387824
I0406 00:14:58.905803 140617241605952 submission.py:296] 89) loss = 0.388, grad_norm = 0.439
I0406 00:14:59.168184 140565286549248 logging_writer.py:48] [90] global_step=90, grad_norm=0.435388, loss=0.383037
I0406 00:14:59.172154 140617241605952 submission.py:296] 90) loss = 0.383, grad_norm = 0.435
I0406 00:14:59.437016 140565294941952 logging_writer.py:48] [91] global_step=91, grad_norm=0.427557, loss=0.382925
I0406 00:14:59.441301 140617241605952 submission.py:296] 91) loss = 0.383, grad_norm = 0.428
I0406 00:14:59.706433 140565286549248 logging_writer.py:48] [92] global_step=92, grad_norm=0.424793, loss=0.378311
I0406 00:14:59.710733 140617241605952 submission.py:296] 92) loss = 0.378, grad_norm = 0.425
I0406 00:14:59.990718 140565294941952 logging_writer.py:48] [93] global_step=93, grad_norm=0.423549, loss=0.374465
I0406 00:14:59.995072 140617241605952 submission.py:296] 93) loss = 0.374, grad_norm = 0.424
I0406 00:15:00.263476 140565286549248 logging_writer.py:48] [94] global_step=94, grad_norm=0.416850, loss=0.372824
I0406 00:15:00.267507 140617241605952 submission.py:296] 94) loss = 0.373, grad_norm = 0.417
I0406 00:15:00.524994 140565294941952 logging_writer.py:48] [95] global_step=95, grad_norm=0.418632, loss=0.368636
I0406 00:15:00.529147 140617241605952 submission.py:296] 95) loss = 0.369, grad_norm = 0.419
I0406 00:15:00.812309 140565286549248 logging_writer.py:48] [96] global_step=96, grad_norm=0.413890, loss=0.368744
I0406 00:15:00.816287 140617241605952 submission.py:296] 96) loss = 0.369, grad_norm = 0.414
I0406 00:15:01.081111 140565294941952 logging_writer.py:48] [97] global_step=97, grad_norm=0.411371, loss=0.367917
I0406 00:15:01.085393 140617241605952 submission.py:296] 97) loss = 0.368, grad_norm = 0.411
I0406 00:15:01.361526 140565286549248 logging_writer.py:48] [98] global_step=98, grad_norm=0.412520, loss=0.363317
I0406 00:15:01.366080 140617241605952 submission.py:296] 98) loss = 0.363, grad_norm = 0.413
I0406 00:15:01.647394 140565294941952 logging_writer.py:48] [99] global_step=99, grad_norm=0.405620, loss=0.363059
I0406 00:15:01.651611 140617241605952 submission.py:296] 99) loss = 0.363, grad_norm = 0.406
I0406 00:15:01.917375 140565286549248 logging_writer.py:48] [100] global_step=100, grad_norm=0.402270, loss=0.360582
I0406 00:15:01.921213 140617241605952 submission.py:296] 100) loss = 0.361, grad_norm = 0.402
I0406 00:16:45.830332 140565294941952 logging_writer.py:48] [500] global_step=500, grad_norm=0.031073, loss=0.067313
I0406 00:16:45.834906 140617241605952 submission.py:296] 500) loss = 0.067, grad_norm = 0.031
I0406 00:18:34.895266 140617241605952 submission_runner.py:373] Before eval at step 921: RAM USED (GB) 26.674118656
I0406 00:18:34.895476 140617241605952 spec.py:298] Evaluating on the training split.
W0406 00:18:48.591556 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:48.790957 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:48.791148 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:48.796140 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:48.796306 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:48.797337 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:48.798441 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:48.798629 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:03.150367 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:03.335847 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:03.336667 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:03.342491 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:03.343248 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:03.343457 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:03.343518 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:03.348377 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:28.519790 140617241605952 spec.py:310] Evaluating on the validation split.
W0406 00:19:28.823768 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.052563 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.053076 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.058730 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.059578 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.059890 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.060012 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.060244 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.190947 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.387435 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.387730 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.393841 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.394212 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.394292 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.394786 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:29.399657 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:31.786231 140617241605952 spec.py:326] Evaluating on the test split.
W0406 00:19:32.091633 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.312766 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.313253 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.318728 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.319295 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.319393 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.319813 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.320139 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.451747 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.641063 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.641785 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.648062 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.648541 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.648547 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.648816 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:32.648977 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:34.950152 140617241605952 submission_runner.py:382] Time since start: 384.15s, 	Step: 921, 	{'train/accuracy': 0.986898522965536, 'train/loss': 0.04968801885843277, 'train/mean_average_precision': 0.07046519534189623, 'validation/accuracy': 0.984290094965767, 'validation/loss': 0.059444330632686615, 'validation/mean_average_precision': 0.07129294700383709, 'validation/num_examples': 43793, 'test/accuracy': 0.9833044744774975, 'test/loss': 0.06273513287305832, 'test/mean_average_precision': 0.07177770538680854, 'test/num_examples': 43793}
I0406 00:19:34.950657 140617241605952 submission_runner.py:396] After eval at step 921: RAM USED (GB) 28.273610752
I0406 00:19:34.959254 140565286549248 logging_writer.py:48] [921] global_step=921, preemption_count=0, score=244.592192, test/accuracy=0.983304, test/loss=0.062735, test/mean_average_precision=0.071778, test/num_examples=43793, total_duration=384.154269, train/accuracy=0.986899, train/loss=0.049688, train/mean_average_precision=0.070465, validation/accuracy=0.984290, validation/loss=0.059444, validation/mean_average_precision=0.071293, validation/num_examples=43793
I0406 00:19:35.048386 140617241605952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_921.
I0406 00:19:35.048921 140617241605952 submission_runner.py:416] After logging and checkpointing eval at step 921: RAM USED (GB) 27.904188416
I0406 00:19:55.893386 140565294941952 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.038085, loss=0.046865
I0406 00:19:55.898145 140617241605952 submission.py:296] 1000) loss = 0.047, grad_norm = 0.038
I0406 00:22:08.835508 140565286549248 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.057454, loss=0.058976
I0406 00:22:08.840459 140617241605952 submission.py:296] 1500) loss = 0.059, grad_norm = 0.057
I0406 00:23:35.279911 140617241605952 submission_runner.py:373] Before eval at step 1833: RAM USED (GB) 28.245307392
I0406 00:23:35.280131 140617241605952 spec.py:298] Evaluating on the training split.
W0406 00:23:49.020286 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:49.271391 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:49.272250 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:49.277726 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:49.278086 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:49.278159 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:49.278480 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:49.279465 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:03.965056 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:04.216434 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:04.216610 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:04.222782 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:04.223625 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:04.223870 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:04.224116 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:04.224817 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:30.211726 140617241605952 spec.py:310] Evaluating on the validation split.
W0406 00:24:30.649302 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.916994 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.917178 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.923981 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.924241 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.924618 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.924613 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.924679 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:31.068079 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:31.333596 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:31.333738 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:31.339375 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:31.340280 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:31.340307 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:31.340429 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:31.340628 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:33.862752 140617241605952 spec.py:326] Evaluating on the test split.
W0406 00:24:34.268541 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.532230 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.532553 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.538620 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.539531 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.539658 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.539827 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.545557 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.704583 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.968891 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.968903 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.975444 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.975664 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.975723 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.976052 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.976172 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:37.438457 140617241605952 submission_runner.py:382] Time since start: 684.54s, 	Step: 1833, 	{'train/accuracy': 0.988069692658786, 'train/loss': 0.04282639920711517, 'train/mean_average_precision': 0.1485575101527088, 'validation/accuracy': 0.9850617882337277, 'validation/loss': 0.052272941917181015, 'validation/mean_average_precision': 0.1401149257564459, 'validation/num_examples': 43793, 'test/accuracy': 0.984087685994308, 'test/loss': 0.055038757622241974, 'test/mean_average_precision': 0.1427954543031437, 'test/num_examples': 43793}
I0406 00:24:37.438833 140617241605952 submission_runner.py:396] After eval at step 1833: RAM USED (GB) 28.67826688
I0406 00:24:37.446578 140565294941952 logging_writer.py:48] [1833] global_step=1833, preemption_count=0, score=483.853158, test/accuracy=0.984088, test/loss=0.055039, test/mean_average_precision=0.142795, test/num_examples=43793, total_duration=684.538970, train/accuracy=0.988070, train/loss=0.042826, train/mean_average_precision=0.148558, validation/accuracy=0.985062, validation/loss=0.052273, validation/mean_average_precision=0.140115, validation/num_examples=43793
I0406 00:24:37.532678 140617241605952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_1833.
I0406 00:24:37.533155 140617241605952 submission_runner.py:416] After logging and checkpointing eval at step 1833: RAM USED (GB) 28.677849088
I0406 00:25:21.650298 140565286549248 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.040954, loss=0.044670
I0406 00:25:21.655165 140617241605952 submission.py:296] 2000) loss = 0.045, grad_norm = 0.041
I0406 00:27:33.131405 140565294941952 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.036429, loss=0.040557
I0406 00:27:33.136496 140617241605952 submission.py:296] 2500) loss = 0.041, grad_norm = 0.036
I0406 00:28:37.770497 140617241605952 submission_runner.py:373] Before eval at step 2749: RAM USED (GB) 29.11039488
I0406 00:28:37.770721 140617241605952 spec.py:298] Evaluating on the training split.
W0406 00:28:52.669196 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:52.913431 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:52.913615 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:52.920210 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:52.920500 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:52.920875 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:52.921017 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:52.921209 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.733595 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.974420 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.975432 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.980585 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.980847 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.981096 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.981745 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.982140 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:33.667683 140617241605952 spec.py:310] Evaluating on the validation split.
W0406 00:29:34.133630 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.388985 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.389166 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.394918 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.395438 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.395666 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.396744 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.397036 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.588670 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.839371 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.839524 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.844749 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.845405 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.845699 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.845941 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:34.846233 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:37.440033 140617241605952 spec.py:326] Evaluating on the test split.
W0406 00:29:37.878432 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.128964 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.129925 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.135098 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.135533 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.135721 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.135931 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.136071 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.357701 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.609431 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.609658 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.614519 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.616360 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.616525 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.616533 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:38.616763 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:41.119637 140617241605952 submission_runner.py:382] Time since start: 987.03s, 	Step: 2749, 	{'train/accuracy': 0.9884436196977583, 'train/loss': 0.040531158447265625, 'train/mean_average_precision': 0.19999430790410594, 'validation/accuracy': 0.9853398576122405, 'validation/loss': 0.05009075254201889, 'validation/mean_average_precision': 0.16890673272053072, 'validation/num_examples': 43793, 'test/accuracy': 0.9844810818301246, 'test/loss': 0.05252925306558609, 'test/mean_average_precision': 0.16947504173196426, 'test/num_examples': 43793}
I0406 00:29:41.120014 140617241605952 submission_runner.py:396] After eval at step 2749: RAM USED (GB) 29.454860288
I0406 00:29:41.128018 140565286549248 logging_writer.py:48] [2749] global_step=2749, preemption_count=0, score=723.111855, test/accuracy=0.984481, test/loss=0.052529, test/mean_average_precision=0.169475, test/num_examples=43793, total_duration=987.029500, train/accuracy=0.988444, train/loss=0.040531, train/mean_average_precision=0.199994, validation/accuracy=0.985340, validation/loss=0.050091, validation/mean_average_precision=0.168907, validation/num_examples=43793
I0406 00:29:41.216243 140617241605952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_2749.
I0406 00:29:41.216736 140617241605952 submission_runner.py:416] After logging and checkpointing eval at step 2749: RAM USED (GB) 29.4544384
I0406 00:30:48.070224 140565294941952 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.049159, loss=0.045873
I0406 00:30:48.074978 140617241605952 submission.py:296] 3000) loss = 0.046, grad_norm = 0.049
I0406 00:32:59.524190 140565286549248 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.027624, loss=0.042679
I0406 00:32:59.529107 140617241605952 submission.py:296] 3500) loss = 0.043, grad_norm = 0.028
I0406 00:33:41.302714 140617241605952 submission_runner.py:373] Before eval at step 3658: RAM USED (GB) 29.824045056
I0406 00:33:41.302930 140617241605952 spec.py:298] Evaluating on the training split.
W0406 00:33:55.398038 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:55.640542 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:55.642391 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:55.646281 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:55.646738 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:55.647569 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:55.647761 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:33:55.647820 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:09.841463 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:10.074374 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:10.079725 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:10.080648 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:10.081336 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:10.081511 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:10.081721 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:10.082014 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:34:33.816084 140617241605952 spec.py:310] Evaluating on the validation split.
W0406 00:34:34.967914 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.221642 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.222375 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.227390 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.228150 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.228211 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.228598 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.239530 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.508883 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.766978 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.767210 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.772417 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.772946 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.773272 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.773624 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:35.773806 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:34:38.209788 140617241605952 spec.py:326] Evaluating on the test split.
W0406 00:34:39.297054 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:39.557483 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:39.557951 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:39.563542 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:39.563558 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:39.563635 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:39.565246 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:39.570606 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:39.813662 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:40.078994 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:40.079228 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:40.085773 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:40.085854 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:40.085897 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:40.086331 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:34:40.086422 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:34:42.462986 140617241605952 submission_runner.py:382] Time since start: 1290.56s, 	Step: 3658, 	{'train/accuracy': 0.9885534218853007, 'train/loss': 0.03948410227894783, 'train/mean_average_precision': 0.212478897654412, 'validation/accuracy': 0.9856958676048768, 'validation/loss': 0.04871562868356705, 'validation/mean_average_precision': 0.18560182365305083, 'validation/num_examples': 43793, 'test/accuracy': 0.9848344641708635, 'test/loss': 0.05108914151787758, 'test/mean_average_precision': 0.18961068142925921, 'test/num_examples': 43793}
I0406 00:34:42.463482 140617241605952 submission_runner.py:396] After eval at step 3658: RAM USED (GB) 30.760988672
I0406 00:34:42.472719 140565294941952 logging_writer.py:48] [3658] global_step=3658, preemption_count=0, score=962.240040, test/accuracy=0.984834, test/loss=0.051089, test/mean_average_precision=0.189611, test/num_examples=43793, total_duration=1290.561762, train/accuracy=0.988553, train/loss=0.039484, train/mean_average_precision=0.212479, validation/accuracy=0.985696, validation/loss=0.048716, validation/mean_average_precision=0.185602, validation/num_examples=43793
I0406 00:34:42.569938 140617241605952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_3658.
I0406 00:34:42.570511 140617241605952 submission_runner.py:416] After logging and checkpointing eval at step 3658: RAM USED (GB) 30.264950784
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0406 00:36:13.553059 140565286549248 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.030651, loss=0.041219
I0406 00:36:13.558752 140617241605952 submission.py:296] 4000) loss = 0.041, grad_norm = 0.031
I0406 00:38:22.898496 140565294941952 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.027141, loss=0.042210
I0406 00:38:22.904479 140617241605952 submission.py:296] 4500) loss = 0.042, grad_norm = 0.027
I0406 00:38:42.589958 140617241605952 submission_runner.py:373] Before eval at step 4577: RAM USED (GB) 30.414000128
I0406 00:38:42.590138 140617241605952 spec.py:298] Evaluating on the training split.
W0406 00:38:56.554492 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:56.794521 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:56.794566 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:56.799613 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:56.800043 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:56.800756 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:56.800883 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:38:56.801661 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:11.114652 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:11.357319 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:11.357389 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:11.362561 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:11.363204 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:11.363322 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:11.363791 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:11.363951 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:39:38.398389 140617241605952 spec.py:310] Evaluating on the validation split.
W0406 00:39:38.858474 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.115403 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.115662 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.121265 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.121373 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.121557 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.121966 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.122652 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.332689 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.578101 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.578346 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.584198 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.584417 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.584698 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.585572 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:39.585592 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:39:42.183415 140617241605952 spec.py:326] Evaluating on the test split.
W0406 00:39:42.637893 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:42.888561 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:42.889333 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:42.894376 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:42.894740 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:42.895335 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:42.895599 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:42.895960 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:43.104156 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:43.353954 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:43.354028 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:43.359621 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:43.359680 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:43.360534 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:43.360623 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:39:43.361010 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:39:45.877372 140617241605952 submission_runner.py:382] Time since start: 1591.85s, 	Step: 4577, 	{'train/accuracy': 0.9889870865897261, 'train/loss': 0.037635233253240585, 'train/mean_average_precision': 0.2434391413525325, 'validation/accuracy': 0.9858635211133817, 'validation/loss': 0.04742865264415741, 'validation/mean_average_precision': 0.20565609668388557, 'validation/num_examples': 43793, 'test/accuracy': 0.9850277925312916, 'test/loss': 0.050023578107357025, 'test/mean_average_precision': 0.20874458200436993, 'test/num_examples': 43793}
I0406 00:39:45.877784 140617241605952 submission_runner.py:396] After eval at step 4577: RAM USED (GB) 30.8057088
I0406 00:39:45.885685 140565286549248 logging_writer.py:48] [4577] global_step=4577, preemption_count=0, score=1201.249268, test/accuracy=0.985028, test/loss=0.050024, test/mean_average_precision=0.208745, test/num_examples=43793, total_duration=1591.849038, train/accuracy=0.988987, train/loss=0.037635, train/mean_average_precision=0.243439, validation/accuracy=0.985864, validation/loss=0.047429, validation/mean_average_precision=0.205656, validation/num_examples=43793
I0406 00:39:45.974920 140617241605952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_4577.
I0406 00:39:45.975393 140617241605952 submission_runner.py:416] After logging and checkpointing eval at step 4577: RAM USED (GB) 30.805291008
I0406 00:41:36.724304 140565294941952 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.014234, loss=0.038271
I0406 00:41:36.729268 140617241605952 submission.py:296] 5000) loss = 0.038, grad_norm = 0.014
I0406 00:43:46.172553 140617241605952 submission_runner.py:373] Before eval at step 5494: RAM USED (GB) 30.935212032
I0406 00:43:46.172763 140617241605952 spec.py:298] Evaluating on the training split.
W0406 00:44:01.033826 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:01.275036 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:01.281038 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:01.281492 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:01.281509 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:01.281622 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:01.282270 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:01.283580 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:16.148231 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:16.386840 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:16.387219 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:16.393011 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:16.393066 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:16.393100 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:16.394485 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:16.394928 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:44:43.607997 140617241605952 spec.py:310] Evaluating on the validation split.
W0406 00:44:44.049046 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.304315 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.304640 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.309492 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.310125 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.311465 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.311496 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.311809 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.503923 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.748563 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.748632 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.754325 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.754840 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.755484 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.755712 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:44.756241 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:44:47.300919 140617241605952 spec.py:326] Evaluating on the test split.
W0406 00:44:47.749850 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.004250 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.004399 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.010235 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.010462 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.010502 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.010583 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.011253 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.220514 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.472545 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.472974 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.478632 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.478803 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.479091 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.479536 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:44:48.479927 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:44:51.953064 140617241605952 submission_runner.py:382] Time since start: 1895.43s, 	Step: 5494, 	{'train/accuracy': 0.989032403590527, 'train/loss': 0.037257932126522064, 'train/mean_average_precision': 0.2540232978735576, 'validation/accuracy': 0.9861034320662301, 'validation/loss': 0.04714905098080635, 'validation/mean_average_precision': 0.2151056513504299, 'validation/num_examples': 43793, 'test/accuracy': 0.9851693139454611, 'test/loss': 0.04959770664572716, 'test/mean_average_precision': 0.2209606820944136, 'test/num_examples': 43793}
I0406 00:44:51.953541 140617241605952 submission_runner.py:396] After eval at step 5494: RAM USED (GB) 31.161483264
I0406 00:44:51.962525 140565286549248 logging_writer.py:48] [5494] global_step=5494, preemption_count=0, score=1440.474396, test/accuracy=0.985169, test/loss=0.049598, test/mean_average_precision=0.220961, test/num_examples=43793, total_duration=1895.431638, train/accuracy=0.989032, train/loss=0.037258, train/mean_average_precision=0.254023, validation/accuracy=0.986103, validation/loss=0.047149, validation/mean_average_precision=0.215106, validation/num_examples=43793
I0406 00:44:52.052601 140617241605952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_5494.
I0406 00:44:52.053125 140617241605952 submission_runner.py:416] After logging and checkpointing eval at step 5494: RAM USED (GB) 31.161356288
I0406 00:44:54.123686 140565294941952 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.023957, loss=0.040847
I0406 00:44:54.128101 140617241605952 submission.py:296] 5500) loss = 0.041, grad_norm = 0.024
I0406 00:47:05.006278 140617241605952 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 31.307567104
I0406 00:47:05.006482 140617241605952 spec.py:298] Evaluating on the training split.
W0406 00:47:18.954487 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:19.216287 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:19.216538 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:19.222740 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:19.222846 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:19.222861 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:19.223530 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:19.223849 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:33.276601 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:33.517953 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:33.518105 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:33.522918 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:33.523446 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:33.524308 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:33.524334 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:47:33.524608 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:48:01.771845 140617241605952 spec.py:310] Evaluating on the validation split.
W0406 00:48:02.219349 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.471629 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.471673 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.478411 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.478489 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.478780 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.478873 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.479230 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.681925 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.934973 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.935082 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.940634 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.940759 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.941669 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.941977 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:02.941982 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:48:05.486571 140617241605952 spec.py:326] Evaluating on the test split.
W0406 00:48:05.933626 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.185871 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.186280 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.191689 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.191809 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.193156 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.193331 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.193715 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.394405 140617241605952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.640334 139715832743744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.640346 139689671128896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.646539 140491634988864 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.646539 139921904072512 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.646814 140218964391744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.647512 140037191497536 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:48:06.647573 140602268116800 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:48:09.138277 140617241605952 submission_runner.py:382] Time since start: 2094.27s, 	Step: 6000, 	{'train/accuracy': 0.9892524761075957, 'train/loss': 0.03652345761656761, 'train/mean_average_precision': 0.2801497188338813, 'validation/accuracy': 0.9861245409825551, 'validation/loss': 0.046827491372823715, 'validation/mean_average_precision': 0.21436635857806036, 'validation/num_examples': 43793, 'test/accuracy': 0.9852164877501843, 'test/loss': 0.04950886592268944, 'test/mean_average_precision': 0.21175796483270665, 'test/num_examples': 43793}
I0406 00:48:09.138671 140617241605952 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 31.714557952
I0406 00:48:09.147263 140565286549248 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1572.890289, test/accuracy=0.985216, test/loss=0.049509, test/mean_average_precision=0.211758, test/num_examples=43793, total_duration=2094.265334, train/accuracy=0.989252, train/loss=0.036523, train/mean_average_precision=0.280150, validation/accuracy=0.986125, validation/loss=0.046827, validation/mean_average_precision=0.214366, validation/num_examples=43793
I0406 00:48:09.236478 140617241605952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0406 00:48:09.236973 140617241605952 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 31.714136064
I0406 00:48:09.244387 140565294941952 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1572.890289
I0406 00:48:09.394992 140617241605952 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0406 00:48:09.552047 140617241605952 submission_runner.py:550] Tuning trial 1/1
I0406 00:48:09.552265 140617241605952 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0406 00:48:09.553378 140617241605952 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4264367895287525, 'train/loss': 0.7727322578430176, 'train/mean_average_precision': 0.02069788431423971, 'validation/accuracy': 0.4293835709304242, 'validation/loss': 0.7737170457839966, 'validation/mean_average_precision': 0.026079616779733864, 'validation/num_examples': 43793, 'test/accuracy': 0.4310954978921312, 'test/loss': 0.7734853625297546, 'test/mean_average_precision': 0.02720952804873771, 'test/num_examples': 43793, 'score': 5.442570447921753, 'total_duration': 5.444582939147949, 'global_step': 1, 'preemption_count': 0}), (921, {'train/accuracy': 0.986898522965536, 'train/loss': 0.04968801885843277, 'train/mean_average_precision': 0.07046519534189623, 'validation/accuracy': 0.984290094965767, 'validation/loss': 0.059444330632686615, 'validation/mean_average_precision': 0.07129294700383709, 'validation/num_examples': 43793, 'test/accuracy': 0.9833044744774975, 'test/loss': 0.06273513287305832, 'test/mean_average_precision': 0.07177770538680854, 'test/num_examples': 43793, 'score': 244.59219193458557, 'total_duration': 384.15426898002625, 'global_step': 921, 'preemption_count': 0}), (1833, {'train/accuracy': 0.988069692658786, 'train/loss': 0.04282639920711517, 'train/mean_average_precision': 0.1485575101527088, 'validation/accuracy': 0.9850617882337277, 'validation/loss': 0.052272941917181015, 'validation/mean_average_precision': 0.1401149257564459, 'validation/num_examples': 43793, 'test/accuracy': 0.984087685994308, 'test/loss': 0.055038757622241974, 'test/mean_average_precision': 0.1427954543031437, 'test/num_examples': 43793, 'score': 483.85315775871277, 'total_duration': 684.5389695167542, 'global_step': 1833, 'preemption_count': 0}), (2749, {'train/accuracy': 0.9884436196977583, 'train/loss': 0.040531158447265625, 'train/mean_average_precision': 0.19999430790410594, 'validation/accuracy': 0.9853398576122405, 'validation/loss': 0.05009075254201889, 'validation/mean_average_precision': 0.16890673272053072, 'validation/num_examples': 43793, 'test/accuracy': 0.9844810818301246, 'test/loss': 0.05252925306558609, 'test/mean_average_precision': 0.16947504173196426, 'test/num_examples': 43793, 'score': 723.1118547916412, 'total_duration': 987.0295004844666, 'global_step': 2749, 'preemption_count': 0}), (3658, {'train/accuracy': 0.9885534218853007, 'train/loss': 0.03948410227894783, 'train/mean_average_precision': 0.212478897654412, 'validation/accuracy': 0.9856958676048768, 'validation/loss': 0.04871562868356705, 'validation/mean_average_precision': 0.18560182365305083, 'validation/num_examples': 43793, 'test/accuracy': 0.9848344641708635, 'test/loss': 0.05108914151787758, 'test/mean_average_precision': 0.18961068142925921, 'test/num_examples': 43793, 'score': 962.2400403022766, 'total_duration': 1290.561761856079, 'global_step': 3658, 'preemption_count': 0}), (4577, {'train/accuracy': 0.9889870865897261, 'train/loss': 0.037635233253240585, 'train/mean_average_precision': 0.2434391413525325, 'validation/accuracy': 0.9858635211133817, 'validation/loss': 0.04742865264415741, 'validation/mean_average_precision': 0.20565609668388557, 'validation/num_examples': 43793, 'test/accuracy': 0.9850277925312916, 'test/loss': 0.050023578107357025, 'test/mean_average_precision': 0.20874458200436993, 'test/num_examples': 43793, 'score': 1201.2492678165436, 'total_duration': 1591.849037885666, 'global_step': 4577, 'preemption_count': 0}), (5494, {'train/accuracy': 0.989032403590527, 'train/loss': 0.037257932126522064, 'train/mean_average_precision': 0.2540232978735576, 'validation/accuracy': 0.9861034320662301, 'validation/loss': 0.04714905098080635, 'validation/mean_average_precision': 0.2151056513504299, 'validation/num_examples': 43793, 'test/accuracy': 0.9851693139454611, 'test/loss': 0.04959770664572716, 'test/mean_average_precision': 0.2209606820944136, 'test/num_examples': 43793, 'score': 1440.4743962287903, 'total_duration': 1895.4316375255585, 'global_step': 5494, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9892524761075957, 'train/loss': 0.03652345761656761, 'train/mean_average_precision': 0.2801497188338813, 'validation/accuracy': 0.9861245409825551, 'validation/loss': 0.046827491372823715, 'validation/mean_average_precision': 0.21436635857806036, 'validation/num_examples': 43793, 'test/accuracy': 0.9852164877501843, 'test/loss': 0.04950886592268944, 'test/mean_average_precision': 0.21175796483270665, 'test/num_examples': 43793, 'score': 1572.890289068222, 'total_duration': 2094.265334367752, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0406 00:48:09.553495 140617241605952 submission_runner.py:553] Timing: 1572.890289068222
I0406 00:48:09.553538 140617241605952 submission_runner.py:554] ====================
I0406 00:48:09.553645 140617241605952 submission_runner.py:613] Final ogbg score: 1572.890289068222
