WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 07:32:46.848324 139892204111680 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 07:32:46.848349 139820659869504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 07:32:46.848363 139836627568448 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 07:32:46.849063 139737384220480 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 07:32:46.849230 139836660827968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 07:32:46.849389 139737384220480 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:32:46.849259 140616693290816 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 07:32:46.849385 139862760970048 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 07:32:46.849535 139836660827968 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:32:46.849559 140616693290816 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:32:46.849417 139728969574208 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 07:32:46.849731 139862760970048 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:32:46.849806 139728969574208 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:32:46.858992 139892204111680 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:32:46.859017 139820659869504 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:32:46.859040 139836627568448 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:32:46.866509 139862760970048 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_adamw/criteo1tb_pytorch.
W0406 07:32:46.902348 139737384220480 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:32:46.902817 139836660827968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:32:46.903165 139820659869504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:32:46.904164 140616693290816 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:32:46.905025 139836627568448 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:32:46.905311 139728969574208 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:32:46.905555 139892204111680 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:32:46.906019 139862760970048 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 07:32:46.909629 139862760970048 submission_runner.py:511] Using RNG seed 3051318352
I0406 07:32:46.910988 139862760970048 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 07:32:46.911096 139862760970048 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_adamw/criteo1tb_pytorch/trial_1.
I0406 07:32:46.911287 139862760970048 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_adamw/criteo1tb_pytorch/trial_1/hparams.json.
I0406 07:32:46.912250 139862760970048 submission_runner.py:230] Starting train once: RAM USED (GB) 5.6351744
I0406 07:32:46.912345 139862760970048 submission_runner.py:231] Initializing dataset.
I0406 07:32:46.912521 139862760970048 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.6351744
I0406 07:32:46.912584 139862760970048 submission_runner.py:240] Initializing model.
I0406 07:33:02.018235 139862760970048 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.287304192
I0406 07:33:02.018418 139862760970048 submission_runner.py:252] Initializing optimizer.
I0406 07:33:02.018994 139862760970048 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.287304192
I0406 07:33:02.019081 139862760970048 submission_runner.py:261] Initializing metrics bundle.
I0406 07:33:02.019133 139862760970048 submission_runner.py:276] Initializing checkpoint and logger.
I0406 07:33:02.022729 139862760970048 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 07:33:02.022836 139862760970048 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 07:33:02.622328 139862760970048 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_adamw/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0406 07:33:02.623147 139862760970048 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_adamw/criteo1tb_pytorch/trial_1/flags_0.json.
I0406 07:33:02.664764 139862760970048 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.33886464
I0406 07:33:02.665819 139862760970048 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.33886464
I0406 07:33:02.665959 139862760970048 submission_runner.py:313] Starting training loop.
I0406 07:35:33.393113 139862760970048 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 54.133649408
I0406 07:35:36.682106 139779723224832 logging_writer.py:48] [0] global_step=0, grad_norm=9.615694, loss=0.857997
I0406 07:35:36.686853 139862760970048 submission.py:119] 0) loss = 0.858, grad_norm = 9.616
I0406 07:35:36.687221 139862760970048 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 58.21693952
I0406 07:35:36.687750 139862760970048 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 58.217455616
I0406 07:35:36.687871 139862760970048 spec.py:298] Evaluating on the training split.
I0406 07:45:24.667637 139862760970048 spec.py:310] Evaluating on the validation split.
I0406 07:50:40.161175 139862760970048 spec.py:326] Evaluating on the test split.
I0406 07:54:39.947061 139862760970048 submission_runner.py:382] Time since start: 154.02s, 	Step: 1, 	{'train/loss': 0.8575717906376472, 'validation/loss': 0.8560069213483146, 'validation/num_examples': 89000000, 'test/loss': 0.8560560823114857, 'test/num_examples': 89274637}
I0406 07:54:39.947514 139862760970048 submission_runner.py:396] After eval at step 1: RAM USED (GB) 105.181495296
I0406 07:54:39.957711 139721568110336 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=154.020363, test/loss=0.856056, test/num_examples=89274637, total_duration=154.022398, train/loss=0.857572, validation/loss=0.856007, validation/num_examples=89000000
I0406 07:54:53.797334 139862760970048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/criteo1tb_pytorch/trial_1/checkpoint_1.
I0406 07:54:53.797791 139862760970048 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 105.214173184
I0406 07:54:53.814542 139862760970048 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 105.173147648
I0406 07:54:53.817835 139862760970048 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:53.817829 139892204111680 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:53.817825 140616693290816 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:53.817826 139737384220480 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:53.817833 139820659869504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:53.817831 139728969574208 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:53.817838 139836627568448 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:53.817839 139836660827968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:55.596885 139721559717632 logging_writer.py:48] [1] global_step=1, grad_norm=9.590942, loss=0.857628
I0406 07:54:55.610001 139862760970048 submission.py:119] 1) loss = 0.858, grad_norm = 9.591
I0406 07:54:55.610505 139862760970048 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 105.2096512
I0406 07:54:57.348036 139721568110336 logging_writer.py:48] [2] global_step=2, grad_norm=9.469723, loss=0.844295
I0406 07:54:57.360328 139862760970048 submission.py:119] 2) loss = 0.844, grad_norm = 9.470
I0406 07:54:59.124682 139721559717632 logging_writer.py:48] [3] global_step=3, grad_norm=9.193894, loss=0.817964
I0406 07:54:59.132595 139862760970048 submission.py:119] 3) loss = 0.818, grad_norm = 9.194
I0406 07:55:00.888620 139721568110336 logging_writer.py:48] [4] global_step=4, grad_norm=8.760797, loss=0.780037
I0406 07:55:00.900211 139862760970048 submission.py:119] 4) loss = 0.780, grad_norm = 8.761
I0406 07:55:02.678401 139721559717632 logging_writer.py:48] [5] global_step=5, grad_norm=8.211150, loss=0.733111
I0406 07:55:02.689691 139862760970048 submission.py:119] 5) loss = 0.733, grad_norm = 8.211
I0406 07:55:04.476909 139721568110336 logging_writer.py:48] [6] global_step=6, grad_norm=7.617798, loss=0.679049
I0406 07:55:04.487960 139862760970048 submission.py:119] 6) loss = 0.679, grad_norm = 7.618
I0406 07:55:06.213528 139721559717632 logging_writer.py:48] [7] global_step=7, grad_norm=7.034025, loss=0.619938
I0406 07:55:06.232387 139862760970048 submission.py:119] 7) loss = 0.620, grad_norm = 7.034
I0406 07:55:07.959862 139721568110336 logging_writer.py:48] [8] global_step=8, grad_norm=6.425640, loss=0.557115
I0406 07:55:07.973580 139862760970048 submission.py:119] 8) loss = 0.557, grad_norm = 6.426
I0406 07:55:09.701547 139721559717632 logging_writer.py:48] [9] global_step=9, grad_norm=5.733734, loss=0.493594
I0406 07:55:09.714078 139862760970048 submission.py:119] 9) loss = 0.494, grad_norm = 5.734
I0406 07:55:11.451450 139721568110336 logging_writer.py:48] [10] global_step=10, grad_norm=5.012899, loss=0.431607
I0406 07:55:11.463990 139862760970048 submission.py:119] 10) loss = 0.432, grad_norm = 5.013
I0406 07:55:13.228512 139721559717632 logging_writer.py:48] [11] global_step=11, grad_norm=4.298378, loss=0.372741
I0406 07:55:13.240468 139862760970048 submission.py:119] 11) loss = 0.373, grad_norm = 4.298
I0406 07:55:15.013259 139721568110336 logging_writer.py:48] [12] global_step=12, grad_norm=3.632751, loss=0.320280
I0406 07:55:15.026177 139862760970048 submission.py:119] 12) loss = 0.320, grad_norm = 3.633
I0406 07:55:16.780728 139721559717632 logging_writer.py:48] [13] global_step=13, grad_norm=2.963158, loss=0.273240
I0406 07:55:16.787395 139862760970048 submission.py:119] 13) loss = 0.273, grad_norm = 2.963
I0406 07:55:18.504714 139721568110336 logging_writer.py:48] [14] global_step=14, grad_norm=2.318369, loss=0.234122
I0406 07:55:18.516286 139862760970048 submission.py:119] 14) loss = 0.234, grad_norm = 2.318
I0406 07:55:20.234202 139721559717632 logging_writer.py:48] [15] global_step=15, grad_norm=1.695192, loss=0.205734
I0406 07:55:20.253608 139862760970048 submission.py:119] 15) loss = 0.206, grad_norm = 1.695
I0406 07:55:22.018576 139721568110336 logging_writer.py:48] [16] global_step=16, grad_norm=1.148536, loss=0.183945
I0406 07:55:22.029966 139862760970048 submission.py:119] 16) loss = 0.184, grad_norm = 1.149
I0406 07:55:23.737816 139721559717632 logging_writer.py:48] [17] global_step=17, grad_norm=0.702512, loss=0.169201
I0406 07:55:23.741150 139862760970048 submission.py:119] 17) loss = 0.169, grad_norm = 0.703
I0406 07:55:25.464589 139721568110336 logging_writer.py:48] [18] global_step=18, grad_norm=0.314244, loss=0.162767
I0406 07:55:25.469079 139862760970048 submission.py:119] 18) loss = 0.163, grad_norm = 0.314
I0406 07:55:27.187104 139721559717632 logging_writer.py:48] [19] global_step=19, grad_norm=0.120465, loss=0.159828
I0406 07:55:27.190406 139862760970048 submission.py:119] 19) loss = 0.160, grad_norm = 0.120
I0406 07:55:28.890948 139721568110336 logging_writer.py:48] [20] global_step=20, grad_norm=0.262474, loss=0.160082
I0406 07:55:28.941771 139862760970048 submission.py:119] 20) loss = 0.160, grad_norm = 0.262
I0406 07:55:30.645301 139721559717632 logging_writer.py:48] [21] global_step=21, grad_norm=0.407554, loss=0.161382
I0406 07:55:30.658743 139862760970048 submission.py:119] 21) loss = 0.161, grad_norm = 0.408
I0406 07:55:32.365100 139721568110336 logging_writer.py:48] [22] global_step=22, grad_norm=0.474903, loss=0.160358
I0406 07:55:32.368120 139862760970048 submission.py:119] 22) loss = 0.160, grad_norm = 0.475
I0406 07:55:34.121400 139721559717632 logging_writer.py:48] [23] global_step=23, grad_norm=0.520318, loss=0.162636
I0406 07:55:34.145933 139862760970048 submission.py:119] 23) loss = 0.163, grad_norm = 0.520
I0406 07:55:35.857984 139721568110336 logging_writer.py:48] [24] global_step=24, grad_norm=0.508819, loss=0.161546
I0406 07:55:35.870259 139862760970048 submission.py:119] 24) loss = 0.162, grad_norm = 0.509
I0406 07:55:37.597967 139721559717632 logging_writer.py:48] [25] global_step=25, grad_norm=0.486682, loss=0.161502
I0406 07:55:37.601122 139862760970048 submission.py:119] 25) loss = 0.162, grad_norm = 0.487
I0406 07:55:39.334551 139721568110336 logging_writer.py:48] [26] global_step=26, grad_norm=0.402563, loss=0.157337
I0406 07:55:39.347399 139862760970048 submission.py:119] 26) loss = 0.157, grad_norm = 0.403
I0406 07:55:41.053089 139721559717632 logging_writer.py:48] [27] global_step=27, grad_norm=0.329579, loss=0.156096
I0406 07:55:41.056173 139862760970048 submission.py:119] 27) loss = 0.156, grad_norm = 0.330
I0406 07:55:42.783389 139721568110336 logging_writer.py:48] [28] global_step=28, grad_norm=0.225259, loss=0.153033
I0406 07:55:42.797986 139862760970048 submission.py:119] 28) loss = 0.153, grad_norm = 0.225
I0406 07:55:44.547139 139721559717632 logging_writer.py:48] [29] global_step=29, grad_norm=0.152353, loss=0.152880
I0406 07:55:44.552988 139862760970048 submission.py:119] 29) loss = 0.153, grad_norm = 0.152
I0406 07:55:46.290130 139721568110336 logging_writer.py:48] [30] global_step=30, grad_norm=0.100362, loss=0.151829
I0406 07:55:46.294816 139862760970048 submission.py:119] 30) loss = 0.152, grad_norm = 0.100
I0406 07:55:48.009216 139721559717632 logging_writer.py:48] [31] global_step=31, grad_norm=0.084853, loss=0.152880
I0406 07:55:48.012215 139862760970048 submission.py:119] 31) loss = 0.153, grad_norm = 0.085
I0406 07:55:49.741217 139721568110336 logging_writer.py:48] [32] global_step=32, grad_norm=0.078217, loss=0.149504
I0406 07:55:49.764714 139862760970048 submission.py:119] 32) loss = 0.150, grad_norm = 0.078
I0406 07:55:51.511098 139721559717632 logging_writer.py:48] [33] global_step=33, grad_norm=0.074607, loss=0.146828
I0406 07:55:51.532419 139862760970048 submission.py:119] 33) loss = 0.147, grad_norm = 0.075
I0406 07:55:53.250869 139721568110336 logging_writer.py:48] [34] global_step=34, grad_norm=0.116086, loss=0.147887
I0406 07:55:53.265654 139862760970048 submission.py:119] 34) loss = 0.148, grad_norm = 0.116
I0406 07:55:54.994658 139721559717632 logging_writer.py:48] [35] global_step=35, grad_norm=0.096320, loss=0.147202
I0406 07:55:55.003366 139862760970048 submission.py:119] 35) loss = 0.147, grad_norm = 0.096
I0406 07:55:56.737205 139721568110336 logging_writer.py:48] [36] global_step=36, grad_norm=0.059215, loss=0.147704
I0406 07:55:56.748145 139862760970048 submission.py:119] 36) loss = 0.148, grad_norm = 0.059
I0406 07:55:58.443295 139721568110336 logging_writer.py:48] [37] global_step=37, grad_norm=0.083007, loss=0.144222
I0406 07:55:58.485702 139862760970048 submission.py:119] 37) loss = 0.144, grad_norm = 0.083
I0406 07:56:00.280908 139721559717632 logging_writer.py:48] [38] global_step=38, grad_norm=0.091712, loss=0.145127
I0406 07:56:00.293889 139862760970048 submission.py:119] 38) loss = 0.145, grad_norm = 0.092
I0406 07:56:02.109862 139721568110336 logging_writer.py:48] [39] global_step=39, grad_norm=0.071663, loss=0.142500
I0406 07:56:02.125176 139862760970048 submission.py:119] 39) loss = 0.143, grad_norm = 0.072
I0406 07:56:03.894115 139721559717632 logging_writer.py:48] [40] global_step=40, grad_norm=0.048300, loss=0.142074
I0406 07:56:03.923735 139862760970048 submission.py:119] 40) loss = 0.142, grad_norm = 0.048
I0406 07:56:05.702871 139721568110336 logging_writer.py:48] [41] global_step=41, grad_norm=0.043973, loss=0.140231
I0406 07:56:05.712139 139862760970048 submission.py:119] 41) loss = 0.140, grad_norm = 0.044
I0406 07:56:07.487356 139721559717632 logging_writer.py:48] [42] global_step=42, grad_norm=0.127953, loss=0.142323
I0406 07:56:07.495465 139862760970048 submission.py:119] 42) loss = 0.142, grad_norm = 0.128
I0406 07:56:09.222836 139721568110336 logging_writer.py:48] [43] global_step=43, grad_norm=0.119340, loss=0.142125
I0406 07:56:09.254372 139862760970048 submission.py:119] 43) loss = 0.142, grad_norm = 0.119
I0406 07:56:11.013049 139721559717632 logging_writer.py:48] [44] global_step=44, grad_norm=0.069974, loss=0.140258
I0406 07:56:11.021627 139862760970048 submission.py:119] 44) loss = 0.140, grad_norm = 0.070
I0406 07:56:12.796417 139721568110336 logging_writer.py:48] [45] global_step=45, grad_norm=0.064136, loss=0.140317
I0406 07:56:12.809459 139862760970048 submission.py:119] 45) loss = 0.140, grad_norm = 0.064
I0406 07:56:14.590341 139721559717632 logging_writer.py:48] [46] global_step=46, grad_norm=0.141032, loss=0.138521
I0406 07:56:14.614511 139862760970048 submission.py:119] 46) loss = 0.139, grad_norm = 0.141
I0406 07:56:16.374812 139721568110336 logging_writer.py:48] [47] global_step=47, grad_norm=0.198983, loss=0.137361
I0406 07:56:16.409149 139862760970048 submission.py:119] 47) loss = 0.137, grad_norm = 0.199
I0406 07:56:18.191484 139721559717632 logging_writer.py:48] [48] global_step=48, grad_norm=0.114776, loss=0.140548
I0406 07:56:18.198695 139862760970048 submission.py:119] 48) loss = 0.141, grad_norm = 0.115
I0406 07:56:19.980859 139721568110336 logging_writer.py:48] [49] global_step=49, grad_norm=0.077570, loss=0.137283
I0406 07:56:20.012125 139862760970048 submission.py:119] 49) loss = 0.137, grad_norm = 0.078
I0406 07:56:21.769871 139721559717632 logging_writer.py:48] [50] global_step=50, grad_norm=0.249503, loss=0.139604
I0406 07:56:21.825825 139862760970048 submission.py:119] 50) loss = 0.140, grad_norm = 0.250
I0406 07:56:23.604026 139721568110336 logging_writer.py:48] [51] global_step=51, grad_norm=0.253687, loss=0.139024
I0406 07:56:23.618241 139862760970048 submission.py:119] 51) loss = 0.139, grad_norm = 0.254
I0406 07:56:25.464888 139721559717632 logging_writer.py:48] [52] global_step=52, grad_norm=0.060458, loss=0.137557
I0406 07:56:25.471575 139862760970048 submission.py:119] 52) loss = 0.138, grad_norm = 0.060
I0406 07:56:27.241700 139721568110336 logging_writer.py:48] [53] global_step=53, grad_norm=0.155732, loss=0.137549
I0406 07:56:27.252310 139862760970048 submission.py:119] 53) loss = 0.138, grad_norm = 0.156
I0406 07:56:29.018498 139721559717632 logging_writer.py:48] [54] global_step=54, grad_norm=0.190539, loss=0.138293
I0406 07:56:29.028801 139862760970048 submission.py:119] 54) loss = 0.138, grad_norm = 0.191
I0406 07:56:30.812006 139721568110336 logging_writer.py:48] [55] global_step=55, grad_norm=0.093173, loss=0.138464
I0406 07:56:30.832180 139862760970048 submission.py:119] 55) loss = 0.138, grad_norm = 0.093
I0406 07:56:32.588220 139721559717632 logging_writer.py:48] [56] global_step=56, grad_norm=0.054636, loss=0.138772
I0406 07:56:32.599918 139862760970048 submission.py:119] 56) loss = 0.139, grad_norm = 0.055
I0406 07:56:34.379358 139721568110336 logging_writer.py:48] [57] global_step=57, grad_norm=0.166569, loss=0.135721
I0406 07:56:34.391789 139862760970048 submission.py:119] 57) loss = 0.136, grad_norm = 0.167
I0406 07:56:36.178029 139721559717632 logging_writer.py:48] [58] global_step=58, grad_norm=0.219246, loss=0.136662
I0406 07:56:36.184897 139862760970048 submission.py:119] 58) loss = 0.137, grad_norm = 0.219
I0406 07:56:37.946006 139721568110336 logging_writer.py:48] [59] global_step=59, grad_norm=0.136513, loss=0.137363
I0406 07:56:37.958171 139862760970048 submission.py:119] 59) loss = 0.137, grad_norm = 0.137
I0406 07:56:39.665951 139721559717632 logging_writer.py:48] [60] global_step=60, grad_norm=0.020534, loss=0.137080
I0406 07:56:39.676208 139862760970048 submission.py:119] 60) loss = 0.137, grad_norm = 0.021
I0406 07:56:41.388177 139721568110336 logging_writer.py:48] [61] global_step=61, grad_norm=0.086251, loss=0.134358
I0406 07:56:41.394559 139862760970048 submission.py:119] 61) loss = 0.134, grad_norm = 0.086
I0406 07:56:43.129018 139721559717632 logging_writer.py:48] [62] global_step=62, grad_norm=0.081575, loss=0.136298
I0406 07:56:43.140450 139862760970048 submission.py:119] 62) loss = 0.136, grad_norm = 0.082
I0406 07:56:44.878248 139721568110336 logging_writer.py:48] [63] global_step=63, grad_norm=0.079484, loss=0.135588
I0406 07:56:44.895067 139862760970048 submission.py:119] 63) loss = 0.136, grad_norm = 0.079
I0406 07:56:46.615721 139721559717632 logging_writer.py:48] [64] global_step=64, grad_norm=0.124058, loss=0.133531
I0406 07:56:46.631516 139862760970048 submission.py:119] 64) loss = 0.134, grad_norm = 0.124
I0406 07:56:48.380812 139721568110336 logging_writer.py:48] [65] global_step=65, grad_norm=0.305114, loss=0.134525
I0406 07:56:48.414628 139862760970048 submission.py:119] 65) loss = 0.135, grad_norm = 0.305
I0406 07:56:50.164894 139721559717632 logging_writer.py:48] [66] global_step=66, grad_norm=0.659573, loss=0.137923
I0406 07:56:50.185061 139862760970048 submission.py:119] 66) loss = 0.138, grad_norm = 0.660
I0406 07:56:51.957125 139721568110336 logging_writer.py:48] [67] global_step=67, grad_norm=0.700940, loss=0.138215
I0406 07:56:51.968970 139862760970048 submission.py:119] 67) loss = 0.138, grad_norm = 0.701
I0406 07:56:53.736248 139721559717632 logging_writer.py:48] [68] global_step=68, grad_norm=0.121541, loss=0.133030
I0406 07:56:53.759371 139862760970048 submission.py:119] 68) loss = 0.133, grad_norm = 0.122
I0406 07:56:55.534512 139721568110336 logging_writer.py:48] [69] global_step=69, grad_norm=0.526637, loss=0.133371
I0406 07:56:55.548189 139862760970048 submission.py:119] 69) loss = 0.133, grad_norm = 0.527
I0406 07:56:57.330739 139721559717632 logging_writer.py:48] [70] global_step=70, grad_norm=0.485125, loss=0.135104
I0406 07:56:57.342697 139862760970048 submission.py:119] 70) loss = 0.135, grad_norm = 0.485
I0406 07:56:59.099079 139721568110336 logging_writer.py:48] [71] global_step=71, grad_norm=0.131350, loss=0.131438
I0406 07:56:59.112386 139862760970048 submission.py:119] 71) loss = 0.131, grad_norm = 0.131
I0406 07:57:00.891563 139721559717632 logging_writer.py:48] [72] global_step=72, grad_norm=0.444941, loss=0.134136
I0406 07:57:00.912594 139862760970048 submission.py:119] 72) loss = 0.134, grad_norm = 0.445
I0406 07:57:02.687285 139721568110336 logging_writer.py:48] [73] global_step=73, grad_norm=0.149287, loss=0.132220
I0406 07:57:02.709040 139862760970048 submission.py:119] 73) loss = 0.132, grad_norm = 0.149
I0406 07:57:04.495042 139721559717632 logging_writer.py:48] [74] global_step=74, grad_norm=0.263876, loss=0.133178
I0406 07:57:04.514143 139862760970048 submission.py:119] 74) loss = 0.133, grad_norm = 0.264
I0406 07:57:06.347812 139721568110336 logging_writer.py:48] [75] global_step=75, grad_norm=0.241978, loss=0.132201
I0406 07:57:06.358976 139862760970048 submission.py:119] 75) loss = 0.132, grad_norm = 0.242
I0406 07:57:08.179876 139721559717632 logging_writer.py:48] [76] global_step=76, grad_norm=0.078489, loss=0.131163
I0406 07:57:08.188742 139862760970048 submission.py:119] 76) loss = 0.131, grad_norm = 0.078
I0406 07:57:09.997544 139721568110336 logging_writer.py:48] [77] global_step=77, grad_norm=0.267008, loss=0.131859
I0406 07:57:10.006375 139862760970048 submission.py:119] 77) loss = 0.132, grad_norm = 0.267
I0406 07:57:11.785828 139721559717632 logging_writer.py:48] [78] global_step=78, grad_norm=0.168641, loss=0.129365
I0406 07:57:11.818425 139862760970048 submission.py:119] 78) loss = 0.129, grad_norm = 0.169
I0406 07:57:13.630431 139721568110336 logging_writer.py:48] [79] global_step=79, grad_norm=0.032547, loss=0.131317
I0406 07:57:13.640639 139862760970048 submission.py:119] 79) loss = 0.131, grad_norm = 0.033
I0406 07:57:15.410474 139721559717632 logging_writer.py:48] [80] global_step=80, grad_norm=0.140934, loss=0.131575
I0406 07:57:15.419812 139862760970048 submission.py:119] 80) loss = 0.132, grad_norm = 0.141
I0406 07:57:17.212621 139721568110336 logging_writer.py:48] [81] global_step=81, grad_norm=0.116496, loss=0.130102
I0406 07:57:17.247723 139862760970048 submission.py:119] 81) loss = 0.130, grad_norm = 0.116
I0406 07:57:19.023034 139721559717632 logging_writer.py:48] [82] global_step=82, grad_norm=0.047969, loss=0.130683
I0406 07:57:19.067266 139862760970048 submission.py:119] 82) loss = 0.131, grad_norm = 0.048
I0406 07:57:20.857192 139721568110336 logging_writer.py:48] [83] global_step=83, grad_norm=0.011478, loss=0.129730
I0406 07:57:20.872199 139862760970048 submission.py:119] 83) loss = 0.130, grad_norm = 0.011
I0406 07:57:22.641272 139721559717632 logging_writer.py:48] [84] global_step=84, grad_norm=0.046311, loss=0.129403
I0406 07:57:22.670989 139862760970048 submission.py:119] 84) loss = 0.129, grad_norm = 0.046
I0406 07:57:24.436953 139721568110336 logging_writer.py:48] [85] global_step=85, grad_norm=0.114335, loss=0.128290
I0406 07:57:24.459805 139862760970048 submission.py:119] 85) loss = 0.128, grad_norm = 0.114
I0406 07:57:26.217606 139721559717632 logging_writer.py:48] [86] global_step=86, grad_norm=0.230810, loss=0.130119
I0406 07:57:26.224614 139862760970048 submission.py:119] 86) loss = 0.130, grad_norm = 0.231
I0406 07:57:27.980592 139721568110336 logging_writer.py:48] [87] global_step=87, grad_norm=0.340919, loss=0.131348
I0406 07:57:28.004575 139862760970048 submission.py:119] 87) loss = 0.131, grad_norm = 0.341
I0406 07:57:29.774027 139721559717632 logging_writer.py:48] [88] global_step=88, grad_norm=0.366673, loss=0.131641
I0406 07:57:29.803383 139862760970048 submission.py:119] 88) loss = 0.132, grad_norm = 0.367
I0406 07:57:31.584341 139721568110336 logging_writer.py:48] [89] global_step=89, grad_norm=0.222096, loss=0.129264
I0406 07:57:31.595721 139862760970048 submission.py:119] 89) loss = 0.129, grad_norm = 0.222
I0406 07:57:33.393433 139721559717632 logging_writer.py:48] [90] global_step=90, grad_norm=0.059331, loss=0.128919
I0406 07:57:33.409863 139862760970048 submission.py:119] 90) loss = 0.129, grad_norm = 0.059
I0406 07:57:35.181739 139721568110336 logging_writer.py:48] [91] global_step=91, grad_norm=0.318427, loss=0.127957
I0406 07:57:35.207689 139862760970048 submission.py:119] 91) loss = 0.128, grad_norm = 0.318
I0406 07:57:36.971477 139721559717632 logging_writer.py:48] [92] global_step=92, grad_norm=0.404479, loss=0.129829
I0406 07:57:36.976271 139862760970048 submission.py:119] 92) loss = 0.130, grad_norm = 0.404
I0406 07:57:38.768964 139721568110336 logging_writer.py:48] [93] global_step=93, grad_norm=0.207095, loss=0.128256
I0406 07:57:38.772272 139862760970048 submission.py:119] 93) loss = 0.128, grad_norm = 0.207
I0406 07:57:40.609983 139721559717632 logging_writer.py:48] [94] global_step=94, grad_norm=0.100752, loss=0.127359
I0406 07:57:40.613157 139862760970048 submission.py:119] 94) loss = 0.127, grad_norm = 0.101
I0406 07:57:42.395855 139721568110336 logging_writer.py:48] [95] global_step=95, grad_norm=0.313065, loss=0.131260
I0406 07:57:42.398845 139862760970048 submission.py:119] 95) loss = 0.131, grad_norm = 0.313
I0406 07:57:44.204563 139721559717632 logging_writer.py:48] [96] global_step=96, grad_norm=0.343326, loss=0.130124
I0406 07:57:44.207575 139862760970048 submission.py:119] 96) loss = 0.130, grad_norm = 0.343
I0406 07:57:45.986685 139721568110336 logging_writer.py:48] [97] global_step=97, grad_norm=0.154644, loss=0.128343
I0406 07:57:45.989631 139862760970048 submission.py:119] 97) loss = 0.128, grad_norm = 0.155
I0406 07:57:47.771783 139721559717632 logging_writer.py:48] [98] global_step=98, grad_norm=0.121679, loss=0.129685
I0406 07:57:47.775247 139862760970048 submission.py:119] 98) loss = 0.130, grad_norm = 0.122
I0406 07:57:49.547897 139721568110336 logging_writer.py:48] [99] global_step=99, grad_norm=0.268153, loss=0.131714
I0406 07:57:49.550941 139862760970048 submission.py:119] 99) loss = 0.132, grad_norm = 0.268
I0406 07:57:51.334983 139721559717632 logging_writer.py:48] [100] global_step=100, grad_norm=0.168258, loss=0.127968
I0406 07:57:51.338215 139862760970048 submission.py:119] 100) loss = 0.128, grad_norm = 0.168
I0406 08:03:55.654045 139862760970048 submission_runner.py:373] Before eval at step 309: RAM USED (GB) 113.213145088
I0406 08:03:55.654238 139862760970048 spec.py:298] Evaluating on the training split.
I0406 08:14:09.749993 139862760970048 spec.py:310] Evaluating on the validation split.
I0406 08:19:19.152029 139862760970048 spec.py:326] Evaluating on the test split.
I0406 08:24:16.462762 139862760970048 submission_runner.py:382] Time since start: 1852.88s, 	Step: 309, 	{'train/loss': 0.12702040289850072, 'validation/loss': 0.1277248202247191, 'validation/num_examples': 89000000, 'test/loss': 0.13009492270464232, 'test/num_examples': 89274637}
I0406 08:24:16.463186 139862760970048 submission_runner.py:396] After eval at step 309: RAM USED (GB) 116.661624832
I0406 08:24:16.471235 139721534539520 logging_writer.py:48] [309] global_step=309, preemption_count=0, score=673.060589, test/loss=0.130095, test/num_examples=89274637, total_duration=1852.879559, train/loss=0.127020, validation/loss=0.127725, validation/num_examples=89000000
I0406 08:24:30.417489 139862760970048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/criteo1tb_pytorch/trial_1/checkpoint_309.
I0406 08:24:30.417915 139862760970048 submission_runner.py:416] After logging and checkpointing eval at step 309: RAM USED (GB) 116.63976448
I0406 08:30:03.245337 139721526146816 logging_writer.py:48] [500] global_step=500, grad_norm=0.075034, loss=0.124630
I0406 08:30:03.249518 139862760970048 submission.py:119] 500) loss = 0.125, grad_norm = 0.075
I0406 08:33:30.626315 139862760970048 submission_runner.py:373] Before eval at step 621: RAM USED (GB) 118.864474112
I0406 08:33:30.626514 139862760970048 spec.py:298] Evaluating on the training split.
I0406 08:43:38.151738 139862760970048 spec.py:310] Evaluating on the validation split.
I0406 08:48:38.260151 139862760970048 spec.py:326] Evaluating on the test split.
I0406 08:52:39.612884 139862760970048 submission_runner.py:382] Time since start: 3627.86s, 	Step: 621, 	{'train/loss': 0.12482190219859501, 'validation/loss': 0.12676258426966291, 'validation/num_examples': 89000000, 'test/loss': 0.12898959197112164, 'test/num_examples': 89274637}
I0406 08:52:39.613337 139862760970048 submission_runner.py:396] After eval at step 621: RAM USED (GB) 121.143259136
I0406 08:52:39.621572 139719371384576 logging_writer.py:48] [621] global_step=621, preemption_count=0, score=1179.353453, test/loss=0.128990, test/num_examples=89274637, total_duration=3627.856709, train/loss=0.124822, validation/loss=0.126763, validation/num_examples=89000000
I0406 08:52:53.538813 139862760970048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/criteo1tb_pytorch/trial_1/checkpoint_621.
I0406 08:52:53.539240 139862760970048 submission_runner.py:416] After logging and checkpointing eval at step 621: RAM USED (GB) 121.091559424
I0406 08:58:04.274624 139862760970048 submission_runner.py:373] Before eval at step 800: RAM USED (GB) 122.713182208
I0406 08:58:04.274819 139862760970048 spec.py:298] Evaluating on the training split.
I0406 09:08:00.682864 139862760970048 spec.py:310] Evaluating on the validation split.
I0406 09:13:01.537882 139862760970048 spec.py:326] Evaluating on the test split.
I0406 09:17:45.960270 139862760970048 submission_runner.py:382] Time since start: 5101.50s, 	Step: 800, 	{'train/loss': 0.12546533131266097, 'validation/loss': 0.12624112359550563, 'validation/num_examples': 89000000, 'test/loss': 0.1285138017419214, 'test/num_examples': 89274637}
I0406 09:17:45.960758 139862760970048 submission_runner.py:396] After eval at step 800: RAM USED (GB) 124.858167296
I0406 09:17:45.968891 139719337813760 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1470.539795, test/loss=0.128514, test/num_examples=89274637, total_duration=5101.499499, train/loss=0.125465, validation/loss=0.126241, validation/num_examples=89000000
I0406 09:18:01.818039 139862760970048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/criteo1tb_pytorch/trial_1/checkpoint_800.
I0406 09:18:01.818560 139862760970048 submission_runner.py:416] After logging and checkpointing eval at step 800: RAM USED (GB) 124.860960768
I0406 09:18:01.826436 139719329421056 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1470.539795
I0406 09:18:20.915222 139862760970048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/criteo1tb_pytorch/trial_1/checkpoint_800.
I0406 09:20:21.756416 139862760970048 submission_runner.py:550] Tuning trial 1/1
I0406 09:20:21.756638 139862760970048 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0406 09:20:21.761862 139862760970048 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/loss': 0.8575717906376472, 'validation/loss': 0.8560069213483146, 'validation/num_examples': 89000000, 'test/loss': 0.8560560823114857, 'test/num_examples': 89274637, 'score': 154.02036333084106, 'total_duration': 154.0223982334137, 'global_step': 1, 'preemption_count': 0}), (309, {'train/loss': 0.12702040289850072, 'validation/loss': 0.1277248202247191, 'validation/num_examples': 89000000, 'test/loss': 0.13009492270464232, 'test/num_examples': 89274637, 'score': 673.0605890750885, 'total_duration': 1852.8795585632324, 'global_step': 309, 'preemption_count': 0}), (621, {'train/loss': 0.12482190219859501, 'validation/loss': 0.12676258426966291, 'validation/num_examples': 89000000, 'test/loss': 0.12898959197112164, 'test/num_examples': 89274637, 'score': 1179.3534529209137, 'total_duration': 3627.8567085266113, 'global_step': 621, 'preemption_count': 0}), (800, {'train/loss': 0.12546533131266097, 'validation/loss': 0.12624112359550563, 'validation/num_examples': 89000000, 'test/loss': 0.1285138017419214, 'test/num_examples': 89274637, 'score': 1470.539794921875, 'total_duration': 5101.499499082565, 'global_step': 800, 'preemption_count': 0})], 'global_step': 800}
I0406 09:20:21.763008 139862760970048 submission_runner.py:553] Timing: 1470.539794921875
I0406 09:20:21.763074 139862760970048 submission_runner.py:554] ====================
I0406 09:20:21.763159 139862760970048 submission_runner.py:613] Final criteo1tb score: 1470.539794921875
