WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 21:46:07.736229 140034035423040 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 21:46:07.736291 139677268350784 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 21:46:07.736295 140501915567936 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 21:46:07.736339 140271176836928 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 21:46:07.736922 139792356681536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 21:46:07.736968 139945620657984 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 21:46:07.737227 139792356681536 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:46:07.737288 139945620657984 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:46:07.737194 140049088534336 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 21:46:07.737244 140669331363648 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 21:46:07.737552 140049088534336 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:46:07.737645 140669331363648 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:46:07.746905 140034035423040 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:46:07.746958 140271176836928 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:46:07.747007 140501915567936 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:46:07.747030 139677268350784 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:46:09.838402 140669331363648 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch.
W0405 21:46:09.839113 140034035423040 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:46:09.840512 139792356681536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:46:09.840929 139945620657984 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:46:09.841355 139677268350784 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:46:09.841416 140271176836928 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:46:09.841718 140501915567936 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:46:09.869538 140669331363648 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:46:09.871853 140049088534336 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 21:46:09.873256 140669331363648 submission_runner.py:511] Using RNG seed 2830671339
I0405 21:46:09.874405 140669331363648 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 21:46:09.874524 140669331363648 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1.
I0405 21:46:09.874732 140669331363648 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0405 21:46:09.875664 140669331363648 submission_runner.py:230] Starting train once: RAM USED (GB) 5.727141888
I0405 21:46:09.875756 140669331363648 submission_runner.py:231] Initializing dataset.
I0405 21:46:14.135048 140669331363648 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 7.740944384
I0405 21:46:14.135218 140669331363648 submission_runner.py:240] Initializing model.
I0405 21:46:18.553085 140669331363648 submission_runner.py:251] After Initializing model: RAM USED (GB) 17.972314112
I0405 21:46:18.553268 140669331363648 submission_runner.py:252] Initializing optimizer.
I0405 21:46:18.554624 140669331363648 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 17.972314112
I0405 21:46:18.554745 140669331363648 submission_runner.py:261] Initializing metrics bundle.
I0405 21:46:18.554807 140669331363648 submission_runner.py:276] Initializing checkpoint and logger.
I0405 21:46:19.246770 140669331363648 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0405 21:46:19.248498 140669331363648 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0405 21:46:19.290585 140669331363648 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 18.024996864
I0405 21:46:19.291755 140669331363648 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.024996864
I0405 21:46:19.291879 140669331363648 submission_runner.py:313] Starting training loop.
I0405 21:46:21.770077 140669331363648 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 23.344336896
I0405 21:46:25.743488 140640490866432 logging_writer.py:48] [0] global_step=0, grad_norm=0.338024, loss=6.907756
I0405 21:46:25.755190 140669331363648 submission.py:119] 0) loss = 6.908, grad_norm = 0.338
I0405 21:46:25.756152 140669331363648 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.732371456
I0405 21:46:25.756793 140669331363648 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.732371456
I0405 21:46:25.756927 140669331363648 spec.py:298] Evaluating on the training split.
I0405 21:47:21.046605 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 21:48:09.210017 140669331363648 spec.py:326] Evaluating on the test split.
I0405 21:48:09.226916 140669331363648 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 21:48:09.233095 140669331363648 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0405 21:48:09.316154 140669331363648 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 21:48:22.313124 140669331363648 submission_runner.py:382] Time since start: 6.47s, 	Step: 1, 	{'train/accuracy': 0.00169921875, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.0018, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0021, 'test/loss': 6.90775546875, 'test/num_examples': 10000}
I0405 21:48:22.313574 140669331363648 submission_runner.py:396] After eval at step 1: RAM USED (GB) 92.282916864
I0405 21:48:22.322911 140635482879744 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.463491, test/accuracy=0.002100, test/loss=6.907755, test/num_examples=10000, total_duration=6.465542, train/accuracy=0.001699, train/loss=6.907756, validation/accuracy=0.001800, validation/loss=6.907756, validation/num_examples=50000
I0405 21:48:22.745990 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_1.
I0405 21:48:22.746612 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 92.284735488
I0405 21:48:22.753201 140669331363648 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 92.286251008
I0405 21:48:22.760073 140669331363648 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:48:22.760207 140501915567936 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:48:22.760658 139945620657984 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:48:22.760697 140034035423040 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:48:22.760696 139677268350784 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:48:22.760612 140049088534336 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:48:22.761135 140271176836928 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:48:22.761276 139792356681536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:48:23.359056 140635474487040 logging_writer.py:48] [1] global_step=1, grad_norm=0.343797, loss=6.907756
I0405 21:48:23.362107 140669331363648 submission.py:119] 1) loss = 6.908, grad_norm = 0.344
I0405 21:48:23.362967 140669331363648 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 92.342689792
I0405 21:48:23.797937 140635482879744 logging_writer.py:48] [2] global_step=2, grad_norm=0.353782, loss=6.907755
I0405 21:48:23.806054 140669331363648 submission.py:119] 2) loss = 6.908, grad_norm = 0.354
I0405 21:48:24.206993 140635474487040 logging_writer.py:48] [3] global_step=3, grad_norm=0.345864, loss=6.907752
I0405 21:48:24.210464 140669331363648 submission.py:119] 3) loss = 6.908, grad_norm = 0.346
I0405 21:48:24.609805 140635482879744 logging_writer.py:48] [4] global_step=4, grad_norm=0.340000, loss=6.907753
I0405 21:48:24.613986 140669331363648 submission.py:119] 4) loss = 6.908, grad_norm = 0.340
I0405 21:48:25.035527 140635474487040 logging_writer.py:48] [5] global_step=5, grad_norm=0.342063, loss=6.907752
I0405 21:48:25.040333 140669331363648 submission.py:119] 5) loss = 6.908, grad_norm = 0.342
I0405 21:48:25.442028 140635482879744 logging_writer.py:48] [6] global_step=6, grad_norm=0.348236, loss=6.907739
I0405 21:48:25.445497 140669331363648 submission.py:119] 6) loss = 6.908, grad_norm = 0.348
I0405 21:48:25.850875 140635474487040 logging_writer.py:48] [7] global_step=7, grad_norm=0.349757, loss=6.907747
I0405 21:48:25.855057 140669331363648 submission.py:119] 7) loss = 6.908, grad_norm = 0.350
I0405 21:48:26.279522 140635482879744 logging_writer.py:48] [8] global_step=8, grad_norm=0.350259, loss=6.907735
I0405 21:48:26.283269 140669331363648 submission.py:119] 8) loss = 6.908, grad_norm = 0.350
I0405 21:48:26.703801 140635474487040 logging_writer.py:48] [9] global_step=9, grad_norm=0.353803, loss=6.907745
I0405 21:48:26.710886 140669331363648 submission.py:119] 9) loss = 6.908, grad_norm = 0.354
I0405 21:48:27.130534 140635482879744 logging_writer.py:48] [10] global_step=10, grad_norm=0.352353, loss=6.907720
I0405 21:48:27.134210 140669331363648 submission.py:119] 10) loss = 6.908, grad_norm = 0.352
I0405 21:48:27.563484 140635474487040 logging_writer.py:48] [11] global_step=11, grad_norm=0.345453, loss=6.907712
I0405 21:48:27.567778 140669331363648 submission.py:119] 11) loss = 6.908, grad_norm = 0.345
I0405 21:48:27.981599 140635482879744 logging_writer.py:48] [12] global_step=12, grad_norm=0.345752, loss=6.907722
I0405 21:48:27.985604 140669331363648 submission.py:119] 12) loss = 6.908, grad_norm = 0.346
I0405 21:48:28.398949 140635474487040 logging_writer.py:48] [13] global_step=13, grad_norm=0.341640, loss=6.907696
I0405 21:48:28.402568 140669331363648 submission.py:119] 13) loss = 6.908, grad_norm = 0.342
I0405 21:48:28.810569 140635482879744 logging_writer.py:48] [14] global_step=14, grad_norm=0.344319, loss=6.907717
I0405 21:48:28.817968 140669331363648 submission.py:119] 14) loss = 6.908, grad_norm = 0.344
I0405 21:48:29.228230 140635474487040 logging_writer.py:48] [15] global_step=15, grad_norm=0.345527, loss=6.907688
I0405 21:48:29.233547 140669331363648 submission.py:119] 15) loss = 6.908, grad_norm = 0.346
I0405 21:48:29.660969 140635482879744 logging_writer.py:48] [16] global_step=16, grad_norm=0.342137, loss=6.907670
I0405 21:48:29.665068 140669331363648 submission.py:119] 16) loss = 6.908, grad_norm = 0.342
I0405 21:48:30.083677 140635474487040 logging_writer.py:48] [17] global_step=17, grad_norm=0.342317, loss=6.907622
I0405 21:48:30.087633 140669331363648 submission.py:119] 17) loss = 6.908, grad_norm = 0.342
I0405 21:48:30.506225 140635482879744 logging_writer.py:48] [18] global_step=18, grad_norm=0.337162, loss=6.907668
I0405 21:48:30.510264 140669331363648 submission.py:119] 18) loss = 6.908, grad_norm = 0.337
I0405 21:48:30.911808 140635474487040 logging_writer.py:48] [19] global_step=19, grad_norm=0.335830, loss=6.907630
I0405 21:48:30.916792 140669331363648 submission.py:119] 19) loss = 6.908, grad_norm = 0.336
I0405 21:48:31.336710 140635482879744 logging_writer.py:48] [20] global_step=20, grad_norm=0.342439, loss=6.907606
I0405 21:48:31.346878 140669331363648 submission.py:119] 20) loss = 6.908, grad_norm = 0.342
I0405 21:48:31.760202 140635474487040 logging_writer.py:48] [21] global_step=21, grad_norm=0.341929, loss=6.907594
I0405 21:48:31.767194 140669331363648 submission.py:119] 21) loss = 6.908, grad_norm = 0.342
I0405 21:48:32.170442 140635482879744 logging_writer.py:48] [22] global_step=22, grad_norm=0.346853, loss=6.907623
I0405 21:48:32.174763 140669331363648 submission.py:119] 22) loss = 6.908, grad_norm = 0.347
I0405 21:48:32.581805 140635474487040 logging_writer.py:48] [23] global_step=23, grad_norm=0.345192, loss=6.907545
I0405 21:48:32.585662 140669331363648 submission.py:119] 23) loss = 6.908, grad_norm = 0.345
I0405 21:48:33.001890 140635482879744 logging_writer.py:48] [24] global_step=24, grad_norm=0.349396, loss=6.907458
I0405 21:48:33.005670 140669331363648 submission.py:119] 24) loss = 6.907, grad_norm = 0.349
I0405 21:48:33.417947 140635474487040 logging_writer.py:48] [25] global_step=25, grad_norm=0.337720, loss=6.907492
I0405 21:48:33.421869 140669331363648 submission.py:119] 25) loss = 6.907, grad_norm = 0.338
I0405 21:48:33.828861 140635482879744 logging_writer.py:48] [26] global_step=26, grad_norm=0.356452, loss=6.907361
I0405 21:48:33.832345 140669331363648 submission.py:119] 26) loss = 6.907, grad_norm = 0.356
I0405 21:48:34.235131 140635474487040 logging_writer.py:48] [27] global_step=27, grad_norm=0.353342, loss=6.907394
I0405 21:48:34.238966 140669331363648 submission.py:119] 27) loss = 6.907, grad_norm = 0.353
I0405 21:48:34.645233 140635482879744 logging_writer.py:48] [28] global_step=28, grad_norm=0.351357, loss=6.907397
I0405 21:48:34.649246 140669331363648 submission.py:119] 28) loss = 6.907, grad_norm = 0.351
I0405 21:48:35.054832 140635474487040 logging_writer.py:48] [29] global_step=29, grad_norm=0.344819, loss=6.907399
I0405 21:48:35.058701 140669331363648 submission.py:119] 29) loss = 6.907, grad_norm = 0.345
I0405 21:48:35.480759 140635482879744 logging_writer.py:48] [30] global_step=30, grad_norm=0.360101, loss=6.907103
I0405 21:48:35.484605 140669331363648 submission.py:119] 30) loss = 6.907, grad_norm = 0.360
I0405 21:48:35.906272 140635474487040 logging_writer.py:48] [31] global_step=31, grad_norm=0.353982, loss=6.907177
I0405 21:48:35.912331 140669331363648 submission.py:119] 31) loss = 6.907, grad_norm = 0.354
I0405 21:48:36.316222 140635482879744 logging_writer.py:48] [32] global_step=32, grad_norm=0.348717, loss=6.907195
I0405 21:48:36.320099 140669331363648 submission.py:119] 32) loss = 6.907, grad_norm = 0.349
I0405 21:48:36.747391 140635474487040 logging_writer.py:48] [33] global_step=33, grad_norm=0.369497, loss=6.907034
I0405 21:48:36.752497 140669331363648 submission.py:119] 33) loss = 6.907, grad_norm = 0.369
I0405 21:48:37.170681 140635482879744 logging_writer.py:48] [34] global_step=34, grad_norm=0.360358, loss=6.906939
I0405 21:48:37.174468 140669331363648 submission.py:119] 34) loss = 6.907, grad_norm = 0.360
I0405 21:48:37.579494 140635474487040 logging_writer.py:48] [35] global_step=35, grad_norm=0.372607, loss=6.906919
I0405 21:48:37.583309 140669331363648 submission.py:119] 35) loss = 6.907, grad_norm = 0.373
I0405 21:48:37.985483 140635482879744 logging_writer.py:48] [36] global_step=36, grad_norm=0.375700, loss=6.906783
I0405 21:48:37.989730 140669331363648 submission.py:119] 36) loss = 6.907, grad_norm = 0.376
I0405 21:48:38.400279 140635474487040 logging_writer.py:48] [37] global_step=37, grad_norm=0.348218, loss=6.907067
I0405 21:48:38.408518 140669331363648 submission.py:119] 37) loss = 6.907, grad_norm = 0.348
I0405 21:48:38.810964 140635482879744 logging_writer.py:48] [38] global_step=38, grad_norm=0.355305, loss=6.906811
I0405 21:48:38.814840 140669331363648 submission.py:119] 38) loss = 6.907, grad_norm = 0.355
I0405 21:48:39.240818 140635474487040 logging_writer.py:48] [39] global_step=39, grad_norm=0.385748, loss=6.906409
I0405 21:48:39.245880 140669331363648 submission.py:119] 39) loss = 6.906, grad_norm = 0.386
I0405 21:48:39.649351 140635482879744 logging_writer.py:48] [40] global_step=40, grad_norm=0.387609, loss=6.906188
I0405 21:48:39.653315 140669331363648 submission.py:119] 40) loss = 6.906, grad_norm = 0.388
I0405 21:48:40.070179 140635474487040 logging_writer.py:48] [41] global_step=41, grad_norm=0.361453, loss=6.906749
I0405 21:48:40.074694 140669331363648 submission.py:119] 41) loss = 6.907, grad_norm = 0.361
I0405 21:48:40.478121 140635482879744 logging_writer.py:48] [42] global_step=42, grad_norm=0.395575, loss=6.906275
I0405 21:48:40.481979 140669331363648 submission.py:119] 42) loss = 6.906, grad_norm = 0.396
I0405 21:48:40.892867 140635474487040 logging_writer.py:48] [43] global_step=43, grad_norm=0.372544, loss=6.906363
I0405 21:48:40.899043 140669331363648 submission.py:119] 43) loss = 6.906, grad_norm = 0.373
I0405 21:48:41.320888 140635482879744 logging_writer.py:48] [44] global_step=44, grad_norm=0.372532, loss=6.906343
I0405 21:48:41.325157 140669331363648 submission.py:119] 44) loss = 6.906, grad_norm = 0.373
I0405 21:48:41.742984 140635474487040 logging_writer.py:48] [45] global_step=45, grad_norm=0.380024, loss=6.905943
I0405 21:48:41.747060 140669331363648 submission.py:119] 45) loss = 6.906, grad_norm = 0.380
I0405 21:48:42.164239 140635482879744 logging_writer.py:48] [46] global_step=46, grad_norm=0.392776, loss=6.905772
I0405 21:48:42.170031 140669331363648 submission.py:119] 46) loss = 6.906, grad_norm = 0.393
I0405 21:48:42.595006 140635474487040 logging_writer.py:48] [47] global_step=47, grad_norm=0.397184, loss=6.905521
I0405 21:48:42.599074 140669331363648 submission.py:119] 47) loss = 6.906, grad_norm = 0.397
I0405 21:48:43.004944 140635482879744 logging_writer.py:48] [48] global_step=48, grad_norm=0.399590, loss=6.905561
I0405 21:48:43.010120 140669331363648 submission.py:119] 48) loss = 6.906, grad_norm = 0.400
I0405 21:48:43.425638 140635474487040 logging_writer.py:48] [49] global_step=49, grad_norm=0.402994, loss=6.905559
I0405 21:48:43.432652 140669331363648 submission.py:119] 49) loss = 6.906, grad_norm = 0.403
I0405 21:48:43.854305 140635482879744 logging_writer.py:48] [50] global_step=50, grad_norm=0.407913, loss=6.904938
I0405 21:48:43.858154 140669331363648 submission.py:119] 50) loss = 6.905, grad_norm = 0.408
I0405 21:48:44.265686 140635474487040 logging_writer.py:48] [51] global_step=51, grad_norm=0.384242, loss=6.904849
I0405 21:48:44.269255 140669331363648 submission.py:119] 51) loss = 6.905, grad_norm = 0.384
I0405 21:48:44.672297 140635482879744 logging_writer.py:48] [52] global_step=52, grad_norm=0.406863, loss=6.904765
I0405 21:48:44.676765 140669331363648 submission.py:119] 52) loss = 6.905, grad_norm = 0.407
I0405 21:48:45.082395 140635474487040 logging_writer.py:48] [53] global_step=53, grad_norm=0.386963, loss=6.904877
I0405 21:48:45.085820 140669331363648 submission.py:119] 53) loss = 6.905, grad_norm = 0.387
I0405 21:48:45.493727 140635482879744 logging_writer.py:48] [54] global_step=54, grad_norm=0.412873, loss=6.904585
I0405 21:48:45.497639 140669331363648 submission.py:119] 54) loss = 6.905, grad_norm = 0.413
I0405 21:48:45.903746 140635474487040 logging_writer.py:48] [55] global_step=55, grad_norm=0.407798, loss=6.904595
I0405 21:48:45.907799 140669331363648 submission.py:119] 55) loss = 6.905, grad_norm = 0.408
I0405 21:48:46.314894 140635482879744 logging_writer.py:48] [56] global_step=56, grad_norm=0.381883, loss=6.904485
I0405 21:48:46.319358 140669331363648 submission.py:119] 56) loss = 6.904, grad_norm = 0.382
I0405 21:48:46.734502 140635474487040 logging_writer.py:48] [57] global_step=57, grad_norm=0.410456, loss=6.904612
I0405 21:48:46.738153 140669331363648 submission.py:119] 57) loss = 6.905, grad_norm = 0.410
I0405 21:48:47.156691 140635482879744 logging_writer.py:48] [58] global_step=58, grad_norm=0.409553, loss=6.904235
I0405 21:48:47.160185 140669331363648 submission.py:119] 58) loss = 6.904, grad_norm = 0.410
I0405 21:48:47.568437 140635474487040 logging_writer.py:48] [59] global_step=59, grad_norm=0.411425, loss=6.903138
I0405 21:48:47.573936 140669331363648 submission.py:119] 59) loss = 6.903, grad_norm = 0.411
I0405 21:48:47.992607 140635482879744 logging_writer.py:48] [60] global_step=60, grad_norm=0.417973, loss=6.903571
I0405 21:48:47.996188 140669331363648 submission.py:119] 60) loss = 6.904, grad_norm = 0.418
I0405 21:48:48.400104 140635474487040 logging_writer.py:48] [61] global_step=61, grad_norm=0.417957, loss=6.902987
I0405 21:48:48.405096 140669331363648 submission.py:119] 61) loss = 6.903, grad_norm = 0.418
I0405 21:48:48.812073 140635482879744 logging_writer.py:48] [62] global_step=62, grad_norm=0.411701, loss=6.902981
I0405 21:48:48.816245 140669331363648 submission.py:119] 62) loss = 6.903, grad_norm = 0.412
I0405 21:48:49.229992 140635474487040 logging_writer.py:48] [63] global_step=63, grad_norm=0.393059, loss=6.902837
I0405 21:48:49.234153 140669331363648 submission.py:119] 63) loss = 6.903, grad_norm = 0.393
I0405 21:48:49.638790 140635482879744 logging_writer.py:48] [64] global_step=64, grad_norm=0.431246, loss=6.902482
I0405 21:48:49.642457 140669331363648 submission.py:119] 64) loss = 6.902, grad_norm = 0.431
I0405 21:48:50.064172 140635474487040 logging_writer.py:48] [65] global_step=65, grad_norm=0.403019, loss=6.902970
I0405 21:48:50.070456 140669331363648 submission.py:119] 65) loss = 6.903, grad_norm = 0.403
I0405 21:48:50.476325 140635482879744 logging_writer.py:48] [66] global_step=66, grad_norm=0.423107, loss=6.901516
I0405 21:48:50.480617 140669331363648 submission.py:119] 66) loss = 6.902, grad_norm = 0.423
I0405 21:48:50.889920 140635474487040 logging_writer.py:48] [67] global_step=67, grad_norm=0.410409, loss=6.901838
I0405 21:48:50.893285 140669331363648 submission.py:119] 67) loss = 6.902, grad_norm = 0.410
I0405 21:48:51.299989 140635482879744 logging_writer.py:48] [68] global_step=68, grad_norm=0.426383, loss=6.901454
I0405 21:48:51.303848 140669331363648 submission.py:119] 68) loss = 6.901, grad_norm = 0.426
I0405 21:48:51.707117 140635474487040 logging_writer.py:48] [69] global_step=69, grad_norm=0.418273, loss=6.901222
I0405 21:48:51.713714 140669331363648 submission.py:119] 69) loss = 6.901, grad_norm = 0.418
I0405 21:48:52.118821 140635482879744 logging_writer.py:48] [70] global_step=70, grad_norm=0.414006, loss=6.900289
I0405 21:48:52.123478 140669331363648 submission.py:119] 70) loss = 6.900, grad_norm = 0.414
I0405 21:48:52.527742 140635474487040 logging_writer.py:48] [71] global_step=71, grad_norm=0.434043, loss=6.899553
I0405 21:48:52.531310 140669331363648 submission.py:119] 71) loss = 6.900, grad_norm = 0.434
I0405 21:48:52.940507 140635482879744 logging_writer.py:48] [72] global_step=72, grad_norm=0.440003, loss=6.900504
I0405 21:48:52.944901 140669331363648 submission.py:119] 72) loss = 6.901, grad_norm = 0.440
I0405 21:48:53.360960 140635474487040 logging_writer.py:48] [73] global_step=73, grad_norm=0.445181, loss=6.900045
I0405 21:48:53.367247 140669331363648 submission.py:119] 73) loss = 6.900, grad_norm = 0.445
I0405 21:48:53.772152 140635482879744 logging_writer.py:48] [74] global_step=74, grad_norm=0.431640, loss=6.899533
I0405 21:48:53.781346 140669331363648 submission.py:119] 74) loss = 6.900, grad_norm = 0.432
I0405 21:48:54.186821 140635474487040 logging_writer.py:48] [75] global_step=75, grad_norm=0.432732, loss=6.898005
I0405 21:48:54.190788 140669331363648 submission.py:119] 75) loss = 6.898, grad_norm = 0.433
I0405 21:48:54.595008 140635482879744 logging_writer.py:48] [76] global_step=76, grad_norm=0.452888, loss=6.898442
I0405 21:48:54.598721 140669331363648 submission.py:119] 76) loss = 6.898, grad_norm = 0.453
I0405 21:48:55.004908 140635474487040 logging_writer.py:48] [77] global_step=77, grad_norm=0.435531, loss=6.899329
I0405 21:48:55.008780 140669331363648 submission.py:119] 77) loss = 6.899, grad_norm = 0.436
I0405 21:48:55.411932 140635482879744 logging_writer.py:48] [78] global_step=78, grad_norm=0.427984, loss=6.899461
I0405 21:48:55.415985 140669331363648 submission.py:119] 78) loss = 6.899, grad_norm = 0.428
I0405 21:48:55.819925 140635474487040 logging_writer.py:48] [79] global_step=79, grad_norm=0.440904, loss=6.897675
I0405 21:48:55.823675 140669331363648 submission.py:119] 79) loss = 6.898, grad_norm = 0.441
I0405 21:48:56.231615 140635482879744 logging_writer.py:48] [80] global_step=80, grad_norm=0.447240, loss=6.895306
I0405 21:48:56.235244 140669331363648 submission.py:119] 80) loss = 6.895, grad_norm = 0.447
I0405 21:48:56.639198 140635474487040 logging_writer.py:48] [81] global_step=81, grad_norm=0.435411, loss=6.895487
I0405 21:48:56.643121 140669331363648 submission.py:119] 81) loss = 6.895, grad_norm = 0.435
I0405 21:48:57.050460 140635482879744 logging_writer.py:48] [82] global_step=82, grad_norm=0.454896, loss=6.895963
I0405 21:48:57.057360 140669331363648 submission.py:119] 82) loss = 6.896, grad_norm = 0.455
I0405 21:48:57.461686 140635474487040 logging_writer.py:48] [83] global_step=83, grad_norm=0.451446, loss=6.897014
I0405 21:48:57.467520 140669331363648 submission.py:119] 83) loss = 6.897, grad_norm = 0.451
I0405 21:48:57.877094 140635482879744 logging_writer.py:48] [84] global_step=84, grad_norm=0.448957, loss=6.896965
I0405 21:48:57.888686 140669331363648 submission.py:119] 84) loss = 6.897, grad_norm = 0.449
I0405 21:48:58.302575 140635474487040 logging_writer.py:48] [85] global_step=85, grad_norm=0.443253, loss=6.895179
I0405 21:48:58.306094 140669331363648 submission.py:119] 85) loss = 6.895, grad_norm = 0.443
I0405 21:48:58.720224 140635482879744 logging_writer.py:48] [86] global_step=86, grad_norm=0.449862, loss=6.896763
I0405 21:48:58.724010 140669331363648 submission.py:119] 86) loss = 6.897, grad_norm = 0.450
I0405 21:48:59.129181 140635474487040 logging_writer.py:48] [87] global_step=87, grad_norm=0.445235, loss=6.894962
I0405 21:48:59.133721 140669331363648 submission.py:119] 87) loss = 6.895, grad_norm = 0.445
I0405 21:48:59.555077 140635482879744 logging_writer.py:48] [88] global_step=88, grad_norm=0.453448, loss=6.894753
I0405 21:48:59.558562 140669331363648 submission.py:119] 88) loss = 6.895, grad_norm = 0.453
I0405 21:48:59.985405 140635474487040 logging_writer.py:48] [89] global_step=89, grad_norm=0.469356, loss=6.891836
I0405 21:48:59.989824 140669331363648 submission.py:119] 89) loss = 6.892, grad_norm = 0.469
I0405 21:49:00.407600 140635482879744 logging_writer.py:48] [90] global_step=90, grad_norm=0.438126, loss=6.893019
I0405 21:49:00.411331 140669331363648 submission.py:119] 90) loss = 6.893, grad_norm = 0.438
I0405 21:49:00.815191 140635474487040 logging_writer.py:48] [91] global_step=91, grad_norm=0.459187, loss=6.892598
I0405 21:49:00.819182 140669331363648 submission.py:119] 91) loss = 6.893, grad_norm = 0.459
I0405 21:49:01.228835 140635482879744 logging_writer.py:48] [92] global_step=92, grad_norm=0.466817, loss=6.894962
I0405 21:49:01.234055 140669331363648 submission.py:119] 92) loss = 6.895, grad_norm = 0.467
I0405 21:49:01.648217 140635474487040 logging_writer.py:48] [93] global_step=93, grad_norm=0.460073, loss=6.892029
I0405 21:49:01.652223 140669331363648 submission.py:119] 93) loss = 6.892, grad_norm = 0.460
I0405 21:49:02.067543 140635482879744 logging_writer.py:48] [94] global_step=94, grad_norm=0.437234, loss=6.896771
I0405 21:49:02.072090 140669331363648 submission.py:119] 94) loss = 6.897, grad_norm = 0.437
I0405 21:49:02.477868 140635474487040 logging_writer.py:48] [95] global_step=95, grad_norm=0.445657, loss=6.892592
I0405 21:49:02.481469 140669331363648 submission.py:119] 95) loss = 6.893, grad_norm = 0.446
I0405 21:49:02.898924 140635482879744 logging_writer.py:48] [96] global_step=96, grad_norm=0.438106, loss=6.890636
I0405 21:49:02.902590 140669331363648 submission.py:119] 96) loss = 6.891, grad_norm = 0.438
I0405 21:49:03.318870 140635474487040 logging_writer.py:48] [97] global_step=97, grad_norm=0.450961, loss=6.893769
I0405 21:49:03.324166 140669331363648 submission.py:119] 97) loss = 6.894, grad_norm = 0.451
I0405 21:49:03.729510 140635482879744 logging_writer.py:48] [98] global_step=98, grad_norm=0.463882, loss=6.891307
I0405 21:49:03.734431 140669331363648 submission.py:119] 98) loss = 6.891, grad_norm = 0.464
I0405 21:49:04.148933 140635474487040 logging_writer.py:48] [99] global_step=99, grad_norm=0.466689, loss=6.891075
I0405 21:49:04.153207 140669331363648 submission.py:119] 99) loss = 6.891, grad_norm = 0.467
I0405 21:49:04.568573 140635482879744 logging_writer.py:48] [100] global_step=100, grad_norm=0.452798, loss=6.891203
I0405 21:49:04.572374 140669331363648 submission.py:119] 100) loss = 6.891, grad_norm = 0.453
I0405 21:51:45.678240 140635474487040 logging_writer.py:48] [500] global_step=500, grad_norm=0.729673, loss=6.643633
I0405 21:51:45.683383 140669331363648 submission.py:119] 500) loss = 6.644, grad_norm = 0.730
I0405 21:55:06.446907 140635482879744 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.165779, loss=6.353296
I0405 21:55:06.452542 140669331363648 submission.py:119] 1000) loss = 6.353, grad_norm = 1.166
I0405 21:55:22.910591 140669331363648 submission_runner.py:373] Before eval at step 1042: RAM USED (GB) 98.502688768
I0405 21:55:22.910822 140669331363648 spec.py:298] Evaluating on the training split.
I0405 21:56:08.209817 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 21:56:59.783307 140669331363648 spec.py:326] Evaluating on the test split.
I0405 21:57:01.221081 140669331363648 submission_runner.py:382] Time since start: 543.62s, 	Step: 1042, 	{'train/accuracy': 0.03408203125, 'train/loss': 5.910564575195313, 'validation/accuracy': 0.03232, 'validation/loss': 5.9467175, 'validation/num_examples': 50000, 'test/accuracy': 0.0253, 'test/loss': 6.069477734375, 'test/num_examples': 10000}
I0405 21:57:01.221504 140669331363648 submission_runner.py:396] After eval at step 1042: RAM USED (GB) 98.574712832
I0405 21:57:01.231563 140626716780288 logging_writer.py:48] [1042] global_step=1042, preemption_count=0, score=424.186762, test/accuracy=0.025300, test/loss=6.069478, test/num_examples=10000, total_duration=543.618665, train/accuracy=0.034082, train/loss=5.910565, validation/accuracy=0.032320, validation/loss=5.946718, validation/num_examples=50000
I0405 21:57:01.673408 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_1042.
I0405 21:57:01.674123 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 1042: RAM USED (GB) 98.565779456
I0405 22:00:07.551174 140626725172992 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.561293, loss=6.286068
I0405 22:00:07.560962 140669331363648 submission.py:119] 1500) loss = 6.286, grad_norm = 1.561
I0405 22:03:28.592525 140626716780288 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.893963, loss=6.215343
I0405 22:03:28.597329 140669331363648 submission.py:119] 2000) loss = 6.215, grad_norm = 0.894
I0405 22:04:01.899564 140669331363648 submission_runner.py:373] Before eval at step 2084: RAM USED (GB) 100.015104
I0405 22:04:01.899796 140669331363648 spec.py:298] Evaluating on the training split.
I0405 22:04:46.554349 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 22:05:34.801481 140669331363648 spec.py:326] Evaluating on the test split.
I0405 22:05:36.219279 140669331363648 submission_runner.py:382] Time since start: 1062.61s, 	Step: 2084, 	{'train/accuracy': 0.0798828125, 'train/loss': 5.279635620117188, 'validation/accuracy': 0.07602, 'validation/loss': 5.33196875, 'validation/num_examples': 50000, 'test/accuracy': 0.0555, 'test/loss': 5.55208203125, 'test/num_examples': 10000}
I0405 22:05:36.219655 140669331363648 submission_runner.py:396] After eval at step 2084: RAM USED (GB) 100.011982848
I0405 22:05:36.227362 140626725172992 logging_writer.py:48] [2084] global_step=2084, preemption_count=0, score=841.854252, test/accuracy=0.055500, test/loss=5.552082, test/num_examples=10000, total_duration=1062.607468, train/accuracy=0.079883, train/loss=5.279636, validation/accuracy=0.076020, validation/loss=5.331969, validation/num_examples=50000
I0405 22:05:36.650180 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_2084.
I0405 22:05:36.651154 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 2084: RAM USED (GB) 100.011073536
I0405 22:08:23.613025 140626716780288 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.975993, loss=5.816293
I0405 22:08:23.616664 140669331363648 submission.py:119] 2500) loss = 5.816, grad_norm = 0.976
I0405 22:11:47.198046 140626725172992 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.173767, loss=5.844352
I0405 22:11:47.202594 140669331363648 submission.py:119] 3000) loss = 5.844, grad_norm = 1.174
I0405 22:12:36.793722 140669331363648 submission_runner.py:373] Before eval at step 3124: RAM USED (GB) 100.450738176
I0405 22:12:36.794011 140669331363648 spec.py:298] Evaluating on the training split.
I0405 22:13:20.967171 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 22:14:06.456800 140669331363648 spec.py:326] Evaluating on the test split.
I0405 22:14:07.877270 140669331363648 submission_runner.py:382] Time since start: 1577.50s, 	Step: 3124, 	{'train/accuracy': 0.14447265625, 'train/loss': 4.688694763183594, 'validation/accuracy': 0.1348, 'validation/loss': 4.749979375, 'validation/num_examples': 50000, 'test/accuracy': 0.1022, 'test/loss': 5.03214375, 'test/num_examples': 10000}
I0405 22:14:07.877692 140669331363648 submission_runner.py:396] After eval at step 3124: RAM USED (GB) 100.507078656
I0405 22:14:07.886404 140626716780288 logging_writer.py:48] [3124] global_step=3124, preemption_count=0, score=1259.483280, test/accuracy=0.102200, test/loss=5.032144, test/num_examples=10000, total_duration=1577.501557, train/accuracy=0.144473, train/loss=4.688695, validation/accuracy=0.134800, validation/loss=4.749979, validation/num_examples=50000
I0405 22:14:08.326692 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_3124.
I0405 22:14:08.327477 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 3124: RAM USED (GB) 100.506161152
I0405 22:16:39.833961 140626725172992 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.954866, loss=5.588783
I0405 22:16:39.838819 140669331363648 submission.py:119] 3500) loss = 5.589, grad_norm = 0.955
I0405 22:20:02.760225 140626716780288 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.810749, loss=5.572000
I0405 22:20:02.767501 140669331363648 submission.py:119] 4000) loss = 5.572, grad_norm = 0.811
I0405 22:21:08.484950 140669331363648 submission_runner.py:373] Before eval at step 4164: RAM USED (GB) 100.704550912
I0405 22:21:08.485260 140669331363648 spec.py:298] Evaluating on the training split.
I0405 22:21:53.173100 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 22:22:39.190177 140669331363648 spec.py:326] Evaluating on the test split.
I0405 22:22:40.610692 140669331363648 submission_runner.py:382] Time since start: 2089.19s, 	Step: 4164, 	{'train/accuracy': 0.1928515625, 'train/loss': 4.223826599121094, 'validation/accuracy': 0.1802, 'validation/loss': 4.3118975, 'validation/num_examples': 50000, 'test/accuracy': 0.1332, 'test/loss': 4.6783359375, 'test/num_examples': 10000}
I0405 22:22:40.611064 140669331363648 submission_runner.py:396] After eval at step 4164: RAM USED (GB) 100.77384704
I0405 22:22:40.618871 140626725172992 logging_writer.py:48] [4164] global_step=4164, preemption_count=0, score=1677.120799, test/accuracy=0.133200, test/loss=4.678336, test/num_examples=10000, total_duration=2089.192877, train/accuracy=0.192852, train/loss=4.223827, validation/accuracy=0.180200, validation/loss=4.311897, validation/num_examples=50000
I0405 22:22:41.050751 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_4164.
I0405 22:22:41.051486 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 4164: RAM USED (GB) 100.71013376
I0405 22:24:57.040553 140626716780288 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.908110, loss=5.282690
I0405 22:24:57.044790 140669331363648 submission.py:119] 4500) loss = 5.283, grad_norm = 0.908
I0405 22:28:17.578916 140626725172992 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.779653, loss=5.516229
I0405 22:28:17.583929 140669331363648 submission.py:119] 5000) loss = 5.516, grad_norm = 0.780
I0405 22:29:41.445889 140669331363648 submission_runner.py:373] Before eval at step 5204: RAM USED (GB) 100.568608768
I0405 22:29:41.446122 140669331363648 spec.py:298] Evaluating on the training split.
I0405 22:30:25.389150 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 22:31:10.872592 140669331363648 spec.py:326] Evaluating on the test split.
I0405 22:31:12.292671 140669331363648 submission_runner.py:382] Time since start: 2602.15s, 	Step: 5204, 	{'train/accuracy': 0.23458984375, 'train/loss': 3.9104953002929688, 'validation/accuracy': 0.21662, 'validation/loss': 4.0172740625, 'validation/num_examples': 50000, 'test/accuracy': 0.1654, 'test/loss': 4.443267578125, 'test/num_examples': 10000}
I0405 22:31:12.293019 140669331363648 submission_runner.py:396] After eval at step 5204: RAM USED (GB) 100.561088512
I0405 22:31:12.301412 140626716780288 logging_writer.py:48] [5204] global_step=5204, preemption_count=0, score=2095.032057, test/accuracy=0.165400, test/loss=4.443268, test/num_examples=10000, total_duration=2602.154071, train/accuracy=0.234590, train/loss=3.910495, validation/accuracy=0.216620, validation/loss=4.017274, validation/num_examples=50000
I0405 22:31:12.728158 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_5204.
I0405 22:31:12.728944 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 5204: RAM USED (GB) 100.560707584
I0405 22:33:12.541894 140626725172992 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.817379, loss=5.277235
I0405 22:33:12.547196 140669331363648 submission.py:119] 5500) loss = 5.277, grad_norm = 0.817
I0405 22:36:33.646632 140626716780288 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.805197, loss=4.905828
I0405 22:36:33.651618 140669331363648 submission.py:119] 6000) loss = 4.906, grad_norm = 0.805
I0405 22:38:13.103224 140669331363648 submission_runner.py:373] Before eval at step 6249: RAM USED (GB) 100.69940224
I0405 22:38:13.103431 140669331363648 spec.py:298] Evaluating on the training split.
I0405 22:38:57.814726 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 22:39:48.577731 140669331363648 spec.py:326] Evaluating on the test split.
I0405 22:39:49.990620 140669331363648 submission_runner.py:382] Time since start: 3113.81s, 	Step: 6249, 	{'train/accuracy': 0.2858203125, 'train/loss': 3.5124249267578125, 'validation/accuracy': 0.2651, 'validation/loss': 3.649048125, 'validation/num_examples': 50000, 'test/accuracy': 0.1989, 'test/loss': 4.111153125, 'test/num_examples': 10000}
I0405 22:39:49.990968 140669331363648 submission_runner.py:396] After eval at step 6249: RAM USED (GB) 100.727263232
I0405 22:39:49.999562 140626725172992 logging_writer.py:48] [6249] global_step=6249, preemption_count=0, score=2512.902747, test/accuracy=0.198900, test/loss=4.111153, test/num_examples=10000, total_duration=3113.811550, train/accuracy=0.285820, train/loss=3.512425, validation/accuracy=0.265100, validation/loss=3.649048, validation/num_examples=50000
I0405 22:39:50.423499 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_6249.
I0405 22:39:50.424218 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 6249: RAM USED (GB) 100.726042624
I0405 22:41:34.018978 140626716780288 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.932038, loss=4.801419
I0405 22:41:34.023115 140669331363648 submission.py:119] 6500) loss = 4.801, grad_norm = 0.932
I0405 22:44:55.346135 140626725172992 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.861643, loss=4.989471
I0405 22:44:55.350791 140669331363648 submission.py:119] 7000) loss = 4.989, grad_norm = 0.862
I0405 22:46:50.629936 140669331363648 submission_runner.py:373] Before eval at step 7288: RAM USED (GB) 100.65686528
I0405 22:46:50.630193 140669331363648 spec.py:298] Evaluating on the training split.
I0405 22:47:34.766751 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 22:48:19.688231 140669331363648 spec.py:326] Evaluating on the test split.
I0405 22:48:21.108914 140669331363648 submission_runner.py:382] Time since start: 3631.34s, 	Step: 7288, 	{'train/accuracy': 0.3410546875, 'train/loss': 3.18815673828125, 'validation/accuracy': 0.31374, 'validation/loss': 3.335963125, 'validation/num_examples': 50000, 'test/accuracy': 0.2424, 'test/loss': 3.84515546875, 'test/num_examples': 10000}
I0405 22:48:21.109385 140669331363648 submission_runner.py:396] After eval at step 7288: RAM USED (GB) 100.877615104
I0405 22:48:21.118345 140626716780288 logging_writer.py:48] [7288] global_step=7288, preemption_count=0, score=2930.589432, test/accuracy=0.242400, test/loss=3.845155, test/num_examples=10000, total_duration=3631.337950, train/accuracy=0.341055, train/loss=3.188157, validation/accuracy=0.313740, validation/loss=3.335963, validation/num_examples=50000
I0405 22:48:21.540741 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_7288.
I0405 22:48:21.541519 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 7288: RAM USED (GB) 100.876664832
I0405 22:49:47.189659 140626725172992 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.669621, loss=5.120101
I0405 22:49:47.193383 140669331363648 submission.py:119] 7500) loss = 5.120, grad_norm = 0.670
I0405 22:53:10.346214 140626716780288 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.719084, loss=5.175895
I0405 22:53:10.352661 140669331363648 submission.py:119] 8000) loss = 5.176, grad_norm = 0.719
I0405 22:55:21.841877 140669331363648 submission_runner.py:373] Before eval at step 8328: RAM USED (GB) 100.91501568
I0405 22:55:21.842103 140669331363648 spec.py:298] Evaluating on the training split.
I0405 22:56:05.800845 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 22:56:51.515034 140669331363648 spec.py:326] Evaluating on the test split.
I0405 22:56:52.929127 140669331363648 submission_runner.py:382] Time since start: 4142.55s, 	Step: 8328, 	{'train/accuracy': 0.3695703125, 'train/loss': 3.0094805908203126, 'validation/accuracy': 0.33984, 'validation/loss': 3.1786434375, 'validation/num_examples': 50000, 'test/accuracy': 0.2634, 'test/loss': 3.719904296875, 'test/num_examples': 10000}
I0405 22:56:52.929453 140669331363648 submission_runner.py:396] After eval at step 8328: RAM USED (GB) 100.843831296
I0405 22:56:52.938339 140626725172992 logging_writer.py:48] [8328] global_step=8328, preemption_count=0, score=3348.408415, test/accuracy=0.263400, test/loss=3.719904, test/num_examples=10000, total_duration=4142.549627, train/accuracy=0.369570, train/loss=3.009481, validation/accuracy=0.339840, validation/loss=3.178643, validation/num_examples=50000
I0405 22:56:53.356565 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_8328.
I0405 22:56:53.357268 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 8328: RAM USED (GB) 100.842921984
I0405 22:58:02.835613 140626716780288 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.659008, loss=4.480514
I0405 22:58:02.841898 140669331363648 submission.py:119] 8500) loss = 4.481, grad_norm = 0.659
I0405 23:01:26.190741 140626725172992 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.692004, loss=4.409182
I0405 23:01:26.195342 140669331363648 submission.py:119] 9000) loss = 4.409, grad_norm = 0.692
I0405 23:03:53.722340 140669331363648 submission_runner.py:373] Before eval at step 9368: RAM USED (GB) 100.745580544
I0405 23:03:53.722620 140669331363648 spec.py:298] Evaluating on the training split.
I0405 23:04:37.360346 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 23:05:23.836201 140669331363648 spec.py:326] Evaluating on the test split.
I0405 23:05:25.259455 140669331363648 submission_runner.py:382] Time since start: 4654.43s, 	Step: 9368, 	{'train/accuracy': 0.412890625, 'train/loss': 2.744701232910156, 'validation/accuracy': 0.37426, 'validation/loss': 2.937439375, 'validation/num_examples': 50000, 'test/accuracy': 0.2944, 'test/loss': 3.462166796875, 'test/num_examples': 10000}
I0405 23:05:25.259801 140669331363648 submission_runner.py:396] After eval at step 9368: RAM USED (GB) 100.9189888
I0405 23:05:25.267710 140626716780288 logging_writer.py:48] [9368] global_step=9368, preemption_count=0, score=3766.311580, test/accuracy=0.294400, test/loss=3.462167, test/num_examples=10000, total_duration=4654.430220, train/accuracy=0.412891, train/loss=2.744701, validation/accuracy=0.374260, validation/loss=2.937439, validation/num_examples=50000
I0405 23:05:25.688745 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_9368.
I0405 23:05:25.689479 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 9368: RAM USED (GB) 100.918075392
I0405 23:06:19.091208 140626725172992 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.595539, loss=4.809813
I0405 23:06:19.096333 140669331363648 submission.py:119] 9500) loss = 4.810, grad_norm = 0.596
I0405 23:09:39.874550 140626716780288 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.596112, loss=4.080291
I0405 23:09:39.879788 140669331363648 submission.py:119] 10000) loss = 4.080, grad_norm = 0.596
I0405 23:12:25.867411 140669331363648 submission_runner.py:373] Before eval at step 10409: RAM USED (GB) 100.778102784
I0405 23:12:25.867629 140669331363648 spec.py:298] Evaluating on the training split.
I0405 23:13:11.396391 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 23:14:00.242162 140669331363648 spec.py:326] Evaluating on the test split.
I0405 23:14:01.655255 140669331363648 submission_runner.py:382] Time since start: 5166.58s, 	Step: 10409, 	{'train/accuracy': 0.44404296875, 'train/loss': 2.6081640625, 'validation/accuracy': 0.40764, 'validation/loss': 2.790825, 'validation/num_examples': 50000, 'test/accuracy': 0.3135, 'test/loss': 3.37628359375, 'test/num_examples': 10000}
I0405 23:14:01.655671 140669331363648 submission_runner.py:396] After eval at step 10409: RAM USED (GB) 100.835487744
I0405 23:14:01.663903 140626725172992 logging_writer.py:48] [10409] global_step=10409, preemption_count=0, score=4184.024569, test/accuracy=0.313500, test/loss=3.376284, test/num_examples=10000, total_duration=5166.575474, train/accuracy=0.444043, train/loss=2.608164, validation/accuracy=0.407640, validation/loss=2.790825, validation/num_examples=50000
I0405 23:14:02.095933 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_10409.
I0405 23:14:02.096682 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 10409: RAM USED (GB) 100.834942976
I0405 23:14:39.063611 140626716780288 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.782774, loss=4.093399
I0405 23:14:39.067283 140669331363648 submission.py:119] 10500) loss = 4.093, grad_norm = 0.783
I0405 23:17:59.772882 140626725172992 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.616234, loss=4.482761
I0405 23:17:59.777660 140669331363648 submission.py:119] 11000) loss = 4.483, grad_norm = 0.616
I0405 23:21:02.372151 140669331363648 submission_runner.py:373] Before eval at step 11451: RAM USED (GB) 100.780158976
I0405 23:21:02.372549 140669331363648 spec.py:298] Evaluating on the training split.
I0405 23:21:47.787086 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 23:22:35.312077 140669331363648 spec.py:326] Evaluating on the test split.
I0405 23:22:36.727374 140669331363648 submission_runner.py:382] Time since start: 5683.08s, 	Step: 11451, 	{'train/accuracy': 0.47560546875, 'train/loss': 2.4138609313964845, 'validation/accuracy': 0.43774, 'validation/loss': 2.6167728125, 'validation/num_examples': 50000, 'test/accuracy': 0.345, 'test/loss': 3.198433203125, 'test/num_examples': 10000}
I0405 23:22:36.727722 140669331363648 submission_runner.py:396] After eval at step 11451: RAM USED (GB) 100.700839936
I0405 23:22:36.736132 140626716780288 logging_writer.py:48] [11451] global_step=11451, preemption_count=0, score=4601.847118, test/accuracy=0.345000, test/loss=3.198433, test/num_examples=10000, total_duration=5683.080030, train/accuracy=0.475605, train/loss=2.413861, validation/accuracy=0.437740, validation/loss=2.616773, validation/num_examples=50000
I0405 23:22:37.162765 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_11451.
I0405 23:22:37.163459 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 11451: RAM USED (GB) 100.699930624
I0405 23:22:57.240892 140626725172992 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.647456, loss=4.307316
I0405 23:22:57.244318 140669331363648 submission.py:119] 11500) loss = 4.307, grad_norm = 0.647
I0405 23:26:18.638325 140626716780288 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.665293, loss=4.474935
I0405 23:26:18.642583 140669331363648 submission.py:119] 12000) loss = 4.475, grad_norm = 0.665
I0405 23:29:37.379953 140669331363648 submission_runner.py:373] Before eval at step 12496: RAM USED (GB) 100.747259904
I0405 23:29:37.380218 140669331363648 spec.py:298] Evaluating on the training split.
I0405 23:30:24.660892 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 23:31:19.288068 140669331363648 spec.py:326] Evaluating on the test split.
I0405 23:31:20.696884 140669331363648 submission_runner.py:382] Time since start: 6198.09s, 	Step: 12496, 	{'train/accuracy': 0.50322265625, 'train/loss': 2.285833740234375, 'validation/accuracy': 0.4604, 'validation/loss': 2.490535625, 'validation/num_examples': 50000, 'test/accuracy': 0.3605, 'test/loss': 3.0847755859375, 'test/num_examples': 10000}
I0405 23:31:20.697226 140669331363648 submission_runner.py:396] After eval at step 12496: RAM USED (GB) 100.683698176
I0405 23:31:20.706757 140626725172992 logging_writer.py:48] [12496] global_step=12496, preemption_count=0, score=5019.595023, test/accuracy=0.360500, test/loss=3.084776, test/num_examples=10000, total_duration=6198.087799, train/accuracy=0.503223, train/loss=2.285834, validation/accuracy=0.460400, validation/loss=2.490536, validation/num_examples=50000
I0405 23:31:21.131019 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_12496.
I0405 23:31:21.131799 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 12496: RAM USED (GB) 100.68303872
I0405 23:31:23.288093 140626716780288 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.678156, loss=4.479962
I0405 23:31:23.292909 140669331363648 submission.py:119] 12500) loss = 4.480, grad_norm = 0.678
I0405 23:34:46.426223 140626725172992 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.710348, loss=4.463209
I0405 23:34:46.430668 140669331363648 submission.py:119] 13000) loss = 4.463, grad_norm = 0.710
I0405 23:38:07.559413 140626716780288 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.626313, loss=4.555971
I0405 23:38:07.564553 140669331363648 submission.py:119] 13500) loss = 4.556, grad_norm = 0.626
I0405 23:38:21.175515 140669331363648 submission_runner.py:373] Before eval at step 13535: RAM USED (GB) 100.677378048
I0405 23:38:21.175769 140669331363648 spec.py:298] Evaluating on the training split.
I0405 23:39:06.689738 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 23:39:53.175632 140669331363648 spec.py:326] Evaluating on the test split.
I0405 23:39:54.599753 140669331363648 submission_runner.py:382] Time since start: 6721.88s, 	Step: 13535, 	{'train/accuracy': 0.529765625, 'train/loss': 2.12036376953125, 'validation/accuracy': 0.4844, 'validation/loss': 2.34364953125, 'validation/num_examples': 50000, 'test/accuracy': 0.3812, 'test/loss': 2.9407673828125, 'test/num_examples': 10000}
I0405 23:39:54.600181 140669331363648 submission_runner.py:396] After eval at step 13535: RAM USED (GB) 100.68002816
I0405 23:39:54.608853 140626725172992 logging_writer.py:48] [13535] global_step=13535, preemption_count=0, score=5437.162330, test/accuracy=0.381200, test/loss=2.940767, test/num_examples=10000, total_duration=6721.883521, train/accuracy=0.529766, train/loss=2.120364, validation/accuracy=0.484400, validation/loss=2.343650, validation/num_examples=50000
I0405 23:39:55.058750 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_13535.
I0405 23:39:55.059576 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 13535: RAM USED (GB) 100.679110656
I0405 23:43:04.091241 140669331363648 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 100.659654656
I0405 23:43:04.091455 140669331363648 spec.py:298] Evaluating on the training split.
I0405 23:43:47.827600 140669331363648 spec.py:310] Evaluating on the validation split.
I0405 23:44:33.082901 140669331363648 spec.py:326] Evaluating on the test split.
I0405 23:44:34.499073 140669331363648 submission_runner.py:382] Time since start: 7004.80s, 	Step: 14000, 	{'train/accuracy': 0.5375390625, 'train/loss': 2.1175250244140624, 'validation/accuracy': 0.49226, 'validation/loss': 2.3440559375, 'validation/num_examples': 50000, 'test/accuracy': 0.3826, 'test/loss': 2.9515046875, 'test/num_examples': 10000}
I0405 23:44:34.499427 140669331363648 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 100.564885504
I0405 23:44:34.507305 140626716780288 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5625.102354, test/accuracy=0.382600, test/loss=2.951505, test/num_examples=10000, total_duration=7004.799273, train/accuracy=0.537539, train/loss=2.117525, validation/accuracy=0.492260, validation/loss=2.344056, validation/num_examples=50000
I0405 23:44:34.935151 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0405 23:44:34.935893 140669331363648 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.563976192
I0405 23:44:34.943698 140626725172992 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5625.102354
I0405 23:44:35.952719 140669331363648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0405 23:44:36.335449 140669331363648 submission_runner.py:550] Tuning trial 1/1
I0405 23:44:36.335659 140669331363648 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0405 23:44:36.336225 140669331363648 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00169921875, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.0018, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0021, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.463490962982178, 'total_duration': 6.465541839599609, 'global_step': 1, 'preemption_count': 0}), (1042, {'train/accuracy': 0.03408203125, 'train/loss': 5.910564575195313, 'validation/accuracy': 0.03232, 'validation/loss': 5.9467175, 'validation/num_examples': 50000, 'test/accuracy': 0.0253, 'test/loss': 6.069477734375, 'test/num_examples': 10000, 'score': 424.1867616176605, 'total_duration': 543.6186654567719, 'global_step': 1042, 'preemption_count': 0}), (2084, {'train/accuracy': 0.0798828125, 'train/loss': 5.279635620117188, 'validation/accuracy': 0.07602, 'validation/loss': 5.33196875, 'validation/num_examples': 50000, 'test/accuracy': 0.0555, 'test/loss': 5.55208203125, 'test/num_examples': 10000, 'score': 841.8542518615723, 'total_duration': 1062.6074681282043, 'global_step': 2084, 'preemption_count': 0}), (3124, {'train/accuracy': 0.14447265625, 'train/loss': 4.688694763183594, 'validation/accuracy': 0.1348, 'validation/loss': 4.749979375, 'validation/num_examples': 50000, 'test/accuracy': 0.1022, 'test/loss': 5.03214375, 'test/num_examples': 10000, 'score': 1259.4832804203033, 'total_duration': 1577.5015568733215, 'global_step': 3124, 'preemption_count': 0}), (4164, {'train/accuracy': 0.1928515625, 'train/loss': 4.223826599121094, 'validation/accuracy': 0.1802, 'validation/loss': 4.3118975, 'validation/num_examples': 50000, 'test/accuracy': 0.1332, 'test/loss': 4.6783359375, 'test/num_examples': 10000, 'score': 1677.1207988262177, 'total_duration': 2089.1928770542145, 'global_step': 4164, 'preemption_count': 0}), (5204, {'train/accuracy': 0.23458984375, 'train/loss': 3.9104953002929688, 'validation/accuracy': 0.21662, 'validation/loss': 4.0172740625, 'validation/num_examples': 50000, 'test/accuracy': 0.1654, 'test/loss': 4.443267578125, 'test/num_examples': 10000, 'score': 2095.0320570468903, 'total_duration': 2602.1540710926056, 'global_step': 5204, 'preemption_count': 0}), (6249, {'train/accuracy': 0.2858203125, 'train/loss': 3.5124249267578125, 'validation/accuracy': 0.2651, 'validation/loss': 3.649048125, 'validation/num_examples': 50000, 'test/accuracy': 0.1989, 'test/loss': 4.111153125, 'test/num_examples': 10000, 'score': 2512.9027469158173, 'total_duration': 3113.8115499019623, 'global_step': 6249, 'preemption_count': 0}), (7288, {'train/accuracy': 0.3410546875, 'train/loss': 3.18815673828125, 'validation/accuracy': 0.31374, 'validation/loss': 3.335963125, 'validation/num_examples': 50000, 'test/accuracy': 0.2424, 'test/loss': 3.84515546875, 'test/num_examples': 10000, 'score': 2930.589432001114, 'total_duration': 3631.3379504680634, 'global_step': 7288, 'preemption_count': 0}), (8328, {'train/accuracy': 0.3695703125, 'train/loss': 3.0094805908203126, 'validation/accuracy': 0.33984, 'validation/loss': 3.1786434375, 'validation/num_examples': 50000, 'test/accuracy': 0.2634, 'test/loss': 3.719904296875, 'test/num_examples': 10000, 'score': 3348.4084148406982, 'total_duration': 4142.549626588821, 'global_step': 8328, 'preemption_count': 0}), (9368, {'train/accuracy': 0.412890625, 'train/loss': 2.744701232910156, 'validation/accuracy': 0.37426, 'validation/loss': 2.937439375, 'validation/num_examples': 50000, 'test/accuracy': 0.2944, 'test/loss': 3.462166796875, 'test/num_examples': 10000, 'score': 3766.3115799427032, 'total_duration': 4654.430219888687, 'global_step': 9368, 'preemption_count': 0}), (10409, {'train/accuracy': 0.44404296875, 'train/loss': 2.6081640625, 'validation/accuracy': 0.40764, 'validation/loss': 2.790825, 'validation/num_examples': 50000, 'test/accuracy': 0.3135, 'test/loss': 3.37628359375, 'test/num_examples': 10000, 'score': 4184.024568796158, 'total_duration': 5166.575473546982, 'global_step': 10409, 'preemption_count': 0}), (11451, {'train/accuracy': 0.47560546875, 'train/loss': 2.4138609313964845, 'validation/accuracy': 0.43774, 'validation/loss': 2.6167728125, 'validation/num_examples': 50000, 'test/accuracy': 0.345, 'test/loss': 3.198433203125, 'test/num_examples': 10000, 'score': 4601.847118377686, 'total_duration': 5683.080030202866, 'global_step': 11451, 'preemption_count': 0}), (12496, {'train/accuracy': 0.50322265625, 'train/loss': 2.285833740234375, 'validation/accuracy': 0.4604, 'validation/loss': 2.490535625, 'validation/num_examples': 50000, 'test/accuracy': 0.3605, 'test/loss': 3.0847755859375, 'test/num_examples': 10000, 'score': 5019.595023155212, 'total_duration': 6198.087799310684, 'global_step': 12496, 'preemption_count': 0}), (13535, {'train/accuracy': 0.529765625, 'train/loss': 2.12036376953125, 'validation/accuracy': 0.4844, 'validation/loss': 2.34364953125, 'validation/num_examples': 50000, 'test/accuracy': 0.3812, 'test/loss': 2.9407673828125, 'test/num_examples': 10000, 'score': 5437.162330389023, 'total_duration': 6721.8835208415985, 'global_step': 13535, 'preemption_count': 0}), (14000, {'train/accuracy': 0.5375390625, 'train/loss': 2.1175250244140624, 'validation/accuracy': 0.49226, 'validation/loss': 2.3440559375, 'validation/num_examples': 50000, 'test/accuracy': 0.3826, 'test/loss': 2.9515046875, 'test/num_examples': 10000, 'score': 5625.102354288101, 'total_duration': 7004.799273014069, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0405 23:44:36.336323 140669331363648 submission_runner.py:553] Timing: 5625.102354288101
I0405 23:44:36.336367 140669331363648 submission_runner.py:554] ====================
I0405 23:44:36.336465 140669331363648 submission_runner.py:613] Final imagenet_vit score: 5625.102354288101
