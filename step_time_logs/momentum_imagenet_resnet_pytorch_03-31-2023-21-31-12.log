WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0331 21:31:35.744895 139954488350528 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0331 21:31:35.744873 139621905315648 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0331 21:31:35.744918 140132804294464 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0331 21:31:35.744949 139940479506240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0331 21:31:35.745588 140455666984768 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0331 21:31:35.745730 140377305757504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0331 21:31:35.745864 139792923473728 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0331 21:31:35.746538 139648182736704 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0331 21:31:35.746886 139648182736704 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:31:35.755535 139621905315648 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:31:35.755592 139954488350528 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:31:35.755621 140132804294464 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:31:35.755653 139940479506240 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:31:35.756229 140455666984768 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:31:35.756299 140377305757504 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:31:35.756433 139792923473728 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0331 21:31:37.755451 139792923473728 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/imagenet_resnet_pytorch.
W0331 21:31:37.872598 139954488350528 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:31:37.873190 140455666984768 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:31:37.873565 139792923473728 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:31:37.873962 140132804294464 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:31:37.874083 139940479506240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:31:37.874141 139648182736704 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:31:37.874583 140377305757504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0331 21:31:37.875422 139621905315648 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0331 21:31:37.877307 139792923473728 submission_runner.py:504] Using RNG seed 3543153192
I0331 21:31:37.878261 139792923473728 submission_runner.py:513] --- Tuning run 1/1 ---
I0331 21:31:37.878369 139792923473728 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1.
I0331 21:31:37.878535 139792923473728 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/hparams.json.
I0331 21:31:37.879499 139792923473728 submission_runner.py:230] Starting train once: RAM USED (GB) 6.385635328
I0331 21:31:37.879593 139792923473728 submission_runner.py:231] Initializing dataset.
I0331 21:31:42.146928 139792923473728 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 8.40732672
I0331 21:31:42.147121 139792923473728 submission_runner.py:240] Initializing model.
I0331 21:31:46.535510 139792923473728 submission_runner.py:251] After Initializing model: RAM USED (GB) 18.246397952
I0331 21:31:46.535680 139792923473728 submission_runner.py:252] Initializing optimizer.
I0331 21:31:47.013961 139792923473728 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 18.24897024
I0331 21:31:47.014148 139792923473728 submission_runner.py:261] Initializing metrics bundle.
I0331 21:31:47.014203 139792923473728 submission_runner.py:275] Initializing checkpoint and logger.
I0331 21:31:47.445086 139792923473728 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0331 21:31:47.445953 139792923473728 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0331 21:31:47.495995 139792923473728 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 18.299138048
I0331 21:31:47.497012 139792923473728 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.299138048
I0331 21:31:47.497129 139792923473728 submission_runner.py:312] Starting training loop.
I0331 21:31:49.203680 139792923473728 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 23.38424832
I0331 21:31:55.261000 139764107835136 logging_writer.py:48] [0] global_step=0, grad_norm=0.523906, loss=6.928137
I0331 21:31:55.280489 139792923473728 submission.py:139] 0) loss = 6.928, grad_norm = 0.524
I0331 21:31:55.281106 139792923473728 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 32.56569856
I0331 21:31:55.281705 139792923473728 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 32.56569856
I0331 21:31:55.281833 139792923473728 spec.py:298] Evaluating on the training split.
I0331 21:32:43.563153 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 21:33:28.343136 139792923473728 spec.py:326] Evaluating on the test split.
I0331 21:33:28.358530 139792923473728 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0331 21:33:28.364134 139792923473728 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0331 21:33:28.438654 139792923473728 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0331 21:33:40.907557 139792923473728 submission_runner.py:380] Time since start: 7.79s, 	Step: 1, 	{'train/accuracy': 0.0009765625, 'train/loss': 6.92459604691486, 'validation/accuracy': 0.00104, 'validation/loss': 6.92413125, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.927590625, 'test/num_examples': 10000}
I0331 21:33:40.907914 139792923473728 submission_runner.py:390] After eval at step 1: RAM USED (GB) 91.957379072
I0331 21:33:40.932827 139740535822080 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=7.783123, test/accuracy=0.001000, test/loss=6.927591, test/num_examples=10000, total_duration=7.785108, train/accuracy=0.000977, train/loss=6.924596, validation/accuracy=0.001040, validation/loss=6.924131, validation/num_examples=50000
I0331 21:33:41.227143 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_1.
I0331 21:33:41.227752 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 91.957469184
I0331 21:33:41.254499 139792923473728 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 91.957608448
I0331 21:33:41.260632 139792923473728 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:33:41.260677 140132804294464 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:33:41.260909 140455666984768 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:33:41.260895 139954488350528 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:33:41.260906 139648182736704 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:33:41.260915 139940479506240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:33:41.260936 139621905315648 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:33:41.260986 140377305757504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0331 21:33:41.635185 139734500206336 logging_writer.py:48] [1] global_step=1, grad_norm=0.543199, loss=6.925227
I0331 21:33:41.638725 139792923473728 submission.py:139] 1) loss = 6.925, grad_norm = 0.543
I0331 21:33:41.639291 139792923473728 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 91.966590976
I0331 21:33:42.010037 139740535822080 logging_writer.py:48] [2] global_step=2, grad_norm=0.541416, loss=6.936145
I0331 21:33:42.013734 139792923473728 submission.py:139] 2) loss = 6.936, grad_norm = 0.541
I0331 21:33:42.387899 139734500206336 logging_writer.py:48] [3] global_step=3, grad_norm=0.540928, loss=6.925865
I0331 21:33:42.391247 139792923473728 submission.py:139] 3) loss = 6.926, grad_norm = 0.541
I0331 21:33:42.764753 139740535822080 logging_writer.py:48] [4] global_step=4, grad_norm=0.532751, loss=6.928357
I0331 21:33:42.768803 139792923473728 submission.py:139] 4) loss = 6.928, grad_norm = 0.533
I0331 21:33:43.146924 139734500206336 logging_writer.py:48] [5] global_step=5, grad_norm=0.538716, loss=6.937210
I0331 21:33:43.151196 139792923473728 submission.py:139] 5) loss = 6.937, grad_norm = 0.539
I0331 21:33:43.527234 139740535822080 logging_writer.py:48] [6] global_step=6, grad_norm=0.544043, loss=6.932989
I0331 21:33:43.530629 139792923473728 submission.py:139] 6) loss = 6.933, grad_norm = 0.544
I0331 21:33:43.908411 139734500206336 logging_writer.py:48] [7] global_step=7, grad_norm=0.547606, loss=6.934654
I0331 21:33:43.912057 139792923473728 submission.py:139] 7) loss = 6.935, grad_norm = 0.548
I0331 21:33:44.287331 139740535822080 logging_writer.py:48] [8] global_step=8, grad_norm=0.538997, loss=6.924577
I0331 21:33:44.291071 139792923473728 submission.py:139] 8) loss = 6.925, grad_norm = 0.539
I0331 21:33:44.666031 139734500206336 logging_writer.py:48] [9] global_step=9, grad_norm=0.548429, loss=6.921833
I0331 21:33:44.669826 139792923473728 submission.py:139] 9) loss = 6.922, grad_norm = 0.548
I0331 21:33:45.044043 139740535822080 logging_writer.py:48] [10] global_step=10, grad_norm=0.541766, loss=6.922572
I0331 21:33:45.047641 139792923473728 submission.py:139] 10) loss = 6.923, grad_norm = 0.542
I0331 21:33:45.422155 139734500206336 logging_writer.py:48] [11] global_step=11, grad_norm=0.531844, loss=6.926425
I0331 21:33:45.426071 139792923473728 submission.py:139] 11) loss = 6.926, grad_norm = 0.532
I0331 21:33:45.801242 139740535822080 logging_writer.py:48] [12] global_step=12, grad_norm=0.527321, loss=6.921599
I0331 21:33:45.804579 139792923473728 submission.py:139] 12) loss = 6.922, grad_norm = 0.527
I0331 21:33:46.177059 139734500206336 logging_writer.py:48] [13] global_step=13, grad_norm=0.538883, loss=6.928582
I0331 21:33:46.181062 139792923473728 submission.py:139] 13) loss = 6.929, grad_norm = 0.539
I0331 21:33:46.555988 139740535822080 logging_writer.py:48] [14] global_step=14, grad_norm=0.552760, loss=6.931922
I0331 21:33:46.559909 139792923473728 submission.py:139] 14) loss = 6.932, grad_norm = 0.553
I0331 21:33:46.937221 139734500206336 logging_writer.py:48] [15] global_step=15, grad_norm=0.529172, loss=6.925393
I0331 21:33:46.940920 139792923473728 submission.py:139] 15) loss = 6.925, grad_norm = 0.529
I0331 21:33:47.316287 139740535822080 logging_writer.py:48] [16] global_step=16, grad_norm=0.525835, loss=6.929281
I0331 21:33:47.319960 139792923473728 submission.py:139] 16) loss = 6.929, grad_norm = 0.526
I0331 21:33:47.695095 139734500206336 logging_writer.py:48] [17] global_step=17, grad_norm=0.532256, loss=6.922423
I0331 21:33:47.698889 139792923473728 submission.py:139] 17) loss = 6.922, grad_norm = 0.532
I0331 21:33:48.074038 139740535822080 logging_writer.py:48] [18] global_step=18, grad_norm=0.542107, loss=6.927542
I0331 21:33:48.077453 139792923473728 submission.py:139] 18) loss = 6.928, grad_norm = 0.542
I0331 21:33:48.451447 139734500206336 logging_writer.py:48] [19] global_step=19, grad_norm=0.536965, loss=6.918526
I0331 21:33:48.455199 139792923473728 submission.py:139] 19) loss = 6.919, grad_norm = 0.537
I0331 21:33:48.831744 139740535822080 logging_writer.py:48] [20] global_step=20, grad_norm=0.545575, loss=6.933534
I0331 21:33:48.836713 139792923473728 submission.py:139] 20) loss = 6.934, grad_norm = 0.546
I0331 21:33:49.211260 139734500206336 logging_writer.py:48] [21] global_step=21, grad_norm=0.536420, loss=6.927910
I0331 21:33:49.214829 139792923473728 submission.py:139] 21) loss = 6.928, grad_norm = 0.536
I0331 21:33:49.590354 139740535822080 logging_writer.py:48] [22] global_step=22, grad_norm=0.538357, loss=6.925632
I0331 21:33:49.595414 139792923473728 submission.py:139] 22) loss = 6.926, grad_norm = 0.538
I0331 21:33:49.973611 139734500206336 logging_writer.py:48] [23] global_step=23, grad_norm=0.527581, loss=6.926781
I0331 21:33:49.977273 139792923473728 submission.py:139] 23) loss = 6.927, grad_norm = 0.528
I0331 21:33:50.351486 139740535822080 logging_writer.py:48] [24] global_step=24, grad_norm=0.536191, loss=6.921175
I0331 21:33:50.355155 139792923473728 submission.py:139] 24) loss = 6.921, grad_norm = 0.536
I0331 21:33:50.729641 139734500206336 logging_writer.py:48] [25] global_step=25, grad_norm=0.532521, loss=6.927963
I0331 21:33:50.733544 139792923473728 submission.py:139] 25) loss = 6.928, grad_norm = 0.533
I0331 21:33:51.112849 139740535822080 logging_writer.py:48] [26] global_step=26, grad_norm=0.543262, loss=6.922467
I0331 21:33:51.116380 139792923473728 submission.py:139] 26) loss = 6.922, grad_norm = 0.543
I0331 21:33:51.493771 139734500206336 logging_writer.py:48] [27] global_step=27, grad_norm=0.534487, loss=6.927726
I0331 21:33:51.497453 139792923473728 submission.py:139] 27) loss = 6.928, grad_norm = 0.534
I0331 21:33:51.875625 139740535822080 logging_writer.py:48] [28] global_step=28, grad_norm=0.541625, loss=6.927413
I0331 21:33:51.879484 139792923473728 submission.py:139] 28) loss = 6.927, grad_norm = 0.542
I0331 21:33:52.256872 139734500206336 logging_writer.py:48] [29] global_step=29, grad_norm=0.541262, loss=6.920983
I0331 21:33:52.260523 139792923473728 submission.py:139] 29) loss = 6.921, grad_norm = 0.541
I0331 21:33:52.639637 139740535822080 logging_writer.py:48] [30] global_step=30, grad_norm=0.534608, loss=6.923834
I0331 21:33:52.643298 139792923473728 submission.py:139] 30) loss = 6.924, grad_norm = 0.535
I0331 21:33:53.019735 139734500206336 logging_writer.py:48] [31] global_step=31, grad_norm=0.520746, loss=6.923745
I0331 21:33:53.023262 139792923473728 submission.py:139] 31) loss = 6.924, grad_norm = 0.521
I0331 21:33:53.400579 139740535822080 logging_writer.py:48] [32] global_step=32, grad_norm=0.517702, loss=6.914630
I0331 21:33:53.404756 139792923473728 submission.py:139] 32) loss = 6.915, grad_norm = 0.518
I0331 21:33:53.782253 139734500206336 logging_writer.py:48] [33] global_step=33, grad_norm=0.549188, loss=6.926618
I0331 21:33:53.785902 139792923473728 submission.py:139] 33) loss = 6.927, grad_norm = 0.549
I0331 21:33:54.163082 139740535822080 logging_writer.py:48] [34] global_step=34, grad_norm=0.534226, loss=6.925391
I0331 21:33:54.166576 139792923473728 submission.py:139] 34) loss = 6.925, grad_norm = 0.534
I0331 21:33:54.542075 139734500206336 logging_writer.py:48] [35] global_step=35, grad_norm=0.526708, loss=6.925710
I0331 21:33:54.545801 139792923473728 submission.py:139] 35) loss = 6.926, grad_norm = 0.527
I0331 21:33:54.919342 139740535822080 logging_writer.py:48] [36] global_step=36, grad_norm=0.539429, loss=6.909004
I0331 21:33:54.923072 139792923473728 submission.py:139] 36) loss = 6.909, grad_norm = 0.539
I0331 21:33:55.296660 139734500206336 logging_writer.py:48] [37] global_step=37, grad_norm=0.542270, loss=6.918449
I0331 21:33:55.300023 139792923473728 submission.py:139] 37) loss = 6.918, grad_norm = 0.542
I0331 21:33:55.675538 139740535822080 logging_writer.py:48] [38] global_step=38, grad_norm=0.522756, loss=6.918438
I0331 21:33:55.679153 139792923473728 submission.py:139] 38) loss = 6.918, grad_norm = 0.523
I0331 21:33:56.055791 139734500206336 logging_writer.py:48] [39] global_step=39, grad_norm=0.544826, loss=6.923728
I0331 21:33:56.059443 139792923473728 submission.py:139] 39) loss = 6.924, grad_norm = 0.545
I0331 21:33:56.439623 139740535822080 logging_writer.py:48] [40] global_step=40, grad_norm=0.536924, loss=6.917027
I0331 21:33:56.443458 139792923473728 submission.py:139] 40) loss = 6.917, grad_norm = 0.537
I0331 21:33:56.819813 139734500206336 logging_writer.py:48] [41] global_step=41, grad_norm=0.519586, loss=6.923697
I0331 21:33:56.823814 139792923473728 submission.py:139] 41) loss = 6.924, grad_norm = 0.520
I0331 21:33:57.201099 139740535822080 logging_writer.py:48] [42] global_step=42, grad_norm=0.527319, loss=6.913850
I0331 21:33:57.204820 139792923473728 submission.py:139] 42) loss = 6.914, grad_norm = 0.527
I0331 21:33:57.580762 139734500206336 logging_writer.py:48] [43] global_step=43, grad_norm=0.533475, loss=6.917435
I0331 21:33:57.584599 139792923473728 submission.py:139] 43) loss = 6.917, grad_norm = 0.533
I0331 21:33:57.961781 139740535822080 logging_writer.py:48] [44] global_step=44, grad_norm=0.523106, loss=6.910682
I0331 21:33:57.965316 139792923473728 submission.py:139] 44) loss = 6.911, grad_norm = 0.523
I0331 21:33:58.341806 139734500206336 logging_writer.py:48] [45] global_step=45, grad_norm=0.522716, loss=6.919547
I0331 21:33:58.345223 139792923473728 submission.py:139] 45) loss = 6.920, grad_norm = 0.523
I0331 21:33:58.719200 139740535822080 logging_writer.py:48] [46] global_step=46, grad_norm=0.546202, loss=6.911665
I0331 21:33:58.722895 139792923473728 submission.py:139] 46) loss = 6.912, grad_norm = 0.546
I0331 21:33:59.099880 139734500206336 logging_writer.py:48] [47] global_step=47, grad_norm=0.528381, loss=6.913719
I0331 21:33:59.105413 139792923473728 submission.py:139] 47) loss = 6.914, grad_norm = 0.528
I0331 21:33:59.480801 139740535822080 logging_writer.py:48] [48] global_step=48, grad_norm=0.526023, loss=6.914474
I0331 21:33:59.484273 139792923473728 submission.py:139] 48) loss = 6.914, grad_norm = 0.526
I0331 21:33:59.858889 139734500206336 logging_writer.py:48] [49] global_step=49, grad_norm=0.540486, loss=6.915769
I0331 21:33:59.862567 139792923473728 submission.py:139] 49) loss = 6.916, grad_norm = 0.540
I0331 21:34:00.236559 139740535822080 logging_writer.py:48] [50] global_step=50, grad_norm=0.528867, loss=6.923166
I0331 21:34:00.239983 139792923473728 submission.py:139] 50) loss = 6.923, grad_norm = 0.529
I0331 21:34:00.621790 139734500206336 logging_writer.py:48] [51] global_step=51, grad_norm=0.527037, loss=6.916271
I0331 21:34:00.625196 139792923473728 submission.py:139] 51) loss = 6.916, grad_norm = 0.527
I0331 21:34:00.998914 139740535822080 logging_writer.py:48] [52] global_step=52, grad_norm=0.524255, loss=6.911354
I0331 21:34:01.002545 139792923473728 submission.py:139] 52) loss = 6.911, grad_norm = 0.524
I0331 21:34:01.376148 139734500206336 logging_writer.py:48] [53] global_step=53, grad_norm=0.517881, loss=6.905802
I0331 21:34:01.379732 139792923473728 submission.py:139] 53) loss = 6.906, grad_norm = 0.518
I0331 21:34:01.754727 139740535822080 logging_writer.py:48] [54] global_step=54, grad_norm=0.534479, loss=6.913396
I0331 21:34:01.758645 139792923473728 submission.py:139] 54) loss = 6.913, grad_norm = 0.534
I0331 21:34:02.137758 139734500206336 logging_writer.py:48] [55] global_step=55, grad_norm=0.519643, loss=6.918168
I0331 21:34:02.141168 139792923473728 submission.py:139] 55) loss = 6.918, grad_norm = 0.520
I0331 21:34:02.517347 139740535822080 logging_writer.py:48] [56] global_step=56, grad_norm=0.527853, loss=6.912260
I0331 21:34:02.523282 139792923473728 submission.py:139] 56) loss = 6.912, grad_norm = 0.528
I0331 21:34:02.898854 139734500206336 logging_writer.py:48] [57] global_step=57, grad_norm=0.530951, loss=6.910986
I0331 21:34:02.903204 139792923473728 submission.py:139] 57) loss = 6.911, grad_norm = 0.531
I0331 21:34:03.281514 139740535822080 logging_writer.py:48] [58] global_step=58, grad_norm=0.544854, loss=6.912978
I0331 21:34:03.286140 139792923473728 submission.py:139] 58) loss = 6.913, grad_norm = 0.545
I0331 21:34:03.660751 139734500206336 logging_writer.py:48] [59] global_step=59, grad_norm=0.535041, loss=6.909773
I0331 21:34:03.664581 139792923473728 submission.py:139] 59) loss = 6.910, grad_norm = 0.535
I0331 21:34:04.039322 139740535822080 logging_writer.py:48] [60] global_step=60, grad_norm=0.530170, loss=6.911179
I0331 21:34:04.043212 139792923473728 submission.py:139] 60) loss = 6.911, grad_norm = 0.530
I0331 21:34:04.418955 139734500206336 logging_writer.py:48] [61] global_step=61, grad_norm=0.513440, loss=6.903790
I0331 21:34:04.431516 139792923473728 submission.py:139] 61) loss = 6.904, grad_norm = 0.513
I0331 21:34:04.808778 139740535822080 logging_writer.py:48] [62] global_step=62, grad_norm=0.519440, loss=6.910787
I0331 21:34:04.812235 139792923473728 submission.py:139] 62) loss = 6.911, grad_norm = 0.519
I0331 21:34:05.189054 139734500206336 logging_writer.py:48] [63] global_step=63, grad_norm=0.532310, loss=6.905934
I0331 21:34:05.192452 139792923473728 submission.py:139] 63) loss = 6.906, grad_norm = 0.532
I0331 21:34:05.569558 139740535822080 logging_writer.py:48] [64] global_step=64, grad_norm=0.550884, loss=6.901198
I0331 21:34:05.573068 139792923473728 submission.py:139] 64) loss = 6.901, grad_norm = 0.551
I0331 21:34:05.945872 139734500206336 logging_writer.py:48] [65] global_step=65, grad_norm=0.532311, loss=6.906889
I0331 21:34:05.949595 139792923473728 submission.py:139] 65) loss = 6.907, grad_norm = 0.532
I0331 21:34:06.324756 139740535822080 logging_writer.py:48] [66] global_step=66, grad_norm=0.529668, loss=6.908835
I0331 21:34:06.328143 139792923473728 submission.py:139] 66) loss = 6.909, grad_norm = 0.530
I0331 21:34:06.705637 139734500206336 logging_writer.py:48] [67] global_step=67, grad_norm=0.530839, loss=6.909472
I0331 21:34:06.709792 139792923473728 submission.py:139] 67) loss = 6.909, grad_norm = 0.531
I0331 21:34:07.086222 139740535822080 logging_writer.py:48] [68] global_step=68, grad_norm=0.530467, loss=6.904284
I0331 21:34:07.089773 139792923473728 submission.py:139] 68) loss = 6.904, grad_norm = 0.530
I0331 21:34:07.486494 139734500206336 logging_writer.py:48] [69] global_step=69, grad_norm=0.515052, loss=6.909716
I0331 21:34:07.490106 139792923473728 submission.py:139] 69) loss = 6.910, grad_norm = 0.515
I0331 21:34:07.864491 139740535822080 logging_writer.py:48] [70] global_step=70, grad_norm=0.527509, loss=6.901497
I0331 21:34:07.867954 139792923473728 submission.py:139] 70) loss = 6.901, grad_norm = 0.528
I0331 21:34:08.247159 139734500206336 logging_writer.py:48] [71] global_step=71, grad_norm=0.527860, loss=6.907007
I0331 21:34:08.251299 139792923473728 submission.py:139] 71) loss = 6.907, grad_norm = 0.528
I0331 21:34:08.626432 139740535822080 logging_writer.py:48] [72] global_step=72, grad_norm=0.529338, loss=6.901662
I0331 21:34:08.630337 139792923473728 submission.py:139] 72) loss = 6.902, grad_norm = 0.529
I0331 21:34:09.007008 139734500206336 logging_writer.py:48] [73] global_step=73, grad_norm=0.545202, loss=6.901587
I0331 21:34:09.010712 139792923473728 submission.py:139] 73) loss = 6.902, grad_norm = 0.545
I0331 21:34:09.389694 139740535822080 logging_writer.py:48] [74] global_step=74, grad_norm=0.516100, loss=6.910850
I0331 21:34:09.393276 139792923473728 submission.py:139] 74) loss = 6.911, grad_norm = 0.516
I0331 21:34:09.770667 139734500206336 logging_writer.py:48] [75] global_step=75, grad_norm=0.517782, loss=6.902920
I0331 21:34:09.774628 139792923473728 submission.py:139] 75) loss = 6.903, grad_norm = 0.518
I0331 21:34:10.149269 139740535822080 logging_writer.py:48] [76] global_step=76, grad_norm=0.519606, loss=6.905945
I0331 21:34:10.153034 139792923473728 submission.py:139] 76) loss = 6.906, grad_norm = 0.520
I0331 21:34:10.529435 139734500206336 logging_writer.py:48] [77] global_step=77, grad_norm=0.524931, loss=6.900101
I0331 21:34:10.533223 139792923473728 submission.py:139] 77) loss = 6.900, grad_norm = 0.525
I0331 21:34:10.905997 139740535822080 logging_writer.py:48] [78] global_step=78, grad_norm=0.536574, loss=6.897735
I0331 21:34:10.909844 139792923473728 submission.py:139] 78) loss = 6.898, grad_norm = 0.537
I0331 21:34:11.286134 139734500206336 logging_writer.py:48] [79] global_step=79, grad_norm=0.543680, loss=6.901931
I0331 21:34:11.290160 139792923473728 submission.py:139] 79) loss = 6.902, grad_norm = 0.544
I0331 21:34:11.667940 139740535822080 logging_writer.py:48] [80] global_step=80, grad_norm=0.510124, loss=6.896084
I0331 21:34:11.671963 139792923473728 submission.py:139] 80) loss = 6.896, grad_norm = 0.510
I0331 21:34:12.048856 139734500206336 logging_writer.py:48] [81] global_step=81, grad_norm=0.519020, loss=6.901937
I0331 21:34:12.052504 139792923473728 submission.py:139] 81) loss = 6.902, grad_norm = 0.519
I0331 21:34:12.427656 139740535822080 logging_writer.py:48] [82] global_step=82, grad_norm=0.523787, loss=6.891473
I0331 21:34:12.431765 139792923473728 submission.py:139] 82) loss = 6.891, grad_norm = 0.524
I0331 21:34:12.808300 139734500206336 logging_writer.py:48] [83] global_step=83, grad_norm=0.531627, loss=6.896545
I0331 21:34:12.812542 139792923473728 submission.py:139] 83) loss = 6.897, grad_norm = 0.532
I0331 21:34:13.192925 139740535822080 logging_writer.py:48] [84] global_step=84, grad_norm=0.521372, loss=6.907068
I0331 21:34:13.196652 139792923473728 submission.py:139] 84) loss = 6.907, grad_norm = 0.521
I0331 21:34:13.575382 139734500206336 logging_writer.py:48] [85] global_step=85, grad_norm=0.512459, loss=6.894459
I0331 21:34:13.578818 139792923473728 submission.py:139] 85) loss = 6.894, grad_norm = 0.512
I0331 21:34:13.955946 139740535822080 logging_writer.py:48] [86] global_step=86, grad_norm=0.538625, loss=6.888585
I0331 21:34:13.959526 139792923473728 submission.py:139] 86) loss = 6.889, grad_norm = 0.539
I0331 21:34:14.336562 139734500206336 logging_writer.py:48] [87] global_step=87, grad_norm=0.531283, loss=6.890784
I0331 21:34:14.340137 139792923473728 submission.py:139] 87) loss = 6.891, grad_norm = 0.531
I0331 21:34:14.719015 139740535822080 logging_writer.py:48] [88] global_step=88, grad_norm=0.524044, loss=6.886004
I0331 21:34:14.722652 139792923473728 submission.py:139] 88) loss = 6.886, grad_norm = 0.524
I0331 21:34:15.099703 139734500206336 logging_writer.py:48] [89] global_step=89, grad_norm=0.522476, loss=6.885705
I0331 21:34:15.103791 139792923473728 submission.py:139] 89) loss = 6.886, grad_norm = 0.522
I0331 21:34:15.484722 139740535822080 logging_writer.py:48] [90] global_step=90, grad_norm=0.535484, loss=6.901490
I0331 21:34:15.488363 139792923473728 submission.py:139] 90) loss = 6.901, grad_norm = 0.535
I0331 21:34:15.866907 139734500206336 logging_writer.py:48] [91] global_step=91, grad_norm=0.523667, loss=6.890491
I0331 21:34:15.872294 139792923473728 submission.py:139] 91) loss = 6.890, grad_norm = 0.524
I0331 21:34:16.247472 139740535822080 logging_writer.py:48] [92] global_step=92, grad_norm=0.536979, loss=6.884226
I0331 21:34:16.251334 139792923473728 submission.py:139] 92) loss = 6.884, grad_norm = 0.537
I0331 21:34:16.631313 139734500206336 logging_writer.py:48] [93] global_step=93, grad_norm=0.520582, loss=6.889874
I0331 21:34:16.635664 139792923473728 submission.py:139] 93) loss = 6.890, grad_norm = 0.521
I0331 21:34:17.011382 139740535822080 logging_writer.py:48] [94] global_step=94, grad_norm=0.518512, loss=6.892835
I0331 21:34:17.015062 139792923473728 submission.py:139] 94) loss = 6.893, grad_norm = 0.519
I0331 21:34:17.394086 139734500206336 logging_writer.py:48] [95] global_step=95, grad_norm=0.526818, loss=6.892971
I0331 21:34:17.397844 139792923473728 submission.py:139] 95) loss = 6.893, grad_norm = 0.527
I0331 21:34:17.777195 139740535822080 logging_writer.py:48] [96] global_step=96, grad_norm=0.523207, loss=6.889225
I0331 21:34:17.780942 139792923473728 submission.py:139] 96) loss = 6.889, grad_norm = 0.523
I0331 21:34:18.158402 139734500206336 logging_writer.py:48] [97] global_step=97, grad_norm=0.526354, loss=6.886579
I0331 21:34:18.162057 139792923473728 submission.py:139] 97) loss = 6.887, grad_norm = 0.526
I0331 21:34:18.542522 139740535822080 logging_writer.py:48] [98] global_step=98, grad_norm=0.536758, loss=6.887826
I0331 21:34:18.546549 139792923473728 submission.py:139] 98) loss = 6.888, grad_norm = 0.537
I0331 21:34:18.923920 139734500206336 logging_writer.py:48] [99] global_step=99, grad_norm=0.532982, loss=6.883896
I0331 21:34:18.927580 139792923473728 submission.py:139] 99) loss = 6.884, grad_norm = 0.533
I0331 21:34:19.306154 139740535822080 logging_writer.py:48] [100] global_step=100, grad_norm=0.525174, loss=6.884665
I0331 21:34:19.309739 139792923473728 submission.py:139] 100) loss = 6.885, grad_norm = 0.525
I0331 21:36:47.867525 139734500206336 logging_writer.py:48] [500] global_step=500, grad_norm=0.633464, loss=6.575611
I0331 21:36:47.872295 139792923473728 submission.py:139] 500) loss = 6.576, grad_norm = 0.633
I0331 21:39:53.738093 139740535822080 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.667126, loss=6.294386
I0331 21:39:53.744040 139792923473728 submission.py:139] 1000) loss = 6.294, grad_norm = 0.667
I0331 21:42:11.364283 139792923473728 submission_runner.py:371] Before eval at step 1368: RAM USED (GB) 98.380722176
I0331 21:42:11.364522 139792923473728 spec.py:298] Evaluating on the training split.
I0331 21:42:52.424504 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 21:43:36.061772 139792923473728 spec.py:326] Evaluating on the test split.
I0331 21:43:37.442640 139792923473728 submission_runner.py:380] Time since start: 623.76s, 	Step: 1368, 	{'train/accuracy': 0.06096540178571429, 'train/loss': 5.597124372209821, 'validation/accuracy': 0.05894, 'validation/loss': 5.64826, 'validation/num_examples': 50000, 'test/accuracy': 0.0385, 'test/loss': 5.875715234375, 'test/num_examples': 10000}
I0331 21:43:37.442985 139792923473728 submission_runner.py:390] After eval at step 1368: RAM USED (GB) 98.346135552
I0331 21:43:37.451417 139740544214784 logging_writer.py:48] [1368] global_step=1368, preemption_count=0, score=378.064056, test/accuracy=0.038500, test/loss=5.875715, test/num_examples=10000, total_duration=623.758204, train/accuracy=0.060965, train/loss=5.597124, validation/accuracy=0.058940, validation/loss=5.648260, validation/num_examples=50000
I0331 21:43:37.764425 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_1368.
I0331 21:43:37.765177 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 1368: RAM USED (GB) 98.344607744
I0331 21:44:26.886654 139740552607488 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.917229, loss=6.030905
I0331 21:44:26.890306 139792923473728 submission.py:139] 1500) loss = 6.031, grad_norm = 0.917
I0331 21:47:32.327737 139740544214784 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.824809, loss=5.822415
I0331 21:47:32.331803 139792923473728 submission.py:139] 2000) loss = 5.822, grad_norm = 0.825
I0331 21:50:38.044013 139740552607488 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.792703, loss=5.581754
I0331 21:50:38.047886 139792923473728 submission.py:139] 2500) loss = 5.582, grad_norm = 0.793
I0331 21:52:08.068992 139792923473728 submission_runner.py:371] Before eval at step 2740: RAM USED (GB) 99.69260544
I0331 21:52:08.069204 139792923473728 spec.py:298] Evaluating on the training split.
I0331 21:52:50.977625 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 21:53:46.841216 139792923473728 spec.py:326] Evaluating on the test split.
I0331 21:53:48.222011 139792923473728 submission_runner.py:380] Time since start: 1220.46s, 	Step: 2740, 	{'train/accuracy': 0.15704719387755103, 'train/loss': 4.572382401446907, 'validation/accuracy': 0.14258, 'validation/loss': 4.664575, 'validation/num_examples': 50000, 'test/accuracy': 0.1001, 'test/loss': 5.030237109375, 'test/num_examples': 10000}
I0331 21:53:48.222334 139792923473728 submission_runner.py:390] After eval at step 2740: RAM USED (GB) 99.662258176
I0331 21:53:48.230846 139740544214784 logging_writer.py:48] [2740] global_step=2740, preemption_count=0, score=737.350364, test/accuracy=0.100100, test/loss=5.030237, test/num_examples=10000, total_duration=1220.463442, train/accuracy=0.157047, train/loss=4.572382, validation/accuracy=0.142580, validation/loss=4.664575, validation/num_examples=50000
I0331 21:53:48.531594 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_2740.
I0331 21:53:48.532271 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 2740: RAM USED (GB) 99.661283328
I0331 21:55:25.009320 139740552607488 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.892694, loss=5.281390
I0331 21:55:25.012939 139792923473728 submission.py:139] 3000) loss = 5.281, grad_norm = 0.893
I0331 21:58:30.400300 139740544214784 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.914760, loss=5.029411
I0331 21:58:30.404281 139792923473728 submission.py:139] 3500) loss = 5.029, grad_norm = 0.915
I0331 22:01:37.217230 139740552607488 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.829865, loss=4.943654
I0331 22:01:37.222563 139792923473728 submission.py:139] 4000) loss = 4.944, grad_norm = 0.830
I0331 22:02:18.700351 139792923473728 submission_runner.py:371] Before eval at step 4113: RAM USED (GB) 99.858415616
I0331 22:02:18.700568 139792923473728 spec.py:298] Evaluating on the training split.
I0331 22:03:01.058033 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 22:03:58.595320 139792923473728 spec.py:326] Evaluating on the test split.
I0331 22:03:59.968813 139792923473728 submission_runner.py:380] Time since start: 1831.09s, 	Step: 4113, 	{'train/accuracy': 0.263671875, 'train/loss': 3.726690798389668, 'validation/accuracy': 0.2488, 'validation/loss': 3.8150409375, 'validation/num_examples': 50000, 'test/accuracy': 0.184, 'test/loss': 4.309848828125, 'test/num_examples': 10000}
I0331 22:03:59.969187 139792923473728 submission_runner.py:390] After eval at step 4113: RAM USED (GB) 99.836428288
I0331 22:03:59.977734 139740544214784 logging_writer.py:48] [4113] global_step=4113, preemption_count=0, score=1096.448143, test/accuracy=0.184000, test/loss=4.309849, test/num_examples=10000, total_duration=1831.094664, train/accuracy=0.263672, train/loss=3.726691, validation/accuracy=0.248800, validation/loss=3.815041, validation/num_examples=50000
I0331 22:04:00.269989 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_4113.
I0331 22:04:00.270697 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 4113: RAM USED (GB) 99.836485632
I0331 22:06:23.788979 139740552607488 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.710419, loss=4.608061
I0331 22:06:23.792931 139792923473728 submission.py:139] 4500) loss = 4.608, grad_norm = 0.710
I0331 22:09:29.324770 139740544214784 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.742360, loss=4.549726
I0331 22:09:29.329369 139792923473728 submission.py:139] 5000) loss = 4.550, grad_norm = 0.742
I0331 22:12:30.640594 139792923473728 submission_runner.py:371] Before eval at step 5487: RAM USED (GB) 100.037136384
I0331 22:12:30.640805 139792923473728 spec.py:298] Evaluating on the training split.
I0331 22:13:13.642932 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 22:13:57.825971 139792923473728 spec.py:326] Evaluating on the test split.
I0331 22:13:59.181931 139792923473728 submission_runner.py:380] Time since start: 2443.03s, 	Step: 5487, 	{'train/accuracy': 0.36226482780612246, 'train/loss': 3.2159336635044644, 'validation/accuracy': 0.33572, 'validation/loss': 3.34338125, 'validation/num_examples': 50000, 'test/accuracy': 0.2463, 'test/loss': 3.90467890625, 'test/num_examples': 10000}
I0331 22:13:59.182404 139792923473728 submission_runner.py:390] After eval at step 5487: RAM USED (GB) 100.01874944
I0331 22:13:59.192665 139740552607488 logging_writer.py:48] [5487] global_step=5487, preemption_count=0, score=1455.582861, test/accuracy=0.246300, test/loss=3.904679, test/num_examples=10000, total_duration=2443.034649, train/accuracy=0.362265, train/loss=3.215934, validation/accuracy=0.335720, validation/loss=3.343381, validation/num_examples=50000
I0331 22:13:59.509866 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_5487.
I0331 22:13:59.510666 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 5487: RAM USED (GB) 100.017291264
I0331 22:14:04.687806 139740544214784 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.694693, loss=4.470156
I0331 22:14:04.691533 139792923473728 submission.py:139] 5500) loss = 4.470, grad_norm = 0.695
I0331 22:17:09.823682 139740552607488 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.652722, loss=4.323944
I0331 22:17:09.827930 139792923473728 submission.py:139] 6000) loss = 4.324, grad_norm = 0.653
I0331 22:20:16.565905 139740544214784 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.608304, loss=4.185595
I0331 22:20:16.571334 139792923473728 submission.py:139] 6500) loss = 4.186, grad_norm = 0.608
I0331 22:22:29.815302 139792923473728 submission_runner.py:371] Before eval at step 6861: RAM USED (GB) 99.958894592
I0331 22:22:29.815508 139792923473728 spec.py:298] Evaluating on the training split.
I0331 22:23:11.854681 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 22:24:07.991696 139792923473728 spec.py:326] Evaluating on the test split.
I0331 22:24:09.361332 139792923473728 submission_runner.py:380] Time since start: 3042.21s, 	Step: 6861, 	{'train/accuracy': 0.4580676020408163, 'train/loss': 2.5803881275410556, 'validation/accuracy': 0.42478, 'validation/loss': 2.759190625, 'validation/num_examples': 50000, 'test/accuracy': 0.3171, 'test/loss': 3.41964921875, 'test/num_examples': 10000}
I0331 22:24:09.361709 139792923473728 submission_runner.py:390] After eval at step 6861: RAM USED (GB) 99.88960256
I0331 22:24:09.371184 139740552607488 logging_writer.py:48] [6861] global_step=6861, preemption_count=0, score=1814.773370, test/accuracy=0.317100, test/loss=3.419649, test/num_examples=10000, total_duration=3042.209425, train/accuracy=0.458068, train/loss=2.580388, validation/accuracy=0.424780, validation/loss=2.759191, validation/num_examples=50000
I0331 22:24:09.692232 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_6861.
I0331 22:24:09.693056 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 6861: RAM USED (GB) 99.888103424
I0331 22:25:01.337840 139740544214784 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.595762, loss=4.100154
I0331 22:25:01.343253 139792923473728 submission.py:139] 7000) loss = 4.100, grad_norm = 0.596
I0331 22:28:06.882653 139740552607488 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.574756, loss=4.040689
I0331 22:28:06.887434 139792923473728 submission.py:139] 7500) loss = 4.041, grad_norm = 0.575
I0331 22:31:13.209323 139740544214784 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.584536, loss=3.982713
I0331 22:31:13.213118 139792923473728 submission.py:139] 8000) loss = 3.983, grad_norm = 0.585
I0331 22:32:39.892450 139792923473728 submission_runner.py:371] Before eval at step 8235: RAM USED (GB) 99.92200192
I0331 22:32:39.892652 139792923473728 spec.py:298] Evaluating on the training split.
I0331 22:33:22.703329 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 22:34:07.220892 139792923473728 spec.py:326] Evaluating on the test split.
I0331 22:34:08.577068 139792923473728 submission_runner.py:380] Time since start: 3652.29s, 	Step: 8235, 	{'train/accuracy': 0.48610889668367346, 'train/loss': 2.5059126250597896, 'validation/accuracy': 0.44864, 'validation/loss': 2.691711875, 'validation/num_examples': 50000, 'test/accuracy': 0.3367, 'test/loss': 3.34860625, 'test/num_examples': 10000}
I0331 22:34:08.577378 139792923473728 submission_runner.py:390] After eval at step 8235: RAM USED (GB) 99.930058752
I0331 22:34:08.585941 139740552607488 logging_writer.py:48] [8235] global_step=8235, preemption_count=0, score=2173.849488, test/accuracy=0.336700, test/loss=3.348606, test/num_examples=10000, total_duration=3652.286882, train/accuracy=0.486109, train/loss=2.505913, validation/accuracy=0.448640, validation/loss=2.691712, validation/num_examples=50000
I0331 22:34:08.884814 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_8235.
I0331 22:34:08.885487 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 8235: RAM USED (GB) 99.928563712
I0331 22:35:47.222489 139740544214784 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.541031, loss=3.854289
I0331 22:35:47.226389 139792923473728 submission.py:139] 8500) loss = 3.854, grad_norm = 0.541
I0331 22:38:54.140481 139740552607488 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.537103, loss=3.877352
I0331 22:38:54.145621 139792923473728 submission.py:139] 9000) loss = 3.877, grad_norm = 0.537
I0331 22:41:59.237729 139740544214784 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.525222, loss=3.839708
I0331 22:41:59.242149 139792923473728 submission.py:139] 9500) loss = 3.840, grad_norm = 0.525
I0331 22:42:39.272604 139792923473728 submission_runner.py:371] Before eval at step 9609: RAM USED (GB) 100.10228736
I0331 22:42:39.272817 139792923473728 spec.py:298] Evaluating on the training split.
I0331 22:43:22.123945 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 22:44:12.083588 139792923473728 spec.py:326] Evaluating on the test split.
I0331 22:44:13.461378 139792923473728 submission_runner.py:380] Time since start: 4251.67s, 	Step: 9609, 	{'train/accuracy': 0.5706313775510204, 'train/loss': 2.101990524603396, 'validation/accuracy': 0.52194, 'validation/loss': 2.3161125, 'validation/num_examples': 50000, 'test/accuracy': 0.4049, 'test/loss': 2.961247265625, 'test/num_examples': 10000}
I0331 22:44:13.461704 139792923473728 submission_runner.py:390] After eval at step 9609: RAM USED (GB) 100.109848576
I0331 22:44:13.470499 139740552607488 logging_writer.py:48] [9609] global_step=9609, preemption_count=0, score=2533.167667, test/accuracy=0.404900, test/loss=2.961247, test/num_examples=10000, total_duration=4251.666935, train/accuracy=0.570631, train/loss=2.101991, validation/accuracy=0.521940, validation/loss=2.316113, validation/num_examples=50000
I0331 22:44:13.761469 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_9609.
I0331 22:44:13.762136 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 9609: RAM USED (GB) 100.108357632
I0331 22:46:38.972529 139740544214784 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.505918, loss=3.718469
I0331 22:46:38.976579 139792923473728 submission.py:139] 10000) loss = 3.718, grad_norm = 0.506
I0331 22:49:45.512062 139740552607488 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.495039, loss=3.662565
I0331 22:49:45.516364 139792923473728 submission.py:139] 10500) loss = 3.663, grad_norm = 0.495
I0331 22:52:44.037242 139792923473728 submission_runner.py:371] Before eval at step 10983: RAM USED (GB) 99.97266944
I0331 22:52:44.037454 139792923473728 spec.py:298] Evaluating on the training split.
I0331 22:53:27.811548 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 22:54:12.831704 139792923473728 spec.py:326] Evaluating on the test split.
I0331 22:54:14.178005 139792923473728 submission_runner.py:380] Time since start: 4856.43s, 	Step: 10983, 	{'train/accuracy': 0.5935507015306123, 'train/loss': 1.9832262311662947, 'validation/accuracy': 0.54436, 'validation/loss': 2.19985421875, 'validation/num_examples': 50000, 'test/accuracy': 0.4224, 'test/loss': 2.868584765625, 'test/num_examples': 10000}
I0331 22:54:14.178344 139792923473728 submission_runner.py:390] After eval at step 10983: RAM USED (GB) 100.046651392
I0331 22:54:14.187029 139740544214784 logging_writer.py:48] [10983] global_step=10983, preemption_count=0, score=2892.298361, test/accuracy=0.422400, test/loss=2.868585, test/num_examples=10000, total_duration=4856.431508, train/accuracy=0.593551, train/loss=1.983226, validation/accuracy=0.544360, validation/loss=2.199854, validation/num_examples=50000
I0331 22:54:14.484895 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_10983.
I0331 22:54:14.485558 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 10983: RAM USED (GB) 100.045160448
I0331 22:54:21.112998 139740552607488 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.490768, loss=3.576751
I0331 22:54:21.116538 139792923473728 submission.py:139] 11000) loss = 3.577, grad_norm = 0.491
I0331 22:57:27.654950 139740544214784 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.474614, loss=3.522328
I0331 22:57:27.660844 139792923473728 submission.py:139] 11500) loss = 3.522, grad_norm = 0.475
I0331 23:00:32.613914 139740552607488 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.477862, loss=3.563950
I0331 23:00:32.618086 139792923473728 submission.py:139] 12000) loss = 3.564, grad_norm = 0.478
I0331 23:02:44.930508 139792923473728 submission_runner.py:371] Before eval at step 12358: RAM USED (GB) 99.95565056
I0331 23:02:44.930717 139792923473728 spec.py:298] Evaluating on the training split.
I0331 23:03:28.829418 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 23:04:25.683278 139792923473728 spec.py:326] Evaluating on the test split.
I0331 23:04:27.039986 139792923473728 submission_runner.py:380] Time since start: 5457.32s, 	Step: 12358, 	{'train/accuracy': 0.6271723533163265, 'train/loss': 1.7426127219686702, 'validation/accuracy': 0.57556, 'validation/loss': 1.98705765625, 'validation/num_examples': 50000, 'test/accuracy': 0.4382, 'test/loss': 2.719115625, 'test/num_examples': 10000}
I0331 23:04:27.040293 139792923473728 submission_runner.py:390] After eval at step 12358: RAM USED (GB) 99.900715008
I0331 23:04:27.048085 139740544214784 logging_writer.py:48] [12358] global_step=12358, preemption_count=0, score=3251.565533, test/accuracy=0.438200, test/loss=2.719116, test/num_examples=10000, total_duration=5457.324422, train/accuracy=0.627172, train/loss=1.742613, validation/accuracy=0.575560, validation/loss=1.987058, validation/num_examples=50000
I0331 23:04:27.360817 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_12358.
I0331 23:04:27.361521 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 12358: RAM USED (GB) 99.899998208
I0331 23:05:20.168303 139740552607488 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.479116, loss=3.634348
I0331 23:05:20.174194 139792923473728 submission.py:139] 12500) loss = 3.634, grad_norm = 0.479
I0331 23:08:26.581218 139740544214784 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.478306, loss=3.538263
I0331 23:08:26.585474 139792923473728 submission.py:139] 13000) loss = 3.538, grad_norm = 0.478
I0331 23:11:31.873588 139740552607488 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.445290, loss=3.476823
I0331 23:11:31.879619 139792923473728 submission.py:139] 13500) loss = 3.477, grad_norm = 0.445
I0331 23:12:57.620496 139792923473728 submission_runner.py:371] Before eval at step 13732: RAM USED (GB) 100.015661056
I0331 23:12:57.620697 139792923473728 spec.py:298] Evaluating on the training split.
I0331 23:13:40.063558 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 23:14:25.022991 139792923473728 spec.py:326] Evaluating on the test split.
I0331 23:14:26.384684 139792923473728 submission_runner.py:380] Time since start: 6070.02s, 	Step: 13732, 	{'train/accuracy': 0.6598772321428571, 'train/loss': 1.6223090035574776, 'validation/accuracy': 0.60098, 'validation/loss': 1.8854953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4655, 'test/loss': 2.58681015625, 'test/num_examples': 10000}
I0331 23:14:26.385093 139792923473728 submission_runner.py:390] After eval at step 13732: RAM USED (GB) 100.019970048
I0331 23:14:26.394976 139740544214784 logging_writer.py:48] [13732] global_step=13732, preemption_count=0, score=3610.778356, test/accuracy=0.465500, test/loss=2.586810, test/num_examples=10000, total_duration=6070.015414, train/accuracy=0.659877, train/loss=1.622309, validation/accuracy=0.600980, validation/loss=1.885495, validation/num_examples=50000
I0331 23:14:26.704464 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_13732.
I0331 23:14:26.705316 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 13732: RAM USED (GB) 100.018479104
I0331 23:16:07.252640 139792923473728 submission_runner.py:371] Before eval at step 14000: RAM USED (GB) 100.202303488
I0331 23:16:07.252852 139792923473728 spec.py:298] Evaluating on the training split.
I0331 23:16:49.041789 139792923473728 spec.py:310] Evaluating on the validation split.
I0331 23:17:32.840942 139792923473728 spec.py:326] Evaluating on the test split.
I0331 23:17:34.184747 139792923473728 submission_runner.py:380] Time since start: 6259.65s, 	Step: 14000, 	{'train/accuracy': 0.6536591198979592, 'train/loss': 1.6825636260363521, 'validation/accuracy': 0.59144, 'validation/loss': 1.9522696875, 'validation/num_examples': 50000, 'test/accuracy': 0.4597, 'test/loss': 2.6520017578125, 'test/num_examples': 10000}
I0331 23:17:34.185079 139792923473728 submission_runner.py:390] After eval at step 14000: RAM USED (GB) 100.121944064
I0331 23:17:34.194984 139740552607488 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=3681.874933, test/accuracy=0.459700, test/loss=2.652002, test/num_examples=10000, total_duration=6259.647598, train/accuracy=0.653659, train/loss=1.682564, validation/accuracy=0.591440, validation/loss=1.952270, validation/num_examples=50000
I0331 23:17:34.490665 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0331 23:17:34.491374 139792923473728 submission_runner.py:409] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.12045312
I0331 23:17:34.499225 139740544214784 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=3681.874933
I0331 23:17:35.346600 139792923473728 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0331 23:17:35.651655 139792923473728 submission_runner.py:543] Tuning trial 1/1
I0331 23:17:35.651875 139792923473728 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0331 23:17:35.652584 139792923473728 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009765625, 'train/loss': 6.92459604691486, 'validation/accuracy': 0.00104, 'validation/loss': 6.92413125, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.927590625, 'test/num_examples': 10000, 'score': 7.783123254776001, 'total_duration': 7.785107612609863, 'global_step': 1, 'preemption_count': 0}), (1368, {'train/accuracy': 0.06096540178571429, 'train/loss': 5.597124372209821, 'validation/accuracy': 0.05894, 'validation/loss': 5.64826, 'validation/num_examples': 50000, 'test/accuracy': 0.0385, 'test/loss': 5.875715234375, 'test/num_examples': 10000, 'score': 378.0640559196472, 'total_duration': 623.7582039833069, 'global_step': 1368, 'preemption_count': 0}), (2740, {'train/accuracy': 0.15704719387755103, 'train/loss': 4.572382401446907, 'validation/accuracy': 0.14258, 'validation/loss': 4.664575, 'validation/num_examples': 50000, 'test/accuracy': 0.1001, 'test/loss': 5.030237109375, 'test/num_examples': 10000, 'score': 737.3503637313843, 'total_duration': 1220.4634420871735, 'global_step': 2740, 'preemption_count': 0}), (4113, {'train/accuracy': 0.263671875, 'train/loss': 3.726690798389668, 'validation/accuracy': 0.2488, 'validation/loss': 3.8150409375, 'validation/num_examples': 50000, 'test/accuracy': 0.184, 'test/loss': 4.309848828125, 'test/num_examples': 10000, 'score': 1096.4481427669525, 'total_duration': 1831.0946643352509, 'global_step': 4113, 'preemption_count': 0}), (5487, {'train/accuracy': 0.36226482780612246, 'train/loss': 3.2159336635044644, 'validation/accuracy': 0.33572, 'validation/loss': 3.34338125, 'validation/num_examples': 50000, 'test/accuracy': 0.2463, 'test/loss': 3.90467890625, 'test/num_examples': 10000, 'score': 1455.5828614234924, 'total_duration': 2443.0346488952637, 'global_step': 5487, 'preemption_count': 0}), (6861, {'train/accuracy': 0.4580676020408163, 'train/loss': 2.5803881275410556, 'validation/accuracy': 0.42478, 'validation/loss': 2.759190625, 'validation/num_examples': 50000, 'test/accuracy': 0.3171, 'test/loss': 3.41964921875, 'test/num_examples': 10000, 'score': 1814.773369550705, 'total_duration': 3042.2094247341156, 'global_step': 6861, 'preemption_count': 0}), (8235, {'train/accuracy': 0.48610889668367346, 'train/loss': 2.5059126250597896, 'validation/accuracy': 0.44864, 'validation/loss': 2.691711875, 'validation/num_examples': 50000, 'test/accuracy': 0.3367, 'test/loss': 3.34860625, 'test/num_examples': 10000, 'score': 2173.8494880199432, 'total_duration': 3652.286882162094, 'global_step': 8235, 'preemption_count': 0}), (9609, {'train/accuracy': 0.5706313775510204, 'train/loss': 2.101990524603396, 'validation/accuracy': 0.52194, 'validation/loss': 2.3161125, 'validation/num_examples': 50000, 'test/accuracy': 0.4049, 'test/loss': 2.961247265625, 'test/num_examples': 10000, 'score': 2533.1676666736603, 'total_duration': 4251.666934728622, 'global_step': 9609, 'preemption_count': 0}), (10983, {'train/accuracy': 0.5935507015306123, 'train/loss': 1.9832262311662947, 'validation/accuracy': 0.54436, 'validation/loss': 2.19985421875, 'validation/num_examples': 50000, 'test/accuracy': 0.4224, 'test/loss': 2.868584765625, 'test/num_examples': 10000, 'score': 2892.2983605861664, 'total_duration': 4856.431508302689, 'global_step': 10983, 'preemption_count': 0}), (12358, {'train/accuracy': 0.6271723533163265, 'train/loss': 1.7426127219686702, 'validation/accuracy': 0.57556, 'validation/loss': 1.98705765625, 'validation/num_examples': 50000, 'test/accuracy': 0.4382, 'test/loss': 2.719115625, 'test/num_examples': 10000, 'score': 3251.5655329227448, 'total_duration': 5457.324422121048, 'global_step': 12358, 'preemption_count': 0}), (13732, {'train/accuracy': 0.6598772321428571, 'train/loss': 1.6223090035574776, 'validation/accuracy': 0.60098, 'validation/loss': 1.8854953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4655, 'test/loss': 2.58681015625, 'test/num_examples': 10000, 'score': 3610.7783563137054, 'total_duration': 6070.015413761139, 'global_step': 13732, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6536591198979592, 'train/loss': 1.6825636260363521, 'validation/accuracy': 0.59144, 'validation/loss': 1.9522696875, 'validation/num_examples': 50000, 'test/accuracy': 0.4597, 'test/loss': 2.6520017578125, 'test/num_examples': 10000, 'score': 3681.8749330043793, 'total_duration': 6259.647598028183, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0331 23:17:35.652686 139792923473728 submission_runner.py:546] Timing: 3681.8749330043793
I0331 23:17:35.652734 139792923473728 submission_runner.py:547] ====================
I0331 23:17:35.652836 139792923473728 submission_runner.py:606] Final imagenet_resnet score: 3681.8749330043793
