I0401 04:33:31.701809 139923241899840 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/librispeech_conformer_jax.
I0401 04:33:31.761954 139923241899840 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0401 04:33:32.649266 139923241899840 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0401 04:33:32.649945 139923241899840 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0401 04:33:32.655293 139923241899840 submission_runner.py:511] Using RNG seed 1342270836
I0401 04:33:35.143250 139923241899840 submission_runner.py:520] --- Tuning run 1/1 ---
I0401 04:33:35.143443 139923241899840 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1.
I0401 04:33:35.143613 139923241899840 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/hparams.json.
I0401 04:33:35.272735 139923241899840 submission_runner.py:230] Starting train once: RAM USED (GB) 4.379656192
I0401 04:33:35.272907 139923241899840 submission_runner.py:231] Initializing dataset.
I0401 04:33:35.273081 139923241899840 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.379656192
I0401 04:33:35.273160 139923241899840 submission_runner.py:240] Initializing model.
I0401 04:33:41.171315 139923241899840 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.812775936
I0401 04:33:41.171513 139923241899840 submission_runner.py:252] Initializing optimizer.
I0401 04:33:42.017696 139923241899840 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.812923392
I0401 04:33:42.017888 139923241899840 submission_runner.py:261] Initializing metrics bundle.
I0401 04:33:42.017955 139923241899840 submission_runner.py:276] Initializing checkpoint and logger.
I0401 04:33:42.018821 139923241899840 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0401 04:33:42.019084 139923241899840 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 04:33:42.019149 139923241899840 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 04:33:42.773179 139923241899840 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0401 04:33:42.774064 139923241899840 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/flags_0.json.
I0401 04:33:42.779457 139923241899840 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.812071424
I0401 04:33:42.779644 139923241899840 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.812071424
I0401 04:33:42.779702 139923241899840 submission_runner.py:313] Starting training loop.
I0401 04:33:42.976906 139923241899840 input_pipeline.py:20] Loading split = train-clean-100
I0401 04:33:43.008841 139923241899840 input_pipeline.py:20] Loading split = train-clean-360
I0401 04:33:43.328580 139923241899840 input_pipeline.py:20] Loading split = train-other-500
I0401 04:33:47.157597 139923241899840 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.486895616
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0401 04:34:44.455176 139746927965952 logging_writer.py:48] [0] global_step=0, grad_norm=37.788726806640625, loss=31.853103637695312
I0401 04:34:44.475482 139923241899840 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.342425088
I0401 04:34:44.475729 139923241899840 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.342425088
I0401 04:34:44.475813 139923241899840 spec.py:298] Evaluating on the training split.
I0401 04:34:44.759448 139923241899840 input_pipeline.py:20] Loading split = train-clean-100
I0401 04:34:44.785490 139923241899840 input_pipeline.py:20] Loading split = train-clean-360
I0401 04:34:45.092556 139923241899840 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0401 04:35:37.968424 139923241899840 spec.py:310] Evaluating on the validation split.
I0401 04:35:38.033736 139923241899840 input_pipeline.py:20] Loading split = dev-clean
I0401 04:35:38.038594 139923241899840 input_pipeline.py:20] Loading split = dev-other
I0401 04:36:17.265756 139923241899840 spec.py:326] Evaluating on the test split.
I0401 04:36:17.327523 139923241899840 input_pipeline.py:20] Loading split = test-clean
I0401 04:36:44.452935 139923241899840 submission_runner.py:382] Time since start: 61.70s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.73095, dtype=float32), 'train/wer': 1.3853101638798075, 'validation/ctc_loss': DeviceArray(30.308647, dtype=float32), 'validation/wer': 1.3421451244102693, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.405632, dtype=float32), 'test/wer': 1.374504905246481, 'test/num_examples': 2472}
I0401 04:36:44.454170 139923241899840 submission_runner.py:396] After eval at step 1: RAM USED (GB) 19.922366464
I0401 04:36:44.469733 139743354410752 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=61.498795, test/ctc_loss=30.40563201904297, test/num_examples=2472, test/wer=1.374505, total_duration=61.696060, train/ctc_loss=31.73094940185547, train/wer=1.385310, validation/ctc_loss=30.30864715576172, validation/num_examples=5348, validation/wer=1.342145
I0401 04:36:44.835114 139923241899840 checkpoints.py:356] Saving checkpoint at step: 1
I0401 04:36:46.217056 139923241899840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_1
I0401 04:36:46.246871 139923241899840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_1.
I0401 04:36:46.254026 139923241899840 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.19500032
I0401 04:36:46.311253 139923241899840 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.192632832
I0401 04:37:04.715411 139923241899840 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.504567808
I0401 04:38:19.956593 139748671829760 logging_writer.py:48] [100] global_step=100, grad_norm=5.291144847869873, loss=6.46494722366333
I0401 04:39:35.980540 139748680222464 logging_writer.py:48] [200] global_step=200, grad_norm=0.4086938202381134, loss=5.850130558013916
I0401 04:40:52.060266 139748671829760 logging_writer.py:48] [300] global_step=300, grad_norm=0.4095890522003174, loss=5.8074774742126465
I0401 04:42:08.245736 139748680222464 logging_writer.py:48] [400] global_step=400, grad_norm=0.2913964092731476, loss=5.791212558746338
I0401 04:43:24.490407 139748671829760 logging_writer.py:48] [500] global_step=500, grad_norm=0.3822256028652191, loss=5.790679931640625
I0401 04:44:40.768850 139748680222464 logging_writer.py:48] [600] global_step=600, grad_norm=1.7398793697357178, loss=5.794302940368652
I0401 04:45:56.960282 139748671829760 logging_writer.py:48] [700] global_step=700, grad_norm=0.49428966641426086, loss=5.779903888702393
I0401 04:47:13.297074 139748680222464 logging_writer.py:48] [800] global_step=800, grad_norm=1.6368598937988281, loss=5.755760192871094
I0401 04:48:37.565074 139748671829760 logging_writer.py:48] [900] global_step=900, grad_norm=1.287186622619629, loss=5.7646918296813965
I0401 04:50:05.001822 139748680222464 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.2183362245559692, loss=5.755856037139893
I0401 04:51:27.601456 139750788978432 logging_writer.py:48] [1100] global_step=1100, grad_norm=5.131577968597412, loss=5.663473129272461
I0401 04:52:43.847705 139750780585728 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7079892754554749, loss=5.5289692878723145
I0401 04:54:00.017751 139750788978432 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6517000794410706, loss=5.3348469734191895
I0401 04:55:16.265146 139750780585728 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.934346616268158, loss=4.792206764221191
I0401 04:56:32.507984 139750788978432 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.5529485940933228, loss=4.160882472991943
I0401 04:57:48.819010 139750780585728 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0931793451309204, loss=3.7990636825561523
I0401 04:59:04.972340 139750788978432 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.4936866760253906, loss=3.531681776046753
I0401 05:00:21.099082 139750780585728 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9905003309249878, loss=3.3259830474853516
I0401 05:01:37.160756 139750788978432 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.4512288570404053, loss=3.23093843460083
I0401 05:02:53.259258 139750780585728 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.1625601053237915, loss=3.118837594985962
I0401 05:04:13.298333 139749478258432 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.2381260395050049, loss=3.025042772293091
I0401 05:05:29.310006 139749469865728 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.3626313209533691, loss=2.9125754833221436
I0401 05:06:45.435505 139749478258432 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.2188587188720703, loss=2.8907148838043213
I0401 05:08:01.426021 139749469865728 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0451719760894775, loss=2.785097599029541
I0401 05:09:17.347064 139749478258432 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9458962082862854, loss=2.7415051460266113
I0401 05:10:33.225740 139749469865728 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.989125669002533, loss=2.6693925857543945
I0401 05:11:49.088504 139749478258432 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.0266101360321045, loss=2.5857436656951904
I0401 05:13:07.695888 139749469865728 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.863664448261261, loss=2.5849201679229736
I0401 05:14:30.340027 139749478258432 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8879707455635071, loss=2.4708943367004395
I0401 05:15:51.299270 139749469865728 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9919952154159546, loss=2.456348180770874
I0401 05:16:46.751767 139923241899840 submission_runner.py:373] Before eval at step 3070: RAM USED (GB) 22.226640896
I0401 05:16:46.751998 139923241899840 spec.py:298] Evaluating on the training split.
I0401 05:17:20.683000 139923241899840 spec.py:310] Evaluating on the validation split.
I0401 05:18:00.558170 139923241899840 spec.py:326] Evaluating on the test split.
I0401 05:18:20.518600 139923241899840 submission_runner.py:382] Time since start: 2583.97s, 	Step: 3070, 	{'train/ctc_loss': DeviceArray(2.8650804, dtype=float32), 'train/wer': 0.6044159749541719, 'validation/ctc_loss': DeviceArray(3.3021538, dtype=float32), 'validation/wer': 0.6579995947862497, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.9789965, dtype=float32), 'test/wer': 0.6002681128511365, 'test/num_examples': 2472}
I0401 05:18:20.520059 139923241899840 submission_runner.py:396] After eval at step 3070: RAM USED (GB) 22.475829248
I0401 05:18:20.538108 139749549938432 logging_writer.py:48] [3070] global_step=3070, preemption_count=0, score=2454.212152, test/ctc_loss=2.978996515274048, test/num_examples=2472, test/wer=0.600268, total_duration=2583.969573, train/ctc_loss=2.8650803565979004, train/wer=0.604416, validation/ctc_loss=3.3021538257598877, validation/num_examples=5348, validation/wer=0.658000
I0401 05:18:20.878280 139923241899840 checkpoints.py:356] Saving checkpoint at step: 3070
I0401 05:18:22.311631 139923241899840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_3070
I0401 05:18:22.342814 139923241899840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_3070.
I0401 05:18:22.358007 139923241899840 submission_runner.py:416] After logging and checkpointing eval at step 3070: RAM USED (GB) 22.501883904
I0401 05:18:49.752139 139748894578432 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.2314480543136597, loss=2.5268027782440186
I0401 05:20:05.569382 139748886185728 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.849865734577179, loss=2.3567705154418945
I0401 05:21:21.326114 139748894578432 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9895830750465393, loss=2.3894052505493164
I0401 05:22:37.173710 139748886185728 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9118421077728271, loss=2.335419178009033
I0401 05:23:52.938415 139748894578432 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.3894500732421875, loss=2.309156656265259
I0401 05:25:08.750328 139748886185728 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.2640219926834106, loss=2.2892658710479736
I0401 05:26:25.625420 139748894578432 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9144638776779175, loss=2.248467445373535
I0401 05:27:47.657733 139748886185728 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.784696638584137, loss=2.229541778564453
I0401 05:29:05.460978 139748894578432 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7645421028137207, loss=2.1517248153686523
I0401 05:30:23.359760 139748886185728 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8839529752731323, loss=2.1599202156066895
I0401 05:31:42.982613 139748894578432 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7962062954902649, loss=2.030001640319824
I0401 05:33:02.927041 139749549938432 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7265239953994751, loss=2.1137988567352295
I0401 05:34:18.430566 139749541545728 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8373071551322937, loss=2.098055601119995
I0401 05:35:34.113523 139749549938432 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.823498547077179, loss=2.0987367630004883
I0401 05:36:49.824746 139749541545728 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7458717226982117, loss=2.036407709121704
I0401 05:38:05.475588 139749549938432 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7731691598892212, loss=2.0061933994293213
I0401 05:39:21.056494 139749541545728 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.70465087890625, loss=1.9847891330718994
I0401 05:40:36.769573 139749549938432 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7004010677337646, loss=2.000474214553833
I0401 05:41:58.638462 139749541545728 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7710095047950745, loss=1.9733548164367676
I0401 05:43:18.369085 139749549938432 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7652022838592529, loss=1.971746802330017
I0401 05:44:36.367218 139749541545728 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8347324132919312, loss=1.9489799737930298
I0401 05:45:58.074039 139750788978432 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7748866081237793, loss=1.9039850234985352
I0401 05:47:13.744554 139750780585728 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8495060801506042, loss=1.9281349182128906
I0401 05:48:29.281488 139750788978432 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7434192299842834, loss=1.9027835130691528
I0401 05:49:44.879546 139750780585728 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6074034571647644, loss=1.7878080606460571
I0401 05:51:00.330613 139750788978432 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6456423997879028, loss=1.890986442565918
I0401 05:52:15.825614 139750780585728 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6746152639389038, loss=1.8465551137924194
I0401 05:53:31.318382 139750788978432 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8163435459136963, loss=1.8197674751281738
I0401 05:54:46.636775 139750780585728 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6555907130241394, loss=1.7985831499099731
I0401 05:56:07.157279 139750788978432 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7257751822471619, loss=1.8741943836212158
I0401 05:57:26.785977 139750780585728 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5536249279975891, loss=1.8403135538101196
I0401 05:58:22.898216 139923241899840 submission_runner.py:373] Before eval at step 6172: RAM USED (GB) 23.21401856
I0401 05:58:22.898417 139923241899840 spec.py:298] Evaluating on the training split.
I0401 05:58:58.780010 139923241899840 spec.py:310] Evaluating on the validation split.
I0401 05:59:37.249172 139923241899840 spec.py:326] Evaluating on the test split.
I0401 05:59:57.844474 139923241899840 submission_runner.py:382] Time since start: 5080.11s, 	Step: 6172, 	{'train/ctc_loss': DeviceArray(0.56129974, dtype=float32), 'train/wer': 0.19616972538156247, 'validation/ctc_loss': DeviceArray(0.87899894, dtype=float32), 'validation/wer': 0.26434408436164364, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.60127467, dtype=float32), 'test/wer': 0.2013080657282717, 'test/num_examples': 2472}
I0401 05:59:57.845832 139923241899840 submission_runner.py:396] After eval at step 6172: RAM USED (GB) 22.44528128
I0401 05:59:57.863770 139750788978432 logging_writer.py:48] [6172] global_step=6172, preemption_count=0, score=4847.045279, test/ctc_loss=0.6012746691703796, test/num_examples=2472, test/wer=0.201308, total_duration=5080.112994, train/ctc_loss=0.561299741268158, train/wer=0.196170, validation/ctc_loss=0.8789989352226257, validation/num_examples=5348, validation/wer=0.264344
I0401 05:59:58.187936 139923241899840 checkpoints.py:356] Saving checkpoint at step: 6172
I0401 05:59:59.603183 139923241899840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_6172
I0401 05:59:59.635366 139923241899840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_6172.
I0401 05:59:59.649332 139923241899840 submission_runner.py:416] After logging and checkpointing eval at step 6172: RAM USED (GB) 22.46649856
I0401 06:00:25.547965 139750788978432 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5545464754104614, loss=1.8296215534210205
I0401 06:01:40.882050 139750780585728 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5601702928543091, loss=1.7741236686706543
I0401 06:02:56.201303 139750788978432 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6005033254623413, loss=1.7961894273757935
I0401 06:04:11.520148 139750780585728 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5519118905067444, loss=1.7750955820083618
I0401 06:05:26.932320 139750788978432 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7949641942977905, loss=1.847345232963562
I0401 06:06:42.468439 139750780585728 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5500411987304688, loss=1.740189790725708
I0401 06:07:57.827849 139750788978432 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5895139575004578, loss=1.7543294429779053
I0401 06:09:13.187213 139750780585728 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5256609916687012, loss=1.6985299587249756
I0401 06:10:30.893800 139750788978432 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5501955151557922, loss=1.6952039003372192
I0401 06:11:52.186280 139750780585728 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6148003339767456, loss=1.7368125915527344
I0401 06:13:12.228237 139750788978432 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5747264623641968, loss=1.7137172222137451
I0401 06:14:31.959992 139750461298432 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5196930170059204, loss=1.674743413925171
I0401 06:15:47.242553 139750452905728 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5713767409324646, loss=1.686069369316101
I0401 06:17:02.582548 139750461298432 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.48155155777931213, loss=1.735684871673584
I0401 06:18:17.963986 139750452905728 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5646092891693115, loss=1.6613715887069702
I0401 06:19:33.366931 139750461298432 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4847685992717743, loss=1.7102606296539307
I0401 06:20:49.469189 139750452905728 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5405097007751465, loss=1.678568959236145
I0401 06:22:09.775945 139750461298432 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5455225110054016, loss=1.7335196733474731
I0401 06:23:30.104375 139750452905728 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6462199687957764, loss=1.704167127609253
I0401 06:24:48.455754 139750461298432 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.474977046251297, loss=1.6587337255477905
I0401 06:26:07.577174 139750452905728 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5448033213615417, loss=1.705051064491272
I0401 06:27:29.098420 139750461298432 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.797060489654541, loss=1.692442774772644
I0401 06:28:44.338939 139750452905728 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5289931893348694, loss=1.6794328689575195
I0401 06:29:59.606950 139750461298432 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6191206574440002, loss=1.6553332805633545
I0401 06:31:14.897179 139750452905728 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5174785852432251, loss=1.6493197679519653
I0401 06:32:30.189949 139750461298432 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5766340494155884, loss=1.5752848386764526
I0401 06:33:45.479088 139750452905728 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.4879510998725891, loss=1.6055563688278198
I0401 06:35:00.770082 139750461298432 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.434307336807251, loss=1.616997241973877
I0401 06:36:19.761890 139750452905728 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.49720630049705505, loss=1.5706775188446045
I0401 06:37:39.546309 139750461298432 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.4352370798587799, loss=1.5921193361282349
I0401 06:38:58.778653 139750452905728 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5276215076446533, loss=1.6224346160888672
I0401 06:40:00.087960 139923241899840 submission_runner.py:373] Before eval at step 9272: RAM USED (GB) 23.0114304
I0401 06:40:00.088484 139923241899840 spec.py:298] Evaluating on the training split.
I0401 06:40:36.848533 139923241899840 spec.py:310] Evaluating on the validation split.
I0401 06:41:16.287391 139923241899840 spec.py:326] Evaluating on the test split.
I0401 06:41:35.910841 139923241899840 submission_runner.py:382] Time since start: 7577.30s, 	Step: 9272, 	{'train/ctc_loss': DeviceArray(0.36697546, dtype=float32), 'train/wer': 0.1359488906810886, 'validation/ctc_loss': DeviceArray(0.69848716, dtype=float32), 'validation/wer': 0.21119354745342453, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44714653, dtype=float32), 'test/wer': 0.1498791461011923, 'test/num_examples': 2472}
I0401 06:41:35.912216 139923241899840 submission_runner.py:396] After eval at step 9272: RAM USED (GB) 22.180315136
I0401 06:41:35.932610 139749662578432 logging_writer.py:48] [9272] global_step=9272, preemption_count=0, score=7239.861589, test/ctc_loss=0.44714653491973877, test/num_examples=2472, test/wer=0.149879, total_duration=7577.304649, train/ctc_loss=0.36697545647621155, train/wer=0.135949, validation/ctc_loss=0.6984871625900269, validation/num_examples=5348, validation/wer=0.211194
I0401 06:41:36.262919 139923241899840 checkpoints.py:356] Saving checkpoint at step: 9272
I0401 06:41:37.671051 139923241899840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_9272
I0401 06:41:37.702813 139923241899840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_9272.
I0401 06:41:37.717481 139923241899840 submission_runner.py:416] After logging and checkpointing eval at step 9272: RAM USED (GB) 22.209040384
I0401 06:41:59.541025 139749654185728 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.4548076093196869, loss=1.580660343170166
I0401 06:43:14.755412 139748612396800 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.43881621956825256, loss=1.5006399154663086
I0401 06:44:29.973875 139749654185728 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.4164676368236542, loss=1.5450584888458252
I0401 06:45:45.255508 139748612396800 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3888932466506958, loss=1.551994800567627
I0401 06:47:00.394324 139749654185728 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.44741323590278625, loss=1.5630712509155273
I0401 06:48:15.819619 139748612396800 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5327982306480408, loss=1.5886077880859375
I0401 06:49:31.103143 139749654185728 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.44735708832740784, loss=1.5236326456069946
I0401 06:50:45.236872 139923241899840 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 22.99447296
I0401 06:50:45.237059 139923241899840 spec.py:298] Evaluating on the training split.
I0401 06:51:21.921720 139923241899840 spec.py:310] Evaluating on the validation split.
I0401 06:52:00.383313 139923241899840 spec.py:326] Evaluating on the test split.
I0401 06:52:20.419406 139923241899840 submission_runner.py:382] Time since start: 8222.46s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(0.36558476, dtype=float32), 'train/wer': 0.1318633559886479, 'validation/ctc_loss': DeviceArray(0.67235726, dtype=float32), 'validation/wer': 0.20440139316346514, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.41991657, dtype=float32), 'test/wer': 0.14262791217272966, 'test/num_examples': 2472}
I0401 06:52:20.420851 139923241899840 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 23.094132736
I0401 06:52:20.436442 139750031218432 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7785.608404, test/ctc_loss=0.419916570186615, test/num_examples=2472, test/wer=0.142628, total_duration=8222.455681, train/ctc_loss=0.36558476090431213, train/wer=0.131863, validation/ctc_loss=0.6723572611808777, validation/num_examples=5348, validation/wer=0.204401
I0401 06:52:20.761838 139923241899840 checkpoints.py:356] Saving checkpoint at step: 10000
I0401 06:52:22.141123 139923241899840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_10000
I0401 06:52:22.173892 139923241899840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0401 06:52:22.190142 139923241899840 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 23.112716288
I0401 06:52:22.197418 139750022825728 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7785.608404
I0401 06:52:22.421043 139923241899840 checkpoints.py:356] Saving checkpoint at step: 10000
I0401 06:52:24.259309 139923241899840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_10000
I0401 06:52:24.293502 139923241899840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0401 06:52:25.676953 139923241899840 submission_runner.py:550] Tuning trial 1/1
I0401 06:52:25.677202 139923241899840 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0401 06:52:25.682414 139923241899840 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.73095, dtype=float32), 'train/wer': 1.3853101638798075, 'validation/ctc_loss': DeviceArray(30.308647, dtype=float32), 'validation/wer': 1.3421451244102693, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.405632, dtype=float32), 'test/wer': 1.374504905246481, 'test/num_examples': 2472, 'score': 61.49879455566406, 'total_duration': 61.69606041908264, 'global_step': 1, 'preemption_count': 0}), (3070, {'train/ctc_loss': DeviceArray(2.8650804, dtype=float32), 'train/wer': 0.6044159749541719, 'validation/ctc_loss': DeviceArray(3.3021538, dtype=float32), 'validation/wer': 0.6579995947862497, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.9789965, dtype=float32), 'test/wer': 0.6002681128511365, 'test/num_examples': 2472, 'score': 2454.212151527405, 'total_duration': 2583.969573497772, 'global_step': 3070, 'preemption_count': 0}), (6172, {'train/ctc_loss': DeviceArray(0.56129974, dtype=float32), 'train/wer': 0.19616972538156247, 'validation/ctc_loss': DeviceArray(0.87899894, dtype=float32), 'validation/wer': 0.26434408436164364, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.60127467, dtype=float32), 'test/wer': 0.2013080657282717, 'test/num_examples': 2472, 'score': 4847.045278549194, 'total_duration': 5080.112993717194, 'global_step': 6172, 'preemption_count': 0}), (9272, {'train/ctc_loss': DeviceArray(0.36697546, dtype=float32), 'train/wer': 0.1359488906810886, 'validation/ctc_loss': DeviceArray(0.69848716, dtype=float32), 'validation/wer': 0.21119354745342453, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44714653, dtype=float32), 'test/wer': 0.1498791461011923, 'test/num_examples': 2472, 'score': 7239.861588716507, 'total_duration': 7577.304648637772, 'global_step': 9272, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(0.36558476, dtype=float32), 'train/wer': 0.1318633559886479, 'validation/ctc_loss': DeviceArray(0.67235726, dtype=float32), 'validation/wer': 0.20440139316346514, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.41991657, dtype=float32), 'test/wer': 0.14262791217272966, 'test/num_examples': 2472, 'score': 7785.608403921127, 'total_duration': 8222.45568060875, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0401 06:52:25.682608 139923241899840 submission_runner.py:553] Timing: 7785.608403921127
I0401 06:52:25.682656 139923241899840 submission_runner.py:554] ====================
I0401 06:52:25.683013 139923241899840 submission_runner.py:613] Final librispeech_conformer score: 7785.608403921127
