I0329 21:52:11.865673 140100805351232 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/ogbg_jax.
I0329 21:52:11.912450 140100805351232 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0329 21:52:12.730736 140100805351232 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0329 21:52:12.732116 140100805351232 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0329 21:52:12.737091 140100805351232 submission_runner.py:504] Using RNG seed 4058287287
I0329 21:52:13.994985 140100805351232 submission_runner.py:513] --- Tuning run 1/1 ---
I0329 21:52:13.995292 140100805351232 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/ogbg_jax/trial_1.
I0329 21:52:13.995548 140100805351232 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/ogbg_jax/trial_1/hparams.json.
I0329 21:52:14.129632 140100805351232 submission_runner.py:230] Starting train once: RAM USED (GB) 4.5097984
I0329 21:52:14.129830 140100805351232 submission_runner.py:231] Initializing dataset.
I0329 21:52:14.378327 140100805351232 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0329 21:52:14.383028 140100805351232 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0329 21:52:14.561753 140100805351232 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0329 21:52:14.596144 140100805351232 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.58350592
I0329 21:52:14.596323 140100805351232 submission_runner.py:240] Initializing model.
I0329 21:52:22.095283 140100805351232 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.402878464
I0329 21:52:22.095488 140100805351232 submission_runner.py:252] Initializing optimizer.
I0329 21:52:22.461455 140100805351232 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.403386368
I0329 21:52:22.461631 140100805351232 submission_runner.py:261] Initializing metrics bundle.
I0329 21:52:22.461682 140100805351232 submission_runner.py:275] Initializing checkpoint and logger.
I0329 21:52:22.462604 140100805351232 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum/ogbg_jax/trial_1 with prefix checkpoint_
I0329 21:52:22.462827 140100805351232 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0329 21:52:22.462891 140100805351232 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0329 21:52:23.284625 140100805351232 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/ogbg_jax/trial_1/meta_data_0.json.
I0329 21:52:23.285517 140100805351232 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/ogbg_jax/trial_1/flags_0.json.
I0329 21:52:23.288208 140100805351232 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 8.405528576
I0329 21:52:23.288413 140100805351232 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.405528576
I0329 21:52:23.288478 140100805351232 submission_runner.py:312] Starting training loop.
I0329 21:52:25.016951 140100805351232 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 8.567107584
I0329 21:52:42.559168 139924623845120 logging_writer.py:48] [0] global_step=0, grad_norm=2.5758957862854004, loss=0.6989443898200989
I0329 21:52:42.567282 140100805351232 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 11.049484288
I0329 21:52:42.567515 140100805351232 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 11.049484288
I0329 21:52:42.567595 140100805351232 spec.py:298] Evaluating on the training split.
I0329 21:52:42.575759 140100805351232 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0329 21:52:42.580744 140100805351232 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0329 21:52:42.640741 140100805351232 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0329 21:53:00.086129 140100805351232 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0329 21:54:20.990107 140100805351232 spec.py:310] Evaluating on the validation split.
I0329 21:54:20.993410 140100805351232 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0329 21:54:20.997561 140100805351232 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0329 21:54:21.054736 140100805351232 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0329 21:55:30.608942 140100805351232 spec.py:326] Evaluating on the test split.
I0329 21:55:30.611964 140100805351232 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0329 21:55:30.616135 140100805351232 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0329 21:55:30.670586 140100805351232 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0329 21:56:40.710158 140100805351232 submission_runner.py:380] Time since start: 19.28s, 	Step: 1, 	{'train/accuracy': 0.5708729028701782, 'train/loss': 0.697172999382019, 'train/mean_average_precision': 0.02376386393423731, 'validation/accuracy': 0.5639336109161377, 'validation/loss': 0.7030148506164551, 'validation/mean_average_precision': 0.026133059806144658, 'validation/num_examples': 43793, 'test/accuracy': 0.5636829137802124, 'test/loss': 0.7035066485404968, 'test/mean_average_precision': 0.02838361241347788, 'test/num_examples': 43793}
I0329 21:56:40.710608 140100805351232 submission_runner.py:390] After eval at step 1: RAM USED (GB) 12.431212544
I0329 21:56:40.718030 139915295270656 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=19.200959, test/accuracy=0.563683, test/loss=0.703507, test/mean_average_precision=0.028384, test/num_examples=43793, total_duration=19.279083, train/accuracy=0.570873, train/loss=0.697173, train/mean_average_precision=0.023764, validation/accuracy=0.563934, validation/loss=0.703015, validation/mean_average_precision=0.026133, validation/num_examples=43793
I0329 21:56:40.744293 140100805351232 checkpoints.py:356] Saving checkpoint at step: 1
I0329 21:56:40.810345 140100805351232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_1
I0329 21:56:40.810572 140100805351232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_1.
I0329 21:56:40.811197 140100805351232 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 12.430176256
I0329 21:56:41.066615 140100805351232 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 12.460527616
I0329 21:56:41.078467 140100805351232 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 12.461027328
I0329 21:57:06.764844 139915303663360 logging_writer.py:48] [100] global_step=100, grad_norm=0.06369806081056595, loss=0.08786432445049286
I0329 21:57:32.604398 139916570371840 logging_writer.py:48] [200] global_step=200, grad_norm=0.016233762726187706, loss=0.058823008090257645
I0329 21:57:58.276772 139915303663360 logging_writer.py:48] [300] global_step=300, grad_norm=0.011316919699311256, loss=0.057966966181993484
I0329 21:58:23.861650 139916570371840 logging_writer.py:48] [400] global_step=400, grad_norm=0.00865943357348442, loss=0.05648386478424072
I0329 21:58:49.554326 139915303663360 logging_writer.py:48] [500] global_step=500, grad_norm=0.011901349760591984, loss=0.06360461562871933
I0329 21:59:15.226894 139916570371840 logging_writer.py:48] [600] global_step=600, grad_norm=0.009927335195243359, loss=0.05179346725344658
I0329 21:59:40.921190 139915303663360 logging_writer.py:48] [700] global_step=700, grad_norm=0.01083639170974493, loss=0.05107026547193527
I0329 22:00:06.674221 139916570371840 logging_writer.py:48] [800] global_step=800, grad_norm=0.011867314577102661, loss=0.052665114402770996
I0329 22:00:32.383723 139915303663360 logging_writer.py:48] [900] global_step=900, grad_norm=0.026374582201242447, loss=0.062388718128204346
I0329 22:00:40.938114 140100805351232 submission_runner.py:371] Before eval at step 934: RAM USED (GB) 13.131272192
I0329 22:00:40.938324 140100805351232 spec.py:298] Evaluating on the training split.
I0329 22:02:01.119711 140100805351232 spec.py:310] Evaluating on the validation split.
I0329 22:02:03.804251 140100805351232 spec.py:326] Evaluating on the test split.
I0329 22:02:06.487049 140100805351232 submission_runner.py:380] Time since start: 497.65s, 	Step: 934, 	{'train/accuracy': 0.986667811870575, 'train/loss': 0.055386703461408615, 'train/mean_average_precision': 0.03211609938618122, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06530918926000595, 'validation/mean_average_precision': 0.035167372694086596, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06869218498468399, 'test/mean_average_precision': 0.03554562067874888, 'test/num_examples': 43793}
I0329 22:02:06.487518 140100805351232 submission_runner.py:390] After eval at step 934: RAM USED (GB) 13.614522368
I0329 22:02:06.495951 139916570371840 logging_writer.py:48] [934] global_step=934, preemption_count=0, score=258.429694, test/accuracy=0.983142, test/loss=0.068692, test/mean_average_precision=0.035546, test/num_examples=43793, total_duration=497.649321, train/accuracy=0.986668, train/loss=0.055387, train/mean_average_precision=0.032116, validation/accuracy=0.984118, validation/loss=0.065309, validation/mean_average_precision=0.035167, validation/num_examples=43793
I0329 22:02:06.523082 140100805351232 checkpoints.py:356] Saving checkpoint at step: 934
I0329 22:02:06.595125 140100805351232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_934
I0329 22:02:06.595340 140100805351232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_934.
I0329 22:02:06.595927 140100805351232 submission_runner.py:409] After logging and checkpointing eval at step 934: RAM USED (GB) 13.615546368
I0329 22:02:24.024637 139915303663360 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.03567269816994667, loss=0.05731045827269554
I0329 22:02:49.825633 139916403009280 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.028471946716308594, loss=0.05284116789698601
I0329 22:03:15.488363 139915303663360 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.02819298394024372, loss=0.05516846850514412
I0329 22:03:41.318300 139916403009280 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.06996966898441315, loss=0.05166096240282059
I0329 22:04:07.201812 139915303663360 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.09087137877941132, loss=0.04601820930838585
I0329 22:04:32.899904 139916403009280 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.04728713631629944, loss=0.052527546882629395
I0329 22:04:58.940013 139915303663360 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0351107157766819, loss=0.05627574771642685
I0329 22:05:25.110978 139916403009280 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.04509231820702553, loss=0.05473501980304718
I0329 22:05:50.956368 139915303663360 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.04135080426931381, loss=0.055833600461483
I0329 22:06:06.716491 140100805351232 submission_runner.py:371] Before eval at step 1861: RAM USED (GB) 14.094905344
I0329 22:06:06.716693 140100805351232 spec.py:298] Evaluating on the training split.
I0329 22:07:27.605850 140100805351232 spec.py:310] Evaluating on the validation split.
I0329 22:07:30.259220 140100805351232 spec.py:326] Evaluating on the test split.
I0329 22:07:32.850529 140100805351232 submission_runner.py:380] Time since start: 823.43s, 	Step: 1861, 	{'train/accuracy': 0.9868665933609009, 'train/loss': 0.05275765806436539, 'train/mean_average_precision': 0.046070582905547344, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06271835416555405, 'validation/mean_average_precision': 0.04686281191833037, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06612920016050339, 'test/mean_average_precision': 0.04749624598853497, 'test/num_examples': 43793}
I0329 22:07:32.850965 140100805351232 submission_runner.py:390] After eval at step 1861: RAM USED (GB) 14.4367616
I0329 22:07:32.859020 139916403009280 logging_writer.py:48] [1861] global_step=1861, preemption_count=0, score=497.611285, test/accuracy=0.983142, test/loss=0.066129, test/mean_average_precision=0.047496, test/num_examples=43793, total_duration=823.427594, train/accuracy=0.986867, train/loss=0.052758, train/mean_average_precision=0.046071, validation/accuracy=0.984118, validation/loss=0.062718, validation/mean_average_precision=0.046863, validation/num_examples=43793
I0329 22:07:32.885814 140100805351232 checkpoints.py:356] Saving checkpoint at step: 1861
I0329 22:07:32.944643 140100805351232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_1861
I0329 22:07:32.944872 140100805351232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_1861.
I0329 22:07:32.945427 140100805351232 submission_runner.py:409] After logging and checkpointing eval at step 1861: RAM USED (GB) 14.433849344
I0329 22:07:43.316861 139915303663360 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.05906681343913078, loss=0.05385708436369896
I0329 22:08:09.122924 139916386223872 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.08178940415382385, loss=0.05099290981888771
I0329 22:08:34.682492 139915303663360 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.1366843432188034, loss=0.050657037645578384
I0329 22:09:00.058469 139916386223872 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.07358274608850479, loss=0.0531328022480011
I0329 22:09:25.417985 139915303663360 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.04586562141776085, loss=0.057760775089263916
I0329 22:09:50.856439 139916386223872 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.07346487045288086, loss=0.050375714898109436
I0329 22:10:16.398962 139915303663360 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.025670068338513374, loss=0.047734733670949936
I0329 22:10:42.095254 139916386223872 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.02763116918504238, loss=0.05333400517702103
I0329 22:11:07.716237 139915303663360 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.073157899081707, loss=0.05276407673954964
I0329 22:11:33.087812 140100805351232 submission_runner.py:371] Before eval at step 2800: RAM USED (GB) 14.836965376
I0329 22:11:33.088004 140100805351232 spec.py:298] Evaluating on the training split.
I0329 22:12:52.348690 140100805351232 spec.py:310] Evaluating on the validation split.
I0329 22:12:55.001264 140100805351232 spec.py:326] Evaluating on the test split.
I0329 22:12:57.574128 140100805351232 submission_runner.py:380] Time since start: 1149.80s, 	Step: 2800, 	{'train/accuracy': 0.9869030117988586, 'train/loss': 0.05022964999079704, 'train/mean_average_precision': 0.07334029759400794, 'validation/accuracy': 0.9841597676277161, 'validation/loss': 0.05943762883543968, 'validation/mean_average_precision': 0.06602029501756106, 'validation/num_examples': 43793, 'test/accuracy': 0.9831589460372925, 'test/loss': 0.062453173100948334, 'test/mean_average_precision': 0.06836353965757284, 'test/num_examples': 43793}
I0329 22:12:57.574620 140100805351232 submission_runner.py:390] After eval at step 2800: RAM USED (GB) 15.115870208
I0329 22:12:57.582717 139916386223872 logging_writer.py:48] [2800] global_step=2800, preemption_count=0, score=736.831954, test/accuracy=0.983159, test/loss=0.062453, test/mean_average_precision=0.068364, test/num_examples=43793, total_duration=1149.799046, train/accuracy=0.986903, train/loss=0.050230, train/mean_average_precision=0.073340, validation/accuracy=0.984160, validation/loss=0.059438, validation/mean_average_precision=0.066020, validation/num_examples=43793
I0329 22:12:57.608417 140100805351232 checkpoints.py:356] Saving checkpoint at step: 2800
I0329 22:12:57.665614 140100805351232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_2800
I0329 22:12:57.665799 140100805351232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_2800.
I0329 22:12:57.666354 140100805351232 submission_runner.py:409] After logging and checkpointing eval at step 2800: RAM USED (GB) 15.116894208
I0329 22:12:57.929068 139915303663360 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.11262574791908264, loss=0.047459233552217484
I0329 22:13:23.257028 139916377831168 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.03336981311440468, loss=0.0486045777797699
I0329 22:13:48.370879 139915303663360 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.04475504159927368, loss=0.05313543975353241
I0329 22:14:13.340632 139916377831168 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.03881550207734108, loss=0.04894549027085304
I0329 22:14:38.508122 139915303663360 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.07726603746414185, loss=0.05001217871904373
I0329 22:15:03.570656 139916377831168 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.037616945803165436, loss=0.04849555343389511
I0329 22:15:28.795534 139915303663360 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.04337019845843315, loss=0.05002277344465256
I0329 22:15:53.999247 139916377831168 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.04283735901117325, loss=0.046663228422403336
I0329 22:16:19.091821 139915303663360 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.02760506048798561, loss=0.04703856259584427
I0329 22:16:44.326697 139916377831168 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.05313079059123993, loss=0.04879571497440338
I0329 22:16:57.737059 140100805351232 submission_runner.py:371] Before eval at step 3754: RAM USED (GB) 15.325761536
I0329 22:16:57.737243 140100805351232 spec.py:298] Evaluating on the training split.
I0329 22:18:15.792851 140100805351232 spec.py:310] Evaluating on the validation split.
I0329 22:18:18.395845 140100805351232 spec.py:326] Evaluating on the test split.
I0329 22:18:20.968913 140100805351232 submission_runner.py:380] Time since start: 1474.45s, 	Step: 3754, 	{'train/accuracy': 0.9868566989898682, 'train/loss': 0.049105267971754074, 'train/mean_average_precision': 0.09763867470227229, 'validation/accuracy': 0.9842239022254944, 'validation/loss': 0.05929805710911751, 'validation/mean_average_precision': 0.09428788854751854, 'validation/num_examples': 43793, 'test/accuracy': 0.9832313656806946, 'test/loss': 0.062439337372779846, 'test/mean_average_precision': 0.09587095417925677, 'test/num_examples': 43793}
I0329 22:18:20.969354 140100805351232 submission_runner.py:390] After eval at step 3754: RAM USED (GB) 15.66040064
I0329 22:18:20.977885 139915303663360 logging_writer.py:48] [3754] global_step=3754, preemption_count=0, score=975.997093, test/accuracy=0.983231, test/loss=0.062439, test/mean_average_precision=0.095871, test/num_examples=43793, total_duration=1474.448253, train/accuracy=0.986857, train/loss=0.049105, train/mean_average_precision=0.097639, validation/accuracy=0.984224, validation/loss=0.059298, validation/mean_average_precision=0.094288, validation/num_examples=43793
I0329 22:18:21.004723 140100805351232 checkpoints.py:356] Saving checkpoint at step: 3754
I0329 22:18:21.063400 140100805351232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_3754
I0329 22:18:21.063611 140100805351232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_3754.
I0329 22:18:21.064296 140100805351232 submission_runner.py:409] After logging and checkpointing eval at step 3754: RAM USED (GB) 15.657480192
I0329 22:18:32.817018 139916377831168 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.04685636982321739, loss=0.05329621583223343
I0329 22:18:57.902264 139916369438464 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.07871861755847931, loss=0.05191244184970856
I0329 22:19:22.989032 139916377831168 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.028208164498209953, loss=0.04721043258905411
I0329 22:19:48.161639 139916369438464 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.07245081663131714, loss=0.05341920629143715
I0329 22:20:13.307857 139916377831168 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.07099900394678116, loss=0.04882834106683731
I0329 22:20:38.302198 139916369438464 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.05477318912744522, loss=0.04660020023584366
I0329 22:21:03.532562 139916377831168 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.037476103752851486, loss=0.05442335829138756
I0329 22:21:28.601235 139916369438464 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0642927885055542, loss=0.047582898288965225
I0329 22:21:53.703396 139916377831168 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.11828674376010895, loss=0.04872532933950424
I0329 22:22:19.072474 139916369438464 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.06548405438661575, loss=0.047405872493982315
I0329 22:22:21.094455 140100805351232 submission_runner.py:371] Before eval at step 4709: RAM USED (GB) 15.765266432
I0329 22:22:21.094636 140100805351232 spec.py:298] Evaluating on the training split.
I0329 22:23:41.198344 140100805351232 spec.py:310] Evaluating on the validation split.
I0329 22:23:43.867547 140100805351232 spec.py:326] Evaluating on the test split.
I0329 22:23:46.446524 140100805351232 submission_runner.py:380] Time since start: 1797.81s, 	Step: 4709, 	{'train/accuracy': 0.9873520731925964, 'train/loss': 0.046329520642757416, 'train/mean_average_precision': 0.12003758293791747, 'validation/accuracy': 0.9846188426017761, 'validation/loss': 0.05541257932782173, 'validation/mean_average_precision': 0.11600723525971822, 'validation/num_examples': 43793, 'test/accuracy': 0.9836386442184448, 'test/loss': 0.05833474546670914, 'test/mean_average_precision': 0.11754884820571243, 'test/num_examples': 43793}
I0329 22:23:46.446986 140100805351232 submission_runner.py:390] After eval at step 4709: RAM USED (GB) 16.140312576
I0329 22:23:46.455238 139916377831168 logging_writer.py:48] [4709] global_step=4709, preemption_count=0, score=1215.091021, test/accuracy=0.983639, test/loss=0.058335, test/mean_average_precision=0.117549, test/num_examples=43793, total_duration=1797.805622, train/accuracy=0.987352, train/loss=0.046330, train/mean_average_precision=0.120038, validation/accuracy=0.984619, validation/loss=0.055413, validation/mean_average_precision=0.116007, validation/num_examples=43793
I0329 22:23:46.481530 140100805351232 checkpoints.py:356] Saving checkpoint at step: 4709
I0329 22:23:46.539786 140100805351232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_4709
I0329 22:23:46.540024 140100805351232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_4709.
I0329 22:23:46.540683 140100805351232 submission_runner.py:409] After logging and checkpointing eval at step 4709: RAM USED (GB) 16.139497472
I0329 22:24:10.070740 139916369438464 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.08498526364564896, loss=0.04806134104728699
I0329 22:24:35.743518 139916352653056 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.04509514197707176, loss=0.04896662384271622
I0329 22:25:01.271629 139916369438464 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.05442855879664421, loss=0.05043557658791542
I0329 22:25:27.147619 139916352653056 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.04385226592421532, loss=0.04628082364797592
I0329 22:25:53.156890 139916369438464 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.03890729323029518, loss=0.045880138874053955
I0329 22:26:19.141472 139916352653056 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.05696552246809006, loss=0.04411955922842026
I0329 22:26:45.061483 139916369438464 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.03545170649886131, loss=0.047224849462509155
I0329 22:27:10.728359 139916352653056 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.06082220748066902, loss=0.0436934158205986
I0329 22:27:35.966706 139916369438464 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.052801597863435745, loss=0.0448489785194397
I0329 22:27:46.568792 140100805351232 submission_runner.py:371] Before eval at step 5643: RAM USED (GB) 16.265969664
I0329 22:27:46.568975 140100805351232 spec.py:298] Evaluating on the training split.
I0329 22:29:05.802252 140100805351232 spec.py:310] Evaluating on the validation split.
I0329 22:29:08.425797 140100805351232 spec.py:326] Evaluating on the test split.
I0329 22:29:11.005827 140100805351232 submission_runner.py:380] Time since start: 2123.28s, 	Step: 5643, 	{'train/accuracy': 0.9873390197753906, 'train/loss': 0.04504424333572388, 'train/mean_average_precision': 0.1376726378200428, 'validation/accuracy': 0.9847236275672913, 'validation/loss': 0.05372678115963936, 'validation/mean_average_precision': 0.1333320028280179, 'validation/num_examples': 43793, 'test/accuracy': 0.983758270740509, 'test/loss': 0.056615591049194336, 'test/mean_average_precision': 0.1328887147661132, 'test/num_examples': 43793}
I0329 22:29:11.006283 140100805351232 submission_runner.py:390] After eval at step 5643: RAM USED (GB) 16.602169344
I0329 22:29:11.014579 139916352653056 logging_writer.py:48] [5643] global_step=5643, preemption_count=0, score=1454.227281, test/accuracy=0.983758, test/loss=0.056616, test/mean_average_precision=0.132889, test/num_examples=43793, total_duration=2123.279960, train/accuracy=0.987339, train/loss=0.045044, train/mean_average_precision=0.137673, validation/accuracy=0.984724, validation/loss=0.053727, validation/mean_average_precision=0.133332, validation/num_examples=43793
I0329 22:29:11.041320 140100805351232 checkpoints.py:356] Saving checkpoint at step: 5643
I0329 22:29:11.097255 140100805351232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_5643
I0329 22:29:11.097450 140100805351232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_5643.
I0329 22:29:11.098017 140100805351232 submission_runner.py:409] After logging and checkpointing eval at step 5643: RAM USED (GB) 16.60135424
I0329 22:29:25.719801 139916369438464 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.04546069726347923, loss=0.048215512186288834
I0329 22:29:50.876464 139916344260352 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.13714677095413208, loss=0.052919335663318634
I0329 22:30:16.151762 139916369438464 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.029844369739294052, loss=0.04275764152407646
I0329 22:30:41.155000 140100805351232 submission_runner.py:371] Before eval at step 6000: RAM USED (GB) 16.649859072
I0329 22:30:41.155226 140100805351232 spec.py:298] Evaluating on the training split.
I0329 22:32:00.589397 140100805351232 spec.py:310] Evaluating on the validation split.
I0329 22:32:03.245855 140100805351232 spec.py:326] Evaluating on the test split.
I0329 22:32:05.776595 140100805351232 submission_runner.py:380] Time since start: 2297.87s, 	Step: 6000, 	{'train/accuracy': 0.9872749447822571, 'train/loss': 0.045318834483623505, 'train/mean_average_precision': 0.14054111987970969, 'validation/accuracy': 0.9847199320793152, 'validation/loss': 0.0547560378909111, 'validation/mean_average_precision': 0.1336090161497026, 'validation/num_examples': 43793, 'test/accuracy': 0.9837355613708496, 'test/loss': 0.05784699320793152, 'test/mean_average_precision': 0.13513625220055575, 'test/num_examples': 43793}
I0329 22:32:05.777060 140100805351232 submission_runner.py:390] After eval at step 6000: RAM USED (GB) 16.99018752
I0329 22:32:05.785250 139916344260352 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1543.942426, test/accuracy=0.983736, test/loss=0.057847, test/mean_average_precision=0.135136, test/num_examples=43793, total_duration=2297.866245, train/accuracy=0.987275, train/loss=0.045319, train/mean_average_precision=0.140541, validation/accuracy=0.984720, validation/loss=0.054756, validation/mean_average_precision=0.133609, validation/num_examples=43793
I0329 22:32:05.811355 140100805351232 checkpoints.py:356] Saving checkpoint at step: 6000
I0329 22:32:05.876744 140100805351232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_6000
I0329 22:32:05.876943 140100805351232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_6000.
I0329 22:32:05.877509 140100805351232 submission_runner.py:409] After logging and checkpointing eval at step 6000: RAM USED (GB) 16.992403456
I0329 22:32:05.884289 139916369438464 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1543.942426
I0329 22:32:05.904153 140100805351232 checkpoints.py:356] Saving checkpoint at step: 6000
I0329 22:32:06.008236 140100805351232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_6000
I0329 22:32:06.008464 140100805351232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/ogbg_jax/trial_1/checkpoint_6000.
I0329 22:32:06.154132 140100805351232 submission_runner.py:543] Tuning trial 1/1
I0329 22:32:06.154367 140100805351232 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0329 22:32:06.155604 140100805351232 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5708729028701782, 'train/loss': 0.697172999382019, 'train/mean_average_precision': 0.02376386393423731, 'validation/accuracy': 0.5639336109161377, 'validation/loss': 0.7030148506164551, 'validation/mean_average_precision': 0.026133059806144658, 'validation/num_examples': 43793, 'test/accuracy': 0.5636829137802124, 'test/loss': 0.7035066485404968, 'test/mean_average_precision': 0.02838361241347788, 'test/num_examples': 43793, 'score': 19.20095944404602, 'total_duration': 19.279083251953125, 'global_step': 1, 'preemption_count': 0}), (934, {'train/accuracy': 0.986667811870575, 'train/loss': 0.055386703461408615, 'train/mean_average_precision': 0.03211609938618122, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06530918926000595, 'validation/mean_average_precision': 0.035167372694086596, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06869218498468399, 'test/mean_average_precision': 0.03554562067874888, 'test/num_examples': 43793, 'score': 258.42969393730164, 'total_duration': 497.64932131767273, 'global_step': 934, 'preemption_count': 0}), (1861, {'train/accuracy': 0.9868665933609009, 'train/loss': 0.05275765806436539, 'train/mean_average_precision': 0.046070582905547344, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06271835416555405, 'validation/mean_average_precision': 0.04686281191833037, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06612920016050339, 'test/mean_average_precision': 0.04749624598853497, 'test/num_examples': 43793, 'score': 497.61128544807434, 'total_duration': 823.4275944232941, 'global_step': 1861, 'preemption_count': 0}), (2800, {'train/accuracy': 0.9869030117988586, 'train/loss': 0.05022964999079704, 'train/mean_average_precision': 0.07334029759400794, 'validation/accuracy': 0.9841597676277161, 'validation/loss': 0.05943762883543968, 'validation/mean_average_precision': 0.06602029501756106, 'validation/num_examples': 43793, 'test/accuracy': 0.9831589460372925, 'test/loss': 0.062453173100948334, 'test/mean_average_precision': 0.06836353965757284, 'test/num_examples': 43793, 'score': 736.8319544792175, 'total_duration': 1149.7990455627441, 'global_step': 2800, 'preemption_count': 0}), (3754, {'train/accuracy': 0.9868566989898682, 'train/loss': 0.049105267971754074, 'train/mean_average_precision': 0.09763867470227229, 'validation/accuracy': 0.9842239022254944, 'validation/loss': 0.05929805710911751, 'validation/mean_average_precision': 0.09428788854751854, 'validation/num_examples': 43793, 'test/accuracy': 0.9832313656806946, 'test/loss': 0.062439337372779846, 'test/mean_average_precision': 0.09587095417925677, 'test/num_examples': 43793, 'score': 975.997092962265, 'total_duration': 1474.4482526779175, 'global_step': 3754, 'preemption_count': 0}), (4709, {'train/accuracy': 0.9873520731925964, 'train/loss': 0.046329520642757416, 'train/mean_average_precision': 0.12003758293791747, 'validation/accuracy': 0.9846188426017761, 'validation/loss': 0.05541257932782173, 'validation/mean_average_precision': 0.11600723525971822, 'validation/num_examples': 43793, 'test/accuracy': 0.9836386442184448, 'test/loss': 0.05833474546670914, 'test/mean_average_precision': 0.11754884820571243, 'test/num_examples': 43793, 'score': 1215.091020822525, 'total_duration': 1797.8056223392487, 'global_step': 4709, 'preemption_count': 0}), (5643, {'train/accuracy': 0.9873390197753906, 'train/loss': 0.04504424333572388, 'train/mean_average_precision': 0.1376726378200428, 'validation/accuracy': 0.9847236275672913, 'validation/loss': 0.05372678115963936, 'validation/mean_average_precision': 0.1333320028280179, 'validation/num_examples': 43793, 'test/accuracy': 0.983758270740509, 'test/loss': 0.056615591049194336, 'test/mean_average_precision': 0.1328887147661132, 'test/num_examples': 43793, 'score': 1454.2272808551788, 'total_duration': 2123.27995967865, 'global_step': 5643, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9872749447822571, 'train/loss': 0.045318834483623505, 'train/mean_average_precision': 0.14054111987970969, 'validation/accuracy': 0.9847199320793152, 'validation/loss': 0.0547560378909111, 'validation/mean_average_precision': 0.1336090161497026, 'validation/num_examples': 43793, 'test/accuracy': 0.9837355613708496, 'test/loss': 0.05784699320793152, 'test/mean_average_precision': 0.13513625220055575, 'test/num_examples': 43793, 'score': 1543.9424259662628, 'total_duration': 2297.8662452697754, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0329 22:32:06.155742 140100805351232 submission_runner.py:546] Timing: 1543.9424259662628
I0329 22:32:06.155797 140100805351232 submission_runner.py:547] ====================
I0329 22:32:06.155916 140100805351232 submission_runner.py:606] Final ogbg score: 1543.9424259662628
