WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 01:47:05.530794 140685019285312 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 01:47:05.530842 140130779113280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 01:47:05.530849 140148540061504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 01:47:05.531734 140381314582336 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 01:47:05.531772 139651216226112 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 01:47:05.531779 139663255484224 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 01:47:05.531810 139699861108544 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 01:47:05.532458 140056081393472 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 01:47:05.532768 140056081393472 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:47:05.541374 140685019285312 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:47:05.541416 140130779113280 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:47:05.541447 140148540061504 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:47:05.542326 139699861108544 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:47:05.542358 139663255484224 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:47:05.542383 140381314582336 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:47:05.542411 139651216226112 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 01:47:07.622224 139663255484224 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/imagenet_resnet_pytorch.
W0402 01:47:07.659152 140685019285312 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:47:07.659151 139651216226112 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:47:07.659372 140056081393472 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:47:07.659393 140148540061504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:47:07.660083 139699861108544 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:47:07.660281 140381314582336 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:47:07.661000 140130779113280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 01:47:07.661232 139663255484224 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 01:47:07.666260 139663255484224 submission_runner.py:511] Using RNG seed 2976109676
I0402 01:47:07.667886 139663255484224 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 01:47:07.668026 139663255484224 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1.
I0402 01:47:07.668255 139663255484224 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0402 01:47:07.669421 139663255484224 submission_runner.py:230] Starting train once: RAM USED (GB) 6.27638272
I0402 01:47:07.669541 139663255484224 submission_runner.py:231] Initializing dataset.
I0402 01:47:12.178334 139663255484224 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 8.291708928
I0402 01:47:12.178523 139663255484224 submission_runner.py:240] Initializing model.
I0402 01:47:18.605550 139663255484224 submission_runner.py:251] After Initializing model: RAM USED (GB) 18.083581952
I0402 01:47:18.605744 139663255484224 submission_runner.py:252] Initializing optimizer.
I0402 01:47:18.606922 139663255484224 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 18.083581952
I0402 01:47:18.607033 139663255484224 submission_runner.py:261] Initializing metrics bundle.
I0402 01:47:18.607082 139663255484224 submission_runner.py:276] Initializing checkpoint and logger.
I0402 01:47:19.380862 139663255484224 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0402 01:47:19.381779 139663255484224 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0402 01:47:19.414489 139663255484224 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 18.14245376
I0402 01:47:19.415469 139663255484224 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.14245376
I0402 01:47:19.415578 139663255484224 submission_runner.py:313] Starting training loop.
I0402 01:47:27.423878 139663255484224 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 23.497244672
I0402 01:47:32.474027 139634386384640 logging_writer.py:48] [0] global_step=0, grad_norm=0.591013, loss=6.923360
I0402 01:47:32.486299 139663255484224 submission.py:296] 0) loss = 6.923, grad_norm = 0.591
I0402 01:47:32.486845 139663255484224 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.62578944
I0402 01:47:32.487406 139663255484224 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.62578944
I0402 01:47:32.487540 139663255484224 spec.py:298] Evaluating on the training split.
I0402 01:48:27.272883 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 01:49:14.068265 139663255484224 spec.py:326] Evaluating on the test split.
I0402 01:49:14.085980 139663255484224 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0402 01:49:14.092414 139663255484224 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0402 01:49:14.158823 139663255484224 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
2023-04-02 01:49:14.752373: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:49:15.948360: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:49:17.161864: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:49:18.231989: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-02 01:49:19.365451: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
I0402 01:49:31.797888 139663255484224 submission_runner.py:382] Time since start: 13.07s, 	Step: 1, 	{'train/accuracy': 0.0008569834183673469, 'train/loss': 6.918406583824936, 'validation/accuracy': 0.00064, 'validation/loss': 6.91802625, 'validation/num_examples': 50000, 'test/accuracy': 0.0007, 'test/loss': 6.91997421875, 'test/num_examples': 10000}
I0402 01:49:31.798293 139663255484224 submission_runner.py:396] After eval at step 1: RAM USED (GB) 91.535572992
I0402 01:49:31.809001 139610889889536 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=13.070488, test/accuracy=0.000700, test/loss=6.919974, test/num_examples=10000, total_duration=13.072334, train/accuracy=0.000857, train/loss=6.918407, validation/accuracy=0.000640, validation/loss=6.918026, validation/num_examples=50000
I0402 01:49:32.280689 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_1.
I0402 01:49:32.281471 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 91.536203776
I0402 01:49:32.286926 139663255484224 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 91.535962112
I0402 01:49:32.294449 139663255484224 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:49:32.294525 140130779113280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:49:32.295021 140148540061504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:49:32.295031 139651216226112 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:49:32.295041 140685019285312 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:49:32.295056 140381314582336 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:49:32.295067 139699861108544 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:49:32.295245 140056081393472 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 01:49:32.685177 139610881496832 logging_writer.py:48] [1] global_step=1, grad_norm=0.615313, loss=6.930926
I0402 01:49:32.688835 139663255484224 submission.py:296] 1) loss = 6.931, grad_norm = 0.615
I0402 01:49:32.689573 139663255484224 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 91.553345536
I0402 01:49:33.074283 139610889889536 logging_writer.py:48] [2] global_step=2, grad_norm=0.604779, loss=6.921191
I0402 01:49:33.077986 139663255484224 submission.py:296] 2) loss = 6.921, grad_norm = 0.605
I0402 01:49:33.466233 139610881496832 logging_writer.py:48] [3] global_step=3, grad_norm=0.601888, loss=6.922734
I0402 01:49:33.469678 139663255484224 submission.py:296] 3) loss = 6.923, grad_norm = 0.602
I0402 01:49:33.855442 139610889889536 logging_writer.py:48] [4] global_step=4, grad_norm=0.600945, loss=6.926811
I0402 01:49:33.859348 139663255484224 submission.py:296] 4) loss = 6.927, grad_norm = 0.601
I0402 01:49:34.253090 139610881496832 logging_writer.py:48] [5] global_step=5, grad_norm=0.592257, loss=6.930310
I0402 01:49:34.257729 139663255484224 submission.py:296] 5) loss = 6.930, grad_norm = 0.592
I0402 01:49:34.645554 139610889889536 logging_writer.py:48] [6] global_step=6, grad_norm=0.617266, loss=6.927813
I0402 01:49:34.649569 139663255484224 submission.py:296] 6) loss = 6.928, grad_norm = 0.617
I0402 01:49:35.037800 139610881496832 logging_writer.py:48] [7] global_step=7, grad_norm=0.621935, loss=6.936570
I0402 01:49:35.041592 139663255484224 submission.py:296] 7) loss = 6.937, grad_norm = 0.622
I0402 01:49:35.429041 139610889889536 logging_writer.py:48] [8] global_step=8, grad_norm=0.607641, loss=6.920267
I0402 01:49:35.433476 139663255484224 submission.py:296] 8) loss = 6.920, grad_norm = 0.608
I0402 01:49:35.824249 139610881496832 logging_writer.py:48] [9] global_step=9, grad_norm=0.625065, loss=6.930810
I0402 01:49:35.827841 139663255484224 submission.py:296] 9) loss = 6.931, grad_norm = 0.625
I0402 01:49:36.215327 139610889889536 logging_writer.py:48] [10] global_step=10, grad_norm=0.601984, loss=6.918242
I0402 01:49:36.218735 139663255484224 submission.py:296] 10) loss = 6.918, grad_norm = 0.602
I0402 01:49:36.605949 139610881496832 logging_writer.py:48] [11] global_step=11, grad_norm=0.602011, loss=6.926406
I0402 01:49:36.609840 139663255484224 submission.py:296] 11) loss = 6.926, grad_norm = 0.602
I0402 01:49:36.997899 139610889889536 logging_writer.py:48] [12] global_step=12, grad_norm=0.591390, loss=6.923599
I0402 01:49:37.001482 139663255484224 submission.py:296] 12) loss = 6.924, grad_norm = 0.591
I0402 01:49:37.387646 139610881496832 logging_writer.py:48] [13] global_step=13, grad_norm=0.600522, loss=6.927866
I0402 01:49:37.391098 139663255484224 submission.py:296] 13) loss = 6.928, grad_norm = 0.601
I0402 01:49:37.779048 139610889889536 logging_writer.py:48] [14] global_step=14, grad_norm=0.619639, loss=6.934754
I0402 01:49:37.782485 139663255484224 submission.py:296] 14) loss = 6.935, grad_norm = 0.620
I0402 01:49:38.170637 139610881496832 logging_writer.py:48] [15] global_step=15, grad_norm=0.586524, loss=6.915628
I0402 01:49:38.179920 139663255484224 submission.py:296] 15) loss = 6.916, grad_norm = 0.587
I0402 01:49:38.578829 139610889889536 logging_writer.py:48] [16] global_step=16, grad_norm=0.588443, loss=6.918806
I0402 01:49:38.589746 139663255484224 submission.py:296] 16) loss = 6.919, grad_norm = 0.588
I0402 01:49:38.977413 139610881496832 logging_writer.py:48] [17] global_step=17, grad_norm=0.606075, loss=6.931487
I0402 01:49:38.981100 139663255484224 submission.py:296] 17) loss = 6.931, grad_norm = 0.606
I0402 01:49:39.370612 139610889889536 logging_writer.py:48] [18] global_step=18, grad_norm=0.604990, loss=6.922296
I0402 01:49:39.374666 139663255484224 submission.py:296] 18) loss = 6.922, grad_norm = 0.605
I0402 01:49:39.774368 139610881496832 logging_writer.py:48] [19] global_step=19, grad_norm=0.596613, loss=6.919809
I0402 01:49:39.778598 139663255484224 submission.py:296] 19) loss = 6.920, grad_norm = 0.597
I0402 01:49:40.167155 139610889889536 logging_writer.py:48] [20] global_step=20, grad_norm=0.602852, loss=6.921136
I0402 01:49:40.170590 139663255484224 submission.py:296] 20) loss = 6.921, grad_norm = 0.603
I0402 01:49:40.559253 139610881496832 logging_writer.py:48] [21] global_step=21, grad_norm=0.608779, loss=6.929120
I0402 01:49:40.562852 139663255484224 submission.py:296] 21) loss = 6.929, grad_norm = 0.609
I0402 01:49:41.025978 139610889889536 logging_writer.py:48] [22] global_step=22, grad_norm=0.607299, loss=6.929460
I0402 01:49:41.029833 139663255484224 submission.py:296] 22) loss = 6.929, grad_norm = 0.607
I0402 01:49:41.416479 139610881496832 logging_writer.py:48] [23] global_step=23, grad_norm=0.587865, loss=6.918260
I0402 01:49:41.420557 139663255484224 submission.py:296] 23) loss = 6.918, grad_norm = 0.588
I0402 01:49:41.808153 139610889889536 logging_writer.py:48] [24] global_step=24, grad_norm=0.603319, loss=6.916071
I0402 01:49:41.812103 139663255484224 submission.py:296] 24) loss = 6.916, grad_norm = 0.603
I0402 01:49:42.199382 139610881496832 logging_writer.py:48] [25] global_step=25, grad_norm=0.598845, loss=6.930902
I0402 01:49:42.202760 139663255484224 submission.py:296] 25) loss = 6.931, grad_norm = 0.599
I0402 01:49:42.592898 139610889889536 logging_writer.py:48] [26] global_step=26, grad_norm=0.612311, loss=6.931820
I0402 01:49:42.596417 139663255484224 submission.py:296] 26) loss = 6.932, grad_norm = 0.612
I0402 01:49:42.982999 139610881496832 logging_writer.py:48] [27] global_step=27, grad_norm=0.598027, loss=6.922151
I0402 01:49:42.987366 139663255484224 submission.py:296] 27) loss = 6.922, grad_norm = 0.598
I0402 01:49:43.377094 139610889889536 logging_writer.py:48] [28] global_step=28, grad_norm=0.610398, loss=6.932045
I0402 01:49:43.381156 139663255484224 submission.py:296] 28) loss = 6.932, grad_norm = 0.610
I0402 01:49:43.773965 139610881496832 logging_writer.py:48] [29] global_step=29, grad_norm=0.607940, loss=6.931208
I0402 01:49:43.777507 139663255484224 submission.py:296] 29) loss = 6.931, grad_norm = 0.608
I0402 01:49:44.164303 139610889889536 logging_writer.py:48] [30] global_step=30, grad_norm=0.606898, loss=6.927882
I0402 01:49:44.167759 139663255484224 submission.py:296] 30) loss = 6.928, grad_norm = 0.607
I0402 01:49:44.557086 139610881496832 logging_writer.py:48] [31] global_step=31, grad_norm=0.588477, loss=6.923177
I0402 01:49:44.561021 139663255484224 submission.py:296] 31) loss = 6.923, grad_norm = 0.588
I0402 01:49:44.947806 139610889889536 logging_writer.py:48] [32] global_step=32, grad_norm=0.583145, loss=6.913640
I0402 01:49:44.952469 139663255484224 submission.py:296] 32) loss = 6.914, grad_norm = 0.583
I0402 01:49:45.348579 139610881496832 logging_writer.py:48] [33] global_step=33, grad_norm=0.619985, loss=6.923894
I0402 01:49:45.351970 139663255484224 submission.py:296] 33) loss = 6.924, grad_norm = 0.620
I0402 01:49:45.740220 139610889889536 logging_writer.py:48] [34] global_step=34, grad_norm=0.589180, loss=6.912407
I0402 01:49:45.743783 139663255484224 submission.py:296] 34) loss = 6.912, grad_norm = 0.589
I0402 01:49:46.133114 139610881496832 logging_writer.py:48] [35] global_step=35, grad_norm=0.592739, loss=6.920662
I0402 01:49:46.137092 139663255484224 submission.py:296] 35) loss = 6.921, grad_norm = 0.593
I0402 01:49:46.528485 139610889889536 logging_writer.py:48] [36] global_step=36, grad_norm=0.608612, loss=6.922768
I0402 01:49:46.531893 139663255484224 submission.py:296] 36) loss = 6.923, grad_norm = 0.609
I0402 01:49:46.920238 139610881496832 logging_writer.py:48] [37] global_step=37, grad_norm=0.603634, loss=6.916959
I0402 01:49:46.924666 139663255484224 submission.py:296] 37) loss = 6.917, grad_norm = 0.604
I0402 01:49:47.314682 139610889889536 logging_writer.py:48] [38] global_step=38, grad_norm=0.590421, loss=6.928930
I0402 01:49:47.318130 139663255484224 submission.py:296] 38) loss = 6.929, grad_norm = 0.590
I0402 01:49:47.750818 139610881496832 logging_writer.py:48] [39] global_step=39, grad_norm=0.601937, loss=6.925408
I0402 01:49:47.754411 139663255484224 submission.py:296] 39) loss = 6.925, grad_norm = 0.602
I0402 01:49:48.150662 139610889889536 logging_writer.py:48] [40] global_step=40, grad_norm=0.608354, loss=6.931482
I0402 01:49:48.154375 139663255484224 submission.py:296] 40) loss = 6.931, grad_norm = 0.608
I0402 01:49:48.541988 139610881496832 logging_writer.py:48] [41] global_step=41, grad_norm=0.583443, loss=6.921806
I0402 01:49:48.545432 139663255484224 submission.py:296] 41) loss = 6.922, grad_norm = 0.583
I0402 01:49:48.933310 139610889889536 logging_writer.py:48] [42] global_step=42, grad_norm=0.594022, loss=6.913768
I0402 01:49:48.937525 139663255484224 submission.py:296] 42) loss = 6.914, grad_norm = 0.594
I0402 01:49:49.325370 139610881496832 logging_writer.py:48] [43] global_step=43, grad_norm=0.611105, loss=6.913462
I0402 01:49:49.328888 139663255484224 submission.py:296] 43) loss = 6.913, grad_norm = 0.611
I0402 01:49:49.718373 139610889889536 logging_writer.py:48] [44] global_step=44, grad_norm=0.596776, loss=6.910128
I0402 01:49:49.721942 139663255484224 submission.py:296] 44) loss = 6.910, grad_norm = 0.597
I0402 01:49:50.114276 139610881496832 logging_writer.py:48] [45] global_step=45, grad_norm=0.583323, loss=6.917253
I0402 01:49:50.117783 139663255484224 submission.py:296] 45) loss = 6.917, grad_norm = 0.583
I0402 01:49:50.506655 139610889889536 logging_writer.py:48] [46] global_step=46, grad_norm=0.610647, loss=6.918685
I0402 01:49:50.510710 139663255484224 submission.py:296] 46) loss = 6.919, grad_norm = 0.611
I0402 01:49:50.898035 139610881496832 logging_writer.py:48] [47] global_step=47, grad_norm=0.596788, loss=6.909388
I0402 01:49:50.901803 139663255484224 submission.py:296] 47) loss = 6.909, grad_norm = 0.597
I0402 01:49:51.289846 139610889889536 logging_writer.py:48] [48] global_step=48, grad_norm=0.600436, loss=6.921690
I0402 01:49:51.293329 139663255484224 submission.py:296] 48) loss = 6.922, grad_norm = 0.600
I0402 01:49:51.680264 139610881496832 logging_writer.py:48] [49] global_step=49, grad_norm=0.607413, loss=6.920940
I0402 01:49:51.684117 139663255484224 submission.py:296] 49) loss = 6.921, grad_norm = 0.607
I0402 01:49:52.077464 139610889889536 logging_writer.py:48] [50] global_step=50, grad_norm=0.588821, loss=6.914267
I0402 01:49:52.081131 139663255484224 submission.py:296] 50) loss = 6.914, grad_norm = 0.589
I0402 01:49:52.474579 139610881496832 logging_writer.py:48] [51] global_step=51, grad_norm=0.592399, loss=6.915550
I0402 01:49:52.478392 139663255484224 submission.py:296] 51) loss = 6.916, grad_norm = 0.592
I0402 01:49:52.865910 139610889889536 logging_writer.py:48] [52] global_step=52, grad_norm=0.582612, loss=6.911569
I0402 01:49:52.869545 139663255484224 submission.py:296] 52) loss = 6.912, grad_norm = 0.583
I0402 01:49:53.257073 139610881496832 logging_writer.py:48] [53] global_step=53, grad_norm=0.586833, loss=6.917349
I0402 01:49:53.260461 139663255484224 submission.py:296] 53) loss = 6.917, grad_norm = 0.587
I0402 01:49:53.662741 139610889889536 logging_writer.py:48] [54] global_step=54, grad_norm=0.600266, loss=6.902426
I0402 01:49:53.668440 139663255484224 submission.py:296] 54) loss = 6.902, grad_norm = 0.600
I0402 01:49:54.056901 139610881496832 logging_writer.py:48] [55] global_step=55, grad_norm=0.584496, loss=6.914978
I0402 01:49:54.060905 139663255484224 submission.py:296] 55) loss = 6.915, grad_norm = 0.584
I0402 01:49:54.454689 139610889889536 logging_writer.py:48] [56] global_step=56, grad_norm=0.596482, loss=6.906426
I0402 01:49:54.458164 139663255484224 submission.py:296] 56) loss = 6.906, grad_norm = 0.596
I0402 01:49:54.846669 139610881496832 logging_writer.py:48] [57] global_step=57, grad_norm=0.599319, loss=6.908474
I0402 01:49:54.850417 139663255484224 submission.py:296] 57) loss = 6.908, grad_norm = 0.599
I0402 01:49:55.237654 139610889889536 logging_writer.py:48] [58] global_step=58, grad_norm=0.612898, loss=6.902565
I0402 01:49:55.241496 139663255484224 submission.py:296] 58) loss = 6.903, grad_norm = 0.613
I0402 01:49:55.637801 139610881496832 logging_writer.py:48] [59] global_step=59, grad_norm=0.602970, loss=6.907773
I0402 01:49:55.641616 139663255484224 submission.py:296] 59) loss = 6.908, grad_norm = 0.603
I0402 01:49:56.029493 139610889889536 logging_writer.py:48] [60] global_step=60, grad_norm=0.594985, loss=6.904782
I0402 01:49:56.033424 139663255484224 submission.py:296] 60) loss = 6.905, grad_norm = 0.595
I0402 01:49:56.421074 139610881496832 logging_writer.py:48] [61] global_step=61, grad_norm=0.578408, loss=6.903120
I0402 01:49:56.424624 139663255484224 submission.py:296] 61) loss = 6.903, grad_norm = 0.578
I0402 01:49:56.813434 139610889889536 logging_writer.py:48] [62] global_step=62, grad_norm=0.580833, loss=6.910639
I0402 01:49:56.816992 139663255484224 submission.py:296] 62) loss = 6.911, grad_norm = 0.581
I0402 01:49:57.210882 139610881496832 logging_writer.py:48] [63] global_step=63, grad_norm=0.604998, loss=6.907196
I0402 01:49:57.214343 139663255484224 submission.py:296] 63) loss = 6.907, grad_norm = 0.605
I0402 01:49:57.602669 139610889889536 logging_writer.py:48] [64] global_step=64, grad_norm=0.620372, loss=6.896845
I0402 01:49:57.606177 139663255484224 submission.py:296] 64) loss = 6.897, grad_norm = 0.620
I0402 01:49:57.992788 139610881496832 logging_writer.py:48] [65] global_step=65, grad_norm=0.590144, loss=6.901575
I0402 01:49:57.996539 139663255484224 submission.py:296] 65) loss = 6.902, grad_norm = 0.590
I0402 01:49:58.383954 139610889889536 logging_writer.py:48] [66] global_step=66, grad_norm=0.591343, loss=6.899029
I0402 01:49:58.387523 139663255484224 submission.py:296] 66) loss = 6.899, grad_norm = 0.591
I0402 01:49:58.794799 139610881496832 logging_writer.py:48] [67] global_step=67, grad_norm=0.602875, loss=6.899045
I0402 01:49:58.798811 139663255484224 submission.py:296] 67) loss = 6.899, grad_norm = 0.603
I0402 01:49:59.187183 139610889889536 logging_writer.py:48] [68] global_step=68, grad_norm=0.608943, loss=6.898450
I0402 01:49:59.190773 139663255484224 submission.py:296] 68) loss = 6.898, grad_norm = 0.609
I0402 01:49:59.578392 139610881496832 logging_writer.py:48] [69] global_step=69, grad_norm=0.577603, loss=6.905537
I0402 01:49:59.582136 139663255484224 submission.py:296] 69) loss = 6.906, grad_norm = 0.578
I0402 01:49:59.976181 139610889889536 logging_writer.py:48] [70] global_step=70, grad_norm=0.592452, loss=6.896026
I0402 01:49:59.980136 139663255484224 submission.py:296] 70) loss = 6.896, grad_norm = 0.592
I0402 01:50:00.366497 139610881496832 logging_writer.py:48] [71] global_step=71, grad_norm=0.600322, loss=6.902844
I0402 01:50:00.369952 139663255484224 submission.py:296] 71) loss = 6.903, grad_norm = 0.600
I0402 01:50:00.760455 139610889889536 logging_writer.py:48] [72] global_step=72, grad_norm=0.595401, loss=6.903931
I0402 01:50:00.764607 139663255484224 submission.py:296] 72) loss = 6.904, grad_norm = 0.595
I0402 01:50:01.152085 139610881496832 logging_writer.py:48] [73] global_step=73, grad_norm=0.613575, loss=6.900175
I0402 01:50:01.220093 139663255484224 submission.py:296] 73) loss = 6.900, grad_norm = 0.614
I0402 01:50:01.611801 139610889889536 logging_writer.py:48] [74] global_step=74, grad_norm=0.580765, loss=6.894257
I0402 01:50:01.615564 139663255484224 submission.py:296] 74) loss = 6.894, grad_norm = 0.581
I0402 01:50:02.002731 139610881496832 logging_writer.py:48] [75] global_step=75, grad_norm=0.583482, loss=6.893266
I0402 01:50:02.006147 139663255484224 submission.py:296] 75) loss = 6.893, grad_norm = 0.583
I0402 01:50:02.396133 139610889889536 logging_writer.py:48] [76] global_step=76, grad_norm=0.584552, loss=6.895279
I0402 01:50:02.399734 139663255484224 submission.py:296] 76) loss = 6.895, grad_norm = 0.585
I0402 01:50:02.788859 139610881496832 logging_writer.py:48] [77] global_step=77, grad_norm=0.592384, loss=6.899256
I0402 01:50:02.792992 139663255484224 submission.py:296] 77) loss = 6.899, grad_norm = 0.592
I0402 01:50:03.182228 139610889889536 logging_writer.py:48] [78] global_step=78, grad_norm=0.604923, loss=6.895368
I0402 01:50:03.186149 139663255484224 submission.py:296] 78) loss = 6.895, grad_norm = 0.605
I0402 01:50:03.579663 139610881496832 logging_writer.py:48] [79] global_step=79, grad_norm=0.612142, loss=6.904195
I0402 01:50:03.583717 139663255484224 submission.py:296] 79) loss = 6.904, grad_norm = 0.612
I0402 01:50:03.974573 139610889889536 logging_writer.py:48] [80] global_step=80, grad_norm=0.578193, loss=6.892114
I0402 01:50:03.978382 139663255484224 submission.py:296] 80) loss = 6.892, grad_norm = 0.578
I0402 01:50:04.367048 139610881496832 logging_writer.py:48] [81] global_step=81, grad_norm=0.576805, loss=6.893183
I0402 01:50:04.371468 139663255484224 submission.py:296] 81) loss = 6.893, grad_norm = 0.577
I0402 01:50:04.771228 139610889889536 logging_writer.py:48] [82] global_step=82, grad_norm=0.581814, loss=6.880787
I0402 01:50:04.774817 139663255484224 submission.py:296] 82) loss = 6.881, grad_norm = 0.582
I0402 01:50:05.165106 139610881496832 logging_writer.py:48] [83] global_step=83, grad_norm=0.596012, loss=6.890239
I0402 01:50:05.169029 139663255484224 submission.py:296] 83) loss = 6.890, grad_norm = 0.596
I0402 01:50:05.557487 139610889889536 logging_writer.py:48] [84] global_step=84, grad_norm=0.585730, loss=6.894369
I0402 01:50:05.561065 139663255484224 submission.py:296] 84) loss = 6.894, grad_norm = 0.586
I0402 01:50:05.954531 139610881496832 logging_writer.py:48] [85] global_step=85, grad_norm=0.583089, loss=6.892012
I0402 01:50:05.958686 139663255484224 submission.py:296] 85) loss = 6.892, grad_norm = 0.583
I0402 01:50:06.346771 139610889889536 logging_writer.py:48] [86] global_step=86, grad_norm=0.603035, loss=6.882348
I0402 01:50:06.350744 139663255484224 submission.py:296] 86) loss = 6.882, grad_norm = 0.603
I0402 01:50:06.744336 139610881496832 logging_writer.py:48] [87] global_step=87, grad_norm=0.592437, loss=6.884099
I0402 01:50:06.748028 139663255484224 submission.py:296] 87) loss = 6.884, grad_norm = 0.592
I0402 01:50:07.136724 139610889889536 logging_writer.py:48] [88] global_step=88, grad_norm=0.586481, loss=6.873427
I0402 01:50:07.140519 139663255484224 submission.py:296] 88) loss = 6.873, grad_norm = 0.586
I0402 01:50:07.532154 139610881496832 logging_writer.py:48] [89] global_step=89, grad_norm=0.591252, loss=6.877662
I0402 01:50:07.535626 139663255484224 submission.py:296] 89) loss = 6.878, grad_norm = 0.591
I0402 01:50:07.978309 139610889889536 logging_writer.py:48] [90] global_step=90, grad_norm=0.605675, loss=6.888943
I0402 01:50:07.982236 139663255484224 submission.py:296] 90) loss = 6.889, grad_norm = 0.606
I0402 01:50:08.369340 139610881496832 logging_writer.py:48] [91] global_step=91, grad_norm=0.579691, loss=6.886560
I0402 01:50:08.373027 139663255484224 submission.py:296] 91) loss = 6.887, grad_norm = 0.580
I0402 01:50:08.760616 139610889889536 logging_writer.py:48] [92] global_step=92, grad_norm=0.600974, loss=6.876341
I0402 01:50:08.764376 139663255484224 submission.py:296] 92) loss = 6.876, grad_norm = 0.601
I0402 01:50:09.157662 139610881496832 logging_writer.py:48] [93] global_step=93, grad_norm=0.593148, loss=6.882656
I0402 01:50:09.161451 139663255484224 submission.py:296] 93) loss = 6.883, grad_norm = 0.593
I0402 01:50:09.549524 139610889889536 logging_writer.py:48] [94] global_step=94, grad_norm=0.579150, loss=6.887846
I0402 01:50:09.552857 139663255484224 submission.py:296] 94) loss = 6.888, grad_norm = 0.579
I0402 01:50:09.945856 139610881496832 logging_writer.py:48] [95] global_step=95, grad_norm=0.594442, loss=6.878733
I0402 01:50:09.949974 139663255484224 submission.py:296] 95) loss = 6.879, grad_norm = 0.594
I0402 01:50:10.339251 139610889889536 logging_writer.py:48] [96] global_step=96, grad_norm=0.588807, loss=6.875087
I0402 01:50:10.344005 139663255484224 submission.py:296] 96) loss = 6.875, grad_norm = 0.589
I0402 01:50:10.731218 139610881496832 logging_writer.py:48] [97] global_step=97, grad_norm=0.591493, loss=6.866369
I0402 01:50:10.734611 139663255484224 submission.py:296] 97) loss = 6.866, grad_norm = 0.591
I0402 01:50:11.123906 139610889889536 logging_writer.py:48] [98] global_step=98, grad_norm=0.596284, loss=6.880475
I0402 01:50:11.127946 139663255484224 submission.py:296] 98) loss = 6.880, grad_norm = 0.596
I0402 01:50:11.515168 139610881496832 logging_writer.py:48] [99] global_step=99, grad_norm=0.597483, loss=6.878197
I0402 01:50:11.519149 139663255484224 submission.py:296] 99) loss = 6.878, grad_norm = 0.597
I0402 01:50:11.906042 139610889889536 logging_writer.py:48] [100] global_step=100, grad_norm=0.591334, loss=6.881010
I0402 01:50:11.909733 139663255484224 submission.py:296] 100) loss = 6.881, grad_norm = 0.591
I0402 01:52:45.794909 139610881496832 logging_writer.py:48] [500] global_step=500, grad_norm=0.926524, loss=6.319764
I0402 01:52:45.798743 139663255484224 submission.py:296] 500) loss = 6.320, grad_norm = 0.927
I0402 01:55:58.149702 139610889889536 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.030708, loss=5.654925
I0402 01:55:58.153903 139663255484224 submission.py:296] 1000) loss = 5.655, grad_norm = 2.031
I0402 01:58:02.503881 139663255484224 submission_runner.py:373] Before eval at step 1321: RAM USED (GB) 98.002120704
I0402 01:58:02.504099 139663255484224 spec.py:298] Evaluating on the training split.
I0402 01:58:44.967357 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 01:59:29.679952 139663255484224 spec.py:326] Evaluating on the test split.
I0402 01:59:31.055184 139663255484224 submission_runner.py:382] Time since start: 643.09s, 	Step: 1321, 	{'train/accuracy': 0.12191087372448979, 'train/loss': 4.739671979631696, 'validation/accuracy': 0.11242, 'validation/loss': 4.81864, 'validation/num_examples': 50000, 'test/accuracy': 0.0758, 'test/loss': 5.2427625, 'test/num_examples': 10000}
I0402 01:59:31.055604 139663255484224 submission_runner.py:396] After eval at step 1321: RAM USED (GB) 98.039463936
I0402 01:59:31.064638 139610898282240 logging_writer.py:48] [1321] global_step=1321, preemption_count=0, score=521.009080, test/accuracy=0.075800, test/loss=5.242763, test/num_examples=10000, total_duration=643.088450, train/accuracy=0.121911, train/loss=4.739672, validation/accuracy=0.112420, validation/loss=4.818640, validation/num_examples=50000
I0402 01:59:31.545227 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_1321.
I0402 01:59:31.546088 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 1321: RAM USED (GB) 98.037563392
I0402 02:00:40.068374 139610906674944 logging_writer.py:48] [1500] global_step=1500, grad_norm=7.422530, loss=5.244964
I0402 02:00:40.072238 139663255484224 submission.py:296] 1500) loss = 5.245, grad_norm = 7.423
I0402 02:03:50.819130 139610898282240 logging_writer.py:48] [2000] global_step=2000, grad_norm=6.522353, loss=4.890563
I0402 02:03:50.825505 139663255484224 submission.py:296] 2000) loss = 4.891, grad_norm = 6.522
I0402 02:07:02.835160 139610906674944 logging_writer.py:48] [2500] global_step=2500, grad_norm=5.253686, loss=4.592286
I0402 02:07:02.839150 139663255484224 submission.py:296] 2500) loss = 4.592, grad_norm = 5.254
I0402 02:08:01.837112 139663255484224 submission_runner.py:373] Before eval at step 2652: RAM USED (GB) 99.483746304
I0402 02:08:01.837329 139663255484224 spec.py:298] Evaluating on the training split.
I0402 02:08:47.177518 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 02:09:37.297360 139663255484224 spec.py:326] Evaluating on the test split.
I0402 02:09:38.652875 139663255484224 submission_runner.py:382] Time since start: 1242.42s, 	Step: 2652, 	{'train/accuracy': 0.27455357142857145, 'train/loss': 3.5280618472975127, 'validation/accuracy': 0.24996, 'validation/loss': 3.6715490625, 'validation/num_examples': 50000, 'test/accuracy': 0.1738, 'test/loss': 4.2908234375, 'test/num_examples': 10000}
I0402 02:09:38.653234 139663255484224 submission_runner.py:396] After eval at step 2652: RAM USED (GB) 99.562205184
I0402 02:09:38.661893 139610898282240 logging_writer.py:48] [2652] global_step=2652, preemption_count=0, score=1029.042293, test/accuracy=0.173800, test/loss=4.290823, test/num_examples=10000, total_duration=1242.421533, train/accuracy=0.274554, train/loss=3.528062, validation/accuracy=0.249960, validation/loss=3.671549, validation/num_examples=50000
I0402 02:09:39.119500 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_2652.
I0402 02:09:39.120342 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 2652: RAM USED (GB) 99.560484864
I0402 02:11:51.984706 139610906674944 logging_writer.py:48] [3000] global_step=3000, grad_norm=7.425354, loss=4.340039
I0402 02:11:51.988682 139663255484224 submission.py:296] 3000) loss = 4.340, grad_norm = 7.425
I0402 02:15:02.567279 139610898282240 logging_writer.py:48] [3500] global_step=3500, grad_norm=8.005899, loss=4.012965
I0402 02:15:02.571031 139663255484224 submission.py:296] 3500) loss = 4.013, grad_norm = 8.006
I0402 02:18:09.333389 139663255484224 submission_runner.py:373] Before eval at step 3987: RAM USED (GB) 99.705741312
I0402 02:18:09.333601 139663255484224 spec.py:298] Evaluating on the training split.
I0402 02:18:51.958135 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 02:19:40.758240 139663255484224 spec.py:326] Evaluating on the test split.
I0402 02:19:42.117571 139663255484224 submission_runner.py:382] Time since start: 1849.92s, 	Step: 3987, 	{'train/accuracy': 0.3946508290816326, 'train/loss': 2.8045249471859055, 'validation/accuracy': 0.3636, 'validation/loss': 2.9866853125, 'validation/num_examples': 50000, 'test/accuracy': 0.2671, 'test/loss': 3.6398546875, 'test/num_examples': 10000}
I0402 02:19:42.117907 139663255484224 submission_runner.py:396] After eval at step 3987: RAM USED (GB) 99.667382272
I0402 02:19:42.127279 139610906674944 logging_writer.py:48] [3987] global_step=3987, preemption_count=0, score=1537.093814, test/accuracy=0.267100, test/loss=3.639855, test/num_examples=10000, total_duration=1849.917831, train/accuracy=0.394651, train/loss=2.804525, validation/accuracy=0.363600, validation/loss=2.986685, validation/num_examples=50000
I0402 02:19:42.605159 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_3987.
I0402 02:19:42.605957 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 3987: RAM USED (GB) 99.664883712
I0402 02:19:47.898733 139610898282240 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.726776, loss=3.934445
I0402 02:19:47.902870 139663255484224 submission.py:296] 4000) loss = 3.934, grad_norm = 4.727
I0402 02:22:58.100485 139610906674944 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.055518, loss=3.639760
I0402 02:22:58.105413 139663255484224 submission.py:296] 4500) loss = 3.640, grad_norm = 4.056
I0402 02:26:09.809580 139610898282240 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.263959, loss=3.656672
I0402 02:26:09.813826 139663255484224 submission.py:296] 5000) loss = 3.657, grad_norm = 3.264
I0402 02:28:12.901803 139663255484224 submission_runner.py:373] Before eval at step 5321: RAM USED (GB) 99.825913856
I0402 02:28:12.902019 139663255484224 spec.py:298] Evaluating on the training split.
I0402 02:28:57.383946 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 02:29:44.574704 139663255484224 spec.py:326] Evaluating on the test split.
I0402 02:29:45.930560 139663255484224 submission_runner.py:382] Time since start: 2453.49s, 	Step: 5321, 	{'train/accuracy': 0.48009008290816324, 'train/loss': 2.3474283023756377, 'validation/accuracy': 0.44212, 'validation/loss': 2.55423671875, 'validation/num_examples': 50000, 'test/accuracy': 0.3193, 'test/loss': 3.317873828125, 'test/num_examples': 10000}
I0402 02:29:45.930882 139663255484224 submission_runner.py:396] After eval at step 5321: RAM USED (GB) 99.783430144
I0402 02:29:45.939385 139610906674944 logging_writer.py:48] [5321] global_step=5321, preemption_count=0, score=2045.203736, test/accuracy=0.319300, test/loss=3.317874, test/num_examples=10000, total_duration=2453.486270, train/accuracy=0.480090, train/loss=2.347428, validation/accuracy=0.442120, validation/loss=2.554237, validation/num_examples=50000
I0402 02:29:46.408774 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_5321.
I0402 02:29:46.409566 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 5321: RAM USED (GB) 99.78222592
I0402 02:30:54.815979 139610898282240 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.689052, loss=3.498564
I0402 02:30:54.819809 139663255484224 submission.py:296] 5500) loss = 3.499, grad_norm = 2.689
I0402 02:34:05.051747 139610906674944 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.742162, loss=3.356622
I0402 02:34:05.056830 139663255484224 submission.py:296] 6000) loss = 3.357, grad_norm = 2.742
I0402 02:37:17.625339 139610898282240 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.576465, loss=3.264185
I0402 02:37:17.629519 139663255484224 submission.py:296] 6500) loss = 3.264, grad_norm = 2.576
I0402 02:38:16.558066 139663255484224 submission_runner.py:373] Before eval at step 6656: RAM USED (GB) 99.947388928
I0402 02:38:16.558311 139663255484224 spec.py:298] Evaluating on the training split.
I0402 02:38:59.048014 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 02:39:51.307236 139663255484224 spec.py:326] Evaluating on the test split.
I0402 02:39:52.662850 139663255484224 submission_runner.py:382] Time since start: 3057.14s, 	Step: 6656, 	{'train/accuracy': 0.5318279655612245, 'train/loss': 2.074802320830676, 'validation/accuracy': 0.488, 'validation/loss': 2.303048125, 'validation/num_examples': 50000, 'test/accuracy': 0.3649, 'test/loss': 3.0487283203125, 'test/num_examples': 10000}
I0402 02:39:52.663213 139663255484224 submission_runner.py:396] After eval at step 6656: RAM USED (GB) 99.942166528
I0402 02:39:52.671127 139610906674944 logging_writer.py:48] [6656] global_step=6656, preemption_count=0, score=2553.197050, test/accuracy=0.364900, test/loss=3.048728, test/num_examples=10000, total_duration=3057.142478, train/accuracy=0.531828, train/loss=2.074802, validation/accuracy=0.488000, validation/loss=2.303048, validation/num_examples=50000
I0402 02:39:53.173533 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_6656.
I0402 02:39:53.174301 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 6656: RAM USED (GB) 99.946573824
I0402 02:42:04.316726 139610898282240 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.687862, loss=3.207189
I0402 02:42:04.320902 139663255484224 submission.py:296] 7000) loss = 3.207, grad_norm = 3.688
I0402 02:45:15.878854 139610906674944 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.331897, loss=3.213107
I0402 02:45:15.883040 139663255484224 submission.py:296] 7500) loss = 3.213, grad_norm = 2.332
I0402 02:48:23.249285 139663255484224 submission_runner.py:373] Before eval at step 7990: RAM USED (GB) 99.685101568
I0402 02:48:23.249538 139663255484224 spec.py:298] Evaluating on the training split.
I0402 02:49:07.504788 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 02:49:54.083060 139663255484224 spec.py:326] Evaluating on the test split.
I0402 02:49:55.442176 139663255484224 submission_runner.py:382] Time since start: 3663.83s, 	Step: 7990, 	{'train/accuracy': 0.5772480867346939, 'train/loss': 1.856952900789222, 'validation/accuracy': 0.52888, 'validation/loss': 2.1067634375, 'validation/num_examples': 50000, 'test/accuracy': 0.3823, 'test/loss': 2.905080859375, 'test/num_examples': 10000}
I0402 02:49:55.442520 139663255484224 submission_runner.py:396] After eval at step 7990: RAM USED (GB) 99.69678336
I0402 02:49:55.451066 139610898282240 logging_writer.py:48] [7990] global_step=7990, preemption_count=0, score=3061.123338, test/accuracy=0.382300, test/loss=2.905081, test/num_examples=10000, total_duration=3663.833776, train/accuracy=0.577248, train/loss=1.856953, validation/accuracy=0.528880, validation/loss=2.106763, validation/num_examples=50000
I0402 02:49:55.912547 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_7990.
I0402 02:49:55.913316 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 7990: RAM USED (GB) 99.6950016
I0402 02:50:00.080213 139610906674944 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.180465, loss=3.093257
I0402 02:50:00.084672 139663255484224 submission.py:296] 8000) loss = 3.093, grad_norm = 2.180
I0402 02:53:10.294580 139610898282240 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.920296, loss=3.062362
I0402 02:53:10.298908 139663255484224 submission.py:296] 8500) loss = 3.062, grad_norm = 1.920
I0402 02:56:22.498754 139610906674944 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.022291, loss=3.022041
I0402 02:56:22.502912 139663255484224 submission.py:296] 9000) loss = 3.022, grad_norm = 2.022
I0402 02:58:25.914966 139663255484224 submission_runner.py:373] Before eval at step 9326: RAM USED (GB) 99.765899264
I0402 02:58:25.915178 139663255484224 spec.py:298] Evaluating on the training split.
I0402 02:59:09.697306 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 03:00:02.572971 139663255484224 spec.py:326] Evaluating on the test split.
I0402 03:00:03.927721 139663255484224 submission_runner.py:382] Time since start: 4266.50s, 	Step: 9326, 	{'train/accuracy': 0.6103714923469388, 'train/loss': 1.702847227758291, 'validation/accuracy': 0.55846, 'validation/loss': 1.96884828125, 'validation/num_examples': 50000, 'test/accuracy': 0.4202, 'test/loss': 2.7514181640625, 'test/num_examples': 10000}
I0402 03:00:03.928063 139663255484224 submission_runner.py:396] After eval at step 9326: RAM USED (GB) 99.71863552
I0402 03:00:03.936735 139610898282240 logging_writer.py:48] [9326] global_step=9326, preemption_count=0, score=3569.017628, test/accuracy=0.420200, test/loss=2.751418, test/num_examples=10000, total_duration=4266.499435, train/accuracy=0.610371, train/loss=1.702847, validation/accuracy=0.558460, validation/loss=1.968848, validation/num_examples=50000
I0402 03:00:04.406339 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_9326.
I0402 03:00:04.407253 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 9326: RAM USED (GB) 99.717423104
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0402 03:01:10.815673 139610906674944 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.564369, loss=3.069247
I0402 03:01:10.819777 139663255484224 submission.py:296] 9500) loss = 3.069, grad_norm = 1.564
I0402 03:04:22.209285 139610898282240 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.719834, loss=2.872124
I0402 03:04:22.213401 139663255484224 submission.py:296] 10000) loss = 2.872, grad_norm = 1.720
I0402 03:07:33.512706 139610906674944 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.587019, loss=2.909057
I0402 03:07:33.517392 139663255484224 submission.py:296] 10500) loss = 2.909, grad_norm = 1.587
I0402 03:08:34.631751 139663255484224 submission_runner.py:373] Before eval at step 10662: RAM USED (GB) 99.62795008
I0402 03:08:34.631967 139663255484224 spec.py:298] Evaluating on the training split.
I0402 03:09:18.172816 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 03:10:03.328033 139663255484224 spec.py:326] Evaluating on the test split.
I0402 03:10:04.677473 139663255484224 submission_runner.py:382] Time since start: 4875.22s, 	Step: 10662, 	{'train/accuracy': 0.6534797512755102, 'train/loss': 1.5304770956234055, 'validation/accuracy': 0.59094, 'validation/loss': 1.8227296875, 'validation/num_examples': 50000, 'test/accuracy': 0.4449, 'test/loss': 2.58611328125, 'test/num_examples': 10000}
I0402 03:10:04.677857 139663255484224 submission_runner.py:396] After eval at step 10662: RAM USED (GB) 99.778535424
I0402 03:10:04.686892 139610898282240 logging_writer.py:48] [10662] global_step=10662, preemption_count=0, score=4077.078169, test/accuracy=0.444900, test/loss=2.586113, test/num_examples=10000, total_duration=4875.216223, train/accuracy=0.653480, train/loss=1.530477, validation/accuracy=0.590940, validation/loss=1.822730, validation/num_examples=50000
I0402 03:10:05.149672 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_10662.
I0402 03:10:05.150488 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 10662: RAM USED (GB) 99.776282624
I0402 03:12:14.137201 139610906674944 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.611326, loss=2.797303
I0402 03:12:14.141109 139663255484224 submission.py:296] 11000) loss = 2.797, grad_norm = 1.611
I0402 03:15:26.582117 139610898282240 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.573880, loss=2.692846
I0402 03:15:26.587227 139663255484224 submission.py:296] 11500) loss = 2.693, grad_norm = 1.574
I0402 03:18:35.263111 139663255484224 submission_runner.py:373] Before eval at step 11998: RAM USED (GB) 99.84106496
I0402 03:18:35.263324 139663255484224 spec.py:298] Evaluating on the training split.
I0402 03:19:18.499245 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 03:20:03.314592 139663255484224 spec.py:326] Evaluating on the test split.
I0402 03:20:04.673068 139663255484224 submission_runner.py:382] Time since start: 5475.85s, 	Step: 11998, 	{'train/accuracy': 0.6730907206632653, 'train/loss': 1.425891331263951, 'validation/accuracy': 0.60534, 'validation/loss': 1.7372759375, 'validation/num_examples': 50000, 'test/accuracy': 0.4711, 'test/loss': 2.4735796875, 'test/num_examples': 10000}
I0402 03:20:04.673408 139663255484224 submission_runner.py:396] After eval at step 11998: RAM USED (GB) 99.83246336
I0402 03:20:04.685526 139610906674944 logging_writer.py:48] [11998] global_step=11998, preemption_count=0, score=4585.043688, test/accuracy=0.471100, test/loss=2.473580, test/num_examples=10000, total_duration=5475.846449, train/accuracy=0.673091, train/loss=1.425891, validation/accuracy=0.605340, validation/loss=1.737276, validation/num_examples=50000
I0402 03:20:05.160223 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_11998.
I0402 03:20:05.161060 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 11998: RAM USED (GB) 99.830980608
I0402 03:20:06.295299 139610898282240 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.397937, loss=2.704631
I0402 03:20:06.299429 139663255484224 submission.py:296] 12000) loss = 2.705, grad_norm = 1.398
I0402 03:23:17.556859 139610906674944 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.910634, loss=2.782614
I0402 03:23:17.564417 139663255484224 submission.py:296] 12500) loss = 2.783, grad_norm = 1.911
I0402 03:26:29.113073 139610898282240 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.603451, loss=2.682025
I0402 03:26:29.117630 139663255484224 submission.py:296] 13000) loss = 2.682, grad_norm = 1.603
I0402 03:28:35.307464 139663255484224 submission_runner.py:373] Before eval at step 13333: RAM USED (GB) 99.942137856
I0402 03:28:35.307692 139663255484224 spec.py:298] Evaluating on the training split.
I0402 03:29:18.471560 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 03:30:03.289924 139663255484224 spec.py:326] Evaluating on the test split.
I0402 03:30:04.642898 139663255484224 submission_runner.py:382] Time since start: 6075.89s, 	Step: 13333, 	{'train/accuracy': 0.6931002869897959, 'train/loss': 1.360708898427535, 'validation/accuracy': 0.61598, 'validation/loss': 1.6912903125, 'validation/num_examples': 50000, 'test/accuracy': 0.4628, 'test/loss': 2.518783984375, 'test/num_examples': 10000}
I0402 03:30:04.643267 139663255484224 submission_runner.py:396] After eval at step 13333: RAM USED (GB) 99.883495424
I0402 03:30:04.657215 139610906674944 logging_writer.py:48] [13333] global_step=13333, preemption_count=0, score=5093.034875, test/accuracy=0.462800, test/loss=2.518784, test/num_examples=10000, total_duration=6075.889895, train/accuracy=0.693100, train/loss=1.360709, validation/accuracy=0.615980, validation/loss=1.691290, validation/num_examples=50000
I0402 03:30:05.139163 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_13333.
I0402 03:30:05.140012 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 13333: RAM USED (GB) 99.88253696
I0402 03:31:09.005057 139610898282240 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.205559, loss=2.719566
I0402 03:31:09.008864 139663255484224 submission.py:296] 13500) loss = 2.720, grad_norm = 1.206
I0402 03:34:21.238277 139663255484224 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 99.838369792
I0402 03:34:21.238520 139663255484224 spec.py:298] Evaluating on the training split.
I0402 03:35:03.516782 139663255484224 spec.py:310] Evaluating on the validation split.
I0402 03:35:48.317744 139663255484224 spec.py:326] Evaluating on the test split.
I0402 03:35:49.670321 139663255484224 submission_runner.py:382] Time since start: 6421.82s, 	Step: 14000, 	{'train/accuracy': 0.7003946109693877, 'train/loss': 1.2996829285913585, 'validation/accuracy': 0.63046, 'validation/loss': 1.6366228125, 'validation/num_examples': 50000, 'test/accuracy': 0.4822, 'test/loss': 2.392014453125, 'test/num_examples': 10000}
I0402 03:35:49.670692 139663255484224 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 99.868336128
I0402 03:35:49.678689 139610906674944 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5348.041076, test/accuracy=0.482200, test/loss=2.392014, test/num_examples=10000, total_duration=6421.822816, train/accuracy=0.700395, train/loss=1.299683, validation/accuracy=0.630460, validation/loss=1.636623, validation/num_examples=50000
I0402 03:35:50.154325 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0402 03:35:50.155164 139663255484224 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 99.867103232
I0402 03:35:50.163170 139610898282240 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5348.041076
I0402 03:35:51.411611 139663255484224 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0402 03:35:51.782857 139663255484224 submission_runner.py:550] Tuning trial 1/1
I0402 03:35:51.783061 139663255484224 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0402 03:35:51.783748 139663255484224 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008569834183673469, 'train/loss': 6.918406583824936, 'validation/accuracy': 0.00064, 'validation/loss': 6.91802625, 'validation/num_examples': 50000, 'test/accuracy': 0.0007, 'test/loss': 6.91997421875, 'test/num_examples': 10000, 'score': 13.070488214492798, 'total_duration': 13.072334051132202, 'global_step': 1, 'preemption_count': 0}), (1321, {'train/accuracy': 0.12191087372448979, 'train/loss': 4.739671979631696, 'validation/accuracy': 0.11242, 'validation/loss': 4.81864, 'validation/num_examples': 50000, 'test/accuracy': 0.0758, 'test/loss': 5.2427625, 'test/num_examples': 10000, 'score': 521.0090796947479, 'total_duration': 643.0884504318237, 'global_step': 1321, 'preemption_count': 0}), (2652, {'train/accuracy': 0.27455357142857145, 'train/loss': 3.5280618472975127, 'validation/accuracy': 0.24996, 'validation/loss': 3.6715490625, 'validation/num_examples': 50000, 'test/accuracy': 0.1738, 'test/loss': 4.2908234375, 'test/num_examples': 10000, 'score': 1029.0422928333282, 'total_duration': 1242.421532869339, 'global_step': 2652, 'preemption_count': 0}), (3987, {'train/accuracy': 0.3946508290816326, 'train/loss': 2.8045249471859055, 'validation/accuracy': 0.3636, 'validation/loss': 2.9866853125, 'validation/num_examples': 50000, 'test/accuracy': 0.2671, 'test/loss': 3.6398546875, 'test/num_examples': 10000, 'score': 1537.0938136577606, 'total_duration': 1849.9178314208984, 'global_step': 3987, 'preemption_count': 0}), (5321, {'train/accuracy': 0.48009008290816324, 'train/loss': 2.3474283023756377, 'validation/accuracy': 0.44212, 'validation/loss': 2.55423671875, 'validation/num_examples': 50000, 'test/accuracy': 0.3193, 'test/loss': 3.317873828125, 'test/num_examples': 10000, 'score': 2045.203735589981, 'total_duration': 2453.4862701892853, 'global_step': 5321, 'preemption_count': 0}), (6656, {'train/accuracy': 0.5318279655612245, 'train/loss': 2.074802320830676, 'validation/accuracy': 0.488, 'validation/loss': 2.303048125, 'validation/num_examples': 50000, 'test/accuracy': 0.3649, 'test/loss': 3.0487283203125, 'test/num_examples': 10000, 'score': 2553.1970500946045, 'total_duration': 3057.142477989197, 'global_step': 6656, 'preemption_count': 0}), (7990, {'train/accuracy': 0.5772480867346939, 'train/loss': 1.856952900789222, 'validation/accuracy': 0.52888, 'validation/loss': 2.1067634375, 'validation/num_examples': 50000, 'test/accuracy': 0.3823, 'test/loss': 2.905080859375, 'test/num_examples': 10000, 'score': 3061.1233382225037, 'total_duration': 3663.8337757587433, 'global_step': 7990, 'preemption_count': 0}), (9326, {'train/accuracy': 0.6103714923469388, 'train/loss': 1.702847227758291, 'validation/accuracy': 0.55846, 'validation/loss': 1.96884828125, 'validation/num_examples': 50000, 'test/accuracy': 0.4202, 'test/loss': 2.7514181640625, 'test/num_examples': 10000, 'score': 3569.0176277160645, 'total_duration': 4266.4994349479675, 'global_step': 9326, 'preemption_count': 0}), (10662, {'train/accuracy': 0.6534797512755102, 'train/loss': 1.5304770956234055, 'validation/accuracy': 0.59094, 'validation/loss': 1.8227296875, 'validation/num_examples': 50000, 'test/accuracy': 0.4449, 'test/loss': 2.58611328125, 'test/num_examples': 10000, 'score': 4077.078169107437, 'total_duration': 4875.21622300148, 'global_step': 10662, 'preemption_count': 0}), (11998, {'train/accuracy': 0.6730907206632653, 'train/loss': 1.425891331263951, 'validation/accuracy': 0.60534, 'validation/loss': 1.7372759375, 'validation/num_examples': 50000, 'test/accuracy': 0.4711, 'test/loss': 2.4735796875, 'test/num_examples': 10000, 'score': 4585.043687582016, 'total_duration': 5475.846448659897, 'global_step': 11998, 'preemption_count': 0}), (13333, {'train/accuracy': 0.6931002869897959, 'train/loss': 1.360708898427535, 'validation/accuracy': 0.61598, 'validation/loss': 1.6912903125, 'validation/num_examples': 50000, 'test/accuracy': 0.4628, 'test/loss': 2.518783984375, 'test/num_examples': 10000, 'score': 5093.034874677658, 'total_duration': 6075.889895439148, 'global_step': 13333, 'preemption_count': 0}), (14000, {'train/accuracy': 0.7003946109693877, 'train/loss': 1.2996829285913585, 'validation/accuracy': 0.63046, 'validation/loss': 1.6366228125, 'validation/num_examples': 50000, 'test/accuracy': 0.4822, 'test/loss': 2.392014453125, 'test/num_examples': 10000, 'score': 5348.041076421738, 'total_duration': 6421.822816133499, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0402 03:35:51.783847 139663255484224 submission_runner.py:553] Timing: 5348.041076421738
I0402 03:35:51.783890 139663255484224 submission_runner.py:554] ====================
I0402 03:35:51.783987 139663255484224 submission_runner.py:613] Final imagenet_resnet score: 5348.041076421738
