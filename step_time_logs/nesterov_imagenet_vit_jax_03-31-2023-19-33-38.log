I0331 19:33:58.777990 140640233387840 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/imagenet_vit_jax.
I0331 19:33:58.827645 140640233387840 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0331 19:33:59.706126 140640233387840 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0331 19:33:59.706877 140640233387840 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0331 19:33:59.712949 140640233387840 submission_runner.py:511] Using RNG seed 406362250
I0331 19:34:02.087528 140640233387840 submission_runner.py:520] --- Tuning run 1/1 ---
I0331 19:34:02.087807 140640233387840 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1.
I0331 19:34:02.088050 140640233387840 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/hparams.json.
I0331 19:34:02.212839 140640233387840 submission_runner.py:230] Starting train once: RAM USED (GB) 4.232937472
I0331 19:34:02.213019 140640233387840 submission_runner.py:231] Initializing dataset.
I0331 19:34:02.224515 140640233387840 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:34:02.232935 140640233387840 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 19:34:02.233115 140640233387840 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 19:34:02.476593 140640233387840 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:34:08.976392 140640233387840 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.401360896
I0331 19:34:08.976588 140640233387840 submission_runner.py:240] Initializing model.
I0331 19:34:19.907911 140640233387840 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.402927616
I0331 19:34:19.908096 140640233387840 submission_runner.py:252] Initializing optimizer.
I0331 19:34:20.464637 140640233387840 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.404197376
I0331 19:34:20.464811 140640233387840 submission_runner.py:261] Initializing metrics bundle.
I0331 19:34:20.464864 140640233387840 submission_runner.py:276] Initializing checkpoint and logger.
I0331 19:34:20.465633 140640233387840 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0331 19:34:21.306900 140640233387840 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/meta_data_0.json.
I0331 19:34:21.307919 140640233387840 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/flags_0.json.
I0331 19:34:21.312150 140640233387840 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.400687104
I0331 19:34:21.312362 140640233387840 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.400687104
I0331 19:34:21.312430 140640233387840 submission_runner.py:313] Starting training loop.
I0331 19:34:24.475012 140640233387840 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 14.13326848
I0331 19:35:06.166512 140461448615680 logging_writer.py:48] [0] global_step=0, grad_norm=0.28885871171951294, loss=6.907753944396973
I0331 19:35:06.177911 140640233387840 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 46.430158848
I0331 19:35:06.178153 140640233387840 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 46.430158848
I0331 19:35:06.178231 140640233387840 spec.py:298] Evaluating on the training split.
I0331 19:35:06.183971 140640233387840 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:35:06.190044 140640233387840 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 19:35:06.190146 140640233387840 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 19:35:06.248759 140640233387840 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:35:24.195354 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 19:35:24.202822 140640233387840 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:35:24.211700 140640233387840 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 19:35:24.211966 140640233387840 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 19:35:24.272647 140640233387840 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:35:43.138234 140640233387840 spec.py:326] Evaluating on the test split.
I0331 19:35:43.145108 140640233387840 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0331 19:35:43.150297 140640233387840 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0331 19:35:43.184199 140640233387840 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0331 19:35:54.393885 140640233387840 submission_runner.py:382] Time since start: 44.87s, 	Step: 1, 	{'train/accuracy': 0.0009374999790452421, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000}
I0331 19:35:54.394416 140640233387840 submission_runner.py:396] After eval at step 1: RAM USED (GB) 105.865805824
I0331 19:35:54.404009 140406817801984 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=44.785585, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=44.865775, train/accuracy=0.000937, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0331 19:35:54.540214 140640233387840 checkpoints.py:356] Saving checkpoint at step: 1
I0331 19:35:55.322905 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_1
I0331 19:35:55.323789 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_1.
I0331 19:35:55.325683 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 105.969369088
I0331 19:35:55.330299 140640233387840 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 105.969369088
I0331 19:36:10.415316 140640233387840 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 105.359663104
I0331 19:36:50.775618 140463935805184 logging_writer.py:48] [100] global_step=100, grad_norm=0.28579387068748474, loss=6.906562328338623
I0331 19:37:31.762369 140463944197888 logging_writer.py:48] [200] global_step=200, grad_norm=0.271575927734375, loss=6.9024553298950195
I0331 19:38:12.852383 140463935805184 logging_writer.py:48] [300] global_step=300, grad_norm=0.3202667832374573, loss=6.87550687789917
I0331 19:38:53.811793 140463944197888 logging_writer.py:48] [400] global_step=400, grad_norm=0.47200652956962585, loss=6.813729763031006
I0331 19:39:34.939582 140463935805184 logging_writer.py:48] [500] global_step=500, grad_norm=0.8730024695396423, loss=6.761527061462402
I0331 19:40:16.177341 140463944197888 logging_writer.py:48] [600] global_step=600, grad_norm=0.7301539778709412, loss=6.6965460777282715
I0331 19:40:57.302620 140463935805184 logging_writer.py:48] [700] global_step=700, grad_norm=0.8861771821975708, loss=6.611175537109375
I0331 19:41:38.497478 140463944197888 logging_writer.py:48] [800] global_step=800, grad_norm=1.5328776836395264, loss=6.56674861907959
I0331 19:42:19.714019 140463935805184 logging_writer.py:48] [900] global_step=900, grad_norm=0.7795548439025879, loss=6.75583553314209
I0331 19:42:55.534712 140640233387840 submission_runner.py:373] Before eval at step 989: RAM USED (GB) 79.875833856
I0331 19:42:55.535043 140640233387840 spec.py:298] Evaluating on the training split.
I0331 19:43:06.391588 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 19:43:12.887764 140640233387840 spec.py:326] Evaluating on the test split.
I0331 19:43:14.785672 140640233387840 submission_runner.py:382] Time since start: 514.22s, 	Step: 989, 	{'train/accuracy': 0.03490234166383743, 'train/loss': 6.064363956451416, 'validation/accuracy': 0.033560000360012054, 'validation/loss': 6.0869011878967285, 'validation/num_examples': 50000, 'test/accuracy': 0.028200000524520874, 'test/loss': 6.183608531951904, 'test/num_examples': 10000}
I0331 19:43:14.786203 140640233387840 submission_runner.py:396] After eval at step 989: RAM USED (GB) 83.249885184
I0331 19:43:14.804859 140407035913984 logging_writer.py:48] [989] global_step=989, preemption_count=0, score=458.122433, test/accuracy=0.028200, test/loss=6.183609, test/num_examples=10000, total_duration=514.219772, train/accuracy=0.034902, train/loss=6.064364, validation/accuracy=0.033560, validation/loss=6.086901, validation/num_examples=50000
I0331 19:43:15.241931 140640233387840 checkpoints.py:356] Saving checkpoint at step: 989
I0331 19:43:17.678984 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_989
I0331 19:43:17.691107 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_989.
I0331 19:43:17.692965 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 989: RAM USED (GB) 95.030501376
I0331 19:43:22.663829 140407044306688 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1035807132720947, loss=6.4941325187683105
I0331 19:44:03.991167 140464002946816 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.1211224794387817, loss=6.5588603019714355
I0331 19:44:45.145711 140407044306688 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8369043469429016, loss=6.452641010284424
I0331 19:45:26.370010 140464002946816 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.9600008726119995, loss=6.446000576019287
I0331 19:46:07.799956 140407044306688 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.769014298915863, loss=6.503331661224365
I0331 19:46:48.983337 140464002946816 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8288589715957642, loss=6.330811500549316
I0331 19:47:30.233775 140407044306688 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8376281261444092, loss=6.295858383178711
I0331 19:48:11.448626 140464002946816 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.6428354978561401, loss=6.305850028991699
I0331 19:48:52.500629 140407044306688 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9773675203323364, loss=6.318161487579346
I0331 19:49:33.756538 140464002946816 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.011668086051941, loss=6.612086296081543
I0331 19:50:15.422696 140407044306688 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8002820611000061, loss=6.337674140930176
I0331 19:50:17.983321 140640233387840 submission_runner.py:373] Before eval at step 2008: RAM USED (GB) 86.979960832
I0331 19:50:17.983561 140640233387840 spec.py:298] Evaluating on the training split.
I0331 19:50:29.050683 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 19:50:35.703357 140640233387840 spec.py:326] Evaluating on the test split.
I0331 19:50:37.386810 140640233387840 submission_runner.py:382] Time since start: 956.67s, 	Step: 2008, 	{'train/accuracy': 0.06593749672174454, 'train/loss': 5.581186771392822, 'validation/accuracy': 0.0647599995136261, 'validation/loss': 5.613847255706787, 'validation/num_examples': 50000, 'test/accuracy': 0.047300003468990326, 'test/loss': 5.786659240722656, 'test/num_examples': 10000}
I0331 19:50:37.387702 140640233387840 submission_runner.py:396] After eval at step 2008: RAM USED (GB) 88.3314688
I0331 19:50:37.401498 140464002946816 logging_writer.py:48] [2008] global_step=2008, preemption_count=0, score=871.111330, test/accuracy=0.047300, test/loss=5.786659, test/num_examples=10000, total_duration=956.670095, train/accuracy=0.065937, train/loss=5.581187, validation/accuracy=0.064760, validation/loss=5.613847, validation/num_examples=50000
I0331 19:50:37.512560 140640233387840 checkpoints.py:356] Saving checkpoint at step: 2008
I0331 19:50:41.297733 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_2008
I0331 19:50:41.307467 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_2008.
I0331 19:50:41.309297 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 2008: RAM USED (GB) 102.288887808
I0331 19:51:19.541777 140407044306688 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8292220830917358, loss=6.245869159698486
I0331 19:52:00.906661 140463969376000 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.91706782579422, loss=6.2644829750061035
I0331 19:52:42.255594 140407044306688 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1357301473617554, loss=6.223535060882568
I0331 19:53:23.419418 140463969376000 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8909111022949219, loss=6.35002326965332
I0331 19:54:04.681359 140407044306688 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.950882077217102, loss=6.245230674743652
I0331 19:54:45.884528 140463969376000 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.0636500120162964, loss=6.209132194519043
I0331 19:55:27.132092 140407044306688 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.860389232635498, loss=6.144641399383545
I0331 19:56:08.401487 140463969376000 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7297914624214172, loss=6.713291168212891
I0331 19:56:49.703761 140407044306688 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8296808004379272, loss=6.1145477294921875
I0331 19:57:31.003048 140463969376000 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8188728094100952, loss=6.69580602645874
I0331 19:57:41.512276 140640233387840 submission_runner.py:373] Before eval at step 3027: RAM USED (GB) 93.180346368
I0331 19:57:41.512601 140640233387840 spec.py:298] Evaluating on the training split.
I0331 19:57:52.942772 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 19:57:59.727272 140640233387840 spec.py:326] Evaluating on the test split.
I0331 19:58:01.405044 140640233387840 submission_runner.py:382] Time since start: 1400.20s, 	Step: 3027, 	{'train/accuracy': 0.09765625, 'train/loss': 5.281352996826172, 'validation/accuracy': 0.08822000026702881, 'validation/loss': 5.336995601654053, 'validation/num_examples': 50000, 'test/accuracy': 0.06750000268220901, 'test/loss': 5.544164180755615, 'test/num_examples': 10000}
I0331 19:58:01.405705 140640233387840 submission_runner.py:396] After eval at step 3027: RAM USED (GB) 94.848507904
I0331 19:58:01.418957 140407044306688 logging_writer.py:48] [3027] global_step=3027, preemption_count=0, score=1284.302900, test/accuracy=0.067500, test/loss=5.544164, test/num_examples=10000, total_duration=1400.199279, train/accuracy=0.097656, train/loss=5.281353, validation/accuracy=0.088220, validation/loss=5.336996, validation/num_examples=50000
I0331 19:58:01.515840 140640233387840 checkpoints.py:356] Saving checkpoint at step: 3027
I0331 19:58:04.631350 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_3027
I0331 19:58:04.644600 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_3027.
I0331 19:58:04.646677 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 3027: RAM USED (GB) 106.294595584
I0331 19:58:35.608810 140463969376000 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6253641247749329, loss=6.547878742218018
I0331 19:59:16.856911 140463960983296 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8732354044914246, loss=6.059431076049805
I0331 19:59:58.055039 140463969376000 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7940707802772522, loss=5.981942176818848
I0331 20:00:39.353958 140463960983296 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7003605365753174, loss=6.1520280838012695
I0331 20:01:20.565423 140463969376000 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7523539066314697, loss=6.0792670249938965
I0331 20:02:01.908309 140463960983296 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7443007230758667, loss=6.1146955490112305
I0331 20:02:43.315606 140463969376000 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8277907967567444, loss=5.999506950378418
I0331 20:03:24.576624 140463960983296 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.6900073885917664, loss=6.166702747344971
I0331 20:04:05.739387 140463969376000 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6080036759376526, loss=6.537095546722412
I0331 20:04:47.106096 140463960983296 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6856665015220642, loss=6.086629867553711
I0331 20:05:04.769639 140640233387840 submission_runner.py:373] Before eval at step 4044: RAM USED (GB) 99.789041664
I0331 20:05:04.769911 140640233387840 spec.py:298] Evaluating on the training split.
I0331 20:05:16.016361 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 20:05:23.541447 140640233387840 spec.py:326] Evaluating on the test split.
I0331 20:05:25.201110 140640233387840 submission_runner.py:382] Time since start: 1843.45s, 	Step: 4044, 	{'train/accuracy': 0.13230468332767487, 'train/loss': 4.88593053817749, 'validation/accuracy': 0.11493999511003494, 'validation/loss': 5.001798152923584, 'validation/num_examples': 50000, 'test/accuracy': 0.08740000426769257, 'test/loss': 5.265015602111816, 'test/num_examples': 10000}
I0331 20:05:25.201684 140640233387840 submission_runner.py:396] After eval at step 4044: RAM USED (GB) 99.620798464
I0331 20:05:25.211738 140463969376000 logging_writer.py:48] [4044] global_step=4044, preemption_count=0, score=1696.808986, test/accuracy=0.087400, test/loss=5.265016, test/num_examples=10000, total_duration=1843.454911, train/accuracy=0.132305, train/loss=4.885931, validation/accuracy=0.114940, validation/loss=5.001798, validation/num_examples=50000
I0331 20:05:25.299993 140640233387840 checkpoints.py:356] Saving checkpoint at step: 4044
I0331 20:05:28.009675 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_4044
I0331 20:05:28.022386 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_4044.
I0331 20:05:28.024369 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 4044: RAM USED (GB) 109.378654208
I0331 20:05:51.563984 140463960983296 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7081995606422424, loss=6.603805065155029
I0331 20:06:32.510355 140463944197888 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7227925658226013, loss=5.877214431762695
I0331 20:07:13.722686 140463960983296 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9298741817474365, loss=5.927814960479736
I0331 20:07:54.830833 140463944197888 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5718551874160767, loss=6.375546932220459
I0331 20:08:36.035844 140463960983296 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6061979532241821, loss=6.642778396606445
I0331 20:09:17.064522 140463944197888 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.49027201533317566, loss=6.323686599731445
I0331 20:09:58.207488 140463960983296 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6112812161445618, loss=6.6360931396484375
I0331 20:10:39.673701 140463944197888 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7349659204483032, loss=5.784224510192871
I0331 20:11:20.892162 140463960983296 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.0100960731506348, loss=5.864480495452881
I0331 20:12:02.193015 140463944197888 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5548655390739441, loss=5.991966247558594
I0331 20:12:28.376231 140640233387840 submission_runner.py:373] Before eval at step 5065: RAM USED (GB) 105.252687872
I0331 20:12:28.376505 140640233387840 spec.py:298] Evaluating on the training split.
I0331 20:12:40.966190 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 20:12:49.942728 140640233387840 spec.py:326] Evaluating on the test split.
I0331 20:12:51.605396 140640233387840 submission_runner.py:382] Time since start: 2287.06s, 	Step: 5065, 	{'train/accuracy': 0.15757812559604645, 'train/loss': 4.650578498840332, 'validation/accuracy': 0.1445399969816208, 'validation/loss': 4.727085113525391, 'validation/num_examples': 50000, 'test/accuracy': 0.10840000212192535, 'test/loss': 5.029891490936279, 'test/num_examples': 10000}
I0331 20:12:51.605986 140640233387840 submission_runner.py:396] After eval at step 5065: RAM USED (GB) 108.460179456
I0331 20:12:51.617444 140463960983296 logging_writer.py:48] [5065] global_step=5065, preemption_count=0, score=2109.750088, test/accuracy=0.108400, test/loss=5.029891, test/num_examples=10000, total_duration=2287.061715, train/accuracy=0.157578, train/loss=4.650578, validation/accuracy=0.144540, validation/loss=4.727085, validation/num_examples=50000
I0331 20:12:51.712943 140640233387840 checkpoints.py:356] Saving checkpoint at step: 5065
I0331 20:12:52.627671 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_5065
I0331 20:12:52.638864 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_5065.
I0331 20:12:52.643637 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 5065: RAM USED (GB) 112.617996288
I0331 20:13:07.593382 140463944197888 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.9907554388046265, loss=5.78611421585083
I0331 20:13:48.806662 140463935805184 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7856503129005432, loss=5.806166648864746
I0331 20:14:29.963208 140463944197888 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.714599609375, loss=5.714576244354248
I0331 20:15:11.054126 140463935805184 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6664767861366272, loss=5.668453693389893
I0331 20:15:52.480956 140463944197888 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6196658611297607, loss=5.923415660858154
I0331 20:16:34.089399 140463935805184 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6078528165817261, loss=6.043928146362305
I0331 20:17:15.533491 140463944197888 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5618520975112915, loss=6.296328067779541
I0331 20:17:57.054523 140463935805184 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7221944332122803, loss=5.6849446296691895
I0331 20:18:38.655857 140463944197888 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7045262455940247, loss=5.6039814949035645
I0331 20:19:20.284802 140463935805184 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.617546796798706, loss=5.907556056976318
I0331 20:19:53.002224 140640233387840 submission_runner.py:373] Before eval at step 6080: RAM USED (GB) 110.85735936
I0331 20:19:53.002568 140640233387840 spec.py:298] Evaluating on the training split.
I0331 20:20:06.486941 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 20:20:15.309826 140640233387840 spec.py:326] Evaluating on the test split.
I0331 20:20:16.964310 140640233387840 submission_runner.py:382] Time since start: 2731.69s, 	Step: 6080, 	{'train/accuracy': 0.1931445300579071, 'train/loss': 4.332148551940918, 'validation/accuracy': 0.17673999071121216, 'validation/loss': 4.4281206130981445, 'validation/num_examples': 50000, 'test/accuracy': 0.13289999961853027, 'test/loss': 4.774477958679199, 'test/num_examples': 10000}
I0331 20:20:16.964978 140640233387840 submission_runner.py:396] After eval at step 6080: RAM USED (GB) 113.638191104
I0331 20:20:16.975493 140463944197888 logging_writer.py:48] [6080] global_step=6080, preemption_count=0, score=2521.010115, test/accuracy=0.132900, test/loss=4.774478, test/num_examples=10000, total_duration=2731.687867, train/accuracy=0.193145, train/loss=4.332149, validation/accuracy=0.176740, validation/loss=4.428121, validation/num_examples=50000
I0331 20:20:17.228365 140640233387840 checkpoints.py:356] Saving checkpoint at step: 6080
I0331 20:20:18.477765 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_6080
I0331 20:20:18.489881 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_6080.
I0331 20:20:18.500410 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 6080: RAM USED (GB) 119.441481728
I0331 20:20:27.143671 140463935805184 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5107609629631042, loss=6.0495381355285645
I0331 20:21:08.351120 140462207792896 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7426037788391113, loss=5.558249473571777
I0331 20:21:49.767840 140463935805184 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.673633337020874, loss=5.634209632873535
I0331 20:22:30.925743 140462207792896 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6721383929252625, loss=5.567270755767822
I0331 20:23:12.163240 140463935805184 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6330857276916504, loss=5.5750274658203125
I0331 20:23:53.797737 140462207792896 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7371667623519897, loss=5.51003360748291
I0331 20:24:35.317000 140463935805184 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7446361184120178, loss=5.959179401397705
I0331 20:25:16.917275 140462207792896 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7146955728530884, loss=5.455842971801758
I0331 20:25:58.577330 140463935805184 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.700518786907196, loss=5.704154968261719
I0331 20:26:40.103097 140462207792896 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7070701122283936, loss=5.456585884094238
I0331 20:27:18.919588 140640233387840 submission_runner.py:373] Before eval at step 7095: RAM USED (GB) 117.075206144
I0331 20:27:18.919945 140640233387840 spec.py:298] Evaluating on the training split.
I0331 20:27:32.700289 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 20:27:41.898917 140640233387840 spec.py:326] Evaluating on the test split.
I0331 20:27:43.547336 140640233387840 submission_runner.py:382] Time since start: 3177.60s, 	Step: 7095, 	{'train/accuracy': 0.23876953125, 'train/loss': 4.006844997406006, 'validation/accuracy': 0.21687999367713928, 'validation/loss': 4.117733478546143, 'validation/num_examples': 50000, 'test/accuracy': 0.1608000099658966, 'test/loss': 4.526920795440674, 'test/num_examples': 10000}
I0331 20:27:43.547917 140640233387840 submission_runner.py:396] After eval at step 7095: RAM USED (GB) 120.340148224
I0331 20:27:43.557999 140463935805184 logging_writer.py:48] [7095] global_step=7095, preemption_count=0, score=2932.252530, test/accuracy=0.160800, test/loss=4.526921, test/num_examples=10000, total_duration=3177.603798, train/accuracy=0.238770, train/loss=4.006845, validation/accuracy=0.216880, validation/loss=4.117733, validation/num_examples=50000
I0331 20:27:43.743980 140640233387840 checkpoints.py:356] Saving checkpoint at step: 7095
I0331 20:27:44.767164 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_7095
I0331 20:27:44.781656 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_7095.
I0331 20:27:44.793322 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 7095: RAM USED (GB) 125.228371968
I0331 20:27:47.312682 140462207792896 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7299537658691406, loss=5.442185878753662
I0331 20:28:28.804371 140462199400192 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6261396408081055, loss=5.58725643157959
I0331 20:29:10.309217 140462207792896 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5654066205024719, loss=6.191893100738525
I0331 20:29:51.970991 140462199400192 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7394649982452393, loss=5.504495143890381
I0331 20:30:33.635597 140462207792896 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6916142106056213, loss=5.38613748550415
I0331 20:31:14.977658 140462199400192 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6647915244102478, loss=5.730693817138672
I0331 20:31:56.586714 140462207792896 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6936792731285095, loss=5.354006290435791
I0331 20:32:38.235204 140462199400192 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.4799278676509857, loss=6.49403190612793
I0331 20:33:19.864220 140462207792896 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.714468777179718, loss=5.385815143585205
I0331 20:34:01.476458 140462199400192 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6338894367218018, loss=5.435543537139893
I0331 20:34:43.207923 140462207792896 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7207713723182678, loss=5.4319891929626465
I0331 20:34:45.002012 140640233387840 submission_runner.py:373] Before eval at step 8106: RAM USED (GB) 123.49628416
I0331 20:34:45.002288 140640233387840 spec.py:298] Evaluating on the training split.
I0331 20:34:59.168168 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 20:35:08.645623 140640233387840 spec.py:326] Evaluating on the test split.
I0331 20:35:10.299700 140640233387840 submission_runner.py:382] Time since start: 3623.69s, 	Step: 8106, 	{'train/accuracy': 0.2545703053474426, 'train/loss': 4.032355308532715, 'validation/accuracy': 0.22540000081062317, 'validation/loss': 4.1766767501831055, 'validation/num_examples': 50000, 'test/accuracy': 0.1729000061750412, 'test/loss': 4.561262130737305, 'test/num_examples': 10000}
I0331 20:35:10.300170 140640233387840 submission_runner.py:396] After eval at step 8106: RAM USED (GB) 126.441320448
I0331 20:35:10.312436 140462199400192 logging_writer.py:48] [8106] global_step=8106, preemption_count=0, score=3343.029680, test/accuracy=0.172900, test/loss=4.561262, test/num_examples=10000, total_duration=3623.687599, train/accuracy=0.254570, train/loss=4.032355, validation/accuracy=0.225400, validation/loss=4.176677, validation/num_examples=50000
I0331 20:35:10.412629 140640233387840 checkpoints.py:356] Saving checkpoint at step: 8106
I0331 20:35:11.227139 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_8106
I0331 20:35:11.241593 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_8106.
I0331 20:35:11.243558 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 8106: RAM USED (GB) 130.177708032
I0331 20:35:50.276364 140462207792896 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7707160711288452, loss=5.2694573402404785
I0331 20:36:31.821137 140462191007488 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6445738673210144, loss=5.323329448699951
I0331 20:37:13.562542 140462207792896 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7810827493667603, loss=5.352231025695801
I0331 20:37:55.208130 140462191007488 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.488186776638031, loss=6.057232856750488
I0331 20:38:36.453420 140462207792896 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5475519299507141, loss=5.224183082580566
I0331 20:39:18.284248 140462191007488 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5052975416183472, loss=6.18539571762085
I0331 20:40:00.550761 140462207792896 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5757964849472046, loss=5.192448139190674
I0331 20:40:42.362670 140462191007488 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6452007293701172, loss=5.273956298828125
I0331 20:41:24.236775 140462207792896 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5982471704483032, loss=5.17549991607666
I0331 20:42:06.054555 140462191007488 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.49736204743385315, loss=6.432519912719727
I0331 20:42:11.619824 140640233387840 submission_runner.py:373] Before eval at step 9115: RAM USED (GB) 128.80205824
I0331 20:42:11.620048 140640233387840 spec.py:298] Evaluating on the training split.
I0331 20:42:25.860056 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 20:42:36.114246 140640233387840 spec.py:326] Evaluating on the test split.
I0331 20:42:37.763267 140640233387840 submission_runner.py:382] Time since start: 4070.31s, 	Step: 9115, 	{'train/accuracy': 0.2938281297683716, 'train/loss': 3.648829221725464, 'validation/accuracy': 0.27028000354766846, 'validation/loss': 3.76326322555542, 'validation/num_examples': 50000, 'test/accuracy': 0.20650000870227814, 'test/loss': 4.228885650634766, 'test/num_examples': 10000}
I0331 20:42:37.763864 140640233387840 submission_runner.py:396] After eval at step 9115: RAM USED (GB) 135.763685376
I0331 20:42:37.776684 140462207792896 logging_writer.py:48] [9115] global_step=9115, preemption_count=0, score=3752.223231, test/accuracy=0.206500, test/loss=4.228886, test/num_examples=10000, total_duration=4070.305488, train/accuracy=0.293828, train/loss=3.648829, validation/accuracy=0.270280, validation/loss=3.763263, validation/num_examples=50000
I0331 20:42:37.900120 140640233387840 checkpoints.py:356] Saving checkpoint at step: 9115
I0331 20:42:38.913507 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_9115
I0331 20:42:38.927838 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_9115.
I0331 20:42:38.930283 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 9115: RAM USED (GB) 140.551589888
I0331 20:43:14.373872 140462191007488 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.43257787823677063, loss=6.435519695281982
I0331 20:43:55.767999 140460748150528 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.4686884582042694, loss=6.460963249206543
I0331 20:44:37.343820 140462191007488 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6628567576408386, loss=5.176487445831299
I0331 20:45:18.932596 140460748150528 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5281708836555481, loss=5.799665451049805
I0331 20:46:00.407490 140462191007488 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7097674012184143, loss=5.162214756011963
I0331 20:46:42.097435 140460748150528 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6555845141410828, loss=5.185820579528809
I0331 20:47:24.079452 140462191007488 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.581426739692688, loss=5.036101818084717
I0331 20:48:05.625119 140460748150528 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6858935952186584, loss=5.4628376960754395
I0331 20:48:47.134105 140462191007488 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6537617444992065, loss=4.991799831390381
I0331 20:49:28.733369 140460748150528 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5767813920974731, loss=5.197799205780029
I0331 20:49:39.279923 140640233387840 submission_runner.py:373] Before eval at step 10127: RAM USED (GB) 135.19126528
I0331 20:49:39.280176 140640233387840 spec.py:298] Evaluating on the training split.
I0331 20:49:53.840887 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 20:50:05.029623 140640233387840 spec.py:326] Evaluating on the test split.
I0331 20:50:06.693119 140640233387840 submission_runner.py:382] Time since start: 4517.97s, 	Step: 10127, 	{'train/accuracy': 0.33195310831069946, 'train/loss': 3.435809850692749, 'validation/accuracy': 0.30504000186920166, 'validation/loss': 3.5587618350982666, 'validation/num_examples': 50000, 'test/accuracy': 0.2355000078678131, 'test/loss': 4.030619144439697, 'test/num_examples': 10000}
I0331 20:50:06.693772 140640233387840 submission_runner.py:396] After eval at step 10127: RAM USED (GB) 143.29987072
I0331 20:50:06.707523 140462191007488 logging_writer.py:48] [10127] global_step=10127, preemption_count=0, score=4162.258282, test/accuracy=0.235500, test/loss=4.030619, test/num_examples=10000, total_duration=4517.965229, train/accuracy=0.331953, train/loss=3.435810, validation/accuracy=0.305040, validation/loss=3.558762, validation/num_examples=50000
I0331 20:50:06.801114 140640233387840 checkpoints.py:356] Saving checkpoint at step: 10127
I0331 20:50:07.860972 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_10127
I0331 20:50:07.882387 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_10127.
I0331 20:50:07.884798 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 10127: RAM USED (GB) 148.173078528
I0331 20:50:38.475299 140460748150528 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6485406756401062, loss=5.035887718200684
I0331 20:51:19.704575 140460060309248 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6301698684692383, loss=5.013454914093018
I0331 20:52:01.145364 140460748150528 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5820862650871277, loss=4.97551965713501
I0331 20:52:43.113236 140460060309248 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.45824718475341797, loss=6.216525077819824
I0331 20:53:25.099556 140460748150528 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6697260737419128, loss=5.058899879455566
I0331 20:54:06.618143 140460060309248 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.651799738407135, loss=5.641174793243408
I0331 20:54:48.404747 140460748150528 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.8940808176994324, loss=5.014530658721924
I0331 20:55:30.040616 140460060309248 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5625844597816467, loss=5.253057956695557
I0331 20:56:11.692130 140460748150528 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7052709460258484, loss=4.981205940246582
I0331 20:56:53.181133 140460060309248 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5857217311859131, loss=5.041611671447754
I0331 20:57:08.214195 140640233387840 submission_runner.py:373] Before eval at step 11138: RAM USED (GB) 141.106835456
I0331 20:57:08.214559 140640233387840 spec.py:298] Evaluating on the training split.
I0331 20:57:23.526112 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 20:57:34.897758 140640233387840 spec.py:326] Evaluating on the test split.
I0331 20:57:36.554162 140640233387840 submission_runner.py:382] Time since start: 4966.90s, 	Step: 11138, 	{'train/accuracy': 0.3599609434604645, 'train/loss': 3.267495632171631, 'validation/accuracy': 0.33191999793052673, 'validation/loss': 3.405557632446289, 'validation/num_examples': 50000, 'test/accuracy': 0.25370001792907715, 'test/loss': 3.908850908279419, 'test/num_examples': 10000}
I0331 20:57:36.554693 140640233387840 submission_runner.py:396] After eval at step 11138: RAM USED (GB) 148.672937984
I0331 20:57:36.564167 140460748150528 logging_writer.py:48] [11138] global_step=11138, preemption_count=0, score=4571.479607, test/accuracy=0.253700, test/loss=3.908851, test/num_examples=10000, total_duration=4966.900298, train/accuracy=0.359961, train/loss=3.267496, validation/accuracy=0.331920, validation/loss=3.405558, validation/num_examples=50000
I0331 20:57:36.668105 140640233387840 checkpoints.py:356] Saving checkpoint at step: 11138
I0331 20:57:37.667872 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_11138
I0331 20:57:37.682282 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_11138.
I0331 20:57:37.686072 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 11138: RAM USED (GB) 153.269784576
I0331 20:58:03.562691 140460060309248 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5904557704925537, loss=4.859918117523193
I0331 20:58:44.872550 140460051916544 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.4907025992870331, loss=6.295599937438965
I0331 20:59:26.424408 140460060309248 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6420087218284607, loss=4.8672099113464355
I0331 21:00:08.161632 140460051916544 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5326076149940491, loss=5.154684543609619
I0331 21:00:49.459215 140460060309248 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.482837975025177, loss=6.295157432556152
I0331 21:01:31.517520 140460051916544 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6448822021484375, loss=4.905590057373047
I0331 21:02:13.154423 140460060309248 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6490914821624756, loss=5.2053680419921875
I0331 21:02:54.579656 140460051916544 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5789749622344971, loss=4.858460426330566
I0331 21:03:36.361253 140460060309248 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.4997084438800812, loss=5.588091850280762
I0331 21:04:18.202695 140460051916544 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.45252174139022827, loss=6.2318339347839355
I0331 21:04:38.049834 140640233387840 submission_runner.py:373] Before eval at step 12149: RAM USED (GB) 147.110313984
I0331 21:04:38.050266 140640233387840 spec.py:298] Evaluating on the training split.
I0331 21:04:53.205725 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 21:05:04.653035 140640233387840 spec.py:326] Evaluating on the test split.
I0331 21:05:06.303061 140640233387840 submission_runner.py:382] Time since start: 5416.73s, 	Step: 12149, 	{'train/accuracy': 0.3894921839237213, 'train/loss': 3.0577595233917236, 'validation/accuracy': 0.35269999504089355, 'validation/loss': 3.2444992065429688, 'validation/num_examples': 50000, 'test/accuracy': 0.27000001072883606, 'test/loss': 3.7697153091430664, 'test/num_examples': 10000}
I0331 21:05:06.303543 140640233387840 submission_runner.py:396] After eval at step 12149: RAM USED (GB) 154.18748928
I0331 21:05:06.313632 140460060309248 logging_writer.py:48] [12149] global_step=12149, preemption_count=0, score=4980.222165, test/accuracy=0.270000, test/loss=3.769715, test/num_examples=10000, total_duration=5416.731058, train/accuracy=0.389492, train/loss=3.057760, validation/accuracy=0.352700, validation/loss=3.244499, validation/num_examples=50000
I0331 21:05:06.402361 140640233387840 checkpoints.py:356] Saving checkpoint at step: 12149
I0331 21:05:07.341098 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_12149
I0331 21:05:07.356274 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_12149.
I0331 21:05:07.358797 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 12149: RAM USED (GB) 158.485139456
I0331 21:05:28.825835 140460051916544 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.45850878953933716, loss=6.290876388549805
I0331 21:06:10.006002 140459322111744 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6554616093635559, loss=4.655888557434082
I0331 21:06:51.199039 140460051916544 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6268296837806702, loss=4.886385917663574
I0331 21:07:32.946093 140459322111744 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6495484113693237, loss=5.167580604553223
I0331 21:08:14.575419 140460051916544 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.571670413017273, loss=5.344712734222412
I0331 21:08:56.310765 140459322111744 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.49568673968315125, loss=6.093280792236328
I0331 21:09:37.878857 140460051916544 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.568428635597229, loss=5.517724990844727
I0331 21:10:19.374833 140459322111744 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6102988123893738, loss=5.341586112976074
I0331 21:11:01.064584 140460051916544 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6628885269165039, loss=4.76190710067749
I0331 21:11:42.923400 140459322111744 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.7434548139572144, loss=4.788856029510498
I0331 21:12:07.569269 140640233387840 submission_runner.py:373] Before eval at step 13161: RAM USED (GB) 153.140854784
I0331 21:12:07.569508 140640233387840 spec.py:298] Evaluating on the training split.
I0331 21:12:22.913361 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 21:12:34.185801 140640233387840 spec.py:326] Evaluating on the test split.
I0331 21:12:35.832033 140640233387840 submission_runner.py:382] Time since start: 5866.25s, 	Step: 13161, 	{'train/accuracy': 0.41240233182907104, 'train/loss': 2.973581552505493, 'validation/accuracy': 0.3707199990749359, 'validation/loss': 3.1600942611694336, 'validation/num_examples': 50000, 'test/accuracy': 0.2896000146865845, 'test/loss': 3.6823103427886963, 'test/num_examples': 10000}
I0331 21:12:35.832536 140640233387840 submission_runner.py:396] After eval at step 13161: RAM USED (GB) 160.938614784
I0331 21:12:35.844713 140460051916544 logging_writer.py:48] [13161] global_step=13161, preemption_count=0, score=5389.448139, test/accuracy=0.289600, test/loss=3.682310, test/num_examples=10000, total_duration=5866.254530, train/accuracy=0.412402, train/loss=2.973582, validation/accuracy=0.370720, validation/loss=3.160094, validation/num_examples=50000
I0331 21:12:35.933732 140640233387840 checkpoints.py:356] Saving checkpoint at step: 13161
I0331 21:12:36.740705 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_13161
I0331 21:12:36.754744 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_13161.
I0331 21:12:36.759309 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 13161: RAM USED (GB) 164.6337024
I0331 21:12:53.283802 140459322111744 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6891578435897827, loss=4.767670154571533
I0331 21:13:34.620863 140458751686400 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6206970810890198, loss=4.845000267028809
I0331 21:14:16.130970 140459322111744 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.632758378982544, loss=4.901505470275879
I0331 21:14:57.385371 140458751686400 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5856079459190369, loss=5.068312168121338
I0331 21:15:38.990090 140459322111744 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.49316492676734924, loss=5.326085090637207
I0331 21:16:20.668812 140458751686400 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6213909387588501, loss=4.66676139831543
I0331 21:17:02.364684 140459322111744 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5276563167572021, loss=6.098135471343994
I0331 21:17:43.849095 140458751686400 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.5421426296234131, loss=5.484297275543213
I0331 21:18:24.898881 140640233387840 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 159.373037568
I0331 21:18:24.899165 140640233387840 spec.py:298] Evaluating on the training split.
I0331 21:18:40.168815 140640233387840 spec.py:310] Evaluating on the validation split.
I0331 21:18:51.584653 140640233387840 spec.py:326] Evaluating on the test split.
I0331 21:18:53.223378 140640233387840 submission_runner.py:382] Time since start: 6243.58s, 	Step: 14000, 	{'train/accuracy': 0.42765623331069946, 'train/loss': 2.937498092651367, 'validation/accuracy': 0.39083999395370483, 'validation/loss': 3.1002705097198486, 'validation/num_examples': 50000, 'test/accuracy': 0.3013000190258026, 'test/loss': 3.6487255096435547, 'test/num_examples': 10000}
I0331 21:18:53.223857 140640233387840 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 166.711349248
I0331 21:18:53.235905 140459322111744 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5727.661412, test/accuracy=0.301300, test/loss=3.648726, test/num_examples=10000, total_duration=6243.583176, train/accuracy=0.427656, train/loss=2.937498, validation/accuracy=0.390840, validation/loss=3.100271, validation/num_examples=50000
I0331 21:18:53.340381 140640233387840 checkpoints.py:356] Saving checkpoint at step: 14000
I0331 21:18:54.261411 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_14000
I0331 21:18:54.275604 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_14000.
I0331 21:18:54.277511 140640233387840 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 171.012591616
I0331 21:18:54.286370 140458751686400 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5727.661412
I0331 21:18:54.423904 140640233387840 checkpoints.py:356] Saving checkpoint at step: 14000
I0331 21:18:55.609643 140640233387840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_14000
I0331 21:18:55.621990 140640233387840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_jax/trial_1/checkpoint_14000.
I0331 21:18:56.519523 140640233387840 submission_runner.py:550] Tuning trial 1/1
I0331 21:18:56.519859 140640233387840 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0331 21:18:56.523212 140640233387840 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009374999790452421, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 44.78558540344238, 'total_duration': 44.865774631500244, 'global_step': 1, 'preemption_count': 0}), (989, {'train/accuracy': 0.03490234166383743, 'train/loss': 6.064363956451416, 'validation/accuracy': 0.033560000360012054, 'validation/loss': 6.0869011878967285, 'validation/num_examples': 50000, 'test/accuracy': 0.028200000524520874, 'test/loss': 6.183608531951904, 'test/num_examples': 10000, 'score': 458.1224331855774, 'total_duration': 514.2197721004486, 'global_step': 989, 'preemption_count': 0}), (2008, {'train/accuracy': 0.06593749672174454, 'train/loss': 5.581186771392822, 'validation/accuracy': 0.0647599995136261, 'validation/loss': 5.613847255706787, 'validation/num_examples': 50000, 'test/accuracy': 0.047300003468990326, 'test/loss': 5.786659240722656, 'test/num_examples': 10000, 'score': 871.1113302707672, 'total_duration': 956.6700947284698, 'global_step': 2008, 'preemption_count': 0}), (3027, {'train/accuracy': 0.09765625, 'train/loss': 5.281352996826172, 'validation/accuracy': 0.08822000026702881, 'validation/loss': 5.336995601654053, 'validation/num_examples': 50000, 'test/accuracy': 0.06750000268220901, 'test/loss': 5.544164180755615, 'test/num_examples': 10000, 'score': 1284.302900314331, 'total_duration': 1400.1992790699005, 'global_step': 3027, 'preemption_count': 0}), (4044, {'train/accuracy': 0.13230468332767487, 'train/loss': 4.88593053817749, 'validation/accuracy': 0.11493999511003494, 'validation/loss': 5.001798152923584, 'validation/num_examples': 50000, 'test/accuracy': 0.08740000426769257, 'test/loss': 5.265015602111816, 'test/num_examples': 10000, 'score': 1696.8089861869812, 'total_duration': 1843.4549114704132, 'global_step': 4044, 'preemption_count': 0}), (5065, {'train/accuracy': 0.15757812559604645, 'train/loss': 4.650578498840332, 'validation/accuracy': 0.1445399969816208, 'validation/loss': 4.727085113525391, 'validation/num_examples': 50000, 'test/accuracy': 0.10840000212192535, 'test/loss': 5.029891490936279, 'test/num_examples': 10000, 'score': 2109.7500879764557, 'total_duration': 2287.061715364456, 'global_step': 5065, 'preemption_count': 0}), (6080, {'train/accuracy': 0.1931445300579071, 'train/loss': 4.332148551940918, 'validation/accuracy': 0.17673999071121216, 'validation/loss': 4.4281206130981445, 'validation/num_examples': 50000, 'test/accuracy': 0.13289999961853027, 'test/loss': 4.774477958679199, 'test/num_examples': 10000, 'score': 2521.0101146698, 'total_duration': 2731.687867164612, 'global_step': 6080, 'preemption_count': 0}), (7095, {'train/accuracy': 0.23876953125, 'train/loss': 4.006844997406006, 'validation/accuracy': 0.21687999367713928, 'validation/loss': 4.117733478546143, 'validation/num_examples': 50000, 'test/accuracy': 0.1608000099658966, 'test/loss': 4.526920795440674, 'test/num_examples': 10000, 'score': 2932.2525300979614, 'total_duration': 3177.603798389435, 'global_step': 7095, 'preemption_count': 0}), (8106, {'train/accuracy': 0.2545703053474426, 'train/loss': 4.032355308532715, 'validation/accuracy': 0.22540000081062317, 'validation/loss': 4.1766767501831055, 'validation/num_examples': 50000, 'test/accuracy': 0.1729000061750412, 'test/loss': 4.561262130737305, 'test/num_examples': 10000, 'score': 3343.029679775238, 'total_duration': 3623.687599182129, 'global_step': 8106, 'preemption_count': 0}), (9115, {'train/accuracy': 0.2938281297683716, 'train/loss': 3.648829221725464, 'validation/accuracy': 0.27028000354766846, 'validation/loss': 3.76326322555542, 'validation/num_examples': 50000, 'test/accuracy': 0.20650000870227814, 'test/loss': 4.228885650634766, 'test/num_examples': 10000, 'score': 3752.223231077194, 'total_duration': 4070.30548787117, 'global_step': 9115, 'preemption_count': 0}), (10127, {'train/accuracy': 0.33195310831069946, 'train/loss': 3.435809850692749, 'validation/accuracy': 0.30504000186920166, 'validation/loss': 3.5587618350982666, 'validation/num_examples': 50000, 'test/accuracy': 0.2355000078678131, 'test/loss': 4.030619144439697, 'test/num_examples': 10000, 'score': 4162.258281946182, 'total_duration': 4517.965229272842, 'global_step': 10127, 'preemption_count': 0}), (11138, {'train/accuracy': 0.3599609434604645, 'train/loss': 3.267495632171631, 'validation/accuracy': 0.33191999793052673, 'validation/loss': 3.405557632446289, 'validation/num_examples': 50000, 'test/accuracy': 0.25370001792907715, 'test/loss': 3.908850908279419, 'test/num_examples': 10000, 'score': 4571.479606866837, 'total_duration': 4966.900298118591, 'global_step': 11138, 'preemption_count': 0}), (12149, {'train/accuracy': 0.3894921839237213, 'train/loss': 3.0577595233917236, 'validation/accuracy': 0.35269999504089355, 'validation/loss': 3.2444992065429688, 'validation/num_examples': 50000, 'test/accuracy': 0.27000001072883606, 'test/loss': 3.7697153091430664, 'test/num_examples': 10000, 'score': 4980.222165107727, 'total_duration': 5416.731057882309, 'global_step': 12149, 'preemption_count': 0}), (13161, {'train/accuracy': 0.41240233182907104, 'train/loss': 2.973581552505493, 'validation/accuracy': 0.3707199990749359, 'validation/loss': 3.1600942611694336, 'validation/num_examples': 50000, 'test/accuracy': 0.2896000146865845, 'test/loss': 3.6823103427886963, 'test/num_examples': 10000, 'score': 5389.448139429092, 'total_duration': 5866.254529714584, 'global_step': 13161, 'preemption_count': 0}), (14000, {'train/accuracy': 0.42765623331069946, 'train/loss': 2.937498092651367, 'validation/accuracy': 0.39083999395370483, 'validation/loss': 3.1002705097198486, 'validation/num_examples': 50000, 'test/accuracy': 0.3013000190258026, 'test/loss': 3.6487255096435547, 'test/num_examples': 10000, 'score': 5727.661412239075, 'total_duration': 6243.583176136017, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0331 21:18:56.523376 140640233387840 submission_runner.py:553] Timing: 5727.661412239075
I0331 21:18:56.523419 140640233387840 submission_runner.py:554] ====================
I0331 21:18:56.523524 140640233387840 submission_runner.py:613] Final imagenet_vit score: 5727.661412239075
