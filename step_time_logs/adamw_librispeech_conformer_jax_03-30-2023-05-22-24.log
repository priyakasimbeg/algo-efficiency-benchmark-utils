I0330 05:22:44.417732 140152604170048 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/librispeech_conformer_jax.
I0330 05:22:44.469070 140152604170048 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0330 05:22:45.340853 140152604170048 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0330 05:22:45.341463 140152604170048 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0330 05:22:45.348092 140152604170048 submission_runner.py:504] Using RNG seed 3966093301
I0330 05:22:47.795169 140152604170048 submission_runner.py:513] --- Tuning run 1/1 ---
I0330 05:22:47.795380 140152604170048 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1.
I0330 05:22:47.795607 140152604170048 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/hparams.json.
I0330 05:22:47.936034 140152604170048 submission_runner.py:230] Starting train once: RAM USED (GB) 4.320518144
I0330 05:22:47.936204 140152604170048 submission_runner.py:231] Initializing dataset.
I0330 05:22:47.936377 140152604170048 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.320518144
I0330 05:22:47.936439 140152604170048 submission_runner.py:240] Initializing model.
I0330 05:22:54.216204 140152604170048 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.74459392
I0330 05:22:54.216420 140152604170048 submission_runner.py:252] Initializing optimizer.
I0330 05:22:55.101606 140152604170048 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.744528384
I0330 05:22:55.101788 140152604170048 submission_runner.py:261] Initializing metrics bundle.
I0330 05:22:55.101840 140152604170048 submission_runner.py:275] Initializing checkpoint and logger.
I0330 05:22:55.102780 140152604170048 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0330 05:22:55.103048 140152604170048 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0330 05:22:55.103123 140152604170048 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0330 05:22:55.801598 140152604170048 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0330 05:22:55.802453 140152604170048 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/flags_0.json.
I0330 05:22:55.808575 140152604170048 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 7.743397888
I0330 05:22:55.808788 140152604170048 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.743397888
I0330 05:22:55.808854 140152604170048 submission_runner.py:312] Starting training loop.
I0330 05:22:56.018524 140152604170048 input_pipeline.py:20] Loading split = train-clean-100
I0330 05:22:56.055720 140152604170048 input_pipeline.py:20] Loading split = train-clean-360
I0330 05:22:56.401947 140152604170048 input_pipeline.py:20] Loading split = train-other-500
I0330 05:22:59.987656 140152604170048 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 8.29034496
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0330 05:23:58.384121 139976566105856 logging_writer.py:48] [0] global_step=0, grad_norm=84.38898468017578, loss=31.847980499267578
I0330 05:23:58.402827 140152604170048 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 13.148770304
I0330 05:23:58.403125 140152604170048 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 13.148770304
I0330 05:23:58.403217 140152604170048 spec.py:298] Evaluating on the training split.
I0330 05:23:58.512296 140152604170048 input_pipeline.py:20] Loading split = train-clean-100
I0330 05:23:58.732877 140152604170048 input_pipeline.py:20] Loading split = train-clean-360
I0330 05:23:59.070471 140152604170048 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0330 05:24:55.274697 140152604170048 spec.py:310] Evaluating on the validation split.
I0330 05:24:55.339564 140152604170048 input_pipeline.py:20] Loading split = dev-clean
I0330 05:24:55.345339 140152604170048 input_pipeline.py:20] Loading split = dev-other
I0330 05:25:35.547874 140152604170048 spec.py:326] Evaluating on the test split.
I0330 05:25:35.612606 140152604170048 input_pipeline.py:20] Loading split = test-clean
I0330 05:26:06.011065 140152604170048 submission_runner.py:380] Time since start: 62.59s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.081089, dtype=float32), 'train/wer': 0.9755898215438435, 'validation/ctc_loss': DeviceArray(30.660217, dtype=float32), 'validation/wer': 0.9285087169196037, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.706396, dtype=float32), 'test/wer': 0.9353482420327829, 'test/num_examples': 2472}
I0330 05:26:06.012080 140152604170048 submission_runner.py:390] After eval at step 1: RAM USED (GB) 20.0109056
I0330 05:26:06.026296 139972816402176 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=62.384532, test/ctc_loss=30.706396102905273, test/num_examples=2472, test/wer=0.935348, total_duration=62.594292, train/ctc_loss=31.08108901977539, train/wer=0.975590, validation/ctc_loss=30.66021728515625, validation/num_examples=5348, validation/wer=0.928509
I0330 05:26:06.382040 140152604170048 checkpoints.py:356] Saving checkpoint at step: 1
I0330 05:26:07.685010 140152604170048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_1
I0330 05:26:07.710194 140152604170048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_1.
I0330 05:26:07.716891 140152604170048 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 20.275658752
I0330 05:26:07.778874 140152604170048 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 20.274642944
I0330 05:26:26.324248 140152604170048 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 20.547137536
I0330 05:27:42.281683 139977932494592 logging_writer.py:48] [100] global_step=100, grad_norm=1.4578194618225098, loss=6.314972877502441
I0330 05:28:59.353864 139977940887296 logging_writer.py:48] [200] global_step=200, grad_norm=0.8258934617042542, loss=5.857981204986572
I0330 05:30:16.412754 139977932494592 logging_writer.py:48] [300] global_step=300, grad_norm=0.4909331798553467, loss=5.805020332336426
I0330 05:31:33.416857 139977940887296 logging_writer.py:48] [400] global_step=400, grad_norm=1.1682428121566772, loss=5.79763650894165
I0330 05:32:50.478946 139977932494592 logging_writer.py:48] [500] global_step=500, grad_norm=1.0639643669128418, loss=5.767561435699463
I0330 05:34:07.485634 139977940887296 logging_writer.py:48] [600] global_step=600, grad_norm=1.8074949979782104, loss=5.788615703582764
I0330 05:35:24.599291 139977932494592 logging_writer.py:48] [700] global_step=700, grad_norm=1.7602548599243164, loss=5.793944835662842
I0330 05:36:41.755884 139977940887296 logging_writer.py:48] [800] global_step=800, grad_norm=2.4868950843811035, loss=5.808252334594727
I0330 05:37:58.904503 139977932494592 logging_writer.py:48] [900] global_step=900, grad_norm=2.789466619491577, loss=5.775640487670898
I0330 05:39:22.183301 139977940887296 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7089659571647644, loss=5.747386455535889
I0330 05:40:43.592123 139979377497856 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.9375256299972534, loss=5.617721080780029
I0330 05:42:00.687860 139979369105152 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2801154851913452, loss=5.535625457763672
I0330 05:43:17.713379 139979377497856 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.883889675140381, loss=5.345205307006836
I0330 05:44:34.578966 139979369105152 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8964641690254211, loss=4.560057640075684
I0330 05:45:51.486771 139979377497856 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9368467926979065, loss=3.9880337715148926
I0330 05:47:08.431367 139979369105152 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.9056048393249512, loss=3.6733129024505615
I0330 05:48:25.283221 139979377497856 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9822331666946411, loss=3.5149433612823486
I0330 05:49:42.030133 139979369105152 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0311561822891235, loss=3.3886799812316895
I0330 05:50:58.809395 139979377497856 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9987379312515259, loss=3.190701723098755
I0330 05:52:16.646343 139979369105152 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.2662254571914673, loss=3.127473831176758
I0330 05:53:40.973503 139980032857856 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1927863359451294, loss=3.0275771617889404
I0330 05:54:57.585609 139980024465152 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.2185804843902588, loss=2.975282907485962
I0330 05:56:14.346558 139980032857856 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9830958247184753, loss=2.76714825630188
I0330 05:57:31.068494 139980024465152 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.499464511871338, loss=2.8338890075683594
I0330 05:58:47.731733 139980032857856 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9440362453460693, loss=2.7427830696105957
I0330 06:00:04.327403 139980024465152 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9194790124893188, loss=2.763279914855957
I0330 06:01:20.921439 139980032857856 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9472543001174927, loss=2.6762568950653076
I0330 06:02:37.508121 139980024465152 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.0286211967468262, loss=2.607046127319336
I0330 06:03:54.080506 139980032857856 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0518358945846558, loss=2.5786750316619873
I0330 06:05:10.621349 139980024465152 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9341009259223938, loss=2.545289993286133
I0330 06:06:08.034742 140152604170048 submission_runner.py:371] Before eval at step 3074: RAM USED (GB) 23.292633088
I0330 06:06:08.035020 140152604170048 spec.py:298] Evaluating on the training split.
I0330 06:06:53.015434 140152604170048 spec.py:310] Evaluating on the validation split.
I0330 06:07:36.858369 140152604170048 spec.py:326] Evaluating on the test split.
I0330 06:07:58.821911 140152604170048 submission_runner.py:380] Time since start: 2592.22s, 	Step: 3074, 	{'train/ctc_loss': DeviceArray(2.488258, dtype=float32), 'train/wer': 0.5462245885769603, 'validation/ctc_loss': DeviceArray(2.9625845, dtype=float32), 'validation/wer': 0.5974683788555606, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.59663, dtype=float32), 'test/wer': 0.5451627973107469, 'test/num_examples': 2472}
I0330 06:07:58.823276 140152604170048 submission_runner.py:390] After eval at step 3074: RAM USED (GB) 22.618468352
I0330 06:07:58.844413 139980032857856 logging_writer.py:48] [3074] global_step=3074, preemption_count=0, score=2455.004662, test/ctc_loss=2.596630096435547, test/num_examples=2472, test/wer=0.545163, total_duration=2592.223545, train/ctc_loss=2.488257884979248, train/wer=0.546225, validation/ctc_loss=2.9625844955444336, validation/num_examples=5348, validation/wer=0.597468
I0330 06:07:59.166306 140152604170048 checkpoints.py:356] Saving checkpoint at step: 3074
I0330 06:08:00.495092 140152604170048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_3074
I0330 06:08:00.521138 140152604170048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_3074.
I0330 06:08:00.535674 140152604170048 submission_runner.py:409] After logging and checkpointing eval at step 3074: RAM USED (GB) 22.654255104
I0330 06:08:25.357506 139979377497856 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8060763478279114, loss=2.4936795234680176
I0330 06:09:41.817553 139979369105152 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9099122881889343, loss=2.389822483062744
I0330 06:10:58.376677 139979377497856 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8505733013153076, loss=2.412919044494629
I0330 06:12:14.892517 139979369105152 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9668290019035339, loss=2.4004786014556885
I0330 06:13:31.359716 139979377497856 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7385913729667664, loss=2.2850773334503174
I0330 06:14:47.871330 139979369105152 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8381330370903015, loss=2.2959752082824707
I0330 06:16:04.427223 139979377497856 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7667850255966187, loss=2.2576048374176025
I0330 06:17:20.920680 139979369105152 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0647929906845093, loss=2.1818385124206543
I0330 06:18:42.194859 139979377497856 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7089134454727173, loss=2.206585168838501
I0330 06:20:03.283399 139979369105152 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8509653806686401, loss=2.166442394256592
I0330 06:21:23.693606 139979377497856 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6867023706436157, loss=2.0989673137664795
I0330 06:22:44.349265 139979377497856 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6628606915473938, loss=2.06429123878479
I0330 06:24:00.766681 139979369105152 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9914892315864563, loss=2.065549612045288
I0330 06:25:17.197221 139979377497856 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8353161811828613, loss=2.1129488945007324
I0330 06:26:33.570872 139979369105152 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7190484404563904, loss=2.0480387210845947
I0330 06:27:49.932935 139979377497856 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8445976376533508, loss=1.9977774620056152
I0330 06:29:06.299576 139979369105152 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.1165225505828857, loss=2.042296886444092
I0330 06:30:22.670683 139979377497856 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6395576596260071, loss=2.0135538578033447
I0330 06:31:39.999552 139979369105152 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7791566252708435, loss=1.9966368675231934
I0330 06:33:00.919171 139979377497856 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.9802742004394531, loss=1.9671341180801392
I0330 06:34:22.547147 139979369105152 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.579038679599762, loss=1.8932222127914429
I0330 06:35:42.809458 139979377497856 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.619766354560852, loss=1.939063549041748
I0330 06:36:59.130425 139979369105152 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6789411306381226, loss=1.95552396774292
I0330 06:38:15.426123 139979377497856 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7619315385818481, loss=1.9282963275909424
I0330 06:39:31.675439 139979369105152 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6131515502929688, loss=1.8242534399032593
I0330 06:40:47.835688 139979377497856 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6529345512390137, loss=1.8977010250091553
I0330 06:42:03.975708 139979369105152 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6077126860618591, loss=1.8080631494522095
I0330 06:43:20.148325 139979377497856 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9108140468597412, loss=1.8230010271072388
I0330 06:44:36.415615 139979369105152 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7957807779312134, loss=1.8328193426132202
I0330 06:45:52.602362 139979377497856 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5449199676513672, loss=1.8380911350250244
I0330 06:47:08.715512 139979369105152 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6204536557197571, loss=1.863189697265625
I0330 06:48:00.828757 140152604170048 submission_runner.py:371] Before eval at step 6169: RAM USED (GB) 23.255003136
I0330 06:48:00.829000 140152604170048 spec.py:298] Evaluating on the training split.
I0330 06:48:46.532422 140152604170048 spec.py:310] Evaluating on the validation split.
I0330 06:49:29.847155 140152604170048 spec.py:326] Evaluating on the test split.
I0330 06:49:51.588135 140152604170048 submission_runner.py:380] Time since start: 5105.02s, 	Step: 6169, 	{'train/ctc_loss': DeviceArray(0.5742795, dtype=float32), 'train/wer': 0.2009322643368937, 'validation/ctc_loss': DeviceArray(0.9075868, dtype=float32), 'validation/wer': 0.26896545070381767, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6329976, dtype=float32), 'test/wer': 0.20732029329920987, 'test/num_examples': 2472}
I0330 06:49:51.589480 140152604170048 submission_runner.py:390] After eval at step 6169: RAM USED (GB) 22.788190208
I0330 06:49:51.608882 139979377497856 logging_writer.py:48] [6169] global_step=6169, preemption_count=0, score=4847.544782, test/ctc_loss=0.6329975724220276, test/num_examples=2472, test/wer=0.207320, total_duration=5105.017547, train/ctc_loss=0.5742794871330261, train/wer=0.200932, validation/ctc_loss=0.9075868129730225, validation/num_examples=5348, validation/wer=0.268965
I0330 06:49:51.926843 140152604170048 checkpoints.py:356] Saving checkpoint at step: 6169
I0330 06:49:53.271360 140152604170048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_6169
I0330 06:49:53.302703 140152604170048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_6169.
I0330 06:49:53.317961 140152604170048 submission_runner.py:409] After logging and checkpointing eval at step 6169: RAM USED (GB) 22.808731648
I0330 06:50:21.307129 139980032857856 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5215165019035339, loss=1.8158832788467407
I0330 06:51:37.377896 139980024465152 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6551706194877625, loss=1.8437985181808472
I0330 06:52:53.544905 139980032857856 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5534507036209106, loss=1.7674410343170166
I0330 06:54:09.666221 139980024465152 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6070119142532349, loss=1.8031620979309082
I0330 06:55:25.724049 139980032857856 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5848137736320496, loss=1.7927818298339844
I0330 06:56:41.722340 139980024465152 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6661626100540161, loss=1.7518140077590942
I0330 06:57:57.826123 139980032857856 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6360952258110046, loss=1.8102130889892578
I0330 06:59:17.420938 139980024465152 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5244998335838318, loss=1.7722868919372559
I0330 07:00:37.836046 139980032857856 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6721402406692505, loss=1.7952390909194946
I0330 07:01:58.570363 139980024465152 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5166673064231873, loss=1.7393677234649658
I0330 07:03:17.425274 139980032857856 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.45651301741600037, loss=1.7202080488204956
I0330 07:04:38.317849 139979377497856 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5100811719894409, loss=1.6633541584014893
I0330 07:05:54.319040 139979369105152 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4446212947368622, loss=1.6764895915985107
I0330 07:07:10.356521 139979377497856 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5863908529281616, loss=1.6668751239776611
I0330 07:08:26.389937 139979369105152 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.49479660391807556, loss=1.7175151109695435
I0330 07:09:42.373733 139979377497856 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.49153631925582886, loss=1.7084935903549194
I0330 07:10:58.476635 139979369105152 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.42126792669296265, loss=1.7031941413879395
I0330 07:12:19.395226 139979377497856 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4507420063018799, loss=1.664663314819336
I0330 07:13:44.311868 139979369105152 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.45628929138183594, loss=1.648422122001648
I0330 07:15:04.197367 139979377497856 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6889747381210327, loss=1.6821496486663818
I0330 07:16:22.979012 139979369105152 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6850676536560059, loss=1.630640983581543
I0330 07:17:44.622313 139979377497856 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4823780655860901, loss=1.6411257982254028
I0330 07:19:00.583854 139979369105152 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.4084165096282959, loss=1.6523678302764893
I0330 07:20:16.556189 139979377497856 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.4992872178554535, loss=1.6990519762039185
I0330 07:21:32.623636 139979369105152 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6135161519050598, loss=1.6445003747940063
I0330 07:22:48.648902 139979377497856 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5148611664772034, loss=1.6794352531433105
I0330 07:24:04.706328 139979369105152 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.4481990933418274, loss=1.6830644607543945
I0330 07:25:20.791704 139979377497856 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6580595970153809, loss=1.5874712467193604
I0330 07:26:36.882709 139979369105152 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.42397722601890564, loss=1.5188212394714355
I0330 07:27:53.031659 139979377497856 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.36076799035072327, loss=1.572137475013733
I0330 07:29:12.015683 139979369105152 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5082840919494629, loss=1.6543179750442505
I0330 07:29:53.782532 140152604170048 submission_runner.py:371] Before eval at step 9252: RAM USED (GB) 24.005214208
I0330 07:29:53.782744 140152604170048 spec.py:298] Evaluating on the training split.
I0330 07:30:39.931738 140152604170048 spec.py:310] Evaluating on the validation split.
I0330 07:31:23.879324 140152604170048 spec.py:326] Evaluating on the test split.
I0330 07:31:46.809538 140152604170048 submission_runner.py:380] Time since start: 7617.97s, 	Step: 9252, 	{'train/ctc_loss': DeviceArray(0.3740169, dtype=float32), 'train/wer': 0.1384368855785906, 'validation/ctc_loss': DeviceArray(0.71463543, dtype=float32), 'validation/wer': 0.2143870177232776, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4600629, dtype=float32), 'test/wer': 0.15485548311092154, 'test/num_examples': 2472}
I0330 07:31:46.810823 140152604170048 submission_runner.py:390] After eval at step 9252: RAM USED (GB) 22.674481152
I0330 07:31:46.830510 139979377497856 logging_writer.py:48] [9252] global_step=9252, preemption_count=0, score=7240.438764, test/ctc_loss=0.4600628912448883, test/num_examples=2472, test/wer=0.154855, total_duration=7617.971366, train/ctc_loss=0.3740169107913971, train/wer=0.138437, validation/ctc_loss=0.71463543176651, validation/num_examples=5348, validation/wer=0.214387
I0330 07:31:47.147332 140152604170048 checkpoints.py:356] Saving checkpoint at step: 9252
I0330 07:31:48.492622 140152604170048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_9252
I0330 07:31:48.523810 140152604170048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_9252.
I0330 07:31:48.539102 140152604170048 submission_runner.py:409] After logging and checkpointing eval at step 9252: RAM USED (GB) 22.693588992
I0330 07:32:29.451788 139978722137856 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.43155109882354736, loss=1.5884277820587158
I0330 07:33:45.421633 139978713745152 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5642895698547363, loss=1.5912365913391113
I0330 07:35:01.418223 139978722137856 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.41464927792549133, loss=1.5610982179641724
I0330 07:36:17.453292 139978713745152 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.42991068959236145, loss=1.5495359897613525
I0330 07:37:33.509067 139978722137856 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.41143766045570374, loss=1.5790128707885742
I0330 07:38:49.448199 139978713745152 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.4004226326942444, loss=1.622267484664917
I0330 07:40:05.477273 139978722137856 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.42898648977279663, loss=1.631523847579956
I0330 07:41:21.149716 140152604170048 submission_runner.py:371] Before eval at step 10000: RAM USED (GB) 23.673827328
I0330 07:41:21.149947 140152604170048 spec.py:298] Evaluating on the training split.
I0330 07:42:07.541340 140152604170048 spec.py:310] Evaluating on the validation split.
I0330 07:42:50.811721 140152604170048 spec.py:326] Evaluating on the test split.
I0330 07:43:12.357603 140152604170048 submission_runner.py:380] Time since start: 8305.34s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(0.38563475, dtype=float32), 'train/wer': 0.13828817979034233, 'validation/ctc_loss': DeviceArray(0.6996923, dtype=float32), 'validation/wer': 0.20796148539783307, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45154586, dtype=float32), 'test/wer': 0.1525602746125566, 'test/num_examples': 2472}
I0330 07:43:12.358926 140152604170048 submission_runner.py:390] After eval at step 10000: RAM USED (GB) 22.439784448
I0330 07:43:12.376608 139979602777856 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7811.199404, test/ctc_loss=0.4515458643436432, test/num_examples=2472, test/wer=0.152560, total_duration=8305.339167, train/ctc_loss=0.38563475012779236, train/wer=0.138288, validation/ctc_loss=0.6996923089027405, validation/num_examples=5348, validation/wer=0.207961
I0330 07:43:12.684679 140152604170048 checkpoints.py:356] Saving checkpoint at step: 10000
I0330 07:43:13.996036 140152604170048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_10000
I0330 07:43:14.026768 140152604170048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0330 07:43:14.042059 140152604170048 submission_runner.py:409] After logging and checkpointing eval at step 10000: RAM USED (GB) 22.458146816
I0330 07:43:14.050181 139979594385152 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7811.199404
I0330 07:43:14.268193 140152604170048 checkpoints.py:356] Saving checkpoint at step: 10000
I0330 07:43:16.054599 140152604170048 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_10000
I0330 07:43:16.085703 140152604170048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0330 07:43:17.287117 140152604170048 submission_runner.py:543] Tuning trial 1/1
I0330 07:43:17.287360 140152604170048 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0330 07:43:17.292392 140152604170048 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.081089, dtype=float32), 'train/wer': 0.9755898215438435, 'validation/ctc_loss': DeviceArray(30.660217, dtype=float32), 'validation/wer': 0.9285087169196037, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.706396, dtype=float32), 'test/wer': 0.9353482420327829, 'test/num_examples': 2472, 'score': 62.38453245162964, 'total_duration': 62.59429168701172, 'global_step': 1, 'preemption_count': 0}), (3074, {'train/ctc_loss': DeviceArray(2.488258, dtype=float32), 'train/wer': 0.5462245885769603, 'validation/ctc_loss': DeviceArray(2.9625845, dtype=float32), 'validation/wer': 0.5974683788555606, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.59663, dtype=float32), 'test/wer': 0.5451627973107469, 'test/num_examples': 2472, 'score': 2455.0046622753143, 'total_duration': 2592.2235448360443, 'global_step': 3074, 'preemption_count': 0}), (6169, {'train/ctc_loss': DeviceArray(0.5742795, dtype=float32), 'train/wer': 0.2009322643368937, 'validation/ctc_loss': DeviceArray(0.9075868, dtype=float32), 'validation/wer': 0.26896545070381767, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6329976, dtype=float32), 'test/wer': 0.20732029329920987, 'test/num_examples': 2472, 'score': 4847.5447816848755, 'total_duration': 5105.017547369003, 'global_step': 6169, 'preemption_count': 0}), (9252, {'train/ctc_loss': DeviceArray(0.3740169, dtype=float32), 'train/wer': 0.1384368855785906, 'validation/ctc_loss': DeviceArray(0.71463543, dtype=float32), 'validation/wer': 0.2143870177232776, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4600629, dtype=float32), 'test/wer': 0.15485548311092154, 'test/num_examples': 2472, 'score': 7240.438764333725, 'total_duration': 7617.971365690231, 'global_step': 9252, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(0.38563475, dtype=float32), 'train/wer': 0.13828817979034233, 'validation/ctc_loss': DeviceArray(0.6996923, dtype=float32), 'validation/wer': 0.20796148539783307, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45154586, dtype=float32), 'test/wer': 0.1525602746125566, 'test/num_examples': 2472, 'score': 7811.199404001236, 'total_duration': 8305.339167118073, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0330 07:43:17.292557 140152604170048 submission_runner.py:546] Timing: 7811.199404001236
I0330 07:43:17.292609 140152604170048 submission_runner.py:547] ====================
I0330 07:43:17.292933 140152604170048 submission_runner.py:606] Final librispeech_conformer score: 7811.199404001236
