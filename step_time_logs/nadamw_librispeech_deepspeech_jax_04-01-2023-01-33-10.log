I0401 01:33:24.962307 139920213247808 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/librispeech_deepspeech_jax.
I0401 01:33:25.023680 139920213247808 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0401 01:33:25.971176 139920213247808 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0401 01:33:25.971913 139920213247808 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0401 01:33:25.975410 139920213247808 submission_runner.py:511] Using RNG seed 2286312552
I0401 01:33:27.343073 139920213247808 submission_runner.py:520] --- Tuning run 1/1 ---
I0401 01:33:27.343264 139920213247808 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1.
I0401 01:33:27.343447 139920213247808 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/hparams.json.
I0401 01:33:27.466637 139920213247808 submission_runner.py:230] Starting train once: RAM USED (GB) 4.575010816
I0401 01:33:27.466801 139920213247808 submission_runner.py:231] Initializing dataset.
I0401 01:33:27.466957 139920213247808 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.575010816
I0401 01:33:27.467013 139920213247808 submission_runner.py:240] Initializing model.
I0401 01:33:43.708019 139920213247808 submission_runner.py:251] After Initializing model: RAM USED (GB) 9.079103488
I0401 01:33:43.708223 139920213247808 submission_runner.py:252] Initializing optimizer.
I0401 01:33:44.459619 139920213247808 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 9.085575168
I0401 01:33:44.459801 139920213247808 submission_runner.py:261] Initializing metrics bundle.
I0401 01:33:44.459850 139920213247808 submission_runner.py:276] Initializing checkpoint and logger.
I0401 01:33:44.462518 139920213247808 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0401 01:33:44.462814 139920213247808 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 01:33:44.462881 139920213247808 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 01:33:45.734736 139920213247808 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0401 01:33:45.735687 139920213247808 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0401 01:33:45.739777 139920213247808 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 9.092796416
I0401 01:33:45.740012 139920213247808 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 9.092796416
I0401 01:33:45.740078 139920213247808 submission_runner.py:313] Starting training loop.
I0401 01:33:45.944950 139920213247808 input_pipeline.py:20] Loading split = train-clean-100
I0401 01:33:45.983362 139920213247808 input_pipeline.py:20] Loading split = train-clean-360
I0401 01:33:46.370722 139920213247808 input_pipeline.py:20] Loading split = train-other-500
I0401 01:33:50.420971 139920213247808 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 9.886052352
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0401 01:34:47.099688 139742481995520 logging_writer.py:48] [0] global_step=0, grad_norm=19.85755157470703, loss=32.40348815917969
I0401 01:34:47.116819 139920213247808 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 14.347411456
I0401 01:34:47.117086 139920213247808 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 14.347411456
I0401 01:34:47.117179 139920213247808 spec.py:298] Evaluating on the training split.
I0401 01:34:47.245830 139920213247808 input_pipeline.py:20] Loading split = train-clean-100
I0401 01:34:47.500497 139920213247808 input_pipeline.py:20] Loading split = train-clean-360
I0401 01:34:47.600406 139920213247808 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0401 01:36:30.286925 139920213247808 spec.py:310] Evaluating on the validation split.
I0401 01:36:30.376253 139920213247808 input_pipeline.py:20] Loading split = dev-clean
I0401 01:36:30.381411 139920213247808 input_pipeline.py:20] Loading split = dev-other
I0401 01:37:30.597682 139920213247808 spec.py:326] Evaluating on the test split.
I0401 01:37:30.688269 139920213247808 input_pipeline.py:20] Loading split = test-clean
I0401 01:38:09.151639 139920213247808 submission_runner.py:382] Time since start: 61.38s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.655176, dtype=float32), 'train/wer': 5.1693979967932036, 'validation/ctc_loss': DeviceArray(30.083109, dtype=float32), 'validation/wer': 4.88757248019759, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.383625, dtype=float32), 'test/wer': 5.046086974183982, 'test/num_examples': 2472}
I0401 01:38:09.152998 139920213247808 submission_runner.py:396] After eval at step 1: RAM USED (GB) 19.605815296
I0401 01:38:09.168776 139740485515008 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=61.172102, test/ctc_loss=30.383625030517578, test/num_examples=2472, test/wer=5.046087, total_duration=61.377071, train/ctc_loss=31.655176162719727, train/wer=5.169398, validation/ctc_loss=30.08310890197754, validation/num_examples=5348, validation/wer=4.887572
I0401 01:38:09.313354 139920213247808 checkpoints.py:356] Saving checkpoint at step: 1
I0401 01:38:09.999394 139920213247808 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0401 01:38:10.010482 139920213247808 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0401 01:38:10.017915 139920213247808 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 19.555659776
I0401 01:38:10.063473 139920213247808 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 19.553595392
I0401 01:38:31.000787 139920213247808 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 19.95143168
I0401 01:40:28.025988 139744209106688 logging_writer.py:48] [100] global_step=100, grad_norm=5.588249683380127, loss=9.072707176208496
I0401 01:42:26.448429 139744217499392 logging_writer.py:48] [200] global_step=200, grad_norm=1.0190120935440063, loss=6.543152809143066
I0401 01:44:25.262554 139744209106688 logging_writer.py:48] [300] global_step=300, grad_norm=1.0275299549102783, loss=6.051272392272949
I0401 01:46:24.762952 139744217499392 logging_writer.py:48] [400] global_step=400, grad_norm=0.671940803527832, loss=5.910064220428467
I0401 01:48:24.174544 139744209106688 logging_writer.py:48] [500] global_step=500, grad_norm=0.5815101861953735, loss=5.835578441619873
I0401 01:50:23.431251 139744217499392 logging_writer.py:48] [600] global_step=600, grad_norm=0.5797622203826904, loss=5.734785079956055
I0401 01:52:22.262888 139744209106688 logging_writer.py:48] [700] global_step=700, grad_norm=0.4359753429889679, loss=5.665745258331299
I0401 01:54:20.917644 139744217499392 logging_writer.py:48] [800] global_step=800, grad_norm=0.5305796265602112, loss=5.549388408660889
I0401 01:56:20.068932 139744209106688 logging_writer.py:48] [900] global_step=900, grad_norm=0.8142136335372925, loss=5.381991386413574
I0401 01:58:19.912232 139744217499392 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7012147307395935, loss=5.059700012207031
I0401 02:00:22.138759 139746292684544 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7895716428756714, loss=4.679912567138672
I0401 02:02:20.094611 139746284291840 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1263517141342163, loss=4.354610443115234
I0401 02:04:17.963292 139746292684544 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.106452465057373, loss=4.128364562988281
I0401 02:06:15.949649 139746284291840 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.514018177986145, loss=3.865832567214966
I0401 02:08:14.345843 139746292684544 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.7573150396347046, loss=3.7193894386291504
I0401 02:10:12.106676 139746284291840 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.702915668487549, loss=3.586742401123047
I0401 02:12:10.370572 139746292684544 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.402637243270874, loss=3.4935379028320312
I0401 02:14:08.792459 139746284291840 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.1423537731170654, loss=3.3224315643310547
I0401 02:16:06.289687 139746292684544 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.337613582611084, loss=3.2374868392944336
I0401 02:18:04.797961 139746284291840 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.188358783721924, loss=3.1392264366149902
I0401 02:18:10.532366 139920213247808 submission_runner.py:373] Before eval at step 2006: RAM USED (GB) 20.186943488
I0401 02:18:10.532571 139920213247808 spec.py:298] Evaluating on the training split.
I0401 02:18:40.025473 139920213247808 spec.py:310] Evaluating on the validation split.
I0401 02:19:16.380612 139920213247808 spec.py:326] Evaluating on the test split.
I0401 02:19:34.995334 139920213247808 submission_runner.py:382] Time since start: 2664.79s, 	Step: 2006, 	{'train/ctc_loss': DeviceArray(5.989604, dtype=float32), 'train/wer': 0.9397643714856543, 'validation/ctc_loss': DeviceArray(5.980784, dtype=float32), 'validation/wer': 0.8938822371658193, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.862755, dtype=float32), 'test/wer': 0.8971218491662096, 'test/num_examples': 2472}
I0401 02:19:34.996555 139920213247808 submission_runner.py:396] After eval at step 2006: RAM USED (GB) 18.931355648
I0401 02:19:35.016917 139746292684544 logging_writer.py:48] [2006] global_step=2006, preemption_count=0, score=2457.394233, test/ctc_loss=5.862754821777344, test/num_examples=2472, test/wer=0.897122, total_duration=2664.788478, train/ctc_loss=5.9896039962768555, train/wer=0.939764, validation/ctc_loss=5.980783939361572, validation/num_examples=5348, validation/wer=0.893882
I0401 02:19:35.175874 139920213247808 checkpoints.py:356] Saving checkpoint at step: 2006
I0401 02:19:35.840393 139920213247808 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_2006
I0401 02:19:35.855752 139920213247808 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_2006.
I0401 02:19:35.863058 139920213247808 submission_runner.py:416] After logging and checkpointing eval at step 2006: RAM USED (GB) 18.902818816
I0401 02:21:32.512742 139746292684544 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.987757682800293, loss=3.0328609943389893
I0401 02:23:30.690540 139746284291840 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.2557733058929443, loss=2.9484214782714844
I0401 02:25:30.276897 139746292684544 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.2914464473724365, loss=2.8504605293273926
I0401 02:27:29.560462 139746284291840 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.7266011238098145, loss=2.8982093334198
I0401 02:29:26.831327 139746292684544 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.3719191551208496, loss=2.6988298892974854
I0401 02:31:24.403172 139746284291840 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.563429117202759, loss=2.7062039375305176
I0401 02:33:22.133157 139746292684544 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.0201852321624756, loss=2.6533260345458984
I0401 02:35:19.712705 139746284291840 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.1009321212768555, loss=2.6558618545532227
I0401 02:37:17.243481 139746292684544 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.9328930377960205, loss=2.580399990081787
I0401 02:39:14.751907 139746284291840 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.678947925567627, loss=2.5150208473205566
I0401 02:41:16.305296 139746292684544 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.3113491535186768, loss=2.5308098793029785
I0401 02:43:13.985929 139746284291840 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.4098498821258545, loss=2.460310935974121
I0401 02:45:11.468175 139746292684544 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.019349098205566, loss=2.484915018081665
I0401 02:47:09.117214 139746284291840 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.473809719085693, loss=2.446063280105591
I0401 02:49:07.588346 139746292684544 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.541663646697998, loss=2.4009294509887695
I0401 02:51:06.958382 139746284291840 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.937337875366211, loss=2.27146577835083
I0401 02:53:05.142257 139746292684544 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.4499199390411377, loss=2.3022003173828125
I0401 02:55:02.430660 139746284291840 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.7196102142333984, loss=2.304593801498413
I0401 02:57:00.056136 139746292684544 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.7142229080200195, loss=2.2477867603302
I0401 02:58:57.656510 139746284291840 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.878938913345337, loss=2.321686029434204
I0401 02:59:36.236093 139920213247808 submission_runner.py:373] Before eval at step 4034: RAM USED (GB) 20.090683392
I0401 02:59:36.236289 139920213247808 spec.py:298] Evaluating on the training split.
I0401 03:00:12.233812 139920213247808 spec.py:310] Evaluating on the validation split.
I0401 03:00:49.203521 139920213247808 spec.py:326] Evaluating on the test split.
I0401 03:01:09.253365 139920213247808 submission_runner.py:382] Time since start: 5150.49s, 	Step: 4034, 	{'train/ctc_loss': DeviceArray(2.5171034, dtype=float32), 'train/wer': 0.5953812961137895, 'validation/ctc_loss': DeviceArray(2.9472437, dtype=float32), 'validation/wer': 0.6356838946830167, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.4034386, dtype=float32), 'test/wer': 0.5636463347754554, 'test/num_examples': 2472}
I0401 03:01:09.254649 139920213247808 submission_runner.py:396] After eval at step 4034: RAM USED (GB) 19.234349056
I0401 03:01:09.273325 139745709004544 logging_writer.py:48] [4034] global_step=4034, preemption_count=0, score=4853.482800, test/ctc_loss=2.4034385681152344, test/num_examples=2472, test/wer=0.563646, total_duration=5150.491861, train/ctc_loss=2.517103433609009, train/wer=0.595381, validation/ctc_loss=2.9472436904907227, validation/num_examples=5348, validation/wer=0.635684
I0401 03:01:09.433234 139920213247808 checkpoints.py:356] Saving checkpoint at step: 4034
I0401 03:01:10.097134 139920213247808 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_4034
I0401 03:01:10.112841 139920213247808 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_4034.
I0401 03:01:10.120259 139920213247808 submission_runner.py:416] After logging and checkpointing eval at step 4034: RAM USED (GB) 19.20141312
I0401 03:02:28.534548 139745700611840 logging_writer.py:48] [4100] global_step=4100, grad_norm=4.66987419128418, loss=2.2792272567749023
I0401 03:04:29.826802 139745709004544 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.872678756713867, loss=2.1762330532073975
I0401 03:06:26.936167 139745700611840 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.698826789855957, loss=2.2273974418640137
I0401 03:08:24.793782 139745709004544 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.970078945159912, loss=2.1141164302825928
I0401 03:10:22.507956 139745700611840 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.324798107147217, loss=2.1389663219451904
I0401 03:12:20.315347 139745709004544 logging_writer.py:48] [4600] global_step=4600, grad_norm=5.081162929534912, loss=2.1628127098083496
I0401 03:14:18.591723 139745700611840 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.2144112586975098, loss=2.103407382965088
I0401 03:16:16.535625 139745709004544 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.7673721313476562, loss=2.1285197734832764
I0401 03:18:14.380890 139745700611840 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.315021991729736, loss=2.1559250354766846
I0401 03:20:12.288260 139745709004544 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.1212220191955566, loss=2.0582149028778076
I0401 03:22:10.849272 139745700611840 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.624729633331299, loss=2.033151626586914
I0401 03:24:13.280944 139745053644544 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.1754908561706543, loss=2.0370194911956787
I0401 03:26:11.813605 139745045251840 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.980438232421875, loss=2.0480494499206543
I0401 03:28:10.247276 139745053644544 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.5273919105529785, loss=1.9848965406417847
I0401 03:30:08.326284 139745045251840 logging_writer.py:48] [5500] global_step=5500, grad_norm=6.599161624908447, loss=1.997348427772522
I0401 03:32:06.846664 139745053644544 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.717771530151367, loss=2.0202507972717285
I0401 03:34:04.952743 139745045251840 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.775609254837036, loss=1.96821928024292
I0401 03:36:02.510169 139745053644544 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.5662646293640137, loss=1.9209249019622803
I0401 03:37:59.999851 139745045251840 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.470801830291748, loss=1.9218926429748535
I0401 03:39:58.265405 139745053644544 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.937439203262329, loss=1.959837555885315
I0401 03:41:11.191351 139920213247808 submission_runner.py:373] Before eval at step 6063: RAM USED (GB) 20.174077952
I0401 03:41:11.191548 139920213247808 spec.py:298] Evaluating on the training split.
I0401 03:41:50.322249 139920213247808 spec.py:310] Evaluating on the validation split.
I0401 03:42:29.882064 139920213247808 spec.py:326] Evaluating on the test split.
I0401 03:42:50.766221 139920213247808 submission_runner.py:382] Time since start: 7645.45s, 	Step: 6063, 	{'train/ctc_loss': DeviceArray(0.6671563, dtype=float32), 'train/wer': 0.23441972263073832, 'validation/ctc_loss': DeviceArray(1.105752, dtype=float32), 'validation/wer': 0.31036478885469226, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.74553704, dtype=float32), 'test/wer': 0.24193122499136757, 'test/num_examples': 2472}
I0401 03:42:50.767615 139920213247808 submission_runner.py:396] After eval at step 6063: RAM USED (GB) 19.066085376
I0401 03:42:50.788174 139745053644544 logging_writer.py:48] [6063] global_step=6063, preemption_count=0, score=7250.108556, test/ctc_loss=0.7455370426177979, test/num_examples=2472, test/wer=0.241931, total_duration=7645.447008, train/ctc_loss=0.6671562790870667, train/wer=0.234420, validation/ctc_loss=1.1057519912719727, validation/num_examples=5348, validation/wer=0.310365
I0401 03:42:50.950409 139920213247808 checkpoints.py:356] Saving checkpoint at step: 6063
I0401 03:42:51.701574 139920213247808 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_6063
I0401 03:42:51.716491 139920213247808 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_6063.
I0401 03:42:51.723975 139920213247808 submission_runner.py:416] After logging and checkpointing eval at step 6063: RAM USED (GB) 19.03562752
I0401 03:43:36.592659 139745045251840 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.8397748470306396, loss=1.9395556449890137
I0401 03:45:38.561201 139745709004544 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.0147571563720703, loss=1.9352291822433472
I0401 03:47:36.003234 139745700611840 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.9082040786743164, loss=1.9418312311172485
I0401 03:49:32.949465 139745709004544 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.8844196796417236, loss=1.9085679054260254
I0401 03:51:30.472096 139745700611840 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.6477584838867188, loss=1.9221453666687012
I0401 03:53:27.901348 139745709004544 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.3685195446014404, loss=1.900869607925415
I0401 03:55:25.448841 139745700611840 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.3526527881622314, loss=1.9297292232513428
I0401 03:57:23.194757 139745709004544 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.7703099250793457, loss=1.9494247436523438
I0401 03:59:20.903404 139745700611840 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.4759840965270996, loss=1.9053705930709839
I0401 04:01:18.696016 139745709004544 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.2406904697418213, loss=1.805037498474121
I0401 04:03:17.181120 139745700611840 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.9870638847351074, loss=1.9253129959106445
I0401 04:05:14.699020 139745709004544 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.6109042167663574, loss=1.8611705303192139
I0401 04:07:16.714054 139745053644544 logging_writer.py:48] [7300] global_step=7300, grad_norm=4.095022678375244, loss=1.8218039274215698
I0401 04:09:14.003055 139745045251840 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.277831554412842, loss=1.815702199935913
I0401 04:11:11.341404 139745053644544 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.552295207977295, loss=1.7969216108322144
I0401 04:13:09.239302 139745045251840 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.5429341793060303, loss=1.8375811576843262
I0401 04:15:06.810619 139745053644544 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.0948290824890137, loss=1.8203072547912598
I0401 04:17:04.597836 139745045251840 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.778827667236328, loss=1.856826663017273
I0401 04:19:01.748200 139745053644544 logging_writer.py:48] [7900] global_step=7900, grad_norm=4.800364017486572, loss=1.825692892074585
I0401 04:20:57.791041 139920213247808 submission_runner.py:373] Before eval at step 8000: RAM USED (GB) 20.380540928
I0401 04:20:57.791251 139920213247808 spec.py:298] Evaluating on the training split.
I0401 04:21:37.283423 139920213247808 spec.py:310] Evaluating on the validation split.
I0401 04:22:16.417430 139920213247808 spec.py:326] Evaluating on the test split.
I0401 04:22:36.896970 139920213247808 submission_runner.py:382] Time since start: 10032.05s, 	Step: 8000, 	{'train/ctc_loss': DeviceArray(0.543008, dtype=float32), 'train/wer': 0.18708961241608982, 'validation/ctc_loss': DeviceArray(0.92615396, dtype=float32), 'validation/wer': 0.26582986811257225, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5889816, dtype=float32), 'test/wer': 0.19464586760912397, 'test/num_examples': 2472}
I0401 04:22:36.898392 139920213247808 submission_runner.py:396] After eval at step 8000: RAM USED (GB) 19.695071232
I0401 04:22:36.918900 139744183236352 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9532.060521, test/ctc_loss=0.5889816284179688, test/num_examples=2472, test/wer=0.194646, total_duration=10032.046447, train/ctc_loss=0.543008029460907, train/wer=0.187090, validation/ctc_loss=0.9261539578437805, validation/num_examples=5348, validation/wer=0.265830
I0401 04:22:37.075669 139920213247808 checkpoints.py:356] Saving checkpoint at step: 8000
I0401 04:22:37.742378 139920213247808 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0401 04:22:37.757758 139920213247808 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0401 04:22:37.765140 139920213247808 submission_runner.py:416] After logging and checkpointing eval at step 8000: RAM USED (GB) 19.665129472
I0401 04:22:37.771981 139744174843648 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9532.060521
I0401 04:22:37.896723 139920213247808 checkpoints.py:356] Saving checkpoint at step: 8000
I0401 04:22:38.840379 139920213247808 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0401 04:22:38.855881 139920213247808 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0401 04:22:40.193740 139920213247808 submission_runner.py:550] Tuning trial 1/1
I0401 04:22:40.193953 139920213247808 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0401 04:22:40.198821 139920213247808 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.655176, dtype=float32), 'train/wer': 5.1693979967932036, 'validation/ctc_loss': DeviceArray(30.083109, dtype=float32), 'validation/wer': 4.88757248019759, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.383625, dtype=float32), 'test/wer': 5.046086974183982, 'test/num_examples': 2472, 'score': 61.172102212905884, 'total_duration': 61.377071142196655, 'global_step': 1, 'preemption_count': 0}), (2006, {'train/ctc_loss': DeviceArray(5.989604, dtype=float32), 'train/wer': 0.9397643714856543, 'validation/ctc_loss': DeviceArray(5.980784, dtype=float32), 'validation/wer': 0.8938822371658193, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.862755, dtype=float32), 'test/wer': 0.8971218491662096, 'test/num_examples': 2472, 'score': 2457.3942334651947, 'total_duration': 2664.788477897644, 'global_step': 2006, 'preemption_count': 0}), (4034, {'train/ctc_loss': DeviceArray(2.5171034, dtype=float32), 'train/wer': 0.5953812961137895, 'validation/ctc_loss': DeviceArray(2.9472437, dtype=float32), 'validation/wer': 0.6356838946830167, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.4034386, dtype=float32), 'test/wer': 0.5636463347754554, 'test/num_examples': 2472, 'score': 4853.482799768448, 'total_duration': 5150.491860628128, 'global_step': 4034, 'preemption_count': 0}), (6063, {'train/ctc_loss': DeviceArray(0.6671563, dtype=float32), 'train/wer': 0.23441972263073832, 'validation/ctc_loss': DeviceArray(1.105752, dtype=float32), 'validation/wer': 0.31036478885469226, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.74553704, dtype=float32), 'test/wer': 0.24193122499136757, 'test/num_examples': 2472, 'score': 7250.108556032181, 'total_duration': 7645.447008371353, 'global_step': 6063, 'preemption_count': 0}), (8000, {'train/ctc_loss': DeviceArray(0.543008, dtype=float32), 'train/wer': 0.18708961241608982, 'validation/ctc_loss': DeviceArray(0.92615396, dtype=float32), 'validation/wer': 0.26582986811257225, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5889816, dtype=float32), 'test/wer': 0.19464586760912397, 'test/num_examples': 2472, 'score': 9532.060521364212, 'total_duration': 10032.046446800232, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0401 04:22:40.199018 139920213247808 submission_runner.py:553] Timing: 9532.060521364212
I0401 04:22:40.199076 139920213247808 submission_runner.py:554] ====================
I0401 04:22:40.199413 139920213247808 submission_runner.py:613] Final librispeech_deepspeech score: 9532.060521364212
