WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0401 02:42:34.438030 139661500798784 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0401 02:42:34.473443 140388431898432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0401 02:42:34.474208 140261786756928 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0401 02:42:34.474231 139847888303936 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0401 02:42:34.474250 139886737520448 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0401 02:42:34.475449 139812532709184 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0401 02:42:34.475572 139788367148864 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0401 02:42:34.484442 140663602321216 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0401 02:42:34.484772 140663602321216 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:42:34.484846 139886737520448 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:42:34.484880 140261786756928 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:42:34.484915 139847888303936 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:42:34.486047 139812532709184 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:42:34.486102 139788367148864 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:42:34.493966 139661500798784 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:42:34.494324 140388431898432 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 02:42:38.334618 140663602321216 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/wmt_pytorch.
W0401 02:42:38.367494 139661500798784 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:42:38.368287 140663602321216 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:42:38.368560 139847888303936 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:42:38.369351 140388431898432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:42:38.369437 139812532709184 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:42:38.369612 140261786756928 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:42:38.369723 139788367148864 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 02:42:38.370091 139886737520448 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0401 02:42:38.371823 140663602321216 submission_runner.py:504] Using RNG seed 203339024
I0401 02:42:38.372860 140663602321216 submission_runner.py:513] --- Tuning run 1/1 ---
I0401 02:42:38.372972 140663602321216 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/wmt_pytorch/trial_1.
I0401 02:42:38.373204 140663602321216 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/wmt_pytorch/trial_1/hparams.json.
I0401 02:42:38.374126 140663602321216 submission_runner.py:230] Starting train once: RAM USED (GB) 15.251709952
I0401 02:42:38.374222 140663602321216 submission_runner.py:231] Initializing dataset.
I0401 02:42:38.374421 140663602321216 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 15.251709952
I0401 02:42:38.374479 140663602321216 submission_runner.py:240] Initializing model.
I0401 02:42:41.887275 140663602321216 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.537670144
I0401 02:42:41.887458 140663602321216 submission_runner.py:252] Initializing optimizer.
I0401 02:42:41.888601 140663602321216 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.537666048
I0401 02:42:41.888722 140663602321216 submission_runner.py:261] Initializing metrics bundle.
I0401 02:42:41.888774 140663602321216 submission_runner.py:275] Initializing checkpoint and logger.
I0401 02:42:41.890180 140663602321216 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 02:42:41.890302 140663602321216 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 02:42:42.519116 140663602321216 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/wmt_pytorch/trial_1/meta_data_0.json.
I0401 02:42:42.519956 140663602321216 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/wmt_pytorch/trial_1/flags_0.json.
I0401 02:42:42.552787 140663602321216 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 19.59172096
I0401 02:42:42.553938 140663602321216 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.59172096
I0401 02:42:42.554067 140663602321216 submission_runner.py:312] Starting training loop.
I0401 02:42:42.564371 140663602321216 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0401 02:42:42.568705 140663602321216 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0401 02:42:42.568851 140663602321216 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0401 02:42:42.620536 140663602321216 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0401 02:42:44.763891 140663602321216 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 19.879079936
I0401 02:42:46.381067 140623696865024 logging_writer.py:48] [0] global_step=0, grad_norm=5.673095, loss=11.146623
I0401 02:42:46.385299 140663602321216 submission.py:119] 0) loss = 11.147, grad_norm = 5.673
I0401 02:42:46.386115 140663602321216 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 19.980926976
I0401 02:42:46.402956 140663602321216 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 19.980926976
I0401 02:42:46.403078 140663602321216 spec.py:298] Evaluating on the training split.
I0401 02:42:46.404905 140663602321216 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0401 02:42:46.407170 140663602321216 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0401 02:42:46.407273 140663602321216 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0401 02:42:46.434765 140663602321216 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0401 02:42:50.584581 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 02:47:23.872843 140663602321216 spec.py:310] Evaluating on the validation split.
I0401 02:47:23.875925 140663602321216 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0401 02:47:23.879293 140663602321216 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0401 02:47:23.879407 140663602321216 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0401 02:47:23.907095 140663602321216 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0401 02:47:27.754301 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 02:51:55.137981 140663602321216 spec.py:326] Evaluating on the test split.
I0401 02:51:55.140823 140663602321216 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0401 02:51:55.143966 140663602321216 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0401 02:51:55.144076 140663602321216 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0401 02:51:55.171254 140663602321216 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0401 02:51:59.061308 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 02:56:32.146907 140663602321216 submission_runner.py:380] Time since start: 3.85s, 	Step: 1, 	{'train/accuracy': 0.0006417381937361768, 'train/loss': 11.161182288025852, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.148851223171443, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.145201469990123, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0401 02:56:32.147367 140663602321216 submission_runner.py:390] After eval at step 1: RAM USED (GB) 20.356100096
I0401 02:56:32.155434 140605903378176 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=3.847353, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.145201, test/num_examples=3003, total_duration=3.849582, train/accuracy=0.000642, train/bleu=0.000000, train/loss=11.161182, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.148851, validation/num_examples=3000
I0401 02:56:34.400013 140663602321216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_pytorch/trial_1/checkpoint_1.
I0401 02:56:34.400648 140663602321216 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 20.356050944
I0401 02:56:34.405237 140663602321216 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 20.356050944
I0401 02:56:34.408715 140663602321216 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:56:34.408743 139886737520448 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:56:34.408849 140388431898432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:56:34.408891 139788367148864 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:56:34.408956 140261786756928 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:56:34.409166 139847888303936 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:56:34.409180 139661500798784 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:56:34.409183 139812532709184 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 02:56:34.852145 140605894985472 logging_writer.py:48] [1] global_step=1, grad_norm=5.766303, loss=11.158217
I0401 02:56:34.855916 140663602321216 submission.py:119] 1) loss = 11.158, grad_norm = 5.766
I0401 02:56:34.856771 140663602321216 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 20.356009984
I0401 02:56:35.295120 140605903378176 logging_writer.py:48] [2] global_step=2, grad_norm=5.712477, loss=11.134455
I0401 02:56:35.298226 140663602321216 submission.py:119] 2) loss = 11.134, grad_norm = 5.712
I0401 02:56:35.735859 140605894985472 logging_writer.py:48] [3] global_step=3, grad_norm=5.589230, loss=11.123631
I0401 02:56:35.738899 140663602321216 submission.py:119] 3) loss = 11.124, grad_norm = 5.589
I0401 02:56:36.181456 140605903378176 logging_writer.py:48] [4] global_step=4, grad_norm=5.544198, loss=11.096761
I0401 02:56:36.184502 140663602321216 submission.py:119] 4) loss = 11.097, grad_norm = 5.544
I0401 02:56:36.625055 140605894985472 logging_writer.py:48] [5] global_step=5, grad_norm=5.489178, loss=11.077113
I0401 02:56:36.628164 140663602321216 submission.py:119] 5) loss = 11.077, grad_norm = 5.489
I0401 02:56:37.067753 140605903378176 logging_writer.py:48] [6] global_step=6, grad_norm=5.456922, loss=11.026307
I0401 02:56:37.071171 140663602321216 submission.py:119] 6) loss = 11.026, grad_norm = 5.457
I0401 02:56:37.514456 140605894985472 logging_writer.py:48] [7] global_step=7, grad_norm=5.293587, loss=10.991428
I0401 02:56:37.517433 140663602321216 submission.py:119] 7) loss = 10.991, grad_norm = 5.294
I0401 02:56:37.959152 140605903378176 logging_writer.py:48] [8] global_step=8, grad_norm=5.231684, loss=10.932065
I0401 02:56:37.962736 140663602321216 submission.py:119] 8) loss = 10.932, grad_norm = 5.232
I0401 02:56:38.399305 140605894985472 logging_writer.py:48] [9] global_step=9, grad_norm=4.958029, loss=10.873281
I0401 02:56:38.403376 140663602321216 submission.py:119] 9) loss = 10.873, grad_norm = 4.958
I0401 02:56:38.842912 140605903378176 logging_writer.py:48] [10] global_step=10, grad_norm=4.795124, loss=10.809768
I0401 02:56:38.846128 140663602321216 submission.py:119] 10) loss = 10.810, grad_norm = 4.795
I0401 02:56:39.293005 140605894985472 logging_writer.py:48] [11] global_step=11, grad_norm=4.673236, loss=10.746398
I0401 02:56:39.296371 140663602321216 submission.py:119] 11) loss = 10.746, grad_norm = 4.673
I0401 02:56:39.736406 140605903378176 logging_writer.py:48] [12] global_step=12, grad_norm=4.441508, loss=10.673725
I0401 02:56:39.740088 140663602321216 submission.py:119] 12) loss = 10.674, grad_norm = 4.442
I0401 02:56:40.213264 140605894985472 logging_writer.py:48] [13] global_step=13, grad_norm=4.210921, loss=10.603410
I0401 02:56:40.217415 140663602321216 submission.py:119] 13) loss = 10.603, grad_norm = 4.211
I0401 02:56:40.662597 140605903378176 logging_writer.py:48] [14] global_step=14, grad_norm=3.994947, loss=10.535426
I0401 02:56:40.666081 140663602321216 submission.py:119] 14) loss = 10.535, grad_norm = 3.995
I0401 02:56:41.106401 140605894985472 logging_writer.py:48] [15] global_step=15, grad_norm=3.714486, loss=10.459850
I0401 02:56:41.110099 140663602321216 submission.py:119] 15) loss = 10.460, grad_norm = 3.714
I0401 02:56:41.554306 140605903378176 logging_writer.py:48] [16] global_step=16, grad_norm=3.558163, loss=10.379184
I0401 02:56:41.558145 140663602321216 submission.py:119] 16) loss = 10.379, grad_norm = 3.558
I0401 02:56:41.997275 140605894985472 logging_writer.py:48] [17] global_step=17, grad_norm=3.333950, loss=10.306708
I0401 02:56:42.001250 140663602321216 submission.py:119] 17) loss = 10.307, grad_norm = 3.334
I0401 02:56:42.449400 140605903378176 logging_writer.py:48] [18] global_step=18, grad_norm=3.143764, loss=10.226472
I0401 02:56:42.453183 140663602321216 submission.py:119] 18) loss = 10.226, grad_norm = 3.144
I0401 02:56:42.896395 140605894985472 logging_writer.py:48] [19] global_step=19, grad_norm=2.915399, loss=10.180686
I0401 02:56:42.900200 140663602321216 submission.py:119] 19) loss = 10.181, grad_norm = 2.915
I0401 02:56:43.351236 140605903378176 logging_writer.py:48] [20] global_step=20, grad_norm=2.749161, loss=10.094804
I0401 02:56:43.355304 140663602321216 submission.py:119] 20) loss = 10.095, grad_norm = 2.749
I0401 02:56:43.793757 140605894985472 logging_writer.py:48] [21] global_step=21, grad_norm=2.603800, loss=10.032311
I0401 02:56:43.797538 140663602321216 submission.py:119] 21) loss = 10.032, grad_norm = 2.604
I0401 02:56:44.244505 140605903378176 logging_writer.py:48] [22] global_step=22, grad_norm=2.445449, loss=9.959802
I0401 02:56:44.248289 140663602321216 submission.py:119] 22) loss = 9.960, grad_norm = 2.445
I0401 02:56:44.692164 140605894985472 logging_writer.py:48] [23] global_step=23, grad_norm=2.241247, loss=9.909647
I0401 02:56:44.696039 140663602321216 submission.py:119] 23) loss = 9.910, grad_norm = 2.241
I0401 02:56:45.136594 140605903378176 logging_writer.py:48] [24] global_step=24, grad_norm=2.106332, loss=9.847027
I0401 02:56:45.140148 140663602321216 submission.py:119] 24) loss = 9.847, grad_norm = 2.106
I0401 02:56:45.596556 140605894985472 logging_writer.py:48] [25] global_step=25, grad_norm=1.988925, loss=9.782001
I0401 02:56:45.600435 140663602321216 submission.py:119] 25) loss = 9.782, grad_norm = 1.989
I0401 02:56:46.053796 140605903378176 logging_writer.py:48] [26] global_step=26, grad_norm=1.849424, loss=9.736770
I0401 02:56:46.057858 140663602321216 submission.py:119] 26) loss = 9.737, grad_norm = 1.849
I0401 02:56:46.511585 140605894985472 logging_writer.py:48] [27] global_step=27, grad_norm=1.728659, loss=9.668998
I0401 02:56:46.515343 140663602321216 submission.py:119] 27) loss = 9.669, grad_norm = 1.729
I0401 02:56:46.956340 140605903378176 logging_writer.py:48] [28] global_step=28, grad_norm=1.594017, loss=9.632103
I0401 02:56:46.959975 140663602321216 submission.py:119] 28) loss = 9.632, grad_norm = 1.594
I0401 02:56:47.401417 140605894985472 logging_writer.py:48] [29] global_step=29, grad_norm=1.519527, loss=9.592247
I0401 02:56:47.404920 140663602321216 submission.py:119] 29) loss = 9.592, grad_norm = 1.520
I0401 02:56:47.844220 140605903378176 logging_writer.py:48] [30] global_step=30, grad_norm=1.432127, loss=9.528553
I0401 02:56:47.847959 140663602321216 submission.py:119] 30) loss = 9.529, grad_norm = 1.432
I0401 02:56:48.290393 140605894985472 logging_writer.py:48] [31] global_step=31, grad_norm=1.340848, loss=9.487816
I0401 02:56:48.294007 140663602321216 submission.py:119] 31) loss = 9.488, grad_norm = 1.341
I0401 02:56:48.734888 140605903378176 logging_writer.py:48] [32] global_step=32, grad_norm=1.260703, loss=9.459692
I0401 02:56:48.738664 140663602321216 submission.py:119] 32) loss = 9.460, grad_norm = 1.261
I0401 02:56:49.180193 140605894985472 logging_writer.py:48] [33] global_step=33, grad_norm=1.172471, loss=9.428292
I0401 02:56:49.183625 140663602321216 submission.py:119] 33) loss = 9.428, grad_norm = 1.172
I0401 02:56:49.626099 140605903378176 logging_writer.py:48] [34] global_step=34, grad_norm=1.118621, loss=9.398671
I0401 02:56:49.629822 140663602321216 submission.py:119] 34) loss = 9.399, grad_norm = 1.119
I0401 02:56:50.078051 140605894985472 logging_writer.py:48] [35] global_step=35, grad_norm=1.049989, loss=9.375234
I0401 02:56:50.081896 140663602321216 submission.py:119] 35) loss = 9.375, grad_norm = 1.050
I0401 02:56:50.520770 140605903378176 logging_writer.py:48] [36] global_step=36, grad_norm=1.006545, loss=9.317693
I0401 02:56:50.524455 140663602321216 submission.py:119] 36) loss = 9.318, grad_norm = 1.007
I0401 02:56:50.964838 140605894985472 logging_writer.py:48] [37] global_step=37, grad_norm=0.944130, loss=9.303915
I0401 02:56:50.968571 140663602321216 submission.py:119] 37) loss = 9.304, grad_norm = 0.944
I0401 02:56:51.411611 140605903378176 logging_writer.py:48] [38] global_step=38, grad_norm=0.895365, loss=9.288239
I0401 02:56:51.414727 140663602321216 submission.py:119] 38) loss = 9.288, grad_norm = 0.895
I0401 02:56:51.853163 140605894985472 logging_writer.py:48] [39] global_step=39, grad_norm=0.858309, loss=9.251375
I0401 02:56:51.856934 140663602321216 submission.py:119] 39) loss = 9.251, grad_norm = 0.858
I0401 02:56:52.300941 140605903378176 logging_writer.py:48] [40] global_step=40, grad_norm=0.824634, loss=9.224163
I0401 02:56:52.304610 140663602321216 submission.py:119] 40) loss = 9.224, grad_norm = 0.825
I0401 02:56:52.746881 140605894985472 logging_writer.py:48] [41] global_step=41, grad_norm=0.783061, loss=9.203558
I0401 02:56:52.750915 140663602321216 submission.py:119] 41) loss = 9.204, grad_norm = 0.783
I0401 02:56:53.192059 140605903378176 logging_writer.py:48] [42] global_step=42, grad_norm=0.750369, loss=9.162749
I0401 02:56:53.195825 140663602321216 submission.py:119] 42) loss = 9.163, grad_norm = 0.750
I0401 02:56:53.638598 140605894985472 logging_writer.py:48] [43] global_step=43, grad_norm=0.727370, loss=9.147120
I0401 02:56:53.642442 140663602321216 submission.py:119] 43) loss = 9.147, grad_norm = 0.727
I0401 02:56:54.082348 140605903378176 logging_writer.py:48] [44] global_step=44, grad_norm=0.682818, loss=9.114194
I0401 02:56:54.086230 140663602321216 submission.py:119] 44) loss = 9.114, grad_norm = 0.683
I0401 02:56:54.527220 140605894985472 logging_writer.py:48] [45] global_step=45, grad_norm=0.667800, loss=9.079501
I0401 02:56:54.531227 140663602321216 submission.py:119] 45) loss = 9.080, grad_norm = 0.668
I0401 02:56:54.970782 140605903378176 logging_writer.py:48] [46] global_step=46, grad_norm=0.618734, loss=9.080647
I0401 02:56:54.974749 140663602321216 submission.py:119] 46) loss = 9.081, grad_norm = 0.619
I0401 02:56:55.413493 140605894985472 logging_writer.py:48] [47] global_step=47, grad_norm=0.595032, loss=9.081781
I0401 02:56:55.417247 140663602321216 submission.py:119] 47) loss = 9.082, grad_norm = 0.595
I0401 02:56:55.859128 140605903378176 logging_writer.py:48] [48] global_step=48, grad_norm=0.590643, loss=9.024612
I0401 02:56:55.862913 140663602321216 submission.py:119] 48) loss = 9.025, grad_norm = 0.591
I0401 02:56:56.303933 140605894985472 logging_writer.py:48] [49] global_step=49, grad_norm=0.550760, loss=9.018464
I0401 02:56:56.307492 140663602321216 submission.py:119] 49) loss = 9.018, grad_norm = 0.551
I0401 02:56:56.744873 140605903378176 logging_writer.py:48] [50] global_step=50, grad_norm=0.539035, loss=8.988667
I0401 02:56:56.748601 140663602321216 submission.py:119] 50) loss = 8.989, grad_norm = 0.539
I0401 02:56:57.190191 140605894985472 logging_writer.py:48] [51] global_step=51, grad_norm=0.514881, loss=9.007454
I0401 02:56:57.194053 140663602321216 submission.py:119] 51) loss = 9.007, grad_norm = 0.515
I0401 02:56:57.636007 140605903378176 logging_writer.py:48] [52] global_step=52, grad_norm=0.499133, loss=8.961019
I0401 02:56:57.639792 140663602321216 submission.py:119] 52) loss = 8.961, grad_norm = 0.499
I0401 02:56:58.077400 140605894985472 logging_writer.py:48] [53] global_step=53, grad_norm=0.477223, loss=8.980615
I0401 02:56:58.081248 140663602321216 submission.py:119] 53) loss = 8.981, grad_norm = 0.477
I0401 02:56:58.522040 140605903378176 logging_writer.py:48] [54] global_step=54, grad_norm=0.453224, loss=8.955214
I0401 02:56:58.525635 140663602321216 submission.py:119] 54) loss = 8.955, grad_norm = 0.453
I0401 02:56:58.964105 140605894985472 logging_writer.py:48] [55] global_step=55, grad_norm=0.438685, loss=8.933684
I0401 02:56:58.967870 140663602321216 submission.py:119] 55) loss = 8.934, grad_norm = 0.439
I0401 02:56:59.410499 140605903378176 logging_writer.py:48] [56] global_step=56, grad_norm=0.429924, loss=8.914577
I0401 02:56:59.414303 140663602321216 submission.py:119] 56) loss = 8.915, grad_norm = 0.430
I0401 02:56:59.854457 140605894985472 logging_writer.py:48] [57] global_step=57, grad_norm=0.421566, loss=8.880133
I0401 02:56:59.857598 140663602321216 submission.py:119] 57) loss = 8.880, grad_norm = 0.422
I0401 02:57:00.296337 140605903378176 logging_writer.py:48] [58] global_step=58, grad_norm=0.400208, loss=8.885902
I0401 02:57:00.300290 140663602321216 submission.py:119] 58) loss = 8.886, grad_norm = 0.400
I0401 02:57:00.745401 140605894985472 logging_writer.py:48] [59] global_step=59, grad_norm=0.379248, loss=8.909222
I0401 02:57:00.748877 140663602321216 submission.py:119] 59) loss = 8.909, grad_norm = 0.379
I0401 02:57:01.199759 140605903378176 logging_writer.py:48] [60] global_step=60, grad_norm=0.373121, loss=8.877100
I0401 02:57:01.203770 140663602321216 submission.py:119] 60) loss = 8.877, grad_norm = 0.373
I0401 02:57:01.641154 140605894985472 logging_writer.py:48] [61] global_step=61, grad_norm=0.368544, loss=8.876669
I0401 02:57:01.644775 140663602321216 submission.py:119] 61) loss = 8.877, grad_norm = 0.369
I0401 02:57:02.098652 140605903378176 logging_writer.py:48] [62] global_step=62, grad_norm=0.355130, loss=8.840648
I0401 02:57:02.102292 140663602321216 submission.py:119] 62) loss = 8.841, grad_norm = 0.355
I0401 02:57:02.545819 140605894985472 logging_writer.py:48] [63] global_step=63, grad_norm=0.349311, loss=8.853510
I0401 02:57:02.549617 140663602321216 submission.py:119] 63) loss = 8.854, grad_norm = 0.349
I0401 02:57:02.988263 140605903378176 logging_writer.py:48] [64] global_step=64, grad_norm=0.333889, loss=8.852448
I0401 02:57:02.992054 140663602321216 submission.py:119] 64) loss = 8.852, grad_norm = 0.334
I0401 02:57:03.436653 140605894985472 logging_writer.py:48] [65] global_step=65, grad_norm=0.324351, loss=8.794562
I0401 02:57:03.440487 140663602321216 submission.py:119] 65) loss = 8.795, grad_norm = 0.324
I0401 02:57:03.880723 140605903378176 logging_writer.py:48] [66] global_step=66, grad_norm=0.314365, loss=8.814823
I0401 02:57:03.884582 140663602321216 submission.py:119] 66) loss = 8.815, grad_norm = 0.314
I0401 02:57:04.324012 140605894985472 logging_writer.py:48] [67] global_step=67, grad_norm=0.319052, loss=8.762074
I0401 02:57:04.327541 140663602321216 submission.py:119] 67) loss = 8.762, grad_norm = 0.319
I0401 02:57:04.790883 140605903378176 logging_writer.py:48] [68] global_step=68, grad_norm=0.308667, loss=8.766002
I0401 02:57:04.794843 140663602321216 submission.py:119] 68) loss = 8.766, grad_norm = 0.309
I0401 02:57:05.252427 140605894985472 logging_writer.py:48] [69] global_step=69, grad_norm=0.301867, loss=8.755580
I0401 02:57:05.255945 140663602321216 submission.py:119] 69) loss = 8.756, grad_norm = 0.302
I0401 02:57:05.714672 140605903378176 logging_writer.py:48] [70] global_step=70, grad_norm=0.296907, loss=8.758258
I0401 02:57:05.717720 140663602321216 submission.py:119] 70) loss = 8.758, grad_norm = 0.297
I0401 02:57:06.168267 140605894985472 logging_writer.py:48] [71] global_step=71, grad_norm=0.283250, loss=8.765186
I0401 02:57:06.172046 140663602321216 submission.py:119] 71) loss = 8.765, grad_norm = 0.283
I0401 02:57:06.618618 140605903378176 logging_writer.py:48] [72] global_step=72, grad_norm=0.275031, loss=8.764130
I0401 02:57:06.622354 140663602321216 submission.py:119] 72) loss = 8.764, grad_norm = 0.275
I0401 02:57:07.064940 140605894985472 logging_writer.py:48] [73] global_step=73, grad_norm=0.270242, loss=8.727462
I0401 02:57:07.068618 140663602321216 submission.py:119] 73) loss = 8.727, grad_norm = 0.270
I0401 02:57:07.509747 140605903378176 logging_writer.py:48] [74] global_step=74, grad_norm=0.275171, loss=8.702469
I0401 02:57:07.513535 140663602321216 submission.py:119] 74) loss = 8.702, grad_norm = 0.275
I0401 02:57:07.957213 140605894985472 logging_writer.py:48] [75] global_step=75, grad_norm=0.268535, loss=8.711925
I0401 02:57:07.960671 140663602321216 submission.py:119] 75) loss = 8.712, grad_norm = 0.269
I0401 02:57:08.404084 140605903378176 logging_writer.py:48] [76] global_step=76, grad_norm=0.249368, loss=8.715692
I0401 02:57:08.407705 140663602321216 submission.py:119] 76) loss = 8.716, grad_norm = 0.249
I0401 02:57:08.857362 140605894985472 logging_writer.py:48] [77] global_step=77, grad_norm=0.255864, loss=8.715118
I0401 02:57:08.860546 140663602321216 submission.py:119] 77) loss = 8.715, grad_norm = 0.256
I0401 02:57:09.306959 140605903378176 logging_writer.py:48] [78] global_step=78, grad_norm=0.253022, loss=8.668766
I0401 02:57:09.310573 140663602321216 submission.py:119] 78) loss = 8.669, grad_norm = 0.253
I0401 02:57:09.752618 140605894985472 logging_writer.py:48] [79] global_step=79, grad_norm=0.241989, loss=8.698722
I0401 02:57:09.756129 140663602321216 submission.py:119] 79) loss = 8.699, grad_norm = 0.242
I0401 02:57:10.197832 140605903378176 logging_writer.py:48] [80] global_step=80, grad_norm=0.248561, loss=8.719474
I0401 02:57:10.201212 140663602321216 submission.py:119] 80) loss = 8.719, grad_norm = 0.249
I0401 02:57:10.642111 140605894985472 logging_writer.py:48] [81] global_step=81, grad_norm=0.251731, loss=8.680156
I0401 02:57:10.645613 140663602321216 submission.py:119] 81) loss = 8.680, grad_norm = 0.252
I0401 02:57:11.088415 140605903378176 logging_writer.py:48] [82] global_step=82, grad_norm=0.265215, loss=8.652739
I0401 02:57:11.091618 140663602321216 submission.py:119] 82) loss = 8.653, grad_norm = 0.265
I0401 02:57:11.532641 140605894985472 logging_writer.py:48] [83] global_step=83, grad_norm=0.241146, loss=8.662676
I0401 02:57:11.536006 140663602321216 submission.py:119] 83) loss = 8.663, grad_norm = 0.241
I0401 02:57:11.980152 140605903378176 logging_writer.py:48] [84] global_step=84, grad_norm=0.257851, loss=8.653217
I0401 02:57:11.983645 140663602321216 submission.py:119] 84) loss = 8.653, grad_norm = 0.258
I0401 02:57:12.426964 140605894985472 logging_writer.py:48] [85] global_step=85, grad_norm=0.230025, loss=8.655145
I0401 02:57:12.430272 140663602321216 submission.py:119] 85) loss = 8.655, grad_norm = 0.230
I0401 02:57:12.871445 140605903378176 logging_writer.py:48] [86] global_step=86, grad_norm=0.236325, loss=8.643889
I0401 02:57:12.874953 140663602321216 submission.py:119] 86) loss = 8.644, grad_norm = 0.236
I0401 02:57:13.318579 140605894985472 logging_writer.py:48] [87] global_step=87, grad_norm=0.239311, loss=8.628202
I0401 02:57:13.322329 140663602321216 submission.py:119] 87) loss = 8.628, grad_norm = 0.239
I0401 02:57:13.769121 140605903378176 logging_writer.py:48] [88] global_step=88, grad_norm=0.250521, loss=8.582412
I0401 02:57:13.773127 140663602321216 submission.py:119] 88) loss = 8.582, grad_norm = 0.251
I0401 02:57:14.223036 140605894985472 logging_writer.py:48] [89] global_step=89, grad_norm=0.236506, loss=8.631239
I0401 02:57:14.226790 140663602321216 submission.py:119] 89) loss = 8.631, grad_norm = 0.237
I0401 02:57:14.671407 140605903378176 logging_writer.py:48] [90] global_step=90, grad_norm=0.283164, loss=8.657072
I0401 02:57:14.675103 140663602321216 submission.py:119] 90) loss = 8.657, grad_norm = 0.283
I0401 02:57:15.121442 140605894985472 logging_writer.py:48] [91] global_step=91, grad_norm=0.220056, loss=8.581394
I0401 02:57:15.125153 140663602321216 submission.py:119] 91) loss = 8.581, grad_norm = 0.220
I0401 02:57:15.568578 140605903378176 logging_writer.py:48] [92] global_step=92, grad_norm=0.260093, loss=8.638184
I0401 02:57:15.572180 140663602321216 submission.py:119] 92) loss = 8.638, grad_norm = 0.260
I0401 02:57:16.018260 140605894985472 logging_writer.py:48] [93] global_step=93, grad_norm=0.229641, loss=8.592283
I0401 02:57:16.021808 140663602321216 submission.py:119] 93) loss = 8.592, grad_norm = 0.230
I0401 02:57:16.465478 140605903378176 logging_writer.py:48] [94] global_step=94, grad_norm=0.248166, loss=8.619803
I0401 02:57:16.469279 140663602321216 submission.py:119] 94) loss = 8.620, grad_norm = 0.248
I0401 02:57:16.908351 140605894985472 logging_writer.py:48] [95] global_step=95, grad_norm=0.237000, loss=8.563076
I0401 02:57:16.912007 140663602321216 submission.py:119] 95) loss = 8.563, grad_norm = 0.237
I0401 02:57:17.358186 140605903378176 logging_writer.py:48] [96] global_step=96, grad_norm=0.208501, loss=8.566639
I0401 02:57:17.361664 140663602321216 submission.py:119] 96) loss = 8.567, grad_norm = 0.209
I0401 02:57:17.804790 140605894985472 logging_writer.py:48] [97] global_step=97, grad_norm=0.237557, loss=8.608709
I0401 02:57:17.808383 140663602321216 submission.py:119] 97) loss = 8.609, grad_norm = 0.238
I0401 02:57:18.252838 140605903378176 logging_writer.py:48] [98] global_step=98, grad_norm=0.226792, loss=8.573897
I0401 02:57:18.256495 140663602321216 submission.py:119] 98) loss = 8.574, grad_norm = 0.227
I0401 02:57:18.702335 140605894985472 logging_writer.py:48] [99] global_step=99, grad_norm=0.218826, loss=8.603313
I0401 02:57:18.705834 140663602321216 submission.py:119] 99) loss = 8.603, grad_norm = 0.219
I0401 02:57:19.150602 140605903378176 logging_writer.py:48] [100] global_step=100, grad_norm=0.209648, loss=8.565033
I0401 02:57:19.154365 140663602321216 submission.py:119] 100) loss = 8.565, grad_norm = 0.210
I0401 03:00:13.782302 140605894985472 logging_writer.py:48] [500] global_step=500, grad_norm=0.592176, loss=6.881315
I0401 03:00:13.785665 140663602321216 submission.py:119] 500) loss = 6.881, grad_norm = 0.592
I0401 03:03:52.187100 140605903378176 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.822486, loss=5.941452
I0401 03:03:52.190912 140663602321216 submission.py:119] 1000) loss = 5.941, grad_norm = 0.822
I0401 03:07:30.295014 140605894985472 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.360676, loss=6.595746
I0401 03:07:30.299427 140663602321216 submission.py:119] 1500) loss = 6.596, grad_norm = 0.361
I0401 03:10:34.817164 140663602321216 submission_runner.py:371] Before eval at step 1924: RAM USED (GB) 20.730712064
I0401 03:10:34.817392 140663602321216 spec.py:298] Evaluating on the training split.
I0401 03:10:38.675952 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 03:14:04.427534 140663602321216 spec.py:310] Evaluating on the validation split.
I0401 03:14:08.170155 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 03:17:30.313138 140663602321216 spec.py:326] Evaluating on the test split.
I0401 03:17:34.107600 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 03:20:45.746266 140663602321216 submission_runner.py:380] Time since start: 1672.26s, 	Step: 1924, 	{'train/accuracy': 0.2489117801226099, 'train/loss': 5.240716658310354, 'train/bleu': 0.8628082359446881, 'validation/accuracy': 0.223419424433671, 'validation/loss': 5.50349499696222, 'validation/bleu': 0.296827804385399, 'validation/num_examples': 3000, 'test/accuracy': 0.21093486723606997, 'test/loss': 5.764669760618209, 'test/bleu': 0.2685576750640024, 'test/num_examples': 3003}
I0401 03:20:45.746708 140663602321216 submission_runner.py:390] After eval at step 1924: RAM USED (GB) 20.91702272
I0401 03:20:45.755659 140605903378176 logging_writer.py:48] [1924] global_step=1924, preemption_count=0, score=841.580116, test/accuracy=0.210935, test/bleu=0.268558, test/loss=5.764670, test/num_examples=3003, total_duration=1672.263093, train/accuracy=0.248912, train/bleu=0.862808, train/loss=5.240717, validation/accuracy=0.223419, validation/bleu=0.296828, validation/loss=5.503495, validation/num_examples=3000
I0401 03:20:47.979197 140663602321216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_pytorch/trial_1/checkpoint_1924.
I0401 03:20:47.979788 140663602321216 submission_runner.py:409] After logging and checkpointing eval at step 1924: RAM USED (GB) 20.916649984
I0401 03:21:21.458583 140605894985472 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.384683, loss=5.827621
I0401 03:21:21.461498 140663602321216 submission.py:119] 2000) loss = 5.828, grad_norm = 0.385
I0401 03:24:59.477444 140605903378176 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.510724, loss=4.606124
I0401 03:24:59.481229 140663602321216 submission.py:119] 2500) loss = 4.606, grad_norm = 0.511
I0401 03:28:37.650072 140605894985472 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.464783, loss=4.276643
I0401 03:28:37.653215 140663602321216 submission.py:119] 3000) loss = 4.277, grad_norm = 0.465
I0401 03:32:15.657206 140605903378176 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.363798, loss=3.913111
I0401 03:32:15.660366 140663602321216 submission.py:119] 3500) loss = 3.913, grad_norm = 0.364
I0401 03:34:48.299885 140663602321216 submission_runner.py:371] Before eval at step 3851: RAM USED (GB) 21.031358464
I0401 03:34:48.300118 140663602321216 spec.py:298] Evaluating on the training split.
I0401 03:34:52.179916 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 03:37:29.803762 140663602321216 spec.py:310] Evaluating on the validation split.
I0401 03:37:33.543412 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 03:39:58.596252 140663602321216 spec.py:326] Evaluating on the test split.
I0401 03:40:02.409334 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 03:42:21.166250 140663602321216 submission_runner.py:380] Time since start: 3125.75s, 	Step: 3851, 	{'train/accuracy': 0.523960752174111, 'train/loss': 2.836873200097892, 'train/bleu': 22.930690241616286, 'validation/accuracy': 0.5259451215731981, 'validation/loss': 2.8223425623984824, 'validation/bleu': 19.084225777911474, 'validation/num_examples': 3000, 'test/accuracy': 0.5238510255069432, 'test/loss': 2.8744358622392654, 'test/bleu': 17.345635991081352, 'test/num_examples': 3003}
I0401 03:42:21.166751 140663602321216 submission_runner.py:390] After eval at step 3851: RAM USED (GB) 21.10691328
I0401 03:42:21.175747 140605894985472 logging_writer.py:48] [3851] global_step=3851, preemption_count=0, score=1679.171231, test/accuracy=0.523851, test/bleu=17.345636, test/loss=2.874436, test/num_examples=3003, total_duration=3125.746092, train/accuracy=0.523961, train/bleu=22.930690, train/loss=2.836873, validation/accuracy=0.525945, validation/bleu=19.084226, validation/loss=2.822343, validation/num_examples=3000
I0401 03:42:23.614909 140663602321216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_pytorch/trial_1/checkpoint_3851.
I0401 03:42:23.615623 140663602321216 submission_runner.py:409] After logging and checkpointing eval at step 3851: RAM USED (GB) 21.105979392
I0401 03:43:29.015167 140605903378176 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.313775, loss=3.829991
I0401 03:43:29.018329 140663602321216 submission.py:119] 4000) loss = 3.830, grad_norm = 0.314
I0401 03:47:07.305624 140605894985472 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.251675, loss=3.608515
I0401 03:47:07.309529 140663602321216 submission.py:119] 4500) loss = 3.609, grad_norm = 0.252
I0401 03:50:45.373232 140605903378176 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.220191, loss=3.575344
I0401 03:50:45.376894 140663602321216 submission.py:119] 5000) loss = 3.575, grad_norm = 0.220
I0401 03:54:23.567211 140605894985472 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.203233, loss=3.554623
I0401 03:54:23.570420 140663602321216 submission.py:119] 5500) loss = 3.555, grad_norm = 0.203
I0401 03:56:23.846151 140663602321216 submission_runner.py:371] Before eval at step 5777: RAM USED (GB) 21.19712768
I0401 03:56:23.846411 140663602321216 spec.py:298] Evaluating on the training split.
I0401 03:56:27.755710 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 03:58:55.039240 140663602321216 spec.py:310] Evaluating on the validation split.
I0401 03:58:58.780333 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 04:01:18.170460 140663602321216 spec.py:326] Evaluating on the test split.
I0401 04:01:21.991352 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 04:03:37.257351 140663602321216 submission_runner.py:380] Time since start: 4421.29s, 	Step: 5777, 	{'train/accuracy': 0.569425764241913, 'train/loss': 2.43135803251875, 'train/bleu': 25.90512884930798, 'validation/accuracy': 0.5816294900249221, 'validation/loss': 2.3333165428822955, 'validation/bleu': 22.95177086905765, 'validation/num_examples': 3000, 'test/accuracy': 0.584695834059613, 'test/loss': 2.3221424525013075, 'test/bleu': 21.777344843388388, 'test/num_examples': 3003}
I0401 04:03:37.257729 140663602321216 submission_runner.py:390] After eval at step 5777: RAM USED (GB) 21.247725568
I0401 04:03:37.266297 140605903378176 logging_writer.py:48] [5777] global_step=5777, preemption_count=0, score=2516.684763, test/accuracy=0.584696, test/bleu=21.777345, test/loss=2.322142, test/num_examples=3003, total_duration=4421.292362, train/accuracy=0.569426, train/bleu=25.905129, train/loss=2.431358, validation/accuracy=0.581629, validation/bleu=22.951771, validation/loss=2.333317, validation/num_examples=3000
I0401 04:03:39.481596 140663602321216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_pytorch/trial_1/checkpoint_5777.
I0401 04:03:39.482300 140663602321216 submission_runner.py:409] After logging and checkpointing eval at step 5777: RAM USED (GB) 21.248303104
I0401 04:05:17.053934 140605894985472 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.191431, loss=3.463457
I0401 04:05:17.056934 140663602321216 submission.py:119] 6000) loss = 3.463, grad_norm = 0.191
I0401 04:08:54.972806 140605903378176 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.179074, loss=3.339671
I0401 04:08:54.975992 140663602321216 submission.py:119] 6500) loss = 3.340, grad_norm = 0.179
I0401 04:12:32.885495 140605894985472 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.180341, loss=3.458618
I0401 04:12:32.889521 140663602321216 submission.py:119] 7000) loss = 3.459, grad_norm = 0.180
I0401 04:16:11.002843 140605903378176 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.224094, loss=3.245489
I0401 04:16:11.006522 140663602321216 submission.py:119] 7500) loss = 3.245, grad_norm = 0.224
I0401 04:17:39.584104 140663602321216 submission_runner.py:371] Before eval at step 7704: RAM USED (GB) 21.656997888
I0401 04:17:39.584304 140663602321216 spec.py:298] Evaluating on the training split.
I0401 04:17:43.461303 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 04:19:55.416537 140663602321216 spec.py:310] Evaluating on the validation split.
I0401 04:19:59.151520 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 04:22:13.784689 140663602321216 spec.py:326] Evaluating on the test split.
I0401 04:22:17.604900 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 04:24:31.949609 140663602321216 submission_runner.py:380] Time since start: 5697.03s, 	Step: 7704, 	{'train/accuracy': 0.5975071973678198, 'train/loss': 2.1849612855869855, 'train/bleu': 28.14156015409407, 'validation/accuracy': 0.6071592416709031, 'validation/loss': 2.1052387369654437, 'validation/bleu': 24.802998249764894, 'validation/num_examples': 3000, 'test/accuracy': 0.6118528847829876, 'test/loss': 2.0813119153448376, 'test/bleu': 23.43553301898075, 'test/num_examples': 3003}
I0401 04:24:31.949989 140663602321216 submission_runner.py:390] After eval at step 7704: RAM USED (GB) 21.715582976
I0401 04:24:31.958092 140605894985472 logging_writer.py:48] [7704] global_step=7704, preemption_count=0, score=3354.064323, test/accuracy=0.611853, test/bleu=23.435533, test/loss=2.081312, test/num_examples=3003, total_duration=5697.030376, train/accuracy=0.597507, train/bleu=28.141560, train/loss=2.184961, validation/accuracy=0.607159, validation/bleu=24.802998, validation/loss=2.105239, validation/num_examples=3000
I0401 04:24:34.170704 140663602321216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_pytorch/trial_1/checkpoint_7704.
I0401 04:24:34.171308 140663602321216 submission_runner.py:409] After logging and checkpointing eval at step 7704: RAM USED (GB) 21.715202048
I0401 04:26:43.488918 140605903378176 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.141828, loss=3.294682
I0401 04:26:43.492249 140663602321216 submission.py:119] 8000) loss = 3.295, grad_norm = 0.142
I0401 04:30:21.520680 140605894985472 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.124949, loss=3.236885
I0401 04:30:21.523718 140663602321216 submission.py:119] 8500) loss = 3.237, grad_norm = 0.125
I0401 04:33:59.750320 140605903378176 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.141924, loss=3.281433
I0401 04:33:59.754217 140663602321216 submission.py:119] 9000) loss = 3.281, grad_norm = 0.142
I0401 04:37:37.823748 140605894985472 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.127454, loss=3.162898
I0401 04:37:37.827085 140663602321216 submission.py:119] 9500) loss = 3.163, grad_norm = 0.127
I0401 04:38:34.507853 140663602321216 submission_runner.py:371] Before eval at step 9631: RAM USED (GB) 21.81783552
I0401 04:38:34.508061 140663602321216 spec.py:298] Evaluating on the training split.
I0401 04:38:38.378421 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 04:41:15.605173 140663602321216 spec.py:310] Evaluating on the validation split.
I0401 04:41:19.319230 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 04:43:32.685999 140663602321216 spec.py:326] Evaluating on the test split.
I0401 04:43:36.492810 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 04:45:40.662950 140663602321216 submission_runner.py:380] Time since start: 6951.95s, 	Step: 9631, 	{'train/accuracy': 0.6081934937386965, 'train/loss': 2.107532630033653, 'train/bleu': 29.177913772583562, 'validation/accuracy': 0.6238856306803388, 'validation/loss': 1.978126821118151, 'validation/bleu': 25.855092340379404, 'validation/num_examples': 3000, 'test/accuracy': 0.6324327464993318, 'test/loss': 1.940668213642438, 'test/bleu': 25.09466661531606, 'test/num_examples': 3003}
I0401 04:45:40.663313 140663602321216 submission_runner.py:390] After eval at step 9631: RAM USED (GB) 21.904658432
I0401 04:45:40.671253 140605903378176 logging_writer.py:48] [9631] global_step=9631, preemption_count=0, score=4191.747718, test/accuracy=0.632433, test/bleu=25.094667, test/loss=1.940668, test/num_examples=3003, total_duration=6951.954130, train/accuracy=0.608193, train/bleu=29.177914, train/loss=2.107533, validation/accuracy=0.623886, validation/bleu=25.855092, validation/loss=1.978127, validation/num_examples=3000
I0401 04:45:42.880728 140663602321216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_pytorch/trial_1/checkpoint_9631.
I0401 04:45:42.881342 140663602321216 submission_runner.py:409] After logging and checkpointing eval at step 9631: RAM USED (GB) 21.905006592
I0401 04:48:23.664858 140663602321216 submission_runner.py:371] Before eval at step 10000: RAM USED (GB) 21.943361536
I0401 04:48:23.665141 140663602321216 spec.py:298] Evaluating on the training split.
I0401 04:48:27.557946 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 04:50:57.140748 140663602321216 spec.py:310] Evaluating on the validation split.
I0401 04:51:00.893825 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 04:53:17.645303 140663602321216 spec.py:326] Evaluating on the test split.
I0401 04:53:21.469332 140663602321216 workload.py:130] Translating evaluation dataset.
I0401 04:55:29.210918 140663602321216 submission_runner.py:380] Time since start: 7541.11s, 	Step: 10000, 	{'train/accuracy': 0.6091030022247196, 'train/loss': 2.0791671205935645, 'train/bleu': 29.33046947150488, 'validation/accuracy': 0.6273077829165169, 'validation/loss': 1.955176470223556, 'validation/bleu': 26.14409179985469, 'validation/num_examples': 3000, 'test/accuracy': 0.6327000174307129, 'test/loss': 1.9146646766602755, 'test/bleu': 25.05485483137483, 'test/num_examples': 3003}
I0401 04:55:29.211426 140663602321216 submission_runner.py:390] After eval at step 10000: RAM USED (GB) 21.997334528
I0401 04:55:29.220510 140605894985472 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4352.010881, test/accuracy=0.632700, test/bleu=25.054855, test/loss=1.914665, test/num_examples=3003, total_duration=7541.110898, train/accuracy=0.609103, train/bleu=29.330469, train/loss=2.079167, validation/accuracy=0.627308, validation/bleu=26.144092, validation/loss=1.955176, validation/num_examples=3000
I0401 04:55:31.654526 140663602321216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_pytorch/trial_1/checkpoint_10000.
I0401 04:55:31.655393 140663602321216 submission_runner.py:409] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.996662784
I0401 04:55:31.663258 140605903378176 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4352.010881
I0401 04:55:35.974209 140663602321216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_pytorch/trial_1/checkpoint_10000.
I0401 04:55:36.000030 140663602321216 submission_runner.py:543] Tuning trial 1/1
I0401 04:55:36.000267 140663602321216 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0401 04:55:36.001412 140663602321216 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006417381937361768, 'train/loss': 11.161182288025852, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.148851223171443, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.145201469990123, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 3.8473527431488037, 'total_duration': 3.8495819568634033, 'global_step': 1, 'preemption_count': 0}), (1924, {'train/accuracy': 0.2489117801226099, 'train/loss': 5.240716658310354, 'train/bleu': 0.8628082359446881, 'validation/accuracy': 0.223419424433671, 'validation/loss': 5.50349499696222, 'validation/bleu': 0.296827804385399, 'validation/num_examples': 3000, 'test/accuracy': 0.21093486723606997, 'test/loss': 5.764669760618209, 'test/bleu': 0.2685576750640024, 'test/num_examples': 3003, 'score': 841.5801160335541, 'total_duration': 1672.2630925178528, 'global_step': 1924, 'preemption_count': 0}), (3851, {'train/accuracy': 0.523960752174111, 'train/loss': 2.836873200097892, 'train/bleu': 22.930690241616286, 'validation/accuracy': 0.5259451215731981, 'validation/loss': 2.8223425623984824, 'validation/bleu': 19.084225777911474, 'validation/num_examples': 3000, 'test/accuracy': 0.5238510255069432, 'test/loss': 2.8744358622392654, 'test/bleu': 17.345635991081352, 'test/num_examples': 3003, 'score': 1679.1712307929993, 'total_duration': 3125.74609208107, 'global_step': 3851, 'preemption_count': 0}), (5777, {'train/accuracy': 0.569425764241913, 'train/loss': 2.43135803251875, 'train/bleu': 25.90512884930798, 'validation/accuracy': 0.5816294900249221, 'validation/loss': 2.3333165428822955, 'validation/bleu': 22.95177086905765, 'validation/num_examples': 3000, 'test/accuracy': 0.584695834059613, 'test/loss': 2.3221424525013075, 'test/bleu': 21.777344843388388, 'test/num_examples': 3003, 'score': 2516.6847627162933, 'total_duration': 4421.292362213135, 'global_step': 5777, 'preemption_count': 0}), (7704, {'train/accuracy': 0.5975071973678198, 'train/loss': 2.1849612855869855, 'train/bleu': 28.14156015409407, 'validation/accuracy': 0.6071592416709031, 'validation/loss': 2.1052387369654437, 'validation/bleu': 24.802998249764894, 'validation/num_examples': 3000, 'test/accuracy': 0.6118528847829876, 'test/loss': 2.0813119153448376, 'test/bleu': 23.43553301898075, 'test/num_examples': 3003, 'score': 3354.0643227100372, 'total_duration': 5697.030376195908, 'global_step': 7704, 'preemption_count': 0}), (9631, {'train/accuracy': 0.6081934937386965, 'train/loss': 2.107532630033653, 'train/bleu': 29.177913772583562, 'validation/accuracy': 0.6238856306803388, 'validation/loss': 1.978126821118151, 'validation/bleu': 25.855092340379404, 'validation/num_examples': 3000, 'test/accuracy': 0.6324327464993318, 'test/loss': 1.940668213642438, 'test/bleu': 25.09466661531606, 'test/num_examples': 3003, 'score': 4191.747717618942, 'total_duration': 6951.954129695892, 'global_step': 9631, 'preemption_count': 0}), (10000, {'train/accuracy': 0.6091030022247196, 'train/loss': 2.0791671205935645, 'train/bleu': 29.33046947150488, 'validation/accuracy': 0.6273077829165169, 'validation/loss': 1.955176470223556, 'validation/bleu': 26.14409179985469, 'validation/num_examples': 3000, 'test/accuracy': 0.6327000174307129, 'test/loss': 1.9146646766602755, 'test/bleu': 25.05485483137483, 'test/num_examples': 3003, 'score': 4352.0108807086945, 'total_duration': 7541.110897541046, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0401 04:55:36.001514 140663602321216 submission_runner.py:546] Timing: 4352.0108807086945
I0401 04:55:36.001572 140663602321216 submission_runner.py:547] ====================
I0401 04:55:36.001664 140663602321216 submission_runner.py:606] Final wmt score: 4352.0108807086945
