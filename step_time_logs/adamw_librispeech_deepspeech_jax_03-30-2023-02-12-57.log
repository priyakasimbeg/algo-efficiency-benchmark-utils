I0330 02:13:12.422457 139973912856384 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/librispeech_deepspeech_jax.
I0330 02:13:12.477322 139973912856384 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0330 02:13:13.624324 139973912856384 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0330 02:13:13.625333 139973912856384 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0330 02:13:13.630532 139973912856384 submission_runner.py:504] Using RNG seed 3135307002
I0330 02:13:15.016431 139973912856384 submission_runner.py:513] --- Tuning run 1/1 ---
I0330 02:13:15.016616 139973912856384 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1.
I0330 02:13:15.016802 139973912856384 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/hparams.json.
I0330 02:13:15.153617 139973912856384 submission_runner.py:230] Starting train once: RAM USED (GB) 4.713160704
I0330 02:13:15.153776 139973912856384 submission_runner.py:231] Initializing dataset.
I0330 02:13:15.153951 139973912856384 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.713160704
I0330 02:13:15.154029 139973912856384 submission_runner.py:240] Initializing model.
I0330 02:13:33.687689 139973912856384 submission_runner.py:251] After Initializing model: RAM USED (GB) 9.309057024
I0330 02:13:33.687897 139973912856384 submission_runner.py:252] Initializing optimizer.
I0330 02:13:34.481112 139973912856384 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 9.308798976
I0330 02:13:34.481291 139973912856384 submission_runner.py:261] Initializing metrics bundle.
I0330 02:13:34.481342 139973912856384 submission_runner.py:275] Initializing checkpoint and logger.
I0330 02:13:34.483345 139973912856384 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0330 02:13:34.483628 139973912856384 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0330 02:13:34.483697 139973912856384 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0330 02:13:36.080520 139973912856384 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0330 02:13:36.081470 139973912856384 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0330 02:13:36.085582 139973912856384 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 9.306021888
I0330 02:13:36.085798 139973912856384 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 9.306021888
I0330 02:13:36.085871 139973912856384 submission_runner.py:312] Starting training loop.
I0330 02:13:36.315264 139973912856384 input_pipeline.py:20] Loading split = train-clean-100
I0330 02:13:36.360028 139973912856384 input_pipeline.py:20] Loading split = train-clean-360
I0330 02:13:36.773831 139973912856384 input_pipeline.py:20] Loading split = train-other-500
I0330 02:13:40.405242 139973912856384 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 11.984986112
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0330 02:14:35.221989 139907980846848 logging_writer.py:48] [0] global_step=0, grad_norm=32.28655242919922, loss=33.11904525756836
I0330 02:14:35.235310 139973912856384 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 14.491070464
I0330 02:14:35.235552 139973912856384 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 14.491070464
I0330 02:14:35.235687 139973912856384 spec.py:298] Evaluating on the training split.
I0330 02:14:35.378971 139973912856384 input_pipeline.py:20] Loading split = train-clean-100
I0330 02:14:35.411876 139973912856384 input_pipeline.py:20] Loading split = train-clean-360
I0330 02:14:35.717046 139973912856384 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0330 02:16:50.002931 139973912856384 spec.py:310] Evaluating on the validation split.
I0330 02:16:50.103142 139973912856384 input_pipeline.py:20] Loading split = dev-clean
I0330 02:16:50.108329 139973912856384 input_pipeline.py:20] Loading split = dev-other
I0330 02:18:01.802842 139973912856384 spec.py:326] Evaluating on the test split.
I0330 02:18:01.900960 139973912856384 input_pipeline.py:20] Loading split = test-clean
I0330 02:18:47.285943 139973912856384 submission_runner.py:380] Time since start: 59.15s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.630753, dtype=float32), 'train/wer': 4.502873623055619, 'validation/ctc_loss': DeviceArray(31.380888, dtype=float32), 'validation/wer': 4.151279800094549, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.527367, dtype=float32), 'test/wer': 4.315032600085309, 'test/num_examples': 2472}
I0330 02:18:47.287021 139973912856384 submission_runner.py:390] After eval at step 1: RAM USED (GB) 19.958882304
I0330 02:18:47.299355 139796454307584 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=58.920238, test/ctc_loss=31.527366638183594, test/num_examples=2472, test/wer=4.315033, total_duration=59.149725, train/ctc_loss=32.63075256347656, train/wer=4.502874, validation/ctc_loss=31.380887985229492, validation/num_examples=5348, validation/wer=4.151280
I0330 02:18:47.474857 139973912856384 checkpoints.py:356] Saving checkpoint at step: 1
I0330 02:18:48.065872 139973912856384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0330 02:18:48.077104 139973912856384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0330 02:18:48.088666 139973912856384 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 19.947585536
I0330 02:18:48.138900 139973912856384 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 19.945046016
I0330 02:19:09.576085 139973912856384 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 20.21433344
I0330 02:21:09.608813 139799677703936 logging_writer.py:48] [100] global_step=100, grad_norm=5.681350231170654, loss=8.184300422668457
I0330 02:23:10.592859 139799686096640 logging_writer.py:48] [200] global_step=200, grad_norm=0.8195521235466003, loss=6.059754371643066
I0330 02:25:11.719915 139799677703936 logging_writer.py:48] [300] global_step=300, grad_norm=0.6111098527908325, loss=5.872119426727295
I0330 02:27:12.704244 139799686096640 logging_writer.py:48] [400] global_step=400, grad_norm=0.7032192349433899, loss=5.845003128051758
I0330 02:29:13.797129 139799677703936 logging_writer.py:48] [500] global_step=500, grad_norm=0.5103294849395752, loss=5.779160499572754
I0330 02:31:15.566352 139799686096640 logging_writer.py:48] [600] global_step=600, grad_norm=0.962908923625946, loss=5.684441566467285
I0330 02:33:17.045814 139799677703936 logging_writer.py:48] [700] global_step=700, grad_norm=1.34037184715271, loss=5.523411750793457
I0330 02:35:18.270939 139799686096640 logging_writer.py:48] [800] global_step=800, grad_norm=1.012414813041687, loss=5.256570816040039
I0330 02:37:19.630429 139799677703936 logging_writer.py:48] [900] global_step=900, grad_norm=1.4119513034820557, loss=4.845478057861328
I0330 02:39:20.871834 139799686096640 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.3513126373291016, loss=4.446602821350098
I0330 02:41:25.843649 139799138662144 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.5614867210388184, loss=4.244537830352783
I0330 02:43:26.579631 139799130269440 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.0609114170074463, loss=3.9157800674438477
I0330 02:45:27.342971 139799138662144 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.580296039581299, loss=3.8365988731384277
I0330 02:47:28.095013 139799130269440 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.682371973991394, loss=3.717613458633423
I0330 02:49:29.048413 139799138662144 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.952581763267517, loss=3.4896652698516846
I0330 02:51:29.810977 139799130269440 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.4062447547912598, loss=3.508402109146118
I0330 02:53:30.713294 139799138662144 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.2904672622680664, loss=3.3059253692626953
I0330 02:55:31.431643 139799130269440 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.212710380554199, loss=3.246664524078369
I0330 02:57:32.328766 139799138662144 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.9947307109832764, loss=3.1840224266052246
I0330 02:58:49.243226 139973912856384 submission_runner.py:371] Before eval at step 1965: RAM USED (GB) 20.51131392
I0330 02:58:49.243421 139973912856384 spec.py:298] Evaluating on the training split.
I0330 02:59:20.600986 139973912856384 spec.py:310] Evaluating on the validation split.
I0330 02:59:58.826567 139973912856384 spec.py:326] Evaluating on the test split.
I0330 03:00:19.440698 139973912856384 submission_runner.py:380] Time since start: 2713.16s, 	Step: 1965, 	{'train/ctc_loss': DeviceArray(6.08357, dtype=float32), 'train/wer': 0.9302331568865729, 'validation/ctc_loss': DeviceArray(6.029016, dtype=float32), 'validation/wer': 0.8892705187700798, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.9623356, dtype=float32), 'test/wer': 0.8921048889972173, 'test/num_examples': 2472}
I0330 03:00:19.442045 139973912856384 submission_runner.py:390] After eval at step 1965: RAM USED (GB) 19.329314816
I0330 03:00:19.462668 139799138662144 logging_writer.py:48] [1965] global_step=1965, preemption_count=0, score=2455.703231, test/ctc_loss=5.962335586547852, test/num_examples=2472, test/wer=0.892105, total_duration=2713.155546, train/ctc_loss=6.0835700035095215, train/wer=0.930233, validation/ctc_loss=6.029016017913818, validation/num_examples=5348, validation/wer=0.889271
I0330 03:00:19.622840 139973912856384 checkpoints.py:356] Saving checkpoint at step: 1965
I0330 03:00:20.248585 139973912856384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_1965
I0330 03:00:20.263350 139973912856384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_1965.
I0330 03:00:20.272286 139973912856384 submission_runner.py:409] After logging and checkpointing eval at step 1965: RAM USED (GB) 19.307999232
I0330 03:01:03.723496 139799130269440 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.821857452392578, loss=3.130634069442749
I0330 03:03:08.479133 139799138662144 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.500248908996582, loss=2.9542975425720215
I0330 03:05:08.751866 139799130269440 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.699368953704834, loss=2.92345929145813
I0330 03:07:09.356215 139799138662144 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.3716843128204346, loss=2.858121871948242
I0330 03:09:09.627464 139799130269440 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.633173704147339, loss=2.8551249504089355
I0330 03:11:10.138846 139799138662144 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.002594470977783, loss=2.802013635635376
I0330 03:13:10.490684 139799130269440 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.305525541305542, loss=2.726088523864746
I0330 03:15:11.134932 139799138662144 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.8321921825408936, loss=2.6567978858947754
I0330 03:17:11.719496 139799130269440 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.8514091968536377, loss=2.710191488265991
I0330 03:19:12.181025 139799138662144 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.652221441268921, loss=2.6421115398406982
I0330 03:21:12.649974 139799130269440 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.4587550163269043, loss=2.4699201583862305
I0330 03:23:16.570569 139799138662144 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.2096967697143555, loss=2.5286331176757812
I0330 03:25:16.571772 139799130269440 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.2981789112091064, loss=2.493032455444336
I0330 03:27:16.572096 139799138662144 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.678285598754883, loss=2.497931480407715
I0330 03:29:16.803644 139799130269440 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.506338357925415, loss=2.3831300735473633
I0330 03:31:17.182874 139799138662144 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.959752082824707, loss=2.3895959854125977
I0330 03:33:17.409932 139799130269440 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.712252140045166, loss=2.3967134952545166
I0330 03:35:17.687147 139799138662144 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.839478015899658, loss=2.358466863632202
I0330 03:37:18.174165 139799130269440 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.676510810852051, loss=2.307457685470581
I0330 03:39:17.950720 139799138662144 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.4393014907836914, loss=2.31567645072937
I0330 03:40:21.082738 139973912856384 submission_runner.py:371] Before eval at step 3954: RAM USED (GB) 20.265046016
I0330 03:40:21.082943 139973912856384 spec.py:298] Evaluating on the training split.
I0330 03:41:07.851103 139973912856384 spec.py:310] Evaluating on the validation split.
I0330 03:41:50.302304 139973912856384 spec.py:326] Evaluating on the test split.
I0330 03:42:12.053048 139973912856384 submission_runner.py:380] Time since start: 5204.99s, 	Step: 3954, 	{'train/ctc_loss': DeviceArray(1.8151625, dtype=float32), 'train/wer': 0.45926004847774554, 'validation/ctc_loss': DeviceArray(2.3310573, dtype=float32), 'validation/wer': 0.5224459473801002, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.8209875, dtype=float32), 'test/wer': 0.44953588040541914, 'test/num_examples': 2472}
I0330 03:42:12.054462 139973912856384 submission_runner.py:390] After eval at step 3954: RAM USED (GB) 19.257618432
I0330 03:42:12.074143 139799261878016 logging_writer.py:48] [3954] global_step=3954, preemption_count=0, score=4852.050470, test/ctc_loss=1.8209874629974365, test/num_examples=2472, test/wer=0.449536, total_duration=5204.994835, train/ctc_loss=1.8151625394821167, train/wer=0.459260, validation/ctc_loss=2.33105731010437, validation/num_examples=5348, validation/wer=0.522446
I0330 03:42:12.244189 139973912856384 checkpoints.py:356] Saving checkpoint at step: 3954
I0330 03:42:12.967141 139973912856384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_3954
I0330 03:42:12.981361 139973912856384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_3954.
I0330 03:42:12.989431 139973912856384 submission_runner.py:409] After logging and checkpointing eval at step 3954: RAM USED (GB) 19.231301632
I0330 03:43:09.511576 139799253485312 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.1661243438720703, loss=2.2633018493652344
I0330 03:45:09.579614 139799203129088 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.1349310874938965, loss=2.5461065769195557
I0330 03:47:14.131609 139799261878016 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.34161901473999, loss=2.3268182277679443
I0330 03:49:14.582305 139799253485312 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.200465202331543, loss=2.2848620414733887
I0330 03:51:14.943585 139799261878016 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.6729021072387695, loss=2.1196558475494385
I0330 03:53:15.079069 139799253485312 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.3115792274475098, loss=2.1765313148498535
I0330 03:55:15.225660 139799261878016 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.42143177986145, loss=2.1993167400360107
I0330 03:57:15.659256 139799253485312 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.764188766479492, loss=2.168698787689209
I0330 03:59:15.831814 139799261878016 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.1869819164276123, loss=2.1713502407073975
I0330 04:01:16.157329 139799253485312 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.949354648590088, loss=2.0867347717285156
I0330 04:03:16.811222 139799261878016 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.030579090118408, loss=2.122875452041626
I0330 04:05:17.441275 139799253485312 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.083132028579712, loss=2.047576427459717
I0330 04:07:22.135847 139799261878016 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.1327428817749023, loss=2.069321632385254
I0330 04:09:22.747249 139799253485312 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.001462936401367, loss=2.0461623668670654
I0330 04:11:22.657808 139799261878016 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.4476420879364014, loss=2.0675575733184814
I0330 04:13:22.609568 139799253485312 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.6631100177764893, loss=2.111010789871216
I0330 04:15:22.828766 139799261878016 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.1993627548217773, loss=2.0223233699798584
I0330 04:17:23.181060 139799253485312 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.5895748138427734, loss=2.017343759536743
I0330 04:19:23.344639 139799261878016 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.012396812438965, loss=2.0275015830993652
I0330 04:21:23.218769 139799253485312 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.3914501667022705, loss=1.9776057004928589
I0330 04:22:13.390721 139973912856384 submission_runner.py:371] Before eval at step 5943: RAM USED (GB) 20.276826112
I0330 04:22:13.390967 139973912856384 spec.py:298] Evaluating on the training split.
I0330 04:23:01.422660 139973912856384 spec.py:310] Evaluating on the validation split.
I0330 04:23:44.616063 139973912856384 spec.py:326] Evaluating on the test split.
I0330 04:24:06.675716 139973912856384 submission_runner.py:380] Time since start: 7717.30s, 	Step: 5943, 	{'train/ctc_loss': DeviceArray(0.74849457, dtype=float32), 'train/wer': 0.25507745992588726, 'validation/ctc_loss': DeviceArray(1.1911885, dtype=float32), 'validation/wer': 0.3280301787764474, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.8262499, dtype=float32), 'test/wer': 0.2626896593748096, 'test/num_examples': 2472}
I0330 04:24:06.676903 139973912856384 submission_runner.py:390] After eval at step 5943: RAM USED (GB) 19.183824896
I0330 04:24:06.695548 139799261878016 logging_writer.py:48] [5943] global_step=5943, preemption_count=0, score=7248.046773, test/ctc_loss=0.826249897480011, test/num_examples=2472, test/wer=0.262690, total_duration=7717.302804, train/ctc_loss=0.748494565486908, train/wer=0.255077, validation/ctc_loss=1.1911884546279907, validation/num_examples=5348, validation/wer=0.328030
I0330 04:24:06.854660 139973912856384 checkpoints.py:356] Saving checkpoint at step: 5943
I0330 04:24:07.481123 139973912856384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_5943
I0330 04:24:07.496126 139973912856384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_5943.
I0330 04:24:07.503989 139973912856384 submission_runner.py:409] After logging and checkpointing eval at step 5943: RAM USED (GB) 19.159277568
I0330 04:25:17.056273 139799253485312 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.1706135272979736, loss=1.960551381111145
I0330 04:27:16.991831 139799138662144 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.6827034950256348, loss=2.094501256942749
I0330 04:29:20.623651 139799261878016 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.1917269229888916, loss=1.8875430822372437
I0330 04:31:20.256965 139799253485312 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.871917724609375, loss=2.0130841732025146
I0330 04:33:19.545893 139799261878016 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.161593437194824, loss=1.9426802396774292
I0330 04:35:19.202126 139799253485312 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.17425274848938, loss=1.9742604494094849
I0330 04:37:19.138942 139799261878016 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.9231616258621216, loss=1.9492493867874146
I0330 04:39:19.485668 139799253485312 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.772986650466919, loss=1.9460569620132446
I0330 04:41:19.302228 139799261878016 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.480987548828125, loss=1.9351842403411865
I0330 04:43:19.268195 139799253485312 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.6082046031951904, loss=1.9379441738128662
I0330 04:45:20.163351 139799261878016 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.2175066471099854, loss=1.9571688175201416
I0330 04:47:20.505481 139799253485312 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.513000249862671, loss=1.9683725833892822
I0330 04:49:20.669590 139799261878016 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.3774874210357666, loss=1.9087907075881958
I0330 04:51:24.604041 139799261878016 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.522839307785034, loss=1.8878254890441895
I0330 04:53:24.426656 139799253485312 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.452754497528076, loss=1.8831032514572144
I0330 04:55:24.236294 139799261878016 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.1439030170440674, loss=1.890138030052185
I0330 04:57:24.412144 139799253485312 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.8374273777008057, loss=1.894643783569336
I0330 04:59:24.603383 139799261878016 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.618030309677124, loss=1.9335204362869263
I0330 05:01:24.591908 139799253485312 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.3112237453460693, loss=1.8560066223144531
I0330 05:03:24.803316 139799261878016 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.4716694355010986, loss=1.859108567237854
I0330 05:04:07.833152 139973912856384 submission_runner.py:371] Before eval at step 7937: RAM USED (GB) 20.334927872
I0330 05:04:07.833344 139973912856384 spec.py:298] Evaluating on the training split.
I0330 05:04:57.672600 139973912856384 spec.py:310] Evaluating on the validation split.
I0330 05:05:40.862242 139973912856384 spec.py:326] Evaluating on the test split.
I0330 05:06:02.981718 139973912856384 submission_runner.py:380] Time since start: 10231.75s, 	Step: 7937, 	{'train/ctc_loss': DeviceArray(0.6199686, dtype=float32), 'train/wer': 0.20857899862811424, 'validation/ctc_loss': DeviceArray(1.0008112, dtype=float32), 'validation/wer': 0.27902825883510696, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6620631, dtype=float32), 'test/wer': 0.21473401986472487, 'test/num_examples': 2472}
I0330 05:06:02.982884 139973912856384 submission_runner.py:390] After eval at step 7937: RAM USED (GB) 19.190460416
I0330 05:06:03.001643 139799415478016 logging_writer.py:48] [7937] global_step=7937, preemption_count=0, score=9643.829659, test/ctc_loss=0.6620631217956543, test/num_examples=2472, test/wer=0.214734, total_duration=10231.745504, train/ctc_loss=0.619968593120575, train/wer=0.208579, validation/ctc_loss=1.000811219215393, validation/num_examples=5348, validation/wer=0.279028
I0330 05:06:03.166545 139973912856384 checkpoints.py:356] Saving checkpoint at step: 7937
I0330 05:06:03.802720 139973912856384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_7937
I0330 05:06:03.817022 139973912856384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_7937.
I0330 05:06:03.820914 139973912856384 submission_runner.py:409] After logging and checkpointing eval at step 7937: RAM USED (GB) 19.25277696
I0330 05:07:19.052409 139973912856384 submission_runner.py:371] Before eval at step 8000: RAM USED (GB) 20.321861632
I0330 05:07:19.052650 139973912856384 spec.py:298] Evaluating on the training split.
I0330 05:08:06.272460 139973912856384 spec.py:310] Evaluating on the validation split.
I0330 05:08:46.349470 139973912856384 spec.py:326] Evaluating on the test split.
I0330 05:09:06.573570 139973912856384 submission_runner.py:380] Time since start: 10422.96s, 	Step: 8000, 	{'train/ctc_loss': DeviceArray(0.5844856, dtype=float32), 'train/wer': 0.1985583509100002, 'validation/ctc_loss': DeviceArray(0.95582587, dtype=float32), 'validation/wer': 0.26809713552470354, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.62387866, dtype=float32), 'test/wer': 0.2002518635874312, 'test/num_examples': 2472}
I0330 05:09:06.574829 139973912856384 submission_runner.py:390] After eval at step 8000: RAM USED (GB) 20.6339072
I0330 05:09:06.591532 139799415478016 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9718.902076, test/ctc_loss=0.6238786578178406, test/num_examples=2472, test/wer=0.200252, total_duration=10422.964996, train/ctc_loss=0.5844855904579163, train/wer=0.198558, validation/ctc_loss=0.9558258652687073, validation/num_examples=5348, validation/wer=0.268097
I0330 05:09:06.743519 139973912856384 checkpoints.py:356] Saving checkpoint at step: 8000
I0330 05:09:07.365363 139973912856384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0330 05:09:07.379748 139973912856384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0330 05:09:07.386473 139973912856384 submission_runner.py:409] After logging and checkpointing eval at step 8000: RAM USED (GB) 20.664815616
I0330 05:09:07.394045 139799407085312 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9718.902076
I0330 05:09:07.491352 139973912856384 checkpoints.py:356] Saving checkpoint at step: 8000
I0330 05:09:08.399042 139973912856384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0330 05:09:08.413494 139973912856384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0330 05:09:09.831923 139973912856384 submission_runner.py:543] Tuning trial 1/1
I0330 05:09:09.832136 139973912856384 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0330 05:09:09.837087 139973912856384 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.630753, dtype=float32), 'train/wer': 4.502873623055619, 'validation/ctc_loss': DeviceArray(31.380888, dtype=float32), 'validation/wer': 4.151279800094549, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.527367, dtype=float32), 'test/wer': 4.315032600085309, 'test/num_examples': 2472, 'score': 58.92023849487305, 'total_duration': 59.14972496032715, 'global_step': 1, 'preemption_count': 0}), (1965, {'train/ctc_loss': DeviceArray(6.08357, dtype=float32), 'train/wer': 0.9302331568865729, 'validation/ctc_loss': DeviceArray(6.029016, dtype=float32), 'validation/wer': 0.8892705187700798, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.9623356, dtype=float32), 'test/wer': 0.8921048889972173, 'test/num_examples': 2472, 'score': 2455.7032310962677, 'total_duration': 2713.1555457115173, 'global_step': 1965, 'preemption_count': 0}), (3954, {'train/ctc_loss': DeviceArray(1.8151625, dtype=float32), 'train/wer': 0.45926004847774554, 'validation/ctc_loss': DeviceArray(2.3310573, dtype=float32), 'validation/wer': 0.5224459473801002, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.8209875, dtype=float32), 'test/wer': 0.44953588040541914, 'test/num_examples': 2472, 'score': 4852.050469636917, 'total_duration': 5204.9948353767395, 'global_step': 3954, 'preemption_count': 0}), (5943, {'train/ctc_loss': DeviceArray(0.74849457, dtype=float32), 'train/wer': 0.25507745992588726, 'validation/ctc_loss': DeviceArray(1.1911885, dtype=float32), 'validation/wer': 0.3280301787764474, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.8262499, dtype=float32), 'test/wer': 0.2626896593748096, 'test/num_examples': 2472, 'score': 7248.04677271843, 'total_duration': 7717.3028037548065, 'global_step': 5943, 'preemption_count': 0}), (7937, {'train/ctc_loss': DeviceArray(0.6199686, dtype=float32), 'train/wer': 0.20857899862811424, 'validation/ctc_loss': DeviceArray(1.0008112, dtype=float32), 'validation/wer': 0.27902825883510696, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6620631, dtype=float32), 'test/wer': 0.21473401986472487, 'test/num_examples': 2472, 'score': 9643.829659223557, 'total_duration': 10231.745503902435, 'global_step': 7937, 'preemption_count': 0}), (8000, {'train/ctc_loss': DeviceArray(0.5844856, dtype=float32), 'train/wer': 0.1985583509100002, 'validation/ctc_loss': DeviceArray(0.95582587, dtype=float32), 'validation/wer': 0.26809713552470354, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.62387866, dtype=float32), 'test/wer': 0.2002518635874312, 'test/num_examples': 2472, 'score': 9718.902075529099, 'total_duration': 10422.964996099472, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0330 05:09:09.837278 139973912856384 submission_runner.py:546] Timing: 9718.902075529099
I0330 05:09:09.837347 139973912856384 submission_runner.py:547] ====================
I0330 05:09:09.837774 139973912856384 submission_runner.py:606] Final librispeech_deepspeech score: 9718.902075529099
