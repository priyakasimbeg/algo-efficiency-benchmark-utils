I0331 18:08:37.589465 140492810225472 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/imagenet_resnet_jax.
I0331 18:08:37.635272 140492810225472 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0331 18:08:38.578596 140492810225472 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0331 18:08:38.579324 140492810225472 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0331 18:08:38.582966 140492810225472 submission_runner.py:511] Using RNG seed 3434951561
I0331 18:08:40.041474 140492810225472 submission_runner.py:520] --- Tuning run 1/1 ---
I0331 18:08:40.041712 140492810225472 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1.
I0331 18:08:40.041934 140492810225472 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/hparams.json.
I0331 18:08:40.168918 140492810225472 submission_runner.py:230] Starting train once: RAM USED (GB) 4.256632832
I0331 18:08:40.169116 140492810225472 submission_runner.py:231] Initializing dataset.
I0331 18:08:40.180363 140492810225472 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:08:40.188429 140492810225472 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 18:08:40.188552 140492810225472 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 18:08:40.408621 140492810225472 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:08:41.355627 140492810225472 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.326391808
I0331 18:08:41.355832 140492810225472 submission_runner.py:240] Initializing model.
I0331 18:08:52.512413 140492810225472 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.398782464
I0331 18:08:52.512602 140492810225472 submission_runner.py:252] Initializing optimizer.
I0331 18:08:53.445229 140492810225472 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.399028224
I0331 18:08:53.445404 140492810225472 submission_runner.py:261] Initializing metrics bundle.
I0331 18:08:53.445450 140492810225472 submission_runner.py:276] Initializing checkpoint and logger.
I0331 18:08:53.446403 140492810225472 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0331 18:08:54.219793 140492810225472 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0331 18:08:54.220697 140492810225472 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/flags_0.json.
I0331 18:08:54.223502 140492810225472 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.398794752
I0331 18:08:54.223704 140492810225472 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.398794752
I0331 18:08:54.223799 140492810225472 submission_runner.py:313] Starting training loop.
I0331 18:08:57.972188 140492810225472 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 14.94110208
I0331 18:09:35.454633 140313951725312 logging_writer.py:48] [0] global_step=0, grad_norm=0.5280007123947144, loss=6.927021026611328
I0331 18:09:35.466224 140492810225472 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 25.62338816
I0331 18:09:35.466446 140492810225472 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 25.62338816
I0331 18:09:35.466533 140492810225472 spec.py:298] Evaluating on the training split.
I0331 18:09:35.936707 140492810225472 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:09:35.942937 140492810225472 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 18:09:35.943043 140492810225472 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 18:09:36.001136 140492810225472 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:09:47.595269 140492810225472 spec.py:310] Evaluating on the validation split.
I0331 18:09:48.244389 140492810225472 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:09:48.259636 140492810225472 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 18:09:48.259971 140492810225472 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 18:09:48.322484 140492810225472 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:10:05.966499 140492810225472 spec.py:326] Evaluating on the test split.
I0331 18:10:06.364492 140492810225472 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0331 18:10:06.369530 140492810225472 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0331 18:10:06.398857 140492810225472 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0331 18:10:14.944413 140492810225472 submission_runner.py:382] Time since start: 41.24s, 	Step: 1, 	{'train/accuracy': 0.0008769132546149194, 'train/loss': 6.913438320159912, 'validation/accuracy': 0.0008199999574571848, 'validation/loss': 6.912744998931885, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.912395477294922, 'test/num_examples': 10000}
I0331 18:10:14.945101 140492810225472 submission_runner.py:396] After eval at step 1: RAM USED (GB) 67.187191808
I0331 18:10:14.951887 140286307067648 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=41.162198, test/accuracy=0.001300, test/loss=6.912395, test/num_examples=10000, total_duration=41.242731, train/accuracy=0.000877, train/loss=6.913438, validation/accuracy=0.000820, validation/loss=6.912745, validation/num_examples=50000
I0331 18:10:15.047121 140492810225472 checkpoints.py:356] Saving checkpoint at step: 1
I0331 18:10:15.481513 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_1
I0331 18:10:15.482503 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_1.
I0331 18:10:15.486857 140492810225472 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 67.180879872
I0331 18:10:15.491685 140492810225472 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 67.181395968
I0331 18:10:15.564553 140492810225472 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 67.498160128
I0331 18:10:49.210071 140286315460352 logging_writer.py:48] [100] global_step=100, grad_norm=0.5383554697036743, loss=6.887284755706787
I0331 18:11:23.153153 140286600648448 logging_writer.py:48] [200] global_step=200, grad_norm=0.5658264756202698, loss=6.8057756423950195
I0331 18:11:56.897124 140286315460352 logging_writer.py:48] [300] global_step=300, grad_norm=0.6047660112380981, loss=6.69887638092041
I0331 18:12:30.624617 140286600648448 logging_writer.py:48] [400] global_step=400, grad_norm=0.6297231912612915, loss=6.634616851806641
I0331 18:13:04.413625 140286315460352 logging_writer.py:48] [500] global_step=500, grad_norm=0.6398776173591614, loss=6.56640625
I0331 18:13:38.195134 140286600648448 logging_writer.py:48] [600] global_step=600, grad_norm=0.6402162909507751, loss=6.447594165802002
I0331 18:14:12.064826 140286315460352 logging_writer.py:48] [700] global_step=700, grad_norm=0.9657257795333862, loss=6.421102523803711
I0331 18:14:45.857380 140286600648448 logging_writer.py:48] [800] global_step=800, grad_norm=0.9059411883354187, loss=6.356993198394775
I0331 18:15:19.468622 140286315460352 logging_writer.py:48] [900] global_step=900, grad_norm=0.737480878829956, loss=6.314388751983643
I0331 18:15:53.279090 140286600648448 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8214510083198547, loss=6.276758670806885
I0331 18:16:27.019155 140286315460352 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8266322016716003, loss=6.177850723266602
I0331 18:17:00.650479 140286600648448 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9075190424919128, loss=6.180686950683594
I0331 18:17:34.325520 140286315460352 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8562682271003723, loss=6.102817058563232
I0331 18:18:08.061670 140286600648448 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.980847954750061, loss=6.028143882751465
I0331 18:18:41.930538 140286315460352 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0607227087020874, loss=6.013572692871094
I0331 18:18:45.679095 140492810225472 submission_runner.py:373] Before eval at step 1513: RAM USED (GB) 69.225291776
I0331 18:18:45.679312 140492810225472 spec.py:298] Evaluating on the training split.
I0331 18:18:52.450220 140492810225472 spec.py:310] Evaluating on the validation split.
I0331 18:19:00.187520 140492810225472 spec.py:326] Evaluating on the test split.
I0331 18:19:02.278394 140492810225472 submission_runner.py:382] Time since start: 591.45s, 	Step: 1513, 	{'train/accuracy': 0.07479671388864517, 'train/loss': 5.3983049392700195, 'validation/accuracy': 0.06746000051498413, 'validation/loss': 5.48199987411499, 'validation/num_examples': 50000, 'test/accuracy': 0.049400001764297485, 'test/loss': 5.71992301940918, 'test/num_examples': 10000}
I0331 18:19:02.279149 140492810225472 submission_runner.py:396] After eval at step 1513: RAM USED (GB) 74.819026944
I0331 18:19:02.286843 140286617433856 logging_writer.py:48] [1513] global_step=1513, preemption_count=0, score=545.908726, test/accuracy=0.049400, test/loss=5.719923, test/num_examples=10000, total_duration=591.454216, train/accuracy=0.074797, train/loss=5.398305, validation/accuracy=0.067460, validation/loss=5.482000, validation/num_examples=50000
I0331 18:19:02.406409 140492810225472 checkpoints.py:356] Saving checkpoint at step: 1513
I0331 18:19:02.976187 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_1513
I0331 18:19:02.977042 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_1513.
I0331 18:19:02.981014 140492810225472 submission_runner.py:416] After logging and checkpointing eval at step 1513: RAM USED (GB) 74.83492352
I0331 18:19:32.628719 140286625826560 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8613177537918091, loss=5.8866071701049805
I0331 18:20:06.382732 140315356813056 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.1776310205459595, loss=5.892334938049316
I0331 18:20:40.181124 140286625826560 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8815357089042664, loss=5.801286697387695
I0331 18:21:13.815902 140315356813056 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8865228891372681, loss=5.758434295654297
I0331 18:21:47.421319 140286625826560 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.000009298324585, loss=5.645257949829102
I0331 18:22:21.166784 140315356813056 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9173833131790161, loss=5.712554454803467
I0331 18:22:54.858562 140286625826560 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0816802978515625, loss=5.594028949737549
I0331 18:23:28.465470 140315356813056 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9841458797454834, loss=5.502130031585693
I0331 18:24:02.314089 140286625826560 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.043479084968567, loss=5.417691230773926
I0331 18:24:35.900094 140315356813056 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.0415542125701904, loss=5.423309326171875
I0331 18:25:09.525074 140286625826560 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9690129160881042, loss=5.353070259094238
I0331 18:25:43.150365 140315356813056 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.008595585823059, loss=5.223794937133789
I0331 18:26:16.801654 140286625826560 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9656664729118347, loss=5.204773426055908
I0331 18:26:50.354003 140315356813056 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9068037867546082, loss=5.197956085205078
I0331 18:27:23.817987 140286625826560 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9431264996528625, loss=5.176587104797363
I0331 18:27:33.001252 140492810225472 submission_runner.py:373] Before eval at step 3029: RAM USED (GB) 75.89136384
I0331 18:27:33.001496 140492810225472 spec.py:298] Evaluating on the training split.
I0331 18:27:39.763522 140492810225472 spec.py:310] Evaluating on the validation split.
I0331 18:27:47.525432 140492810225472 spec.py:326] Evaluating on the test split.
I0331 18:27:49.601066 140492810225472 submission_runner.py:382] Time since start: 1118.78s, 	Step: 3029, 	{'train/accuracy': 0.202985480427742, 'train/loss': 4.152096748352051, 'validation/accuracy': 0.18767999112606049, 'validation/loss': 4.262604713439941, 'validation/num_examples': 50000, 'test/accuracy': 0.13650000095367432, 'test/loss': 4.73514461517334, 'test/num_examples': 10000}
I0331 18:27:49.601783 140492810225472 submission_runner.py:396] After eval at step 3029: RAM USED (GB) 81.157640192
I0331 18:27:49.609652 140315356813056 logging_writer.py:48] [3029] global_step=3029, preemption_count=0, score=1050.682211, test/accuracy=0.136500, test/loss=4.735145, test/num_examples=10000, total_duration=1118.776473, train/accuracy=0.202985, train/loss=4.152097, validation/accuracy=0.187680, validation/loss=4.262605, validation/num_examples=50000
I0331 18:27:49.701378 140492810225472 checkpoints.py:356] Saving checkpoint at step: 3029
I0331 18:27:50.281157 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_3029
I0331 18:27:50.282143 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_3029.
I0331 18:27:50.286281 140492810225472 submission_runner.py:416] After logging and checkpointing eval at step 3029: RAM USED (GB) 81.120632832
I0331 18:28:14.481200 140286625826560 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9775523543357849, loss=5.0537848472595215
I0331 18:28:48.148931 140315323242240 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.978239893913269, loss=5.077554702758789
I0331 18:29:22.006962 140286625826560 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0050644874572754, loss=5.021604537963867
I0331 18:29:55.936008 140315323242240 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9513294696807861, loss=5.024413585662842
I0331 18:30:29.530929 140286625826560 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8755013942718506, loss=4.932283401489258
I0331 18:31:03.226320 140315323242240 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9066846966743469, loss=4.896994590759277
I0331 18:31:36.808559 140286625826560 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8814094662666321, loss=4.843672752380371
I0331 18:32:10.499580 140315323242240 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8633161783218384, loss=4.749738693237305
I0331 18:32:44.239041 140286625826560 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8808960318565369, loss=4.772352695465088
I0331 18:33:17.847061 140315323242240 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8677549958229065, loss=4.72224760055542
I0331 18:33:51.478857 140286625826560 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8390161395072937, loss=4.713026523590088
I0331 18:34:25.119050 140315323242240 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9515456557273865, loss=4.672024250030518
I0331 18:34:58.815683 140286625826560 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8503261208534241, loss=4.616013050079346
I0331 18:35:32.503139 140315323242240 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7948068976402283, loss=4.593279838562012
I0331 18:36:06.117429 140286625826560 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8305069208145142, loss=4.620634078979492
I0331 18:36:20.570893 140492810225472 submission_runner.py:373] Before eval at step 4545: RAM USED (GB) 82.017415168
I0331 18:36:20.571084 140492810225472 spec.py:298] Evaluating on the training split.
I0331 18:36:27.334736 140492810225472 spec.py:310] Evaluating on the validation split.
I0331 18:36:35.312311 140492810225472 spec.py:326] Evaluating on the test split.
I0331 18:36:37.484988 140492810225472 submission_runner.py:382] Time since start: 1646.35s, 	Step: 4545, 	{'train/accuracy': 0.3386479616165161, 'train/loss': 3.2850255966186523, 'validation/accuracy': 0.3125999867916107, 'validation/loss': 3.412715435028076, 'validation/num_examples': 50000, 'test/accuracy': 0.2346000075340271, 'test/loss': 3.9955453872680664, 'test/num_examples': 10000}
I0331 18:36:37.485693 140492810225472 submission_runner.py:396] After eval at step 4545: RAM USED (GB) 87.354507264
I0331 18:36:37.494151 140315323242240 logging_writer.py:48] [4545] global_step=4545, preemption_count=0, score=1555.646478, test/accuracy=0.234600, test/loss=3.995545, test/num_examples=10000, total_duration=1646.346014, train/accuracy=0.338648, train/loss=3.285026, validation/accuracy=0.312600, validation/loss=3.412715, validation/num_examples=50000
I0331 18:36:37.613971 140492810225472 checkpoints.py:356] Saving checkpoint at step: 4545
I0331 18:36:38.402040 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_4545
I0331 18:36:38.411319 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_4545.
I0331 18:36:38.419450 140492810225472 submission_runner.py:416] After logging and checkpointing eval at step 4545: RAM USED (GB) 87.293988864
I0331 18:36:57.225656 140286625826560 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7860990166664124, loss=4.552820205688477
I0331 18:37:30.785152 140315675571968 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8149877786636353, loss=4.493039131164551
I0331 18:38:04.531168 140286625826560 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7964275479316711, loss=4.470127582550049
I0331 18:38:38.252448 140315675571968 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.745957612991333, loss=4.429436683654785
I0331 18:39:11.869593 140286625826560 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7098378539085388, loss=4.3661651611328125
I0331 18:39:45.605305 140315675571968 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7616261839866638, loss=4.43837833404541
I0331 18:40:19.255914 140286625826560 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7884964942932129, loss=4.369521617889404
I0331 18:40:52.876824 140315675571968 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7348169684410095, loss=4.441120624542236
I0331 18:41:26.501645 140286625826560 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7433509826660156, loss=4.36977481842041
I0331 18:42:00.113370 140315675571968 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7087792754173279, loss=4.326714515686035
I0331 18:42:33.783613 140286625826560 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7219356298446655, loss=4.301206588745117
I0331 18:43:07.293565 140315675571968 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.729928195476532, loss=4.197549343109131
I0331 18:43:40.839066 140286625826560 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7075600624084473, loss=4.2756500244140625
I0331 18:44:14.451341 140315675571968 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7229155898094177, loss=4.251308441162109
I0331 18:44:48.127203 140286625826560 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7198875546455383, loss=4.241512775421143
I0331 18:45:08.668172 140492810225472 submission_runner.py:373] Before eval at step 6063: RAM USED (GB) 88.083603456
I0331 18:45:08.668363 140492810225472 spec.py:298] Evaluating on the training split.
I0331 18:45:15.453161 140492810225472 spec.py:310] Evaluating on the validation split.
I0331 18:45:24.769398 140492810225472 spec.py:326] Evaluating on the test split.
I0331 18:45:26.778297 140492810225472 submission_runner.py:382] Time since start: 2174.44s, 	Step: 6063, 	{'train/accuracy': 0.42944833636283875, 'train/loss': 2.7982702255249023, 'validation/accuracy': 0.39973998069763184, 'validation/loss': 2.937286376953125, 'validation/num_examples': 50000, 'test/accuracy': 0.30090001225471497, 'test/loss': 3.5337448120117188, 'test/num_examples': 10000}
I0331 18:45:26.779000 140492810225472 submission_runner.py:396] After eval at step 6063: RAM USED (GB) 93.355900928
I0331 18:45:26.786149 140315675571968 logging_writer.py:48] [6063] global_step=6063, preemption_count=0, score=2056.477093, test/accuracy=0.300900, test/loss=3.533745, test/num_examples=10000, total_duration=2174.443322, train/accuracy=0.429448, train/loss=2.798270, validation/accuracy=0.399740, validation/loss=2.937286, validation/num_examples=50000
I0331 18:45:26.874417 140492810225472 checkpoints.py:356] Saving checkpoint at step: 6063
I0331 18:45:27.336131 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_6063
I0331 18:45:27.337207 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_6063.
I0331 18:45:27.343618 140492810225472 submission_runner.py:416] After logging and checkpointing eval at step 6063: RAM USED (GB) 93.309403136
I0331 18:45:40.154558 140286625826560 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6992717981338501, loss=4.053840637207031
I0331 18:46:13.962481 140313964312320 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6738542914390564, loss=4.143277168273926
I0331 18:46:47.833289 140286625826560 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7005627155303955, loss=4.219181060791016
I0331 18:47:21.699321 140313964312320 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7206849455833435, loss=4.121140003204346
I0331 18:47:55.560688 140286625826560 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6723565459251404, loss=4.1847615242004395
I0331 18:48:29.266291 140313964312320 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7384852170944214, loss=4.181596279144287
I0331 18:49:03.040766 140286625826560 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6697955131530762, loss=4.106357097625732
I0331 18:49:36.924584 140313964312320 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.643068253993988, loss=4.136188507080078
I0331 18:50:10.727104 140286625826560 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6421982049942017, loss=4.054207801818848
I0331 18:50:44.442739 140313964312320 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6560343503952026, loss=4.125535488128662
I0331 18:51:18.166131 140286625826560 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6363911628723145, loss=4.065489292144775
I0331 18:51:51.891533 140313964312320 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6201937794685364, loss=4.093400001525879
I0331 18:52:25.753977 140286625826560 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6277615427970886, loss=4.040008544921875
I0331 18:52:59.750285 140313964312320 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.643976628780365, loss=4.020266056060791
I0331 18:53:33.663094 140286625826560 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6104875206947327, loss=3.9553420543670654
I0331 18:53:57.369476 140492810225472 submission_runner.py:373] Before eval at step 7572: RAM USED (GB) 94.303215616
I0331 18:53:57.369665 140492810225472 spec.py:298] Evaluating on the training split.
I0331 18:54:04.285822 140492810225472 spec.py:310] Evaluating on the validation split.
I0331 18:54:13.763910 140492810225472 spec.py:326] Evaluating on the test split.
I0331 18:54:15.869354 140492810225472 submission_runner.py:382] Time since start: 2703.14s, 	Step: 7572, 	{'train/accuracy': 0.4870455861091614, 'train/loss': 2.4594533443450928, 'validation/accuracy': 0.45795997977256775, 'validation/loss': 2.5899932384490967, 'validation/num_examples': 50000, 'test/accuracy': 0.353300005197525, 'test/loss': 3.235905170440674, 'test/num_examples': 10000}
I0331 18:54:15.870014 140492810225472 submission_runner.py:396] After eval at step 7572: RAM USED (GB) 99.512078336
I0331 18:54:15.878440 140313964312320 logging_writer.py:48] [7572] global_step=7572, preemption_count=0, score=2557.216311, test/accuracy=0.353300, test/loss=3.235905, test/num_examples=10000, total_duration=2703.144650, train/accuracy=0.487046, train/loss=2.459453, validation/accuracy=0.457960, validation/loss=2.589993, validation/num_examples=50000
I0331 18:54:15.975871 140492810225472 checkpoints.py:356] Saving checkpoint at step: 7572
I0331 18:54:16.405646 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_7572
I0331 18:54:16.406666 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_7572.
I0331 18:54:16.413279 140492810225472 submission_runner.py:416] After logging and checkpointing eval at step 7572: RAM USED (GB) 99.45731072
I0331 18:54:26.209949 140286625826560 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6719416975975037, loss=4.0209269523620605
I0331 18:54:59.855043 140313221904128 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6087273955345154, loss=4.111171245574951
I0331 18:55:33.404167 140286625826560 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5966833829879761, loss=3.9760775566101074
I0331 18:56:07.053882 140313221904128 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6092343330383301, loss=3.978454351425171
I0331 18:56:40.866453 140286625826560 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5967317223548889, loss=3.8939950466156006
I0331 18:57:14.374063 140313221904128 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5892918705940247, loss=3.91347336769104
I0331 18:57:47.918097 140286625826560 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5999246835708618, loss=3.9052741527557373
I0331 18:58:21.509016 140313221904128 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5918688178062439, loss=3.90881609916687
I0331 18:58:55.090266 140286625826560 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5970929265022278, loss=3.9269182682037354
I0331 18:59:28.951981 140313221904128 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5607119798660278, loss=3.817401647567749
I0331 19:00:02.575131 140286625826560 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5819985866546631, loss=3.868443489074707
I0331 19:00:36.326116 140313221904128 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5713673830032349, loss=3.8367559909820557
I0331 19:01:09.945755 140286625826560 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5886427760124207, loss=3.8469748497009277
I0331 19:01:43.614118 140313221904128 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6023100018501282, loss=3.8880350589752197
I0331 19:02:17.250952 140286625826560 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5669227242469788, loss=3.742785930633545
I0331 19:02:46.588517 140492810225472 submission_runner.py:373] Before eval at step 9089: RAM USED (GB) 100.272316416
I0331 19:02:46.588712 140492810225472 spec.py:298] Evaluating on the training split.
I0331 19:02:53.547705 140492810225472 spec.py:310] Evaluating on the validation split.
I0331 19:03:03.001533 140492810225472 spec.py:326] Evaluating on the test split.
I0331 19:03:05.007875 140492810225472 submission_runner.py:382] Time since start: 3232.36s, 	Step: 9089, 	{'train/accuracy': 0.5962810516357422, 'train/loss': 1.9831054210662842, 'validation/accuracy': 0.5289799571037292, 'validation/loss': 2.2887020111083984, 'validation/num_examples': 50000, 'test/accuracy': 0.4082000255584717, 'test/loss': 2.9440462589263916, 'test/num_examples': 10000}
I0331 19:03:05.008553 140492810225472 submission_runner.py:396] After eval at step 9089: RAM USED (GB) 105.342070784
I0331 19:03:05.016933 140313221904128 logging_writer.py:48] [9089] global_step=9089, preemption_count=0, score=3059.890028, test/accuracy=0.408200, test/loss=2.944046, test/num_examples=10000, total_duration=3232.363772, train/accuracy=0.596281, train/loss=1.983105, validation/accuracy=0.528980, validation/loss=2.288702, validation/num_examples=50000
I0331 19:03:05.126144 140492810225472 checkpoints.py:356] Saving checkpoint at step: 9089
I0331 19:03:05.617881 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_9089
I0331 19:03:05.622014 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_9089.
I0331 19:03:05.625478 140492810225472 submission_runner.py:416] After logging and checkpointing eval at step 9089: RAM USED (GB) 105.416810496
I0331 19:03:09.681339 140286625826560 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6368611454963684, loss=3.8163483142852783
I0331 19:03:43.459555 140313213511424 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5611737370491028, loss=3.8467624187469482
I0331 19:04:17.203195 140286625826560 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5312787294387817, loss=3.648012638092041
I0331 19:04:50.888273 140313213511424 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.554180383682251, loss=3.7573904991149902
I0331 19:05:24.547823 140286625826560 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5337629318237305, loss=3.7089314460754395
I0331 19:05:58.214776 140313213511424 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5366692543029785, loss=3.6762664318084717
I0331 19:06:31.957419 140286625826560 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5320885181427002, loss=3.8276424407958984
I0331 19:07:05.700115 140313213511424 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5456727147102356, loss=3.746588706970215
I0331 19:07:39.360929 140286625826560 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5177643895149231, loss=3.766353130340576
I0331 19:08:13.124830 140313213511424 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5213523507118225, loss=3.7298126220703125
I0331 19:08:46.730896 140286625826560 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5299716591835022, loss=3.628147840499878
I0331 19:09:20.430251 140313213511424 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5593293309211731, loss=3.7559831142425537
I0331 19:09:54.226902 140286625826560 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.530493974685669, loss=3.84537410736084
I0331 19:10:27.735409 140313213511424 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5199832916259766, loss=3.689140796661377
I0331 19:11:01.397065 140286625826560 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5150021910667419, loss=3.698822498321533
I0331 19:11:35.116393 140313213511424 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5304486751556396, loss=3.657902956008911
I0331 19:11:35.876678 140492810225472 submission_runner.py:373] Before eval at step 10604: RAM USED (GB) 105.932681216
I0331 19:11:35.876888 140492810225472 spec.py:298] Evaluating on the training split.
I0331 19:11:42.907009 140492810225472 spec.py:310] Evaluating on the validation split.
I0331 19:11:51.945608 140492810225472 spec.py:326] Evaluating on the test split.
I0331 19:11:54.050919 140492810225472 submission_runner.py:382] Time since start: 3761.65s, 	Step: 10604, 	{'train/accuracy': 0.6039939522743225, 'train/loss': 1.9151510000228882, 'validation/accuracy': 0.5454199910163879, 'validation/loss': 2.191735029220581, 'validation/num_examples': 50000, 'test/accuracy': 0.4294000267982483, 'test/loss': 2.8266894817352295, 'test/num_examples': 10000}
I0331 19:11:54.051532 140492810225472 submission_runner.py:396] After eval at step 10604: RAM USED (GB) 111.02877696
I0331 19:11:54.058985 140286625826560 logging_writer.py:48] [10604] global_step=10604, preemption_count=0, score=3564.664005, test/accuracy=0.429400, test/loss=2.826689, test/num_examples=10000, total_duration=3761.651464, train/accuracy=0.603994, train/loss=1.915151, validation/accuracy=0.545420, validation/loss=2.191735, validation/num_examples=50000
I0331 19:11:54.193829 140492810225472 checkpoints.py:356] Saving checkpoint at step: 10604
I0331 19:11:54.793018 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_10604
I0331 19:11:54.801549 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_10604.
I0331 19:11:54.806810 140492810225472 submission_runner.py:416] After logging and checkpointing eval at step 10604: RAM USED (GB) 111.053471744
I0331 19:12:27.474653 140313213511424 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5385282039642334, loss=3.7306466102600098
I0331 19:13:01.112032 140313146369792 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5262804627418518, loss=3.679159641265869
I0331 19:13:34.766712 140313213511424 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.544160783290863, loss=3.611457109451294
I0331 19:14:08.284444 140313146369792 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5301781892776489, loss=3.6992270946502686
I0331 19:14:41.815524 140313213511424 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5144355297088623, loss=3.599914073944092
I0331 19:15:15.555694 140313146369792 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5162976980209351, loss=3.5553345680236816
I0331 19:15:49.188533 140313213511424 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5275043845176697, loss=3.6839754581451416
I0331 19:16:22.882348 140313146369792 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.518312394618988, loss=3.593341112136841
I0331 19:16:56.444207 140313213511424 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.515204131603241, loss=3.5890302658081055
I0331 19:17:30.082849 140313146369792 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.49951815605163574, loss=3.5267105102539062
I0331 19:18:03.754755 140313213511424 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5113118886947632, loss=3.6041548252105713
I0331 19:18:37.320554 140313146369792 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.5286405682563782, loss=3.630114793777466
I0331 19:19:10.936380 140313213511424 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.553014874458313, loss=3.6498312950134277
I0331 19:19:44.485633 140313146369792 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5095570087432861, loss=3.7276782989501953
I0331 19:20:18.165629 140313213511424 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.49603599309921265, loss=3.5369338989257812
I0331 19:20:24.997839 140492810225472 submission_runner.py:373] Before eval at step 12122: RAM USED (GB) 111.723859968
I0331 19:20:24.998034 140492810225472 spec.py:298] Evaluating on the training split.
I0331 19:20:31.910368 140492810225472 spec.py:310] Evaluating on the validation split.
I0331 19:20:41.532772 140492810225472 spec.py:326] Evaluating on the test split.
I0331 19:20:43.666049 140492810225472 submission_runner.py:382] Time since start: 4290.77s, 	Step: 12122, 	{'train/accuracy': 0.6340680718421936, 'train/loss': 1.7694759368896484, 'validation/accuracy': 0.5816799998283386, 'validation/loss': 2.0031824111938477, 'validation/num_examples': 50000, 'test/accuracy': 0.46330001950263977, 'test/loss': 2.6617660522460938, 'test/num_examples': 10000}
I0331 19:20:43.666699 140492810225472 submission_runner.py:396] After eval at step 12122: RAM USED (GB) 117.043228672
I0331 19:20:43.674240 140313146369792 logging_writer.py:48] [12122] global_step=12122, preemption_count=0, score=4069.396455, test/accuracy=0.463300, test/loss=2.661766, test/num_examples=10000, total_duration=4290.772007, train/accuracy=0.634068, train/loss=1.769476, validation/accuracy=0.581680, validation/loss=2.003182, validation/num_examples=50000
I0331 19:20:43.794797 140492810225472 checkpoints.py:356] Saving checkpoint at step: 12122
I0331 19:20:44.343502 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_12122
I0331 19:20:44.346443 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_12122.
I0331 19:20:44.352561 140492810225472 submission_runner.py:416] After logging and checkpointing eval at step 12122: RAM USED (GB) 116.999286784
I0331 19:21:10.914103 140313213511424 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5080870985984802, loss=3.5582895278930664
I0331 19:21:44.519250 140313129584384 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5077957510948181, loss=3.5886189937591553
I0331 19:22:18.126544 140313213511424 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.493487685918808, loss=3.5275189876556396
I0331 19:22:51.605801 140313129584384 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5075830817222595, loss=3.538856267929077
I0331 19:23:25.194053 140313213511424 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.4960487484931946, loss=3.516317844390869
I0331 19:23:58.745284 140313129584384 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.4806284010410309, loss=3.5036630630493164
I0331 19:24:32.340851 140313213511424 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.4836663007736206, loss=3.473027229309082
I0331 19:25:05.986172 140313129584384 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5008271932601929, loss=3.549330949783325
I0331 19:25:39.613646 140313213511424 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.47089576721191406, loss=3.439565658569336
I0331 19:26:13.232453 140313129584384 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.4830458164215088, loss=3.4751033782958984
I0331 19:26:46.826045 140313213511424 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.4925789535045624, loss=3.4901161193847656
I0331 19:27:20.518279 140313129584384 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.48415905237197876, loss=3.500460624694824
I0331 19:27:54.115464 140313213511424 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.49107077717781067, loss=3.502621650695801
I0331 19:28:27.770826 140313129584384 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5068657994270325, loss=3.5303196907043457
I0331 19:29:01.480172 140313213511424 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5013185143470764, loss=3.4818153381347656
I0331 19:29:14.577357 140492810225472 submission_runner.py:373] Before eval at step 13641: RAM USED (GB) 117.695352832
I0331 19:29:14.577537 140492810225472 spec.py:298] Evaluating on the training split.
I0331 19:29:21.515455 140492810225472 spec.py:310] Evaluating on the validation split.
I0331 19:29:31.277309 140492810225472 spec.py:326] Evaluating on the test split.
I0331 19:29:33.071792 140492810225472 submission_runner.py:382] Time since start: 4820.35s, 	Step: 13641, 	{'train/accuracy': 0.6539381146430969, 'train/loss': 1.632784128189087, 'validation/accuracy': 0.6012799739837646, 'validation/loss': 1.8768268823623657, 'validation/num_examples': 50000, 'test/accuracy': 0.47460001707077026, 'test/loss': 2.5645172595977783, 'test/num_examples': 10000}
I0331 19:29:33.072464 140492810225472 submission_runner.py:396] After eval at step 13641: RAM USED (GB) 122.83066368
I0331 19:29:33.095486 140313129584384 logging_writer.py:48] [13641] global_step=13641, preemption_count=0, score=4574.204206, test/accuracy=0.474600, test/loss=2.564517, test/num_examples=10000, total_duration=4820.351521, train/accuracy=0.653938, train/loss=1.632784, validation/accuracy=0.601280, validation/loss=1.876827, validation/num_examples=50000
I0331 19:29:33.264942 140492810225472 checkpoints.py:356] Saving checkpoint at step: 13641
I0331 19:29:33.945388 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_13641
I0331 19:29:33.955091 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_13641.
I0331 19:29:33.959867 140492810225472 submission_runner.py:416] After logging and checkpointing eval at step 13641: RAM USED (GB) 122.955337728
I0331 19:29:54.176049 140313213511424 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.487022340297699, loss=3.472588539123535
I0331 19:30:27.688810 140313121191680 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.491245836019516, loss=3.4691829681396484
I0331 19:31:01.265837 140313213511424 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.5006395578384399, loss=3.4167566299438477
I0331 19:31:34.129054 140492810225472 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 123.558309888
I0331 19:31:34.129243 140492810225472 spec.py:298] Evaluating on the training split.
I0331 19:31:40.978670 140492810225472 spec.py:310] Evaluating on the validation split.
I0331 19:31:50.796964 140492810225472 spec.py:326] Evaluating on the test split.
I0331 19:31:52.810320 140492810225472 submission_runner.py:382] Time since start: 4959.90s, 	Step: 14000, 	{'train/accuracy': 0.6615114808082581, 'train/loss': 1.6735057830810547, 'validation/accuracy': 0.6079599857330322, 'validation/loss': 1.912116527557373, 'validation/num_examples': 50000, 'test/accuracy': 0.4846000373363495, 'test/loss': 2.5559675693511963, 'test/num_examples': 10000}
I0331 19:31:52.810961 140492810225472 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 128.780619776
I0331 19:31:52.818494 140313121191680 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4692.979426, test/accuracy=0.484600, test/loss=2.555968, test/num_examples=10000, total_duration=4959.904311, train/accuracy=0.661511, train/loss=1.673506, validation/accuracy=0.607960, validation/loss=1.912117, validation/num_examples=50000
I0331 19:31:52.934369 140492810225472 checkpoints.py:356] Saving checkpoint at step: 14000
I0331 19:31:53.608665 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_14000
I0331 19:31:53.618153 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0331 19:31:53.625060 140492810225472 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 128.77275136
I0331 19:31:53.631908 140313213511424 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4692.979426
I0331 19:31:53.750041 140492810225472 checkpoints.py:356] Saving checkpoint at step: 14000
I0331 19:31:54.627262 140492810225472 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_14000
I0331 19:31:54.635446 140492810225472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0331 19:31:54.790606 140492810225472 submission_runner.py:550] Tuning trial 1/1
I0331 19:31:54.790778 140492810225472 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0331 19:31:54.793672 140492810225472 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008769132546149194, 'train/loss': 6.913438320159912, 'validation/accuracy': 0.0008199999574571848, 'validation/loss': 6.912744998931885, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.912395477294922, 'test/num_examples': 10000, 'score': 41.16219758987427, 'total_duration': 41.24273109436035, 'global_step': 1, 'preemption_count': 0}), (1513, {'train/accuracy': 0.07479671388864517, 'train/loss': 5.3983049392700195, 'validation/accuracy': 0.06746000051498413, 'validation/loss': 5.48199987411499, 'validation/num_examples': 50000, 'test/accuracy': 0.049400001764297485, 'test/loss': 5.71992301940918, 'test/num_examples': 10000, 'score': 545.9087255001068, 'total_duration': 591.4542157649994, 'global_step': 1513, 'preemption_count': 0}), (3029, {'train/accuracy': 0.202985480427742, 'train/loss': 4.152096748352051, 'validation/accuracy': 0.18767999112606049, 'validation/loss': 4.262604713439941, 'validation/num_examples': 50000, 'test/accuracy': 0.13650000095367432, 'test/loss': 4.73514461517334, 'test/num_examples': 10000, 'score': 1050.6822109222412, 'total_duration': 1118.7764728069305, 'global_step': 3029, 'preemption_count': 0}), (4545, {'train/accuracy': 0.3386479616165161, 'train/loss': 3.2850255966186523, 'validation/accuracy': 0.3125999867916107, 'validation/loss': 3.412715435028076, 'validation/num_examples': 50000, 'test/accuracy': 0.2346000075340271, 'test/loss': 3.9955453872680664, 'test/num_examples': 10000, 'score': 1555.6464779376984, 'total_duration': 1646.3460144996643, 'global_step': 4545, 'preemption_count': 0}), (6063, {'train/accuracy': 0.42944833636283875, 'train/loss': 2.7982702255249023, 'validation/accuracy': 0.39973998069763184, 'validation/loss': 2.937286376953125, 'validation/num_examples': 50000, 'test/accuracy': 0.30090001225471497, 'test/loss': 3.5337448120117188, 'test/num_examples': 10000, 'score': 2056.47709274292, 'total_duration': 2174.4433221817017, 'global_step': 6063, 'preemption_count': 0}), (7572, {'train/accuracy': 0.4870455861091614, 'train/loss': 2.4594533443450928, 'validation/accuracy': 0.45795997977256775, 'validation/loss': 2.5899932384490967, 'validation/num_examples': 50000, 'test/accuracy': 0.353300005197525, 'test/loss': 3.235905170440674, 'test/num_examples': 10000, 'score': 2557.2163105010986, 'total_duration': 2703.1446495056152, 'global_step': 7572, 'preemption_count': 0}), (9089, {'train/accuracy': 0.5962810516357422, 'train/loss': 1.9831054210662842, 'validation/accuracy': 0.5289799571037292, 'validation/loss': 2.2887020111083984, 'validation/num_examples': 50000, 'test/accuracy': 0.4082000255584717, 'test/loss': 2.9440462589263916, 'test/num_examples': 10000, 'score': 3059.890027999878, 'total_duration': 3232.363771915436, 'global_step': 9089, 'preemption_count': 0}), (10604, {'train/accuracy': 0.6039939522743225, 'train/loss': 1.9151510000228882, 'validation/accuracy': 0.5454199910163879, 'validation/loss': 2.191735029220581, 'validation/num_examples': 50000, 'test/accuracy': 0.4294000267982483, 'test/loss': 2.8266894817352295, 'test/num_examples': 10000, 'score': 3564.6640050411224, 'total_duration': 3761.6514644622803, 'global_step': 10604, 'preemption_count': 0}), (12122, {'train/accuracy': 0.6340680718421936, 'train/loss': 1.7694759368896484, 'validation/accuracy': 0.5816799998283386, 'validation/loss': 2.0031824111938477, 'validation/num_examples': 50000, 'test/accuracy': 0.46330001950263977, 'test/loss': 2.6617660522460938, 'test/num_examples': 10000, 'score': 4069.3964552879333, 'total_duration': 4290.772006750107, 'global_step': 12122, 'preemption_count': 0}), (13641, {'train/accuracy': 0.6539381146430969, 'train/loss': 1.632784128189087, 'validation/accuracy': 0.6012799739837646, 'validation/loss': 1.8768268823623657, 'validation/num_examples': 50000, 'test/accuracy': 0.47460001707077026, 'test/loss': 2.5645172595977783, 'test/num_examples': 10000, 'score': 4574.204206466675, 'total_duration': 4820.351521492004, 'global_step': 13641, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6615114808082581, 'train/loss': 1.6735057830810547, 'validation/accuracy': 0.6079599857330322, 'validation/loss': 1.912116527557373, 'validation/num_examples': 50000, 'test/accuracy': 0.4846000373363495, 'test/loss': 2.5559675693511963, 'test/num_examples': 10000, 'score': 4692.979425907135, 'total_duration': 4959.904310703278, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0331 19:31:54.793777 140492810225472 submission_runner.py:553] Timing: 4692.979425907135
I0331 19:31:54.793820 140492810225472 submission_runner.py:554] ====================
I0331 19:31:54.793910 140492810225472 submission_runner.py:613] Final imagenet_resnet score: 4692.979425907135
